pmid,citations,title,date,text
32112901,3.0,Biofluid diagnostics by FTIR spectroscopy: A platform technology for cancer detection,2020 May 1;477:122-130.,"Fourier Transform Infrared Spectroscopy (FTIR) has been largely employed by scientific researchers to improve diagnosis and treatment of cancer, using various biofluids and tissues. The technology has proved to be easy to use, rapid and cost-effective for analysis on human blood serum to discriminate between cancer versus healthy control samples. The high sensitivity and specificity achievable during samples classification aided by machine learning algorithms, offers an opportunity to transform cancer referral pathways, as it has been demonstrated in a unique and recent prospective clinical validation study on brain tumours. We herein highlight the importance of early detection in cancer research using FTIR, discussing the technique, the suitability of serum for analysis and previous studies, with special focus on pre-clinical factors and clinical translation requirements and development."
32111636,4.0,Big data in IBD: big progress for clinical practice,2020 Aug;69(8):1520-1532.,"IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research."
32111372,2.0,Electronic health records for the diagnosis of rare diseases,2020 Apr;97(4):676-686.,"With the emergence of electronic health records, the reuse of clinical data offers new perspectives for the diagnosis and management of patients with rare diseases. However, there are many obstacles to the repurposing of clinical data. The development of decision support systems depends on the ability to recruit patients, extract and integrate the patients' data, mine and stratify these data, and integrate the decision support algorithm into patient care. This last step requires an adaptability of the electronic health records to integrate learning health system tools. In this literature review, we examine the research that provides solutions to unlock these barriers and accelerate translational research: structured electronic health records and free-text search engines to find patients, data warehouses and natural language processing to extract phenotypes, machine learning algorithms to classify patients, and similarity metrics to diagnose patients. Medical informatics is experiencing an impellent request to develop decision support systems, and this requires ethical considerations for clinicians and patients to ensure appropriate use of health data."
32109428,6.0,Imaging research in fibrotic lung disease; applying deep learning to unsolved problems,2020 Nov;8(11):1144-1153.,"Over the past decade, there has been a groundswell of research interest in computer-based methods for objectively quantifying fibrotic lung disease on high resolution CT of the chest. In the past 5 years, the arrival of deep learning-based image analysis has created exciting new opportunities for enhancing the understanding of, and the ability to interpret, fibrotic lung disease on CT. Specific unsolved problems for which computer-based imaging analysis might provide solutions include the development of reliable methods for assisting with diagnosis, detecting early disease, and predicting disease behaviour using baseline imaging data. However, to harness this technology, technical and societal challenges must be overcome. Large CT datasets will be needed to power the training of deep learning algorithms. Open science research and collaboration between academia and industry must be encouraged. Prospective clinical utility studies will be needed to test computer algorithm performance in real-world clinical settings and demonstrate patient benefit over current best practice. Finally, ethical standards, which ensure patient confidentiality and mitigate against biases in training datasets, that can be encoded in machine-learning systems will be needed as well as bespoke data governance and accountability frameworks to encourage buy-in from health-care professionals, patients, and the public."
32108409,4.0,Will machine learning applied to neuroimaging in bipolar disorder help the clinician? A critical review and methodological suggestions,2020 Jun;22(4):334-355.,"Objectives:                    The existence of anatomofunctional brain abnormalities in bipolar disorder (BD) is now well established by magnetic resonance imaging (MRI) studies. To create diagnostic and prognostic tools, as well as identifying biologically valid subtypes of BD, research has recently turned towards the use of machine learning (ML) techniques. We assessed both supervised ML and unsupervised ML studies in BD to evaluate their robustness, reproducibility and the potential need for improvement.              Method:                    We systematically searched for studies using ML algorithms based on MRI data of patients with BD until February 2019.              Result:                    We identified 47 studies, 45 using supervised ML techniques and 2 including unsupervised ML analyses. Among supervised studies, 43 focused on diagnostic classification. The reported accuracies for classification of BD ranged between (a) 57% and 100%, for BD vs healthy controls; (b) 49.5% and 93.1% for BD vs patients with major depressive disorder; and (c) 50% and 96.2% for BD vs patients with schizophrenia. Reported accuracies for discriminating subjects genetically at risk for BD (either from control or from patients with BD) ranged between 64.3% and 88.93%.              Conclusions:                    Although there are strong methodological limitations in previous studies and an important need for replication in large multicentric samples, the conclusions of our review bring hope of future computer-aided diagnosis of BD and pave the way for other applications, such as treatment response prediction. To reinforce the reliability of future results we provide methodological suggestions for good practice in conducting and reporting MRI-based ML studies in BD."
32105592,,New criteria and new methodological tools for devising criteria sets of inflammatory rheumatic diseases,Jul-Aug 2020;38(4):776-782.,"Rheumatologists use classification criteria to separate patients with inflammatory rheumatic diseases (IRD). They change over time, and the concepts of the diseases also change. The paradigm is currently moving as the goal of classification in the future will be more to select which patients may be relevant for a specific treatment rather than to describe their characteristics. Therefore, the challenge will be to reclassify multifactorial diseases on the basis of their biological mechanisms rather than their clinical phenotype. Currently, various projects are trying to reclassify diseases using bioinformatics approaches and in the near future the use of advanced machine learning algorithms with large omics datasets could lead to new classification models not only based on a clinical phenotype but also on complex biological profile and common sensitivity to targeted treatment. These models would highlight common biological pathways between patients classified in the same cluster and provide a deep understanding of the mechanisms involved in the patient's clinical phenotype. Such approaches would ultimately lead to classification models that rely more on biological causes than on symptoms. This overview on current classification of subgroups of IRD summarises the classification criteria that we use routinely, and how we will classify IRD in the future using bioinformatics and artificial intelligence techniques."
32101448,13.0,Radiomics: from qualitative to quantitative imaging,2020 Apr;93(1108):20190948.,"Historically, medical imaging has been a qualitative or semi-quantitative modality. It is difficult to quantify what can be seen in an image, and to turn it into valuable predictive outcomes. As a result of advances in both computational hardware and machine learning algorithms, computers are making great strides in obtaining quantitative information from imaging and correlating it with outcomes. Radiomics, in its two forms ""handcrafted and deep,"" is an emerging field that translates medical images into quantitative data to yield biological information and enable radiologic phenotypic profiling for diagnosis, theragnosis, decision support, and monitoring. Handcrafted radiomics is a multistage process in which features based on shape, pixel intensities, and texture are extracted from radiographs. Within this review, we describe the steps: starting with quantitative imaging data, how it can be extracted, how to correlate it with clinical and biological outcomes, resulting in models that can be used to make predictions, such as survival, or for detection and classification used in diagnostics. The application of deep learning, the second arm of radiomics, and its place in the radiomics workflow is discussed, along with its advantages and disadvantages. To better illustrate the technologies being used, we provide real-world clinical applications of radiomics in oncology, showcasing research on the applications of radiomics, as well as covering its limitations and its future direction."
32101126,1.0,Decoding Protein-protein Interactions: An Overview,2020;20(10):855-882.,"Drug discovery has focused on the paradigm ""one drug, one target"" for a long time. However, small molecules can act at multiple macromolecular targets, which serves as the basis for drug repurposing. In an effort to expand the target space, and given advances in X-ray crystallography, protein-protein interactions have become an emerging focus area of drug discovery enterprises. Proteins interact with other biomolecules and it is this intricate network of interactions that determines the behavior of the system and its biological processes. In this review, we briefly discuss networks in disease, followed by computational methods for protein-protein complex prediction. Computational methodologies and techniques employed towards objectives such as protein-protein docking, protein-protein interactions, and interface predictions are described extensively. Docking aims at producing a complex between proteins, while interface predictions identify a subset of residues on one protein that could interact with a partner, and protein-protein interaction sites address whether two proteins interact. In addition, approaches to predict hot spots and binding sites are presented along with a representative example of our internal project on the chemokine CXC receptor 3 B-isoform and predictive modeling with IP10 and PF4."
32100604,4.0,Nanotoxicology data for in silico tools: a literature review,2020 Jun;14(5):612-637.,"The exercise of non-testing approaches in nanoparticles (NPs) hazard assessment is necessary for the risk assessment, considering cost and time efficiency, to identify, assess, and classify potential risks. One strategy for investigating the toxicological properties of a variety of NPs is by means of computational tools that decode how nano-specific features relate to toxicity and enable its prediction. This literature review records systematically the data used in published studies that predict nano (eco)-toxicological endpoints using machine learning models. Instead of seeking mechanistic interpretations this review maps the pathways followed, involving biological features in relation to NPs exposure, their physico-chemical characteristics and the most commonly predicted outcomes. The results, derived from published research of the last decade, are summarized visually, providing prior-based data mining paradigms to be readily used by the nanotoxicology community in computational studies."
32096675,2.0,Time-lapse technology for embryo culture and selection,2020 May;125(2):77-84.,"Culturing of human embryos in optimal conditions is crucial for a successful in vitro fertilisation (IVF) programme. In addition, the capacity to assess and rank embryos correctly for quality will allow for transfer of the potentially 'best' embryo first, thereby shortening the time to pregnancy, although not improving cumulative pregnancy and live birth rates. It will also encourage and facilitate the implementation of single embryo transfers, thereby increasing safety for mother and offspring. Time-lapse technology introduces the concept of stable culture conditions, in connection with the possibility of continuous viewing and documenting of the embryo throughout development. However, so far, even when embryo quality scoring is based on large datasets, or when using the time-lapse technology, the morphokinetic scores are still mainly based on subjective and intermittent annotations of morphology and timings. Also, the construction of powerful algorithms for widespread use is hampered by large variations in culture conditions between individual IVF laboratories. New methodology, involving machine learning, where every image from the time-lapse documentation is analysed by a computer programme, looking for patterns that link to outcome, may in the future provide a more accurate and non-biased embryo selection."
32093027,3.0,Application of Artificial Intelligence Techniques to Predict Survival in Kidney Transplantation: A Review,2020 Feb 19;9(2):572.,"A key issue in the field of kidney transplants is the analysis of transplant recipients' survival. By means of the information obtained from transplant patients, it is possible to analyse in which cases a transplant has a higher likelihood of success and the factors on which it will depend. In general, these analyses have been conducted by applying traditional statistical techniques, as the amount and variety of data available about kidney transplant processes were limited. However, two main changes have taken place in this field in the last decade. Firstly, the digitalisation of medical information through the use of electronic health records (EHRs), which store patients' medical histories electronically. This facilitates automatic information processing through specialised software. Secondly, medical Big Data has provided access to vast amounts of data on medical processes. The information currently available on kidney transplants is huge and varied by comparison to that initially available for this kind of study. This new context has led to the use of other non-traditional techniques more suitable to conduct survival analyses in these new conditions. Specifically, this paper provides a review of the main machine learning methods and tools that are being used to conduct kidney transplant patient and graft survival analyses."
32091866,,The Personal Data Is Political,,"The success of personalized medicine does not only rely on methodological advances but also on the availability of data to learn from. While the generation and sharing of large data sets is becoming increasingly easier, there is a remarkable lack of diversity within shared datasets, rendering any novel scientific findings directly applicable only to a small portion of the human population. Here, we are investigating two fields that have been majorly impacted by data sharing initiatives, neuroscience and genetics. Exploring the limitations that are a result of a lack of participant diversity, we propose that data sharing in itself is not enough to enable a global personalized medicine."
32091446,1.0,Machine Learning and Deep Neural Networks Applications in Coronary Flow Assessment: The Case of Computed Tomography Fractional Flow Reserve,2020 May;35 Suppl 1:S66-S71.,"Coronary computed tomography angiography (cCTA) is a reliable and clinically proven method for the evaluation of coronary artery disease. cCTA data sets can be used to derive fractional flow reserve (FFR) as CT-FFR. This method has respectable results when compared in previous trials to invasive FFR, with the aim of detecting lesion-specific ischemia. Results from previous studies have shown many benefits, including improved therapeutic guidance to efficiently justify the management of patients with suspected coronary artery disease and enhanced outcomes and reduced health care costs. More recently, a technical approach to the calculation of CT-FFR using an artificial intelligence deep machine learning (ML) algorithm has been introduced. ML algorithms provide information in a more objective, reproducible, and rational manner and with improved diagnostic accuracy in comparison to cCTA. This review gives an overview of the technical background, clinical validation, and implementation of ML applications in CT-FFR."
32089788,32.0,Causability and explainability of artificial intelligence in medicine,Jul-Aug 2019;9(4):e1312.,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge > Human Centricity and User Interaction."
32088032,1.0,Oocyte and embryo evaluation by AI and multi-spectral auto-fluorescence imaging: Livestock embryology needs to catch-up to clinical practice,2020 Jul 1;150:255-262.,"A highly accurate 'non-invasive quantitative embryo assessment for pregnancy' (NQEAP) technique that determines embryo quality has been an elusive goal. If developed, NQEAP would transform the selection of embryos from both Multiple Ovulation and Embryo Transfer (MOET), and even more so, in vitro produced (IVP) embryos for livestock breeding. The area where this concept is already having impact is in the field of clinical embryology, where great strides have been taken in the application of morphokinetics and artificial intelligence (AI); while both are already in practice, rigorous and robust evidence of efficacy is still required. Even the translation of advances in the qualitative scoring of human IVF embryos have yet to be translated to the livestock IVP industry, which remains dependent on the MOET-standardised 3-point scoring system. Furthermore, there are new ways to interrogate the biochemistry of individual embryonic cells by using new, light-based methodologies, such as FLIM and hyperspectral microscopy. Combinations of these technologies, in particular combining new imaging systems with AI, will lead to very accurate NQEAP predictive tools, improving embryo selection and recipient pregnancy success."
32087008,,"The promise of toxicogenomics for genetic toxicology: past, present and future",2020 Mar 27;35(2):153-159.,"Toxicogenomics, the application of genomics to toxicology, was described as 'a new era' for toxicology. Standard toxicity tests typically involve a number of short-term bioassays that are costly, time consuming, require large numbers of animals and generally focus on a single end point. Toxicogenomics was heralded as a way to improve the efficiency of toxicity testing by assessing gene regulation across the genome, allowing rapid classification of compounds based on characteristic expression profiles. Gene expression microarrays could measure and characterise genome-wide gene expression changes in a single study and while transcriptomic profiles that can discriminate between genotoxic and non-genotoxic carcinogens have been identified, challenges with the approach limited its application. As such, toxicogenomics did not transform the field of genetic toxicology in the way it was predicted. More recently, next generation sequencing (NGS) technologies have revolutionised genomics owing to the fact that hundreds of billions of base pairs can be sequenced simultaneously cheaper and quicker than traditional Sanger methods. In relation to genetic toxicology, and thousands of cancer genomes have been sequenced with single-base substitution mutational signatures identified, and mutation signatures have been identified following treatment of cells with known or suspected environmental carcinogens. RNAseq has been applied to detect transcriptional changes following treatment with genotoxins; modified RNAseq protocols have been developed to identify adducts in the genome and Duplex sequencing is an example of a technique that has recently been developed to accurately detect mutation. Machine learning, including MutationSeq and SomaticSeq, has also been applied to somatic mutation detection and improvements in automation and/or the application of machine learning algorithms may allow high-throughput mutation sequencing in the future. This review will discuss the initial promise of transcriptomics for genetic toxicology, and how the development of NGS technologies and new machine learning algorithms may finally realise that promise."
32086157,,Patient-derived model systems and the development of next-generation anticancer therapeutics,2020 Jun;56:72-78.,"Anticancer drug discovery and development using conventional cell line and animal models has traditionally had a low overall success rate. Despite yielding game-changing new therapeutics, 10-20 new molecules have to be brought to the clinic to obtain one new approval, making this approach costly and inefficient. The use of in vitro experimental models based on primary human tumour tissues has the potential to provide a representation of human cancer biology that is closer to an actual patient and to 'bridge the translational gap' between preclinical and clinical research. Here, we review recent advances in the use of human tumour samples for preclinical research through organoid development or as primary patient materials. While challenges still remain regarding analysis, validation and scalability, evidence is mounting for the applicability of both models as preclinical research tools."
32086017,2.0,Technological advances in field studies of pollinator ecology and the future of e-ecology,2020 Apr;38:15-25.,"Our review looks at recent advances in technologies applied to studying pollinators in the field. These include RFID, radar and lidar for detecting and tracking pollinators; wireless sensor networks (e.g. 'smart' hives); automated visual and audio monitoring systems including vision motion software for monitoring fine-scale pollinator behaviours over extended periods; and automated species identification systems based on machine learning that can vastly reduce the bottleneck in (big) data analysis. An improved e-ecology platform that leverages these tools is needed for ecologists to acquire and understand large spatiotemporal datasets, and thus inform knowledge gaps in environmental policy-making. Developing the next generation of e-ecology tools will require synergistic partnerships between academia and industry and significant investment in a cross-disciplinary scientific consortia."
32085921,8.0,Data-Driven Diagnostics and the Potential of Mobile Artificial Intelligence for Digital Therapeutic Phenotyping in Computational Psychiatry,2020 Aug;5(8):759-769.,"Data science and digital technologies have the potential to transform diagnostic classification. Digital technologies enable the collection of big data, and advances in machine learning and artificial intelligence enable scalable, rapid, and automated classification of medical conditions. In this review, we summarize and categorize various data-driven methods for diagnostic classification. In particular, we focus on autism as an example of a challenging disorder due to its highly heterogeneous nature. We begin by describing the frontier of data science methods for the neuropsychiatry of autism. We discuss early signs of autism as defined by existing pen-and-paper-based diagnostic instruments and describe data-driven feature selection techniques for determining the behaviors that are most salient for distinguishing children with autism from neurologically typical children. We then describe data-driven detection techniques, particularly computer vision and eye tracking, that provide a means of quantifying behavioral differences between cases and controls. We also describe methods of preserving the privacy of collected videos and prior efforts of incorporating humans in the diagnostic loop. Finally, we summarize existing digital therapeutic interventions that allow for data capture and longitudinal outcome tracking as the diagnosis moves along a positive trajectory. Digital phenotyping of autism is paving the way for quantitative psychiatry more broadly and will set the stage for more scalable, accessible, and precise diagnostic techniques in the field."
32085599,4.0,A Review of Data Analytic Applications in Road Traffic Safety. Part 1: Descriptive and Predictive Modeling,2020 Feb 18;20(4):1107.,"This part of the review aims to reduce the start-up burden of data collection and descriptive analytics for statistical modeling and route optimization of risk associated with motor vehicles. From a data-driven bibliometric analysis, we show that the literature is divided into two disparate research streams: (a) predictive or explanatory models that attempt to understand and quantify crash risk based on different driving conditions, and (b) optimization techniques that focus on minimizing crash risk through route/path-selection and rest-break scheduling. Translation of research outcomes between these two streams is limited. To overcome this issue, we present publicly available high-quality data sources (different study designs, outcome variables, and predictor variables) and descriptive analytic techniques (data summarization, visualization, and dimension reduction) that can be used to achieve safer-routing and provide code to facilitate data collection/exploration by practitioners/researchers. Then, we review the statistical and machine learning models used for crash risk modeling. We show that (near) real-time crash risk is rarely considered, which might explain why the optimization models (reviewed in Part 2) have not capitalized on the research outcomes from the first stream."
32083960,4.0,Entering the era of computationally driven drug development,2020 May;52(2):283-298.,"Historically, failure rates in drug development are high; increased sophistication and investment throughout the process has shifted the reasons for attrition, but the overall success rates have remained stubbornly and consistently low. Only 8% of new entities entering clinical testing gain regulatory approval, indicating that significant obstacles still exist for efficient therapeutic development. The continued high failure rate can be partially attributed to the inability to link drug exposure with the magnitude of observed safety and efficacy-related pharmacodynamic (PD) responses; frequently, this is a result of nonclinical models exhibiting poor prediction of human outcomes across a wide range of disease conditions, resulting in faulty evaluation of drug toxicology and efficacy. However, the increasing quality and standardization of experimental methods in preclinical stages of testing has created valuable data sets within companies that can be leveraged to further improve the efficiency and accuracy of preclinical prediction for both pharmacokinetics (PK) and PD. Models of Quantitative structure-activity relationships (QSAR), physiologically based pharmacokinetics (PBPK), and PK/PD relationships have also improved efficiency. Founded on a core understanding of biochemistry and physiological interactions of xenobiotics, these in silico methods have the potential to increase the probability of compound success in clinical trials. Integration of traditional computational methods with machine-learning approaches and existing internal pharma databases stands to make a fundamental impact on the speed and accuracy of predictions during the process of drug development and approval."
32082358,3.0,"Probing lncRNA-Protein Interactions: Data Repositories, Models, and Algorithms",2020 Jan 31;10:1346.,"Identifying lncRNA-protein interactions (LPIs) is vital to understanding various key biological processes. Wet experiments found a few LPIs, but experimental methods are costly and time-consuming. Therefore, computational methods are increasingly exploited to capture LPI candidates. We introduced relevant data repositories, focused on two types of LPI prediction models: network-based methods and machine learning-based methods. Machine learning-based methods contain matrix factorization-based techniques and ensemble learning-based techniques. To detect the performance of computational methods, we compared parts of LPI prediction models on Leave-One-Out cross-validation (LOOCV) and fivefold cross-validation. The results show that SFPEL-LPI obtained the best performance of AUC. Although computational models have efficiently unraveled some LPI candidates, there are many limitations involved. We discussed future directions to further boost LPI predictive performance."
32079904,2.0,"Machine Learning and Deep Neural Networks: Applications in Patient and Scan Preparation, Contrast Medium, and Radiation Dose Optimization",2020 May;35 Suppl 1:S17-S20.,"Artificial intelligence (AI) algorithms are dependent on a high amount of robust data and the application of appropriate computational power and software. AI offers the potential for major changes in cardiothoracic imaging. Beyond image processing, machine learning and deep learning have the potential to support the image acquisition process. AI applications may improve patient care through superior image quality and have the potential to lower radiation dose with AI-driven reconstruction algorithms and may help avoid overscanning. This review summarizes recent promising applications of AI in patient and scan preparation as well as contrast medium and radiation dose optimization."
32073494,,The rise and fall of the model for end-stage liver disease score and the need for an optimized machine learning approach for liver allocation,2020 Apr;25(2):122-125.,"Purpose of review:                    The Model for End-Stage Liver Disease (MELD) has been used to rank liver transplant candidates since 2002, and at the time bringing much needed objectivity to the liver allocation process. However, and despite numerous revisions to the MELD score, current liver allocation still does not allow for equitable access to all waitlisted liver candidates.              Recent findings:                    An optimized prediction of mortality (OPOM) was developed utilizing novel machine-learning optimal classification tree models trained to predict a liver candidate's 3-month waitlist mortality or removal. When compared to MELD and MELD-Na, OPOM more accurately and objectively prioritized candidates for liver transplantation based on disease severity. In simulation analysis, OPOM allowed for more equitable allocation of livers with a resultant significant number of additional lives saved every year when compared with MELD-based allocation.              Summary:                    Machine learning technology holds the potential to help guide transplant clinical practice, and thus potentially guide national organ allocation policy."
32067019,3.0,A review of mathematical representations of biomolecular data,2020 Feb 26;22(8):4343-4367.,"Recently, machine learning (ML) has established itself in various worldwide benchmarking competitions in computational biology, including Critical Assessment of Structure Prediction (CASP) and Drug Design Data Resource (D3R) Grand Challenges. However, the intricate structural complexity and high ML dimensionality of biomolecular datasets obstruct the efficient application of ML algorithms in the field. In addition to data and algorithm, an efficient ML machinery for biomolecular predictions must include structural representation as an indispensable component. Mathematical representations that simplify the biomolecular structural complexity and reduce ML dimensionality have emerged as a prime winner in D3R Grand Challenges. This review is devoted to the recent advances in developing low-dimensional and scalable mathematical representations of biomolecules in our laboratory. We discuss three classes of mathematical approaches, including algebraic topology, differential geometry, and graph theory. We elucidate how the physical and biological challenges have guided the evolution and development of these mathematical apparatuses for massive and diverse biomolecular data. We focus the performance analysis on protein-ligand binding predictions in this review although these methods have had tremendous success in many other applications, such as protein classification, virtual screening, and the predictions of solubility, solvation free energies, toxicity, partition coefficients, protein folding stability changes upon mutation, etc."
32066289,2.0,Machine learning for predicting cardiac events: what does the future hold?,2020 Feb;18(2):77-84.,"Introduction: With the increase in the number of patients with cardiovascular diseases, better risk-prediction models for cardiovascular events are needed. Statistical-based risk-prediction models for cardiovascular events (CVEs) are available, but they lack the ability to predict individual-level risk. Machine learning (ML) methods are especially equipped to handle complex data and provide accurate risk-prediction models at the individual level.Areas covered: In this review, the authors summarize the literature comparing the performance of machine learning methods to that of traditional, statistical-based models in predicting CVEs. They provide a brief summary of ML methods and then discuss risk-prediction models for CVEs such as major adverse cardiovascular events, heart failure and arrhythmias.Expert opinion: Current evidence supports the superiority of ML methods over statistical-based models in predicting CVEs. Statistical models are applicable at the population level and are subject to overfitting, while ML methods can provide an individualized risk level for CVEs. Further prospective research on ML-guided treatments to prevent CVEs is needed."
32065835,2.0,"Reading patterns of proteome damage by glycation, oxidation and nitration: quantitation by stable isotopic dilution analysis LC-MS/MS",2020 Feb 17;64(1):169-183.,"Liquid chromatography-tandem mass spectrometry (LC-MS/MS) provides a high sensitivity, high specificity multiplexed method for concurrent detection of adducts formed by protein glycation, oxidation and nitration, also called AGEomics. Combined with stable isotopic dilution analysis, it provides for robust quantitation of protein glycation, oxidation and nitration adduct analytes. It is the reference method for such measurements. LC-MS/MS has been used to measure glycated, oxidized and nitrated amino acids - also called glycation, oxidation and nitration free adducts, with a concurrent quantitation of the amino acid metabolome in physiological fluids. Similar adduct residues in proteins may be quantitated with prior exhaustive enzymatic hydrolysis. It has also been applied to quantitation of other post-translation modifications, such as citrullination and formation of Nε-(γ-glutamyl)lysine crosslink by transglutaminases. Application to cellular and extracellular proteins gives estimates of the steady-state levels of protein modification by glycation, oxidation and nitration, and measurement of the accumulation of glycation, oxidation and nitration adducts in cell culture medium and urinary excretion gives an indication of flux of adduct formation. Measurement of glycation, oxidation and nitration free adducts in plasma and urine provides for estimates of renal clearance of free adducts. Diagnostic potential in clinical studies has been enhanced by the combination of estimates of multiple adducts in optimized diagnostic algorithms by machine learning. Recent applications have been in early-stage detection of metabolic, vascular and renal disease, and arthritis, metabolic control and risk of developing vascular complication in diabetes, and a blood test for autism."
32063919,3.0,Heterogeneous Multi-Layered Network Model for Omics Data Integration and Analysis,2020 Jan 28;10:1381.,"Advances in next-generation sequencing and high-throughput techniques have enabled the generation of vast amounts of diverse omics data. These big data provide an unprecedented opportunity in biology, but impose great challenges in data integration, data mining, and knowledge discovery due to the complexity, heterogeneity, dynamics, uncertainty, and high-dimensionality inherited in the omics data. Network has been widely used to represent relations between entities in biological system, such as protein-protein interaction, gene regulation, and brain connectivity (i.e. network construction) as well as to infer novel relations given a reconstructed network (aka link prediction). Particularly, heterogeneous multi-layered network (HMLN) has proven successful in integrating diverse biological data for the representation of the hierarchy of biological system. The HMLN provides unparalleled opportunities but imposes new computational challenges on establishing causal genotype-phenotype associations and understanding environmental impact on organisms. In this review, we focus on the recent advances in developing novel computational methods for the inference of novel biological relations from the HMLN. We first discuss the properties of biological HMLN. Then we survey four categories of state-of-the-art methods (matrix factorization, random walk, knowledge graph, and deep learning). Thirdly, we demonstrate their applications to omics data integration and analysis. Finally, we outline strategies for future directions in the development of new HMLN models."
32062850,2.0,DNA Methylation-Based Biomarkers of Environmental Exposures for Human Population Studies,2020 Jun;7(2):121-128.,"Purpose of review:                    This manuscript orients the reader to the underlying motivations of environmental biomarker development for human population studies and provides the foundation for applying these novel biomarkers in future research. In this review, we focus our attention on the DNA methylation-based biomarkers of (i) smoking, among adults and pregnant women, (ii) lifetime cannabis use, (iii) alcohol consumption, and (iv) cumulative exposure to lead.              Recent findings:                    Prior environmental exposures and lifestyle modulate DNA methylation levels. Exposure-related DNA methylation changes can either be persistent or reversible once the exposure is no longer present, and this combination of both persistent and reversible changes has essential value for biomarker development. Here, we present available biomarkers representing past and cumulative exposures using individual DNA methylation profiles. In the present work, we describe how the field of environmental epigenetics can leverage machine learning algorithms to develop exposure biomarkers and reduce problems of misreporting exposures or limited access technology. We emphasize the crucial role of the individual DNA methylation profiles in those predictions, providing a summary of each biomarker, and highlighting their advantages, and limitations. Future research can cautiously leverage these DNA methylation-based biomarkers to understand the onset and progression of diseases."
32062767,2.0,Big data and data processing in rheumatology: bioethical perspectives,2020 Apr;39(4):1007-1014.,"Big data analytics and processing through artificial intelligence (AI) are increasingly being used in the health sector. This includes both clinical and research settings, and newly in specialties like rheumatology. It is, however, important to consider how these new methodologies are used, and particularly the sensitivities associated with personal information. Based on current applications in rheumatology, this article provides a narrative review of the bioethical perspectives of big data. It presents examples of databases, data analytic methods, and AI in this specialty to address four main ethical issues: privacy and confidentiality, informed consent, the impact on the medical profession, and justice. The use of big data and AI processing in healthcare has great potential to improve the quality of clinical care, including through better diagnosis, treatment, and prognosis. They may also increase patient and societal participation and engagement in healthcare and research. Developing these methodologies and using the information generated from them in line with ethical standards could positively affect the design of global health policies and introduce a new phase in the democratization of health.Key Points• Current applications of big data, data analytics, and AI in rheumatology-including registries, machine learning algorithms, and consumer-facing platforms-raise issues in four main bioethical areas: privacy and confidentiality, informed consent, the impact on the medical profession, and justice.• Bioethical concerns about rheumatology registries require careful consideration of privacy provisions, set within the context of local, national, and regional law.• Machine learning and big data aid diagnosis, treatment, and prognosis, but the final decision about the use of information from algorithms should be left to rheumatology specialists to maintain the promise of fiduciary obligations in the physician-patient relationship.• International collaboration in big data projects and increased patient engagement could be ways to counteract health inequalities in the practice of rheumatology, even on a global scale."
32061798,6.0,"Machine learning in infection management using routine electronic health records: tools, techniques, and reporting of future technologies",2020 Oct;26(10):1291-1299.,"Background:                    Machine learning (ML) is increasingly being used in many areas of health care. Its use in infection management is catching up as identified in a recent review in this journal. We present here a complementary review to this work.              Objectives:                    To support clinicians and researchers in navigating through the methodological aspects of ML approaches in the field of infection management.              Sources:                    A Medline search was performed with the keywords artificial intelligence, machine learning, infection∗, and infectious disease∗ for the years 2014-2019. Studies using routinely available electronic hospital record data from an inpatient setting with a focus on bacterial and fungal infections were included.              Content:                    Fifty-two studies were included and divided into six groups based on their focus. These studies covered detection/prediction of sepsis (n = 19), hospital-acquired infections (n = 11), surgical site infections and other postoperative infections (n = 11), microbiological test results (n = 4), infections in general (n = 2), musculoskeletal infections (n = 2), and other topics (urinary tract infections, deep fungal infections, antimicrobial prescriptions; n = 1 each). In total, 35 different ML techniques were used. Logistic regression was applied in 18 studies followed by random forest, support vector machines, and artificial neural networks in 18, 12, and seven studies, respectively. Overall, the studies were very heterogeneous in their approach and their reporting. Detailed information on data handling and software code was often missing. Validation on new datasets and/or in other institutions was rarely done. Clinical studies on the impact of ML in infection management were lacking.              Implications:                    Promising approaches for ML use in infectious diseases were identified. But building trust in these new technologies will require improved reporting. Explainability and interpretability of the models used were rarely addressed and should be further explored. Independent model validation and clinical studies evaluating the added value of ML approaches are needed."
32061795,7.0,Machine learning in the clinical microbiology laboratory: has the time come for routine practice?,2020 Oct;26(10):1300-1309.,"Background:                    Machine learning (ML) allows the analysis of complex and large data sets and has the potential to improve health care. The clinical microbiology laboratory, at the interface of clinical practice and diagnostics, is of special interest for the development of ML systems.              Aims:                    This narrative review aims to explore the current use of ML In clinical microbiology.              Sources:                    References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, arXiV, ACM Digital Library and IEEE Xplore Digital Library up to November 2019.              Content:                    We found 97 ML systems aiming to assist clinical microbiologists. Overall, 82 ML systems (85%) targeted bacterial infections, 11 (11%) parasitic infections, nine (9%) viral infections and three (3%) fungal infections. Forty ML systems (41%) focused on microorganism detection, identification and quantification, 36 (37%) evaluated antimicrobial susceptibility, and 21 (22%) targeted the diagnosis, disease classification and prediction of clinical outcomes. The ML systems used very diverse data sources: 21 (22%) used genomic data of microorganisms, 19 (20%) microbiota data obtained by metagenomic sequencing, 19 (20%) analysed microscopic images, 17 (18%) spectroscopy data, eight (8%) targeted gene sequencing, six (6%) volatile organic compounds, four (4%) photographs of bacterial colonies, four (4%) transcriptome data, three (3%) protein structure, and three (3%) clinical data. Most systems used data from high-income countries (n = 71, 73%) but a significant number used data from low- and middle-income countries (n = 36, 37%). Performance measures were reported for the 97 ML systems, but no article described their use in clinical practice or reported impact on processes or clinical outcomes.              Implications:                    In clinical microbiology, ML has been used with various data sources and diverse practical applications. The evaluation and implementation processes represent the main gap in existing ML systems, requiring a focus on their interpretability and potential integration into real-world settings."
32060715,6.0,Radiomics of computed tomography and magnetic resonance imaging in renal cell carcinoma-a systematic review and meta-analysis,2020 Jun;30(6):3558-3566.,"Objectives:                    (1) To assess the methodological quality of radiomics studies investigating histological subtypes, therapy response, and survival in patients with renal cell carcinoma (RCC) and (2) to determine the risk of bias in these radiomics studies.              Methods:                    In this systematic review, literature published since 2000 on radiomics in RCC was included and assessed for methodological quality using the Radiomics Quality Score. The risk of bias was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool and a meta-analysis of radiomics studies focusing on differentiating between angiomyolipoma without visible fat and RCC was performed.              Results:                    Fifty-seven studies investigating the use of radiomics in renal cancer were identified, including 4590 patients in total. The average Radiomics Quality Score was 3.41 (9.4% of total) with good inter-rater agreement (ICC 0.96, 95% CI 0.93-0.98). Three studies validated results with an independent dataset, one used a publically available validation dataset. None of the studies shared the code, images, or regions of interest. The meta-analysis showed moderate heterogeneity among the included studies and an odds ratio of 6.24 (95% CI 4.27-9.12; p < 0.001) for the differentiation of angiomyolipoma without visible fat from RCC.              Conclusions:                    Radiomics algorithms show promise for answering clinical questions where subjective interpretation is challenging or not established. However, the generalizability of findings to prospective cohorts needs to be demonstrated in future trials for progression towards clinical translation. Improved sharing of methods including code and images could facilitate independent validation of radiomics signatures.              Key points:                    • Studies achieved an average Radiomics Quality Score of 10.8%. Common reasons for low Radiomics Quality Scores were unvalidated results, retrospective study design, absence of open science, and insufficient control for multiple comparisons. • A previous training phase allowed reaching almost perfect inter-rater agreement in the application of the Radiomics Quality Score. • Meta-analysis of radiomics studies distinguishing angiomyolipoma without visible fat from renal cell carcinoma show moderate diagnostic odds ratios of 6.24 and moderate methodological diversity."
32060219,14.0,Introduction to Radiomics,2020 Apr;61(4):488-495.,"Radiomics is a rapidly evolving field of research concerned with the extraction of quantitative metrics-the so-called radiomic features-within medical images. Radiomic features capture tissue and lesion characteristics such as heterogeneity and shape and may, alone or in combination with demographic, histologic, genomic, or proteomic data, be used for clinical problem solving. The goal of this continuing education article is to provide an introduction to the field, covering the basic radiomics workflow: feature calculation and selection, dimensionality reduction, and data processing. Potential clinical applications in nuclear medicine that include PET radiomics-based prediction of treatment response and survival will be discussed. Current limitations of radiomics, such as sensitivity to acquisition parameter variations, and common pitfalls will also be covered."
32057708,3.0,Accelerating the future of cardiac CT: Social media as sine qua non?,Sep-Oct 2020;14(5):382-385.,"The vision for the Journal of Cardiovascular Computed Tomography's social media efforts is to amplify the impact of the Journal while driving engagement, increasing journal visibility and disseminating content to new audiences globally. Serving as ""the front door"" to the Journal, this digital evolution represents an important step forward for a field in which advancements in hardware, image processing and clinical evidence have evolved rapidly. However, is social media the panem et circenses of cardiovascular computed tomography (CT), that of superficial appeasement, or of sine qua non; an essential ingredient to the acceleration of the Journal and of the field of cardiovascular CT? This paper aims to present the initial impact of social media within a dedicated cardiovascular CT journal."
32057617,1.0,Magnetic Resonance Texture Analysis in Alzheimer's disease,2020 Dec;27(12):1774-1783.,"Texture analysis is an emerging field that allows mathematical detection of changes in MRI signals that are not visible among image pixels. Alzheimer's disease, a progressive neurodegenerative disease, is the most common cause of dementia. Recently, multiple texture analysis studies in patients with Alzheimer's disease have been performed. This review summarizes the main contributors to Alzheimer's disease-associated cognitive decline, presents a brief overview of texture analysis, followed by review of various MR imaging texture analysis applications in Alzheimer's disease. We also discuss the current challenges for widespread clinical utilization. MR texture analysis could potentially be applied to develop neuroimaging biomarkers for use in Alzheimer's disease clinical trials and diagnosis."
32057098,1.0,Methylation-based algorithms for diagnosis: experience from neuro-oncology,2020 Apr;250(5):510-517.,"Brain tumours are the most common tumour-related cause of death in young people. Survivors are at risk of significant disability, at least in part related to the effects of treatment. Therefore, there is a need for a precise diagnosis that stratifies patients for the most suitable treatment, matched to the underlying biology of their tumour. Although traditional histopathology has been accurate in predicting treatment responses in many cases, molecular profiling has revealed a remarkable, previously unappreciated, level of biological complexity in the classification of these tumours. Among different molecular technologies, DNA methylation profiling has had the most pronounced impact on brain tumour classification. Furthermore, using machine learning-based algorithms, DNA methylation profiling is changing diagnostic practice. This can be regarded as an exemplar for how molecular pathology can influence diagnostic practice and illustrates some of the unanticipated benefits and risks. © 2020 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd."
32056910,7.0,Feeling down? A systematic review of the gut microbiota in anxiety/depression and irritable bowel syndrome,2020 Apr 1;266:429-446.,"Background Anxiety/depression and irritable bowel syndrome (IBS) are highly prevalent and burdensome conditions, whose co-occurrence is estimated between 44 and 84%. Shared gut microbiota alterations have been identified in these separate disorders relative to controls; however, studies have not adequately considered their comorbidity. This review set out to identify case-control studies comparing the gut microbiota in anxiety/depression, IBS, and both conditions comorbidly relative to each other and to controls, as well as gut microbiota investigations including measures of both IBS and anxiety/depression. Methods Four databases were systematically searched using comprehensive search terms (OVID Medline, Embase, PsycINFO, and PubMed), following PRISMA guidelines. Results Systematic review identified 17 studies (10 human, 7 animal). Most studies investigated the gut microbiota and anxiety/depression symptoms in IBS cohorts. Participants with IBS and high anxiety/depression symptoms had lower alpha diversity compared to controls and IBS-only cohorts. Machine learning and beta diversity distinguished between IBS participants with and without anxiety/depression by their gut microbiota. Comorbid IBS and anxiety/depression also had higher abundance of Proteobacteria, Prevotella/Prevotellaceae, Bacteroides and lower Lachnospiraceae relative to controls. Limitations A large number of gut microbiota estimation methods and statistical techniques were utilized; therefore, meta-analysis was not possible. Conclusions Well-designed case-control and longitudinal studies are required to disentangle whether the gut microbiota is predicted as a continuum of gastrointestinal and anxiety/depression symptom severity, or whether reported dysbiosis is unique to IBS and anxiety/depression comorbidity. These findings may inform the development of targeted treatment through the gut microbiota for individuals with both anxiety/depression and IBS."
32055582,14.0,A review of computational drug repurposing,2019 Jun;27(2):59-63.,"Although sciences and technology have progressed rapidly, de novo drug development has been a costly and time-consuming process over the past decades. In view of these circumstances, 'drug repurposing' (or 'drug repositioning') has appeared as an alternative tool to accelerate drug development process by seeking new indications for already approved drugs rather than discovering de novo drug compounds, nowadays accounting for 30% of newly marked drugs in the U.S. In the meantime, the explosive and large-scale growth of molecular, genomic and phenotypic data of pharmacological compounds is enabling the development of new area of drug repurposing called computational drug repurposing. This review provides an overview of recent progress in the area of computational drug repurposing. First, it summarizes available repositioning strategies, followed by computational methods commonly used. Then, it describes validation techniques for repurposing studies. Finally, it concludes by discussing the remaining challenges in computational repurposing."
32055502,7.0,Teledermatology and its Current Perspective,2020 Jan 13;11(1):12-20.,"Teledermatology is one of the most important and commonly employed subsets of telemedicine, a special alternative to face-to-face (FTF) doctor--patient consultation that refers to the use of electronic telecommunication tools to facilitate the provision of healthcare between the ""seeker"" and ""provider."" It is used for consultation, education, second opinion, and monitoring medical conditions. This article will review basic concepts, the integration of noninvasive imaging technique images, artificial intelligence, and the current ethical and legal issues."
32054042,7.0,Deep Learning in Physiological Signal Data: A Survey,2020 Feb 11;20(4):969.,"Deep Learning (DL), a successful promising approach for discriminative and generative tasks, has recently proved its high potential in 2D medical imaging analysis; however, physiological data in the form of 1D signals have yet to be beneficially exploited from this novel approach to fulfil the desired medical tasks. Therefore, in this paper we survey the latest scientific research on deep learning in physiological signal data such as electromyogram (EMG), electrocardiogram (ECG), electroencephalogram (EEG), and electrooculogram (EOG). We found 147 papers published between January 2018 and October 2019 inclusive from various journals and publishers. The objective of this paper is to conduct a detailed study to comprehend, categorize, and compare the key parameters of the deep-learning approaches that have been used in physiological signal analysis for various medical applications. The key parameters of deep-learning approach that we review are the input data type, deep-learning task, deep-learning model, training architecture, and dataset sources. Those are the main key parameters that affect system performance. We taxonomize the research works using deep-learning method in physiological signal analysis based on: (1) physiological signal data perspective, such as data modality and medical application; and (2) deep-learning concept perspective such as training architecture and dataset sources."
32049766,2.0,Anomalies in language as a biomarker for schizophrenia,2020 May;33(3):212-218.,"Purpose of review:                    After more than a century of neuroscience research, reproducible, clinically relevant biomarkers for schizophrenia have not yet been established. This article reviews current advances in evaluating the use of language as a diagnostic or prognostic tool in schizophrenia.              Recent findings:                    The development of computational linguistic tools to quantify language disturbances is rapidly gaining ground in the field of schizophrenia research. Current applications are the use of semantic space models and acoustic analyses focused on phonetic markers. These features are used in machine learning models to distinguish patients with schizophrenia from healthy controls or to predict conversion to psychosis in high-risk groups, reaching accuracy scores (generally ranging from 80 to 90%) that exceed clinical raters. Other potential applications for a language biomarker in schizophrenia are monitoring of side effects, differential diagnostics and relapse prevention.              Summary:                    Language disturbances are a key feature of schizophrenia. Although in its early stages, the emerging field of research focused on computational linguistics suggests an important role for language analyses in the diagnosis and prognosis of schizophrenia. Spoken language as a biomarker for schizophrenia has important advantages because it can be objectively and reproducibly quantified. Furthermore, language analyses are low-cost, time efficient and noninvasive in nature."
32049747,2.0,Machine learning in nephrology: scratching the surface,2020 Mar 20;133(6):687-698.,"Machine learning shows enormous potential in facilitating decision-making regarding kidney diseases. With the development of data preservation and processing, as well as the advancement of machine learning algorithms, machine learning is expected to make remarkable breakthroughs in nephrology. Machine learning models have yielded many preliminaries to moderate and several excellent achievements in the fields, including analysis of renal pathological images, diagnosis and prognosis of chronic kidney diseases and acute kidney injury, as well as management of dialysis treatments. However, it is just scratching the surface of the field; at the same time, machine learning and its applications in renal diseases are facing a number of challenges. In this review, we discuss the application status, challenges and future prospects of machine learning in nephrology to help people further understand and improve the capacity for prediction, detection, and care quality in kidney diseases."
32049743,2.0,Localizing the epileptogenic zone,2020 Apr;33(2):198-206.,"Purpose of review:                    Epilepsy surgery is the therapy of choice for 30-40% of people with focal drug-resistant epilepsy. Currently only ∼60% of well selected patients become postsurgically seizure-free underlining the need for better tools to identify the epileptogenic zone. This article reviews the latest neurophysiological advances for EZ localization with emphasis on ictal EZ identification, interictal EZ markers, and noninvasive neurophysiological mapping procedures.              Recent findings:                    We will review methods for computerized EZ assessment, summarize computational network approaches for outcome prediction and individualized surgical planning. We will discuss electrical stimulation as an option to reduce the time needed for presurgical work-up. We will summarize recent research regarding high-frequency oscillations, connectivity measures, and combinations of multiple markers using machine learning. This latter was shown to outperform single markers. The role of NREM sleep for best identification of the EZ interictally will be discussed. We will summarize recent large-scale studies using electrical or magnetic source imaging for clinical decision-making.              Summary:                    New approaches based on technical advancements paired with artificial intelligence are on the horizon for better EZ identification. They are ultimately expected to result in a more efficient, less invasive, and less time-demanding presurgical investigation."
32048244,4.0,Machine Learning for Pulmonary and Critical Care Medicine: A Narrative Review,2020 Jun;6(1):67-77.,"Machine learning (ML) is a discipline of computer science in which statistical methods are applied to data in order to classify, predict, or optimize, based on previously observed data. Pulmonary and critical care medicine have seen a surge in the application of this methodology, potentially delivering improvements in our ability to diagnose, treat, and better understand a multitude of disease states. Here we review the literature and provide a detailed overview of the recent advances in ML as applied to these areas of medicine. In addition, we discuss both the significant benefits of this work as well as the challenges in the implementation and acceptance of this non-traditional methodology for clinical purposes."
32047606,,Objective Pain Assessment: a Key for the Management of Chronic Pain,2020 Jan 23;9:F1000 Faculty Rev-35.,"The individual and social burdens associated with chronic pain have been escalating globally. Accurate pain measurement facilitates early diagnosis, disease progression monitoring and therapeutic efficacy evaluation, thus is a key for the management of chronic pain. Although the ""golden standards"" of pain measurement are self-reported scales in clinical practice, the reliability of these subjective methods could be easily affected by patients' physiological and psychological status, as well as the assessors' predispositions. Therefore, objective pain assessment has attracted substantial attention recently. Previous studies of functional magnetic resonance imaging (fMRI) revealed that certain cortices and subcortical areas are commonly activated in subjects suffering from pain. Dynamic pain connectome analysis also found various alterations of neural network connectivity that are correlated with the severity of clinical pain symptoms. Electroencephalograph (EEG) demonstrated suppressed spontaneous oscillations during pain experience. Spectral power and coherence analysis of EEG also identified signatures of different types of chronic pain. Furthermore, fMRI and EEG can visualize objective brain activities modulated by analgesics in a mechanism-based way, thus bridge the gaps between animal studies and clinical trials. Using fMRI and EEG, researchers are able to predict therapeutic efficacy and identify personalized optimal first-line regimens. In the future, the emergence of magnetic resonance spectroscopy and cell labelling in MRI would encourage the investigation on metabolic and cellular pain biomarkers. The incorporation of machine learning algorithms with neuroimaging or behavior analysis could further enhance the specificity and accuracy of objective pain assessments."
32047333,9.0,"Artificial intelligence, machine learning, computer-aided diagnosis, and radiomics: advances in imaging towards to precision medicine",Nov-Dec 2019;52(6):387-396.,"The discipline of radiology and diagnostic imaging has evolved greatly in recent years. We have observed an exponential increase in the number of exams performed, subspecialization of medical fields, and increases in accuracy of the various imaging methods, making it a challenge for the radiologist to ""know everything about all exams and regions"". In addition, imaging exams are no longer only qualitative and diagnostic, providing now quantitative information on disease severity, as well as identifying biomarkers of prognosis and treatment response. In view of this, computer-aided diagnosis systems have been developed with the objective of complementing diagnostic imaging and helping the therapeutic decision-making process. With the advent of artificial intelligence, ""big data"", and machine learning, we are moving toward the rapid expansion of the use of these tools in daily life of physicians, making each patient unique, as well as leading radiology toward the concept of multidisciplinary approach and precision medicine. In this article, we will present the main aspects of the computational tools currently available for analysis of images and the principles of such analysis, together with the main terms and concepts involved, as well as examining the impact that the development of artificial intelligence has had on radiology and diagnostic imaging."
32042829,2.0,Gene-gene interaction: the curse of dimensionality,2019 Dec;7(24):813.,"Identified genetic variants from genome wide association studies frequently show only modest effects on the disease risk, leading to the ""missing heritability"" problem. An avenue, to account for a part of this ""missingness"" is to evaluate gene-gene interactions (epistasis) thereby elucidating their effect on complex diseases. This can potentially help with identifying gene functions, pathways, and drug targets. However, the exhaustive evaluation of all possible genetic interactions among millions of single nucleotide polymorphisms (SNPs) raises several issues, otherwise known as the ""curse of dimensionality"". The dimensionality involved in the epistatic analysis of such exponentially growing SNPs diminishes the usefulness of traditional, parametric statistical methods. With the immense popularity of multifactor dimensionality reduction (MDR), a non-parametric method, proposed in 2001, that classifies multi-dimensional genotypes into one- dimensional binary approaches, led to the emergence of a fast-growing collection of methods that were based on the MDR approach. Moreover, machine-learning (ML) methods such as random forests and neural networks (NNs), deep-learning (DL) approaches, and hybrid approaches have also been applied profusely, in the recent years, to tackle this dimensionality issue associated with whole genome gene-gene interaction studies. However, exhaustive searching in MDR based approaches or variable selection in ML methods, still pose the risk of missing out on relevant SNPs. Furthermore, interpretability issues are a major hindrance for DL methods. To minimize this loss of information, Python based tools such as PySpark can potentially take advantage of distributed computing resources in the cloud, to bring back smaller subsets of data for further local analysis. Parallel computing can be a powerful resource that stands to fight this ""curse"". PySpark supports all standard Python libraries and C extensions thus making it convenient to write codes to deliver dramatic improvements in processing speed for extraordinarily large sets of data."
32041303,,Solution of Levinthal's Paradox and a Physical Theory of Protein Folding Times,2020 Feb 6;10(2):250.,"""How do proteins fold?"" Researchers have been studying different aspects of this question for more than 50 years. The most conceptual aspect of the problem is how protein can find the global free energy minimum in a biologically reasonable time, without exhaustive enumeration of all possible conformations, the so-called ""Levinthal's paradox."" Less conceptual but still critical are aspects about factors defining folding times of particular proteins and about perspectives of machine learning for their prediction. We will discuss in this review the key ideas and discoveries leading to the current understanding of folding kinetics, including the solution of Levinthal's paradox, as well as the current state of the art in the prediction of protein folding times."
32039241,13.0,Image-Based Cardiac Diagnosis With Machine Learning: A Review,2020 Jan 24;7:1.,"Cardiac imaging plays an important role in the diagnosis of cardiovascular disease (CVD). Until now, its role has been limited to visual and quantitative assessment of cardiac structure and function. However, with the advent of big data and machine learning, new opportunities are emerging to build artificial intelligence tools that will directly assist the clinician in the diagnosis of CVDs. This paper presents a thorough review of recent works in this field and provide the reader with a detailed presentation of the machine learning methods that can be further exploited to enable more automated, precise and early diagnosis of most CVDs."
32039240,1.0,Artificial Intelligence for Cardiac Imaging-Genetics Research,2020 Jan 21;6:195.,"Cardiovascular conditions remain the leading cause of mortality and morbidity worldwide, with genotype being a significant influence on disease risk. Cardiac imaging-genetics aims to identify and characterize the genetic variants that influence functional, physiological, and anatomical phenotypes derived from cardiovascular imaging. High-throughput DNA sequencing and genotyping have greatly accelerated genetic discovery, making variant interpretation one of the key challenges in contemporary clinical genetics. Heterogeneous, low-fidelity phenotyping and difficulties integrating and then analyzing large-scale genetic, imaging and clinical datasets using traditional statistical approaches have impeded process. Artificial intelligence (AI) methods, such as deep learning, are particularly suited to tackle the challenges of scalability and high dimensionality of data and show promise in the field of cardiac imaging-genetics. Here we review the current state of AI as applied to imaging-genetics research and discuss outstanding methodological challenges, as the field moves from pilot studies to mainstream applications, from one dimensional global descriptors to high-resolution models of whole-organ shape and function, from univariate to multivariate analysis and from candidate gene to genome-wide approaches. Finally, we consider the future directions and prospects of AI imaging-genetics for ultimately helping understand the genetic and environmental underpinnings of cardiovascular health and disease."
32039237,6.0,Machine Learning for Assessment of Coronary Artery Disease in Cardiac CT: A Survey,2019 Nov 26;6:172.,"Cardiac computed tomography (CT) allows rapid visualization of the heart and coronary arteries with high spatial resolution. However, analysis of cardiac CT scans for manifestation of coronary artery disease is time-consuming and challenging. Machine learning (ML) approaches have the potential to address these challenges with high accuracy and consistent performance. In this mini review, we present a survey of the literature on ML-based analysis of coronary artery disease in cardiac CT. We summarize ML methods for detection and characterization of atherosclerotic plaque as well as anatomically and functionally significant coronary artery stenosis."
32039134,5.0,Deep Learning for Deep Chemistry: Optimizing the Prediction of Chemical Patterns,2019 Nov 26;7:809.,"Computational Chemistry is currently a synergistic assembly between ab initio calculations, simulation, machine learning (ML) and optimization strategies for describing, solving and predicting chemical data and related phenomena. These include accelerated literature searches, analysis and prediction of physical and quantum chemical properties, transition states, chemical structures, chemical reactions, and also new catalysts and drug candidates. The generalization of scalability to larger chemical problems, rather than specialization, is now the main principle for transforming chemical tasks in multiple fronts, for which systematic and cost-effective solutions have benefited from ML approaches, including those based on deep learning (e.g. quantum chemistry, molecular screening, synthetic route design, catalysis, drug discovery). The latter class of ML algorithms is capable of combining raw input into layers of intermediate features, enabling bench-to-bytes designs with the potential to transform several chemical domains. In this review, the most exciting developments concerning the use of ML in a range of different chemical scenarios are described. A range of different chemical problems and respective rationalization, that have hitherto been inaccessible due to the lack of suitable analysis tools, is thus detailed, evidencing the breadth of potential applications of these emerging multidimensional approaches. Focus is given to the models, algorithms and methods proposed to facilitate research on compound design and synthesis, materials design, prediction of binding, molecular activity, and soft matter behavior. The information produced by pairing Chemistry and ML, through data-driven analyses, neural network predictions and monitoring of chemical systems, allows (i) prompting the ability to understand the complexity of chemical data, (ii) streamlining and designing experiments, (ii) discovering new molecular targets and materials, and also (iv) planning or rethinking forthcoming chemical challenges. In fact, optimization engulfs all these tasks directly."
32038544,12.0,Computer-Aided Design of Antimicrobial Peptides: Are We Generating Effective Drug Candidates?,2020 Jan 22;10:3097.,"Antimicrobial peptides (AMPs), especially antibacterial peptides, have been widely investigated as potential alternatives to antibiotic-based therapies. Indeed, naturally occurring and synthetic AMPs have shown promising results against a series of clinically relevant bacteria. Even so, this class of antimicrobials has continuously failed clinical trials at some point, highlighting the importance of AMP optimization. In this context, the computer-aided design of AMPs has put together crucial information on chemical parameters and bioactivities in AMP sequences, thus providing modes of prediction to evaluate the antibacterial potential of a candidate sequence before synthesis. Quantitative structure-activity relationship (QSAR) computational models, for instance, have greatly contributed to AMP sequence optimization aimed at improved biological activities. In addition to machine-learning methods, the de novo design, linguistic model, pattern insertion methods, and genetic algorithms, have shown the potential to boost the automated design of AMPs. However, how successful have these approaches been in generating effective antibacterial drug candidates? Bearing this in mind, this review will focus on the main computational strategies that have generated AMPs with promising activities against pathogenic bacteria, as well as anti-infective potential in different animal models, including sepsis and cutaneous infections. Moreover, we will point out recent studies on the computer-aided design of antibiofilm peptides. As expected from automated design strategies, diverse candidate sequences with different structural arrangements have been generated and deposited in databases. We will, therefore, also discuss the structural diversity that has been engendered."
32038482,4.0,Nodular Thyroid Disease in the Era of Precision Medicine,2020 Jan 23;10:907.,"Management of thyroid nodules in the era of precision medicine is continuously changing. Neck ultrasound plays a pivotal role in the diagnosis and several ultrasound stratification systems have been proposed in order to predict malignancy and help clinicians in therapeutic and follow-up decision. Ultrasound elastosonography is another powerful diagnostic technique and can be an added value to stratify the risk of malignancy of thyroid nodules. Moreover, the development of new techniques in the era of ""Deep Learning,"" has led to a creation of machine-learning algorithms based on ultrasound examinations that showed similar accuracy to that obtained by expert radiologists. Despite new technologies in thyroid imaging, diagnostic surgery in 50-70% of patients with indeterminate cytology is still performed. Molecular tests can increase accuracy in diagnosis when performed on ""indeterminate"" nodules. However, the more updated tools that can be used to this purpose in order to ""rule out"" (Afirma GSC) or ""rule in"" (Thyroseq v3) malignancy, have a main limitation: the high costs. In the last years various image-guided procedures have been proposed as alternative and less invasive approaches to surgery for symptomatic thyroid nodules. These minimally invasive techniques (laser and radio-frequency ablation, high intensity focused ultrasound and percutaneous microwave ablation) results in nodule shrinkage and improvement of local symptoms, with a lower risk of complications and minor costs compared to surgery. Finally, ultrasound-guided ablation therapy was introduced with promising results as a feasible treatment for low-risk papillary thyroid microcarcinoma or cervical lymph node metastases."
32038208,10.0,Intra- and Inter-subject Variability in EEG-Based Sensorimotor Brain Computer Interface: A Review,2020 Jan 21;13:87.,"Brain computer interfaces (BCI) for the rehabilitation of motor impairments exploit sensorimotor rhythms (SMR) in the electroencephalogram (EEG). However, the neurophysiological processes underpinning the SMR often vary over time and across subjects. Inherent intra- and inter-subject variability causes covariate shift in data distributions that impede the transferability of model parameters amongst sessions/subjects. Transfer learning includes machine learning-based methods to compensate for inter-subject and inter-session (intra-subject) variability manifested in EEG-derived feature distributions as a covariate shift for BCI. Besides transfer learning approaches, recent studies have explored psychological and neurophysiological predictors as well as inter-subject associativity assessment, which may augment transfer learning in EEG-based BCI. Here, we highlight the importance of measuring inter-session/subject performance predictors for generalized BCI frameworks for both normal and motor-impaired people, reducing the necessity for tedious and annoying calibration sessions and BCI training."
32035758,4.0,Creating Artificial Images for Radiology Applications Using Generative Adversarial Networks (GANs) - A Systematic Review,2020 Aug;27(8):1175-1185.,"Rationale and objectives:                    Generative adversarial networks (GANs) are deep learning models aimed at generating fake realistic looking images. These novel models made a great impact on the computer vision field. Our study aims to review the literature on GANs applications in radiology.              Materials and methods:                    This systematic review followed the PRISMA guidelines. Electronic datasets were searched for studies describing applications of GANs in radiology. We included studies published up-to September 2019.              Results:                    Data were extracted from 33 studies published between 2017 and 2019. Eighteen studies focused on CT images generation, ten on MRI, three on PET/MRI and PET/CT, one on ultrasound and one on X-ray. Applications in radiology included image reconstruction and denoising for dose and scan time reduction (fourteen studies), data augmentation (six studies), transfer between modalities (eight studies) and image segmentation (five studies). All studies reported that generated images improved the performance of the developed algorithms.              Conclusion:                    GANs are increasingly studied for various radiology applications. They enable the creation of new data, which can be used to improve clinical care, education and research."
32034573,4.0,Integrating radiomics into holomics for personalised oncology: from algorithms to bedside,2020 Feb 7;4(1):11.,"Radiomics, artificial intelligence, and deep learning figure amongst recent buzzwords in current medical imaging research and technological development. Analysis of medical big data in assessment and follow-up of personalised treatments has also become a major research topic in the area of precision medicine. In this review, current research trends in radiomics are analysed, from handcrafted radiomics feature extraction and statistical analysis to deep learning. Radiomics algorithms now include genomics and immunomics data to improve patient stratification and prediction of treatment response. Several applications have already shown conclusive results demonstrating the potential of including other ""omics"" data to existing imaging features. We also discuss further challenges of data harmonisation and management infrastructure to shed a light on the much-needed integration of radiomics and all other ""omics"" into clinical workflows. In particular, we point to the emerging paradigm shift in the implementation of big data infrastructures to facilitate databanks growth, data extraction and the development of expert software tools. Secured access, sharing, and integration of all health data, called ""holomics"", will accelerate the revolution of personalised medicine and oncology as well as expand the role of imaging specialists."
32030670,,Deep Learning Technique for Musculoskeletal Analysis,2020;1213:165-176.,"Advancements in musculoskeletal analysis have been achieved by adopting deep learning technology in image recognition and analysis. Unlike musculoskeletal modeling based on computational anatomy, deep learning-based methods can obtain muscle information automatically. Through analysis of image features, both approaches can obtain muscle characteristics such as shape, volume, and area, and derive additional information by analyzing other image textures. In this chapter, we first discuss the necessity of musculoskeletal analysis and the required image processing technology. Then, the limitations of skeletal muscle recognition based on conventional handcrafted features are discussed, and developments in skeletal muscle recognition using machine learning and deep learning technology are described. Next, a technique for analyzing musculoskeletal systems using whole-body computed tomography (CT) images is shown. This study aims to achieve automatic recognition of skeletal muscles throughout the body and automatic classification of atrophic muscular disease using only image features, to demonstrate an application of whole-body musculoskeletal analysis driven by deep learning. Finally, we discuss future development of musculoskeletal analysis that effectively combines deep learning with handcrafted feature-based modeling techniques."
32030669,,Techniques and Applications in Skin OCT Analysis,2020;1213:149-163.,"The skin is the largest organ of our body. Skin disease abnormalities which occur within the skin layers are difficult to examine visually and often require biopsies to make a confirmation on a suspected condition. Such invasive methods are not well-accepted by children and women due to the possibility of scarring. Optical coherence tomography (OCT) is a non-invasive technique enabling in vivo examination of sub-surface skin tissue without the need for excision of tissue. However, one of the challenges in OCT imaging is the interpretation and analysis of OCT images. In this review, we discuss the various methodologies in skin layer segmentation and how it could potentially improve the management of skin diseases. We also present a review of works which use advanced machine learning techniques to achieve layers segmentation and detection of skin diseases. Lastly, current challenges in analysis and applications are also discussed."
32030661,3.0,Medical Image Synthesis via Deep Learning,2020;1213:23-44.,"Medical images have been widely used in clinics, providing visual representations of under-skin tissues in human body. By applying different imaging protocols, diverse modalities of medical images with unique characteristics of visualization can be produced. Considering the cost of scanning high-quality single modality images or homogeneous multiple modalities of images, medical image synthesis methods have been extensively explored for clinical applications. Among them, deep learning approaches, especially convolutional neural networks (CNNs) and generative adversarial networks (GANs), have rapidly become dominating for medical image synthesis in recent years. In this chapter, based on a general review of the medical image synthesis methods, we will focus on introducing typical CNNs and GANs models for medical image synthesis. Especially, we will elaborate our recent work about low-dose to high-dose PET image synthesis, and cross-modality MR image synthesis, using these models."
32030660,4.0,Deep Learning in Medical Image Analysis,2020;1213:3-21.,"Deep learning is the state-of-the-art machine learning approach. The success of deep learning in many pattern recognition applications has brought excitement and high expectations that deep learning, or artificial intelligence (AI), can bring revolutionary changes in health care. Early studies of deep learning applied to lesion detection or classification have reported superior performance compared to those by conventional techniques or even better than radiologists in some tasks. The potential of applying deep-learning-based medical image analysis to computer-aided diagnosis (CAD), thus providing decision support to clinicians and improving the accuracy and efficiency of various diagnostic and treatment processes, has spurred new research and development efforts in CAD. Despite the optimism in this new era of machine learning, the development and implementation of CAD or AI tools in clinical practice face many challenges. In this chapter, we will discuss some of these issues and efforts needed to develop robust deep-learning-based CAD tools and integrate these tools into the clinical workflow, thereby advancing towards the goal of providing reliable intelligent aids for patient care."
32024055,7.0,Precision Psychiatry Applications with Pharmacogenomics: Artificial Intelligence and Machine Learning Approaches,2020 Feb 1;21(3):969.,"A growing body of evidence now suggests that precision psychiatry, an interdisciplinary field of psychiatry, precision medicine, and pharmacogenomics, serves as an indispensable foundation of medical practices by offering the accurate medication with the accurate dose at the accurate time to patients with psychiatric disorders. In light of the latest advancements in artificial intelligence and machine learning techniques, numerous biomarkers and genetic loci associated with psychiatric diseases and relevant treatments are being discovered in precision psychiatry research by employing neuroimaging and multi-omics. In this review, we focus on the latest developments for precision psychiatry research using artificial intelligence and machine learning approaches, such as deep learning and neural network algorithms, together with multi-omics and neuroimaging data. Firstly, we review precision psychiatry and pharmacogenomics studies that leverage various artificial intelligence and machine learning techniques to assess treatment prediction, prognosis prediction, diagnosis prediction, and the detection of potential biomarkers. In addition, we describe potential biomarkers and genetic loci that have been discovered to be associated with psychiatric diseases and relevant treatments. Moreover, we outline the limitations in regard to the previous precision psychiatry and pharmacogenomics studies. Finally, we present a discussion of directions and challenges for future research."
32023986,2.0,Machine Learning and Multidrug-Resistant Gram-Negative Bacteria: An Interesting Combination for Current and Future Research,2020 Jan 31;9(2):54.,"The dissemination of multidrug-resistant Gram-negative bacteria (MDR-GNB) is associated with increased morbidity and mortality in several countries. Machine learning (ML) is a branch of artificial intelligence that consists of conferring on computers the ability to learn from data. In this narrative review, we discuss three existing examples of the application of ML algorithms for assessing three different types of risk: (i) the risk of developing a MDR-GNB infection, (ii) the risk of MDR-GNB etiology in patients with an already clinically evident infection, and (iii) the risk of anticipating the emergence of MDR in GNB through the misuse of antibiotics. In the next few years, we expect to witness an increasingly large number of research studies perfecting the application of ML techniques in the field of MDR-GNB infections. Very importantly, this cannot be separated from the availability of a continuously refined and updated ethical framework allowing an appropriate use of the large datasets of medical data needed to build efficient ML-based support systems that could be shared through appropriate standard infrastructures."
32022759,,Machine Intelligence in Cardiovascular Medicine,Mar/Apr 2020;28(2):53-64.,"The computer science technology trend called artificial intelligence (AI) is not new. Both machine learning and deep learning AI applications have recently begun to impact cardiovascular medicine. Scientists working in the AI domain have long recognized the importance of data quality and provenance to AI algorithm efficiency and accuracy. A diverse array of cardiovascular raw data sources of variable quality-electronic medical records, radiological picture archiving and communication systems, laboratory results, omics, etc.-are available to train AI algorithms for predictive modeling of clinical outcomes (in-hospital mortality, acute coronary syndrome risk stratification, etc.), accelerated image interpretation (edge detection, tissue characterization, etc.) and enhanced phenotyping of heterogeneous conditions (heart failure with preserved ejection fraction, hypertension, etc.). A number of software as medical device narrow AI products for cardiac arrhythmia characterization and advanced image deconvolution are now Food and Drug Administration approved, and many others are in the pipeline. Present and future health professionals using AI-infused analytics and wearable devices have 3 critical roles to play in their informed development and ethical application in practice: (1) medical domain experts providing clinical context to computer and data scientists, (2) data stewards assuring the quality, relevance and provenance of data inputs, and (3) real-time and post-hoc interpreters of AI black box solutions and recommendations to patients. The next wave of so-called contextual adaption AI technologies will more closely approximate human decision-making, potentially augmenting cardiologists' real-time performance in emergency rooms, catheterization laboratories, imaging suites, and clinics. However, before such higher order AI technologies are adopted in the clinical setting and by healthcare systems, regulatory agencies, and industry must jointly develop robust AI standards of practice and transparent technology insertion rule sets."
32022730,,ICU management based on big data,2020 Apr;33(2):162-169.,"Purpose of review:                    The availability of large datasets and computational power has prompted a revolution in Intensive Care. Data represent a great opportunity for clinical practice, benchmarking, and research. Machine learning algorithms can help predict events in a way the human brain can simply not process. This possibility comes with benefits and risks for the clinician, as finding associations does not mean proving causality.              Recent findings:                    Current applications of Data Science still focus on data documentation and visualization, and on basic rules to identify critical lab values. Recently, algorithms have been put in place for prediction of outcomes such as length of stay, mortality, and development of complications. These results have begun being implemented for more efficient allocation of resources and in benchmarking processes, to allow identification of successful practices and margins for improvement. In parallel, machine learning models are increasingly being applied in research to expand medical knowledge.              Summary:                    Data have always been part of the work of intensivists, but the current availability has not been completely exploited. The intensive care community has to embrace and guide the data science revolution in order to decline it in favor of patients' care."
32016902,,What Next for Quantum Mechanics in Structure-Based Drug Discovery?,2020;2114:339-353.,"There is significant potential for electronic structure methods to improve the quality of the predictions furnished by the tools of computer-aided drug design, which typically rely on empirically derived functions. In this perspective, we consider some recent examples of how quantum mechanics has been applied in predicting protein-ligand geometries, protein-ligand binding affinities and ligand strain on binding. We then outline several significant developments in quantum mechanics methodology likely to influence these approaches: in particular, we note the advent of more computationally expedient ab initio quantum mechanical methods that can provide chemical accuracy for larger molecular systems than hitherto possible. We highlight the emergence of increasingly accurate semiempirical quantum mechanical methods and the associated role of machine learning and molecular databases in their development. Indeed, the convergence of improved algorithms for solving and analyzing electronic structure, modern machine learning methods, and increasingly comprehensive benchmark data sets of molecular geometries and energies provides a context in which the potential of quantum mechanics will be increasingly realized in driving future developments and applications in structure-based drug discovery."
32016900,,QM Calculations in ADMET Prediction,2020;2114:285-305.,"In recent years, there has been an increase in the application of quantum mechanics (QM) methods to describe properties related to the ADMET profile of small molecules. The application of these methods allows calculating useful descriptors and physiochemical properties contributing to ADMET prediction. Considering that QM methods are the only one that describe the electronic state of a molecules, such methods are particularly useful for studying the metabolism of drugs; furthermore, the introduction of mixed QM and molecular mechanics (QM/MM) is also increasing the understanding of drug interaction with cytochromes from a mechanistic point of view. Finally, combining the increase number of experimental data with machine learning algorithms and QM-derived descriptors allowed the creation of an end-user software capable of affecting the drug discovery process."
32013105,3.0,Toward a Standardized Strategy of Clinical Metabolomics for the Advancement of Precision Medicine,2020 Jan 29;10(2):51.,"Despite the tremendous success, pitfalls have been observed in every step of a clinical metabolomics workflow, which impedes the internal validity of the study. Furthermore, the demand for logistics, instrumentations, and computational resources for metabolic phenotyping studies has far exceeded our expectations. In this conceptual review, we will cover inclusive barriers of a metabolomics-based clinical study and suggest potential solutions in the hope of enhancing study robustness, usability, and transferability. The importance of quality assurance and quality control procedures is discussed, followed by a practical rule containing five phases, including two additional ""pre-pre-"" and ""post-post-"" analytical steps. Besides, we will elucidate the potential involvement of machine learning and demonstrate that the need for automated data mining algorithms to improve the quality of future research is undeniable. Consequently, we propose a comprehensive metabolomics framework, along with an appropriate checklist refined from current guidelines and our previously published assessment, in the attempt to accurately translate achievements in metabolomics into clinical and epidemiological research. Furthermore, the integration of multifaceted multi-omics approaches with metabolomics as the pillar member is in urgent need. When combining with other social or nutritional factors, we can gather complete omics profiles for a particular disease. Our discussion reflects the current obstacles and potential solutions toward the progressing trend of utilizing metabolomics in clinical research to create the next-generation healthcare system."
32012941,1.0,Mass Spectrometry-Based Multivariate Proteomic Tests for Prediction of Outcomes on Immune Checkpoint Blockade Therapy: The Modern Analytical Approach,2020 Jan 28;21(3):838.,"The remarkable success of immune checkpoint inhibitors (ICIs) has given hope of cure for some patients with advanced cancer; however, the fraction of responding patients is 15-35%, depending on tumor type, and the proportion of durable responses is even smaller. Identification of biomarkers with strong predictive potential remains a priority. Until now most of the efforts were focused on biomarkers associated with the assumed mechanism of action of ICIs, such as levels of expression of programmed death-ligand 1 (PD-L1) and mutation load in tumor tissue, as a proxy of immunogenicity; however, their performance is unsatisfactory. Several assays designed to capture the complexity of the disease by measuring the immune response in tumor microenvironment show promise but still need validation in independent studies. The circulating proteome contains an additional layer of information characterizing tumor-host interactions that can be integrated into multivariate tests using modern machine learning techniques. Here we describe several validated serum-based proteomic tests and their utility in the context of ICIs. We discuss test performances, demonstrate their independence from currently used biomarkers, and discuss various aspects of associated biological mechanisms. We propose that serum-based multivariate proteomic tests add a missing piece to the puzzle of predicting benefit from ICIs."
32012695,10.0,The Roles of the NLRP3 Inflammasome in Neurodegenerative and Metabolic Diseases and in Relevant Advanced Therapeutic Interventions,2020 Jan 27;11(2):131.,"Inflammasomes are intracellular multiprotein complexes in the cytoplasm that regulate inflammation activation in the innate immune system in response to pathogens and to host self-derived molecules. Recent advances greatly improved our understanding of the activation of nucleotide-binding oligomerization domain-like receptor (NLR) family pyrin domain containing 3 (NLRP3) inflammasomes at the molecular level. The NLRP3 belongs to the subfamily of NLRP which activates caspase 1, thus causing the production of proinflammatory cytokines (interleukin 1β and interleukin 18) and pyroptosis. This inflammasome is involved in multiple neurodegenerative and metabolic disorders including Alzheimer's disease, multiple sclerosis, type 2 diabetes mellitus, and gout. Therefore, therapeutic targeting to the NLRP3 inflammasome complex is a promising way to treat these diseases. Recent research advances paved the way toward drug research and development using a variety of machine learning-based and artificial intelligence-based approaches. These state-of-the-art approaches will lead to the discovery of better drugs after the training of such a system."
32012057,1.0,Sentiment Analysis in Health and Well-Being: Systematic Review,2020 Jan 28;8(1):e16023.,"Background:                    Sentiment analysis (SA) is a subfield of natural language processing whose aim is to automatically classify the sentiment expressed in a free text. It has found practical applications across a wide range of societal contexts including marketing, economy, and politics. This review focuses specifically on applications related to health, which is defined as ""a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity.""              Objective:                    This study aimed to establish the state of the art in SA related to health and well-being by conducting a systematic review of the recent literature. To capture the perspective of those individuals whose health and well-being are affected, we focused specifically on spontaneously generated content and not necessarily that of health care professionals.              Methods:                    Our methodology is based on the guidelines for performing systematic reviews. In January 2019, we used PubMed, a multifaceted interface, to perform a literature search against MEDLINE. We identified a total of 86 relevant studies and extracted data about the datasets analyzed, discourse topics, data creators, downstream applications, algorithms used, and their evaluation.              Results:                    The majority of data were collected from social networking and Web-based retailing platforms. The primary purpose of online conversations is to exchange information and provide social support online. These communities tend to form around health conditions with high severity and chronicity rates. Different treatments and services discussed include medications, vaccination, surgery, orthodontic services, individual physicians, and health care services in general. We identified 5 roles with respect to health and well-being among the authors of the types of spontaneously generated narratives considered in this review: a sufferer, an addict, a patient, a carer, and a suicide victim. Out of 86 studies considered, only 4 reported the demographic characteristics. A wide range of methods were used to perform SA. Most common choices included support vector machines, naïve Bayesian learning, decision trees, logistic regression, and adaptive boosting. In contrast with general trends in SA research, only 1 study used deep learning. The performance lags behind the state of the art achieved in other domains when measured by F-score, which was found to be below 60% on average. In the context of SA, the domain of health and well-being was found to be resource poor: few domain-specific corpora and lexica are shared publicly for research purposes.              Conclusions:                    SA results in the area of health and well-being lag behind those in other domains. It is yet unclear if this is because of the intrinsic differences between the domains and their respective sublanguages, the size of training datasets, the lack of domain-specific sentiment lexica, or the choice of algorithms."
32010196,2.0,Integrating Computational Methods to Investigate the Macroecology of Microbiomes,2020 Jan 17;10:1344.,"Studies in microbiology have long been mostly restricted to small spatial scales. However, recent technological advances, such as new sequencing methodologies, have ushered an era of large-scale sequencing of environmental DNA data from multiple biomes worldwide. These global datasets can now be used to explore long standing questions of microbial ecology. New methodological approaches and concepts are being developed to study such large-scale patterns in microbial communities, resulting in new perspectives that represent a significant advances for both microbiology and macroecology. Here, we identify and review important conceptual, computational, and methodological challenges and opportunities in microbial macroecology. Specifically, we discuss the challenges of handling and analyzing large amounts of microbiome data to understand taxa distribution and co-occurrence patterns. We also discuss approaches for modeling microbial communities based on environmental data, including information on biological interactions to make full use of available Big Data. Finally, we summarize the methods presented in a general approach aimed to aid microbiologists in addressing fundamental questions in microbial macroecology, including classical propositions (such as ""everything is everywhere, but the environment selects"") as well as applied ecological problems, such as those posed by human induced global environmental changes."
32008639,,Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model,2020 Mar;40(1):51-59.,"The recent increase in accessible medical and clinical laboratory ""Big Data"" has led to a corresponding increase in the use of machine-learning tools to develop integrative diagnostic models incorporating both existing and new test data. The rise of direct-to-consumer (DTC) testing paradigms raises the possibility of predictive models that use these new sources. This article discusses several distinct challenges raised by the DTC approach, including issues of centralized data collection, ascertainment bias, linkage to medical outcomes, and standardization/harmonization of results. Several solutions to maximize the promise of machine-learning data analytics for DTC data are suggested."
32008510,1.0,A Survey on Medical Image Analysis in Capsule Endoscopy,2019;15(7):622-636.,"Background and objective:                    Capsule Endoscopy (CE) is a non-invasive, patient-friendly alternative to conventional endoscopy procedure. However, CE produces 6 to 8 hrs long video posing a tedious challenge to a gastroenterologist for abnormality detection. Major challenges to an expert are lengthy videos, need of constant concentration and subjectivity of the abnormality. To address these challenges along with high diagnostic accuracy, design and development of automated abnormality detection system is a must. Machine learning and computer vision techniques are devised to develop such automated systems.              Methods:                    Study presents a review of quality research papers published in IEEE, Scopus, and Science Direct database with search criteria as capsule endoscopy, engineering, and journal papers. The initial search retrieved 144 publications. After evaluating all articles, 62 publications pertaining to image analysis are selected.              Results:                    This paper presents a rigorous review comprising all the aspects of medical image analysis concerning capsule endoscopy namely video summarization and redundant image elimination, Image enhancement and interpretation, segmentation and region identification, Computer-aided abnormality detection in capsule endoscopy, Image and video compression. The study provides a comparative analysis of various approaches, experimental setup, performance, strengths, and limitations of the aspects stated above.              Conclusions:                    The analyzed image analysis techniques for capsule endoscopy have not yet overcome all current challenges mainly due to lack of dataset and complex nature of the gastrointestinal tract."
32008107,1.0,Policy Implications of Artificial Intelligence and Machine Learning in Diabetes Management,2020 Feb 1;20(2):5.,"Purpose of review:                    Machine learning (ML) is increasingly being studied for the screening, diagnosis, and management of diabetes and its complications. Although various models of ML have been developed, most have not led to practical solutions for real-world problems. There has been a disconnect between ML developers, regulatory bodies, health services researchers, clinicians, and patients in their efforts. Our aim is to review the current status of ML in various aspects of diabetes care and identify key challenges that must be overcome to leverage ML to its full potential.              Recent findings:                    ML has led to impressive progress in development of automated insulin delivery systems and diabetic retinopathy screening tools. Compared with these, use of ML in other aspects of diabetes is still at an early stage. The Food & Drug Administration (FDA) is adopting some innovative models to help bring technologies to the market in an expeditious and safe manner. ML has great potential in managing diabetes and the future is in furthering the partnership of regulatory bodies with health service researchers, clinicians, developers, and patients to improve the outcomes of populations and individual patients with diabetes."
32007208,5.0,MRI-guided adaptive radiotherapy for liver tumours: visualising the future,2020 Feb;21(2):e74-e82.,"MRI-guided radiotherapy is a novel and rapidly evolving technology that might enhance the risk-benefit ratio. Through direct visualisation of the tumour and the nearby healthy tissues, the radiation oncologist can deliver highly accurate treatment even to mobile targets. Each individual treatment can be customised to changing anatomy, potentially reducing the risk of radiation-related toxicities while simultaneously increasing the dose delivered to the tumour. MRI-guided radiotherapy offers a new tool for the radiation oncologist, and creates an opportunity to achieve durable local control of liver tumours that might not otherwise be possible. Future work will allow us to expand the population eligible for curative-intent radiotherapy, optimise and customise radiation doses to specific tumours, and hopefully create opportunities for improving outcomes through machine learning and radiomics-based approaches. This Review outlines the current and future applications for MRI-guided radiotherapy with respect to metastatic and primary liver cancers."
32006410,,CRISPR/Cas9 Guide RNA Design Rules for Predicting Activity,2020;2115:351-364.,"A critical stage in performing gene editing experiments using the CRISPR/Cas9 system is the design of guide RNA (gRNA). In this chapter, we conduct a review of the current gRNA design rules for maximizing on-target Cas9 activity while minimizing off-target activity. In addition, we present some of the currently available computational tools for gRNA activity prediction and assay design."
32004537,1.0,"Transitioning Chemistry, Manufacturing, and Controls Content With a Structured Data Management Solution: Streamlining Regulatory Submissions",2020 Apr;109(4):1427-1438.,"The process of assembling regulatory documents for submission to multiple global health agencies can present a repetitive cycle of authoring, editing, and data verification, which increases in complexity as changes are made for approved products, particularly from a chemistry, manufacturing, and controls (CMC) perspective. Currently, pharmaceutical companies rely on a workflow that involves manual CMC change management across documents. Similarly, when regulators review submissions, they provide feedback and insight into regulatory decision making in a narrative format. As accelerated review pathways are increasingly used and pressure mounts to bring products to market quickly, innovative solutions for assembling, distributing, and reviewing regulatory information are being considered. Structured content management (SCM) solutions, in which data are collated into centrally organized content blocks for use across different documents, may aid in the efficient processing of data and create opportunities for automation and machine learning in its interpretation. The US Food and Drug Administration (FDA) has recently created initiatives that encourage application of SCM for CMC data, though many challenges could impede their success and efficiency. The goal is for industry and health authorities to collaborate in the development of SCM for CMC applications, to potentially streamline compilation of quality data in regulatory submissions."
32002838,2.0,Applications of Computer Modeling and Simulation in Cartilage Tissue Engineering,2020 Feb;17(1):1-13.,"Background:                    Advances in cartilage tissue engineering have demonstrated noteworthy potential for developing cartilage for implantation onto sites impacted by joint degeneration and injury. To supplement resource-intensive in vivo and in vitro studies required for cartilage tissue engineering, computational models and simulations can assist in enhancing experimental design.              Methods:                    Research articles pertinent to cartilage tissue engineering and computer modeling were identified, reviewed, and summarized. Various applications of computer modeling for cartilage tissue engineering are highlighted, limitations of in silico modeling are addressed, and suggestions for future work are enumerated.              Results:                    Computational modeling can help better characterize shear stresses generated by bioreactor fluid flow, refine scaffold geometry, customize the mechanical properties of engineered cartilage tissue, and model rates of cell growth and dynamics. Thus, results from in silico studies can help resourcefully enhance in vitro and in vivo studies; however, the limitations of these studies, such as the underlying assumptions and simplifications applied in each model, should always be addressed and justified where applicable. In silico models should also seek validation and verification when possible.              Conclusion:                    Future studies may adopt similar approaches to supplement in vitro trials and further investigate effects of mechanical stimulation on chondrocyte and stem cell dynamics. Additionally, as precision medicine, machine learning, and powerful open-source software become more popular and accessible, applications of multi-scale and multiphysics computational models in cartilage tissue engineering are expected to increase."
32000922,,Dysmorphology in a Genomic Era,2020 Mar;47(1):15-23.,Dysmorphology is the practice of defining the morphologic phenotype of syndromic disorders. Genomic sequencing has advanced our understanding of human variation and molecular dysmorphology has evolved in response to the science of relating embryologic developmental implications of abnormal gene signaling pathways to the resultant phenotypic presentation. Machine learning has enabled the application of deep convoluted neural networks to recognize the comparative likeness of these phenotypes relative to the causal genotype or disrupted gene pathway.
31998756,3.0,Machine Learning Approaches for Myocardial Motion and Deformation Analysis,2020 Jan 9;6:190.,"Information about myocardial motion and deformation is key to differentiate normal and abnormal conditions. With the advent of approaches relying on data rather than pre-conceived models, machine learning could either improve the robustness of motion quantification or reveal patterns of motion and deformation (rather than single parameters) that differentiate pathologies. We review machine learning strategies for extracting motion-related descriptors and analyzing such features among populations, keeping in mind constraints specific to the cardiac application."
31998708,1.0,Engineering Tissue Fabrication With Machine Intelligence: Generating a Blueprint for Regeneration,2020 Jan 10;7:443.,"Regenerating lost or damaged tissue is the primary goal of Tissue Engineering. 3D bioprinting technologies have been widely applied in many research areas of tissue regeneration and disease modeling with unprecedented spatial resolution and tissue-like complexity. However, the extraction of tissue architecture and the generation of high-resolution blueprints are challenging tasks for tissue regeneration. Traditionally, such spatial information is obtained from a collection of microscopic images and then combined together to visualize regions of interest. To fabricate such engineered tissues, rendered microscopic images are transformed to code to inform a 3D bioprinting process. If this process is augmented with data-driven approaches and streamlined with machine intelligence, identification of an optimal blueprint can become an achievable task for functional tissue regeneration. In this review, our perspective is guided by an emerging paradigm to generate a blueprint for regeneration with machine intelligence. First, we reviewed recent articles with respect to our perspective for machine intelligence-driven information retrieval and fabrication. After briefly introducing recent trends in information retrieval methods from publicly available data, our discussion is focused on recent works that use machine intelligence to discover tissue architectures from imaging and spectral data. Then, our focus is on utilizing optimization approaches to increase print fidelity and enhance biomimicry with machine learning (ML) strategies to acquire a blueprint ready for 3D bioprinting."
31998109,4.0,Lizard Brain: Tackling Locally Low-Dimensional Yet Globally Complex Organization of Multi-Dimensional Datasets,2020 Jan 9;13:110.,"Machine learning deals with datasets characterized by high dimensionality. However, in many cases, the intrinsic dimensionality of the datasets is surprisingly low. For example, the dimensionality of a robot's perception space can be large and multi-modal but its variables can have more or less complex non-linear interdependencies. Thus multidimensional data point clouds can be effectively located in the vicinity of principal varieties possessing locally small dimensionality, but having a globally complicated organization which is sometimes difficult to represent with regular mathematical objects (such as manifolds). We review modern machine learning approaches for extracting low-dimensional geometries from multi-dimensional data and their applications in various scientific fields."
31995745,,A survey on machine and statistical learning for longitudinal analysis of neuroimaging data in Alzheimer's disease,2020 Jun;189:105348.,"Background and objectives:                    Recently, longitudinal studies of Alzheimer's disease have gathered a substantial amount of neuroimaging data. New methods are needed to successfully leverage and distill meaningful information on the progression of the disease from the deluge of available data. Machine learning has been used successfully for many different tasks, including neuroimaging related problems. In this paper, we review recent statistical and machine learning applications in Alzheimer's disease using longitudinal neuroimaging.              Methods:                    We search for papers using longitudinal imaging data, focused on Alzheimer's Disease and published between 2007 and 2019 on four different search engines.              Results:                    After the search, we obtain 104 relevant papers. We analyze their approach to typical challenges in longitudinal data analysis, such as missing data and variability in the number and extent of acquisitions.              Conclusions:                    Reviewed works show that machine learning methods using longitudinal data have potential for disease progression modelling and computer-aided diagnosis. We compare results and models, and propose future research directions in the field."
31994465,,Outwitting an Old Neglected Nemesis: A Review on Leveraging Integrated Data-Driven Approaches to Aid in Unraveling of Leishmanicides of Therapeutic Potential,2020;20(5):349-366.,"The global prevalence of leishmaniasis has increased with skyrocketed mortality in the past decade. The causative agent of leishmaniasis is Leishmania species, which infects populations in almost all the continents. Prevailing treatment regimens are consistently inefficient with reported side effects, toxicity and drug resistance. This review complements existing ones by discussing the current state of treatment options, therapeutic bottlenecks including chemoresistance and toxicity, as well as drug targets. It further highlights innovative applications of nanotherapeutics-based formulations, inhibitory potential of leishmanicides, anti-microbial peptides and organometallic compounds on leishmanial species. Moreover, it provides essential insights into recent machine learning-based models that have been used to predict novel leishmanicides and also discusses other new models that could be adopted to develop fast, efficient, robust and novel algorithms to aid in unraveling the next generation of anti-leishmanial drugs. A plethora of enriched functional genomic, proteomic, structural biology, high throughput bioassay and drug-related datasets are currently warehoused in both general and leishmania-specific databases. The warehoused datasets are essential inputs for training and testing algorithms to augment the prediction of biotherapeutic entities. In addition, we demonstrate how pharmacoinformatics techniques including ligand-, structure- and pharmacophore-based virtual screening approaches have been utilized to screen ligand libraries against both modeled and experimentally solved 3D structures of essential drug targets. In the era of data-driven decision-making, we believe that highlighting intricately linked topical issues relevant to leishmanial drug discovery offers a one-stop-shop opportunity to decipher critical literature with the potential to unlock implicit breakthroughs."
31993440,2.0,Challenges of Integrative Disease Modeling in Alzheimer's Disease,2020 Jan 14;6:158.,"Dementia-related diseases like Alzheimer's Disease (AD) have a tremendous social and economic cost. A deeper understanding of its underlying pathophysiologies may provide an opportunity for earlier detection and therapeutic intervention. Previous approaches for characterizing AD were targeted at single aspects of the disease. Yet, due to the complex nature of AD, the success of these approaches was limited. However, in recent years, advancements in integrative disease modeling, built on a wide range of AD biomarkers, have taken a global view on the disease, facilitating more comprehensive analysis and interpretation. Integrative AD models can be sorted in two primary types, namely hypothetical models and data-driven models. The latter group split into two subgroups: (i) Models that use traditional statistical methods such as linear models, (ii) Models that take advantage of more advanced artificial intelligence approaches such as machine learning. While many integrative AD models have been published over the last decade, their impact on clinical practice is limited. There exist major challenges in the course of integrative AD modeling, namely data missingness and censoring, imprecise human-involved priori knowledge, model reproducibility, dataset interoperability, dataset integration, and model interpretability. In this review, we highlight recent advancements and future possibilities of integrative modeling in the field of AD research, showcase and discuss the limitations and challenges involved, and finally, propose avenues to address several of these challenges."
31991452,,Quantification in Musculoskeletal Imaging Using Computational Analysis and Machine Learning: Segmentation and Radiomics,2020 Feb;24(1):50-64.,"Although still limited in clinical practice, quantitative analysis is expected to increase the value of musculoskeletal (MSK) imaging. Segmentation aims at isolating the tissues and/or regions of interest in the image and is crucial to the extraction of quantitative features such as size, signal intensity, or image texture. These features may serve to support the diagnosis and monitoring of disease. Radiomics refers to the process of extracting large amounts of features from radiologic images and combining them with clinical, biological, genetic, or any other type of complementary data to build diagnostic, prognostic, or predictive models. The advent of machine learning offers promising prospects for automatic segmentation and integration of large amounts of data. We present commonly used segmentation methods and describe the radiomics pipeline, highlighting the challenges to overcome for adoption in clinical practice. We provide some examples of applications from the MSK literature."
31989876,,"MRI Imaging, Comparison of MRI with other Modalities, Noise in MRI Images and Machine Learning Techniques for Noise Removal: A Review",2019;15(3):243-254.,"Background:                    Medical imaging is to assume greater and greater significance in an efficient and precise diagnosis process.              Discussion:                    It is a set of various methodologies which are used to capture internal or external images of the human body and organs for clinical and diagnosis needs to examine human form for various kind of ailments. Computationally intelligent machine learning techniques and their application in medical imaging can play a significant role in expediting the diagnosis process and making it more precise.              Conclusion:                    This review presents an up-to-date coverage about research topics which include recent literature in the areas of MRI imaging, comparison with other modalities, noise in MRI and machine learning techniques to remove the noise."
31989390,3.0,Computer-Aided Histopathological Image Analysis Techniques for Automated Nuclear Atypia Scoring of Breast Cancer: a Review,2020 Oct;33(5):1091-1121.,"Breast cancer is the most common type of malignancy diagnosed in women. Through early detection and diagnosis, there is a great chance of recovery and thereby reduce the mortality rate. Many preliminary tests like non-invasive radiological diagnosis using ultrasound, mammography, and MRI are widely used for the diagnosis of breast cancer. However, histopathological analysis of breast biopsy specimen is inevitable and is considered to be the golden standard for the affirmation of cancer. With the advancements in the digital computing capabilities, memory capacity, and imaging modalities, the development of computer-aided powerful analytical techniques for histopathological data has increased dramatically. These automated techniques help to alleviate the laborious work of the pathologist and to improve the reproducibility and reliability of the interpretation. This paper reviews and summarizes digital image computational algorithms applied on histopathological breast cancer images for nuclear atypia scoring and explores the future possibilities. The algorithms for nuclear pleomorphism scoring of breast cancer can be widely grouped into two categories: handcrafted feature-based and learned feature-based. Handcrafted feature-based algorithms mainly include the computational steps like pre-processing the images, segmenting the nuclei, extracting unique features, feature selection, and machine learning-based classification. However, most of the recent algorithms are based on learned features, that extract high-level abstractions directly from the histopathological images utilizing deep learning techniques. In this paper, we discuss the various algorithms applied for the nuclear pleomorphism scoring of breast cancer, discourse the challenges to be dealt with, and outline the importance of benchmark datasets. A comparative analysis of some prominent works on breast cancer nuclear atypia scoring is done using a benchmark dataset which enables to quantitatively measure and compare the different features and algorithms used for breast cancer grading. Results show that improvements are still required, to have an automated cancer grading system suitable for clinical applications."
31986218,2.0,Holistic cancer genome profiling for every patient,2020 Jan 27;150:w20158.,"Technological advances in the ability to read the human genome have accelerated the speed of sequencing, such that today we can perform whole genome sequencing (WGS) in one day. Until recently, genomic studies have largely been limited to seeking novel scientific discoveries. The application of new insights gained through cancer WGS into the clinical domain, have been relatively limited. Looking ahead, a vast amount of data can be generated by genomic studies. Of note, excellent organisation of genomic and clinical data permits the application of machine-learning methods which can lead to the development of clinical algorithms that could assist future clinicians and genomicists in the analysis and interpretation of individual cancer genomes. Here, we describe what can be gleaned from holistic whole cancer genome profiling and argue that we must build the infrastructure and educational frameworks to support the modern clinical genomicist to prepare for a future where WGS will be the norm."
31984250,1.0,Combination of Single-Molecule Electrical Measurements and Machine Learning for the Identification of Single Biomolecules,2020 Jan 7;5(2):959-964.,"The development of a next-generation DNA sequencer has provided a method for electrically measuring single molecules. Methods for electrically measuring one molecule are roughly divided into methods for measuring tunneling and ion currents. These methods enable identification of a single molecule of DNA, a RNA nucleotide, or a single protein based on current histograms. However, overlapping of current histograms of molecules with similar properties has been a major barrier to identifying single molecules with high accuracy. This barrier was broken by introducing machine learning. Combining single-molecule electrical measurement and machine learning enables high-precision identification of single molecules. Highly accurate discrimination has been demonstrated for DNA nucleotides, RNA nucleotides, amino acids, sugars, viruses, and bacteria. This combination enables quantitative evaluation of molecular recognition ability. Furthermore, a device structure suitable for high-precision identification has been designed. Combining single-molecule electrical measurement with machine learning enables digital analytical chemistry that can count certain types of molecules. Digital analytical chemistry enables comprehensive analysis of chemical reactions. This new analytical method will lead to the discovery of unknown or missed valuable molecules."
31983042,5.0,"Machine Learning and Artificial Intelligence: Definitions, Applications, and Future Directions",2020 Feb;13(1):69-76.,"Purpose of review:                    With the unprecedented advancement of data aggregation and deep learning algorithms, artificial intelligence (AI) and machine learning (ML) are poised to transform the practice of medicine. The field of orthopedics, in particular, is uniquely suited to harness the power of big data, and in doing so provide critical insight into elevating the many facets of care provided by orthopedic surgeons. The purpose of this review is to critically evaluate the recent and novel literature regarding ML in the field of orthopedics and to address its potential impact on the future of musculoskeletal care.              Recent findings:                    Recent literature demonstrates that the incorporation of ML into orthopedics has the potential to elevate patient care through alternative patient-specific payment models, rapidly analyze imaging modalities, and remotely monitor patients. Just as the business of medicine was once considered outside the domain of the orthopedic surgeon, we report evidence that demonstrates these emerging applications of AI warrant ownership, leverage, and application by the orthopedic surgeon to better serve their patients and deliver optimal, value-based care."
31982357,5.0,Machine Learning With Neuroimaging: Evaluating Its Applications in Psychiatry,2020 Aug;5(8):791-798.,"Psychiatric disorders are complex, involving heterogeneous symptomatology and neurobiology that rarely involves the disruption of single, isolated brain structures. In an attempt to better describe and understand the complexities of psychiatric disorders, investigators have increasingly applied multivariate pattern classification approaches to neuroimaging data and in particular supervised machine learning methods. However, supervised machine learning approaches also come with unique challenges and trade-offs, requiring additional study design and interpretation considerations. The goal of this review is to provide a set of best practices for evaluating machine learning applications to psychiatric disorders. We discuss how to evaluate two common efforts: 1) making predictions that have the potential to aid in diagnosis, prognosis, and treatment and 2) interrogating the complex neurophysiological mechanisms underlying psychopathology. We focus here on machine learning as applied to functional connectivity with magnetic resonance imaging, as an example to ground discussion. We argue that for machine learning classification to have translational utility for individual-level predictions, investigators must ensure that the classification is clinically informative, independent of confounding variables, and appropriately assessed for both performance and generalizability. We contend that shedding light on the complex mechanisms underlying psychiatric disorders will require consideration of the unique utility, interpretability, and reliability of the neuroimaging features (e.g., regions, networks, connections) identified from machine learning approaches. Finally, we discuss how the rise of large, multisite, publicly available datasets may contribute to the utility of machine learning approaches in psychiatry."
31982325,12.0,Next-generation drug repurposing using human genetics and network biology,2020 Apr;51:78-92.,"Drug repurposing has attracted increased attention, especially in the context of drug discovery rates that remain too low despite a recent wave of approvals for biological therapeutics (e.g. gene therapy). These new biological entities-based treatments have high costs that are difficult to justify for small markets that include rare diseases. Drug repurposing, involving the identification of single or combinations of existing drugs based on human genetics data and network biology approaches represents a next-generation approach that has the potential to increase the speed of drug discovery at a lower cost. This Pharmacological Perspective reviews progress and perspectives in combining human genetics, especially genome-wide association studies, with network biology to drive drug repurposing for rare and common diseases with monogenic or polygenic etiologies. Also, highlighted here are important features of this next generation approach to drug repurposing, which can be combined with machine learning methods to meet the challenges of personalized medicine."
31981309,4.0,Deep Learning-Based Single-Cell Optical Image Studies,2020 Mar;97(3):226-240.,"Optical imaging technology that has the advantages of high sensitivity and cost-effectiveness greatly promotes the progress of nondestructive single-cell studies. Complex cellular image analysis tasks such as three-dimensional reconstruction call for machine-learning technology in cell optical image research. With the rapid developments of high-throughput imaging flow cytometry, big data cell optical images are always obtained that may require machine learning for data analysis. In recent years, deep learning has been prevalent in the field of machine learning for large-scale image processing and analysis, which brings a new dawn for single-cell optical image studies with an explosive growth of data availability. Popular deep learning techniques offer new ideas for multimodal and multitask single-cell optical image research. This article provides an overview of the basic knowledge of deep learning and its applications in single-cell optical image studies. We explore the feasibility of applying deep learning techniques to single-cell optical image analysis, where popular techniques such as transfer learning, multimodal learning, multitask learning, and end-to-end learning have been reviewed. Image preprocessing and deep learning model training methods are then summarized. Applications based on deep learning techniques in the field of single-cell optical image studies are reviewed, which include image segmentation, super-resolution image reconstruction, cell tracking, cell counting, cross-modal image reconstruction, and design and control of cell imaging systems. In addition, deep learning in popular single-cell optical imaging techniques such as label-free cell optical imaging, high-content screening, and high-throughput optical imaging cytometry are also mentioned. Finally, the perspectives of deep learning technology for single-cell optical image analysis are discussed. © 2020 International Society for Advancement of Cytometry."
31980099,4.0,Clinical Decision Support Systems for Triage in the Emergency Department using Intelligent Systems: a Review,2020 Jan;102:101762.,"Motivation:                    Emergency Departments' (ED) modern triage systems implemented worldwide are solely based upon medical knowledge and experience. This is a limitation of these systems, since there might be hidden patterns that can be explored in big volumes of clinical historical data. Intelligent techniques can be applied to these data to develop clinical decision support systems (CDSS) thereby providing the health professionals with objective criteria. Therefore, it is of foremost importance to identify what has been hampering the application of such systems for ED triage.              Objectives:                    The objective of this paper is to assess how intelligent CDSS for triage have been contributing to the improvement of quality of care in the ED as well as to identify the challenges they have been facing regarding implementation.              Methods:                    We applied a standard scoping review method with the manual search of 6 digital libraries, namely: ScienceDirect, IEEE Xplore, Google Scholar, Springer, MedlinePlus and Web of Knowledge. Search queries were created and customized for each digital library in order to acquire the information. The core search consisted of searching in the papers' title, abstract and key words for the topics ""triage"", ""emergency department""/""emergency room"" and concepts within the field of intelligent systems.              Results:                    From the review search, we found that logistic regression was the most frequently used technique for model design and the area under the receiver operating curve (AUC) the most frequently used performance measure. Beside triage priority, the most frequently used variables for modelling were patients' age, gender, vital signs and chief complaints. The main contributions of the selected papers consisted in the improvement of a patient's prioritization, prediction of need for critical care, hospital or Intensive Care Unit (ICU) admission, ED Length of Stay (LOS) and mortality from information available at the triage.              Conclusions:                    In the papers where CDSS were validated in the ED, the authors found that there was an improvement in the health professionals' decision-making thereby leading to better clinical management and patients' outcomes. However, we found that more than half of the studies lacked this implementation phase. We concluded that for these studies, it is necessary to validate the CDSS and to define key performance measures in order to demonstrate the extent to which incorporation of CDSS at triage can actually improve care."
31978628,5.0,Artificial intelligence approaches to predicting and detecting cognitive decline in older adults: A conceptual review,2020 Feb;284:112732.,"Preserving cognition and mental capacity is critical to aging with autonomy. Early detection of pathological cognitive decline facilitates the greatest impact of restorative or preventative treatments. Artificial Intelligence (AI) in healthcare is the use of computational algorithms that mimic human cognitive functions to analyze complex medical data. AI technologies like machine learning (ML) support the integration of biological, psychological, and social factors when approaching diagnosis, prognosis, and treatment of disease. This paper serves to acquaint clinicians and other stakeholders with the use, benefits, and limitations of AI for predicting, diagnosing, and classifying mild and major neurocognitive impairments, by providing a conceptual overview of this topic with emphasis on the features explored and AI techniques employed. We present studies that fell into six categories of features used for these purposes: (1) sociodemographics; (2) clinical and psychometric assessments; (3) neuroimaging and neurophysiology; (4) electronic health records and claims; (5) novel assessments (e.g., sensors for digital data); and (6) genomics/other omics. For each category we provide examples of AI approaches, including supervised and unsupervised ML, deep learning, and natural language processing. AI technology, still nascent in healthcare, has great potential to transform the way we diagnose and treat patients with neurocognitive disorders."
31972477,5.0,Machine learning approaches for analyzing and enhancing molecular dynamics simulations,2020 Apr;61:139-145.,"Molecular dynamics (MD) has become a powerful tool for studying biophysical systems, due to increasing computational power and availability of software. Although MD has made many contributions to better understanding these complex biophysical systems, there remain methodological difficulties to be surmounted. First, how to make the deluge of data generated in running even a microsecond long MD simulation human comprehensible. Second, how to efficiently sample the underlying free energy surface and kinetics. In this short perspective, we summarize machine learning based ideas that are solving both of these limitations, with a focus on their key theoretical underpinnings and remaining challenges."
31971112,1.0,Comparative Analysis of Classification Methods with PCA and LDA for Diabetes,2020;16(8):833-850.,"Background:                    The modern society is extremely prone to many life-threatening diseases, which can be easily controlled as well as cured if diagnosed at an early stage. The development and implementation of a disease diagnostic system have gained huge popularity over the years. In the current scenario, there are certain factors such as environment, sedentary lifestyle, genetic (hereditary) are the major factors behind the life threatening diseases such as 'diabetes.' Moreover, diabetes has achieved the status of the modern man's leading chronic disease. So one of the prime needs of this generation is to develop a state-of-the-art expert system which can predict diabetes at a very early stage with a minimum of complexity and in an expedited manner. The primary objective of this work is to develop an indigenous and efficient diagnostic technique for detection of diabetes. Method & Discussion: The proposed methodology comprises of two phases: In the first phase The Pima Indian Diabetes Dataset (PIDD) has been collected from the UCI machine learning repository databases and Localized Diabetes Dataset (LDD) has been gathered from Bombay Medical Hall, Upper Bazar Ranchi, Jharkhand, India. In the second phase, the dataset has been processed through two different approaches. The first approach entails classification through Adaboost, Classification via Regression (CVR), Radial Basis Function Network (RBFN), K-Nearest Neighbor (KNN) on Pima Indian Diabetes Dataset and Localized Diabetes Dataset. In the second approach, Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) have been applied as a feature reduction method followed by using the same set of classification methods used in the first approach. Among all of the implemented classification methods, PCA_CVR achieves the maximum performance for both the above mentioned datasets.              Conclusion:                    In this article, comparative analysis of outcomes obtained by with and without the use of PCA and LDA for the same set of classification method has been done w.r.t performance assessment. Finally, it has been concluded that PCA & LDA both are useful to remove the insignificant features, decreasing the expense and computation time while improving the ROC and accuracy. The used methodology may similarly be applied to other medical diseases."
