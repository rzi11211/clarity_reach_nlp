pmid,title,date,text,citations
31522268,"Distress, Suicidality, and Affective Disorders at the Time of Social Networks",2019 Sep 14;21(10):98.,"Purpose of review:                    We reviewed how scholars recently addressed the complex relationship that binds distress, affective disorders, and suicidal behaviors on the one hand and social networking on the other. We considered the latest machine learning performances in detecting affective-related outcomes from social media data, and reviewed understandings of how, why, and with what consequences distressed individuals use social network sites. Finally, we examined how these insights may concretely instantiate on the individual level with a qualitative case series.              Recent findings:                    Machine learning classifiers are progressively stabilizing with moderate to high performances in detecting affective-related diagnosis, symptoms, and risks from social media linguistic markers. Qualitatively, such markers appear to translate ambivalent and socially constrained motivations such as self-disclosure, passive support seeking, and connectedness reinforcement. Binding data science and psychosocial research appears as the unique condition to ground a translational web-clinic for treating and preventing affective-related issues on social media.",
31522256,Fatal and Non-fatal Self-Injury in the USA: Critical Review of Current Trends and Innovations in Prevention,2019 Sep 14;21(10):104.,"Purpose of review:                    To examine current trends in suicide and self-injury in the USA, as well as potential contributors to their change over time, and to reflect on innovations in prevention and intervention that can guide policies and programs to reduce the burden of suicide and self-injury in the USA.              Recent findings:                    Suicide and non-fatal self-injury are on the rise in the USA. Reasons for such trends over time remain speculative, although they seem linked to coincident increases in mood disorders and drug use and overdose. Promising innovative prevention and intervention programs that engage new technologies, such as machine learning-derived prediction tools and computerized ecologic momentary assessments, are currently in development and require additional evidence. Recent increases in fatal and non-fatal self-harm in the USA raise questions about the causes, interventions, and preventive measures that should be taken. Most innovative prevention efforts target individuals seeking to improve risk prediction and access to evidence-based care. However, as Durkheim pointed out over 100 years ago, suicide rates vary enormously between societal groups, suggesting that certain causal factors of suicide act and, hence, should be targeted at an ecological level. In the next generation of suicide research, it is critical to examine factors beyond the proximal and clinical to allow for a reimagining of prevention that is life course and socially focused.",2.0
31521533,The genetic architecture of Parkinson's disease,2020 Feb;19(2):170-178.,"Parkinson's disease is a complex neurodegenerative disorder for which both rare and common genetic variants contribute to disease risk, onset, and progression. Mutations in more than 20 genes have been associated with the disease, most of which are highly penetrant and often cause early onset or atypical symptoms. Although our understanding of the genetic basis of Parkinson's disease has advanced considerably, much remains to be done. Further disease-related common genetic variability remains to be identified and the work in identifying rare risk alleles has only just begun. To date, genome-wide association studies have identified 90 independent risk-associated variants. However, most of them have been identified in patients of European ancestry and we know relatively little of the genetics of Parkinson's disease in other populations. We have a limited understanding of the biological functions of the risk alleles that have been identified, although Parkinson's disease risk variants appear to be in close proximity to known Parkinson's disease genes and lysosomal-related genes. In the past decade, multiple efforts have been made to investigate the genetic architecture of Parkinson's disease, and emerging technologies, such as machine learning, single-cell RNA sequencing, and high-throughput screens, will improve our understanding of genetic risk.",49.0
31521378,Machine learning in the electrocardiogram,Nov-Dec 2019;57S:S61-S64.,"The electrocardiogram is the most widely used diagnostic tool that records the electrical activity of the heart and, therefore, its use for identifying markers for early diagnosis and detection is of paramount importance. In the last years, the huge increase of electronic health records containing a systematised collection of different type of digitalised medical data, together with new tools to analyse this large amount of data in an efficient way have re-emerged the field of machine learning in healthcare innovation. This review describes the most recent machine learning-based systems applied to the electrocardiogram as well as pros and cons in the use of these techniques. Machine learning, including deep learning, have shown to be powerful tools for aiding clinicians in patient screening and risk stratification tasks. However, they do not provide the physiological basis of classification outcomes. Computational modelling and simulation can help in the interpretation and understanding of key physiologically meaningful ECG biomarkers extracted from machine learning techniques.",4.0
31521235,Computational prediction of functions of intrinsically disordered regions,2019;166:341-369.,"Intrinsically disorder regions (IDRs) are abundant in nature, particularly among Eukaryotes. While they facilitate a wide spectrum of cellular functions including signaling, molecular assembly and recognition, translation, transcription and regulation, only several hundred IDRs are annotated functionally. This annotation gap motivates the development of fast and accurate computational methods that predict IDR functions directly from protein sequences. We introduce and describe a comprehensive collection of 25 methods that provide accurate predictions of IDRs that interact with proteins and nucleic acids, that function as flexible linkers and that moonlight multiple functions. Virtually all of these predictors can be accessed online and many were developed in the last few years. They utilize a wide range of predictive architectures and take advantage of modern machine learning algorithms. Our empirical analysis shows that predictors that are available as webservers enjoy high rates of citations, attesting to their practical value and popularity. The most cited methods include DISOPRED3, ANCHOR, alpha-MoRFpred, MoRFpred, fMoRFpred and MoRFCHiBi. We present two case studies to demonstrate that predictions produced by these computational tools are relatively easy to interpret and that they deliver valuable functional clues. However, the current computational tools cover a relatively narrow range of disorder functions. Further development efforts that would cover a broader range of functions should be pursued. We demonstrate that a sufficient amount of functionally annotated IDRs that are associated with several other disorder functions is already available and can be used to design and validate novel predictors.",3.0
31521231,Intrinsically disordered proteins in various hypotheses on the pathogenesis of Alzheimer's and Parkinson's diseases,2019;166:145-223.,"Amyloid-β (Aβ) and α-synuclein (αS) are two intrinsically disordered proteins (IDPs) at the centers of the pathogenesis of Alzheimer's and Parkinson's diseases, respectively. Different hypotheses have been proposed for explanation of the molecular mechanisms of the pathogenesis of these two diseases, with these two IDPs being involved in many of these hypotheses. Currently, we do not know, which of these hypothesis is more accurate. Experiments face challenges due to the rapid conformational changes, fast aggregation processes, solvent and paramagnetic effects in studying these two IDPs in detail. Furthermore, pathological modifications impact their structures and energetics. Theoretical studies using computational chemistry and computational biology have been utilized to understand the structures and energetics of Aβ and αS. In this chapter, we introduce Aβ and αS in light of various hypotheses, and discuss different experimental and theoretical techniques that are used to study these two proteins along with their weaknesses and strengths. We suggest that a promising solution for studying Aβ and αS at the center of varying hypotheses could be provided by developing new techniques that link quantum mechanics, statistical mechanics, thermodynamics, bioinformatics to machine learning. Such new developments could also lead to development in experimental techniques.",3.0
31518831,Comprehensive intra-individual genomic and transcriptional heterogeneity: Evidence-based Colorectal Cancer Precision Medicine,2019 Nov;80:101894.,"Despite advances in translating conventional research into multi-modal treatment for colorectal cancer (CRC), therapeutic resistance and relapse remain unresolved in advanced resectable and, particularly, non-resectable disease. Genome and transcriptome sequencing and editing technologies, coupled with interaction mapping and machine learning, are transforming biomedical research, representing the most rational hope to overcome unmet research and clinical challenges. Rapid progress in both bulk and single-cell next-generation sequencing (NGS) analyses in the identification of primary and metastatic intratumor genomic and transcriptional heterogeneity (ITH) and the detection of circulating cell-free DNA (cfDNA) alterations is providing critical insight into the origins and spatiotemporal evolution of genomic clones responsible for early and late therapeutic resistance and relapse. Moreover, DNA and RNA editing pave new avenues towards the discovery of novel drug targets. Breakthrough combinations of sequencing and editing systems with technologies exploring dynamic interaction networks within pioneering studies could delineate how coding and non-coding mutations perturb regulatory networks and gene expression. This review discusses latest data on genomic and transcriptomic landscapes in time and space, as well as early-phase clinical trials on targeted drug combinations, highlighting the transition from research to clinical Colorectal Cancer Precision Medicine, through non-invasive screening, individualized drug response prediction and development of multiple novel drugs. Future studies exploring the potential to target key transcriptional drivers and regulators will contribute to the next-generation pharmaceutical controllability of multi-layered aberrant transcriptional biocircuits.",9.0
31518513,Big Data and Artificial Intelligence Modeling for Drug Discovery,2020 Jan 6;60:573-589.,"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health.",9.0
31513851,Optical coherence tomography diagnostic signs in posterior uveitis,2020 Mar;75:100797.,"A diagnostic sign refers to a quantifiable biological parameter that is measured and evaluated as an indicator of normal biological, pathogenic, or pharmacologic responses to a therapeutic intervention. When used in translational research discussions, the term itself often alludes to a sign used to accelerate or aid in diagnosis or monitoring and provide insight into ""personalized"" medicine. Many new diagnostic signs are being developed that involve imaging technology. Optical coherence tomography is an imaging technique that provides in vivo quasi-histological images of the ocular tissues and as such it's able to capture the structural and functional modifications that accompany inflammation and infection of the posterior part of the eye. From the hyperreflective inflammatory cells and deposits in the vitreous and on the hyaloid, to the swollen photoreceptors bodies in multiple evanescent white dots syndrome, and from optical differences in the subretinal fluid compartments in Vogt-Koyanagi-Harada disease to the hyporeflective granulomas in the choroid, these tomographical signs can be validated to reach the status of biomarkers. Such non-invasive imaging diagnostic signs of inflammation can be very useful to clinicians seeking to make a diagnosis and can represent a dataset for machine learning to offer a more empirical approach to the detection of posterior uveitis.",5.0
31513757,Remaining challenges in predicting patient outcomes for diffuse large B-cell lymphoma,2019 Nov;12(11):959-973.,"Introduction: Diffuse large B-cell lymphoma (DLBCL) is the most common non-Hodgkin lymphoma and is an aggressive malignancy with heterogeneous outcomes. Diverse methods for DLBCL outcomes assessment ranging from clinical to genomic have been developed with variable predictive and prognostic success.Areas covered: The authors provide an overview of the various methods currently used to estimate prognosis in DLBCL patients. Models incorporating cell of origin, genomic features, sociodemographic factors, treatment effectiveness measures, and machine learning are described.Expert opinion: The clinical and genetic heterogeneity of DLBCL presents distinct challenges in predicting response to therapy and overall prognosis. Successful integration of predictive and prognostic tools in clinical trials and in a standard clinical workflow for DLBCL will likely require a combination of methods incorporating clinical, sociodemographic, and molecular factors with the aid of machine learning and high-dimensional data analysis.",1.0
31513441,4D- quantitative structure-activity relationship modeling: making a comeback,2019 Dec;14(12):1227-1235.,"Introduction: Predictive Quantitative Structure-Activity Relationship (QSAR) modeling has become an essential methodology for rapidly assessing various properties of chemicals. The vast majority of these QSAR models utilize numerical descriptors derived from the two- and/or three-dimensional structures of molecules. However, the conformation-dependent characteristics of flexible molecules and their dynamic interactions with biological target(s) is/are not encoded by these descriptors, leading to limited prediction performances and reduced interpretability. 2D/3D QSAR models are successful for virtual screening, but typically suffer at lead optimization stages. That is why conformation-dependent 4D-QSAR modeling methods were developed two decades ago. However, these methods have always suffered from the associated computational cost. Recently, 4D-QSAR has been experiencing a significant come-back due to rapid advances in GPU-accelerated molecular dynamic simulations and modern machine learning techniques. Areas covered: Herein, the authors briefly review the literature regarding 4D-QSAR modeling and describe its modern workflow called MD-QSAR. Challenges and current limitations are also highlighted. Expert opinion: The development of hyper-predictive MD-QSAR models could represent a disruptive technology for analyzing, understanding, and optimizing dynamic protein-ligand interactions with countless applications for drug discovery and chemical toxicity assessment. Therefore, there has never been a better time and relevance for molecular modeling teams to engage in hyper-predictive MD-QSAR modeling.",5.0
31510795,Cardiac Computed Tomography: Assessment of Coronary Inflammation and Other Plaque Features,2019 Nov;39(11):2207-2219.,"Unstable coronary plaques that are prone to erosion and rupture are the major cause of acute coronary syndromes. Our expanding understanding of the biological mechanisms of coronary atherosclerosis and rapid technological advances in the field of medical imaging has established cardiac computed tomography as a first-line diagnostic test in the assessment of suspected coronary artery disease, and as a powerful method of detecting the vulnerable plaque and patient. Cardiac computed tomography can provide a noninvasive, yet comprehensive, qualitative and quantitative assessment of coronary plaque burden, detect distinct high-risk morphological plaque features, assess the hemodynamic significance of coronary lesions and quantify the coronary inflammatory burden by tracking the effects of arterial inflammation on the composition of the adjacent perivascular fat. Furthermore, advances in machine learning, computational fluid dynamic modeling, and the development of targeted contrast agents continue to expand the capabilities of cardiac computed tomography imaging. In our Review, we discuss the current role of cardiac computed tomography in the assessment of coronary atherosclerosis, highlighting its dual function as a clinical and research tool that provides a wealth of structural and functional information, with far-reaching diagnostic and prognostic implications.",2.0
31510695,scOrange-a tool for hands-on training of concepts from single-cell data analytics,2019 Jul 15;35(14):i4-i12.,"Motivation:                    Single-cell RNA sequencing allows us to simultaneously profile the transcriptomes of thousands of cells and to indulge in exploring cell diversity, development and discovery of new molecular mechanisms. Analysis of scRNA data involves a combination of non-trivial steps from statistics, data visualization, bioinformatics and machine learning. Training molecular biologists in single-cell data analysis and empowering them to review and analyze their data can be challenging, both because of the complexity of the methods and the steep learning curve.              Results:                    We propose a workshop-style training in single-cell data analytics that relies on an explorative data analysis toolbox and a hands-on teaching style. The training relies on scOrange, a newly developed extension of a data mining framework that features workflow design through visual programming and interactive visualizations. Workshops with scOrange can proceed much faster than similar training methods that rely on computer programming and analysis through scripting in R or Python, allowing the trainer to cover more ground in the same time-frame. We here review the design principles of the scOrange toolbox that support such workshops and propose a syllabus for the course. We also provide examples of data analysis workflows that instructors can use during the training.              Availability and implementation:                    scOrange is an open-source software. The software, documentation and an emerging set of educational videos are available at http://singlecell.biolab.si.",2.0
31509997,Integrations between Autonomous Systems and Modern Computing Techniques: A Mini Review,2019 Sep 10;19(18):3897.,"The emulation of human behavior for autonomous problem solving has been an interdisciplinary field of research. Generally, classical control systems are used for static environments, where external disturbances and changes in internal parameters can be fully modulated before or neglected during operation. However, classical control systems are inadequate at addressing environmental uncertainty. By contrast, autonomous systems, which were first studied in the field of control systems, can be applied in an unknown environment. This paper summarizes the state of the art autonomous systems by first discussing the definition, modeling, and system structure of autonomous systems and then providing a perspective on how autonomous systems can be integrated with advanced resources (e.g., the Internet of Things, big data, Over-the-Air, and federated learning). Finally, what comes after reaching full autonomy is briefly discussed.",2.0
31505121,Vector-Space Models of Semantic Representation From a Cognitive Perspective: A Discussion of Common Misconceptions,2019 Nov;14(6):1006-1033.,"Models that represent meaning as high-dimensional numerical vectors-such as latent semantic analysis (LSA), hyperspace analogue to language (HAL), bound encoding of the aggregate language environment (BEAGLE), topic models, global vectors (GloVe), and word2vec-have been introduced as extremely powerful machine-learning proxies for human semantic representations and have seen an explosive rise in popularity over the past 2 decades. However, despite their considerable advancements and spread in the cognitive sciences, one can observe problems associated with the adequate presentation and understanding of some of their features. Indeed, when these models are examined from a cognitive perspective, a number of unfounded arguments tend to appear in the psychological literature. In this article, we review the most common of these arguments and discuss (a) what exactly these models represent at the implementational level and their plausibility as a cognitive theory, (b) how they deal with various aspects of meaning such as polysemy or compositionality, and (c) how they relate to the debate on embodied and grounded cognition. We identify common misconceptions that arise as a result of incomplete descriptions, outdated arguments, and unclear distinctions between theory and implementation of the models. We clarify and amend these points to provide a theoretical basis for future research and discussions on vector models of semantic representation.",5.0
31500324,Independent Component Analysis for Unraveling the Complexity of Cancer Omics Datasets,2019 Sep 7;20(18):4414.,"Independent component analysis (ICA) is a matrix factorization approach where the signals captured by each individual matrix factors are optimized to become as mutually independent as possible. Initially suggested for solving source blind separation problems in various fields, ICA was shown to be successful in analyzing functional magnetic resonance imaging (fMRI) and other types of biomedical data. In the last twenty years, ICA became a part of the standard machine learning toolbox, together with other matrix factorization methods such as principal component analysis (PCA) and non-negative matrix factorization (NMF). Here, we review a number of recent works where ICA was shown to be a useful tool for unraveling the complexity of cancer biology from the analysis of different types of omics data, mainly collected for tumoral samples. Such works highlight the use of ICA in dimensionality reduction, deconvolution, data pre-processing, meta-analysis, and others applied to different data types (transcriptome, methylome, proteome, single-cell data). We particularly focus on the technical aspects of ICA application in omics studies such as using different protocols, determining the optimal number of components, assessing and improving reproducibility of the ICA results, and comparison with other popular matrix factorization techniques. We discuss the emerging ICA applications to the integrative analysis of multi-level omics datasets and introduce a conceptual view on ICA as a tool for defining functional subsystems of a complex biological system and their interactions under various conditions. Our review is accompanied by a Jupyter notebook which illustrates the discussed concepts and provides a practical tool for applying ICA to the analysis of cancer omics datasets.",7.0
31498973,Machine learning applications in the diagnosis of leukemia: Current trends and future directions,2019 Dec;41(6):717-725.,"Machine learning (ML) offers opportunities to advance pathological diagnosis, especially with increasing trends in digitalizing microscopic images. Diagnosing leukemia is time-consuming and challenging in many areas globally and there is a growing trend in utilizing ML techniques for its diagnosis. In this review, we aimed to describe the literature of ML utilization in the diagnosis of the four common types of leukemia: acute lymphocytic leukemia (ALL), chronic lymphocytic leukemia (CLL), acute myeloid leukemia (AML), and chronic myelogenous leukemia (CML). Using a strict selection criterion, utilizing MeSH terminology and Boolean logic, an electronic search of MEDLINE and IEEE Xplore Digital Library was performed. The electronic search was complemented by handsearching of references of related studies and the top results of Google Scholar. The full texts of 58 articles were reviewed, out of which, 22 studies were included. The number of studies discussing ALL, AML, CLL, and CML was 12, 8, 3, and 1, respectively. No studies were prospectively applying algorithms in real-world scenarios. Majority of studies had small and homogenous samples and used supervised learning for classification tasks. 91% of the studies were performed after 2010, and 74% of the included studies applied ML algorithms to microscopic diagnosis of leukemia. The included studies illustrated the need to develop the field of ML research, including the transformation from solely designing algorithms to practically applying them clinically.",4.0
31498057,Twin Registries Moving Forward and Meeting the Future: A Review,2019 Aug;22(4):201-209.,"Twin registries have developed as a valuable resource for the study of many aspects of disease and society over the years in many different countries. A number of these registries include large numbers of twins with data collected at varying information levels for twin cohorts over the past several decades. More recent expansion of twin datasets has allowed for the collection of genetic data, together with many other levels of 'omic' information along with multiple demographic, physiological, health outcomes and other measures typically used in epidemiologic research. Other twin data sources outside these registries reflect research interests in particular aspects of disease or specific phenotypic assessment. Twin registries have the potential to play a key role in many aspects of the artificial intelligence/machine learning-driven projects of the future and will continue to keep adapting to the changing research landscape.",1.0
31497064,Measuring crops in 3D: using geometry for plant phenotyping,2019 Sep 3;15:103.,"Using 3D sensing for plant phenotyping has risen within the last years. This review provides an overview on 3D traits for the demands of plant phenotyping considering different measuring techniques, derived traits and use-cases of biological applications. A comparison between a high resolution 3D measuring device and an established measuring tool, the leaf meter, is shown to categorize the possible measurement accuracy. Furthermore, different measuring techniques such as laser triangulation, structure from motion, time-of-flight, terrestrial laser scanning or structured light approaches enable the assessment of plant traits such as leaf width and length, plant size, volume and development on plant and organ level. The introduced traits were shown with respect to the measured plant types, the used measuring technique and the link to their biological use case. These were trait and growth analysis for measurements over time as well as more complex investigation on water budget, drought responses and QTL (quantitative trait loci) analysis. The used processing pipelines were generalized in a 3D point cloud processing workflow showing the single processing steps to derive plant parameters on plant level, on organ level using machine learning or over time using time series measurements. Finally the next step in plant sensing, the fusion of different sensor types namely 3D and spectral measurements is introduced by an example on sugar beet. This multi-dimensional plant model is the key to model the influence of geometry on radiometric measurements and to correct it. This publication depicts the state of the art for 3D measuring of plant traits as they were used in plant phenotyping regarding how the data is acquired, how this data is processed and what kind of traits is measured at the single plant, the miniplot, the experimental field and the open field scale. Future research will focus on highly resolved point clouds on the experimental and field scale as well as on the automated trait extraction of organ traits to track organ development at these scales.",11.0
31496926,Inter-Species/Host-Parasite Protein Interaction Predictions Reviewed,2018 Aug;13(4):396-406.,"Background:                    Host-parasite protein interactions (HPPI) are those interactions occurring between a parasite and its host. Host-parasite protein interaction enhances the understanding of how parasite can infect its host. The interaction plays an important role in initiating infections, although it is not all host-parasite interactions that result in infection. Identifying the protein-protein interactions (PPIs) that allow a parasite to infect its host has a lot do in discovering possible drug targets. Such PPIs, when altered, would prevent the host from being infected by the parasite and in some cases, result in the parasite inability to complete specific stages of its life cycle and invariably lead to the death of such parasite. It therefore becomes important to understand the workings of host-parasite interactions which are the major causes of most infectious diseases.              Objective:                    Many studies have been conducted in literature to predict HPPI, mostly using computational methods with few experimental methods. Computational method has proved to be faster and more efficient in manipulating and analyzing real life data. This study looks at various computational methods used in literature for host-parasite/inter-species protein-protein interaction predictions with the hope of getting a better insight into computational methods used and identify whether machine learning approaches have been extensively used for the same purpose.              Methods:                    The various methods involved in host-parasite protein interactions were reviewed with their individual strengths. Tabulations of studies that carried out host-parasite/inter-species protein interaction predictions were performed, analyzing their predictive methods, filters used, potential protein-protein interactions discovered in those studies and various validation measurements used as the case may be. The commonly used measurement indexes for such studies were highlighted displaying the various formulas. Finally, future prospects of studies specific to human-plasmodium falciparum PPI predictions were proposed.              Result:                    We discovered that quite a few studies reviewed implemented machine learning approach for HPPI predictions when compared with methods such as sequence homology search and protein structure and domain-motif. The key challenge well noted in HPPI predictions is getting relevant information.              Conclusion:                    This review presents useful knowledge and future directions on the subject matter.",3.0
31496867,Toward a grey box approach for cardiovascular physiome,2019 Sep;23(5):305-310.,"The physiomic approach is now widely used in the diagnosis of cardiovascular diseases. There are two possible methods for cardiovascular physiome: the traditional mathematical model and the machine learning (ML) algorithm. ML is used in almost every area of society for various tasks formerly performed by humans. Specifically, various ML techniques in cardiovascular medicine are being developed and improved at unprecedented speed. The benefits of using ML for various tasks is that the inner working mechanism of the system does not need to be known, which can prove convenient in situations where determining the inner workings of the system can be difficult. The computation speed is also often higher than that of the traditional mathematical models. The limitations with ML are that it inherently leads to an approximation, and special care must be taken in cases where a high accuracy is required. Traditional mathematical models are, however, constructed based on underlying laws either proven or assumed. The results from the mathematical models are accurate as long as the model is. Combining the advantages of both the mathematical models and ML would increase both the accuracy and efficiency of the simulation for many problems. In this review, examples of cardiovascular physiome where approaches of mathematical modeling and ML can be combined are introduced.",1.0
31495281,Artificial Intelligence in Radiotherapy Treatment Planning: Present and Future,2019 Jan 1;18:1533033819873922.,"Treatment planning is an essential step of the radiotherapy workflow. It has become more sophisticated over the past couple of decades with the help of computer science, enabling planners to design highly complex radiotherapy plans to minimize the normal tissue damage while persevering sufficient tumor control. As a result, treatment planning has become more labor intensive, requiring hours or even days of planner effort to optimize an individual patient case in a trial-and-error fashion. More recently, artificial intelligence has been utilized to automate and improve various aspects of medical science. For radiotherapy treatment planning, many algorithms have been developed to better support planners. These algorithms focus on automating the planning process and/or optimizing dosimetric trade-offs, and they have already made great impact on improving treatment planning efficiency and plan quality consistency. In this review, the smart planning tools in current clinical use are summarized in 3 main categories: automated rule implementation and reasoning, modeling of prior knowledge in clinical practice, and multicriteria optimization. Novel artificial intelligence-based treatment planning applications, such as deep learning-based algorithms and emerging research directions, are also reviewed. Finally, the challenges of artificial intelligence-based treatment planning are discussed for future works.",8.0
31493727,Screening mammography beyond breast cancer: breast arterial calcifications as a sex-specific biomarker of cardiovascular risk,2019 Oct;119:108636.,"Purpose:                    To highlight the importance of quantitative breast arterial calcifications (BAC) assessment for an effective stratification of cardiovascular (CV) risk in women, for whom current preventive strategies are inadequate. BAC, easily detectable on mammograms, are associated with CV disease and represent a potential imaging biomarker for CV disease prevention in women.              Method:                    We summarized the available evidence on this topic.              Results:                    Age, parity, diabetes, and hyperlipidemia were found to positively correlate with BAC. Women with BAC have a higher CV risk than those without BAC: the relative risk was reported to be 1.4 for transient ischemic attack/stroke, 1.5 for thrombosis, 1.8 for myocardial infarction; the reported hazard ratio was 1.32 for coronary artery disease (CAD), 1.52 for heart failure, 1.29 for CV death, 1.44 for death from CAD. However, BAC do not alarm radiologists; when reported, they are commonly mentioned as ""present"", not impacting on CV decision-making. Of 18 published studies, 9 reported only presence/absence of BAC, 4 used a semi-quantitative scale, and 5 a continuous scale (with manual, automatic or semiautomatic segmentation). Various appearance, topological complexity, and vessels overlap make BAC quantification difficult to standardize. Nevertheless, machine learning approaches showed promising results in BAC quantification on mammograms.              Conclusions:                    There is a strong rationale for mammography to become a dual test for breast cancer screening and CV disease prevention. However, robust and automated quantification methods are needed for a deeper insight on the association between BAC and CV disease, to stratifying CV risk and define personalized preventive actions.",3.0
31492414,Artificial Intelligence and Clinical Decision Support for Radiologists and Referring Providers,2019 Sep;16(9 Pt B):1351-1356.,"Recent advances in artificial intelligence (AI) are providing an opportunity to enhance existing clinical decision support (CDS) tools to improve patient safety and drive value-based imaging. We discuss the advantages and potential applications that may be realized with the synergy between AI and CDS systems. From the perspective of both radiologist and ordering provider, CDS could be significantly empowered using AI. CDS enhanced by AI could reduce friction in radiology workflows and can aid AI developers to identify relevant imaging features their tools should be seeking to extract from images. Furthermore, these systems can generate structured data to be used as input to develop machine learning algorithms, which can drive downstream care pathways. For referring providers, an AI-enabled CDS solution could enable an evolution from existing imaging-centric CDS toward decision support that takes into account a holistic patient perspective. More intelligent CDS could suggest imaging examinations in highly complex clinical scenarios, assist on the identification of appropriate imaging opportunities at the health system level, suggest appropriate individualized screening, or aid health care providers to ensure continuity of care. AI has the potential to enable the next generation of CDS, improving patient care and enhancing providers' and radiologists' experience.",2.0
31492411,The Potential Role of Radiomics and Radiogenomics in Patient Stratification by Tumor Hypoxia Status,2019 Sep;16(9 Pt B):1329-1337.,"Background:                    Despite the clinical knowledge accumulated over a century about tumor hypoxia, this biologic parameter remains a major challenge in cancer treatment. Patients presenting with hypoxic tumors are more resistant to radiotherapy and often poor responders to chemotherapy. Treatment failure because of hypoxia is, therefore, very common. Several methods have been trialed to measure and quantify tumor hypoxia, with varied success. Over the last couple of decades, hypoxia-specific functional imaging has started to play an important role in personalized treatment planning and delivery. Yet, there are no gold standards in place, owing to inter- and intrapatient phenotypic variations that further complicate the overall picture. The aim of the current article is to analyze, through the review of the literature, the potential role of radiomics and radiogenomics in patient stratification by tumor hypoxia status.              Methods:                    Search of literature published in English since 2000 was conducted using Medline. Additional articles were retrieved via pearling of identified literature. Publications were reviewed and summarized in text and in a tabulated format.              Results:                    Although still an immature area of science, radiomics has shown its potential in the quantification of hypoxia within the heterogeneous tumor, quantification of changes regarding the degree of hypoxia after radiotherapy and drug delivery, monitoring tumor response to anti-angiogenic therapy, and assisting with patient stratification and outcome prediction based on the hypoxic status.              Conclusions:                    The lack of technique standardization to measure and quantify tumor hypoxia presents an opportunity for data mining and machine learning in radiogenomics.",
31492410,A Survey of Deep-Learning Applications in Ultrasound: Artificial Intelligence-Powered Ultrasound for Improving Clinical Workflow,2019 Sep;16(9 Pt B):1318-1328.,"Ultrasound is the most commonly used imaging modality in clinical practice because it is a nonionizing, low-cost, and portable point-of-care imaging tool that provides real-time images. Artificial intelligence (AI)-powered ultrasound is becoming more mature and getting closer to routine clinical applications in recent times because of an increased need for efficient and objective acquisition and evaluation of ultrasound images. Because ultrasound images involve operator-, patient-, and scanner-dependent variations, the adaptation of classical machine learning methods to clinical applications becomes challenging. With their self-learning ability, deep-learning (DL) methods are able to harness exponentially growing graphics processing unit computing power to identify abstract and complex imaging features. This has given rise to tremendous opportunities such as providing robust and generalizable AI models for improving image acquisition, real-time assessment of image quality, objective diagnosis and detection of diseases, and optimizing ultrasound clinical workflow. In this report, the authors review current DL approaches and research directions in rapidly advancing ultrasound technology and present their outlook on future directions and trends for DL techniques to further improve diagnosis, reduce health care cost, and optimize ultrasound clinical workflow.",10.0
31492404,Using Artificial Intelligence to Improve the Quality and Safety of Radiation Therapy,2019 Sep;16(9 Pt B):1267-1272.,"Within artificial intelligence, machine learning (ML) efforts in radiation oncology have augmented the transition from generalized to personalized treatment delivery. Although their impact on quality and safety of radiation therapy has been limited, they are increasingly being used throughout radiation therapy workflows. Various data-driven approaches have been used for outcome prediction, CT simulation, clinical decision support, knowledge-based planning, adaptive radiation therapy, plan validation, machine quality assurance, and process quality assurance; however, there are many challenges that need to be addressed with the creation and usage of ML algorithms as well as the interpretation and dissemination of findings. In this review, the authors present current applications of ML in radiation oncology quality and safety initiatives, discuss challenges faced by the radiation oncology community, and suggest future directions.",4.0
31492403,The Application of Machine Learning to Quality Improvement Through the Lens of the Radiology Value Network,2019 Sep;16(9 Pt B):1254-1258.,"Recent advances in machine learning and artificial intelligence offer promising applications to radiology quality improvement initiatives as they relate to the radiology value network. Coordination within the interlocking web of systems, events, and stakeholders in the radiology value network may be mitigated though standardization, automation, and a focus on workflow efficiency. In this article the authors present applications of these various strategies via use cases for quality improvement projects at different points in the radiology value network. In addition, the authors discuss opportunities for machine-learning applications in data aggregation as opposed to traditional applications in data extraction.",
31492401,"Strengths, Weaknesses, Opportunities, and Threats Analysis of Artificial Intelligence and Machine Learning Applications in Radiology",2019 Sep;16(9 Pt B):1239-1247.,"Currently, the use of artificial intelligence (AI) in radiology, particularly machine learning (ML), has become a reality in clinical practice. Since the end of the last century, several ML algorithms have been introduced for a wide range of common imaging tasks, not only for diagnostic purposes but also for image acquisition and postprocessing. AI is now recognized to be a driving initiative in every aspect of radiology. There is growing evidence of the advantages of AI in radiology creating seamless imaging workflows for radiologists or even replacing radiologists. Most of the current AI methods have some internal and external disadvantages that are impeding their ultimate implementation in the clinical arena. As such, AI can be considered a portion of a business trying to be introduced in the health care market. For this reason, this review analyzes the current status of AI, and specifically ML, applied to radiology from the scope of strengths, weaknesses, opportunities, and threats (SWOT) analysis.",10.0
31491852,Cardioprotective Melatonin: Translating from Proof-of-Concept Studies to Therapeutic Use,2019 Sep 5;20(18):4342.,"In this review we summarized the actual clinical data for a cardioprotective therapeutic role of melatonin, listed melatonin and its agonists in different stages of development, and evaluated the melatonin cardiovascular target tractability and prediction using machine learning on ChEMBL. To date, most clinical trials investigating a cardioprotective therapeutic role of melatonin are in phase 2a. Selective melatonin receptor agonists Tasimelteon, Ramelteon, and combined melatonergic-serotonin Agomelatine, and other agonists with registered structures in CHEMBL were not yet investigated as cardioprotective or cardiovascular drugs. As drug-able for these therapeutic targets, melatonin receptor agonists have the benefit over melatonin of well-characterized pharmacologic profiles and extensive safety data. Recent reports of the X-ray crystal structures of MT1 and MT2 receptors shall lead to the development of highly selective melatonin receptor agonists. Predictive models using machine learning could help to identify cardiovascular targets for melatonin. Selecting ChEMBL scores > 4.5 in cardiovascular assays, and melatonin scores > 4, we obtained 284 records from 162 cardiovascular assays carried out with 80 molecules with predicted or measured melatonin activity. Melatonin activities (agonistic or antagonistic) found in these experimental cardiovascular assays and models include arrhythmias, coronary and large vessel contractility, and hypertension. Preclinical proof-of-concept and early clinical studies (phase 2a) suggest a cardioprotective benefit from melatonin in various heart diseases. However, larger phase 3 randomized interventional studies are necessary to establish melatonin and its agonists' actions as cardioprotective therapeutic agents.",5.0
31488886,Artificial intelligence for diabetic retinopathy screening: a review,2020 Mar;34(3):451-460.,"Diabetes is a global eye health issue. Given the rising in diabetes prevalence and ageing population, this poses significant challenge to perform diabetic retinopathy (DR) screening for these patients. Artificial intelligence (AI) using machine learning and deep learning have been adopted by various groups to develop automated DR detection algorithms. This article aims to describe the state-of-art AI DR screening technologies that have been described in the literature, some of which are already commercially available. All these technologies were designed using different training datasets and technical methodologies. Although many groups have published robust diagnostic performance of the AI algorithms for DR screening, future research is required to address several challenges, for examples medicolegal implications, ethics, and clinical deployment model in order to expedite the translation of these novel technologies into the healthcare setting.",17.0
31486179,Machine Learning Interatomic Potentials as Emerging Tools for Materials Science,2019 Nov;31(46):e1902765.,"Atomic-scale modeling and understanding of materials have made remarkable progress, but they are still fundamentally limited by the large computational cost of explicit electronic-structure methods such as density-functional theory. This Progress Report shows how machine learning (ML) is currently enabling a new degree of realism in materials modeling: by ""learning"" electronic-structure data, ML-based interatomic potentials give access to atomistic simulations that reach similar accuracy levels but are orders of magnitude faster. A brief introduction to the new tools is given, and then, applications to some select problems in materials science are highlighted: phase-change materials for memory devices; nanoparticle catalysts; and carbon-based electrodes for chemical sensing, supercapacitors, and batteries. It is hoped that the present work will inspire the development and wider use of ML-based interatomic potentials in diverse areas of materials research.",8.0
31481588,Radiomics: Data Are Also Images,2019 Sep;60(Suppl 2):38S-44S.,"The aim of this review is to provide readers with an update on the state of the art, pitfalls, solutions for those pitfalls, future perspectives, and challenges in the quickly evolving field of radiomics in nuclear medicine imaging and associated oncology applications. The main pitfalls were identified in study design, data acquisition, segmentation, feature calculation, and modeling; however, in most cases, potential solutions are available and existing recommendations should be followed to improve the overall quality and reproducibility of published radiomics studies. The techniques from the field of deep learning have some potential to provide solutions, especially in terms of automation. Some important challenges remain to be addressed but, overall, striking advances have been made in the field in the last 5 y.",6.0
31481587,Artificial Intelligence in Nuclear Medicine,2019 Sep;60(Suppl 2):29S-37S.,"Despite the great media attention for artificial intelligence (AI), for many health care professionals the term and the functioning of AI remain a ""black box,"" leading to exaggerated expectations on the one hand and unfounded fears on the other. In this review, we provide a conceptual classification and a brief summary of the technical fundamentals of AI. Possible applications are discussed on the basis of a typical work flow in medical imaging, grouped by planning, scanning, interpretation, and reporting. The main limitations of current AI techniques, such as issues with interpretability or the need for large amounts of annotated data, are briefly addressed. Finally, we highlight the possible impact of AI on the nuclear medicine profession, the associated challenges and, last but not least, the opportunities.",6.0
31480359,Review on Smart Gas Sensing Technology,2019 Aug 30;19(17):3760.,"With the development of the Internet-of-Things (IoT) technology, the applications of gas sensors in the fields of smart homes, wearable devices, and smart mobile terminals have developed by leaps and bounds. In such complex sensing scenarios, the gas sensor shows the defects of cross sensitivity and low selectivity. Therefore, smart gas sensing methods have been proposed to address these issues by adding sensor arrays, signal processing, and machine learning techniques to traditional gas sensing technologies. This review introduces the reader to the overall framework of smart gas sensing technology, including three key points; gas sensor arrays made of different materials, signal processing for drift compensation and feature extraction, and gas pattern recognition including Support Vector Machine (SVM), Artificial Neural Network (ANN), and other techniques. The implementation, evaluation, and comparison of the proposed solutions in each step have been summarized covering most of the relevant recently published studies. This review also highlights the challenges facing smart gas sensing technology represented by repeatability and reusability, circuit integration and miniaturization, and real-time sensing. Besides, the proposed solutions, which show the future directions of smart gas sensing, are explored. Finally, the recommendations for smart gas sensing based on brain-like sensing are provided in this paper.",21.0
31479500,Evaluation of parameters affecting performance and reliability of machine learning-based antibiotic susceptibility testing from whole genome sequencing data,2019 Sep 3;15(9):e1007349.,"Prediction of antibiotic resistance phenotypes from whole genome sequencing data by machine learning methods has been proposed as a promising platform for the development of sequence-based diagnostics. However, there has been no systematic evaluation of factors that may influence performance of such models, how they might apply to and vary across clinical populations, and what the implications might be in the clinical setting. Here, we performed a meta-analysis of seven large Neisseria gonorrhoeae datasets, as well as Klebsiella pneumoniae and Acinetobacter baumannii datasets, with whole genome sequence data and antibiotic susceptibility phenotypes using set covering machine classification, random forest classification, and random forest regression models to predict resistance phenotypes from genotype. We demonstrate how model performance varies by drug, dataset, resistance metric, and species, reflecting the complexities of generating clinically relevant conclusions from machine learning-derived models. Our findings underscore the importance of incorporating relevant biological and epidemiological knowledge into model design and assessment and suggest that doing so can inform tailored modeling for individual drugs, pathogens, and clinical populations. We further suggest that continued comprehensive sampling and incorporation of up-to-date whole genome sequence data, resistance phenotypes, and treatment outcome data into model training will be crucial to the clinical utility and sustainability of machine learning-based molecular diagnostics.",15.0
31478577,Machine learning applications in epilepsy,2019 Oct;60(10):2037-2047.,"Machine learning leverages statistical and computer science principles to develop algorithms capable of improving performance through interpretation of data rather than through explicit instructions. Alongside widespread use in image recognition, language processing, and data mining, machine learning techniques have received increasing attention in medical applications, ranging from automated imaging analysis to disease forecasting. This review examines the parallel progress made in epilepsy, highlighting applications in automated seizure detection from electroencephalography (EEG), video, and kinetic data, automated imaging analysis and pre-surgical planning, prediction of medication response, and prediction of medical and surgical outcomes using a wide variety of data sources. A brief overview of commonly used machine learning approaches, as well as challenges in further application of machine learning techniques in epilepsy, is also presented. With increasing computational capabilities, availability of effective machine learning algorithms, and accumulation of larger datasets, clinicians and researchers will increasingly benefit from familiarity with these techniques and the significant progress already made in their application in epilepsy.",13.0
31472738,The Evolving Use of Electronic Health Records (EHR) for Research,2019 Oct;29(4):354-361.,"Electronic health records (EHR) have been implemented successfully in a majority of United States healthcare systems in some form. There has been a rise in secondary uses of EHR, especially for research. EHR data is large, heterogenous, incomplete, noisy, and primarily created for purposes other than research. This presents many challenges, many of which are beginning to be overcome with the application of computer science artificial intelligence techniques, such as natural language processing and machine learning. EHR are gradually being redesigned to facilitate future research, though we are still far from a ""complete EHR.""",2.0
31472734,Use of Big Data for Quality Assurance in Radiation Therapy,2019 Oct;29(4):326-332.,"The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.",2.0
31468267,Current Controversies Concerning Capsule Endoscopy,2019 Nov;64(11):3040-3047.,"Video capsule endoscopy became a reality in 2001. This device enabled us to directly view the mucosa of the small intestine for the first time. The main indications for the video capsule remain the detection of small intestinal bleeding and iron deficiency anemia, diagnosis and management of Crohn's disease, and detection of tumors. The device is extraordinarily safe and can be used in the very young to the very old. However, there remain several areas of controversy and difficulty. These are covered in this article and include details of indications and contraindications, whether to prepare patients, whether or not to use simethicone and prokinetics. Detection of location of the capsule remains a major engineering challenge. Reading the videos reliably and quickly remains challenging. However, artificial intelligence and machine learning are already on the horizon to provide assistance. New uses for capsule endoscopy promise more accurate diagnosis and hence improved management of acute gastrointestinal bleeding. The colon capsule may eventually help those who refuse conventional colonoscopy, and robotically controlled capsules may be helpful in screening for serious disease in patients with upper abdominal complaints. The advent of the broadening use of video capsule endoscopy is, though it will be controversial, embraced by some and derided by others; such is the nature of technological development. In the long run, if the use of the video capsule, based on sound evidence-based studies, can be shown to improve the care of our patients and reduce the cost of health care, its use will continue to expand.",1.0
31466916,Looking beyond the hype: Applied AI and machine learning in translational medicine,2019 Sep;47:607-615.,"Big data problems are becoming more prevalent for laboratory scientists who look to make clinical impact. A large part of this is due to increased computing power, in parallel with new technologies for high quality data generation. Both new and old techniques of artificial intelligence (AI) and machine learning (ML) can now help increase the success of translational studies in three areas: drug discovery, imaging, and genomic medicine. However, ML technologies do not come without their limitations and shortcomings. Current technical limitations and other limitations including governance, reproducibility, and interpretation will be discussed in this article. Overcoming these limitations will enable ML methods to be more powerful for discovery and reduce ambiguity within translational medicine, allowing data-informed decision-making to deliver the next generation of diagnostics and therapeutics to patients quicker, at lowered costs, and at scale.",7.0
31465619,Machine learning and big data analytics in bipolar disorder: A position paper from the International Society for Bipolar Disorders Big Data Task Force,2019 Nov;21(7):582-594.,"Objectives:                    The International Society for Bipolar Disorders Big Data Task Force assembled leading researchers in the field of bipolar disorder (BD), machine learning, and big data with extensive experience to evaluate the rationale of machine learning and big data analytics strategies for BD.              Method:                    A task force was convened to examine and integrate findings from the scientific literature related to machine learning and big data based studies to clarify terminology and to describe challenges and potential applications in the field of BD. We also systematically searched PubMed, Embase, and Web of Science for articles published up to January 2019 that used machine learning in BD.              Results:                    The results suggested that big data analytics has the potential to provide risk calculators to aid in treatment decisions and predict clinical prognosis, including suicidality, for individual patients. This approach can advance diagnosis by enabling discovery of more relevant data-driven phenotypes, as well as by predicting transition to the disorder in high-risk unaffected subjects. We also discuss the most frequent challenges that big data analytics applications can face, such as heterogeneity, lack of external validation and replication of some studies, cost and non-stationary distribution of the data, and lack of appropriate funding.              Conclusion:                    Machine learning-based studies, including atheoretical data-driven big data approaches, provide an opportunity to more accurately detect those who are at risk, parse-relevant phenotypes as well as inform treatment selection and prognosis. However, several methodological challenges need to be addressed in order to translate research findings to clinical settings.",6.0
31463515,Challenges of big data integration in the life sciences,2019 Oct;411(26):6791-6800.,"Big data has been reported to be revolutionizing many areas of life, including science. It summarizes data that is unprecedentedly large, rapidly generated, heterogeneous, and hard to accurately interpret. This availability has also brought new challenges: How to properly annotate data to make it searchable? What are the legal and ethical hurdles when sharing data? How to store data securely, preventing loss and corruption? The life sciences are not the only disciplines that must align themselves with big data requirements to keep up with the latest developments. The large hadron collider, for instance, generates research data at a pace beyond any current biomedical research center. There are three recent major coinciding events that explain the emergence of big data in the context of research: the technological revolution for data generation, the development of tools for data analysis, and a conceptual change towards open science and data. The true potential of big data lies in pattern discovery in large datasets, as well as the formulation of new models and hypotheses. Confirmation of the existence of the Higgs boson, for instance, is one of the most recent triumphs of big data analysis in physics. Digital representations of biological systems have become more comprehensive. This, in combination with advances in machine learning, creates exciting new research possibilities. In this paper, we review the state of big data in bioanalytical research and provide an overview of the guidelines for its proper usage.",1.0
31463458,Artificial intelligence and machine learning in spine research,2019 Mar 5;2(1):e1044.,"Artificial intelligence (AI) and machine learning (ML) techniques are revolutionizing several industrial and research fields like computer vision, autonomous driving, natural language processing, and speech recognition. These novel tools are already having a major impact in radiology, diagnostics, and many other fields in which the availability of automated solution may benefit the accuracy and repeatability of the execution of critical tasks. In this narrative review, we first present a brief description of the various techniques that are being developed nowadays, with special focus on those used in spine research. Then, we describe the applications of AI and ML to problems related to the spine which have been published so far, including the localization of vertebrae and discs in radiological images, image segmentation, computer-aided diagnosis, prediction of clinical outcomes and complications, decision support systems, content-based image retrieval, biomechanics, and motion analysis. Finally, we briefly discuss major ethical issues related to the use of AI in healthcare, namely, accountability, risk of biased decisions as well as data privacy and security, which are nowadays being debated in the scientific community and by regulatory agencies.",10.0
31458044,Artificial Intelligence: The Future for Organic Chemistry?,2018 Oct 16;3(10):13263-13266.,"On the basis of a recent article ""Predicting reaction performance in C-N cross-coupling using machine learning"" that appeared in Science, we had decided to highlight the way forward for artificial intelligence in chemistry. Synthesis of molecules remains one of the most important challenges in organic chemistry, and the standard approach involved by a chemist to solve a problem is based on experience and constitutes a repetitive, time-consuming task, often resulting in nonoptimized solutions. Thus, considering the recent phenomenal progresses that have been made in machine learning, there is little doubt that these systems, once fully operational in organic chemistry, will dramatically speed up development of new drugs and will constitute the future of chemistry.",3.0
31456318,Imaging signatures of glioblastoma molecular characteristics: A radiogenomics review,2020 Jul;52(1):54-69.,"Over the past few decades, the advent and development of genomic assessment methods and computational approaches have raised the hopes for identifying therapeutic targets that may aid in the treatment of glioblastoma. However, the targeted therapies have barely been successful in their effort to cure glioblastoma patients, leaving them with a grim prognosis. Glioblastoma exhibits high heterogeneity, both spatially and temporally. The existence of different genetic subpopulations in glioblastoma allows this tumor to adapt itself to environmental forces. Therefore, patients with glioblastoma respond poorly to the prescribed therapies, as treatments are directed towards the whole tumor and not to the specific genetic subregions. Genomic alterations within the tumor develop distinct radiographic phenotypes. In this regard, MRI plays a key role in characterizing molecular signatures of glioblastoma, based on regional variations and phenotypic presentation of the tumor. Radiogenomics has emerged as a (relatively) new field of research to explore the connections between genetic alterations and imaging features. Radiogenomics offers numerous advantages, including noninvasive and global assessment of the tumor and its response to therapies. In this review, we summarize the potential role of radiogenomic techniques to stratify patients according to their specific tumor characteristics with the goal of designing patient-specific therapies. Level of Evidence: 5 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;52:54-69.",7.0
31454239,Programmable One-Pot Synthesis of Oligosaccharides,2020 Sep 1;59(34):3078-3088.,"Carbohydrates make up one of the four major classes of biomolecules, often conjugated with proteins as glycoproteins or with lipids as glycolipids, and participate in many important biochemical functions in living species. However, glycoproteins or glycolipids often exist as mixtures, and as a consequence, it is difficult to isolate individual glycoproteins or glycolipids as pure forms to understand the role carbohydrates play in the glycoconjugate. Currently, the only feasible way to obtain pure glycoconjugates is through synthesis, and of the many methods developed for the synthesis of oligosaccharides, those with automatic and programmable potential are considered to be more effective for addressing the issues of carbohydrate diversity and related functions. In this Perspective, we describe how data science, including algorithm and machine learning, can be used to assist the chemical synthesis of oligosaccharide in a programmable and one-pot manner and how the programmable method can be used to accelerate the construction of diverse oligosaccharides to facilitate our understanding of glycosylation in biology.",1.0
31453373,Putting the data before the algorithm in big data addressing personalized healthcare,2019 Aug 19;2:78.,"Technologies leveraging big data, including predictive algorithms and machine learning, are playing an increasingly important role in the delivery of healthcare. However, evidence indicates that such algorithms have the potential to worsen disparities currently intrinsic to the contemporary healthcare system, including racial biases. Blame for these deficiencies has often been placed on the algorithm-but the underlying training data bears greater responsibility for these errors, as biased outputs are inexorably produced by biased inputs. The utility, equity, and generalizability of predictive models depend on population-representative training data with robust feature sets. So while the conventional paradigm of big data is deductive in nature-clinical decision support-a future model harnesses the potential of big data for inductive reasoning. This may be conceptualized as clinical decision questioning, intended to liberate the human predictive process from preconceived lenses in data solicitation and/or interpretation. Efficacy, representativeness and generalizability are all heightened in this schema. Thus, the possible risks of biased big data arising from the inputs themselves must be acknowledged and addressed. Awareness of data deficiencies, structures for data inclusiveness, strategies for data sanitation, and mechanisms for data correction can help realize the potential of big data for a personalized medicine era. Applied deliberately, these considerations could help mitigate risks of perpetuation of health inequity amidst widespread adoption of novel applications of big data.",12.0
31451912,Artificial intelligence as an emerging technology in the current care of neurological disorders,2021 May;268(5):1623-1642.,"Background:                    Artificial intelligence (AI) has influenced all aspects of human life and neurology is no exception to this growing trend. The aim of this paper is to guide medical practitioners on the relevant aspects of artificial intelligence, i.e., machine learning, and deep learning, to review the development of technological advancement equipped with AI, and to elucidate how machine learning can revolutionize the management of neurological diseases. This review focuses on unsupervised aspects of machine learning, and how these aspects could be applied to precision neurology to improve patient outcomes. We have mentioned various forms of available AI, prior research, outcomes, benefits and limitations of AI, effective accessibility and future of AI, keeping the current burden of neurological disorders in mind.              Discussion:                    The smart device system to monitor tremors and to recognize its phenotypes for better outcomes of deep brain stimulation, applications evaluating fine motor functions, AI integrated electroencephalogram learning to diagnose epilepsy and psychological non-epileptic seizure, predict outcome of seizure surgeries, recognize patterns of autonomic instability to prevent sudden unexpected death in epilepsy (SUDEP), identify the pattern of complex algorithm in neuroimaging classifying cognitive impairment, differentiating and classifying concussion phenotypes, smartwatches monitoring atrial fibrillation to prevent strokes, and prediction of prognosis in dementia are unique examples of experimental utilizations of AI in the field of neurology. Though there are obvious limitations of AI, the general consensus among several nationwide studies is that this new technology has the ability to improve the prognosis of neurological disorders and as a result should become a staple in the medical community.              Conclusion:                    AI not only helps to analyze medical data in disease prevention, diagnosis, patient monitoring, and development of new protocols, but can also assist clinicians in dealing with voluminous data in a more accurate and efficient manner.",8.0
31451418,Multigene signatures of responses to chemotherapy derived by biochemically-inspired machine learning,Sep-Oct 2019;128(1-2):45-52.,"Pharmacogenomic responses to chemotherapy drugs can be modeled by supervised machine learning of expression and copy number of relevant gene combinations. Such biochemical evidence can form the basis of derived gene signatures using cell line data, which can subsequently be examined in patients that have been treated with the same drugs. These gene signatures typically contain elements of multiple biochemical pathways which together comprise multiple origins of drug resistance or sensitivity. The signatures can capture variation in these responses to the same drug among different patients.",2.0
31451330,"Artificial Intelligence in Nephrology: Core Concepts, Clinical Applications, and Perspectives",2019 Dec;74(6):803-810.,"Artificial intelligence is playing an increasingly important role in many fields of medicine, assisting physicians in most steps of patient management. In nephrology, artificial intelligence can already be used to improve clinical care, hemodialysis prescriptions, and follow-up of transplant recipients. However, many nephrologists are still unfamiliar with the basic principles of medical artificial intelligence. This review seeks to provide an overview of medical artificial intelligence relevant to the practicing nephrologist, in all fields of nephrology. We define the core concepts of artificial intelligence and machine learning and cover the basics of the functioning of neural networks and deep learning. We also discuss the most recent clinical applications of artificial intelligence in nephrology and medicine; as an example, we describe how artificial intelligence can predict the occurrence of progressive immunoglobulin A nephropathy. Finally, we consider the future of artificial intelligence in clinical nephrology and its impact on medical practice, and conclude with a discussion of the ethical issues that the use of artificial intelligence raises in terms of clinical decision making, physician-patient relationship, patient privacy, and data collection.",14.0
31450799,Cancer Diagnosis Using Deep Learning: A Bibliographic Review,2019 Aug 23;11(9):1235.,"In this paper, we first describe the basics of the field of cancer diagnosis, which includes steps of cancer diagnosis followed by the typical classification methods used by doctors, providing a historical idea of cancer classification techniques to the readers. These methods include Asymmetry, Border, Color and Diameter (ABCD) method, seven-point detection method, Menzies method, and pattern analysis. They are used regularly by doctors for cancer diagnosis, although they are not considered very efficient for obtaining better performance. Moreover, considering all types of audience, the basic evaluation criteria are also discussed. The criteria include the receiver operating characteristic curve (ROC curve), Area under the ROC curve (AUC), F1 score, accuracy, specificity, sensitivity, precision, dice-coefficient, average accuracy, and Jaccard index. Previously used methods are considered inefficient, asking for better and smarter methods for cancer diagnosis. Artificial intelligence and cancer diagnosis are gaining attention as a way to define better diagnostic tools. In particular, deep neural networks can be successfully used for intelligent image analysis. The basic framework of how this machine learning works on medical imaging is provided in this study, i.e., pre-processing, image segmentation and post-processing. The second part of this manuscript describes the different deep learning techniques, such as convolutional neural networks (CNNs), generative adversarial models (GANs), deep autoencoders (DANs), restricted Boltzmann's machine (RBM), stacked autoencoders (SAE), convolutional autoencoders (CAE), recurrent neural networks (RNNs), long short-term memory (LTSM), multi-scale convolutional neural network (M-CNN), multi-instance learning convolutional neural network (MIL-CNN). For each technique, we provide Python codes, to allow interested readers to experiment with the cited algorithms on their own diagnostic problems. The third part of this manuscript compiles the successfully applied deep learning models for different types of cancers. Considering the length of the manuscript, we restrict ourselves to the discussion of breast cancer, lung cancer, brain cancer, and skin cancer. The purpose of this bibliographic review is to provide researchers opting to work in implementing deep learning and artificial neural networks for cancer diagnosis a knowledge from scratch of the state-of-the-art achievements.",16.0
31447800,Machine Learning Approaches for Epidemiological Investigations of Food-Borne Disease Outbreaks,2019 Aug 6;10:1722.,"Foodborne diseases (FBDs) are infections of the gastrointestinal tract caused by foodborne pathogens (FBPs) such as bacteria [Salmonella, Listeria monocytogenes and Shiga toxin-producing E. coli (STEC)] and several viruses, but also parasites and some fungi. Artificial intelligence (AI) and its sub-discipline machine learning (ML) are re-emerging and gaining an ever increasing popularity in the scientific community and industry, and could lead to actionable knowledge in diverse ranges of sectors including epidemiological investigations of FBD outbreaks and antimicrobial resistance (AMR). As genotyping using whole-genome sequencing (WGS) is becoming more accessible and affordable, it is increasingly used as a routine tool for the detection of pathogens, and has the potential to differentiate between outbreak strains that are closely related, identify virulence/resistance genes and provide improved understanding of transmission events within hours to days. In most cases, the computational pipeline of WGS data analysis can be divided into four (though, not necessarily consecutive) major steps: de novo genome assembly, genome characterization, comparative genomics, and inference of phylogeny or phylogenomics. In each step, ML could be used to increase the speed and potentially the accuracy (provided increasing amounts of high-quality input data) of identification of the source of ongoing outbreaks, leading to more efficient treatment and prevention of additional cases. In this review, we explore whether ML or any other form of AI algorithms have already been proposed for the respective tasks and compare those with mechanistic model-based approaches.",3.0
31447631,Computational Methods for Resting-State EEG of Patients With Disorders of Consciousness,2019 Aug 6;13:807.,"Patients who survive brain injuries may develop Disorders of Consciousness (DOC) such as Coma, Vegetative State (VS) or Minimally Conscious State (MCS). Unfortunately, the rate of misdiagnosis between VS and MCS due to clinical judgment is high. Therefore, diagnostic decision support systems aiming to correct any differentiation between VS and MCS are essential for the characterization of an adequate treatment and an effective prognosis. In recent decades, there has been a growing interest in the new EEG computational techniques. We have reviewed how resting-state EEG is computationally analyzed to support differential diagnosis between VS and MCS in view of applicability of these methods in clinical practice. The studies available so far have used different techniques and analyses; it is therefore hard to draw general conclusions. Studies using a discriminant analysis with a combination of various factors and reporting a cut-off are among the most interesting ones for a future clinical application.",2.0
31446573,Eye Movements in Neuropsychological Tasks,2019;41:393-418.,"This chapter reviews how recording and analysis of eye movements have been applied to understanding cognitive functioning in patients with neurological disease. Measures derived from the performance of instructed eye movement tests such as the anti-saccade and memory-guided saccade tasks have been shown to be associated with cognitive test performance and the early stages of neurodegenerative disorders including Alzheimer's and Parkinson's disease. Other researchers have taken an ecological approach and recorded the uninstructed pattern of saccades made by patients during performance of established neuropsychological tasks. Studies that have analysed the eye movement strategies used in a number of widely used tests are reviewed, including the Corsi blocks, Tower of London, 'CANTAB' Spatial Working Memory and Brixton Spatial Anticipation test. The findings illustrate that eye movements are not purely in the service of vision, but support visuospatial working memory and forward action planning. Eye movement tests and measures also have potential for application in the assessment and diagnosis of neurological disease and cognitive impairment. Establishing large-scale normative data sets in healthy older adults and use of machine learning multivariate classifier algorithms may be key to further developing eye tracking applications in neuropsychological assessment.",1.0
31442779,Protein secondary structure prediction using neural networks and deep learning: A review,2019 Aug;81:1-8.,"Literature contains over fifty years of accumulated methods proposed by researchers for predicting the secondary structures of proteins in silico. A large part of this collection is comprised of artificial neural network-based approaches, a field of artificial intelligence and machine learning that is gaining increasing popularity in various application areas. The primary objective of this paper is to put together the summary of works that are important but sparse in time, to help new researchers have a clear view of the domain in a single place. An informative introduction to protein secondary structure and artificial neural networks is also included for context. This review will be valuable in designing future methods to improve protein secondary structure prediction accuracy. The various neural network methods found in this problem domain employ varying architectures and feature spaces, and a handful stand out due to significant improvements in prediction. Neural networks with larger feature scope and higher architecture complexity have been found to produce better protein secondary structure prediction. The current prediction accuracy lies around the 84% marks, leaving much room for further improvement in the prediction of secondary structures in silico. It was found that the estimated limit of 88% prediction accuracy has not been reached yet, hence further research is a timely demand.",6.0
31440497,Augmenting Basin-Hopping With Techniques From Unsupervised Machine Learning: Applications in Spectroscopy and Ion Mobility,2019 Aug 7;7:519.,"Evolutionary algorithms such as the basin-hopping (BH) algorithm have proven to be useful for difficult non-linear optimization problems with multiple modalities and variables. Applications of these algorithms range from characterization of molecular states in statistical physics and molecular biology to geometric packing problems. A key feature of BH is the fact that one can generate a coarse-grained mapping of a potential energy surface (PES) in terms of local minima. These results can then be utilized to gain insights into molecular dynamics and thermodynamic properties. Here we describe how one can employ concepts from unsupervised machine learning to augment BH PES searches to more efficiently identify local minima and the transition states connecting them. Specifically, we introduce the concepts of similarity indices, hierarchical clustering, and multidimensional scaling to the BH methodology. These same machine learning techniques can be used as tools for interpreting and rationalizing experimental results from spectroscopic and ion mobility investigations (e.g., spectral assignment, dynamic collision cross sections). We exemplify this in two case studies: (1) assigning the infrared multiple photon dissociation spectrum of the protonated serine dimer and (2) determining the temperature-dependent collision cross-section of protonated alanine tripeptide.",
31437875,Graft Rejection Prediction Following Kidney Transplantation Using Machine Learning Techniques: A Systematic Review and Meta-Analysis,2019 Aug 21;264:10-14.,"Kidney transplantation is recommended for patients with End-Stage Renal Disease (ESRD). However, complications, such as graft rejection are hard to predict due to donor and recipient variability. This study discusses the role of machine learning (ML) in predicting graft rejection following kidney transplantation, by reviewing the available related literature. PubMed, DBLP, and Scopus databases were searched to identify studies that utilized ML methods, in predicting outcome following kidney transplants. Fourteen studies were included. This study reviewed the deployment of ML in 109,317 kidney transplant patients from 14 studies. We extracted five different ML algorithms from reviewed studies. Decision Tree (DT) algorithms revealed slightly higher performance with overall mean Area Under the Curve (AUC) for DT (79.5% ± 0.06) was higher than Artificial Neural Network (ANN) (78.2% ± 0.08). For predicting graft rejection, ANN and DT were at the top among ML models that had higher accuracy and AUC.",
31437696,Can Machine Learning help us in dealing with treatment resistant depression? A review,2019 Dec 1;259:21-26.,"Background:                    About one third of patients treated with antidepressant do not show sufficient symptoms relief and up to 15% of patients remain symptomatic even after multiple trials are applied, configuring a state called treatment resistant depression (TRD). A clear definition of this state and the understanding of underlying mechanisms contributing to chronic disability caused by major depressive disorder is still unknown. Therefore, Machine Learning (ML) techniques emerged in the last years as interesting approaches to deal with such complex problems.              Methods:                    We performed a bibliographic search on Pubmed, Google Scholar and Medline of clinical, imaging, genetic and EEG ML classification studies on treatment-responding depression and TRD as well as studies trying to predict response to a specific treatment in already established TRD. The inclusion criteria were met by eleven studies. Seven focused on the definition of predictors of TRD onset while four attempted to predict the response to specific treatments in TRD.              Results:                    The results showed that it seems possible to classify between responders MDD and TRD with good accuracies based on clinical variables. Moreover, some studies reported the possibility of using EEG measures to predict response to different pharmacological and non-pharmacological treatments in established TRD.              Limitations:                    The definition of TRD, the selection of variables together with ML algorithms and pipelines varies across the studies, ultimately determining the unfeasibility to implement these models in clinical practice.              Conclusions:                    The findings suggest that ML could be a valid approach to increase our understanding of TRD and to better classify and stratify this disorder, which may ultimately help clinicians in the assessment of major depressive disorders.",5.0
31436850,Artificial intelligence: Implications for the future of work,2019 Nov;62(11):917-926.,"Artificial intelligence (AI) is a broad transdisciplinary field with roots in logic, statistics, cognitive psychology, decision theory, neuroscience, linguistics, cybernetics, and computer engineering. The modern field of AI began at a small summer workshop at Dartmouth College in 1956. Since then, AI applications made possible by machine learning (ML), an AI subdiscipline, include Internet searches, e-commerce sites, goods and services recommender systems, image and speech recognition, sensor technologies, robotic devices, and cognitive decision support systems (DSSs). As more applications are integrated into everyday life, AI is predicted to have a globally transformative influence on economic and social structures similar to the effect that other general-purpose technologies, such as steam engines, railroads, electricity, electronics, and the Internet, have had. Novel AI applications in the workplace of the future raise important issues for occupational safety and health. This commentary reviews the origins of AI, use of ML methods, and emerging AI applications embedded in physical objects like sensor technologies, robotic devices, or operationalized in intelligent DSSs. Selected implications on the future of work arising from the use of AI applications, including job displacement from automation and management of human-machine interactions, are also reviewed. Engaging in strategic foresight about AI workplace applications will shift occupational research and practice from a reactive posture to a proactive one. Understanding the possibilities and challenges of AI for the future of work will help mitigate the unfavorable effects of AI on worker safety, health, and well-being.",4.0
31435868,Deep Learning and Neurology: A Systematic Review,2019 Dec;8(2):351-365.,"Deciphering the massive volume of complex electronic data that has been compiled by hospital systems over the past decades has the potential to revolutionize modern medicine, as well as present significant challenges. Deep learning is uniquely suited to address these challenges, and recent advances in techniques and hardware have poised the field of medical machine learning for transformational growth. The clinical neurosciences are particularly well positioned to benefit from these advances given the subtle presentation of symptoms typical of neurologic disease. Here we review the various domains in which deep learning algorithms have already provided impetus for change-areas such as medical image analysis for the improved diagnosis of Alzheimer's disease and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of Alzheimer's, autism spectrum disorder, and attention deficit hyperactivity disorder; and mining of microscopic electroencephalogram signals and granular genetic signatures. We additionally note important challenges in the integration of deep learning tools in the clinical setting and discuss the barriers to tackling the challenges that currently exist.",11.0
31434208,Application of Artificial Intelligence in the Detection and Differentiation of Colon Polyps: A Technical Review for Physicians,2019 Aug 20;9(3):99.,"Research in computer-aided diagnosis (CAD) and the application of artificial intelligence (AI) in the endoscopic evaluation of the gastrointestinal tract is novel. Since colonoscopy and detection of polyps can decrease the risk of colon cancer, it is recommended by multiple national and international societies. However, the procedure of colonoscopy is performed by humans where there are significant interoperator and interpatient variations, and hence, the risk of missing detection of adenomatous polyps. Early studies involving CAD and AI for the detection and differentiation of polyps show great promise. In this appraisal, we review existing scientific aspects of AI in CAD of colon polyps and discuss the pitfalls and future directions for advancing the science. This review addresses the technical intricacies in a manner that physicians can comprehend to promote a better understanding of this novel application.",5.0
31434042,Ambulatory cardiac bio-signals: From mirage to clinical reality through a decade of progress,2019 Oct;130:103928.,"Background:                    Health monitoring is shifting towards continuous, ambulatory and clinically comparable wearable devices. Telemedicine and remote diagnosis could harness the capability of mobile cardiac health information, as the technology on bio-physical signal monitoring has improved significantly.              Objectives:                    The purpose of this review article is (1) to systematically assess the viability of ambulatory electrocardiography (ECG), (2) to provide a systems level understanding of a broad spectrum of wearable heart signal monitoring approaches and (3) to identify areas of improvement in the existing technology needed to attain clinical grade diagnosis.              Results:                    Based on the included literature, we have identified (1) that the developments in ECG monitoring through wearable devices are reaching feasibility, and are capable of delivering diagnostic and prognostic information, (2) that reliable sensing is the major bottleneck in the entire process of ambulatory monitoring, (3) that there is a strong need for artificial intelligence and machine learning techniques to parse and infer the biosignals and (4) that aspects of wearer comfort has largely been ignored in the prevailing developments, which can become a key factor for consumer acceptance.              Conclusions:                    Cardiac health information is crucial for diagnosis and prevention of several disease onsets. Mobile and continuous monitoring can aid avoiding risks involved with acute symptoms. The health information obtained through continuous monitoring can serve as the BigData of heart signals, and can facilitate new treatment methods and devise effective health policies.",1.0
31433750,Blood Brain Barrier Permeability Prediction Using Machine Learning Techniques: An Update,2019;20(14):1163-1171.,"Blood Brain Barrier (BBB) is the collection of vessels of blood with special properties of permeability that allow a limited range of drug and compounds to pass through it. The BBB plays a vital role in maintaining balance between intracellular and extracellular environment for brain. Brain Capillary Endothelial Cells (BECs) act as vehicle for transport and the transport mechanisms across BBB involve active and passive diffusion of compounds. Efficient prediction models of BBB permeability can be vital at the preliminary stages of drug development. There have been persistent efforts in identifying the prediction of BBB permeability of compounds employing multiple machine learning methods in an attempt to minimize the attrition rate of drug candidates taking up preclinical and clinical trials. However, there is an urgent need to review the progress of such machine learning derived prediction models in the prediction of BBB permeability. In the current article, we have analyzed the recently developed prediction model for BBB permeability using machine learning.",2.0
31431299,Designing Eukaryotic Gene Expression Regulation Using Machine Learning,2020 Feb;38(2):191-201.,"Controlling the expression of genes is one of the key challenges of synthetic biology. Until recently fine-tuned control has been out of reach, particularly in eukaryotes owing to their complexity of gene regulation. With advances in machine learning (ML) and in particular with increasing dataset sizes, models predicting gene expression levels from regulatory sequences can now be successfully constructed. Such models form the cornerstone of algorithms that allow users to design regulatory regions to achieve a specific gene expression level. In this review we discuss strategies for data collection, data encoding, ML practices, design algorithm choices, and finally model interpretation. Ultimately, these developments will provide synthetic biologists with highly specific genetic building blocks to rationally engineer complex pathways and circuits.",2.0
31429375,Improving PET Imaging Acquisition and Analysis With Machine Learning: A Narrative Review With Focus on Alzheimer's Disease and Oncology,Jan-Dec 2019;18:1536012119869070.,"Machine learning (ML) algorithms have found increasing utility in the medical imaging field and numerous applications in the analysis of digital biomarkers within positron emission tomography (PET) imaging have emerged. Interest in the use of artificial intelligence in PET imaging for the study of neurodegenerative diseases and oncology stems from the potential for such techniques to streamline decision support for physicians providing early and accurate diagnosis and allowing personalized treatment regimens. In this review, the use of ML to improve PET image acquisition and reconstruction is presented, along with an overview of its applications in the analysis of PET images for the study of Alzheimer's disease and oncology.",3.0
31427808,Do no harm: a roadmap for responsible machine learning for health care,2019 Sep;25(9):1337-1340.,"Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).",43.0
31426055,"Integrating sleep, neuroimaging, and computational approaches for precision psychiatry",2020 Jan;45(1):192-204.,"In advancing precision psychiatry, we focus on what imaging technology and computational approaches offer for the future of diagnostic subtyping and personalized tailoring of interventions for sleep impairment in mood and anxiety disorders. Current diagnostic criteria for mood and anxiety tend to lump different forms of sleep disturbance together. Parsing the biological features of sleep impairment and brain circuit dysfunction is one approach to identifying subtypes within these disorders that are mechanistically coherent and offer targets for intervention. We focus on two large-scale neural circuits implicated in sleep impairment and in mood and anxiety disorders: the default mode network and negative affective network. Through a synthesis of existing knowledge about these networks, we pose a testable framework for understanding how hyper- versus hypo-engagement of these networks may underlie distinct features of mood and sleep impairment. Within this framework we consider whether poor sleep quality may have an explanatory role in previously observed associations between network dysfunction and mood symptoms. We expand this framework to future directions including the potential for connecting circuit-defined subtypes to more distal features derived from digital phenotyping and wearable technologies, and how new discovery may be advanced through machine learning approaches.",5.0
31424335,A Roadmap for Automatic Surgical Site Infection Detection and Evaluation Using User-Generated Incision Images,2019 Oct;20(7):555-565.,"Background: Emerging technologies such as smartphones and wearable sensors have enabled the paradigm shift to new patient-centered healthcare, together with recent mobile health (mHealth) app development. One such promising healthcare app is incision monitoring based on patient-taken incision images. In this review, challenges and potential solution strategies are investigated for surgical site infection (SSI) detection and evaluation using surgical site images taken at home. Methods: Potential image quality issues, feature extraction, and surgical site image analysis challenges are discussed. Recent image analysis and machine learning solutions are reviewed to extract meaningful representations as image markers for incision monitoring. Discussions on opportunities and challenges of applying these methods to derive accurate SSI prediction are provided. Conclusions: Interactive image acquisition as well as customized image analysis and machine learning methods for SSI monitoring will play critical roles in developing sustainable mHealth apps to achieve the expected outcomes of patient-taken incision images for effective out-of-clinic patient-centered healthcare with substantially reduced cost.",1.0
31420515,"Closed-loop cycles of experiment design, execution, and learning accelerate systems biology model development in yeast",2019 Sep 3;116(36):18142-18147.,"One of the most challenging tasks in modern science is the development of systems biology models: Existing models are often very complex but generally have low predictive performance. The construction of high-fidelity models will require hundreds/thousands of cycles of model improvement, yet few current systems biology research studies complete even a single cycle. We combined multiple software tools with integrated laboratory robotics to execute three cycles of model improvement of the prototypical eukaryotic cellular transformation, the yeast (Saccharomyces cerevisiae) diauxic shift. In the first cycle, a model outperforming the best previous diauxic shift model was developed using bioinformatic and systems biology tools. In the second cycle, the model was further improved using automatically planned experiments. In the third cycle, hypothesis-led experiments improved the model to a greater extent than achieved using high-throughput experiments. All of the experiments were formalized and communicated to a cloud laboratory automation system (Eve) for automatic execution, and the results stored on the semantic web for reuse. The final model adds a substantial amount of knowledge about the yeast diauxic shift: 92 genes (+45%), and 1,048 interactions (+147%). This knowledge is also relevant to understanding cancer, the immune system, and aging. We conclude that systems biology software tools can be combined and integrated with laboratory robots in closed-loop cycles.",
31419831,Contributions from the 2018 Literature on Bioinformatics and Translational Informatics,2019 Aug;28(1):190-193.,"Objectives:                    To summarize recent research and select the best papers published in 2018 in the field of Bioinformatics and Translational Informatics (BTI) for the corresponding section of the International Medical Informatics Association (IMIA) Yearbook.              Methods:                    A literature review was performed for retrieving from PubMed papers indexed with keywords and free terms related to BTI. Independent review allowed the two section editors to select a list of 14 candidate best papers which were subsequently peer-reviewed. A final consensus meeting gathering the whole IMIA Yearbook editorial committee was organized to finally decide on the selection of the best papers.              Results:                    Among the 636 retrieved papers published in 2018 in the various subareas of BTI, the review process selected four best papers. The first paper presents a computational method to identify molecular markers for targeted treatment of acute myeloid leukemia using multi-omics data (genome-wide gene expression profiles) and in vitro sensitivity to 160 chemotherapy drugs. The second paper describes a deep neural network approach to predict the survival of patients suffering from glioma on the basis of digitalised pathology images and genomics biomarkers. The authors of the third paper adopt a pan-cancer approach to take benefit of multi-omics data for drug repurposing. The fourth paper presents a graph-based semi-supervised method to accurate phenotype classification applied to ovarian cancer.              Conclusions:                    Thanks to the normalization of open data and open science practices, research in BTI continues to develop and mature. Noteworthy achievements are sophisticated applications of leading edge machine-learning methods dedicated to personalized medicine.",1.0
31419823,"Advancing Artificial Intelligence in Sensors, Signals, and Imaging Informatics",2019 Aug;28(1):115-117.,"Objective:                    To identify research works that exemplify recent developments in the field of sensors, signals, and imaging informatics.              Method:                    A broad literature search was conducted using PubMed and Web of Science, supplemented with individual papers that were nominated by section editors. A predefined query made from a combination of Medical Subject Heading (MeSH) terms and keywords were used to search both sources. Section editors then filtered the entire set of retrieved papers with each paper having been reviewed by two section editors. Papers were assessed on a three-point Likert scale by two section editors, rated from 0 (do not include) to 2 (should be included). Only papers with a combined score of 2 or above were considered.              Results:                    A search for papers was executed at the start of January 2019, resulting in a combined set of 1,459 records published in 2018 in 119 unique journals. Section editors jointly filtered the list of candidates down to 14 nominations. The 14 candidate best papers were then ranked by a group of eight external reviewers. Four papers, representing different international groups and journals, were selected as the best papers by consensus of the International Medical Informatics Association (IMIA) Yearbook editorial board.              Conclusions:                    The fields of sensors, signals, and imaging informatics have rapidly evolved with the application of novel artificial intelligence/machine learning techniques. Studies have been able to discover hidden patterns and integrate different types of data towards improving diagnostic accuracy and patient outcomes. However, the quality of papers varied widely without clear reporting standards for these types of models. Nevertheless, a number of papers have demonstrated useful techniques to improve the generalizability, interpretability, and reproducibility of increasingly sophisticated models.",
31419822,A Broader Look: Camera-Based Vital Sign Estimation across the Spectrum,2019 Aug;28(1):102-114.,"Objectives:                    Camera-based vital sign estimation allows the contactless assessment of important physiological parameters. Seminal contributions were made in the 1930s, 1980s, and 2000s, and the speed of development seems ever increasing. In this suivey, we aim to overview the most recent works in this area, describe their common features as well as shortcomings, and highlight interesting ""outliers"".              Methods:                    We performed a comprehensive literature research and quantitative analysis of papers published between 2016 and 2018. Quantitative information about the number of subjects, studies with healthy volunteers vs. pathological conditions, public datasets, laboratory vs. real-world works, types of camera, usage of machine learning, and spectral properties of data was extracted. Moreover, a qualitative analysis of illumination used and recent advantages in terms of algorithmic developments was also performed.              Results:                    Since 2016, 116 papers were published on camera-based vital sign estimation and 59% of papers presented results on 20 or fewer subjects. While the average number of participants increased from 15.7 in 2016 to 22.9 in 2018, the vast majority of papers (n=100) were on healthy subjects. Four public datasets were used in 10 publications. We found 27 papers whose application scenario could be considered a real-world use case, such as monitoring during exercise or driving. These include 16 papers that dealt with non-healthy subjects. The majority of papers (n=61) presented results based on visual, red-green-blue (RGB) information, followed by RGB combined with other parts of the electromagnetic spectrum (n=18), and thermography only (n=12), while other works (n=25) used other mono- or polychromatic non-RGB data. Surprisingly, a minority of publications (n=39) made use of consumer-grade equipment. Lighting conditions were primarily uncontrolled or ambient. While some works focused on specialized aspects such as the removal of vital sign information from video streams to protect privacy or the influence of video compression, most algorithmic developments were related to three areas: region of interest selection, tracking, or extraction of a one-dimensional signal. Seven papers used deep learning techniques, 17 papers used other machine learning approaches, and 92 made no explicit use of machine learning.              Conclusion:                    Although some general trends and frequent shortcomings are obvious, the spectrum of publications related to camera-based vital sign estimation is broad. While many creative solutions and unique approaches exist, the lack of standardization hinders comparability of these techniques and of their performance. We believe that sharing algorithms and/ or datasets will alleviate this and would allow the application of newer techniques such as deep learning.",4.0
31419815,"Artificial Intelligence in Health in 2018: New Opportunities, Challenges, and Practical Implications",2019 Aug;28(1):52-54.,"Objective:                    To summarize significant research contributions to the field of artificial intelligence (AI) in health in 2018.              Methods:                    Ovid MEDLINE® and Web of Science® databases were searched to identify original research articles that were published in the English language during 2018 and presented advances in the science of AI applied in health. Queries employed Medical Subject Heading (MeSH®) terms and keywords representing AI methodologies and limited results to health applications. Section editors selected 15 best paper candidates that underwent peer review by internationally renowned domain experts. Final best papers were selected by the editorial board of the 2018 International Medical Informatics Association (IMIA) Yearbook.              Results:                    Database searches returned 1,480 unique publications. Best papers employed innovative AI techniques that incorporated domain knowledge or explored approaches to support distributed or federated learning. All top-ranked papers incorporated novel approaches to advance the science of AI in health and included rigorous evaluations of their methodologies.              Conclusions:                    Performance of state-of-the-art AI machine learning algorithms can be enhanced by approaches that employ a multidisciplinary biomedical informatics pipeline to incorporate domain knowledge and can overcome challenges such as sparse, missing, or inconsistent data. Innovative training heuristics and encryption techniques may support distributed learning with preservation of privacy.",
31419421,Scientific Authors in a Changing World of Scholarly Communication: What Does the Future Hold?,2020 Jan;133(1):26-31.,"Scholarly communication in science, technology, and medicine has been organized around journal-based scientific publishing for the past 350 years. Scientific publishing has unique business models and includes stakeholders with conflicting interests-publishers, funders, libraries, and scholars who create, curate, and consume the literature. Massive growth and change in scholarly communication, coinciding with digitalization, have amplified stresses inherent in traditional scientific publishing, as evidenced by overwhelmed editors and reviewers, increased retraction rates, emergence of pseudo-journals, strained library budgets, and debates about the metrics of academic recognition for scholarly achievements. Simultaneously, several open access models are gaining traction and online technologies offer opportunities to augment traditional tasks of scientific publishing, develop integrated discovery services, and establish global and equitable scholarly communication through crowdsourcing, software development, big data management, and machine learning. These rapidly evolving developments raise financial, legal, and ethical dilemmas that require solutions, while successful strategies are difficult to predict. Key challenges and trends are reviewed from the authors' perspective about how to engage the scholarly community in this multifaceted process.",3.0
31417996,Algorithms for immunochromatographic assay: review and impact on future application,2019 Sep 23;144(19):5659-5676.,"Lateral flow immunoassay (LFIA) is a critical choice for applications of point-of-care testing (POCT) in clinical and laboratory environments because of its excellent features and versatility. To obtain authentic values of analyte concentrations and reliable detection results, the relevant research has featured the application of a diversity of methods of mathematical analysis to technical analysis to allow for use with a small quantity of data. Accordingly, a number of signal and image processing strategies have also emerged for the application of gold immunochromatographic and fluorescent strips to improve sensitivity and overcome the limitations of correlative hardware systems. Instead of traditional methods to solve the problem, researchers nowadays are interested in machine learning and its more powerful variant, deep learning technology, for LFIA detection. This review emphasizes different models for the POCT of accurate labels as well as signal processing strategies that use artificial intelligence and machine learning. We focus on the analytical mechanism, procedural flow, and the results of the assay, and conclude by summarizing the advantages and limitations of each algorithm. We also discuss the potential for application of and directions of future research on LFIA technology when combined with Artificial Intelligence and deep learning.",4.0
31416598,Cardiac arrhythmia detection using deep learning: A review,Nov-Dec 2019;57S:S70-S74.,"Due to its simplicity and low cost, analyzing an electrocardiogram (ECG) is the most common technique for detecting cardiac arrhythmia. The massive amount of ECG data collected every day, in home and hospital, may preclude data review by human operators/technicians. Therefore, several methods are proposed for either fully automatic arrhythmia detection or event selection for further verification by human experts. Traditional machine learning approaches have made significant progress in the past years. However, those methods rely on hand-crafted feature extraction, which requires in-depth domain knowledge and preprocessing of the signal (e.g., beat detection). This, plus the high variability in wave morphology among patients and the presence of noise, make it challenging for computerized interpretation to achieve high accuracy. Recent advances in deep learning make it possible to perform automatic high-level feature extraction and classification. Therefore, deep learning approaches have gained interest in arrhythmia detection. In this work, we reviewed the recent advancement of deep learning methods for automatic arrhythmia detection. We summarized existing literature from five aspects: utilized dataset, application, type of input data, model architecture, and performance evaluation. We also reported limitations of reviewed papers and potential future opportunities.",3.0
31416540,Addressing challenges of quantitative methodologies and event interpretation in the study of atrial fibrillation,2019 Sep;178:113-122.,"Atrial fibrillation (AF) is the commonest arrhythmia, yet the mechanisms of its onset and persistence are incompletely known. Although techniques for quantitative assessment have been investigated, there have been few attempts to integrate this information to advance disease treatment protocols. In this review, key quantitative methods for AF analysis are described, and suggestions are provided for the coordination of the available information, and to develop foci and directions for future research efforts. Quantitative biologists may have an interest in this topic in order to develop machine learning and tools for arrhythmia characterization, but they may perhaps have a minimal background in the clinical methodology and in the types of observed events and mechanistic hypotheses that have thus far been developed. We attempt to address these issues via exploration of the published literature. Although no new data is presented in this review, examples are shown of current lines of investigation, and in particular, how electrogram analysis and whole-chamber quantitative modeling of the left atrium may be useful to characterize fibrillatory patterns of activity, so as to propose avenues for more efficacious acquisition and interpretation of AF data.",1.0
31414666,"Privacy-Preserving Methods for Feature Engineering Using Blockchain: Review, Evaluation, and Proof of Concept",2019 Aug 14;21(8):e13600.,"Background:                    The protection of private data is a key responsibility for research studies that collect identifiable information from study participants. Limiting the scope of data collection and preventing secondary use of the data are effective strategies for managing these risks. An ideal framework for data collection would incorporate feature engineering, a process where secondary features are derived from sensitive raw data in a secure environment without a trusted third party.              Objective:                    This study aimed to compare current approaches based on how they maintain data privacy and the practicality of their implementations. These approaches include traditional approaches that rely on trusted third parties, and cryptographic, secure hardware, and blockchain-based techniques.              Methods:                    A set of properties were defined for evaluating each approach. A qualitative comparison was presented based on these properties. The evaluation of each approach was framed with a use case of sharing geolocation data for biomedical research.              Results:                    We found that approaches that rely on a trusted third party for preserving participant privacy do not provide sufficiently strong guarantees that sensitive data will not be exposed in modern data ecosystems. Cryptographic techniques incorporate strong privacy-preserving paradigms but are appropriate only for select use cases or are currently limited because of computational complexity. Blockchain smart contracts alone are insufficient to provide data privacy because transactional data are public. Trusted execution environments (TEEs) may have hardware vulnerabilities and lack visibility into how data are processed. Hybrid approaches combining blockchain and cryptographic techniques or blockchain and TEEs provide promising frameworks for privacy preservation. For reference, we provide a software implementation where users can privately share features of their geolocation data using the hybrid approach combining blockchain with TEEs as a supplement.              Conclusions:                    Blockchain technology and smart contracts enable the development of new privacy-preserving feature engineering methods by obviating dependence on trusted parties and providing immutable, auditable data processing workflows. The overlap between blockchain and cryptographic techniques or blockchain and secure hardware technologies are promising fields for addressing important data privacy needs. Hybrid blockchain and TEE frameworks currently provide practical tools for implementing experimental privacy-preserving applications.",4.0
31413915,A fluorescent biosensor-based platform for the discovery of immunogenic cancer cell death inducers,2019 Apr 26;8(8):1606665.,"Systemic anticancer immunity can be reinstated via the induction of immunogenic cell death (ICD) in malignant cells. Thus, certain classes of cytotoxic compounds, for example, anthracyclines, oxaliplatin and taxanes are endowed with the capacity to act on cancer cells to ignite premortem stress pathways that lead to the surface exposure of calreticulin (CALR) and the cellular release of adenosine triphosphate, annexin A1, high mobility group B1 and type-1 interferons. Altogether, these alterations constitute the hallmarks of ICD. Here we report the design of a discovery pipeline for the identification of novel ICD inducers by means of a phenotypic screening platform. The use of fluorescent biosensors as proxies for the manifestation of ICD hallmarks has enabled the exploration of large collections of chemical compounds by automatized screening routines. Imaging-based assessment and phenotypic selection led to the identification of potential ICD inducers that could be validated further in vitro and in vivo, confirming that bona fide ICD inducers possess the capacity to induce immunological long-term memory and to confer resistance against rechallenge with syngeneic tumors. Machine learning algorithms analyzing the physicochemical properties of ICD inducers can assist in the preselection of compounds with potential ICD-stimulatory properties, further accelerating the screening efforts designed to develop new immunotherapeutic agents.",3.0
31411796,Big Data in sleep apnoea: Opportunities and challenges,2020 May;25(5):486-494.,"Sleep apnoea is now regarded as a highly prevalent systemic, multimorbid, chronic disease requiring a combination of long-term home-based treatments. Optimization of personalized treatment strategies requires accurate patient phenotyping. Data to describe the broad variety of phenotypes can come from electronic health records, health insurance claims, socio-economic administrative databases, environmental monitoring, social media, etc. Connected devices in and outside homes collect vast amount of data amassed in databases. All this contributes to 'Big Data' that, if used appropriately, has great potential for the benefit of health, well-being and therapeutics. Sleep apnoea is particularly well placed with regards to Big Data because the primary treatment is positive airway pressure (PAP). PAP devices, used every night over long periods by millions of patients across the world, generate an enormous amount of data. In this review, we discuss how different types of Big Data have, and could be, used to improve our understanding of sleep-disordered breathing, to identify undiagnosed sleep apnoea, to personalize treatment and to adapt health policies and better allocate resources. We discuss some of the challenges of Big Data including the need for appropriate data management, compilation and analysis techniques employing innovative statistical approaches alongside machine learning/artificial intelligence; closer collaboration between data scientists and physicians; and respect of the ethical and regulatory constraints of collecting and using Big Data. Lastly, we consider how Big Data can be used to overcome the limitations of randomized clinical trials and advance real-life evidence-based medicine for sleep apnoea.",1.0
31411491,Effects of Distance Measure Choice on K-Nearest Neighbor Classifier Performance: A Review,2019 Dec;7(4):221-248.,"The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested examples and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures available? This review attempts to answer this question through evaluating the performance (measured by accuracy, precision, and recall) of the KNN using a large number of distance measures, tested on a number of real-world data sets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances. We found that a recently proposed nonconvex distance performed the best when applied on most data sets comparing with the other tested distances. In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing with other distances.",3.0
31410728,The Future of Digital Psychiatry,2019 Aug 13;21(9):88.,"Purpose of review:                    Treatments in psychiatry have been rapidly changing over the last century, following the development of psychopharmacology and new research achievements. However, with advances in technology, the practice of psychiatry in the future will likely be influenced by new trends based on computerized approaches and digital communication. We examined four major areas that will probably impact on the clinical practice in the next few years: telepsychiatry; social media; mobile applications and internet of things; artificial intelligence; and machine learning.              Recent findings:                    Developments in these four areas will benefit patients throughout the journey of the illness, encompassing early diagnosis, even before the patients present to a clinician; personalized treatment on demand at anytime and anywhere; better prediction on patient outcomes; and even how mental illnesses are diagnosed in the future. Though the evidence for many technology-based interventions or mobile applications is still insufficient, it is likely that such advances in technology will play a larger role in the way that patient receives mental health interventions in the future, leading to easier access to them and improved outcomes.",3.0
33324889,Computer-aided imaging analysis in acute ischemic stroke - background and clinical applications,2019 Aug 15;1:23.,"Tools for medical image analysis have been developed to reduce the time needed to detect abnormalities and to provide more accurate results. Particularly, tools based on artificial intelligence and machine learning techniques have led to significant improvements in medical imaging interpretation in the last decade. Automatic evaluation of acute ischemic stroke in medical imaging is one of the fields that witnessed a major development. Commercially available products so far aim to identify (and quantify) the ischemic core, the ischemic penumbra, the site of arterial occlusion and the collateral flow but they are not (yet) intended as standalone diagnostic tools. Their use can be complementary; they are intended to support physicians' interpretation of medical images and hence standardise selection of patients for acute treatment. This review provides an introduction into the field of computer-aided diagnosis and focuses on the automatic analysis of non-contrast-enhanced computed tomography, computed tomography angiography and perfusion imaging. Future studies are necessary that allow the evaluation and comparison of different imaging strategies and post-processing algorithms during the diagnosis process in patients with suspected acute ischemic stroke; which may further facilitate the standardisation of treatment and stroke management.",4.0
31408024,Primer on machine learning: utilization of large data set analyses to individualize pain management,2019 Oct;32(5):653-660.,"Purpose of review:                    Pain researchers and clinicians increasingly encounter machine learning algorithms in both research methods and clinical practice. This review provides a summary of key machine learning principles, as well as applications to both structured and unstructured datasets.              Recent findings:                    Aside from increasing use in the analysis of electronic health record data, machine and deep learning algorithms are now key tools in the analyses of neuroimaging and facial expression recognition data used in pain research.              Summary:                    In the coming years, machine learning is likely to become a key component of evidence-based medicine, yet will require additional skills and perspectives for its successful and ethical use in research and clinical settings.",1.0
31401617,Intelligent Imaging: Anatomy of Machine Learning and Deep Learning,2019 Dec;47(4):273-281.,"The emergence of artificial intelligence (AI) in nuclear medicine and radiology has been accompanied by AI commentators and experts predicting that AI would make radiologists, in particular, extinct. More realistic perspectives suggest significant changes will occur in medical practice. There is no escaping the disruptive technology associated with AI, neural networks, and deep learning, the most significant perhaps since the early days of Roentgen, Becquerel, and Curie. AI is an omen, but it need not be foreshadowing a negative event; rather, it is heralding great opportunity. The key to sustainability lies not in resisting AI but in having a deep understanding and exploiting the capabilities of AI in nuclear medicine while mastering those capabilities unique to the human resources.",2.0
31401616,Intelligent Imaging: Artificial Intelligence Augmented Nuclear Medicine,2019 Sep;47(3):217-222.,"Artificial intelligence (AI) in nuclear medicine and radiology represents a significant disruptive technology. Although there has been much debate about the impact of AI on the careers of radiologists, the opportunities in nuclear medicine enhance the capability of the physician and at the same time have an impact on the responsibilities of physicists and technologists. This transformative technology requires insight into the principles and opportunities for seamless assimilation into practice without the associated displacement of human resources. This article introduces the current clinical applications of machine learning and deep learning.",1.0
31399886,Artificial intelligence in cardiovascular imaging: state of the art and implications for the imaging cardiologist,2019 Sep;27(9):403-413.,"Healthcare, conceivably more than any other area of human endeavour, has the greatest potential to be affected by artificial intelligence (AI). This potential has been shown by several reports that demonstrate equal or superhuman performance in medical tasks that aim to improve efficiency, diagnosis and prognosis. This review focuses on the state of the art of AI applications in cardiovascular imaging. It provides an overview of the current applications and studies performed, including the potential value, implications, limitations and future directions of AI in cardiovascular imaging.It is envisioned that AI will dramatically change the way doctors practise medicine. In the short term, it will assist physicians with easy tasks, such as automating measurements, making predictions based on big data, and putting clinical findings into an evidence-based context. In the long term, AI will not only assist doctors, it has the potential to significantly improve access to health and well-being data for patients and their caretakers. This empowers patients. From a physician's perspective, reliable AI assistance will be available to support clinical decision-making. Although cardiovascular studies implementing AI are increasing in number, the applications have only just started to penetrate contemporary clinical care.",11.0
31399699,Artificial intelligence in digital pathology - new tools for diagnosis and precision oncology,2019 Nov;16(11):703-715.,"In the past decade, advances in precision oncology have resulted in an increased demand for predictive assays that enable the selection and stratification of patients for treatment. The enormous divergence of signalling and transcriptional networks mediating the crosstalk between cancer, stromal and immune cells complicates the development of functionally relevant biomarkers based on a single gene or protein. However, the result of these complex processes can be uniquely captured in the morphometric features of stained tissue specimens. The possibility of digitizing whole-slide images of tissue has led to the advent of artificial intelligence (AI) and machine learning tools in digital pathology, which enable mining of subvisual morphometric phenotypes and might, ultimately, improve patient management. In this Perspective, we critically evaluate various AI-based computational approaches for digital pathology, focusing on deep neural networks and 'hand-crafted' feature-based methodologies. We aim to provide a broad framework for incorporating AI and machine learning tools into clinical oncology, with an emphasis on biomarker development. We discuss some of the challenges relating to the use of AI, including the need for well-curated validation datasets, regulatory approval and fair reimbursement strategies. Finally, we present potential future opportunities for precision oncology.",70.0
31396720,Advanced Imaging Modalities to Monitor for Cardiotoxicity,2019 Aug 8;20(9):73.,"Early detection and treatment of cardiotoxicity from cancer therapies is key to preventing a rise in adverse cardiovascular outcomes in cancer patients. Over-diagnosis of cardiotoxicity in this context is however equally hazardous, leading to patients receiving suboptimal cancer treatment, thereby impacting cancer outcomes. Accurate screening therefore depends on the widespread availability of sensitive and reproducible biomarkers of cardiotoxicity, which can clearly discriminate early disease. Blood biomarkers are limited in cardiovascular disease and clinicians generally still use generic screening with ejection fraction, based on historical local expertise and resources. Recently, however, there has been growing recognition that simple measurement of left ventricular ejection fraction using 2D echocardiography may not be optimal for screening: diagnostic accuracy, reproducibility and feasibility are limited. Modern cancer therapies affect many myocardial pathways: inflammatory, fibrotic, metabolic, vascular and myocyte function, meaning that multiple biomarkers may be needed to track myocardial cardiotoxicity. Advanced imaging modalities including cardiovascular magnetic resonance (CMR), computed tomography (CT) and positron emission tomography (PET) add improved sensitivity and insights into the underlying pathophysiology, as well as the ability to screen for other cardiotoxicities including coronary artery, valve and pericardial diseases resulting from cancer treatment. Delivering screening for cardiotoxicity using advanced imaging modalities will however require a significant change in current clinical pathways, with incorporation of machine learning algorithms into imaging analysis fundamental to improving efficiency and precision. In the future, we should aspire to personalized rather than generic screening, based on a patient's individual risk factors and the pathophysiological mechanisms of the cancer treatment they are receiving. We should aspire that progress in cardiooncology is able to track progress in oncology, and to ensure that the current 'one size fits all' approach to screening be obsolete in the very near future.",4.0
31396245,Hydroponic Solutions for Soilless Production Systems: Issues and Opportunities in a Smart Agriculture Perspective,2019 Jul 24;10:923.,"Soilless cultivation represent a valid opportunity for the agricultural production sector, especially in areas characterized by severe soil degradation and limited water availability. Furthermore, this agronomic practice embodies a favorable response toward an environment-friendly agriculture and a promising tool in the vision of a general challenge in terms of food security. This review aims therefore at unraveling limitations and opportunities of hydroponic solutions used in soilless cropping systems focusing on the plant mineral nutrition process. In particular, this review provides information (1) on the processes and mechanisms occurring in the hydroponic solutions that ensure an adequate nutrient concentration and thus an optimal nutrient acquisition without leading to nutritional disorders influencing ultimately also crop quality (e.g., solubilization/precipitation of nutrients/elements in the hydroponic solution, substrate specificity in the nutrient uptake process, nutrient competition/antagonism and interactions among nutrients); (2) on new emerging technologies that might improve the management of soilless cropping systems such as the use of nanoparticles and beneficial microorganism like plant growth-promoting rhizobacteria (PGPRs); (3) on tools (multi-element sensors and interpretation algorithms based on machine learning logics to analyze such data) that might be exploited in a smart agriculture approach to monitor the availability of nutrients/elements in the hydroponic solution and to modify its composition in realtime. These aspects are discussed considering what has been recently demonstrated at the scientific level and applied in the industrial context.",5.0
31395609,Use of Natural Language Processing to Extract Clinical Cancer Phenotypes from Electronic Medical Records,2019 Nov 1;79(21):5463-5470.,"Current models for correlating electronic medical records with -omics data largely ignore clinical text, which is an important source of phenotype information for patients with cancer. This data convergence has the potential to reveal new insights about cancer initiation, progression, metastasis, and response to treatment. Insights from this real-world data will catalyze clinical care, research, and regulatory activities. Natural language processing (NLP) methods are needed to extract these rich cancer phenotypes from clinical text. Here, we review the advances of NLP and information extraction methods relevant to oncology based on publications from PubMed as well as NLP and machine learning conference proceedings in the last 3 years. Given the interdisciplinary nature of the fields of oncology and information extraction, this analysis serves as a critical trail marker on the path to higher fidelity oncology phenotypes from real-world data.",9.0
31394043,"Deep Learning: The Good, the Bad, and the Ugly",2019 Sep 15;5:399-426.,Artificial vision has often been described as one of the key remaining challenges to be solved before machines can act intelligently. Recent developments in a branch of machine learning known as deep learning have catalyzed impressive gains in machine vision-giving a sense that the problem of vision is getting closer to being solved. The goal of this review is to provide a comprehensive overview of recent deep learning developments and to critically assess actual progress toward achieving human-level visual intelligence. I discuss the implications of the successes and limitations of modern machine vision algorithms for biological vision and the prospect for neuroscience to inform the design of future artificial vision systems.,16.0
31392526,Machine learning applications in prostate cancer magnetic resonance imaging,2019 Aug 7;3(1):35.,"With this review, we aimed to provide a synopsis of recently proposed applications of machine learning (ML) in radiology focusing on prostate magnetic resonance imaging (MRI). After defining the difference between ML and classical rule-based algorithms and the distinction among supervised, unsupervised and reinforcement learning, we explain the characteristic of deep learning (DL), a particular new type of ML, including its structure mimicking human neural networks and its 'black box' nature. Differences in the pipeline for applying ML and DL to prostate MRI are highlighted. The following potential clinical applications in different settings are outlined, many of them based only on MRI-unenhanced sequences: gland segmentation; assessment of lesion aggressiveness to distinguish between clinically significant and indolent cancers, allowing for active surveillance; cancer detection/diagnosis and localisation (transition versus peripheral zone, use of prostate imaging reporting and data system (PI-RADS) version 2), reading reproducibility, differentiation of cancers from prostatitis benign hyperplasia; local staging and pre-treatment assessment (detection of extraprostatic disease extension, planning of radiation therapy); and prediction of biochemical recurrence. Results are promising, but clinical applicability still requires more robust validation across scanner vendors, field strengths and institutions.",12.0
31390781,"Retrotransposons in Plant Genomes: Structure, Identification, and Classification through Bioinformatics and Machine Learning",2019 Aug 6;20(15):3837.,"Transposable elements (TEs) are genomic units able to move within the genome of virtually all organisms. Due to their natural repetitive numbers and their high structural diversity, the identification and classification of TEs remain a challenge in sequenced genomes. Although TEs were initially regarded as ""junk DNA"", it has been demonstrated that they play key roles in chromosome structures, gene expression, and regulation, as well as adaptation and evolution. A highly reliable annotation of these elements is, therefore, crucial to better understand genome functions and their evolution. To date, much bioinformatics software has been developed to address TE detection and classification processes, but many problematic aspects remain, such as the reliability, precision, and speed of the analyses. Machine learning and deep learning are algorithms that can make automatic predictions and decisions in a wide variety of scientific applications. They have been tested in bioinformatics and, more specifically for TEs, classification with encouraging results. In this review, we will discuss important aspects of TEs, such as their structure, importance in the evolution and architecture of the host, and their current classifications and nomenclatures. We will also address current methods and their limitations in identifying and classifying TEs.",7.0
31388566,The medical AI insurgency: what physicians must know about data to practice with intelligent machines,2019 Jun 28;2:62.,"Machine learning (ML) and its parent technology trend, artificial intelligence (AI), are deriving novel insights from ever larger and more complex datasets. Efficient and accurate AI analytics require fastidious data science-the careful curating of knowledge representations in databases, decomposition of data matrices to reduce dimensionality, and preprocessing of datasets to mitigate the confounding effects of messy (i.e., missing, redundant, and outlier) data. Messier, bigger and more dynamic medical datasets create the potential for ML computing systems querying databases to draw erroneous data inferences, portending real-world human health consequences. High-dimensional medical datasets can be static or dynamic. For example, principal component analysis (PCA) used within R computing packages can speed & scale disease association analytics for deriving polygenic risk scores from static gene-expression microarrays. Robust PCA of k-dimensional subspace data accelerates image acquisition and reconstruction of dynamic 4-D magnetic resonance imaging studies, enhancing tracking of organ physiology, tissue relaxation parameters, and contrast agent effects. Unlike other data-dense business and scientific sectors, medical AI users must be aware that input data quality limitations can have health implications, potentially reducing analytic model accuracy for predicting clinical disease risks and patient outcomes. As AI technologies find more health applications, physicians should contribute their health domain expertize to rules-/ML-based computer system development, inform input data provenance and recognize the importance of data preprocessing quality assurance before interpreting the clinical implications of intelligent machine outputs to patients.",1.0
31388413,Radiomics and Artificial Intelligence for Biomarker and Prediction Model Development in Oncology,2019 Jul 12;17:995-1008.,Unlabelled Image.,18.0
31387067,"Quantitative and qualitative phenotyping of disease resistance of crops by hyperspectral sensors: seamless interlocking of phytopathology, sensors, and machine learning is needed!",2019 Aug;50:156-162.,"Determination and characterization of resistance reactions of crops against fungal pathogens are essential to select resistant genotypes. In plant breeding, phenotyping of genotypes is realized by time consuming and expensive visual plant ratings. During resistance reactions and during pathogenesis plants initiate different structural and biochemical defence mechanisms, which partly affect the optical properties of plant organs. Recently, intensive research has been conducted to develop innovative optical methods for an assessment of compatible and incompatible plant pathogen interaction. These approaches, combining classical phytopathology or microbiology with technology driven methods - such as sensors, robotics, machine learning, and artificial intelligence - are summarized by the term digital phenotyping. In contrast to common visual rating, detection and assessment methods, optical sensors in combination with advanced data analysis methods are able to retrieve pathogen induced changes in the physiology of susceptible or resistant plants non-invasively and objectively. Phenotyping disease resistance aims different tasks. In an early breeding step, a qualitative assessment and characterization of specific resistance action is aimed to link it, for example, to a genetic marker. Later, during greenhouse and field screening, the assessment of the level of susceptibility of different genotypes is relevant. Within this review, recent advances of digital phenotyping technologies for the detection of subtle resistance reactions and resistance breeding are highlighted and methodological requirements are critically discussed.",7.0
31386605,Data-Driven Approaches to Understanding Visual Neuron Activity,2019 Sep 15;5:451-477.,"With modern neurophysiological methods able to record neural activity throughout the visual pathway in the context of arbitrarily complex visual stimulation, our understanding of visual system function is becoming limited by the available models of visual neurons that can be directly related to such data. Different forms of statistical models are now being used to probe the cellular and circuit mechanisms shaping neural activity, understand how neural selectivity to complex visual features is computed, and derive the ways in which neurons contribute to systems-level visual processing. However, models that are able to more accurately reproduce observed neural activity often defy simple interpretations. As a result, rather than being used solely to connect with existing theories of visual processing, statistical modeling will increasingly drive the evolution of more sophisticated theories.",
31386306,Improving the Rate of Translation of Tissue Engineering Products,2019 Oct;8(19):e1900538.,"Over 100 000 research articles and 9000 patents have been published on tissue engineering (TE) in the past 20 years. Yet, very few TE products have made their way to the market during the same period. Experts have proposed a variety of strategies to address the lack of translation of TE products. However, since these proposals are guided by qualitative insights, they are limited in scope and impact. Machine learning is utilized in the current study to analyze the entire body of patents that have been published over the past twenty years and understand patenting trends, topics, areas of application, and exemplifications. This analysis yields surprising and little-known insights about the differences in research priorities and perceptions of innovativeness of tissue engineers in academia and industry, as well as aids to chart true advances in the field during the past twenty years. It is hoped that this analysis and subsequent proposal to improve translational rates of TE products will spur much needed dialogue about this important pursuit.",3.0
