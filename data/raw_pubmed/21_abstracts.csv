pmid,citations,title,date,text
30439700,2.0,Modern Information Technology for Cancer Research: What's in IT for Me? An Overview of Technologies and Approaches,2020;98(6):363-369.,"Information technology (IT) can enhance or change many scenarios in cancer research for the better. In this paper, we introduce several examples, starting with clinical data reuse and collaboration including data sharing in research networks. Key challenges are semantic interoperability and data access (including data privacy). We deal with gathering and analyzing genomic information, where cloud computing, uncertainties and reproducibility challenge researchers. Also, new sources for additional phenotypical data are shown in patient-reported outcome and machine learning in imaging. Last, we focus on therapy assistance, introducing tools used in molecular tumor boards and techniques for computer-assisted surgery. We discuss the need for metadata to aggregate and analyze data sets reliably. We conclude with an outlook towards a learning health care system in oncology, which connects bench and bedside by employing modern IT solutions."
30430428,17.0,Radiomics and liquid biopsy in oncology: the holons of systems medicine,2018 Dec;9(6):915-924.,"Radiomics is a process of extraction and analysis of quantitative features from diagnostic images. Liquid biopsy is a test done on a sample of blood to look for cancer cells or for pieces of tumourigenic DNA circulating in the blood. Radiomics and liquid biopsy have great potential in oncology, since both are minimally invasive, easy to perform, and can be repeated in patient follow-up visits, enabling the extraction of valuable information regarding tumour type, aggressiveness, progression, and response to treatment. Both methods are in their infancy, with major evidence of application in lung and gastrointestinal cancer, while still undergoing evaluation in other cancer types. In this paper, the main oncologic applications of radiomics and liquid biopsy are reviewed, and a synergistic approach incorporating both tests for cancer diagnosis and follow-up is discussed within the context of systems medicine. TEACHING POINTS: • Radiomics is a process of extraction and analysis of quantitative features from diagnostic images. • Most clinical applications of radiomics are in the field of oncologic imaging. • Radiomics applies to all imaging modalities. • A cluster of radiomic features is a ""radiomic signature"". • Machine learning may improve the efficacy of radiomics analysis."
30421670,,Machine Learning Methods in Precision Medicine Targeting Epigenetic Diseases,2018;24(34):3998-4006.,"Background:                    On a tide of big data, machine learning is coming to its day. Referring to huge amounts of epigenetic data coming from biological experiments and clinic, machine learning can help in detecting epigenetic features in genome, finding correlations between phenotypes and modifications in histone or genes, accelerating the screen of lead compounds targeting epigenetics diseases and many other aspects around the study on epigenetics, which consequently realizes the hope of precision medicine.              Methods:                    In this minireview, we will focus on reviewing the fundamentals and applications of machine learning methods which are regularly used in epigenetics filed and explain their features. Their advantages and disadvantages will also be discussed.              Results:                    Machine learning algorithms have accelerated studies in precision medicine targeting epigenetics diseases.              Conclusion:                    In order to make full use of machine learning algorithms, one should get familiar with the pros and cons of them, which will benefit from big data by choosing the most suitable method(s)."
30420264,3.0,Prognostic models in primary biliary cholangitis,2018 Dec;95:171-178.,"Risk prediction modelling is important to better understand the determinants of the course and outcome of PBC and to inform the risk across the disease continuum in PBC enabling risk-stratified follow-up care and personalised therapy. Current prognostic models in PBC are based on treatment response to ursodeoxycholic acid because of the well-established relationship between alkaline phosphatase on treatment and long-term outcome. In addition, serum alkaline phosphatase correlates with ductular reaction and biliary metaplasia, which are hallmark of biliary injury. Considering the waiting time for treatment failure in high-risk patients is not inconsequential, efforts are focused on bringing forward risk stratification at diagnosis by predicting treatment response at onset. There is a need for better prognostic variables that are central to the disease process. We should take an integrative approach that incorporates multiple layers of information including genetic and environmental influences, host characteristics, clinical data, and molecular alterations for risk assessments. Biomarker discovery has an accelerated pace taking advantage of the emergence of large-scale omics platforms (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and others) and whole-genome sequencing. In the digital era, applications of artificial intelligence, such as machine learning, can support the computing power required to analyse the vast amount of data produced by omics. The information is then used for the development of personalised risk prediction models that through clinical trials and hopefully industry partnerships can guide risk management strategies. We are facing an unprecedented opportunity for the integration of molecular diagnostics into the clinic, which promotes progress toward the personalised management of patients with PBC."
30420244,12.0,Computational methods for Gene Regulatory Networks reconstruction and analysis: A review,2019 Apr;95:133-145.,"In the recent years, the vast amount of genetic information generated by new-generation approaches, have led to the need of new data handling methods. The integrative analysis of diverse-nature gene information could provide a much-sought overview to study complex biological systems and processes. In this sense, Gene Regulatory Networks (GRN) arise as an increasingly-promising tool for the modelling and analysis of biological processes. This review is an attempt to summarize the state of the art in the field of GRNs. Essential points in the field are addressed, thereof: (a) the type of data used for network generation, (b) machine learning methods and tools used for network generation, (c) model optimization and (d) computational approaches used for network validation. This survey is intended to provide an overview of the subject for readers to improve their knowledge in the field of GRN for future research."
30418085,8.0,Systems Biology and Machine Learning in Plant-Pathogen Interactions,2019 Jan;32(1):45-55.,"Systems biology is an inclusive approach to study the static and dynamic emergent properties on a global scale by integrating multiomics datasets to establish qualitative and quantitative associations among multiple biological components. With an abundance of improved high throughput -omics datasets, network-based analyses and machine learning technologies are playing a pivotal role in comprehensive understanding of biological systems. Network topological features reveal most important nodes within a network as well as prioritize significant molecular components for diverse biological networks, including coexpression, protein-protein interaction, and gene regulatory networks. Machine learning techniques provide enormous predictive power through specific feature extraction from biological data. Deep learning, a subtype of machine learning, has plausible future applications because a domain expert for feature extraction is not needed in this algorithm. Inspired by diverse domains of biology, we here review classic systems biology techniques applied in plant immunity thus far. We also discuss additional advanced approaches in both graph theory and machine learning, which may provide new insights for understanding plant-microbe interactions. Finally, we propose a hybrid approach in plant immune systems that harnesses the power of both network biology and machine learning, with a potential to be applicable to both model systems and agronomically important crop plants."
30417778,3.0,Convolutional Neural Networks for ATC Classification,2018;24(34):4007-4012.,"Background:                    Anatomical Therapeutic Chemical (ATC) classification of unknown compound has raised high significance for both drug development and basic research. The ATC system is a multi-label classification system proposed by the World Health Organization (WHO), which categorizes drugs into classes according to their therapeutic effects and characteristics. This system comprises five levels and includes several classes in each level; the first level includes 14 main overlapping classes. The ATC classification system simultaneously considers anatomical distribution, therapeutic effects, and chemical characteristics, the prediction for an unknown compound of its ATC classes is an essential problem, since such a prediction could be used to deduce not only a compound's possible active ingredients but also its therapeutic, pharmacological, and chemical properties. Nevertheless, the problem of automatic prediction is very challenging due to the high variability of the samples and the presence of overlapping among classes, resulting in multiple predictions and making machine learning extremely difficult.              Methods:                    In this paper, we propose a multi-label classifier system based on deep learned features to infer the ATC classification. The system is based on a 2D representation of the samples: first a 1D feature vector is obtained extracting information about a compound's chemical-chemical interaction and its structural and fingerprint similarities to other compounds belonging to the different ATC classes, then the original 1D feature vector is reshaped to obtain a 2D matrix representation of the compound. Finally, a convolutional neural network (CNN) is trained and used as a feature extractor. Two general purpose classifiers designed for multi-label classification are trained using the deep learned features and resulting scores are fused by the average rule.              Results:                    Experimental evaluation based on rigorous cross-validation demonstrates the superior prediction quality of this method compared to other state-of-the-art approaches developed for this problem.              Conclusion:                    Extensive experiments demonstrate that the new predictor, based on CNN, outperforms other existing predictors in the literature in almost all the five metrics used to examine the performance for multi-label systems, particularly in the ""absolute true"" rate and the ""absolute false"" rate, the two most significant indexes. Matlab code will be available at https://github.com/LorisNanni."
30417258,9.0,Applying machine learning to continuously monitored physiological data,2019 Oct;33(5):887-893.,"The use of machine learning (ML) in healthcare has enormous potential for improving disease detection, clinical decision support, and workflow efficiencies. In this commentary, we review published and potential applications for the use of ML for monitoring within the hospital environment. We present use cases as well as several questions regarding the application of ML to the analysis of the vast amount of complex data that clinicians must interpret in the realm of continuous physiological monitoring. ML, especially employed in bidirectional conjunction with electronic health record data, has the potential to extract much more useful information out of this currently under-analyzed data source from a population level. As a data driven entity, ML is dependent on copious, high quality input data so that error can be introduced by low quality data sources. At present, while ML is being studied in hybrid formulations along with static expert systems for monitoring applications, it is not yet actively incorporated in the formal artificial learning sense of an algorithm constantly learning and updating its rules without external intervention. Finally, innovations in monitoring, including those supported by ML, will pose regulatory and medico-legal challenges, as well as questions regarding precisely how to incorporate these features into clinical care and medical education. Rigorous evaluation of ML techniques compared to traditional methods or other AI methods will be required to validate the algorithms developed with consideration of database limitations and potential learning errors. Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development: Future research is needed to evaluate all AI based programs before clinical implementation in non-research settings."
30414395,5.0,Human and computational models of atopic dermatitis: A review and perspectives by an expert panel of the International Eczema Council,2019 Jan;143(1):36-45.,"Atopic dermatitis (AD) is a prevalent disease worldwide and is associated with systemic comorbidities representing a significant burden on patients, their families, and society. Therapeutic options for AD remain limited, in part because of a lack of well-characterized animal models. There has been increasing interest in developing experimental approaches to study the pathogenesis of human AD in vivo, in vitro, and in silico to better define pathophysiologic mechanisms and identify novel therapeutic targets and biomarkers that predict therapeutic response. This review critically appraises a range of models, including genetic mutations relevant to AD, experimental challenge of human skin in vivo, tissue culture models, integration of ""omics"" data sets, and development of predictive computational models. Although no one individual model recapitulates the complex AD pathophysiology, our review highlights insights gained into key elements of cutaneous biology, molecular pathways, and therapeutic target identification through each approach. Recent developments in computational analysis, including application of machine learning and a systems approach to data integration and predictive modeling, highlight the applicability of these methods to AD subclassification (endotyping), therapy development, and precision medicine. Such predictive modeling will highlight knowledge gaps, further inform refinement of biological models, and support new experimental and systems approaches to AD."
30410432,20.0,Deep Learning With Spiking Neurons: Opportunities and Challenges,2018 Oct 25;12:774.,"Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications."
30407665,17.0,Applying the latest advances in genomics and phenomics for trait discovery in polyploid wheat,2019 Jan;97(1):56-72.,"Improving traits in wheat has historically been challenging due to its large and polyploid genome, limited genetic diversity and in-field phenotyping constraints. However, within recent years many of these barriers have been lowered. The availability of a chromosome-level assembly of the wheat genome now facilitates a step-change in wheat genetics and provides a common platform for resources, including variation data, gene expression data and genetic markers. The development of sequenced mutant populations and gene-editing techniques now enables the rapid assessment of gene function in wheat directly. The ability to alter gene function in a targeted manner will unmask the effects of homoeolog redundancy and allow the hidden potential of this polyploid genome to be discovered. New techniques to identify and exploit the genetic diversity within wheat wild relatives now enable wheat breeders to take advantage of these additional sources of variation to address challenges facing food production. Finally, advances in phenomics have unlocked rapid screening of populations for many traits of interest both in greenhouses and in the field. Looking forwards, integrating diverse data types, including genomic, epigenetic and phenomics data, will take advantage of big data approaches including machine learning to understand trait biology in wheat in unprecedented detail."
30403523,,Machine Learning in Neurooncology Imaging: From Study Request to Diagnosis and Treatment,2019 Jan;212(1):52-56.,"Objective:                    Machine learning has potential to play a key role across a variety of medical imaging applications. This review seeks to elucidate the ways in which machine learning can aid and enhance diagnosis, treatment, and follow-up in neurooncology.              Conclusion:                    Given the rapid pace of development in machine learning over the past several years, a basic proficiency of the key tenets and use cases in the field is critical to assessing potential opportunities and challenges of this exciting new technology."
30400053,18.0,Artificial intelligence and echocardiography,2018 Dec 1;5(4):R115-R125.,"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome."
30397993,12.0,"Legal, regulatory, and ethical frameworks for development of standards in artificial intelligence (AI) and autonomous robotic surgery",2019 Feb;15(1):e1968.,"Background:                    This paper aims to move the debate forward regarding the potential for artificial intelligence (AI) and autonomous robotic surgery with a particular focus on ethics, regulation and legal aspects (such as civil law, international law, tort law, liability, medical malpractice, privacy and product/device legislation, among other aspects).              Methods:                    We conducted an intensive literature search on current or emerging AI and autonomous technologies (eg, vehicles), military and medical technologies (eg, surgical robots), relevant frameworks and standards, cyber security/safety- and legal-systems worldwide. We provide a discussion on unique challenges for robotic surgery faced by proposals made for AI more generally (eg, Explainable AI) and machine learning more specifically (eg, black box), as well as recommendations for developing and improving relevant frameworks or standards.              Conclusion:                    We classify responsibility into the following: (1) Accountability; (2) Liability; and (3) Culpability. All three aspects were addressed when discussing responsibility for AI and autonomous surgical robots, be these civil or military patients (however, these aspects may require revision in cases where robots become citizens). The component which produces the least clarity is Culpability, since it is unthinkable in the current state of technology. We envision that in the near future a surgical robot can learn and perform routine operative tasks that can then be supervised by a human surgeon. This represents a surgical parallel to autonomously driven vehicles. Here a human remains in the 'driving seat' as a 'doctor-in-the-loop' thereby safeguarding patients undergoing operations that are supported by surgical machines with autonomous capabilities."
30394238,4.0,Introduction to Machine Learning in Digital Healthcare Epidemiology,2018 Dec;39(12):1457-1462.,"To exploit the full potential of big routine data in healthcare and to efficiently communicate and collaborate with information technology specialists and data analysts, healthcare epidemiologists should have some knowledge of large-scale analysis techniques, particularly about machine learning. This review focuses on the broad area of machine learning and its first applications in the emerging field of digital healthcare epidemiology."
30391107,,"Status epilepticus prevention, ambulatory monitoring, early seizure detection and prediction in at-risk patients",2019 May;68:31-37.,"Purpose:                    Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.              Method:                    Narrative review.              Results:                    Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.              Conclusions:                    Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families."
30383900,21.0,"Quantitative imaging of cancer in the postgenomic era: Radio(geno)mics, deep learning, and habitats",2018 Dec 15;124(24):4633-4649.,"Although cancer often is referred to as ""a disease of the genes,"" it is indisputable that the (epi)genetic properties of individual cancer cells are highly variable, even within the same tumor. Hence, preexisting resistant clones will emerge and proliferate after therapeutic selection that targets sensitive clones. Herein, the authors propose that quantitative image analytics, known as ""radiomics,"" can be used to quantify and characterize this heterogeneity. Virtually every patient with cancer is imaged radiologically. Radiomics is predicated on the beliefs that these images reflect underlying pathophysiologies, and that they can be converted into mineable data for improved diagnosis, prognosis, prediction, and therapy monitoring. In the last decade, the radiomics of cancer has grown from a few laboratories to a worldwide enterprise. During this growth, radiomics has established a convention, wherein a large set of annotated image features (1-2000 features) are extracted from segmented regions of interest and used to build classifier models to separate individual patients into their appropriate class (eg, indolent vs aggressive disease). An extension of this conventional radiomics is the application of ""deep learning,"" wherein convolutional neural networks can be used to detect the most informative regions and features without human intervention. A further extension of radiomics involves automatically segmenting informative subregions (""habitats"") within tumors, which can be linked to underlying tumor pathophysiology. The goal of the radiomics enterprise is to provide informed decision support for the practice of precision oncology."
30382605,7.0,A Machine Learning Approach to Predicting Need for Hospitalization for Pediatric Asthma Exacerbation at the Time of Emergency Department Triage,2018 Dec;25(12):1463-1470.,"Objectives:                    Pediatric asthma is a leading cause of emergency department (ED) utilization and hospitalization. Earlier identification of need for hospital-level care could triage patients more efficiently to high- or low-resource ED tracks. Existing tools to predict disposition for pediatric asthma use only clinical data, perform best several hours into the ED stay, and are static or score-based. Machine learning offers a population-specific, dynamic option that allows real-time integration of available nonclinical data at triage. Our objective was to compare the performance of four common machine learning approaches, incorporating clinical data available at the time of triage with information about weather, neighborhood characteristics, and community viral load for early prediction of the need for hospital-level care in pediatric asthma.              Methods:                    Retrospective analysis of patients ages 2 to 18 years seen at two urban pediatric EDs with asthma exacerbation over 4 years. Asthma exacerbation was defined as receiving both albuterol and systemic corticosteroids. We included patient features, measures of illness severity available in triage, weather features, and Centers for Disease Control and Prevention influenza patterns. We tested four models: decision trees, LASSO logistic regression, random forests, and gradient boosting machines. For each model, 80% of the data set was used for training and 20% was used to validate the models. The area under the receiver operating characteristic (AUC) curve was calculated for each model.              Results:                    There were 29,392 patients included in the analyses: mean (±SD) age of 7.0 (±4.2) years, 42% female, 77% non-Hispanic black, and 76% public insurance. The AUCs for each model were: decision tree 0.72 (95% confidence interval [CI] = 0.66-0.77), logistic regression 0.83 (95% CI = 0.82-0.83), random forests 0.82 (95% CI = 0.81-0.83), and gradient boosting machines 0.84 (95% CI = 0.83-0.85). In the lowest decile of risk, only 3% of patients required hospitalization; in the highest decile this rate was 100%. After patient vital signs and acuity, age and weight, followed by socioeconomic status (SES) and weather-related features, were the most important for predicting hospitalization.              Conclusions:                    Three of the four machine learning models performed well with decision trees preforming the worst. The gradient boosting machines model demonstrated a slight advantage over other approaches at predicting need for hospital-level care at the time of triage in pediatric patients presenting with asthma exacerbation. The addition of weight, SES, and weather data improved the performance of this model."
30381431,10.0,Latent Factors and Dynamics in Motor Cortex and Their Application to Brain-Machine Interfaces,2018 Oct 31;38(44):9390-9401.,"In the 1960s, Evarts first recorded the activity of single neurons in motor cortex of behaving monkeys (Evarts, 1968). In the 50 years since, great effort has been devoted to understanding how single neuron activity relates to movement. Yet these single neurons exist within a vast network, the nature of which has been largely inaccessible. With advances in recording technologies, algorithms, and computational power, the ability to study these networks is increasing exponentially. Recent experimental results suggest that the dynamical properties of these networks are critical to movement planning and execution. Here we discuss this dynamical systems perspective and how it is reshaping our understanding of the motor cortices. Following an overview of key studies in motor cortex, we discuss techniques to uncover the ""latent factors"" underlying observed neural population activity. Finally, we discuss efforts to use these factors to improve the performance of brain-machine interfaces, promising to make these findings broadly relevant to neuroengineering as well as systems neuroscience."
30381421,41.0,Genome-Based Prediction of Bacterial Antibiotic Resistance,2019 Feb 27;57(3):e01405-18.,"Clinical microbiology has long relied on growing bacteria in culture to determine antimicrobial susceptibility profiles, but the use of whole-genome sequencing for antibiotic susceptibility testing (WGS-AST) is now a powerful alternative. This review discusses the technologies that made this possible and presents results from recent studies to predict resistance based on genome sequences. We examine differences between calling antibiotic resistance profiles by the simple presence or absence of previously known genes and single-nucleotide polymorphisms (SNPs) against approaches that deploy machine learning and statistical models. Often, the limitations to genome-based prediction arise from limitations of accuracy of culture-based AST in addition to an incomplete knowledge of the genetic basis of resistance. However, we need to maintain phenotypic testing even as genome-based prediction becomes more widespread to ensure that the results do not diverge over time. We argue that standardization of WGS-AST by challenge with consistently phenotyped strain sets of defined genetic diversity is necessary to compare the efficacy of methods of prediction of antibiotic resistance based on genome sequences."
33500999,,Survey of Image Processing Techniques for Brain Pathology Diagnosis: Challenges and Opportunities,2018 Nov 2;5:120.,"In recent years, a number of new products introduced to the global market combine intelligent robotics, artificial intelligence and smart interfaces to provide powerful tools to support professional decision making. However, while brain disease diagnosis from the brain scan images is supported by imaging robotics, the data analysis to form a medical diagnosis is performed solely by highly trained medical professionals. Recent advances in medical imaging techniques, artificial intelligence, machine learning and computer vision present new opportunities to build intelligent decision support tools to aid the diagnostic process, increase the disease detection accuracy, reduce error, automate the monitoring of patient's recovery, and discover new knowledge about the disease cause and its treatment. This article introduces the topic of medical diagnosis of brain diseases from the MRI based images. We describe existing, multi-modal imaging techniques of the brain's soft tissue and describe in detail how are the resulting images are analyzed by a radiologist to form a diagnosis. Several comparisons between the best results of classifying natural scenes and medical image analysis illustrate the challenges of applying existing image processing techniques to the medical image analysis domain. The survey of medical image processing methods also identified several knowledge gaps, the need for automation of image processing analysis, and the identification of the brain structures in the medical images that differentiate healthy tissue from a pathology. This survey is grounded in the cases of brain tumor analysis and the traumatic brain injury diagnoses, as these two case studies illustrate the vastly different approaches needed to define, extract, and synthesize meaningful information from multiple MRI image sets for a diagnosis. Finally, the article summarizes artificial intelligence frameworks that are built as multi-stage, hybrid, hierarchical information processing work-flows and the benefits of applying these models for medical diagnosis to build intelligent physician's aids with knowledge transparency, expert knowledge embedding, and increased analytical quality."
30379703,5.0,Neurotrauma as a big-data problem,2018 Dec;31(6):702-708.,"Purpose of review:                    The field of neurotrauma research faces a reproducibility crisis. In response, research leaders in traumatic brain injury (TBI) and spinal cord injury (SCI) are leveraging data curation and analytics methods to encourage transparency, and improve the rigor and reproducibility. Here we review the current challenges and opportunities that come from efforts to transform neurotrauma's big data to knowledge.              Recent findings:                    Three parallel movements are driving data-driven-discovery in neurotrauma. First, large multicenter consortia are collecting large quantities of neurotrauma data, refining common data elements (CDEs) that can be used across studies. Investigators are now testing the validity of CDEs in diverse research settings. Second, data sharing initiatives are working to make neurotrauma data findable, accessible, interoperable, and reusable (FAIR). These efforts are reflected by recent open data repository projects for preclinical and clinical neurotrauma. Third, machine learning analytics are allowing researchers to uncover novel data-driven-hypotheses and test new therapeutics in multidimensional outcome space.              Summary:                    We are on the threshold of a new era in data collection, curation, and analysis. The next phase of big data in neurotrauma research will require responsible data stewardship, a culture of data-sharing, and the illumination of 'dark data'."
30378494,28.0,Recent Advances in Machine Learning Methods for Predicting Heat Shock Proteins,2019;20(3):224-228.,"Background:                    As molecular chaperones, Heat Shock Proteins (HSPs) not only play key roles in protein folding and maintaining protein stabilities, but are also linked with multiple kinds of diseases. Therefore, HSPs have been regarded as the focus of drug design. Since HSPs from different families play distinct functions, accurately classifying the families of HSPs is the key step to clearly understand their biological functions. In contrast to laborintensive and cost-ineffective experimental methods, computational classification of HSP families has emerged to be an alternative approach.              Methods:                    We reviewed the paper that described the existing datasets of HSPs and the representative computational approaches developed for the identification and classification of HSPs.              Results:                    The two benchmark datasets of HSPs, namely HSPIR and sHSPdb were introduced, which provided invaluable resources for computationally identifying HSPs. The gold standard dataset and sequence encoding schemes for building computational methods of classifying HSPs were also introduced. The three representative web-servers for identifying HSPs and their families were described.              Conclusion:                    The existing machine learning methods for identifying the different families of HSPs indeed yielded quite encouraging results and did play a role in promoting the research on HSPs. However, the number of HSPs with known structures is very limited. Therefore, determining the structure of the HSPs is also urgent, which will be helpful in revealing their functions."
30378482,2.0,Advanced in Silico Methods for the Development of Anti- Leishmaniasis and Anti-Trypanosomiasis Agents,2020;27(5):697-718.,"Leishmaniasis and trypanosomiasis occur primarily in undeveloped countries and account for millions of deaths and disability-adjusted life years. Limited therapeutic options, high toxicity of chemotherapeutic drugs and the emergence of drug resistance associated with these diseases demand urgent development of novel therapeutic agents for the treatment of these dreadful diseases. In the last decades, different in silico methods have been successfully implemented for supporting the lengthy and expensive drug discovery process. In the current review, we discuss recent advances pertaining to in silico analyses towards lead identification, lead modification and target identification of antileishmaniasis and anti-trypanosomiasis agents. We describe recent applications of some important in silico approaches, such as 2D-QSAR, 3D-QSAR, pharmacophore mapping, molecular docking, and so forth, with the aim of understanding the utility of these techniques for the design of novel therapeutic anti-parasitic agents. This review focuses on: (a) advanced computational drug design options; (b) diverse methodologies - e.g.: use of machine learning tools, software solutions, and web-platforms; (c) recent applications and advances in the last five years; (d) experimental validations of in silico predictions; (e) virtual screening tools; and (f) rationale or justification for the selection of these in silico methods."
30378077,,Computational Methods for Subtyping of Tumors and Their Applications for Deciphering Tumor Heterogeneity,2019;1878:193-207.,"With the rapid development of deep sequencing technologies, many programs are generating multi-platform genomic profiles (e.g., somatic mutation, DNA methylation, and gene expression) for a large number of tumors. This activity has provided unique opportunities and challenges to stratify tumors and decipher tumor heterogeneity. In this chapter, we summarize several computational methods to address the challenge of tumor stratification with different types of genomic data. We further introduce their applications in emerging large-scale genomic data to show their effectiveness in deciphering tumor heterogeneity and clinical relevance."
30378021,2.0,Prediction of Peroxisomal Matrix Proteins in Plants,2018;89:125-138.,"Our knowledge of the proteome of plant peroxisomes is far from being complete, and the functional complexity and plasticity of this cell organelle are amazingly high particularly in plants, as exemplified by the model species Arabidopsis thaliana. Plant-specific peroxisome functions that have been uncovered only recently include, for instance, the participation of peroxisomes in phylloquinone and biotin biosynthesis. Experimental proteome studies have been proved very successful in defining the proteome of Arabidopsis peroxisomes but this approach also faces significant challenges and limitations. Complementary to experimental approaches, computational methods have emerged as important powerful tools to define the proteome of soluble matrix proteins of plant peroxisomes. Compared to other cell organelles such as mitochondria, plastids and the ER, the simultaneous operation of two major import pathways for soluble proteins in peroxisomes is rather atypical. Novel machine learning prediction approaches have been developed for peroxisome targeting signals type 1 (PTS1) and revealed high sensitivity and specificity, as validated by in vivo subcellular targeting analyses in diverse transient plant expression systems. Accordingly, the algorithms allow the correct prediction of many novel peroxisome-targeted proteins from plant genome sequences and the discovery of additional organelle functions. In contrast, the prediction of PTS2 proteins largely remains restricted to genome searches by conserved patterns contrary to more advanced machine learning methods. Here, we summarize and discuss the capabilities and accuracies of available prediction algorithms for PTS1 and PTS2 carrying proteins."
30375164,3.0,Multiscale Modeling of Silk and Silk-Based Biomaterials-A Review,2019 Mar;19(3):e1800253.,"Silk embodies outstanding material properties and biologically relevant functions achieved through a delicate hierarchical structure. It can be used to create high-performance, multifunctional, and biocompatible materials through mild processes and careful rational material designs. To achieve this goal, computational modeling has proven to be a powerful platform to unravel the causes of the excellent mechanical properties of silk, to predict the properties of the biomaterials derived thereof, and to assist in devising new manufacturing strategies. Fine-scale modeling has been done mainly through all-atom and coarse-grained molecular dynamics simulations, which offer a bottom-up description of silk. In this work, a selection of relevant contributions of computational modeling is reviewed to understand the properties of natural silk, and to the design of silk-based materials, especially combined with experimental methods. Future research directions are also pointed out, including approaches such as 3D printing and machine learning, that may enable a high throughput design and manufacturing of silk-based biomaterials."
30374293,2.0,Analysis Tools for Large Connectomes,2018 Oct 15;12:85.,"New reconstruction techniques are generating connectomes of unprecedented size. These must be analyzed to generate human comprehensible results. The analyses being used fall into three general categories. The first is interactive tools used during reconstruction, to help guide the effort, look for possible errors, identify potential cell classes, and answer other preliminary questions. The second type of analysis is support for formal documents such as papers and theses. Scientific norms here require that the data be archived and accessible, and the analysis reproducible. In contrast to some other ""omic"" fields such as genomics, where a few specific analyses dominate usage, connectomics is rapidly evolving and the analyses used are often specific to the connectome being analyzed. These analyses are typically performed in a variety of conventional programming language, such as Matlab, R, Python, or C++, and read the connectomic data either from a file or through database queries, neither of which are standardized. In the short term we see no alternative to the use of specific analyses, so the best that can be done is to publish the analysis code, and the interface by which it reads connectomic data. A similar situation exists for archiving connectome data. Each group independently makes their data available, but there is no standardized format and long-term accessibility is neither enforced nor funded. In the long term, as connectomics becomes more common, a natural evolution would be a central facility for storing and querying connectomic data, playing a role similar to the National Center for Biotechnology Information for genomes. The final form of analysis is the import of connectome data into downstream tools such as neural simulation or machine learning. In this process, there are two main problems that need to be addressed. First, the reconstructed circuits contain huge amounts of detail, which must be intelligently reduced to a form the downstream tools can use. Second, much of the data needed for these downstream operations must be obtained by other methods (such as genetic or optical) and must be merged with the extracted connectome."
30371751,4.0,Machine Learning in Human Olfactory Research,2019 Jan 1;44(1):11-22.,"The complexity of the human sense of smell is increasingly reflected in complex and high-dimensional data, which opens opportunities for data-driven approaches that complement hypothesis-driven research. Contemporary developments in computational and data science, with its currently most popular implementation as machine learning, facilitate complex data-driven research approaches. The use of machine learning in human olfactory research included major approaches comprising 1) the study of the physiology of pattern-based odor detection and recognition processes, 2) pattern recognition in olfactory phenotypes, 3) the development of complex disease biomarkers including olfactory features, 4) odor prediction from physico-chemical properties of volatile molecules, and 5) knowledge discovery in publicly available big databases. A limited set of unsupervised and supervised machine-learned methods has been used in these projects, however, the increasing use of contemporary methods of computational science is reflected in a growing number of reports employing machine learning for human olfactory research. This review provides key concepts of machine learning and summarizes current applications on human olfactory data."
30370979,1.0,"Membrane Filtration with Liquids: A Global Approach with Prior Successes, New Developments and Unresolved Challenges",2019 Feb 11;58(7):1892-1902.,"After 70 years, modern pressure-driven polymer membrane processes with liquids are mature and accepted in many industries due to their good performance, ease of scale-up, low energy consumption, modular compact construction, and low operating costs compared with thermal systems. Successful isothermal operation of synthetic membranes with liquids requires consideration of three critical aspects or ""legs"" in order of relevance: selectivity, capacity (i.e. permeation flow rate per unit area) and transport of mass and momentum comprising concentration polarization (CP) and fouling (F). Major challenges remain with respect to increasing selectivity and controlling mass transport in, to and away from membranes. Thus, prediction and control of membrane morphology and a deep understanding of the mechanism of dissolved and suspended solute transport near and in the membrane (i.e. diffusional and convective mass transport) is essential. Here, we focus on materials development to address the relatively poor selectivity of liquid membrane filtration with polymers and discuss the critical aspects of transport limitations. Machine learning could help optimize membrane structure design and transport conditions for improved membrane filtration performance."
30370798,3.0,Post-viral atopic airway disease: pathogenesis and potential avenues for intervention,2019 Jan;15(1):49-58.,"Introduction: In early childhood, wheezing due to lower respiratory tract illness is often associated with infection by commonly known respiratory viruses such as respiratory syncytial virus (RSV) and human rhinovirus (RV). How respiratory viral infections lead to wheeze and/or asthma is an area of active research. Areas covered: This review provides an updated summary of the published information on the development of post-viral induced atopy and asthma and the mechanisms involved. We focus on the contribution of animal models in identifying pathways that may contribute to atopy and asthma following respiratory virus infection, different polymorphisms that have been associated with asthma development, and current options for disease management and potential future interventions. Expert commentary: Currently there are no prophylactic therapies that prevent infants infected with respiratory viruses from developing asthma or atopy. Neither are there curative therapies for patients with asthma. Therefore, a better understanding of genetic factors and other associated biomarkers in respiratory viral induced pathogenesis is important for developing effective personalized therapies."
30369809,6.0,History and application of artificial neural networks in dentistry,Oct-Dec 2018;12(4):594-601.,"Artificial intelligence (AI) is a commonly used term in daily life, and there are now two subconcepts that divide the entire range of meanings currently encompassed by the term. The coexistence of the concepts of strong and weak AI can be seen as a result of the recognition of the limits of mathematical and engineering concepts that have dominated the definition. This presentation reviewed the concept, history, and the current application of AI in daily life. Applications of AI are becoming a reality that is commonplace in all areas of modern human life. Efforts to develop robots controlled by AI have been continuously carried out to maximize human convenience. AI has also been applied in the medical decision-making process, and these AI systems can help nonspecialists to obtain expert-level information. Artificial neural networks are highly interconnected networks of computer processors inspired by biological nervous systems. These systems may help connect dental professionals all over the world. Currently, the use of AI is rapidly advancing beyond text-based, image-based dental practice. This presentation reviewed the history of artificial neural networks in the medical and dental fields, as well as current application in dentistry. As the use of AI in the entire medical field increases, the role of AI in dentistry will be greatly expanded. Currently, the use of AI is rapidly advancing beyond text-based, image-based dental practice. In addition to diagnosis of visually confirmed dental caries and impacted teeth, studies applying machine learning based on artificial neural networks to dental treatment through analysis of dental magnetic resonance imaging, computed tomography, and cephalometric radiography are actively underway, and some visible results are emerging at a rapid pace for commercialization."
30368611,11.0,"Machine Learning for Predicting Cognitive Diseases: Methods, Data Sources and Risk Factors",2018 Oct 27;42(12):243.,"Machine learning and data mining approaches are being successfully applied to different fields of life sciences for the past 20 years. Medicine is one of the most suitable application domains for these techniques since they help model diagnostic information based on causal and/or statistical data and therefore reveal hidden dependencies between symptoms and illnesses. In this paper we give a detailed overview of the recent machine learning research and its applications for predicting cognitive diseases, especially the Alzheimer's disease, mild cognitive impairment and the Parkinson's disease. We survey different state-of-the-art methodological approaches, data sources and public data, and provide their comparative analysis. We conclude by identifying the open problems within the field that include an early detection of the cognitive diseases and inclusion of machine learning tools into diagnostic practice and therapy planning."
30367497,58.0,Deep learning in medical imaging and radiation therapy,2019 Jan;46(1):e1-e36.,"The goals of this review paper on deep learning (DL) in medical imaging and radiation therapy are to (a) summarize what has been achieved to date; (b) identify common and unique challenges, and strategies that researchers have taken to address these challenges; and (c) identify some of the promising avenues for the future both in terms of applications as well as technical innovations. We introduce the general principles of DL and convolutional neural networks, survey five major areas of application of DL in medical imaging and radiation therapy, identify common themes, discuss methods for dataset expansion, and conclude by summarizing lessons learned, remaining challenges, and future directions."
30366739,8.0,The unreasonable effectiveness of small neural ensembles in high-dimensional brain,2019 Jul;29:55-88.,"Complexity is an indisputable, well-known, and broadly accepted feature of the brain. Despite the apparently obvious and widely-spread consensus on the brain complexity, sprouts of the single neuron revolution emerged in neuroscience in the 1970s. They brought many unexpected discoveries, including grandmother or concept cells and sparse coding of information in the brain. In machine learning for a long time, the famous curse of dimensionality seemed to be an unsolvable problem. Nevertheless, the idea of the blessing of dimensionality becomes gradually more and more popular. Ensembles of non-interacting or weakly interacting simple units prove to be an effective tool for solving essentially multidimensional and apparently incomprehensible problems. This approach is especially useful for one-shot (non-iterative) correction of errors in large legacy artificial intelligence systems and when the complete re-training is impossible or too expensive. These simplicity revolutions in the era of complexity have deep fundamental reasons grounded in geometry of multidimensional data spaces. To explore and understand these reasons we revisit the background ideas of statistical physics. In the course of the 20th century they were developed into the concentration of measure theory. The Gibbs equivalence of ensembles with further generalizations shows that the data in high-dimensional spaces are concentrated near shells of smaller dimension. New stochastic separation theorems reveal the fine structure of the data clouds. We review and analyse biological, physical, and mathematical problems at the core of the fundamental question: how can high-dimensional brain organise reliable and fast learning in high-dimensional world of data by simple tools? To meet this challenge, we outline and setup a framework based on statistical physics of data. Two critical applications are reviewed to exemplify the approach: one-shot correction of errors in intellectual systems and emergence of static and associative memories in ensembles of single neurons. Error correctors should be simple; not damage the existing skills of the system; allow fast non-iterative learning and correction of new mistakes without destroying the previous fixes. All these demands can be satisfied by new tools based on the concentration of measure phenomena and stochastic separation theory. We show how a simple enough functional neuronal model is capable of explaining: i) the extreme selectivity of single neurons to the information content of high-dimensional data, ii) simultaneous separation of several uncorrelated informational items from a large set of stimuli, and iii) dynamic learning of new items by associating them with already ""known"" ones. These results constitute a basis for organisation of complex memories in ensembles of single neurons."
30364792,28.0,Artificial intelligence in gastrointestinal endoscopy: The future is almost here,2018 Oct 16;10(10):239-249.,"Artificial intelligence (AI) enables machines to provide unparalleled value in a myriad of industries and applications. In recent years, researchers have harnessed artificial intelligence to analyze large-volume, unstructured medical data and perform clinical tasks, such as the identification of diabetic retinopathy or the diagnosis of cutaneous malignancies. Applications of artificial intelligence techniques, specifically machine learning and more recently deep learning, are beginning to emerge in gastrointestinal endoscopy. The most promising of these efforts have been in computer-aided detection and computer-aided diagnosis of colorectal polyps, with recent systems demonstrating high sensitivity and accuracy even when compared to expert human endoscopists. AI has also been utilized to identify gastrointestinal bleeding, to detect areas of inflammation, and even to diagnose certain gastrointestinal infections. Future work in the field should concentrate on creating seamless integration of AI systems with current endoscopy platforms and electronic medical records, developing training modules to teach clinicians how to use AI tools, and determining the best means for regulation and approval of new AI technology."
30361937,5.0,Big Data in Head and Neck Cancer,2018 Oct 25;19(12):62.,"Head and neck cancers can be used as a paradigm for exploring ""big data"" applications in oncology. Computational strategies derived from big data science hold the promise of shedding new light on the molecular mechanisms driving head and neck cancer pathogenesis, identifying new prognostic and predictive factors, and discovering potential therapeutics against this highly complex disease. Big data strategies integrate robust data input, from radiomics, genomics, and clinical-epidemiological data to deeply describe head and neck cancer characteristics. Thus, big data may advance research generating new knowledge and improve head and neck cancer prognosis supporting clinical decision-making and development of treatment recommendations."
30360722,1.0,Elucidating Protein-protein Interactions Through Computational Approaches and Designing Small Molecule Inhibitors Against them for Various Diseases,2018;18(20):1719-1736.,"Background:                    To carry out wide range of cellular functionalities, proteins often associate with one or more proteins in a phenomenon known as Protein-Protein Interaction (PPI). Experimental and computational approaches were applied on PPIs in order to determine the interacting partners, and also to understand how an abnormality in such interactions can become the principle cause of a disease.              Objective:                    This review aims to elucidate the case studies where PPIs involved in various human diseases have been proven or validated with computational techniques, and also to elucidate how small molecule inhibitors of PPIs have been designed computationally to act as effective therapeutic measures against certain diseases.              Results:                    Computational techniques to predict PPIs are emerging rapidly in the modern day. They not only help in predicting new PPIs, but also generate outputs that substantiate the experimentally determined results. Moreover, computation has aided in the designing of novel inhibitor molecules disrupting the PPIs. Some of them are already being tested in the clinical trials.              Conclusion:                    This review delineated the classification of computational tools that are essential to investigate PPIs. Furthermore, the review shed light on how indispensable computational tools have become in the field of medicine to analyze the interaction networks and to design novel inhibitors efficiently against dreadful diseases in a shorter time span."
30357911,8.0,An update on adaptive deep brain stimulation in Parkinson's disease,2018 Dec;33(12):1834-1843.,"Advancing conventional open-loop DBS as a therapy for PD is crucial for overcoming important issues such as the delicate balance between beneficial and adverse effects and limited battery longevity that are currently associated with treatment. Closed-loop or adaptive DBS aims to overcome these limitations by real-time adjustment of stimulation parameters based on continuous feedback input signals that are representative of the patient's clinical state. The focus of this update is to discuss the most recent developments regarding potential input signals and possible stimulation parameter modulation for adaptive DBS in PD. Potential input signals for adaptive DBS include basal ganglia local field potentials, cortical recordings (electrocorticography), wearable sensors, and eHealth and mHealth devices. Furthermore, adaptive DBS can be applied with different approaches of stimulation parameter modulation, the feasibility of which can be adapted depending on specific PD phenotypes. Implementation of technological developments like machine learning show potential in the design of such approaches; however, energy consumption deserves further attention. Furthermore, we discuss future considerations regarding the clinical implementation of adaptive DBS in PD. © 2018 The Authors. Movement Disorders published by Wiley Periodicals, Inc. on behalf of International Parkinson and Movement Disorder Society."
30356770,4.0,Challenges and Future Perspectives on Electroencephalogram-Based Biometrics in Person Recognition,2018 Oct 9;12:66.,"The emergence of the digital world has greatly increased the number of accounts and passwords that users must remember. It has also increased the need for secure access to personal information in the cloud. Biometrics is one approach to person recognition, which can be used in identification as well as authentication. Among the various modalities that have been developed, electroencephalography (EEG)-based biometrics features unparalleled universality, distinctiveness and collectability, while minimizing the risk of circumvention. However, commercializing EEG-based person recognition poses a number of challenges. This article reviews the various systems proposed over the past few years with a focus on the shortcomings that have prevented wide-scale implementation, including issues pertaining to temporal stability, psychological and physiological changes, protocol design, equipment and performance evaluation. We also examine several directions for the further development of usable EEG-based recognition systems as well as the niche markets to which they could be applied. It is expected that rapid advancements in EEG instrumentation, on-device processing and machine learning techniques will lead to the emergence of commercialized person recognition systems in the near future."
30356768,16.0,Network-Based Methods for Prediction of Drug-Target Interactions,2018 Oct 9;9:1134.,"Drug-target interaction (DTI) is the basis of drug discovery. However, it is time-consuming and costly to determine DTIs experimentally. Over the past decade, various computational methods were proposed to predict potential DTIs with high efficiency and low costs. These methods can be roughly divided into several categories, such as molecular docking-based, pharmacophore-based, similarity-based, machine learning-based, and network-based methods. Among them, network-based methods, which do not rely on three-dimensional structures of targets and negative samples, have shown great advantages over the others. In this article, we focused on network-based methods for DTI prediction, in particular our network-based inference (NBI) methods that were derived from recommendation algorithms. We first introduced the methodologies and evaluation of network-based methods, and then the emphasis was put on their applications in a wide range of fields, including target prediction and elucidation of molecular mechanisms of therapeutic effects or safety problems. Finally, limitations and perspectives of network-based methods were discussed. In a word, network-based methods provide alternative tools for studies in drug repurposing, new drug discovery, systems pharmacology and systems toxicology."
30353365,64.0,Artificial intelligence in medical imaging: threat or opportunity? Radiologists again at the forefront of innovation in medicine,2018 Oct 24;2(1):35.,"One of the most promising areas of health innovation is the application of artificial intelligence (AI), primarily in medical imaging. This article provides basic definitions of terms such as ""machine/deep learning"" and analyses the integration of AI into radiology. Publications on AI have drastically increased from about 100-150 per year in 2007-2008 to 700-800 per year in 2016-2017. Magnetic resonance imaging and computed tomography collectively account for more than 50% of current articles. Neuroradiology appears in about one-third of the papers, followed by musculoskeletal, cardiovascular, breast, urogenital, lung/thorax, and abdomen, each representing 6-9% of articles. With an irreversible increase in the amount of data and the possibility to use AI to identify findings either detectable or not by the human eye, radiology is now moving from a subjective perceptual skill to a more objective science. Radiologists, who were on the forefront of the digital era in medicine, can guide the introduction of AI into healthcare. Yet, they will not be replaced because radiology includes communication of diagnosis, consideration of patient's values and preferences, medical judgment, quality assurance, education, policy-making, and interventional procedures. The higher efficiency provided by AI will allow radiologists to perform more value-added tasks, becoming more visible to patients and playing a vital role in multidisciplinary clinical teams."
30347013,32.0,The Science of Prognosis in Psychiatry: A Review,2018 Dec 1;75(12):1289-1297.,"Importance:                    Prognosis is a venerable component of medical knowledge introduced by Hippocrates (460-377 BC). This educational review presents a contemporary evidence-based approach for how to incorporate clinical risk prediction models in modern psychiatry. The article is organized around key methodological themes most relevant for the science of prognosis in psychiatry. Within each theme, the article highlights key challenges and makes pragmatic recommendations to improve scientific understanding of prognosis in psychiatry.              Observations:                    The initial step to building clinical risk prediction models that can affect psychiatric care involves designing the model: preparation of the protocol and definition of the outcomes and of the statistical methods (theme 1). Further initial steps involve carefully selecting the predictors, preparing the data, and developing the model in these data. A subsequent step is the validation of the model to accurately test its generalizability (theme 2). The next consideration is that the accuracy of the clinical prediction model is affected by the incidence of the psychiatric condition under investigation (theme 3). Eventually, clinical prediction models need to be implemented in real-world clinical routine, and this is usually the most challenging step (theme 4). Advanced methods such as machine learning approaches can overcome some problems that undermine the previous steps (theme 5). The relevance of each of these themes to current clinical risk prediction modeling in psychiatry is discussed and recommendations are given.              Conclusions and relevance:                    Together, these perspectives intend to contribute to an integrative, evidence-based science of prognosis in psychiatry. By focusing on the outcome of the individuals, rather than on the disease, clinical risk prediction modeling can become the cornerstone for a scientific and personalized psychiatry."
30342246,6.0,Artificial intelligence and its potential in oncology,2019 Jan;24(1):228-232.,"The two main branches associated with Artificial Intelligence (AI) in medicine are virtual and physical. The virtual component includes machine learning (ML) and algorithms, whereas physical AI includes medical devices and robots for delivering care. AI is used successfully in tumour segmentation, histopathological diagnosis, tracking tumour development, and prognosis prediction. CURATE.AI, developed at the National University of Singapore, is a platform that automatically decides the optimum dose of drugs for a durable response, allowing the patient to resume a completely normal life. With the involvement of technology multinationals, such as Google and Microsoft, in AI and healthcare in association with leading healthcare companies, the future of AI in healthcare looks very promising."
30339893,6.0,Big data and targeted machine learning in action to assist medical decision in the ICU,2019 Aug;38(4):377-384.,"Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient."
30338743,3.0,Virtual Screening Meets Deep Learning,2019;15(1):6-28.,"Background:                    Automated compound testing is currently the de facto standard method for drug screening, but it has not brought the great increase in the number of new drugs that was expected. Computer- aided compounds search, known as Virtual Screening, has shown the benefits to this field as a complement or even alternative to the robotic drug discovery. There are different methods and approaches to address this problem and most of them are often included in one of the main screening strategies. Machine learning, however, has established itself as a virtual screening methodology in its own right and it may grow in popularity with the new trends on artificial intelligence.              Objective:                    This paper will attempt to provide a comprehensive and structured review that collects the most important proposals made so far in this area of research. Particular attention is given to some recent developments carried out in the machine learning field: the deep learning approach, which is pointed out as a future key player in the virtual screening landscape."
30338736,8.0,Survey of Machine Learning Techniques for Prediction of the Isoform Specificity of Cytochrome P450 Substrates,2019 May 22;20(3):229-235.,"Background:                    Determination or prediction of the Absorption, Distribution, Metabolism, and Excretion (ADME) properties of drug candidates and drug-induced toxicity plays crucial roles in drug discovery and development. Metabolism is one of the most complicated pharmacokinetic properties to be understood and predicted. However, experimental determination of the substrate binding, selectivity, sites and rates of metabolism is time- and recourse- consuming. In the phase I metabolism of foreign compounds (i.e., most of drugs), cytochrome P450 enzymes play a key role. To help develop drugs with proper ADME properties, computational models are highly desired to predict the ADME properties of drug candidates, particularly for drugs binding to cytochrome P450.              Objective:                    This narrative review aims to briefly summarize machine learning techniques used in the prediction of the cytochrome P450 isoform specificity of drug candidates.              Results:                    Both single-label and multi-label classification methods have demonstrated good performance on modelling and prediction of the isoform specificity of substrates based on their quantitative descriptors.              Conclusion:                    This review provides a guide for researchers to develop machine learning-based methods to predict the cytochrome P450 isoform specificity of drug candidates."
30337064,4.0,Computer-aided diagnosis of glaucoma using fundus images: A review,2018 Oct;165:1-12.,"Background and objectives:                    Glaucoma is an eye condition which leads to permanent blindness when the disease progresses to an advanced stage. It occurs due to inappropriate intraocular pressure within the eye, resulting in damage to the optic nerve. Glaucoma does not exhibit any symptoms in its nascent stage and thus, it is important to diagnose early to prevent blindness. Fundus photography is widely used by ophthalmologists to assist in diagnosis of glaucoma and is cost-effective.              Methods:                    The morphological features of the disc that is characteristic of glaucoma are clearly seen in the fundus images. However, manual inspection of the acquired fundus images may be prone to inter-observer variation. Therefore, a computer-aided detection (CAD) system is proposed to make an accurate, reliable and fast diagnosis of glaucoma based on the optic nerve features of fundus imaging. In this paper, we reviewed existing techniques to automatically diagnose glaucoma.              Results:                    The use of CAD is very effective in the diagnosis of glaucoma and can assist the clinicians to alleviate their workload significantly. We have also discussed the advantages of employing state-of-art techniques, including deep learning (DL), when developing the automated system. The DL methods are effective in glaucoma diagnosis.              Conclusions:                    Novel DL algorithms with big data availability are required to develop a reliable CAD system. Such techniques can be employed to diagnose other eye diseases accurately."
30334108,3.0,Machine Meets Biology: a Primer on Artificial Intelligence in Cardiology and Cardiac Imaging,2018 Oct 18;20(12):139.,"Purpose of review:                    An understanding of the basics concepts of deep learning can be helpful in not only understanding the potential applications of this technique but also in critically reviewing literature in which neural networks are utilized for analysis and modeling.              Recent findings:                    The term ""deep learning"" has been applied to a subset of machine learning that utilizes a ""neural network"" and is often used interchangeably with ""artificial intelligence."" It has been increasingly utilized in healthcare for computational ""learning"", especially for pattern recognition for diagnostic imaging. Another promising application is the potential for these neural networks to improve the accuracy in the identification of patients who are at risk for cardiovascular events and could benefit most from preventive treatment in comparison with more conventional statistical techniques. The importance of such tailored cardiovascular risk assessment and disease management in individual patients is far reaching given that cardiovascular disease is the leading cause of morbidity and mortality in the world. Nearly half of myocardial infarctions and strokes occur in patients who are not predicted to be at risk for cardiovascular events by current guideline-based approaches. Equally important are individuals who are not at risk for cardiovascular events and yet are given expensive and unnecessary preventive treatment with potential untoward side effects. The application of powerful artificial intelligence/deep learning tools in medicine is likely to result in more effective and efficient health care delivery with the potential for significant cost savings by shifting preventative treatment from inappropriate to appropriate patient subgroups."
30332296,15.0,State of the Art: Machine Learning Applications in Glioma Imaging,2019 Jan;212(1):26-37.,"Objective:                    Machine learning has recently gained considerable attention because of promising results for a wide range of radiology applications. Here we review recent work using machine learning in brain tumor imaging, specifically segmentation and MRI radiomics of gliomas.              Conclusion:                    We discuss available resources, state-of-the-art segmentation methods, and machine learning radiomics for glioma. We highlight the challenges of these techniques as well as the future potential in clinical diagnostics, prognostics, and decision making."
30332290,19.0,Peering Into the Black Box of Artificial Intelligence: Evaluation Metrics of Machine Learning Methods,2019 Jan;212(1):38-43.,"Objective:                    Machine learning (ML) and artificial intelligence (AI) are rapidly becoming the most talked about and controversial topics in radiology and medicine. Over the past few years, the numbers of ML- or AI-focused studies in the literature have increased almost exponentially, and ML has become a hot topic at academic and industry conferences. However, despite the increased awareness of ML as a tool, many medical professionals have a poor understanding of how ML works and how to critically appraise studies and tools that are presented to us. Thus, we present a brief overview of ML, explain the metrics used in ML and how to interpret them, and explain some of the technical jargon associated with the field so that readers with a medical background and basic knowledge of statistics can feel more comfortable when examining ML applications.              Conclusion:                    Attention to sample size, overfitting, underfitting, cross validation, as well as a broad knowledge of the metrics of machine learning, can help those with little or no technical knowledge begin to assess machine learning studies. However, transparency in methods and sharing of algorithms is vital to allow clinicians to assess these tools themselves."
30325042,5.0,Prediction of clinically relevant drug-induced liver injury from structure using machine learning,2019 Mar;39(3):412-419.,"Drug-induced liver injury (DILI) is the most common cause of acute liver failure and often responsible for drug withdrawals from the market. Clinical manifestations vary, and toxicity may or may not appear dose-dependent. We present several machine-learning models (decision tree induction, k-nearest neighbor, support vector machines, artificial neural networks) for the prediction of clinically relevant DILI based solely on drug structure, with data taken from published DILI cases. Our models achieved corrected classification rates of up to 89%. We also studied the association of a drug's interaction with carriers, enzymes and transporters, and the relationship of defined daily doses with hepatotoxicity. The results presented here are useful as a screening tool both in a clinical setting in the assessment of DILI as well as in the early stages of drug development to rule out potentially hepatotoxic candidates."
30322481,5.0,Novel Quantitative PET Techniques for Clinical Decision Support in Oncology,2018 Nov;48(6):548-564.,"Quantitative image analysis has deep roots in the usage of positron emission tomography (PET) in clinical and research settings to address a wide variety of diseases. It has been extensively employed to assess molecular and physiological biomarkers in vivo in healthy and disease states, in oncology, cardiology, neurology, and psychiatry. Quantitative PET allows relating the time-varying activity concentration in tissues/organs of interest and the basic functional parameters governing the biological processes being studied. Yet, quantitative PET is challenged by a number of degrading physical factors related to the physics of PET imaging, the limitations of the instrumentation used, and the physiological status of the patient. Moreover, there is no consensus on the most reliable and robust image-derived PET metric(s) that can be used with confidence in clinical oncology owing to the discrepancies between the conclusions reported in the literature. There is also increasing interest in the use of artificial intelligence based techniques, particularly machine learning and deep learning techniques in a variety of applications to extract quantitative features (radiomics) from PET including image segmentation and outcome prediction in clinical oncology. These novel techniques are revolutionizing clinical practice and are now offering unique capabilities to the clinical molecular imaging community and biomedical researchers at large. In this report, we summarize recent developments and future tendencies in quantitative PET imaging and present example applications in clinical decision support to illustrate its potential in the context of clinical oncology."
30319422,27.0,"Empirical Scoring Functions for Structure-Based Virtual Screening: Applications, Critical Aspects, and Challenges",2018 Sep 24;9:1089.,"Structure-based virtual screening (VS) is a widely used approach that employs the knowledge of the three-dimensional structure of the target of interest in the design of new lead compounds from large-scale molecular docking experiments. Through the prediction of the binding mode and affinity of a small molecule within the binding site of the target of interest, it is possible to understand important properties related to the binding process. Empirical scoring functions are widely used for pose and affinity prediction. Although pose prediction is performed with satisfactory accuracy, the correct prediction of binding affinity is still a challenging task and crucial for the success of structure-based VS experiments. There are several efforts in distinct fronts to develop even more sophisticated and accurate models for filtering and ranking large libraries of compounds. This paper will cover some recent successful applications and methodological advances, including strategies to explore the ligand entropy and solvent effects, training with sophisticated machine-learning techniques, and the use of quantum mechanics. Particular emphasis will be given to the discussion of critical aspects and further directions for the development of more accurate empirical scoring functions."
30317992,1.0,The Development of Machine Learning Methods in Cell-Penetrating Peptides Identification: A Brief Review,2019;20(3):217-223.,"Background:                    Cell-penetrating Peptides (CPPs) are important short peptides that facilitate cellular intake or uptake of various molecules. CPPs can transport drug molecules through the plasma membrane and send these molecules to different cellular organelles. Thus, CPP identification and related mechanisms have been extensively explored. In order to reveal the penetration mechanisms of a large number of CPPs, it is necessary to develop convenient and fast methods for CPPs identification.              Methods:                    Biochemical experiments can provide precise details for accurately identifying CPP, but these methods are expensive and laborious. To overcome these disadvantages, several computational methods have been developed to identify CPPs. We have performed review on the development of machine learning methods in CPP identification. This review provides an insight into CPP identification.              Results:                    We summarized the machine learning-based CPP identification methods and compared the construction strategies of 11 different computational methods. Furthermore, we pointed out the limitations and difficulties in predicting CPPs.              Conclusion:                    In this review, the last studies on CPP identification using machine learning method were reported. We also discussed the future development direction of CPP recognition with computational methods."
30317059,3.0,Automated seizure prediction,2018 Nov;88:251-261.,"In the past two decades, significant advances have been made on automated electroencephalogram (EEG)-based diagnosis of epilepsy and seizure detection. A number of innovative algorithms have been introduced that can aid in epilepsy diagnosis with a high degree of accuracy. In recent years, the frontiers of computational epilepsy research have moved to seizure prediction, a more challenging problem. While antiepileptic medication can result in complete seizure freedom in many patients with epilepsy, up to one-third of patients living with epilepsy will have medically intractable epilepsy, where medications reduce seizure frequency but do not completely control seizures. If a seizure can be predicted prior to its clinical manifestation, then there is potential for abortive treatment to be given, either self-administered or via an implanted device administering medication or electrical stimulation. This will have a far-reaching impact on the treatment of epilepsy and patient's quality of life. This paper presents a state-of-the-art review of recent efforts and journal articles on seizure prediction. The technologies developed for epilepsy diagnosis and seizure detection are being adapted and extended for seizure prediction. The paper ends with some novel ideas for seizure prediction using the increasingly ubiquitous machine learning technology, particularly deep neural network machine learning."
30317006,1.0,Characterization of expressed human meibum using hyperspectral stimulated Raman scattering microscopy,2019 Jan;17(1):151-159.,"Purpose:                    This study examined whether hyperspectral stimulated Raman scattering (hsSRS) microscopy can detect differences in meibum lipid to protein composition of normal and evaporative dry eye subjects with meibomian gland dysfunction.              Methods:                    Subjects were evaluated for tear breakup time (TBUT), staining, meibum expression and gland dropout. Expressed meibum was analyzed using SRS vibrational signatures in the CH stretching region (2800-3050 cm-1). Vertex component analysis and K-means clustering were used to group the spectral signatures into four fractions containing high lipid (G1) to high protein (G4).              Results:                    Thirty-three subjects could be statistically analyzed using pooled meibum (13 with stable tear films (TBUTs > 10 s) and 20 with unstable tear films (TBUTs ≤ 10 s). Significant differences in meibum from subjects with unstable vs. stable TBUTs were found for the G1 fraction (medians 0.164 and 0.020, respectively; p = 0.012) and the G2 fraction (medians 0.244 and 0.272, respectively; p = 0.045). No differences were observed for the G3 and G4 fractions. Single orifice samples were not significantly different vs. pooled samples from the fellow eye, and eyelid sector samples (nasal, central and temporal) G2:G3 fractional components were not significantly different (p = 0.449). Spearman analysis suggested a significant inverse correlation between G1 fraction and TBUT (R = -0.351; p = 0.045).              Conclusions:                    hsSRS microscopy allows compositional analysis of expressed meibum from humans which correlated to changes in TBUT. These findings support the hypothesis that hsSRS may be useful in classifying meibum quality and evaluating the effects of therapy."
30316551,3.0,A Survey on Machine Learning Approaches for Automatic Detection of Voice Disorders,2019 Nov;33(6):947.e11-947.e33.,"The human voice production system is an intricate biological device capable of modulating pitch and loudness. Inherent internal and/or external factors often damage the vocal folds and result in some change of voice. The consequences are reflected in body functioning and emotional standing. Hence, it is paramount to identify voice changes at an early stage and provide the patient with an opportunity to overcome any ramification and enhance their quality of life. In this line of work, automatic detection of voice disorders using machine learning techniques plays a key role, as it is proven to help ease the process of understanding the voice disorder. In recent years, many researchers have investigated techniques for an automated system that helps clinicians with early diagnosis of voice disorders. In this paper, we present a survey of research work conducted on automatic detection of voice disorders and explore how it is able to identify the different types of voice disorders. We also analyze different databases, feature extraction techniques, and machine learning approaches used in these research works."
30315314,5.0,A machine learning approach relating 3D body scans to body composition in humans,2019 Feb;73(2):200-208.,"A long-standing question in nutrition and obesity research involves quantifying the relationship between body fat and anthropometry. To date, the mathematical formulation of these relationships has relied on pairing easily obtained anthropometric measurements such as the body mass index (BMI), waist circumference, or hip circumference to body fat. Recent advances in 3D body shape imaging technology provides a new opportunity for quickly and accurately obtaining hundreds of anthropometric measurements within seconds, however, there does not yet exist a large diverse database that pairs these measurements to body fat. Herein, we leverage 3D scanned anthropometry obtained from a population of United States Army basic training recruits to derive four subpopulations of homogenous body shape archetypes using a combined principal components and cluster analysis. While the Army database was large and diverse, it did not have body composition measurements. Therefore, these body shape archetypes were paired to an alternate smaller sample of participants from the Pennington Biomedical Research Center in Baton Rouge, LA that were not only similarly imaged by the same 3D scanning machine, but also had concomitant measures of body composition by dual-energy X-ray absorptiometry body composition. With this enhanced ability to obtain anthropometry through 3D scanning quickly of large populations, our machine learning approach for pairing body shapes from large datasets to smaller datasets that also contain state-of-the-art body composition measurements can be extended to pair other health outcomes to 3D body shape anthropometry."
30315284,14.0,Artificial Intelligence-Assisted Gastroenterology- Promises and Pitfalls,2019 Mar;114(3):422-428.,"Technological advances in artificial intelligence (AI) represent an enticing opportunity to benefit gastroenterological practice. Moreover, AI, through machine or deep learning, permits the ability to develop predictive models from large datasets. Possibilities of predictive model development in machine learning are numerous dependent on the clinical question. For example, binary classifiers aim to stratify allocation to a categorical outcome, such as the presence or absence of a gastrointestinal disease. In addition, continuous variable fitting techniques can be used to predict quantity of a therapeutic response, thus offering a tool to predict which therapeutic intervention may be most beneficial to the given patient. Namely, this permits an important opportunity for personalization of medicine, including a movement from guideline-specific treatment algorithms to patient-specific ones, providing both clinician and patient the capacity for data-driven decision making. Furthermore, such analyses could predict the development of GI disease prior to the manifestation of symptoms, raising the possibility of prevention or pre-treatment. In addition, computer vision additionally provides an exciting opportunity in endoscopy to automatically detect lesions. In this review, we overview the recent developments in healthcare-based AI and machine learning and describe promises and pitfalls for its application to gastroenterology."
30311557,,Machine-Learning Prediction of Drug-Induced Cardiac Arrhythmia: Analysis of Gene Expression and Clustering,2018;46(3):245-275.,"A marked delay in the electrical repolarization of heart ventricles is characterized by prolongation of the Q-T wave (QT) interval on a surface electrocardiogram. Such a delay can lead to potentially life-threatening cardiac arrhythmia (torsades de pointes). Such prolongation is also a widely accepted cardiac safety biomarker in drug development. Current preclinical drug-safety assays include patch clamp analysis to evaluate drug-related blockade of cardiac repolarizing ion currents. Recently reported patch clamp assay results have shown predictive sensitivities and specificities in the ranges of 64%-82% and 75%-88%, respectively. In this project, we use a support vector machine classifier to find mean sensitivities and specificities of 85% and 90%, respectively, across 77 drug subclassifications. Clustering by gene expression profile similarities shows that drugs known to prolong the QT interval do not always form distinct groups, but the number of groups is limited. The most common biological network links associated with these groups involve genes linked with fatty acid metabolism, G proteins, intracellular glutathione, immune responses, apoptosis, mitochondrial function, electron transport, and mitogen-activated protein kinases. These results suggest that machine-learning analysis of gene expression and clustering may augment cardiac safety predictions for improving drug-safety assessments."
30310652,9.0,Big-data and machine learning to revamp computational toxicology and its use in risk assessment,2018 May 1;7(5):732-744.,"The creation of large toxicological databases and advances in machine-learning techniques have empowered computational approaches in toxicology. Work with these large databases based on regulatory data has allowed reproducibility assessment of animal models, which highlight weaknesses in traditional in vivo methods. This should lower the bars for the introduction of new approaches and represents a benchmark that is achievable for any alternative method validated against these methods. Quantitative Structure Activity Relationships (QSAR) models for skin sensitization, eye irritation, and other human health hazards based on these big databases, however, also have made apparent some of the challenges facing computational modeling, including validation challenges, model interpretation issues, and model selection issues. A first implementation of machine learning-based predictions termed REACHacross achieved unprecedented sensitivities of >80% with specificities >70% in predicting the six most common acute and topical hazards covering about two thirds of the chemical universe. While this is awaiting formal validation, it demonstrates the new quality introduced by big data and modern data-mining technologies. The rapid increase in the diversity and number of computational models, as well as the data they are based on, create challenges and opportunities for the use of computational methods."
30308967,7.0,Chemical Diversity of Metal Sulfide Minerals and Its Implications for the Origin of Life,2018 Oct 10;8(4):46.,"Prebiotic organic synthesis catalyzed by Earth-abundant metal sulfides is a key process for understanding the evolution of biochemistry from inorganic molecules, yet the catalytic functions of sulfides have remained poorly explored in the context of the origin of life. Past studies on prebiotic chemistry have mostly focused on a few types of metal sulfide catalysts, such as FeS or NiS, which form limited types of products with inferior activity and selectivity. To explore the potential of metal sulfides on catalyzing prebiotic chemical reactions, here, the chemical diversity (variations in chemical composition and phase structure) of 304 natural metal sulfide minerals in a mineralogy database was surveyed. Approaches to rationally predict the catalytic functions of metal sulfides are discussed based on advanced theories and analytical tools of electrocatalysis such as proton-coupled electron transfer, structural comparisons between enzymes and minerals, and in situ spectroscopy. To this end, we introduce a model of geoelectrochemistry driven prebiotic synthesis for chemical evolution, as it helps us to predict kinetics and selectivity of targeted prebiotic chemistry under ""chemically messy conditions"". We expect that combining the data-mining of mineral databases with experimental methods, theories, and machine-learning approaches developed in the field of electrocatalysis will facilitate the prediction and verification of catalytic performance under a wide range of pH and Eh conditions, and will aid in the rational screening of mineral catalysts involved in the origin of life."
30308542,2.0,Who is a high-risk surgical patient?,2018 Dec;24(6):547-553.,"Purpose of review:                    Timely identification of high-risk surgical candidates facilitate surgical decision-making and allows appropriate tailoring of perioperative management strategies. This review aims to summarize the recent advances in perioperative risk stratification.              Recent findings:                    Use of indices which include various combinations of preoperative and postoperative variables remain the most commonly used risk-stratification strategy. Incorporation of biomarkers (troponin and natriuretic peptides), comprehensive objective assessment of functional capacity, and frailty into the current framework enhance perioperative risk estimation. Intraoperative hemodynamic parameters can provide further signals towards identifying patients at risk of adverse postoperative outcomes. Implementation of machine-learning algorithms is showing promising results in real-time forecasting of perioperative outcomes.              Summary:                    Perioperative risk estimation is multidimensional including validated indices, biomarkers, functional capacity estimation, and intraoperative hemodynamics. Identification and implementation of targeted strategies which mitigate predicted risk remains a greater challenge."
30307875,3.0,Advances in Acoustic Signal Processing Techniques for Enhanced Bowel Sound Analysis,2019;12:240-253.,"With the invention of the electronic stethoscope and other similar recording and data logging devices, acoustic signal processing concepts and methods can now be applied to bowel sounds. In this paper, the literature pertaining to acoustic signal processing for bowel sound analysis is reviewed and discussed. The paper outlines some of the fundamental approaches and machine learning principles that may be used in bowel sound analysis. The advances in signal processing techniques that have allowed useful information to be obtained from bowel sounds from a historical perspective are provided. The document specifically address the progress in bowel sound analysis, such as improved noise reduction, segmentation, signal enhancement, feature extraction, localization of sounds, and machine learning techniques. We have found that advanced acoustic signal processing incorporating novel machine learning methods and artificial intelligence can lead to better interpretation of acoustic information emanating from the bowel."
30306477,,3D Ultrasound for Orthopedic Interventions,2018;1093:113-129.,"Ultrasound is a real-time, non-radiation-based imaging modality with an ability to acquire two-dimensional (2D) and three-dimensional (3D) data. Due to these capabilities, research has been carried out in order to incorporate it as an intraoperative imaging modality for various orthopedic surgery procedures. However, high levels of noise, different imaging artifacts, and bone surfaces appearing blurred with several mm in thickness have prohibited the widespread use of ultrasound as a standard of care imaging modality in orthopedics. In this chapter, we provided a detailed overview of numerous applications of 3D ultrasound in the domain of orthopedic surgery. Specifically, we discuss the advantages and disadvantages of methods proposed for segmentation and enhancement of bone ultrasound data and the successful application of these methods in clinical domain. Finally, a number of challenges are identified which need to be overcome in order for ultrasound to become a preferred imaging modality in orthopedics."
30306468,1.0,Computer-Aided Orthopaedic Surgery: State-of-the-Art and Future Perspectives,2018;1093:1-20.,"Introduced more than two decades ago, computer-aided orthopaedic surgery (CAOS) has emerged as a new and independent area, due to the importance of treatment of musculoskeletal diseases in orthopaedics and traumatology, increasing availability of different imaging modalities and advances in analytics and navigation tools. The aim of this chapter is to present the basic elements of CAOS devices and to review state-of-the-art examples of different imaging modalities used to create the virtual representations, of different position tracking devices for navigation systems, of different surgical robots, of different methods for registration and referencing, and of CAOS modules that have been realized for different surgical procedures. Future perspectives will be outlined. It is expected that the recent advancement on smart instrumentation, medical robotics, artificial intelligence, machine learning, and deep learning techniques, in combination with big data analytics, may lead to smart CAOS systems and intelligent orthopaedics in the near future."
30303758,5.0,Hippocampal replays under the scrutiny of reinforcement learning models,2018 Dec 1;120(6):2877-2896.,"Multiple in vivo studies have shown that place cells from the hippocampus replay previously experienced trajectories. These replays are commonly considered to mainly reflect memory consolidation processes. Some data, however, have highlighted a functional link between replays and reinforcement learning (RL). This theory, extensively used in machine learning, has introduced efficient algorithms and can explain various behavioral and physiological measures from different brain regions. RL algorithms could constitute a mechanistic description of replays and explain how replays can reduce the number of iterations required to explore the environment during learning. We review the main findings concerning the different hippocampal replay types and the possible associated RL models (either model-based, model-free, or hybrid model types). We conclude by tying these frameworks together. We illustrate the link between data and RL through a series of model simulations. This review, at the frontier between informatics and biology, paves the way for future work on replays."
30301571,14.0,Deep Learning with Microfluidics for Biotechnology,2019 Mar;37(3):310-324.,"Advances in high-throughput and multiplexed microfluidics have rewarded biotechnology researchers with vast amounts of data but not necessarily the ability to analyze complex data effectively. Over the past few years, deep artificial neural networks (ANNs) leveraging modern graphics processing units (GPUs) have enabled the rapid analysis of structured input data - sequences, images, videos - to predict complex outputs with unprecedented accuracy. While there have been early successes in flow cytometry, for example, the extensive potential of pairing microfluidics (to acquire data) and deep learning (to analyze data) to tackle biotechnology challenges remains largely untapped. Here we provide a roadmap to integrating deep learning and microfluidics in biotechnology laboratories that matches computational architectures to problem types, and provide an outlook on emerging opportunities."
30298337,44.0,Medical Image Analysis using Convolutional Neural Networks: A Review,2018 Oct 8;42(11):226.,"The science of solving clinical problems by analyzing images generated in clinical practice is known as medical image analysis. The aim is to extract information in an affective and efficient manner for improved clinical diagnosis. The recent advances in the field of biomedical engineering have made medical image analysis one of the top research and development area. One of the reasons for this advancement is the application of machine learning techniques for the analysis of medical images. Deep learning is successfully used as a tool for machine learning, where a neural network is capable of automatically learning features. This is in contrast to those methods where traditionally hand crafted features are used. The selection and calculation of these features is a challenging task. Among deep learning techniques, deep convolutional networks are actively used for the purpose of medical image analysis. This includes application areas such as segmentation, abnormality detection, disease classification, computer aided diagnosis and retrieval. In this study, a comprehensive review of the current state-of-the-art in medical image analysis using deep convolutional networks is presented. The challenges and potential of these techniques are also highlighted."
30298124,5.0,Classification of Pediatric Asthma: From Phenotype Discovery to Clinical Practice,2018 Sep 20;6:258.,"Advances in big data analytics have created an opportunity for a step change in unraveling mechanisms underlying the development of complex diseases such as asthma, providing valuable insights that drive better diagnostic decision-making in clinical practice, and opening up paths to individualized treatment plans. However, translating findings from data-driven analyses into meaningful insights and actionable solutions requires approaches and tools which move beyond mining and patterning longitudinal data. The purpose of this review is to summarize recent advances in phenotyping of asthma, to discuss key hurdles currently hampering the translation of phenotypic variation into mechanistic insights and clinical setting, and to suggest potential solutions that may address these limitations and accelerate moving discoveries into practice. In order to advance the field of phenotypic discovery, greater focus should be placed on investigating the extent of within-phenotype variation. We advocate a more cautious modeling approach by ""supervising"" the findings to delineate more precisely the characteristics of the individual trajectories assigned to each phenotype. Furthermore, it is important to employ different methods within a study to compare the stability of derived phenotypes, and to assess the immutability of individual assignments to phenotypes. If we are to make a step change toward precision (stratified or personalized) medicine and capitalize on the available big data assets, we have to develop genuine cross-disciplinary collaborations, wherein data scientists who turn data into information using algorithms and machine learning, team up with medical professionals who provide deep insights on specific subjects from a clinical perspective."
30297207,3.0,'Nonlinear' Biochemistry of Nucleosome Detergents,2018 Dec;43(12):951-959.,"The transcriptional activation domains (TADs) are critical for life, yet intrinsically disordered polypeptides with no specific consensus sequence, interacting with multiple targets via low-specificity fuzzy contacts. The recent integration of machine learning approaches in biochemistry allows analysis of large experimental datasets of functional TADs as a whole and clear observation of TAD features. The emerging picture describes TADs as sequences without consensus but with a variety of detergent-like mini-motifs enriched in negatively charged and aromatic amino acids. Comparison of the canonical direct coactivator recruitment model and a new model describing TADs as nucleosome detergents that trigger chromatin remodeling during gene activation helps solve a fundamental enigma of molecular biology spanning 30 years."
30295871,48.0,Multi-omic and multi-view clustering algorithms: review and cancer benchmark,2018 Nov 16;46(20):10546-10562.,"Recent high throughput experimental methods have been used to collect large biomedical omics datasets. Clustering of single omic datasets has proven invaluable for biological and medical research. The decreasing cost and development of additional high throughput methods now enable measurement of multi-omic data. Clustering multi-omic data has the potential to reveal further systems-level insights, but raises computational and biological challenges. Here, we review algorithms for multi-omics clustering, and discuss key issues in applying these algorithms. Our review covers methods developed specifically for omic data as well as generic multi-view methods developed in the machine learning community for joint clustering of multiple data types. In addition, using cancer data from TCGA, we perform an extensive benchmark spanning ten different cancer types, providing the first systematic comparison of leading multi-omics and multi-view clustering algorithms. The results highlight key issues regarding the use of single- versus multi-omics, the choice of clustering strategy, the power of generic multi-view methods and the use of approximated p-values for gauging solution quality. Due to the growing use of multi-omics data, we expect these issues to be important for future progress in the field."
30293864,2.0,A review on microelectrode recording selection of features for machine learning in deep brain stimulation surgery for Parkinson's disease,2019 Jan;130(1):145-154.,"Objective:                    This study seeks to systematically review the selection of features and algorithms for machine learning and automation in deep brain stimulation surgery (DBS) for Parkinson's disease. This will assist in consolidating current knowledge and accuracy levels to allow greater understanding and research to be performed in automating this process, which could lead to improved clinical outcomes.              Methods:                    A systematic literature review search was conducted for all studies that utilized machine learning and DBS in Parkinson's disease.              Results:                    Ten studies were identified from 2006 utilizing machine learning in DBS surgery for Parkinson's disease. Different combinations of both spike independent and spike dependent features have been utilized with different machine learning algorithms to attempt to delineate the subthalamic nucleus (STN) and its surrounding structures.              Conclusion:                    The state-of-the-art algorithms achieve good accuracy and error rates with relatively short computing time, however, the currently achievable accuracy is not sufficiently robust enough for clinical practice. Moreover, further research is required for identifying subterritories of the STN.              Significance:                    This is a comprehensive summary of current machine learning algorithms that discriminate the STN and its adjacent structures for DBS surgery in Parkinson's disease."
30290208,9.0,Translational machine learning for psychiatric neuroimaging,2019 Apr 20;91:113-121.,"Despite its initial promise, neuroimaging has not been widely translated into clinical psychiatry to assist in the prediction of diagnoses, prognoses, and optimal therapeutic strategies. Machine learning approaches may enhance the translational potential of neuroimaging because they specifically focus on overcoming biases by optimizing the generalizability of pipelines that measure complex brain patterns to predict targets at a single-subject level. This article introduces some fundamentals of a translational machine learning approach before selectively reviewing literature to-date. Promising initial results are then balanced by the description of limitations that should be considered in order to interpret existing research and maximize the possibility of future translation. Future directions are then presented in order to inspire further research and progress the field towards clinical translation."
30287797,11.0,Machine Learning Approaches for Protein⁻Protein Interaction Hot Spot Prediction: Progress and Comparative Assessment,2018 Oct 4;23(10):2535.,"Hot spots are the subset of interface residues that account for most of the binding free energy, and they play essential roles in the stability of protein binding. Effectively identifying which specific interface residues of protein⁻protein complexes form the hot spots is critical for understanding the principles of protein interactions, and it has broad application prospects in protein design and drug development. Experimental methods like alanine scanning mutagenesis are labor-intensive and time-consuming. At present, the experimentally measured hot spots are very limited. Hence, the use of computational approaches to predicting hot spots is becoming increasingly important. Here, we describe the basic concepts and recent advances of machine learning applications in inferring the protein⁻protein interaction hot spots, and assess the performance of widely used features, machine learning algorithms, and existing state-of-the-art approaches. We also discuss the challenges and future directions in the prediction of hot spots."
30286415,15.0,A systematic meta-review of predictors of antidepressant treatment outcome in major depressive disorder,2019 Jan 15;243:503-515.,"Introduction:                    The heterogeneity of symptoms and complex etiology of depression pose a significant challenge to the personalization of treatment. Meanwhile, the current application of generic treatment approaches to patients with vastly differing biological and clinical profiles is far from optimal. Here, we conduct a meta-review to identify predictors of response to antidepressant therapy in order to select robust input features for machine learning models of treatment response. These machine learning models will allow us to learn associations between patient features and treatment response which have predictive value at the individual patient level; this learning can be optimized by selecting high-quality input features for the model. While current research is difficult to directly apply to the clinic, machine learning models built using knowledge gleaned from current research may become useful clinical tools.              Methods:                    The EMBASE and MEDLINE/PubMed online databases were searched from January 1996 to August 2017, using a combination of MeSH terms and keywords to identify relevant literature reviews. We identified a total of 1909 articles, wherein 199 articles met our inclusion criteria.              Results:                    An array of genetic, immune, endocrine, neuroimaging, sociodemographic, and symptom-based predictors of treatment response were extracted, varying widely in clinical utility.              Limitations:                    Due to heterogeneous sample sizes, effect sizes, publication biases, and methodological disparities across reviews, we could not accurately assess the strength and directionality of every predictor.              Conclusion:                    Notwithstanding our cautious interpretation of the results, we have identified a multitude of predictors that can be used to formulate a priori hypotheses regarding the input features for a computational model. We highlight the importance of large-scale research initiatives and clinically accessible biomarkers, as well as the need for replication studies of current findings. In addition, we provide recommendations for future improvement and standardization of research efforts in this field."
30279002,28.0,"Machine learning in human movement biomechanics: Best practices, common pitfalls, and new opportunities",2018 Nov 16;81:1-11.,"Traditional laboratory experiments, rehabilitation clinics, and wearable sensors offer biomechanists a wealth of data on healthy and pathological movement. To harness the power of these data and make research more efficient, modern machine learning techniques are starting to complement traditional statistical tools. This survey summarizes the current usage of machine learning methods in human movement biomechanics and highlights best practices that will enable critical evaluation of the literature. We carried out a PubMed/Medline database search for original research articles that used machine learning to study movement biomechanics in patients with musculoskeletal and neuromuscular diseases. Most studies that met our inclusion criteria focused on classifying pathological movement, predicting risk of developing a disease, estimating the effect of an intervention, or automatically recognizing activities to facilitate out-of-clinic patient monitoring. We found that research studies build and evaluate models inconsistently, which motivated our discussion of best practices. We provide recommendations for training and evaluating machine learning models and discuss the potential of several underutilized approaches, such as deep learning, to generate new knowledge about human movement. We believe that cross-training biomechanists in data science and a cultural shift toward sharing of data and tools are essential to maximize the impact of biomechanics research."
30277175,5.0,"Review: Precision nutrition of ruminants: approaches, challenges and potential gains",2018 Dec;12(s2):s246-s261.,"A plethora of sensors and information technologies with applications to the precision nutrition of herbivores have been developed and continue to be developed. The nutritional processes start outside of the animal body with the available feed (quantity and quality) and continue inside it once the feed is consumed, degraded in the gastrointestinal tract and metabolised by organs and tissues. Finally, some nutrients are wasted via urination, defecation and gaseous emissions through breathing and belching whereas remaining nutrients ensure maintenance and production. Nowadays, several processes can be monitored in real-time using new technologies, but although these provide valuable data 'as is', further gains could be obtained using this information as inputs to nutrition simulation models to predict unmeasurable variables in real-time and to forecast outcomes of interest. Data provided by sensors can create synergies with simulation models and this approach has the potential to expand current applications. In addition, data provided by sensors could be used with advanced analytical techniques such as data fusion, optimisation techniques and machine learning to improve their value for applications in precision animal nutrition. The present paper reviews technologies that can monitor different nutritional processes relevant to animal production, profitability, environmental management and welfare. We discussed the model-data fusion approach in which data provided by sensor technologies can be used as input of nutrition simulation models in near-real time to produce more accurate, certain and timely predictions. We also discuss some examples that have taken this model-data fusion approach to complement the capabilities of both models and sensor data, and provided examples such as predicting feed intake and methane emissions. Challenges with automatising the nutritional management of individual animals include monitoring and predicting of the flow of nutrients including nutrient intake, quantity and composition of body growth and milk production, gestation, maintenance and physical activities at the individual animal level. We concluded that the livestock industries are already seeing benefits from the development of sensor and information technologies, and this benefit is expected to grow exponentially soon with the integration of nutrition simulation models and techniques for big data analysis. However, this approach may need re-evaluating or performing new empirical research in both fields of animal nutrition and simulation modelling to accommodate a new type of data provided by the sensor technologies."
30277150,5.0,A Survey for Predicting Enzyme Family Classes Using Machine Learning Methods,2019;20(5):540-550.,"Enzymes are proteins that act as biological catalysts to speed up cellular biochemical processes. According to their main Enzyme Commission (EC) numbers, enzymes are divided into six categories: EC-1: oxidoreductase; EC-2: transferase; EC-3: hydrolase; EC-4: lyase; EC-5: isomerase and EC-6: synthetase. Different enzymes have different biological functions and acting objects. Therefore, knowing which family an enzyme belongs to can help infer its catalytic mechanism and provide information about the relevant biological function. With the large amount of protein sequences influxing into databanks in the post-genomics age, the annotation of the family for an enzyme is very important. Since the experimental methods are cost ineffective, bioinformatics tool will be a great help for accurately classifying the family of the enzymes. In this review, we summarized the application of machine learning methods in the prediction of enzyme family from different aspects. We hope that this review will provide insights and inspirations for the researches on enzyme family classification."
30275936,78.0,Machine Learning Methods for Histopathological Image Analysis,2018 Feb 9;16:34-42.,"Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions."
30269984,4.0,Patient-Driven Diabetes Care of the Future in the Technology Era,2019 Mar 5;29(3):564-575.,"The growing burden of diabetes is fueled by obesity-inducing lifestyle behaviors including high-calorie diets and lack of physical activity. Challenges in access to diabetes specialists and educators, low adherence to medications, and inadequate motivational support for proper disease self-management contribute to poor glycemic control in patients with diabetes. Simultaneously, high patient volumes and low reimbursement rates limit physicians' time spent on lifestyle behavior counseling. These barriers to efficient diabetes care lead to high rates of diabetes-related complications, driving healthcare costs up and reducing the quality of patients' lives. Considering recent advancements in healthcare delivery technologies such as smartphone applications, telemedicine, m-health, device connectivity, machine-learning technology, and artificial intelligence, there is significant opportunity to achieve better efficiency in diabetes care and increase patient involvement in diabetes self-management, which ultimately may put an end to soaring diabetes-related healthcare expenditures. This review explores the patient-driven diabetes care of the future in the technology era."
30267935,3.0,Structural biology meets data science: does anything change?,2018 Oct;52:95-102.,"Data science has emerged from the proliferation of digital data, coupled with advances in algorithms, software and hardware (e.g., GPU computing). Innovations in structural biology have been driven by similar factors, spurring us to ask: can these two fields impact one another in deep and hitherto unforeseen ways? We posit that the answer is yes. New biological knowledge lies in the relationships between sequence, structure, function and disease, all of which play out on the stage of evolution, and data science enables us to elucidate these relationships at scale. Here, we consider the above question from the five key pillars of data science: acquisition, engineering, analytics, visualization and policy, with an emphasis on machine learning as the premier analytics approach."
30265280,22.0,Deep learning in omics: a survey and guideline,2019 Feb 14;18(1):41-57.,"Omics, such as genomics, transcriptome and proteomics, has been affected by the era of big data. A huge amount of high dimensional and complex structured data has made it no longer applicable for conventional machine learning algorithms. Fortunately, deep learning technology can contribute toward resolving these challenges. There is evidence that deep learning can handle omics data well and resolve omics problems. This survey aims to provide an entry-level guideline for researchers, to understand and use deep learning in order to solve omics problems. We first introduce several deep learning models and then discuss several research areas which have combined omics and deep learning in recent years. In addition, we summarize the general steps involved in using deep learning which have not yet been systematically discussed in the existent literature on this topic. Finally, we compare the features and performance of current mainstream open source deep learning frameworks and present the opportunities and challenges involved in deep learning. This survey will be a good starting point and guideline for omics researchers to understand deep learning."
30255463,17.0,Deep learning with convolutional neural network for objective skill evaluation in robot-assisted surgery,2018 Dec;13(12):1959-1970.,"Purpose:                    With the advent of robot-assisted surgery, the role of data-driven approaches to integrate statistics and machine learning is growing rapidly with prominent interests in objective surgical skill assessment. However, most existing work requires translating robot motion kinematics into intermediate features or gesture segments that are expensive to extract, lack efficiency, and require significant domain-specific knowledge.              Methods:                    We propose an analytical deep learning framework for skill assessment in surgical training. A deep convolutional neural network is implemented to map multivariate time series data of the motion kinematics to individual skill levels.              Results:                    We perform experiments on the public minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our proposed learning model achieved competitive accuracies of 92.5%, 95.4%, and 91.3%, in the standard training tasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need of engineered features or carefully tuned gesture segmentation, our model can successfully decode skill information from raw motion profiles via end-to-end learning. Meanwhile, the proposed model is able to reliably interpret skills within a 1-3 second window, without needing an observation of entire training trial.              Conclusion:                    This study highlights the potential of deep architectures for efficient online skill assessment in modern surgical training."
30254580,10.0,Optimizing Clinical Assessments in Parkinson's Disease Through the Use of Wearable Sensors and Data Driven Modeling,2018 Sep 11;12:72.,"The emergence of motion sensors as a tool that provides objective motor performance data on individuals afflicted with Parkinson's disease offers an opportunity to expand the horizon of clinical care for this neurodegenerative condition. Subjective clinical scales and patient based motor diaries have limited clinometric properties and produce a glimpse rather than continuous real time perspective into motor disability. Furthermore, the expansion of machine learn algorithms is yielding novel classification and probabilistic clinical models that stand to change existing treatment paradigms, refine the application of advance therapeutics, and may facilitate the development and testing of disease modifying agents for this disease. We review the use of inertial sensors and machine learning algorithms in Parkinson's disease."
30253869,6.0,"Parkinson's disease: Cause factors, measurable indicators, and early diagnosis",2018 Nov 1;102:234-241.,"Parkinson's disease (PD) is a neurodegenerative disease of the central nervous system caused due to the loss of dopaminergic neurons. It is classified under movement disorder as patients with PD present with tremor, rigidity, postural changes, and a decrease in spontaneous movements. Comorbidities including anxiety, depression, fatigue, and sleep disorders are observed prior to the diagnosis of PD. Gene mutations, exposure to toxic substances, and aging are considered as the causative factors of PD even though its genesis is unknown. This paper reviews PD etiologies, progression, and in particular measurable indicators of PD such as neuroimaging and electrophysiology modalities. In addition to gene therapy, neuroprotective, pharmacological, and neural transplantation treatments, researchers are actively aiming at identifying biological markers of PD with the goal of early diagnosis. Neuroimaging modalities used together with advanced machine learning techniques offer a promising path for the early detection and intervention in PD patients."
30248307,10.0,The current state of artificial intelligence in ophthalmology,Mar-Apr 2019;64(2):233-240.,"Artificial intelligence (AI) is a branch of computer science that deals with the development of algorithms that seek to simulate human intelligence. We provide an overview of the basic principles in AI that are essential to the understanding of AI and its application in health care. We also present a descriptive analysis of the current state of AI in various fields of medicine, especially ophthalmology. Finally, we review the potential limitations and challenges that come along with the development and implementation of this new technology that will likely play a major role in clinical medicine in the near future."
30247662,14.0,Radiomics with artificial intelligence for precision medicine in radiation therapy,2019 Jan 1;60(1):150-157.,"Recently, the concept of radiomics has emerged from radiation oncology. It is a novel approach for solving the issues of precision medicine and how it can be performed, based on multimodality medical images that are non-invasive, fast and low in cost. Radiomics is the comprehensive analysis of massive numbers of medical images in order to extract a large number of phenotypic features (radiomic biomarkers) reflecting cancer traits, and it explores the associations between the features and patients' prognoses in order to improve decision-making in precision medicine. Individual patients can be stratified into subtypes based on radiomic biomarkers that contain information about cancer traits that determine the patient's prognosis. Machine-learning algorithms of AI are boosting the powers of radiomics for prediction of prognoses or factors associated with treatment strategies, such as survival time, recurrence, adverse events, and subtypes. Therefore, radiomic approaches, in combination with AI, may potentially enable practical use of precision medicine in radiation therapy by predicting outcomes and toxicity for individual patients."
30246637,1.0,Towards Computational Models of Identifying Protein Ubiquitination Sites,2019;20(5):565-578.,"Ubiquitination is an important post-translational modification (PTM) process for the regulation of protein functions, which is associated with cancer, cardiovascular and other diseases. Recent initiatives have focused on the detection of potential ubiquitination sites with the aid of physicochemical test approaches in conjunction with the application of computational methods. The identification of ubiquitination sites using laboratory tests is especially susceptible to the temporality and reversibility of the ubiquitination processes, and is also costly and time-consuming. It has been demonstrated that computational methods are effective in extracting potential rules or inferences from biological sequence collections. Up to the present, the computational strategy has been one of the critical research approaches that have been applied for the identification of ubiquitination sites, and currently, there are numerous state-of-the-art computational methods that have been developed from machine learning and statistical analysis to undertake such work. In the present study, the construction of benchmark datasets is summarized, together with feature representation methods, feature selection approaches and the classifiers involved in several previous publications. In an attempt to explore pertinent development trends for the identification of ubiquitination sites, an independent test dataset was constructed and the predicting results obtained from five prediction tools are reported here, together with some related discussions."
30240882,6.0,A review study: Computational techniques for expecting the impact of non-synonymous single nucleotide variants in human diseases,2019 Jan 5;680:20-33.,"Non-Synonymous Single-Nucleotide Variants (nsSNVs) and mutations can create a diversity effect on proteins as changing genotype and phenotype, which interrupts its stability. The alterations in the protein stability may cause diseases like cancer. Discovering of nsSNVs and mutations can be a useful tool for diagnosing the disease at a beginning stage. Many studies introduced the various predicting singular and consensus tools that based on different Machine Learning Techniques (MLTs) using diverse datasets. Therefore, we introduce the current comprehensive review of the most popular and recent unique tools that predict pathogenic variations and Meta-tool that merge some of them for enhancing their predictive power. Also, we scanned the several types computational techniques in the state-of-the-art and methods for predicting the effect both of coding and noncoding variants. We then displayed, the protein stability predictors. We offer the details of the most common benchmark database for variations including the main predictive features used by the different methods. Finally, we address the most common fundamental criteria for performance assessment of predictive tools. This review is targeted at bioinformaticians attentive in the characterization of regulatory variants, geneticists, molecular biologists attentive in understanding more about the nature and effective role of such variants from a functional point of views, and clinicians who may hope to learn about variants in human associated with a specific disease and find out what to do next to uncover how they impact on the underlying mechanisms."
30240646,8.0,Data and Power Efficient Intelligence with Neuromorphic Learning Machines,2018 Jul 27;5:52-68.,"The success of deep networks and recent industry involvement in brain-inspired computing is igniting a widespread interest in neuromorphic hardware that emulates the biological processes of the brain on an electronic substrate. This review explores interdisciplinary approaches anchored in machine learning theory that enable the applicability of neuromorphic technologies to real-world, human-centric tasks. We find that (1) recent work in binary deep networks and approximate gradient descent learning are strikingly compatible with a neuromorphic substrate; (2) where real-time adaptability and autonomy are necessary, neuromorphic technologies can achieve significant advantages over main-stream ones; and (3) challenges in memory technologies, compounded by a tradition of bottom-up approaches in the field, block the road to major breakthroughs. We suggest that a neuromorphic learning framework, tuned specifically for the spatial and temporal constraints of the neuromorphic substrate, will help guiding hardware algorithm co-design and deploying neuromorphic hardware for proactive learning of real-world data."
30240512,3.0,Applications of mechanistic modelling to clinical and experimental immunology: an emerging technology to accelerate immunotherapeutic discovery and development,2018 Sep;193(3):284-292.,"The application of in-silico modelling is beginning to emerge as a key methodology to advance our understanding of mechanisms of disease pathophysiology and related drug action, and in the design of experimental medicine and clinical studies. From this perspective, we will present a non-technical discussion of a small number of recent and historical applications of mathematical, statistical and computational modelling to clinical and experimental immunology. We focus specifically upon mechanistic questions relating to human viral infection, tumour growth and metastasis and T cell activation. These exemplar applications highlight the potential of this approach to impact upon human immunology informed by ever-expanding experimental, clinical and 'omics' data. Despite the capacity of mechanistic modelling to accelerate therapeutic discovery and development and to de-risk clinical trial design, it is not utilized widely across the field. We outline ongoing challenges facing the integration of mechanistic modelling with experimental and clinical immunology, and suggest how these may be overcome. Advances in key technologies, including multi-scale modelling, machine learning and the wealth of 'omics' data sets, coupled with advancements in computational capacity, are providing the basis for mechanistic modelling to impact on immunotherapeutic discovery and development during the next decade."
30238167,5.0,"Dermatoscopy of Neoplastic Skin Lesions: Recent Advances, Updates, and Revisions",2018 Sep 20;19(11):56.,"Dermatoscopy (dermoscopy) improves the diagnosis of benign and malignant cutaneous neoplasms in comparison with examination with the unaided eye and should be used routinely for all pigmented and non-pigmented cutaneous neoplasms. It is especially useful for the early stage of melanoma when melanoma-specific criteria are invisible to the unaided eye. Preselection by the unaided eye is therefore not recommended. The increased availability of polarized dermatoscopes, and the extended use of dermatoscopy in non-pigmented lesions led to the discovery of new criteria, and we recommend that lesions should be examined with polarized and non-polarized dermatoscopy. The ""chaos and clues algorithm"" is a good starting point for beginners because it is easy to use, accurate, and it works for all types of pigmented lesions not only for those melanocytic. Physicians, who use dermatoscopy routinely, should be aware of new clues for acral melanomas, nail matrix melanomas, melanoma in situ, and nodular melanoma. Dermatoscopy should also be used to distinguish between different subtypes of basal cell carcinoma and to discriminate highly from poorly differentiated squamous cell carcinomas to optimize therapy and management of non-melanoma skin cancer. One of the most exciting areas of research is the use of dermatoscopic images for machine learning and automated diagnosis. Convolutional neural networks trained with dermatoscopic images are able to diagnose pigmented lesions with the same accuracy as human experts. We humans should not be afraid of this new and exciting development because it will most likely lead to a peaceful and fruitful coexistence of human experts and decision support systems."
30237869,15.0,POLLAR: Impact of air POLLution on Asthma and Rhinitis; a European Institute of Innovation and Technology Health (EIT Health) project,2018 Sep 17;8:36.,"Allergic rhinitis (AR) is impacted by allergens and air pollution but interactions between air pollution, sleep and allergic diseases are insufficiently understood. POLLAR (Impact of air POLLution on sleep, Asthma and Rhinitis) is a project of the European Institute of Innovation and Technology (EIT Health). It will use a freely-existing application for AR monitoring that has been tested in 23 countries (the Allergy Diary, iOS and Android, 17,000 users, TLR8). The Allergy Diary will be combined with a new tool allowing queries on allergen, pollen (TLR2), sleep quality and disorders (TRL2) as well as existing longitudinal and geolocalized pollution data. Machine learning will be used to assess the relationship between air pollution, sleep and AR comparing polluted and non-polluted areas in 6 EU countries. Data generated in 2018 will be confirmed in 2019 and extended by the individual prospective assessment of pollution (portable sensor, TLR7) in AR. Sleep apnea patients will be used as a demonstrator of sleep disorder that can be modulated in terms of symptoms and severity by air pollution and AR. The geographic information system GIS will map the results. Consequences on quality of life (EQ-5D), asthma, school, work and sleep will be monitored and disseminated towards the population. The impacts of POLLAR will be (1) to propose novel care pathways integrating pollution, sleep and patients' literacy, (2) to study sleep consequences of pollution and its impact on frequent chronic diseases, (3) to improve work productivity, (4) to propose the basis for a sentinel network at the EU level for pollution and allergy, (5) to assess the societal implications of the interaction. MASK paper N°32."
30233373,4.0,Future Information Technology Tools for Fighting Substandard and Falsified Medicines in Low- and Middle-Income Countries,2018 Aug 31;9:995.,"Substandard and falsified (SF) medicines have emerged as a global public health issue within the last two decades especially in low- and middle-income countries (LMICs). Serious consequences of this problem include a loss of trust and increased financial costs due to less disease control and more frequent complications during therapy. Of note, antimicrobial resistance is an additional long-term implication of poor-quality antimicrobials. This review covers information technology tools including medicines authentication tools (MAT) as mobile apps and messaging service, 2D barcoding approaches with drug safety alert systems, web based drug safety alerts, radiofrequency identification tags, databases to support visual inspection, digital aids to enhance the performance of quality evaluation kits, reference libraries for identification of falsified and substandard medicines, and quality evaluation kits based on machine learning for field testing. While being easy to access and simple to use, these initiatives are gaining acceptance in LMICs. Implementing 2D barcoding based on end-to-end verification and ""Track and Trace"" systems has emerged as a step toward global security in the supply chain. A breakthrough in web-based drug safety alert systems and data bases was the establishment of the Global Surveillance and Monitoring System by the World Health Organization in 2013. Future applications include concepts including ""lab on a chip"" and ""paper analytical devices"" and are claimed to be convenient and simple to use as well as affordable. The principles discussed herein are making profound impact in the fight against substandard and falsified medicines, offering cheap and accessible solutions."
30230414,7.0,An accessible and efficient autism screening method for behavioural data and predictive analyses,2019 Dec;25(4):1739-1755.,"Autism spectrum disorder is associated with significant healthcare costs, and early diagnosis can substantially reduce these. Unfortunately, waiting times for an autism spectrum disorder diagnosis are lengthy due to the fact that current diagnostic procedures are time-consuming and not cost-effective. Overall, the economic impact of autism and the increase in the number of autism spectrum disorder cases across the world reveal an urgent need for the development of easily implemented and effective screening methods. This article proposes a new mobile application to overcome the problem by offering users and the health community a friendly, time-efficient and accessible mobile-based autism spectrum disorder screening tool called ASDTests. The proposed ASDTests app can be used by health professionals to assist their practice or to inform individuals whether they should pursue formal clinical diagnosis. Unlike existing autism screening apps being tested, the proposed app covers a larger audience since it contains four different tests, one each for toddlers, children, adolescents and adults as well as being available in 11 different languages. More importantly, the proposed app is a vital tool for data collection related to autism spectrum disorder for toddlers, children, adolescent and adults since initially over 1400 instances of cases and controls have been collected. Feature and predictive analyses demonstrate small groups of autistic traits improving the efficiency and accuracy of screening processes. In addition, classifiers derived using machine learning algorithms report promising results with respect to sensitivity, specificity and accuracy rates."
30225234,13.0,Application of artificial intelligence in ophthalmology,2018 Sep 18;11(9):1555-1561.,"Artificial intelligence is a general term that means to accomplish a task mainly by a computer, with the least human beings participation, and it is widely accepted as the invention of robots. With the development of this new technology, artificial intelligence has been one of the most influential information technology revolutions. We searched these English-language studies relative to ophthalmology published on PubMed and Springer databases. The application of artificial intelligence in ophthalmology mainly concentrates on the diseases with a high incidence, such as diabetic retinopathy, age-related macular degeneration, glaucoma, retinopathy of prematurity, age-related or congenital cataract and few with retinal vein occlusion. According to the above studies, we conclude that the sensitivity of detection and accuracy for proliferative diabetic retinopathy ranged from 75% to 91.7%, for non-proliferative diabetic retinopathy ranged from 75% to 94.7%, for age-related macular degeneration it ranged from 75% to 100%, for retinopathy of prematurity ranged over 95%, for retinal vein occlusion just one study reported ranged over 97%, for glaucoma ranged 63.7% to 93.1%, and for cataract it achieved a more than 70% similarity against clinical grading."
30222245,2.0,Synchrotron Big Data Science,2018 Nov;14(46):e1802291.,"The rapid development of synchrotrons has massively increased the speed at which experiments can be performed, while new techniques have increased the amount of raw data collected during each experiment. While this has created enormous new opportunities, it has also created tremendous challenges for national facilities and users. With the huge increase in data volume, the manual analysis of data is no longer possible. As a result, only a fraction of the data collected during the time- and money-expensive synchrotron beam-time is analyzed and used to deliver new science. Additionally, the lack of an appropriate data analysis environment limits the realization of experiments that generate a large amount of data in a very short period of time. The current lack of automated data analysis pipelines prevents the fine-tuning of beam-time experiments, further reducing their potential usage. These effects, collectively known as the ""data deluge,"" affect synchrotrons in several different ways including fast data collection, available local storage, data management systems, and curation of the data. This review highlights the Big Data strategies adopted nowadays at synchrotrons, documenting this novel and promising hybridization between science and technology, which promise a dramatic increase in the number of scientific discoveries."
30221328,5.0,Posttraumatic Stress Disorder and Death From Suicide,2018 Sep 17;20(11):98.,"Purpose of review:                    This review summarizes the increasing public health concern about PTSD and suicide, and the population-based studies that have examined this association. Further, we discuss methodological issues that provide important context for the examination of this association.              Recent findings:                    The majority of epidemiologic studies have shown that PTSD is associated with an increased risk of suicide; however, a notable minority of studies have documented a decreased risk of suicide among persons with PTSD. Methodological (e.g., sample size and misclassification) and etiologic issues (e.g., complicated psychiatric comorbidity) may explain the conflicting evidence. PTSD may be associated with an increased risk of suicide, but further research is needed. Increasing the use of appropriate methods (e.g., marginal structural models that can evaluate both confounding and effect modification, machine learning methods, quantification of systematic error) will strengthen the evidence base and advance our understanding."
