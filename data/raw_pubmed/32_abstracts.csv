pmid,citations,title,date,text
21924741,29.0,Methods and tools for objective assessment of psychomotor skills in laparoscopic surgery,2011 Nov;171(1):e81-95.,"Training and assessment paradigms for laparoscopic surgical skills are evolving from traditional mentor-trainee tutorship towards structured, more objective and safer programs. Accreditation of surgeons requires reaching a consensus on metrics and tasks used to assess surgeons' psychomotor skills. Ongoing development of tracking systems and software solutions has allowed for the expansion of novel training and assessment means in laparoscopy. The current challenge is to adapt and include these systems within training programs, and to exploit their possibilities for evaluation purposes. This paper describes the state of the art in research on measuring and assessing psychomotor laparoscopic skills. It gives an overview on tracking systems as well as on metrics and advanced statistical and machine learning techniques employed for evaluation purposes. The later ones have a potential to be used as an aid in deciding on the surgical competence level, which is an important aspect when accreditation of the surgeons in particular, and patient safety in general, are considered. The prospective of these methods and tools make them complementary means for surgical assessment of motor skills, especially in the early stages of training. Successful examples such as the Fundamentals of Laparoscopic Surgery should help drive a paradigm change to structured curricula based on objective parameters. These may improve the accreditation of new surgeons, as well as optimize their already overloaded training schedules."
21911058,27.0,Spelling with non-invasive Brain-Computer Interfaces--current and future trends,Jan-Jun 2011;105(1-3):106-14.,"Brain-Computer Interfaces (BCIs) have become a large research field that include challenges mainly in neuroscience, signal processing, machine learning and user interface. A non-invasive BCI can allow the direct communication between humans and computers by analyzing electrical brain activity, recorded at the surface of the scalp with electroencephalography. The main purpose for BCIs is to enable communication for people with severe disabilities. Spelling is one of the first BCI application, it corresponds to the main communication mean for people who are unable to speak. While spelling can be the most basic application it remains a benchmark for communication applications and one challenge in the BCI community for some patients. This paper proposes a review of the current main strategies, and their limitations, for spelling words. It includes recent BCIs based on P300, steady-state visual evoked potentials and motor imagery. By considering some challenges in BCI spellers and virtual keyboards, some pragmatic issues are pointed out to eliminate false hopes about BCI for both disabled and healthy people."
21864218,5.0,Computational modeling of P450s for toxicity prediction,2011 Oct;7(10):1211-31.,"Introduction:                    Drug development is a time-consuming and cost-intensive process. On average, it takes around 12 - 15 years and approximately â‚¬800 billion to bring a new drug to the market. Despite introduction of combinatorial chemistry and establishment of high-throughput screening (HTS), the number of new drug entities is limited. In fact, a number of established drug entities have been withdrawn from the market because of drug-drug interactions (DDIs) and adverse drug reactions (ADRs).              Areas covered:                    This review covers the advancements in cytochrome P450 (CYP450) modeling using different computational/machine learning (ML) tools over the past decade. A computational model for identifying non-toxic drug molecule from the pool of small chemical molecules is always welcome in the drug industry. Any computational tool that identifies the toxic molecule at early stage reduces the economic burden by slashing the number of molecules to be screened. This review covers all issues related to CYP-mediated toxicity such as specificity, inhibition, induction and regioselectivity.              Expert opinion:                    Several computational methods for CYP-mediated toxicity are available, which are popular in computer-aided drug designing (CADD). These models may become helpful in toxicity prediction during early stages and can reduce high failure rates in preclinical and clinical trials. There is an urgent need to improve the accuracy, interpretability and confidence of the computation models used in drug discovery pathways."
21863503,1.0,"Representation, simulation, and hypothesis generation in graph and logical models of biological networks",2011;759:465-82.,"This chapter presents a discussion of metabolic modeling from graph theory and logical modeling perspectives. These perspectives are closely related and focus on the coarse structure of metabolism, rather than the finer details of system behavior. The models have been used as background knowledge for hypothesis generation by Robot Scientists using yeast as a model eukaryote, where experimentation and machine learning are used to identify additional knowledge to improve the metabolic model. The logical modeling concept is being adapted to cell signaling and transduction biological networks."
21846786,161.0,Natural language processing: an introduction,Sep-Oct 2011;18(5):544-51.,"Objectives:                    To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design.              Target audience:                    This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the current state of the art.              Scope:                    We describe the historical evolution of NLP, and summarize common NLP sub-problems in this extensive field. We then provide a synopsis of selected highlights of medical NLP efforts. After providing a brief description of common machine-learning approaches that are being used for diverse NLP sub-problems, we discuss how modern NLP architectures are designed, with a summary of the Apache Foundation's Unstructured Information Management Architecture. We finally consider possible future directions for NLP, and reflect on the possible impact of IBM Watson on the medical field."
21795890,22.0,Use of advanced machine-learning techniques for noninvasive monitoring of hemorrhage,2011 Jul;71(1 Suppl):S25-32.,"Background:                    Hemorrhagic shock is a leading cause of death in both civilian and battlefield trauma. Currently available medical monitors provide measures of standard vital signs that are insensitive and nonspecific. More important, hypotension and other signs and symptoms of shock can appear when it may be too late to apply effective life-saving interventions. The resulting challenge is that early diagnosis is difficult because hemorrhagic shock is first recognized by late-responding vital signs and symptoms. The purpose of these experiments was to test the hypothesis that state-of-the-art machine-learning techniques, when integrated with novel non-invasive monitoring technologies, could detect early indicators of blood volume loss and impending circulatory failure in conscious, healthy humans who experience reduced central blood volume.              Methods:                    Humans were exposed to progressive reductions in central blood volume using lower body negative pressure as a model of hemorrhage until the onset of hemodynamic decompensation. Continuous, noninvasively measured hemodynamic signals were used for the development of machine-learning algorithms. Accuracy estimates were obtained by building models using signals from all but one subject and testing on that subject. This process was repeated, each time using a different subject.              Results:                    The model was 96.5% accurate in predicting the estimated amount of reduced central blood volume, and the correlation between predicted and actual lower body negative pressure level for hemodynamic decompensation was 0.89.              Conclusions:                    Machine modeling can accurately identify reduced central blood volume and predict impending hemodynamic decompensation (shock onset) in individuals. Such a capability can provide decision support for earlier intervention."
21787301,4.0,Machine learning algorithms for predicting protein folding rates and stability of mutant proteins: comparison with statistical methods,2011 Sep;12(6):490-502.,"Machine learning algorithms have wide range of applications in bioinformatics and computational biology such as prediction of protein secondary structures, solvent accessibility, binding site residues in protein complexes, protein folding rates, stability of mutant proteins, and discrimination of proteins based on their structure and function. In this work, we focus on two aspects of predictions: (i) protein folding rates and (ii) stability of proteins upon mutations. We briefly introduce the concepts of protein folding rates and stability along with available databases, features for prediction methods and measures for prediction performance. Subsequently, the development of structure based parameters and their relationship with protein folding rates will be outlined. The structure based parameters are helpful to understand the physical basis for protein folding and stability. Further, basic principles of major machine learning techniques will be mentioned and their applications for predicting protein folding rates and stability of mutant proteins will be illustrated. The machine learning techniques could achieve the highest accuracy of predicting protein folding rates and stability. In essence, statistical methods and machine learning algorithms are complimenting each other for understanding and predicting protein folding rates and the stability of protein mutants. The available online resources on protein folding rates and stability will be listed."
21787299,8.0,Structural protein descriptors in 1-dimension and their sequence-based predictions,2011 Sep;12(6):470-89.,"The last few decades observed an increasing interest in development and application of 1-dimensional (1D) descriptors of protein structure. These descriptors project 3D structural features onto 1D strings of residue-wise structural assignments. They cover a wide-range of structural aspects including conformation of the backbone, burying depth/solvent exposure and flexibility of residues, and inter-chain residue-residue contacts. We perform first-of-its-kind comprehensive comparative review of the existing 1D structural descriptors. We define, review and categorize ten structural descriptors and we also describe, summarize and contrast over eighty computational models that are used to predict these descriptors from the protein sequences. We show that the majority of the recent sequence-based predictors utilize machine learning models, with the most popular being neural networks, support vector machines, hidden Markov models, and support vector and linear regressions. These methods provide high-throughput predictions and most of them are accessible to a non-expert user via web servers and/or stand-alone software packages. We empirically evaluate several recent sequence-based predictors of secondary structure, disorder, and solvent accessibility descriptors using a benchmark set based on CASP8 targets. Our analysis shows that the secondary structure can be predicted with over 80% accuracy and segment overlap (SOV), disorder with over 0.9 AUC, 0.6 Matthews Correlation Coefficient (MCC), and 75% SOV, and relative solvent accessibility with PCC of 0.7 and MCC of 0.6 (0.86 when homology is used). We demonstrate that the secondary structure predicted from sequence without the use of homology modeling is as good as the structure extracted from the 3D folds predicted by top-performing template-based methods."
21787298,2.0,Cellular automata and its applications in protein bioinformatics,2011 Sep;12(6):508-19.,"With the explosion of protein sequences generated in the postgenomic era, it is highly desirable to develop high-throughput tools for rapidly and reliably identifying various attributes of uncharacterized proteins based on their sequence information alone. The knowledge thus obtained can help us timely utilize these newly found protein sequences for both basic research and drug discovery. Many bioinformatics tools have been developed by means of machine learning methods. This review is focused on the applications of a new kind of science (cellular automata) in protein bioinformatics. A cellular automaton (CA) is an open, flexible and discrete dynamic model that holds enormous potentials in modeling complex systems, in spite of the simplicity of the model itself. Researchers, scientists and practitioners from different fields have utilized cellular automata for visualizing protein sequences, investigating their evolution processes, and predicting their various attributes. Owing to its impressive power, intuitiveness and relative simplicity, the CA approach has great potential for use as a tool for bioinformatics."
21780006,2.0,In silico prediction of post-translational modifications,2011;760:325-40.,"Methods for predicting protein post-translational modifications have been developed extensively. In this chapter, we review major post-translational modification prediction strategies, with a particular focus on statistical and machine learning approaches. We present the workflow of the methods and summarize the advantages and disadvantages of the methods."
21721140,12.0,Informatics and standards for nanomedicine technology,Sep-Oct 2011;3(5):511-532.,"There are several issues to be addressed concerning the management and effective use of information (or data), generated from nanotechnology studies in biomedical research and medicine. These data are large in volume, diverse in content, and are beset with gaps and ambiguities in the description and characterization of nanomaterials. In this work, we have reviewed three areas of nanomedicine informatics: information resources; taxonomies, controlled vocabularies, and ontologies; and information standards. Informatics methods and standards in each of these areas are critical for enabling collaboration; data sharing; unambiguous representation and interpretation of data; semantic (meaningful) search and integration of data; and for ensuring data quality, reliability, and reproducibility. In particular, we have considered four types of information standards in this article, which are standard characterization protocols, common terminology standards, minimum information standards, and standard data communication (exchange) formats. Currently, because of gaps and ambiguities in the data, it is also difficult to apply computational methods and machine learning techniques to analyze, interpret, and recognize patterns in data that are high dimensional in nature, and also to relate variations in nanomaterial properties to variations in their chemical composition, synthesis, characterization protocols, and so on. Progress toward resolving the issues of information management in nanomedicine using informatics methods and standards discussed in this article will be essential to the rapidly growing field of nanomedicine informatics."
21707399,13.0,Computational tools for polypharmacology and repurposing,2011 Jun;3(8):961-8.,"Most drugs act on a multitude of targets rather than on one single target. Polypharmacology, an upcoming branch of pharmaceutical science, deals with the recognition of these off-target activities of small chemical compounds. Due to the high amount of data to be processed, application of computational methods is indispensable in this area. This review summarizes the most important in silico approaches for polypharmacology. The described methods comprise network pharmacology, machine learning techniques and chemogenomic approaches. The use of these methods for drug repurposing as a branch of drug discovery and development is discussed. Furthermore, a broad range of prospective applications is summarized to give the reader an overview of possibilities and limitations of the described techniques."
21607908,3.0,Advanced technology development for remote triage applications in bleeding combat casualties,Apr-Jun 2011;61-72.,"Combat developers within the Army have envisioned development of a ""wear-and-forget"" physiological status monitor (PSM) that will enhance far forward capabilities for assessment of Warrior readiness for battle, as well as for remote triage, diagnosis and decision-making once Soldiers are injured. This paper will review recent work testing remote triage system prototypes in both the laboratory and during field exercises. Current PSM prototypes measure the electrocardiogram and respiration, but we have shown that information derived from these measurements alone will not be suited for specific, accurate triage of combat injuries. Because of this, we have suggested that development of a capability to provide a metric of circulating blood volume status is required for remote triage. Recently, volume status has been successfully modeled using low-level physiological signals obtained from wearable devices as input to machine-learning algorithms; these algorithms are already able to discriminate between a state of physical activity (common in combat) and that of central hypovolemia, and thus show promise for use in wearable remote triage devices."
21470173,1.0,Present perspectives on the automated classification of the G-protein coupled receptors (GPCRs) at the protein sequence level,2011;11(15):1994-2009.,"The G-protein coupled receptors--or GPCRs--comprise simultaneously one of the largest and one of the most multi-functional protein families known to modern-day molecular bioscience. From a drug discovery and pharmaceutical industry perspective, the GPCRs constitute one of the most commercially and economically important groups of proteins known. The GPCRs undertake numerous vital metabolic functions and interact with a hugely diverse range of small and large ligands. Many different methodologies have been developed to efficiently and accurately classify the GPCRs. These range from motif-based techniques to machine learning as well as a variety of alignment-free techniques based on the physiochemical properties of sequences. We review here the available methodologies for the classification of GPCRs. Part of this work focuses on how we have tried to build the intrinsically hierarchical nature of sequence relations, implicit within the family, into an adaptive approach to classification. Importantly, we also allude to some of the key innate problems in developing an effective approach to classifying the GPCRs: the lack of sequence similarity between the six classes that comprise the GPCR family and the low sequence similarity to other family members evinced by many newly revealed members of the family."
21470169,2.0,Quantitative chemogenomics: machine-learning models of protein-ligand interaction,2011;11(15):1978-93.,"Chemogenomics is an emerging interdisciplinary field that lies in the interface of biology, chemistry, and informatics. Most of the currently used drugs are small molecules that interact with proteins. Understanding protein-ligand interaction is therefore central to drug discovery and design. In the subfield of chemogenomics known as proteochemometrics, protein-ligand-interaction models are induced from data matrices that consist of both protein and ligand information along with some experimentally measured variable. The two general aims of this quantitative multi-structure-property-relationship modeling (QMSPR) approach are to exploit sparse/incomplete information sources and to obtain more general models covering larger parts of the protein-ligand space, than traditional approaches that focuses mainly on specific targets or ligands. The data matrices, usually obtained from multiple sparse/incomplete sources, typically contain series of proteins and ligands together with quantitative information about their interactions. A useful model should ideally be easy to interpret and generalize well to new unseen protein-ligand combinations. Resolving this requires sophisticated machine-learning methods for model induction, combined with adequate validation. This review is intended to provide a guide to methods and data sources suitable for this kind of protein-ligand-interaction modeling. An overview of the modeling process is presented including data collection, protein and ligand descriptor computation, data preprocessing, machine-learning-model induction and validation. Concerns and issues specific for each step in this kind of data-driven modeling will be discussed."
21466461,2.0,From data processing to multivariate validation--essential steps in extracting interpretable information from metabolomics data,2011 Jul;12(7):996-1004.,"In metabolomics studies there is a clear increase of data. This indicates the necessity of both having a battery of suitable analysis methods and validation procedures able to handle large amounts of data. In this review, an overview of the metabolomics data processing pipeline is presented. A selection of recently developed and most cited data processing methods is discussed. In addition, commonly used chemometric and machine learning analysis methods as well as validation approaches are described."
21464505,31.0,Parameter estimation using meta-heuristics in systems biology: a comprehensive review,Jan-Feb 2012;9(1):185-202.,"This paper gives a comprehensive review of the application of meta-heuristics to optimization problems in systems biology, mainly focussing on the parameter estimation problem (also called the inverse problem or model calibration). It is intended for either the system biologist who wishes to learn more about the various optimization techniques available and/or the meta-heuristic optimizer who is interested in applying such techniques to problems in systems biology. First, the parameter estimation problems emerging from different areas of systems biology are described from the point of view of machine learning. Brief descriptions of various meta-heuristics developed for these problems follow, along with outlines of their advantages and disadvantages. Several important issues in applying meta-heuristics to the systems biology modelling problem are addressed, including the reliability and identifiability of model parameters, optimal design of experiments, and so on. Finally, we highlight some possible future research directions in this field."
21452981,4.0,"Informatics, machine learning and computational medicinal chemistry",2011 Mar;3(4):451-67.,"This article reviews the use of informatics and computational chemistry methods in medicinal chemistry, with special consideration of how computational techniques can be adapted and extended to obtain more and higher-quality information. Special consideration is given to the computation of protein-ligand binding affinities, to the prediction of off-target bioactivities, bioactivity spectra and computational toxicology, and also to calculating absorption-, distribution-, metabolism- and excretion-relevant properties, such as solubility."
21431559,19.0,Decision tree and ensemble learning algorithms with their applications in bioinformatics,2011;696:191-9.,"Machine learning approaches have wide applications in bioinformatics, and decision tree is one of the successful approaches applied in this field. In this chapter, we briefly review decision tree and related ensemble algorithms and show the successful applications of such approaches on solving biological problems. We hope that by learning the algorithms of decision trees and ensemble classifiers, biologists can get the basic ideas of how machine learning algorithms work. On the other hand, by being exposed to the applications of decision trees and ensemble algorithms in bioinformatics, computer scientists can get better ideas of which bioinformatics topics they may work on in their future research directions. We aim to provide a platform to bridge the gap between biologists and computer scientists."
21393536,203.0,"How to grow a mind: statistics, structure, and abstraction",2011 Mar 11;331(6022):1279-85.,"In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?"
21303343,13.0,Development of anti-viral agents using molecular modeling and virtual screening techniques,2011 Feb;11(1):64-93.,"Computational chemistry has always played a key role in anti-viral drug development. The challenges and the quickly rising public interest when a virus is becoming a threat has significantly influenced computational drug discovery. The most obvious example is anti-AIDS research, where HIV protease and reverse transcriptase have triggered enormous efforts in developing and improving computational methods. Methods applied to anti-viral research include (i) ligand-based approaches that rely on known active compounds to extrapolate biological activity, such as machine learning techniques or classical QSAR, (ii) structure-based methods that rely on an experimentally determined 3D structure of the targets, such as molecular docking or molecular dynamics, and (iii) universal approaches that can be applied in a structure- or ligand-based way, such as 3D QSAR or 3D pharmacophore elucidation. In this review we summarize these molecular modeling approaches as they were applied to fight anti-viral diseases and highlight their importance for anti-viral research. We discuss the role of computational chemistry in the development of small molecules as agents against HIV integrase, HIV-1 protease, HIV-1 reverse transcriptase, the influenza virus M2 channel protein, influenza virus neuraminidase, the SARS coronavirus main proteinase and spike protein, thymidine kinases of herpes viruses, hepatitis c virus proteins and other flaviviruses as well as human rhinovirus coat protein and proteases, and other picornaviridae. We highlight how computational approaches have helped in discovering anti-viral activities of natural products and give an overview on polypharmacology approaches that help to optimize drugs against several viruses or help to optimize the metabolic profile of and anti-viral drug."
21269608,26.0,A tutorial introduction to Bayesian models of cognitive development,2011 Sep;120(3):302-21.,"We present an introduction to Bayesian inference as it is used in probabilistic models of cognitive development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian inference, but also include information about additional resources for those interested in the cognitive science applications, mathematical foundations, or machine learning details in more depth. In addition, we discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive science."
27467875,13.0,Chemoinformatics as a Theoretical Chemistry Discipline,2011 Jan 17;30(1):20-32.,"Here, chemoinformatics is considered as a theoretical chemistry discipline complementary to quantum chemistry and force-field molecular modeling. These three fields are compared with respect to molecular representation, inference mechanisms, basic concepts and application areas. A chemical space, a fundamental concept of chemoinformatics, is considered with respect to complex relations between chemical objects (graphs or descriptor vectors). Statistical Learning Theory, one of the main mathematical approaches in structure-property modeling, is briefly reviewed. Links between chemoinformatics and its ""sister"" fields - machine learning, chemometrics and bioinformatics are discussed."
22889876,55.0,Random forests for genetic association studies,2011;10(1):32.,"The Random Forests (RF) algorithm has become a commonly used machine learning algorithm for genetic association studies. It is well suited for genetic applications since it is both computationally efficient and models genetic causal mechanisms well. With its growing ubiquity, there has been inconsistent and less than optimal use of RF in the literature. The purpose of this review is to breakdown the theoretical and statistical basis of RF so that practitioners are able to apply it in their work. An emphasis is placed on showing how the various components contribute to bias and variance, as well as discussing variable importance measures. Applications specific to genetic studies are highlighted. To provide context, RF is compared to other commonly used machine learning algorithms."
21172442,140.0,Introduction to machine learning for brain imaging,2011 May 15;56(2):387-99.,"Machine learning and pattern recognition algorithms have in the past years developed to become a working horse in brain imaging and the computational neurosciences, as they are instrumental for mining vast amounts of neural data of ever increasing measurement precision and detecting minuscule signals from an overwhelming noise floor. They provide the means to decode and characterize task relevant brain states and to distinguish them from non-informative brain signals. While undoubtedly this machinery has helped to gain novel biological insights, it also holds the danger of potential unintentional abuse. Ideally machine learning techniques should be usable for any non-expert, however, unfortunately they are typically not. Overfitting and other pitfalls may occur and lead to spurious and nonsensical interpretation. The goal of this review is therefore to provide an accessible and clear introduction to the strengths and also the inherent dangers of machine learning usage in the neurosciences."
21039646,157.0,Supervised classification of human microbiota,2011 Mar;35(2):343-59.,"Recent advances in DNA sequencing technology have allowed the collection of high-dimensional data from human-associated microbial communities on an unprecedented scale. A major goal of these studies is the identification of important groups of microorganisms that vary according to physiological or disease states in the host, but the incidence of rare taxa and the large numbers of taxa observed make that goal difficult to obtain using traditional approaches. Fortunately, similar problems have been addressed by the machine learning community in other fields of study such as microarray analysis and text classification. In this review, we demonstrate that several existing supervised classifiers can be applied effectively to microbiota classification, both for selecting subsets of taxa that are highly discriminative of the type of community, and for building models that can accurately classify unlabeled data. To encourage the development of new approaches to supervised classification of microbiota, we discuss several structures inherent in microbial community data that may be available for exploitation in novel approaches, and we include as supplemental information several benchmark classification tasks for use by the community."
25076977,46.0,Mapping Mental Function to Brain Structure: How Can Cognitive Neuroimaging Succeed?,2010 Nov;5(6):753-61.,"The goal of cognitive neuroscience is to identify the mapping between brain function and mental processing. In this article, I examine the strategies that have been used to identify such mappings and argue that they may be fundamentally unable to identify selective structure-function mappings. To understand the functional anatomy of mental processes, it will be necessary for researchers to move from the brain-mapping strategies that the field has employed toward a search for selective associations. This will require a greater focus on the structure of cognitive processes, which can be achieved through the development of formal ontologies that describe the structure of mental processes. In this article, I outline the Cognitive Atlas Project, which is developing such ontologies, and show how this knowledge could be used in conjunction with data-mining approaches to more directly relate mental processes and brain function."
21029850,23.0,"Detecting, characterizing, and interpreting nonlinear gene-gene interactions using multifactor dimensionality reduction",2010;72:101-16.,"Human health is a complex process that is dependent on many genes, many environmental factors and chance events that are perhaps not measurable with current technology or are simply unknowable. Success in the design and execution of population-based association studies to identify those genetic and environmental factors that play an important role in human disease will depend on our ability to embrace, rather that ignore, complexity in the genotype to phenotype mapping relationship for any given human ecology. We review here three general computational challenges that must be addressed. First, data mining and machine learning methods are needed to model nonlinear interactions between multiple genetic and environmental factors. Second, filter and wrapper methods are needed to identify attribute interactions in large and complex solution landscapes. Third, visualization methods are needed to help interpret computational models and results. We provide here an overview of the multifactor dimensionality reduction (MDR) method that was developed for addressing each of these challenges."
21029849,16.0,Multigenic modeling of complex disease by random forests,2010;72:73-99.,"The genetics and heredity of complex human traits have been studied for over a century. Many genes have been implicated in these complex traits. Genome-wide association studies (GWAS) were designed to investigate the association between common genetic variation and complex human traits using high-throughput platforms that measured hundreds of thousands of common single-nucleotide polymorphisms (SNPs). GWAS have successfully identified many novel genetic loci associated with complex traits using a univariate regression-based approach. Even for traits with a large number of identified variants, only a small fraction of the interindividual variation in risk phenotypes has been explained. In biological systems, protein, DNA, RNA, and metabolites frequently interact to each other to perform their biological functions, and to respond to environmental factors. The complex interactions among genes and between the genes and environment may partially explain the ""missing heritability."" The traditional regression-based methods are limited to address the complex interactions among the hundreds of thousands of SNPs and their environmental context by both the modeling and computational challenge. Random Forests (RF), one of the powerful machine learning methods, is regarded as a useful alternative to capture the complex interaction effects among the GWAS data, and potentially address the genetic heterogeneity underlying these complex traits using a computationally efficient framework. In this chapter, the features of prediction and variable selection, and their applications in genetic association studies are reviewed and discussed. Additional improvements of the original RF method are warranted to make the applications in GWAS to be more successful."
20887262,1.0,Detecting atypical examples of known domain types by sequence similarity searching: the SBASE domain library approach,2010 Nov;11(7):538-49.,"SBASE is a project initiated to detect known domain types and predicting domain architectures using sequence similarity searching (Simon et al., Protein Seq Data Anal, 5: 39-42, 1992, Pongor et al, Nucl. Acids. Res. 21:3111-3115, 1992). The current approach uses a curated collection of domain sequences - the SBASE domain library - and standard similarity search algorithms, followed by postprocessing which is based on a simple statistics of the domain similarity network (http://hydra.icgeb.trieste.it/sbase/). It is especially useful in detecting rare, atypical examples of known domain types which are sometimes missed even by more sophisticated methodologies. This approach does not require multiple alignment or machine learning techniques, and can be a useful complement to other domain detection methodologies. This article gives an overview of the project history as well as of the concepts and principles developed within this the project."
20887261,12.0,Topology prediction of helical transmembrane proteins: how far have we reached?,2010 Nov;11(7):550-61.,"Transmembrane protein topology prediction methods play important roles in structural biology, because the structure determination of these types of proteins is extremely difficult by the common biophysical, biochemical and molecular biological methods. The need for accurate prediction methods is high, as the number of known membrane protein structures fall far behind the estimated number of these proteins in various genomes. The accuracy of these prediction methods appears to be higher than most prediction methods applied on globular proteins, however it decreases slightly with the increasing number of structures. Unfortunately, most prediction algorithms use common machine learning techniques, and they do not reveal why topologies are predicted with such a high success rate and which biophysical or biochemical properties are important to achieve this level of accuracy. Incorporating topology data determined so far into the prediction methods as constraints helps us to reach even higher prediction accuracy, therefore collection of such topology data is also an important issue."
20838967,36.0,Similarity searching using 2D structural fingerprints,2011;672:133-58.,"This chapter reviews the use of molecular fingerprints for chemical similarity searching. The fingerprints encode the presence of 2D substructural fragments in a molecule, and the similarity between a pair of molecules is a function of the number of fragments that they have in common. Although this provides a very simple way of estimating the degree of structural similarity between two molecules, it has been found to provide an effective and an efficient tool for searching large chemical databases. The review describes the historical development of similarity searching since it was first described in the mid-1980s, reviews the many different coefficients, representations, and weightings that can be combined to form a similarity measure, describes quantitative measures of the effectiveness of similarity searching, and concludes by looking at current developments based on the use of data fusion and machine learning techniques."
20810180,25.0,Neuroimaging-based approaches in the brain-computer interface,2010 Nov;28(11):552-60.,"Techniques to enable direct communication between the brain and computers/machines, such as the brain-computer interface (BCI) or the brain-machine interface (BMI), are gaining momentum in the neuroscientific realm, with potential applications ranging from medicine to general consumer electronics. Noninvasive BCI techniques based on neuroimaging modalities are reviewed in terms of their methodological approaches as well as their similarities and differences. Trends in automated data interpretation through machine learning algorithms are also introduced. Applications of functional neuromodulation techniques to BCI systems would allow for bidirectional communication between the brain and the computer. Such bidirectional interfaces can relay information directly from one brain to another using a computer as a medium, ultimately leading to the concept of a brain-to-brain interface (BBI)."
20801638,51.0,Machines that learn to segment images: a crucial technology for connectomics,2010 Oct;20(5):653-66.,"Connections between neurons can be found by checking whether synapses exist at points of contact, which in turn are determined by neural shapes. Finding these shapes is a special case of image segmentation, which is laborious for humans and would ideally be performed by computers. New metrics properly quantify the performance of a computer algorithm using its disagreement with 'true' segmentations of example images. New machine learning methods search for segmentation algorithms that minimize such metrics. These advances have reduced computer errors dramatically. It should now be faster for a human to correct the remaining errors than to segment an image manually. Further reductions in human effort are expected, and crucial for finding connectomes more complex than that of Caenorhabditis elegans."
20699326,11.0,"Evidence for CRHR1 in multiple sclerosis using supervised machine learning and meta-analysis in 12,566 individuals",2010 Nov 1;19(21):4286-95.,"The primary genetic risk factor in multiple sclerosis (MS) is the HLA-DRB1*1501 allele; however, much of the remaining genetic contribution to MS has yet to be elucidated. Several lines of evidence support a role for neuroendocrine system involvement in autoimmunity which may, in part, be genetically determined. Here, we comprehensively investigated variation within eight candidate hypothalamic-pituitary-adrenal (HPA) axis genes and susceptibility to MS. A total of 326 SNPs were investigated in a discovery dataset of 1343 MS cases and 1379 healthy controls of European ancestry using a multi-analytical strategy. Random Forests, a supervised machine-learning algorithm, identified eight intronic SNPs within the corticotrophin-releasing hormone receptor 1 or CRHR1 locus on 17q21.31 as important predictors of MS. On the basis of univariate analyses, six CRHR1 variants were associated with decreased risk for disease following a conservative correction for multiple tests. Independent replication was observed for CRHR1 in a large meta-analysis comprising 2624 MS cases and 7220 healthy controls of European ancestry. Results from a combined meta-analysis of all 3967 MS cases and 8599 controls provide strong evidence for the involvement of CRHR1 in MS. The strongest association was observed for rs242936 (OR = 0.82, 95% CI = 0.74-0.90, P = 9.7 Ã— 10(-5)). Replicated CRHR1 variants appear to exist on a single associated haplotype. Further investigation of mechanisms involved in HPA axis regulation and response to stress in MS pathogenesis is warranted."
20678228,71.0,Automatic de-identification of textual documents in the electronic health record: a review of recent research,2010 Aug 2;10:70.,"Background:                    In the United States, the Health Insurance Portability and Accountability Act (HIPAA) protects the confidentiality of patient data and requires the informed consent of the patient and approval of the Internal Review Board to use data for research purposes, but these requirements can be waived if data is de-identified. For clinical data to be considered de-identified, the HIPAA ""Safe Harbor"" technique requires 18 data elements (called PHI: Protected Health Information) to be removed. The de-identification of narrative text documents is often realized manually, and requires significant resources. Well aware of these issues, several authors have investigated automated de-identification of narrative text documents from the electronic health record, and a review of recent research in this domain is presented here.              Methods:                    This review focuses on recently published research (after 1995), and includes relevant publications from bibliographic queries in PubMed, conference proceedings, the ACM Digital Library, and interesting publications referenced in already included papers.              Results:                    The literature search returned more than 200 publications. The majority focused only on structured data de-identification instead of narrative text, on image de-identification, or described manual de-identification, and were therefore excluded. Finally, 18 publications describing automated text de-identification were selected for detailed analysis of the architecture and methods used, the types of PHI detected and removed, the external resources used, and the types of clinical documents targeted. All text de-identification systems aimed to identify and remove person names, and many included other types of PHI. Most systems used only one or two specific clinical document types, and were mostly based on two different groups of methodologies: pattern matching and machine learning. Many systems combined both approaches for different types of PHI, but the majority relied only on pattern matching, rules, and dictionaries.              Conclusions:                    In general, methods based on dictionaries performed better with PHI that is rarely mentioned in clinical text, but are more difficult to generalize. Methods based on machine learning tend to perform better, especially with PHI that is not mentioned in the dictionaries used. Finally, the issues of anonymization, sufficient performance, and ""over-scrubbing"" are discussed in this publication."
20647054,34.0,Natural Language Processing methods and systems for biomedical ontology learning,2011 Feb;44(1):163-79.,"While the biomedical informatics community widely acknowledges the utility of domain ontologies, there remain many barriers to their effective use. One important requirement of domain ontologies is that they must achieve a high degree of coverage of the domain concepts and concept relationships. However, the development of these ontologies is typically a manual, time-consuming, and often error-prone process. Limited resources result in missing concepts and relationships as well as difficulty in updating the ontology as knowledge changes. Methodologies developed in the fields of Natural Language Processing, information extraction, information retrieval and machine learning provide techniques for automating the enrichment of an ontology from free-text documents. In this article, we review existing methodologies and developed systems, and discuss how existing methods can benefit the development of biomedical ontologies."
20644953,57.0,What do the basal ganglia do? A modeling perspective,2010 Sep;103(3):237-53.,"Basal ganglia (BG) constitute a network of seven deep brain nuclei involved in a variety of crucial brain functions including: action selection, action gating, reward based learning, motor preparation, timing, etc. In spite of the immense amount of data available today, researchers continue to wonder how a single deep brain circuit performs such a bewildering range of functions. Computational models of BG have focused on individual functions and fail to give an integrative picture of BG function. A major breakthrough in our understanding of BG function is perhaps the insight that activities of mesencephalic dopaminergic cells represent some form of 'reward' to the organism. This insight enabled application of tools from 'reinforcement learning,' a branch of machine learning, in the study of BG function. Nevertheless, in spite of these bright spots, we are far from the goal of arriving at a comprehensive understanding of these 'mysterious nuclei.' A comprehensive knowledge of BG function has the potential to radically alter treatment and management of a variety of BG-related neurological disorders (Parkinson's disease, Huntington's chorea, etc.) and neuropsychiatric disorders (schizophrenia, obsessive compulsive disorder, etc.) also. In this article, we review the existing modeling literature on BG and hypothesize an integrative picture of the function of these nuclei."
20617142,5.0,Comparative pathogenesis and systems biology for biodefense virus vaccine development,2010;2010:236528.,"Developing vaccines to biothreat agents presents a number of challenges for discovery, preclinical development, and licensure. The need for high containment to work with live agents limits the amount and types of research that can be done using complete pathogens, and small markets reduce potential returns for industry. However, a number of tools, from comparative pathogenesis of viral strains at the molecular level to novel computational approaches, are being used to understand the basis of viral attenuation and characterize protective immune responses. As the amount of basic molecular knowledge grows, we will be able to take advantage of these tools not only to rationally attenuate virus strains for candidate vaccines, but also to assess immunogenicity and safety in silico. This review discusses how a basic understanding of pathogenesis, allied with systems biology and machine learning methods, can impact biodefense vaccinology."
20596934,,Computational neuroscience in China,2010 Mar;53(3):385-397.,"The ultimate goal of Computational Neuroscience (CNS) is to use and develop mathematical models and approaches to elucidate brain functions. CNS is a young and highly multidisciplinary field. It heavily interacts with experimental neuroscience and such other research areas as artificial intelligence, robotics, computer vision, information science and machine learning. This paper reviews the history of CNS in China, its current status and the prospects for its future development. Examples of CNS research in China are also presented."
20500882,8.0,Semantic annotation of morphological descriptions: an overall strategy,2010 May 25;11:278.,"Background:                    Large volumes of morphological descriptions of whole organisms have been created as print or electronic text in a human-readable format. Converting the descriptions into computer- readable formats gives a new life to the valuable knowledge on biodiversity. Research in this area started 20 years ago, yet not sufficient progress has been made to produce an automated system that requires only minimal human intervention but works on descriptions of various plant and animal groups. This paper attempts to examine the hindering factors by identifying the mismatches between existing research and the characteristics of morphological descriptions.              Results:                    This paper reviews the techniques that have been used for automated annotation, reports exploratory results on characteristics of morphological descriptions as a genre, and identifies challenges facing automated annotation systems. Based on these criteria, the paper proposes an overall strategy for converting descriptions of various taxon groups with the least human effort.              Conclusions:                    A combined unsupervised and supervised machine learning strategy is needed to construct domain ontologies and lexicons and to ultimately achieve automated semantic annotation of morphological descriptions. Further, we suggest that each effort in creating a new description or annotating an individual description collection should be shared and contribute to the ""biodiversity information commons"" for the Semantic Web. This cannot be done without a sound strategy and a close partnership between and among information scientists and biologists."
20491597,16.0,Integrated diagnostics: a conceptual framework with examples,2010 Jul;48(7):989-98.,"With the advent of digital pathology, imaging scientists have begun to develop computerized image analysis algorithms for making diagnostic (disease presence), prognostic (outcome prediction), and theragnostic (choice of therapy) predictions from high resolution images of digitized histopathology. One of the caveats to developing image analysis algorithms for digitized histopathology is the ability to deal with highly dense, information rich datasets; datasets that would overwhelm most computer vision and image processing algorithms. Over the last decade, manifold learning and non-linear dimensionality reduction schemes have emerged as popular and powerful machine learning tools for pattern recognition problems. However, these techniques have thus far been applied primarily to classification and analysis of computer vision problems (e.g., face detection). In this paper, we discuss recent work by a few groups in the application of manifold learning methods to problems in computer aided diagnosis, prognosis, and theragnosis of digitized histopathology. In addition, we discuss some exciting recent developments in the application of these methods for multi-modal data fusion and classification; specifically the building of meta-classifiers by fusion of histological image and proteomic signatures for prostate cancer outcome prediction."
20488979,33.0,Automated image analysis for high-content screening and analysis,2010 Aug;15(7):726-34.,"The field of high-content screening and analysis consists of a set of methodologies for automated discovery in cell biology and drug development using large amounts of image data. In most cases, imaging is carried out by automated microscopes, often assisted by automated liquid handling and cell culture. Image processing, computer vision, and machine learning are used to automatically process high-dimensional image data into meaningful cell biological results. The key is creating automated analysis pipelines typically consisting of 4 basic steps: (1) image processing (normalization, segmentation, tracing, tracking), (2) spatial transformation to bring images to a common reference frame (registration), (3) computation of image features, and (4) machine learning for modeling and interpretation of data. An overview of these image analysis tools is presented here, along with brief descriptions of a few applications."
20465523,2.0,Machine learning algorithms for the prediction of hERG and CYP450 binding in drug development,2010 Jul;6(7):821-33.,"Importance of the field:                    The cost of developing new drugs is estimated at approximately $1 billion; the withdrawal of a marketed compound due to toxicity can result in serious financial loss for a pharmaceutical company. There has been a greater interest in the development of in silico tools that can identify compounds with metabolic liabilities before they are brought to market.              Areas covered in this review:                    The two largest classes of machine learning (ML) models, which will be discussed in this review, have been developed to predict binding to the human ether-a-go-go related gene (hERG) ion channel protein and the various CYP isoforms. Being able to identify potentially toxic compounds before they are made would greatly reduce the number of compound failures and the costs associated with drug development.              What the reader will gain:                    This review summarizes the state of modeling hERG and CYP binding towards this goal since 2003 using ML algorithms.              Take home message:                    A wide variety of ML algorithms that are comparable in their overall performance are available. These ML methods may be applied regularly in discovery projects to flag compounds with potential metabolic liabilities."
20453924,14.0,New insights into the biogenesis of nuclear RNA polymerases?,2010 Apr;88(2):211-21.,"More than 30 years of research on nuclear RNA polymerases (RNAP I, II, and III) has uncovered numerous factors that regulate the activity of these enzymes during the transcription reaction. However, very little is known about the machinery that regulates the fate of RNAPs before or after transcription. In particular, the mechanisms of biogenesis of the 3 nuclear RNAPs, which comprise both common and specific subunits, remains mostly uncharacterized and the proteins involved are yet to be discovered. Using protein affinity purification coupled to mass spectrometry (AP-MS), we recently unraveled a high-density interaction network formed by nuclear RNAP subunits from the soluble fraction of human cell extracts. Validation of the dataset using a machine learning approach trained to minimize the rate of false positives and false negatives yielded a high-confidence dataset and uncovered novel interactors that regulate the RNAP II transcription machinery, including a set of proteins we named the RNAP II-associated proteins (RPAPs). One of the RPAPs, RPAP3, is part of an 11-subunit complex we termed the RPAP3/R2TP/prefoldin-like complex. Here, we review the literature on the subunits of this complex, which points to a role in nuclear RNAP biogenesis."
20381467,6.0,Computational methodologies for studying non-coding RNAs relevant to central nervous system function and dysfunction,2010 Jun 18;1338:131-45.,"Non-coding RNAs (ncRNAs) are a large and diverse group of transcripts that span the eukaryotic genome, of which less than 2% encodes proteins. Several distinct families of ncRNAs have been described and implicated in many aspects of central nervous system (CNS) function including translation, RNA metabolism, gene regulation, and development. The need to distinguish ncRNAs from sequence data, as well as potentially uncovering novel ncRNA families, has ignited the development of customized computational approaches and bioinformatic resources to handle these tasks. In this review, we provide an overview of the numerous procedures developed to predict ncRNAs based on their primary sequence and predicted secondary structure. These methodologies are broadly grouped into genome scanning algorithms, mixed approaches, and machine learning algorithms. Regulatory ncRNAs, particularly microRNAs (miRNAs), are a major focus of current research efforts and this review will therefore center on the prediction of miRNAs and the putative gene targets they act upon. With the advent of ultra high-throughput sequencing technologies 'deep sequencing' has emerged as the cutting-edge method for ncRNA identification and we will also touch on some computational resources that play a key role in analysis of this type of data."
20221928,14.0,Protein secondary structure prediction,2010;609:327-48.,"While the prediction of a native protein structure from sequence continues to remain a challenging problem, over the past decades computational methods have become quite successful in exploiting the mechanisms behind secondary structure formation. The great effort expended in this area has resulted in the development of a vast number of secondary structure prediction methods. Especially the combination of well-optimized/sensitive machine-learning algorithms and inclusion of homologous sequence information has led to increased prediction accuracies of up to 80%. In this chapter, we will first introduce some basic notions and provide a brief history of secondary structure prediction advances. Then a comprehensive overview of state-of-the-art prediction methods will be given. Finally, we will discuss open questions and challenges in this field and provide some practical recommendations for the user."
20131763,18.0,Potential energy surfaces fitted by artificial neural networks,2010 Mar 18;114(10):3371-83.,"Molecular mechanics is the tool of choice for the modeling of systems that are so large or complex that it is impractical or impossible to model them by ab initio methods. For this reason there is a need for accurate potentials that are able to quickly reproduce ab initio quality results at the fraction of the cost. The interactions within force fields are represented by a number of functions. Some interactions are well understood and can be represented by simple mathematical functions while others are not so well understood and their functional form is represented in a simplistic manner or not even known. In the last 20 years there have been the first examples of a new design ethic, where novel and contemporary methods using machine learning, in particular, artificial neural networks, have been used to find the nature of the underlying functions of a force field. Here we appraise what has been achieved over this time and what requires further improvements, while offering some insight and guidance for the development of future force fields."
20064794,4.0,Use of statistical analysis in the biomedical informatics literature,Jan-Feb 2010;17(1):3-5.,"Statistics is an essential aspect of biomedical informatics. To examine the use of statistics in informatics research, a literature review of recent articles in two high-impact factor biomedical informatics journals, the Journal of American Medical Informatics Association (JAMIA) and the International Journal of Medical Informatics was conducted. The use of statistical methods in each paper was examined. Articles of original investigations from 2000 to 2007 were reviewed. For each journal, the results by statistical methods were analyzed as: descriptive, elementary, multivariable, other regression, machine learning, and other statistics. For both journals, descriptive statistics were most often used. Elementary statistics such as t tests, chi(2), and Wilcoxon tests were much more frequent in JAMIA, while machine learning approaches such as decision trees and support vector machines were similar in occurrence across the journals. Also, the use of diagnostic statistics such as sensitivity, specificity, precision, and recall, was more frequent in JAMIA. These results highlight the use of statistics in informatics and the need for biomedical informatics scientists to have, as a minimum, proficiency in descriptive and elementary statistics."
21713313,8.0,Computational intelligence in early diabetes diagnosis: a review,Winter 2010;7(4):252-62.,"The development of an effective diabetes diagnosis system by taking advantage of computational intelligence is regarded as a primary goal nowadays. Many approaches based on artificial network and machine learning algorithms have been developed and tested against diabetes datasets, which were mostly related to individuals of Pima Indian origin. Yet, despite high accuracies of up to 99% in predicting the correct diabetes diagnosis, none of these approaches have reached clinical application so far. One reason for this failure may be that diabetologists or clinical investigators are sparsely informed about, or trained in the use of, computational diagnosis tools. Therefore, this article aims at sketching out an outline of the wide range of options, recent developments, and potentials in machine learning algorithms as diabetes diagnosis tools. One focus is on supervised and unsupervised methods, which have made significant impacts in the detection and diagnosis of diabetes at primary and advanced stages. Particular attention is paid to algorithms that show promise in improving diabetes diagnosis. A key advance has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review presents and explains the most accurate algorithms, and discusses advantages and pitfalls of methodologies. This should provide a good resource for researchers from all backgrounds interested in computational intelligence-based diabetes diagnosis methods, and allows them to extend their knowledge into this kind of research."
20008395,8.0,Learning to represent visual input,2010 Jan 12;365(1537):177-84.,"One of the central problems in computational neuroscience is to understand how the object-recognition pathway of the cortex learns a deep hierarchy of nonlinear feature detectors. Recent progress in machine learning shows that it is possible to learn deep hierarchies without requiring any labelled data. The feature detectors are learned one layer at a time and the goal of the learning procedure is to form a good generative model of images, not to predict the class of each image. The learning procedure only requires the pairwise correlations between the activations of neuron-like processing units in adjacent layers. The original version of the learning procedure is derived from a quadratic 'energy' function but it can be extended to allow third-order, multiplicative interactions in which neurons gate the pairwise interactions between other neurons. A technique for factoring the third-order interactions leads to a learning module that again has a simple learning rule based on pairwise correlations. This module looks remarkably like modules that have been proposed by both biologists trying to explain the responses of neurons and engineers trying to create systems that can recognize objects."
19969101,50.0,Rationalizing the chemical space of protein-protein interaction inhibitors,2010 Mar;15(5-6):220-9.,"Protein-protein interactions (PPIs) are one of the next major classes of therapeutic targets, although they are too intricate to tackle with standard approaches. This is due, in part, to the inadequacy of today's chemical libraries. However, the emergence of a growing number of experimentally validated inhibitors of PPIs (i-PPIs) allows drug designers to use chemoinformatics and machine learning technologies to unravel the nature of the chemical space covered by the reported compounds. Key characteristics of i-PPIs can then be revealed and highlight the importance of specific shapes and/or aromatic bonds, enabling the design of i-PPI-enriched focused libraries and, therefore, of cost-effective screening strategies."
