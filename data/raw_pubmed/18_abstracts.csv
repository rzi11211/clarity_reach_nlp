pmid,citations,title,date,text
31052598,9.0,Revealing Drug-Target Interactions with Computational Models and Algorithms,2019 May 2;24(9):1714.,"Background:                    Identifying possible drug-target interactions (DTIs) has become an important task in drug research and development. Although high-throughput screening is becoming available, experimental methods narrow down the validation space because of extremely high cost, low success rate, and time consumption. Therefore, various computational models have been exploited to infer DTI candidates.              Methods:                    We introduced relevant databases and packages, mainly provided a comprehensive review of computational models for DTI identification, including network-based algorithms and machine learning-based methods. Specially, machine learning-based methods mainly include bipartite local model, matrix factorization, regularized least squares, and deep learning.              Results:                    Although computational methods have obtained significant improvement in the process of DTI prediction, these models have their limitations. We discussed potential avenues for boosting DTI prediction accuracy as well as further directions."
31052580,9.0,Drug Combinations: Mathematical Modeling and Networking Methods,2019 May 2;11(5):208.,"Treatments consisting of mixtures of pharmacological agents have been shown to have superior effects to treatments involving single compounds. Given the vast amount of possible combinations involving multiple drugs and the restrictions in time and resources required to test all such combinations in vitro, mathematical methods are essential to model the interactive behavior of the drug mixture and the target, ultimately allowing one to better predict the outcome of the combination. In this review, we investigate various mathematical methods that model combination therapies. This survey includes the methods that focus on predicting the outcome of drug combinations with respect to synergism and antagonism, as well as the methods that explore the dynamics of combination therapy and its role in combating drug resistance. This comprehensive investigation of the mathematical methods includes models that employ pharmacodynamics equations, those that rely on signaling and how the underlying chemical networks are affected by the topological structure of the target proteins, and models that are based on stochastic models for evolutionary dynamics. Additionally, this article reviews computational methods including mathematical algorithms, machine learning, and search algorithms that can identify promising combinations of drug compounds. A description of existing data and software resources is provided that can support investigations in drug combination therapies. Finally, the article concludes with a summary of future directions for investigation by the research community."
31051023,,Digital imaging applications and informatics in dermatology,2019 Mar 1;38(1):E43-E48.,"In this chapter, we present the use of whole slide imaging (WSI) and dermoscopy in the field of dermatology. Image digitization has allowed for increasing computer-assisted clinical decision-making. An introduction to common digital imaging data sources such as WSI and dermoscopy is provided. We also review some commonly used image quantification methods and their potential applications in dermatology. Finally, we review how machine learning approaches utilize novel large dermatology image datasets."
31051022,1.0,"The role of public challenges and data sets towards algorithm development, trust, and use in clinical practice",2019 Mar 1;38(1):E38-E42.,"In the past decade, machine learning and artificial intelligence have made significant advancements in pattern analysis, including speech and natural language processing, image recognition, object detection, facial recognition, and action categorization. Indeed, in many of these applications, accuracy has reached or exceeded human levels of performance. Subsequently, a multitude of studies have begun to examine the application of these technologies to health care, and in particular, medical image analysis. Perhaps the most difficult subdomain involves skin imaging because of the lack of standards around imaging hardware, technique, color, and lighting conditions. In addition, unlike radiological images, skin image appearance can be significantly affected by skin tone as well as the broad range of diseases. Furthermore, automated algorithm development relies on large high-quality annotated image data sets that incorporate the breadth of this circumstantial and diagnostic variety. These issues, in combination with unique complexities regarding integrating artificial intelligence systems into a clinical workflow, have led to difficulty in using these systems to improve sensitivity and specificity of skin diagnostics in health care networks around the world. In this article, we summarize recent advancements in machine learning, with a focused perspective on the role of public challenges and data sets on the progression of these technologies in skin imaging. In addition, we highlight the remaining hurdles toward effective implementation of technologies to the clinical workflow and discuss how public challenges and data sets can catalyze the development of solutions."
31051017,,"Bioinformatic applications in psoriasis: genetics, transcriptomics, and microbiomics",2019 Mar 1;38(1):E3-E11.,"Bioinformatics uses computationally intensive approaches to make sense of complex biological data sets. Here we review the role of bioinformatics in 3 areas of biology: genetics, transcriptomics, and microbiomics. Examples of bioinformatics in each area are given with respect to psoriasis and psoriatic arthritis, related inflammatory disorders at the forefront of bioinformatic research in dermatology. While bioinformatic technologies and analyses have traditionally been developed and deployed in siloes, the field of integrative omics is on the horizon. Powered by the advent of machine learning, bioinformatic integration of large data sets has the potential to dramatically revolutionize our knowledge of pathogenetic mechanisms and therapeutic targets."
31049651,3.0,Statistical learning approaches in the genetic epidemiology of complex diseases,2020 Jan;139(1):73-84.,"In this paper, we give an overview of methodological issues related to the use of statistical learning approaches when analyzing high-dimensional genetic data. The focus is set on regression models and machine learning algorithms taking genetic variables as input and returning a classification or a prediction for the target variable of interest; for example, the present or future disease status, or the future course of a disease. After briefly explaining the basic motivation and principle of these methods, we review different procedures that can be used to evaluate the accuracy of the obtained models and discuss common flaws that may lead to over-optimistic conclusions with respect to their prediction performance and usefulness."
31049622,9.0,Accelerating the implementation of biocatalysis in industry,2019 Jun;103(12):4733-4739.,"Despite enormous progress in protein engineering, complemented by bioprocess engineering, the revolution awaiting the application of biocatalysis in the fine chemical industry has still not been fully realized. In order to achieve that, further research is required on several topics, including (1) rapid methods for protein engineering using machine learning, (2) mathematical modelling of multi-enzyme cascade processes, (3) process standardization, (4) continuous process technology, (5) methods to identify improvements required to achieve industrial implementation, (6) downstream processing, (7) enzyme stability modelling and prediction, as well as (8) new reactor technology. In this brief mini-review, the status of each of these topics will be briefly discussed."
31049075,3.0,Tools to reverse-engineer multicellular systems: case studies using the fruit fly,2019 Apr 23;13:33.,"Reverse-engineering how complex multicellular systems develop and function is a grand challenge for systems bioengineers. This challenge has motivated the creation of a suite of bioengineering tools to develop increasingly quantitative descriptions of multicellular systems. Here, we survey a selection of these tools including microfluidic devices, imaging and computer vision techniques. We provide a selected overview of the emerging cross-talk between engineering methods and quantitative investigations within developmental biology. In particular, the review highlights selected recent examples from the Drosophila system, an excellent platform for understanding the interplay between genetics and biophysics. In sum, the integrative approaches that combine multiple advances in these fields are increasingly necessary to enable a deeper understanding of how to analyze both natural and synthetic multicellular systems."
31048019,36.0,Deep learning in ophthalmology: The technical and clinical considerations,2019 Sep;72:100759.,"The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally."
31047892,14.0,The behavioral phenotype of early life adversity: A 3-level meta-analysis of rodent studies,2019 Jul;102:299-307.,"Altered cognitive performance is considered an intermediate phenotype mediating early life adversity (ELA) effects on later-life development of mental disorders, e.g. depression. Whereas most human studies are limited to correlational conclusions, rodent studies can prospectively investigate how ELA alters cognitive performance in several domains. Despite the volume of reports, there is no consensus on i) the behavioral domains being affected by ELA and ii) the extent of these effects. To test how ELA (here: aberrant maternal care) affects specific behavioral domains, we used a 3-level mixed-effect meta-analysis, and thoroughly explored heterogeneity with MetaForest, a novel machine-learning approach. Our results are based on >400 independent experiments, involving ∼8600 animals. Especially in males, ELA promotes memory formation during stressful learning but impairs non-stressful learning. Furthermore, ELA increases anxiety-like and decreases social behavior. The ELA phenotype was strongest when i) combined with other negative experiences (""hits""); ii) in rats; iii) in ELA models of ∼10days duration. All data is easily accessible with MaBapp (https://osf.io/ra947/), allowing researchers to run tailor-made meta-analyses, thereby revealing the optimal choice of experimental protocols and study power."
31045948,6.0,"Artificial intelligence, osteoporosis and fragility fractures",2019 Jul;31(4):368-375.,"Purpose of review:                    Artificial intelligence tools have found new applications in medical diagnosis. These tools have the potential to capture underlying trends and patterns, otherwise impossible with previous modeling capabilities. Machine learning and deep learning models have found a role in osteoporosis, both to model the risk of fragility fracture, and to help with the identification and segmentation of images.              Recent findings:                    Here we survey the latest research in the artificial intelligence application to the prediction of osteoporosis that has been published between January 2017 and March 2019. Around half of the articles that are covered here predict (by classification or regression) an indicator of osteoporosis, such as bone mass or fragility fractures; the other half of studies use tools for automatic segmentation of the images of patients with or at risk of osteoporosis. The data for these studies include diverse signal sources: acoustics, MRI, CT, and of course, X-rays.              Summary:                    New methods for automatic image segmentation, and prediction of fracture risk show promising clinical value. Though these recent developments have had a successful initial application to osteoporosis research, their development is still under improvement, such as accounting for positive/negative class bias. We urge care when reporting accuracy metrics, and when comparing such metrics between different studies."
31044724,52.0,Big data and machine learning algorithms for health-care delivery,2019 May;20(5):e262-e273.,"Analysis of big data by machine learning offers considerable advantages for assimilation and evaluation of large amounts of complex health-care data. However, to effectively use machine learning tools in health care, several limitations must be addressed and key issues considered, such as its clinical implementation and ethics in health-care delivery. Advantages of machine learning include flexibility and scalability compared with traditional biostatistical methods, which makes it deployable for many tasks, such as risk stratification, diagnosis and classification, and survival predictions. Another advantage of machine learning algorithms is the ability to analyse diverse data types (eg, demographic data, laboratory findings, imaging data, and doctors' free-text notes) and incorporate them into predictions for disease risk, diagnosis, prognosis, and appropriate treatments. Despite these advantages, the application of machine learning in health-care delivery also presents unique challenges that require data pre-processing, model training, and refinement of the system with respect to the actual clinical problem. Also crucial are ethical considerations, which include medico-legal implications, doctors' understanding of machine learning tools, and data privacy and security. In this Review, we discuss some of the benefits and challenges of big data and machine learning in health care."
31044723,49.0,Digital pathology and artificial intelligence,2019 May;20(5):e253-e261.,"In modern clinical practice, digital pathology has a crucial role and is increasingly a technological requirement in the scientific laboratory environment. The advent of whole-slide imaging, availability of faster networks, and cheaper storage solutions has made it easier for pathologists to manage digital slide images and share them for clinical use. In parallel, unprecedented advances in machine learning have enabled the synergy of artificial intelligence and digital pathology, which offers image-based diagnosis possibilities that were once limited only to radiology and cardiology. Integration of digital slides into the pathology workflow, advanced algorithms, and computer-aided diagnostic techniques extend the frontiers of the pathologist's view beyond a microscopic slide and enable true utilisation and integration of knowledge that is beyond human limits and boundaries, and we believe there is clear potential for artificial intelligence breakthroughs in the pathology setting. In this Review, we discuss advancements in digital slide-based image diagnosis for cancer along with some challenges and opportunities for artificial intelligence in digital pathology."
31044598,4.0,Localizing epileptogenic regions using high-frequency oscillations and machine learning,2019 Apr;13(5):409-418.,"Pathological high frequency oscillations (HFOs) are putative neurophysiological biomarkers of epileptogenic brain tissue. Utilizing HFOs for epilepsy surgery planning offers the promise of improved seizure outcomes for patients with medically refractory epilepsy. This review discusses possible machine learning strategies that can be applied to HFO biomarkers to better identify epileptogenic regions. We discuss the role of HFO rate, and utilizing features such as explicit HFO properties (spectral content, duration, and power) and phase-amplitude coupling for distinguishing pathological HFO (pHFO) events from physiological HFO events. In addition, the review highlights the importance of neuroanatomical localization in machine learning strategies."
31042157,8.0,Data-Driven Blood Glucose Pattern Classification and Anomalies Detection: Machine-Learning Applications in Type 1 Diabetes,2019 May 1;21(5):e11030.,"Background:                    Diabetes mellitus is a chronic metabolic disorder that results in abnormal blood glucose (BG) regulations. The BG level is preferably maintained close to normality through self-management practices, which involves actively tracking BG levels and taking proper actions including adjusting diet and insulin medications. BG anomalies could be defined as any undesirable reading because of either a precisely known reason (normal cause variation) or an unknown reason (special cause variation) to the patient. Recently, machine-learning applications have been widely introduced within diabetes research in general and BG anomaly detection in particular. However, irrespective of their expanding and increasing popularity, there is a lack of up-to-date reviews that materialize the current trends in modeling options and strategies for BG anomaly classification and detection in people with diabetes.              Objective:                    This review aimed to identify, assess, and analyze the state-of-the-art machine-learning strategies and their hybrid systems focusing on BG anomaly classification and detection including glycemic variability (GV), hyperglycemia, and hypoglycemia in type 1 diabetes within the context of personalized decision support systems and BG alarm events applications, which are important constituents for optimal diabetes self-management.              Methods:                    A rigorous literature search was conducted between September 1 and October 1, 2017, and October 15 and November 5, 2018, through various Web-based databases. Peer-reviewed journals and articles were considered. Information from the selected literature was extracted based on predefined categories, which were based on previous research and further elaborated through brainstorming.              Results:                    The initial results were vetted using the title, abstract, and keywords and retrieved 496 papers. After a thorough assessment and screening, 47 articles remained, which were critically analyzed. The interrater agreement was measured using a Cohen kappa test, and disagreements were resolved through discussion. The state-of-the-art classes of machine learning have been developed and tested up to the task and achieved promising performance including artificial neural network, support vector machine, decision tree, genetic algorithm, Gaussian process regression, Bayesian neural network, deep belief network, and others.              Conclusions:                    Despite the complexity of BG dynamics, there are many attempts to capture hypoglycemia and hyperglycemia incidences and the extent of an individual's GV using different approaches. Recently, the advancement of diabetes technologies and continuous accumulation of self-collected health data have paved the way for popularity of machine learning in these tasks. According to the review, most of the identified studies used a theoretical threshold, which suffers from inter- and intrapatient variation. Therefore, future studies should consider the difference among patients and also track its temporal change over time. Moreover, studies should also give more emphasis on the types of inputs used and their associated time lag. Generally, we foresee that these developments might encourage researchers to further develop and test these systems on a large-scale basis."
31041822,1.0,Overview of established and emerging immunohistochemical biomarkers and their role in correlative studies in MRI,2020 Feb;51(2):341-354.,"Clinical practice in radiology and pathology requires professional expertise and many years of training to visually evaluate and interpret abnormal phenotypic features in medical images and tissue sections to generate diagnoses that guide patient management and treatment. Recent advances in digital image analysis methods and machine learning have led to significant interest in extracting additional information from medical and digital whole-slide images in radiology and pathology, respectively. This has led to significant interest and research in radiomics and pathomics to correlate phenotypic features of disease with image analytics in order to identify image-based biomarkers. The expanding role of big data in radiology and pathology parallels the development and role of immunohistochemistry (IHC) in the daily practice of pathology. IHC methods were initially developed to provide additional information to help classify tumors and then transformed into an indispensable tool to guide treatment in many types of cancer. IHC markers are used in daily practice to identify specific types of cells and highlight their distributions in tissues in order to distinguish benign from neoplastic cells, determine tumor origin, subclassify neoplasms, and support and confirm diagnoses. In this regard, radiomics, pathomics, and IHC methods are very similar since they enable the extraction of image-based features to characterize various properties of diseases. Due to the dramatic advancements in recent radiomics research, we provide a brief overview of the role of established and emerging IHC biomarkers in various tumor types that have been correlated with radiologic biomarkers to improve diagnostic accuracy, predict prognosis, guide patient management, and select treatment strategies. Level of Evidence: 5 Technical Efficacy: Stage 3 J. Magn. Reson. Imaging 2020;51:341-354."
31041615,9.0,A Special Report on Changing Trends in Preventive Stroke/Cardiovascular Risk Assessment Via B-Mode Ultrasonography,2019 May 1;21(7):25.,"Purpose of review:                    Cardiovascular disease (CVD) and stroke risk assessment have been largely based on the success of traditional statistically derived risk calculators such as Pooled Cohort Risk Score or Framingham Risk Score. However, over the last decade, automated computational paradigms such as machine learning (ML) and deep learning (DL) techniques have penetrated into a variety of medical domains including CVD/stroke risk assessment. This review is mainly focused on the changing trends in CVD/stroke risk assessment and its stratification from statistical-based models to ML-based paradigms using non-invasive carotid ultrasonography.              Recent findings:                    In this review, ML-based strategies are categorized into two types: non-image (or conventional ML-based) and image-based (or integrated ML-based). The success of conventional (non-image-based) ML-based algorithms lies in the different data-driven patterns or features which are used to train the ML systems. Typically these features are the patients' demographics, serum biomarkers, and multiple clinical parameters. The integrated (image-based) ML-based algorithms integrate the features derived from the ultrasound scans of the arterial walls (such as morphological measurements) with conventional risk factors in ML frameworks. Even though the review covers ML-based system designs for carotid and coronary ultrasonography, the main focus of the review is on CVD/stroke risk scores based on carotid ultrasound. There are two key conclusions from this review: (i) fusion of image-based features with conventional cardiovascular risk factors can lead to more accurate CVD/stroke risk stratification; (ii) the ability to handle multiple sources of information in big data framework using artificial intelligence-based paradigms (such as ML and DL) is likely to be the future in preventive CVD/stroke risk assessment."
31040236,5.0,Meta-analysis of Plasmodium falciparum var Signatures Contributing to Severe Malaria in African Children and Indian Adults,2019 Apr 30;10(2):e00217-19.,"The clinical presentation of severe Plasmodium falciparum malaria differs between children and adults, but the mechanistic basis for this remains unclear. Contributing factors to disease severity include total parasite biomass and the diverse cytoadhesive properties mediated by the polymorphic var gene parasite ligand family displayed on infected erythrocytes. To explore these factors, we performed a multicohort analysis of the contribution of var expression and parasite biomass to severe malaria in two previously published pediatric cohorts in Tanzania and Malawi and an adult cohort in India. Machine learning analysis revealed independent and complementary roles for var adhesion types and parasite biomass in adult and pediatric severe malaria and showed that similar var profiles, including upregulation of group A and DC8 var, predict severe malaria in adults and children. Among adults, patients with multiorgan complications presented infections with significantly higher parasite biomass without significant differences in var adhesion types. Conversely, pediatric patients with specific complications showed distinct var signatures. Cerebral malaria patients showed broadly increased expression of var genes, in particular group A and DC8 var, while children with severe malaria anemia were classified based on high transcription of DC8 var only. This study represents the first large multisite meta-analysis of var expression, and it demonstrates the presence of common var profiles in severe malaria patients of different ages across distant geographical sites, as well as syndrome-specific disease signatures. The complex associations between parasite biomass, var adhesion type, and clinical presentation revealed here represent the most comprehensive picture so far of the relationship between cytoadhesion, parasite load, and clinical syndrome.IMPORTANCE P. falciparum malaria can cause multiple disease complications that differ by patient age. Previous studies have attempted to address the roles of parasite adhesion and biomass in disease severity; however, these studies have been limited to single geographical sites, and there is limited understanding of how parasite adhesion and biomass interact to influence disease manifestations. In this meta-analysis, we compared parasite disease determinants in African children and Indian adults. This study demonstrates that parasite biomass and specific subsets of var genes are independently associated with detrimental outcomes in both childhood and adult malaria. We also explored how parasite var adhesion types and biomass play different roles in the development of specific severe malaria pathologies, including childhood cerebral malaria and multiorgan complications in adults. This work represents the largest study to date of the role of both var adhesion types and biomass in severe malaria."
31038827,2.0,Advanced polysomnographic analysis for OSA: A pathway to personalized management?,2020 Mar;25(3):251-258.,"Obstructive sleep apnea (OSA) is a highly heterogeneous disorder, with diverse pathways to disease, expression of disease, susceptibility to co-morbidities and response to therapy, and is ideally suited to precision medicine approaches. Clinically, the content of the information-rich polysomnogram (PSG) is not currently fully utilized in determining patient management. Novel PSG parameters such as hypoxic burden, pulse transit time, cardiopulmonary coupling and the frequency representations of PSG sensor signals could predict a variety of cardiovascular disease, cancer and neurodegeneration co-morbidities. The PSG can also be used to identify key pathophysiological parameters such as loop gain, arousal threshold and muscle compensation which can enhance understanding of the causes of OSA in an individual, and thereby guide choices on therapy. Machine learning methods performing their own parameter extraction coupled with large PSG data sets offer an exciting opportunity for discovering new links between the PSG variables and disease outcomes. By exploiting existing and emerging analytical methods, the PSG may offer a pathway to personalized management for OSA."
31038007,3.0,Predictive analytics and machine learning in stroke and neurovascular medicine,2019 Aug;41(8):681-690.,"Advances in predictive analytics and machine learning supported by an ever-increasing wealth of data and processing power are transforming almost every industry. Accuracy and precision of predictive analytics have significantly increased over the past few years and are evolving at an exponential pace. There have been significant breakthroughs in using Predictive Analytics in healthcare where it is held as the foundation of precision medicine. Yet, although the research in the field is expanding with the profuse volume of papers applying machine learning algorithms to medical data, very few have contributed meaningfully to clinical care. This lack of impact stands in stark contrast to the enormous relevance of machine learning to many other industries. Regardless of the status of its current contribution, the field of predictive analytics is expected to fundamentally change the way we diagnose and treat diseases, as well as the conduct of biomedical science research. In this review, we describe the main tools and techniques in predictive analytics and will analyze the trends in application of these techniques over the recent years. We will also provide examples of its application in medicine and more specifically in stroke and neurovascular research and outline current limitations."
31036284,2.0,Machine Learning and Other Emerging Decision Support Tools,2019 Jun;39(2):319-331.,"Emerging applications of machine learning and artificial intelligence offer the opportunity to discover new clinical knowledge through secondary exploration of existing patient medical records. This new knowledge may in turn offer a foundation to build new types of clinical decision support (CDS) that provide patient-specific insights and guidance across a wide range of clinical questions and settings. This article will provide an overview of these emerging approaches to CDS, discussing both existing technologies as well as challenges that health systems and informaticists will need to address to allow these emerging approaches to reach their full potential."
31034926,28.0,"Drug repurposing in oncology: Compounds, pathways, phenotypes and computational approaches for colorectal cancer",2019 Apr;1871(2):434-454.,"The strategy of using existing drugs originally developed for one disease to treat other indications has found success across medical fields. Such drug repurposing promises faster access of drugs to patients while reducing costs in the long and difficult process of drug development. However, the number of existing drugs and diseases, together with the heterogeneity of patients and diseases, notably including cancers, can make repurposing time consuming and inefficient. The key question we address is how to efficiently repurpose an existing drug to treat a given indication. As drug efficacy remains the main bottleneck for overall success, we discuss the need for machine-learning computational methods in combination with specific phenotypic studies along with mechanistic studies, chemical genetics and omics assays to successfully predict disease-drug pairs. Such a pipeline could be particularly important to cancer patients who face heterogeneous, recurrent and metastatic disease and need fast and personalized treatments. Here we focus on drug repurposing for colorectal cancer and describe selected therapeutics already repositioned for its prevention and/or treatment as well as potential candidates. We consider this review as a selective compilation of approaches and methodologies, and argue how, taken together, they could bring drug repurposing to the next level."
31033729,2.0,Systems serology for decoding infection and vaccine-induced antibody responses to HIV-1,2019 Jul;14(4):253-264.,"Purpose of review:                    Experimental and analytical advances have enabled systematic, high-resolution studies of humoral immune responses, and are beginning to define mechanisms of immunity to HIV.              Recent findings:                    High-throughput, information-rich experimental and analytical methods, whether genomic, proteomic, or transcriptomic, have firmly established their value across a diversity of fields. Consideration of these tools as trawlers in 'fishing expeditions' has faded as 'data-driven discovery' has come to be valued as an irreplaceable means to develop fundamental understanding of biological systems. Collectively, studies of HIV-1 infection and vaccination including functional, biophysical, and biochemical humoral profiling approaches have provided insights into the phenotypic characteristics of individual and pools of antibodies. Relating these measures to clinical status, protection/efficacy outcomes, and cellular profiling data using machine learning has offered the possibility of identifying unanticipated mechanisms of action and gaining insights into fundamental immunological processes that might otherwise be difficult to decipher.              Summary:                    Recent evidence establishes that systematic data collection and application of machine learning approaches can identify humoral immune correlates that are generalizable across distinct HIV-1 immunogens and vaccine regimens and translatable between model organisms and the clinic. These outcomes provide a strong rationale supporting the utility and further expansion of these approaches both in support of vaccine development and more broadly in defining mechanisms of immunity."
31033569,2.0,Application of machine learning in the diagnosis of axial spondyloarthritis,2019 Jul;31(4):362-367.,"Purpose of review:                    In this review article, we describe the development and application of machine-learning models in the field of rheumatology to improve the detection and diagnosis rates of underdiagnosed rheumatologic conditions, such as ankylosing spondylitis and axial spondyloarthritis (axSpA).              Recent findings:                    In an attempt to aid in the earlier diagnosis of axSpA, we developed machine-learning models to predict a diagnosis of ankylosing spondylitis and axSpA using administrative claims and electronic medical record data. Machine-learning algorithms based on medical claims data predicted the diagnosis of ankylosing spondylitis better than a model developed based on clinical characteristics of ankylosing spondylitis. With additional clinical data, machine-learning algorithms developed using electronic medical records identified patients with axSpA with 82.6-91.8% accuracy. These two algorithms have helped us understand potential opportunities and challenges associated with each data set and with different analytic approaches. Efforts to refine and validate these machine-learning models are ongoing.              Summary:                    We discuss the challenges and benefits of machine-learning models in healthcare, along with potential opportunities for its application in the field of rheumatology, particularly in the early diagnosis of axSpA and ankylosing spondylitis."
31031688,8.0,Spinal Cord Imaging in Amyotrophic Lateral Sclerosis: Historical Concepts-Novel Techniques,2019 Apr 12;10:350.,"Amyotrophic lateral sclerosis (ALS) is the most common adult onset motor neuron disease with no effective disease modifying therapies at present. Spinal cord degeneration is a hallmark feature of ALS, highlighted in the earliest descriptions of the disease by Lockhart Clarke and Jean-Martin Charcot. The anterior horns and corticospinal tracts are invariably affected in ALS, but up to recently it has been notoriously challenging to detect and characterize spinal pathology in vivo. With recent technological advances, spinal imaging now offers unique opportunities to appraise lower motor neuron degeneration, sensory involvement, metabolic alterations, and interneuron pathology in ALS. Quantitative spinal imaging in ALS has now been used in cross-sectional and longitudinal study designs, applied to presymptomatic mutation carriers, and utilized in machine learning applications. Despite its enormous clinical and academic potential, a number of physiological, technological, and methodological challenges limit the routine use of computational spinal imaging in ALS. In this review, we provide a comprehensive overview of emerging spinal cord imaging methods and discuss their advantages, drawbacks, and biomarker potential in clinical applications, clinical trial settings, monitoring, and prognostic roles."
31029649,7.0,"Advanced atherosclerosis imaging by CT: Radiomics, machine learning and deep learning",Sep-Oct 2019;13(5):274-280.,"In the last decade, technical advances in the field of medical imaging significantly improved and broadened the application of coronary CT angiography (CCTA) for the non-invasive assessment of coronary artery disease. Recently, similar breakthroughs are happening in the post-processing, analysis and interpretation of radiological images. Technologies such as radiomics allow to extract significantly more information from scans than what human visual assessment is capable of. This allows the precision phenotyping of diseases based on medical images. The increased amount of information can then be analyzed using novel data analytic techniques such as machine learning (ML) and deep learning (DL), which utilize the power of big data to build predictive models, which seek to mimic human intelligence, artificially. Thanks to big data availability and increased computational power, these novel analytic methods are outperforming conventional statistical techniques. In this current overview we describe the basics of radiomics, ML and DL, highlighting similarities, differences, limitations and potential pitfalls of these techniques. In addition, we provide a brief overview of recently published results on the applications of the aforementioned techniques for the non-invasive assessment of coronary atherosclerosis using CCTA."
31023632,3.0,"Image reconstruction: Part 1 - understanding filtered back projection, noise and image acquisition",May-Jun 2020;14(3):219-225.,"Image reconstruction is an increasingly complex field in CT. Iterative Reconstruction (IR) is at present an adjunct to standard Filtered Back Projection (FBP) reconstruction, but could become a replacement for it. Due to its potential for scanning at lower radiation doses, IR has received a lot of attention in the medical literature and all vendors offer commercial solutions. Its use in cardiovascular CT has been driven in part due to concerns about radiation dose and image quality. This paper is the first manuscript of a pair. It aims to review the basic principles of CT scanning, to describe image reconstruction using Filtered Back Projection, and to identify the physical processes that contribute to image noise which IR may be able to compensate for. The aim is to enable cardiovascular imagers to understand what happens to the raw data prior to the reconstruction process so they may have a better appreciation of the strengths and weaknesses of the various reconstruction techniques available. The second manuscript of this pair will discuss the various vendor permutations of IR in more detail, including the most recent machine learning based offerings, and critically appraise the current clinical research available on the various IR techniques used in cardiovascular CT."
31022958,1.0,"A Review of Remote Sensing Approaches for Monitoring Blue Carbon Ecosystems: Mangroves, Seagrassesand Salt Marshes during 2010⁻2018",2019 Apr 24;19(8):1933.,"Blue carbon (BC) ecosystems are an important coastal resource, as they provide a range of goods and services to the environment. They play a vital role in the global carbon cycle by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, there has been a large reduction in the global BC ecosystems due to their conversion to agriculture and aquaculture, overexploitation, and removal for human settlements. Effectively monitoring BC ecosystems at large scales remains a challenge owing to practical difficulties in monitoring and the time-consuming field measurement approaches used. As a result, sensible policies and actions for the sustainability and conservation of BC ecosystems can be hard to implement. In this context, remote sensing provides a useful tool for mapping and monitoring BC ecosystems faster and at larger scales. Numerous studies have been carried out on various sensors based on optical imagery, synthetic aperture radar (SAR), light detection and ranging (LiDAR), aerial photographs (APs), and multispectral data. Remote sensing-based approaches have been proven effective for mapping and monitoring BC ecosystems by a large number of studies. However, to the best of our knowledge, this is the first comprehensive review on the applications of remote sensing techniques for mapping and monitoring BC ecosystems. The main goal of this review is to provide an overview and summary of the key studies undertaken from 2010 onwards on remote sensing applications for mapping and monitoring BC ecosystems. Our review showed that optical imagery, such as multispectral and hyper-spectral data, is the most common for mapping BC ecosystems, while the Landsat time-series are the most widely-used data for monitoring their changes on larger scales. We investigate the limitations of current studies and suggest several key aspects for future applications of remote sensing combined with state-of-the-art machine learning techniques for mapping coastal vegetation and monitoring their extents and changes."
31022391,11.0,"Data science, artificial intelligence, and machine learning: Opportunities for laboratory medicine and the value of positive regulation",2019 Jul;69:1-7.,"Artificial intelligence (AI) and data science are rapidly developing in healthcare, as is their translation into laboratory medicine. Our review article presents an overview of the data science domain while discussing the reasons for its emergence. We also present several perspectives of its applications in clinical laboratories, along with potential ethical challenges related to AI and data science."
31021749,2.0,A Comparison of Control Strategies in Commercial and Research Knee Prostheses,2020 Jan;67(1):277-290.,"Goal:                    To provide an overview of control strategies in commercial and research microprocessor-controlled prosthetic knees (MPKs).              Methods:                    Five commercially available MPKs described in patents, and five research MPKs reported in scientific literature were compared. Their working principles, intent recognition, and walking controller were analyzed. Speed and slope adaptability of the walking controller was considered as well.              Results:                    Whereas commercial MPKs are mostly passive, i.e., do not inject energy in the system, and employ heuristic rule-based intent classifiers, research MPKs are all powered and often utilize machine learning algorithms for intention detection. Both commercial and research MPKs rely on finite state machine impedance controllers for walking. Yet while commercial MPKs require a prosthetist to adjust impedance settings, scientific research is focused on reducing the tunable parameter space and developing unified controllers, independent of subject anthropometrics, walking speed, and ground slope.              Conclusion:                    The main challenges in the field of powered, active MPKs (A-MPKs) to boost commercial viability are first to demonstrate the benefit of A-MPKs compared to passive MPKs or mechanical non-microprocessor knees using biomechanical, performance-based and patient-reported metrics. Second, to evaluate control strategies and intent recognition in an uncontrolled environment, preferably outside the laboratory setting. And third, even though research MPKs favor sophisticated algorithms, to maintain the possibility of practical and comprehensible tuning of control parameters, considering optimal control cannot be known a priori.              Significance:                    This review identifies main challenges in the development of A-MPKs, which have thus far hindered their broad availability on the market."
31019547,9.0,A Technical Review of Convolutional Neural Network-Based Mammographic Breast Cancer Diagnosis,2019 Mar 25;2019:6509357.,"This study reviews the technique of convolutional neural network (CNN) applied in a specific field of mammographic breast cancer diagnosis (MBCD). It aims to provide several clues on how to use CNN for related tasks. MBCD is a long-standing problem, and massive computer-aided diagnosis models have been proposed. The models of CNN-based MBCD can be broadly categorized into three groups. One is to design shallow or to modify existing models to decrease the time cost as well as the number of instances for training; another is to make the best use of a pretrained CNN by transfer learning and fine-tuning; the third is to take advantage of CNN models for feature extraction, and the differentiation of malignant lesions from benign ones is fulfilled by using machine learning classifiers. This study enrolls peer-reviewed journal publications and presents technical details and pros and cons of each model. Furthermore, the findings, challenges and limitations are summarized and some clues on the future work are also given. Conclusively, CNN-based MBCD is at its early stage, and there is still a long way ahead in achieving the ultimate goal of using deep learning tools to facilitate clinical practice. This review benefits scientific researchers, industrial engineers, and those who are devoted to intelligent cancer diagnosis."
31019502,2.0,2D Visualization of the Psoriasis Transcriptome Fails to Support the Existence of Dual-Secreting IL-17A/IL-22 Th17 T Cells,2019 Apr 4;10:589.,"The present paradigm of psoriasis pathogenesis revolves around the IL-23/IL-17A axis. Dual-secreting Th17 T cells presumably are the predominant sources of the psoriasis phenotype-driving cytokines, IL-17A and IL-22. We thus conducted a meta-analysis of independently acquired RNA-seq psoriasis datasets to explore the relationship between the expression of IL17A and IL22. This analysis failed to support the existence of dual secreting IL-17A/IL-22 Th17 cells as a major source of these cytokines. However, variable relationships amongst the expression of psoriasis susceptibility genes and of IL17A, IL22, and IL23A were identified. Additionally, to shed light on gene expression relationships in psoriasis, we applied a machine learning nonlinear dimensionality reduction strategy (t-SNE) to display the entire psoriasis transcriptome as a 2-dimensonal image. This analysis revealed a variety of gene clusters, relevant to psoriasis pathophysiology but failed to support a relationship between IL17A and IL22. These results support existing theories on alternative sources of IL-17A and IL-22 in psoriasis such as a Th22 cells and non-T cell populations."
31015651,115.0,Artificial intelligence in healthcare,2018 Oct;2(10):719-731.,"Artificial intelligence (AI) is gradually changing medical practice. With recent progress in digitized data acquisition, machine learning and computing infrastructure, AI applications are expanding into areas that were previously thought to be only the province of human experts. In this Review Article, we outline recent breakthroughs in AI technologies and their biomedical applications, identify the challenges for further progress in medical AI systems, and summarize the economic, legal and social implications of AI in healthcare."
31013673,1.0,Augmentative and Alternative Communication (AAC) Advances: A Review of Configurations for Individuals with a Speech Disability,2019 Apr 22;19(8):1911.,"High-tech augmentative and alternative communication (AAC) methods are on a constant rise; however, the interaction between the user and the assistive technology is still challenged for an optimal user experience centered around the desired activity. This review presents a range of signal sensing and acquisition methods utilized in conjunction with the existing high-tech AAC platforms for individuals with a speech disability, including imaging methods, touch-enabled systems, mechanical and electro-mechanical access, breath-activated methods, and brain-computer interfaces (BCI). The listed AAC sensing modalities are compared in terms of ease of access, affordability, complexity, portability, and typical conversational speeds. A revelation of the associated AAC signal processing, encoding, and retrieval highlights the roles of machine learning (ML) and deep learning (DL) in the development of intelligent AAC solutions. The demands and the affordability of most systems hinder the scale of usage of high-tech AAC. Further research is indeed needed for the development of intelligent AAC applications reducing the associated costs and enhancing the portability of the solutions for a real user's environment. The consolidation of natural language processing with current solutions also needs to be further explored for the amelioration of the conversational speeds. The recommendations for prospective advances in coming high-tech AAC are addressed in terms of developments to support mobile health communicative applications."
32477031,1.0,"An Individualized, Data-Driven Digital Approach for Precision Behavior Change",2019 Apr 25;14(3):289-293.,"Chronic disease now affects approximately half of the US population, causes 7 in 10 deaths, and accounts for roughly 80% of US health care expenditure. Because the root causes of chronic diseases are largely behavioral, effective therapies require frequent, individualized interventions that extend beyond the hospital and clinic to reach patients in their day-to-day lives. However, a mismatch currently exists between what the health care system is equipped to provide and the interventions necessary to effectively address the chronic disease burden. To remedy this health crisis, we present an individualized, data-driven digital approach for chronic disease management and prevention through precision behavior change. The rapid growth of information, biological, and communication technologies makes this an opportune time to develop digital tools that deliver precision interventions for health behavior change to address the chronic disease crisis. Building on this rapid growth, we propose a framework that includes the precise targeting of risk-producing behaviors using real-time sensing technology, machine learning data analysis to identify the most effective intervention, and delivery of that intervention with health-reinforcing feedback to provide real-time, individualized support to empower sustainable health behavior change."
31012166,4.0,Toward Design of Novel Materials for Organic Electronics,2019 Jun;31(26):e1808256.,"Materials for organic electronics are presently used in prominent applications, such as displays in mobile devices, while being intensely researched for other purposes, such as organic photovoltaics, large-area devices, and thin-film transistors. Many of the challenges to improve and optimize these applications are material related and there is a nearly infinite chemical space that needs to be explored to identify the most suitable material candidates. Established experimental approaches struggle with the size and complexity of this chemical space. Herein, the development of simulation methods is addressed, with a particular emphasis on predictive multiscale protocols, to complement experimental research in the identification of novel materials and illustrate the potential of these methods with a few prominent recent applications. Finally, the potential of machine learning and methods based on artificial intelligence is discussed to further accelerate the search for new materials."
31009920,2.0,The flux and impact of wastewater infrastructure microorganisms on human and ecosystem health,2019 Jun;57:145-150.,"Wastewater infrastructure is designed, in part, to remove microorganisms. However, many microorganisms are able to colonize infrastructure and resist treatment, resulting in an enormous flux of microorganisms to urban adjacent waters. These urban-associated microorganisms are discharged through three primary routes 1) failing infrastructure, 2) stormwater, and 3) treated wastewater effluent. Bacterial load estimates indicate failing infrastructure should be considered an equivalent source of microbial pollution as the other routes, but overall discharges are not well parameterized. More sophisticated methods, such as machine learning algorithms and microbiome characterization, are now used to track urban-derived microorganisms, including targets beyond fecal indicators, but development of methods to quantify the impact of these microbes/genes on human and ecosystem health is needed."
31009397,9.0,Machine Learning and Deep Neural Networks in Thoracic and Cardiovascular Imaging,2019 May;34(3):192-201.,"Advances in technology have always had the potential and opportunity to shape the practice of medicine, and in no medical specialty has technology been more rapidly embraced and adopted than radiology. Machine learning and deep neural networks promise to transform the practice of medicine, and, in particular, the practice of diagnostic radiology. These technologies are evolving at a rapid pace due to innovations in computational hardware and novel neural network architectures. Several cutting-edge postprocessing analysis applications are actively being developed in the fields of thoracic and cardiovascular imaging, including applications for lesion detection and characterization, lung parenchymal characterization, coronary artery assessment, cardiac volumetry and function, and anatomic localization. Cardiothoracic and cardiovascular imaging lies at the technological forefront of radiology due to a confluence of technical advances. Enhanced equipment has enabled computed tomography and magnetic resonance imaging scanners that can safely capture images that freeze the motion of the heart to exquisitely delineate fine anatomic structures. Computing hardware developments have enabled an explosion in computational capabilities and in data storage. Progress in software and fluid mechanical models is enabling complex 3D and 4D reconstructions to not only visualize and assess the dynamic motion of the heart, but also quantify its blood flow and hemodynamics. And now, innovations in machine learning, particularly in the form of deep neural networks, are enabling us to leverage the increasingly massive data repositories that are prevalent in the field. Here, we discuss developments in machine learning techniques and deep neural networks to highlight their likely role in future radiologic practice, both in and outside of image interpretation and analysis. We discuss the concepts of validation, generalizability, and clinical utility, as they pertain to this and other new technologies, and we reflect upon the opportunities and challenges of bringing these into daily use."
31007871,6.0,"Computational Prediction of MoRFs, Short Disorder-to-order Transitioning Protein Binding Regions",2019 Mar 26;17:454-462.,"Molecular recognition features (MoRFs) are short protein-binding regions that undergo disorder-to-order transitions (induced folding) upon binding protein partners. These regions are abundant in nature and can be predicted from protein sequences based on their distinctive sequence signatures. This first-of-its-kind survey covers 14 MoRF predictors and six related methods for the prediction of short protein-binding linear motifs, disordered protein-binding regions and semi-disordered regions. We show that the development of MoRF predictors has accelerated in the recent years. These predictors depend on machine learning-derived models that were generated using training datasets where MoRFs are annotated using putative disorder. Our analysis reveals that they generate accurate predictions. We identified eight methods that offer area under the ROC curve (AUC) ≥ 0.7 on experimentally-validated test datasets. We show that modern MoRF predictors accurately find experimentally annotated MoRFs even though they were trained using the putative disorder annotations. They are relatively highly-cited, particularly the methods available as webservers that on average secure three times more citations than methods without this option. MoRF predictions contribute to the experimental discovery of protein-protein interactions, annotation of protein functions and computational analysis of a variety of proteomes, protein families, and pathways. We outline future development and application directions for these tools, stressing the importance to develop novel tools that would target interactions of disordered regions with other types of partners."
31005679,2.0,Applications of machine learning in GPCR bioactive ligand discovery,2019 Apr;55:66-76.,"GPCRs constitute the largest druggable family having targets for 475 Food and Drug Administration (FDA) approved drugs. As GPCRs are of great interest to pharmaceutical industry, enormous efforts are being expended to find relevant and potent GPCR ligands as lead compounds. There are tens of millions of compounds present in different chemical databases. In order to scan this immense chemical space, computational methods, especially machine learning (ML) methods, are essential components of GPCR drug discovery pipelines. ML approaches have applications in both ligand-based and structure-based virtual screening. We present here a cheminformatics overview of ML applications to different stages of GPCR drug discovery. Focusing on olfactory receptors, which are the largest family of GPCRs, a case study for predicting agonists for an ectopic olfactory receptor, OR1G1, compares four classical ML methods."
31005165,29.0,The present and future of deep learning in radiology,2019 May;114:14-24.,"The advent of Deep Learning (DL) is poised to dramatically change the delivery of healthcare in the near future. Not only has DL profoundly affected the healthcare industry it has also influenced global businesses. Within a span of very few years, advances such as self-driving cars, robots performing jobs that are hazardous to human, and chat bots talking with human operators have proved that DL has already made large impact on our lives. The open source nature of DL and decreasing prices of computer hardware will further propel such changes. In healthcare, the potential is immense due to the need to automate the processes and evolve error free paradigms. The sheer quantum of DL publications in healthcare has surpassed other domains growing at a very fast pace, particular in radiology. It is therefore imperative for the radiologists to learn about DL and how it differs from other approaches of Artificial Intelligence (AI). The next generation of radiology will see a significant role of DL and will likely serve as the base for augmented radiology (AR). Better clinical judgement by AR will help in improving the quality of life and help in life saving decisions, while lowering healthcare costs. A comprehensive review of DL as well as its implications upon the healthcare is presented in this review. We had analysed 150 articles of DL in healthcare domain from PubMed, Google Scholar, and IEEE EXPLORE focused in medical imagery only. We have further examined the ethic, moral and legal issues surrounding the use of DL in medical imaging."
31001196,8.0,Deep Brain Stimulation Programming 2.0: Future Perspectives for Target Identification and Adaptive Closed Loop Stimulation,2019 Apr 3;10:314.,"Deep brain stimulation has developed into an established treatment for movement disorders and is being actively investigated for numerous other neurological as well as psychiatric disorders. An accurate electrode placement in the target area and the effective programming of DBS devices are considered the most important factors for the individual outcome. Recent research in humans highlights the relevance of widespread networks connected to specific DBS targets. Improving the targeting of anatomical and functional networks involved in the generation of pathological neural activity will improve the clinical DBS effect and limit side-effects. Here, we offer a comprehensive overview over the latest research on target structures and targeting strategies in DBS. In addition, we provide a detailed synopsis of novel technologies that will support DBS programming and parameter selection in the future, with a particular focus on closed-loop stimulation and associated biofeedback signals."
30999271,8.0,Machine learning and big data in psychiatry: toward clinical applications,2019 Apr;55:152-159.,"Psychiatry is a medical field concerned with the treatment of mental illness. Psychiatric disorders broadly relate to higher functions of the brain, and as such are richly intertwined with social, cultural, and experiential factors. This makes them exquisitely complex phenomena that depend on and interact with a large number of variables. Computational psychiatry provides two ways of approaching this complexity. Theory-driven computational approaches employ mechanistic models to make explicit hypotheses at multiple levels of analysis. Data-driven machine-learning approaches can make predictions from high-dimensional data and are generally agnostic as to the underlying mechanisms. Here, we review recent advances in the use of big data and machine-learning approaches toward the aim of alleviating the suffering that arises from psychiatric disorders."
30995728,,Sensors that Learn: The Evolution from Taste Fingerprints to Patterns of Early Disease Detection,2019 Apr 16;10(4):251.,"The McDevitt group has sustained efforts to develop a programmable sensing platform that offers advanced, multiplexed/multiclass chem-/bio-detection capabilities. This scalable chip-based platform has been optimized to service real-world biological specimens and validated for analytical performance. Fashioned as a sensor that learns, the platform can host new content for the application at hand. Identification of biomarker-based fingerprints from complex mixtures has a direct linkage to e-nose and e-tongue research. Recently, we have moved to the point of big data acquisition alongside the linkage to machine learning and artificial intelligence. Here, exciting opportunities are afforded by multiparameter sensing that mimics the sense of taste, overcoming the limitations of salty, sweet, sour, bitter, and glutamate sensing and moving into fingerprints of health and wellness. This article summarizes developments related to the electronic taste chip system evolving into a platform that digitizes biology and affords clinical decision support tools. A dynamic body of literature and key review articles that have contributed to the shaping of these activities are also highlighted. This fully integrated sensor promises more rapid transition of biomarker panels into wide-spread clinical practice yielding valuable new insights into health diagnostics, benefiting early disease detection."
30993807,9.0,Measurement of physical activity in clinical practice using accelerometers,2019 Aug;286(2):137-153.,"Accelerometers are commonly used in clinical and epidemiological research for more detailed measures of physical activity and to target the limitations of self-report methods. Sensors are attached at the hip, wrist and thigh, and the acceleration data are processed and calibrated in different ways to determine activity intensity, body position and/or activity type. Simple linear modelling can be used to assess activity intensity from hip and thigh data, whilst more advanced machine-learning modelling is to prefer for the wrist. The thigh position is most optimal to assess body position and activity type using machine-learning modelling. Frequency filtering and measurement resolution needs to be considered for correct assessment of activity intensity. Simple physical activity measures and statistical methods are mostly used to investigate relationship with health, but do not take advantage of all information provided by accelerometers and do not consider all components of the physical activity behaviour and their interrelationships. More advanced statistical methods are suggested that analyse patterns of multiple measures of physical activity to demonstrate stronger and more specific relationships with health. However, evaluations of accelerometer methods show considerable measurement errors, especially at individual level, which interferes with their use in clinical research and practice. Therefore, better objective methods are needed with improved data processing and calibration techniques, exploring both simple linear and machine-learning alternatives. Development and implementation of accelerometer methods into clinical research and practice requires interdisciplinary collaboration to cover all aspects contributing to useful and accurate measures of physical activity behaviours related to health."
30988417,6.0,Computational advances in combating colloidal aggregation in drug discovery,2019 May;11(5):402-418.,"Small molecule effectors are essential for drug discovery. Specific molecular recognition, reversible binding and dose-dependency are usually key requirements to ensure utility of a novel chemical entity. However, artefactual frequent-hitter and assay interference compounds may divert lead optimization and screening programmes towards attrition-prone chemical matter. Colloidal aggregates are the prime source of false positive readouts, either through protein sequestration or protein-scaffold mimicry. Nevertheless, assessment of colloidal aggregation remains somewhat overlooked and under-appreciated. In this Review, we discuss the impact of aggregation in drug discovery by analysing select examples from the literature and publicly-available datasets. We also examine and comment on technologies used to experimentally identify these potentially problematic entities. We focus on evidence-based computational filters and machine learning algorithms that may be swiftly deployed to flag chemical matter and mitigate the impact of aggregates in discovery programmes. We highlight the tools that can be used to scrutinize libraries, and identify and eliminate these problematic compounds."
30987289,4.0,Computational Methods for the Discovery of Metabolic Markers of Complex Traits,2019 Apr 4;9(4):66.,"Metabolomics uses quantitative analyses of metabolites from tissues or bodily fluids to acquire a functional readout of the physiological state. Complex diseases arise from the influence of multiple factors, such as genetics, environment and lifestyle. Since genes, RNAs and proteins converge onto the terminal downstream metabolome, metabolomics datasets offer a rich source of information in a complex and convoluted presentation. Thus, powerful computational methods capable of deciphering the effects of many upstream influences have become increasingly necessary. In this review, the workflow of metabolic marker discovery is outlined from metabolite extraction to model interpretation and validation. Additionally, current metabolomics research in various complex disease areas is examined to identify gaps and trends in the use of several statistical and computational algorithms. Then, we highlight and discuss three advanced machine-learning algorithms, specifically ensemble learning, artificial neural networks, and genetic programming, that are currently less visible, but are budding with high potential for utility in metabolomics research. With an upward trend in the use of highly-accurate, multivariate models in the metabolomics literature, diagnostic biomarker panels of complex diseases are more recently achieving accuracies approaching or exceeding traditional diagnostic procedures. This review aims to provide an overview of computational methods in metabolomics and promote the use of up-to-date machine-learning and computational methods by metabolomics researchers."
30984469,38.0,Introduction to Digital Image Analysis in Whole-slide Imaging: A White Paper from the Digital Pathology Association,2019 Mar 8;10:9.,"The advent of whole-slide imaging in digital pathology has brought about the advancement of computer-aided examination of tissue via digital image analysis. Digitized slides can now be easily annotated and analyzed via a variety of algorithms. This study reviews the fundamentals of tissue image analysis and aims to provide pathologists with basic information regarding the features, applications, and general workflow of these new tools. The review gives an overview of the basic categories of software solutions available, potential analysis strategies, technical considerations, and general algorithm readouts. Advantages and limitations of tissue image analysis are discussed, and emerging concepts, such as artificial intelligence and machine learning, are introduced. Finally, examples of how digital image analysis tools are currently being used in diagnostic laboratories, translational research, and drug development are discussed."
30983960,7.0,Childhood Trauma in Schizophrenia: Current Findings and Research Perspectives,2019 Mar 21;13:274.,"Schizophrenia is a severe neuropsychiatric disorder with persistence of symptoms throughout adult life in most of the affected patients. This unfavorable course is associated with multiple episodes and residual symptoms, mainly negative symptoms and cognitive deficits. The neural diathesis-stress model proposes that psychosocial stress acts on a pre-existing vulnerability and thus triggers the symptoms of schizophrenia. Childhood trauma is a severe form of stress that renders individuals more vulnerable to developing schizophrenia; neurobiological effects of such trauma on the endocrine system and epigenetic mechanisms are discussed. Childhood trauma is associated with impaired working memory, executive function, verbal learning, and attention in schizophrenia patients, including those at ultra-high risk to develop psychosis. In these patients, higher levels of childhood trauma were correlated with higher levels of attenuated positive symptoms, general symptoms, and depressive symptoms; lower levels of global functioning; and poorer cognitive performance in visual episodic memory end executive functions. In this review, we discuss effects of specific gene variants that interact with childhood trauma in patients with schizophrenia and describe new findings on the brain structural and functional level. Additive effects between childhood trauma and brain-derived neurotrophic factor methionine carriers on volume loss of the hippocampal subregions cornu ammonis (CA)4/dentate gyrus and CA2/3 have been reported in schizophrenia patients. A functional magnetic resonance imaging study showed that childhood trauma exposure resulted in aberrant function of parietal areas involved in working memory and of visual cortical areas involved in attention. In a theory of mind task reflecting social cognition, childhood trauma was associated with activation of the posterior cingulate gyrus, precuneus, and dorsomedial prefrontal cortex in patients with schizophrenia. In addition, decreased connectivity was shown between the posterior cingulate/precuneus region and the amygdala in patients with high levels of physical neglect and sexual abuse during childhood, suggesting that disturbances in specific brain networks underlie cognitive abilities. Finally, we discuss some of the questionnaires that are commonly used to assess childhood trauma and outline possibilities to use recent biostatistical methods, such as machine learning, to analyze the resulting datasets."
33501040,,"The Right Direction Needed to Develop White-Box Deep Learning in Radiology, Pathology, and Ophthalmology: A Short Review",2019 Apr 16;6:24.,"The popularity of deep learning (DL) in the machine learning community has been dramatically increasing since 2012. The theoretical foundations of DL are well-rooted in the classical neural network (NN). Rule extraction is not a new concept, but was originally devised for a shallow NN. For about the past 30 years, extensive efforts have been made by many researchers to resolve the ""black box"" problem of trained shallow NNs using rule extraction technology. A rule extraction technology that is well-balanced between accuracy and interpretability has recently been proposed for shallow NNs as a promising means to address this black box problem. Recently, we have been confronting a ""new black box"" problem caused by highly complex deep NNs (DNNs) generated by DL. In this paper, we first review four rule extraction approaches to resolve the black box problem of DNNs trained by DL in computer vision. Next, we discuss the fundamental limitations and criticisms of current DL approaches in radiology, pathology, and ophthalmology from the black box point of view. We also review the conversion methods from DNNs to decision trees and point out their limitations. Furthermore, we describe a transparent approach for resolving the black box problem of DNNs trained by a deep belief network. Finally, we provide a brief description to realize the transparency of DNNs generated by a convolutional NN and discuss a practical way to realize the transparency of DL in radiology, pathology, and ophthalmology."
30982123,6.0,Treatment of Heart Failure With Preserved Ejection Fraction (HFpEF): the Phenotype-Guided Approach,2019 Apr 13;21(4):20.,"The syndrome of heart failure with preserved ejection (HFpEF) continues to rise in prevalence without persuasive evidence of current pharmacologic interventions that can reduce mortality. Clinical trials thus far have generally enrolled ""all-comers"" with the clinical syndrome of heart failure and objective evidence of a preserved ejection fraction. However, HFpEF is increasingly understood to be a heterogeneous syndrome likely borne from the interplay of genetic predisposition, lifestyle factors, and high burden of associated comorbidities with each contributing to a variety of incompletely understood pathophysiologic abnormalities. Complicating management further, such abnormalities appear to be present to varying degrees among individual patients. Ongoing studies, along with the use of computational statistics/machine learning, offer the hope of clarifying the pathophysiological substrates giving rise to the syndrome of HFpEF in different patient subsets. With better understanding of the syndrome's underpinnings, there will be the potential for development of truly targeted therapies. However, for now, there is substantial evidence for the use of currently available pharmacologic device and lifestyle therapy for the optimized management of patients. Such therapy can be tailored to presently identifiable patient clusters-called ""phenotypes""-distinguished by both the presence of predominant presenting symptoms and/or predominant comorbidity profiles. Examples of clinical presentation phenotypes include lung congestion, chronotropic incompetence, pulmonary hypertension, or skeletal muscle weakness as predominant features. Additionally, such patients may have underlying metabolic syndrome, systemic (arterial) hypertension, renal dysfunction, atrial fibrillation, and/or coronary artery disease as principal underlying comorbidities. Here, we review a ""phenotype-guided"" approach to the management of patients with HFpEF, based on a stepwise method of making the HFpEF diagnosis, identifying the prominent sources of organ dysfunction, and treating accordingly."
30982079,2.0,Computational algorithms for in silico profiling of activating mutations in cancer,2019 Jul;76(14):2663-2679.,"Methods to catalog and computationally assess the mutational landscape of proteins in human cancers are desirable. One approach is to adapt evolutionary or data-driven methods developed for predicting whether a single-nucleotide polymorphism (SNP) is deleterious to protein structure and function. In cases where understanding the mechanism of protein activation and regulation is desired, an alternative approach is to employ structure-based computational approaches to predict the effects of point mutations. Through a case study of mutations in kinase domains of three proteins, namely, the anaplastic lymphoma kinase (ALK) in pediatric neuroblastoma patients, serine/threonine-protein kinase B-Raf (BRAF) in melanoma patients, and erythroblastic oncogene B 2 (ErbB2 or HER2) in breast cancer patients, we compare the two approaches above. We find that the structure-based method is most appropriate for developing a binary classification of several different mutations, especially infrequently occurring ones, concerning the activation status of the given target protein. This approach is especially useful if the effects of mutations on the interactions of inhibitors with the target proteins are being sought. However, many patients will present with mutations spread across different target proteins, making structure-based models computationally demanding to implement and execute. In this situation, data-driven methods-including those based on machine learning techniques and evolutionary methods-are most appropriate for recognizing and illuminate mutational patterns. We show, however, that, in the present status of the field, the two methods have very different accuracies and confidence values, and hence, the optimal choice of their deployment is context-dependent."
30981588,2.0,A Novel Framework for Unconscious Processing,2019 May;23(5):372-376.,"Understanding the distinction between conscious and unconscious cognition remains a priority in psychology and neuroscience. A comprehensive neurocognitive account of conscious awareness will not be possible without a sound framework to isolate and understand unconscious information processing. Here, we provide a brain-based framework that allows the identification of unconscious processes, even with null effects on behaviour."
30981085,25.0,Recent advances in physical reservoir computing: A review,2019 Jul;115:100-123.,"Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems."
30976107,92.0,Applications of machine learning in drug discovery and development,2019 Jun;18(6):463-477.,"Drug discovery and development pipelines are long, complex and depend on numerous factors. Machine learning (ML) approaches provide a set of tools that can improve discovery and decision making for well-specified questions with abundant, high-quality data. Opportunities to apply ML occur in all stages of drug discovery. Examples include target validation, identification of prognostic biomarkers and analysis of digital pathology data in clinical trials. Applications have ranged in context and methodology, with some approaches yielding accurate predictions and insights. The challenges of applying ML lie primarily with the lack of interpretability and repeatability of ML-generated results, which may limit their application. In all areas, systematic and comprehensive high-dimensional data still need to be generated. With ongoing efforts to tackle these issues, as well as increasing awareness of the factors needed to validate ML approaches, the application of ML can promote data-driven decision making and has the potential to speed up the process and reduce failure rates in drug discovery and development."
30975395,9.0,Bayesian Networks for Risk Prediction Using Real-World Data: A Tool for Precision Medicine,2019 Apr;22(4):439-445.,"Objective:                    The fields of medicine and public health are undergoing a data revolution. An increasing availability of data has brought about a growing interest in machine-learning algorithms. Our objective is to present the reader with an introduction to a knowledge representation and machine-learning tool for risk estimation in medical science known as Bayesian networks (BNs).              Study design:                    In this article we review how BNs are compact and intuitive graphical representations of joint probability distributions (JPDs) that can be used to conduct causal reasoning and risk estimation analysis and offer several advantages over regression-based methods. We discuss how BNs represent a different approach to risk estimation in that they are graphical representations of JPDs that take the form of a network representing model random variables and the influences between them, respectively.              Methods:                    We explore some of the challenges associated with traditional risk prediction methods and then describe BNs, their construction, application, and advantages in risk prediction based on examples in cancer and heart disease.              Results:                    Risk modeling with BNs has advantages over regression-based approaches, and in this article we focus on three that are relevant to health outcomes research: (1) the generation of network structures in which relationships between variables can be easily communicated; (2) their ability to apply Bayes's theorem to conduct individual-level risk estimation; and (3) their easy transformation into decision models.              Conclusions:                    Bayesian networks represent a powerful and flexible tool for the analysis of health economics and outcomes research data in the era of precision medicine."
30974797,3.0,"Novel Surface-Enhanced Raman Spectroscopy Techniques for DNA, Protein and Drug Detection",2019 Apr 10;19(7):1712.,"Surface-enhanced Raman spectroscopy (SERS) is a vibrational spectroscopic technique in which the Raman scattering signal strength of molecules, absorbed by rough metals or the surface of nanoparticles, experiences an exponential growth (10³-10⁶ times and even 1014-1015 times) because of electromagnetic or chemical enhancements. Nowadays, SERS has attracted tremendous attention in the field of analytical chemistry due to its specific advantages, including high selectivity, rich informative spectral properties, nondestructive testing, and the prominent multiplexing capabilities of Raman spectroscopy. In this review, we present the applications of state-of-the-art SERS for the detection of DNA, proteins and drugs. Moreover, we focus on highlighting the merits and mechanisms of achieving enhanced SERS signals for food safety and clinical treatment. The machine learning techniques, combined with SERS detection, are also indicated herein. This review concludes with recommendations for future studies on the development of SERS."
30973516,8.0,Artificial Intelligence and Machine Learning in Anesthesiology,2019 Dec;131(6):1346-1359.,"Commercial applications of artificial intelligence and machine learning have made remarkable progress recently, particularly in areas such as image recognition, natural speech processing, language translation, textual analysis, and self-learning. Progress had historically languished in these areas, such that these skills had come to seem ineffably bound to intelligence. However, these commercial advances have performed best at single-task applications in which imperfect outputs and occasional frank errors can be tolerated.The practice of anesthesiology is different. It embodies a requirement for high reliability, and a pressured cycle of interpretation, physical action, and response rather than any single cognitive act. This review covers the basics of what is meant by artificial intelligence and machine learning for the practicing anesthesiologist, describing how decision-making behaviors can emerge from simple equations. Relevant clinical questions are introduced to illustrate how machine learning might help solve them-perhaps bringing anesthesiology into an era of machine-assisted discovery."
30972433,2.0,From biophysics to 'omics and systems biology,2019 Jul;48(5):413-424.,"Recent decades brought a revolution to biology, driven mainly by exponentially increasing amounts of data coming from ""'omics"" sciences. To handle these data, bioinformatics often has to combine biologically heterogeneous signals, for which methods from statistics and engineering (e.g. machine learning) are often used. While such an approach is sometimes necessary, it effectively treats the underlying biological processes as a black box. Similarly, systems biology deals with inherently complex systems, characterized by a large number of degrees of freedom, and interactions that are highly non-linear. To deal with this complexity, the underlying physical interactions are often (over)simplified, such as in Boolean modelling of network dynamics. In this review, we argue for the utility of applying a biophysical approach in bioinformatics and systems biology, including discussion of two examples from our research which address sequence analysis and understanding intracellular gene expression dynamics."
30972291,12.0,Radiomics and Machine Learning for Radiotherapy in Head and Neck Cancers,2019 Mar 27;9:174.,"Introduction: An increasing number of parameters can be considered when making decisions in oncology. Tumor characteristics can also be extracted from imaging through the use of radiomics and add to this wealth of clinical data. Machine learning can encompass these parameters and thus enhance clinical decision as well as radiotherapy workflow. Methods: We performed a description of machine learning applications at each step of treatment by radiotherapy in head and neck cancers. We then performed a systematic review on radiomics and machine learning outcome prediction models in head and neck cancers. Results: Machine Learning has several promising applications in treatment planning with automatic organ at risk delineation improvements and adaptative radiotherapy workflow automation. It may also provide new approaches for Normal Tissue Complication Probability models. Radiomics may provide additional data on tumors for improved machine learning powered predictive models, not only on survival, but also on risk of distant metastasis, in field recurrence, HPV status and extra nodal spread. However, most studies provide preliminary data requiring further validation. Conclusion: Promising perspectives arise from machine learning applications and radiomics based models, yet further data are necessary for their implementation in daily care."
30972108,15.0,Machine Learning SNP Based Prediction for Precision Medicine,2019 Mar 27;10:267.,"In the past decade, precision genomics based medicine has emerged to provide tailored and effective healthcare for patients depending upon their genetic features. Genome Wide Association Studies have also identified population based risk genetic variants for common and complex diseases. In order to meet the full promise of precision medicine, research is attempting to leverage our increasing genomic understanding and further develop personalized medical healthcare through ever more accurate disease risk prediction models. Polygenic risk scoring and machine learning are two primary approaches for disease risk prediction. Despite recent improvements, the results of polygenic risk scoring remain limited due to the approaches that are currently used. By contrast, machine learning algorithms have increased predictive abilities for complex disease risk. This increase in predictive abilities results from the ability of machine learning algorithms to handle multi-dimensional data. Here, we provide an overview of polygenic risk scoring and machine learning in complex disease risk prediction. We highlight recent machine learning application developments and describe how machine learning approaches can lead to improved complex disease prediction, which will help to incorporate genetic features into future personalized healthcare. Finally, we discuss how the future application of machine learning prediction models might help manage complex disease by providing tissue-specific targets for customized, preventive interventions."
30972100,19.0,Recent Advances of Deep Learning in Bioinformatics and Computational Biology,2019 Mar 26;10:214.,"Extracting inherent valuable knowledge from omics big data remains as a daunting problem in bioinformatics and computational biology. Deep learning, as an emerging branch from machine learning, has exhibited unprecedented performance in quite a few applications from academia and industry. We highlight the difference and similarity in widely utilized models in deep learning studies, through discussing their basic structures, and reviewing diverse applications and disadvantages. We anticipate the work can serve as a meaningful perspective for further development of its theory, algorithm and application in bioinformatic and computational biology."
30971806,96.0,Deep learning: new computational modelling techniques for genomics,2019 Jul;20(7):389-403.,"As a data-driven science, genomics largely utilizes machine learning to capture dependencies in data and derive novel biological hypotheses. However, the ability to extract new insights from the exponentially increasing volume of genomics data requires more expressive machine learning models. By effectively leveraging large data sets, deep learning has transformed fields such as computer vision and natural language processing. Now, it is becoming the method of choice for many genomics modelling tasks, including predicting the impact of genetic variation on gene regulatory mechanisms such as DNA accessibility and splicing."
30970326,2.0,Artificial intelligence in reproductive medicine,2019 Oct;158(4):R139-R154.,"Artificial intelligence (AI) has experienced rapid growth over the past few years, moving from the experimental to the implementation phase in various fields, including medicine. Advances in learning algorithms and theories, the availability of large datasets and improvements in computing power have contributed to breakthroughs in current AI applications. Machine learning (ML), a subset of AI, allows computers to detect patterns from large complex datasets automatically and uses these patterns to make predictions. AI is proving to be increasingly applicable to healthcare, and multiple machine learning techniques have been used to improve the performance of assisted reproductive technology (ART). Despite various challenges, the integration of AI and reproductive medicine is bound to give an essential direction to medical development in the future. In this review, we discuss the basic aspects of AI and machine learning, and we address the applications, potential limitations and challenges of AI. We also highlight the prospects and future directions in the context of reproductive medicine."
30968771,2.0,The Computational Models of Drug-target Interaction Prediction,2020;27(5):348-358.,"The identification of Drug-Target Interactions (DTIs) is an important process in drug discovery and medical research. However, the tradition experimental methods for DTIs identification are still time consuming, extremely expensive and challenging. In the past ten years, various computational methods have been developed to identify potential DTIs. In this paper, the identification methods of DTIs are summarized. What's more, several state-of-the-art computational methods are mainly introduced, containing network-based method and machine learning-based method. In particular, for machine learning-based methods, including the supervised and semisupervised models, have essential differences in the approach of negative samples. Although these effective computational models in identification of DTIs have achieved significant improvements, network-based and machine learning-based methods have their disadvantages, respectively. These computational methods are evaluated on four benchmark data sets via values of Area Under the Precision Recall curve (AUPR)."
30963580,13.0,Knowledge-based planning for intensity-modulated radiation therapy: A review of data-driven approaches,2019 Jun;46(6):2760-2775.,"Purpose:                    Intensity-Modulated Radiation Therapy (IMRT), including its variations (including IMRT, Volumetric Arc Therapy (VMAT), and Tomotherapy), is a widely used and critically important technology for cancer treatment. It is a knowledge-intensive technology due not only to its own technical complexity, but also to the inherently conflicting nature of maximizing tumor control while minimizing normal organ damage. As IMRT experience and especially the carefully designed clinical plan data are accumulated during the past two decades, a new set of methods commonly termed knowledge-based planning (KBP) have been developed that aim to improve the quality and efficiency of IMRT planning by learning from the database of past clinical plans. Some of this development has led to commercial products recently that allowed the investigation of KBP in numerous clinical applications. In this literature review, we will attempt to present a summary of published methods of knowledge-based approaches in IMRT and recent clinical validation results.              Methods:                    In March 2018, a literature search was conducted in the NIH Medline database using the PubMed interface to identify publications that describe methods and validations related to KBP in IMRT including variations such as VMAT and Tomotherapy. The search criteria were designed to have a broad scope to capture relevant results with high sensitivity. The authors filtered down the search results according to a predefined selection criteria by reviewing the titles and abstracts first and then by reviewing the full text. A few papers were added to the list based on the references of the reviewed papers. The final set of papers was reviewed and summarized here.              Results:                    The initial search yielded a total of 740 articles. A careful review of the titles, abstracts, and eventually the full text and then adding relevant articles from reviewing the references resulted in a final list of 73 articles published between 2011 and early 2018. These articles described methods for developing knowledge models for predicting such parameters as dosimetric and dose-volume points, voxel-level doses, and objective function weights that improve or automate IMRT planning for various cancer sites, addressing different clinical and quality assurance needs, and using a variety of machine learning approaches. A number of articles reported carefully designed clinical studies that assessed the performance of KBP models in realistic clinical applications. Overwhelming majority of the studies demonstrated the benefits of KBP in achieving comparable and often improved quality of IMRT planning while reducing planning time and plan quality variation.              Conclusions:                    The number of KBP-related studies has been steadily increasing since 2011 indicating a growing interest in applying this approach to clinical applications. Validation studies have generally shown KBP to produce plans with quality comparable to expert planners while reducing the time and efforts to generate plans. However, current studies are mostly retrospective and leverage relatively small datasets. Larger datasets collected through multi-institutional collaboration will enable the development of more advanced models to further improve the performance of KBP in complex clinical cases. Prospective studies will be an important next step toward widespread adoption of this exciting technology."
30962048,7.0,Canadian Association of Radiologists White Paper on Ethical and Legal Issues Related to Artificial Intelligence in Radiology,2019 May;70(2):107-118.,"Artificial intelligence (AI) software that analyzes medical images is becoming increasingly prevalent. Unlike earlier generations of AI software, which relied on expert knowledge to identify imaging features, machine learning approaches automatically learn to recognize these features. However, the promise of accurate personalized medicine can only be fulfilled with access to large quantities of medical data from patients. This data could be used for purposes such as predicting disease, diagnosis, treatment optimization, and prognostication. Radiology is positioned to lead development and implementation of AI algorithms and to manage the associated ethical and legal challenges. This white paper from the Canadian Association of Radiologists provides a framework for study of the legal and ethical issues related to AI in medical imaging, related to patient data (privacy, confidentiality, ownership, and sharing); algorithms (levels of autonomy, liability, and jurisprudence); practice (best practices and current legal framework); and finally, opportunities in AI from the perspective of a universal health care system."
30959828,15.0,Current State and Future Directions of Technology-Based Ecological Momentary Assessment and Intervention for Major Depressive Disorder: A Systematic Review,2019 Apr 5;8(4):465.,"Ecological momentary assessment (EMA) and ecological momentary intervention (EMI) are alternative approaches to retrospective self-reports and face-to-face treatments, and they make it possible to repeatedly assess patients in naturalistic settings and extend psychological support into real life. The increase in smartphone applications and the availability of low-cost wearable biosensors have further improved the potential of EMA and EMI, which, however, have not yet been applied in clinical practice. Here, we conducted a systematic review, using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, to explore the state of the art of technology-based EMA and EMI for major depressive disorder (MDD). A total of 33 articles were included (EMA = 26; EMI = 7). First, we provide a detailed analysis of the included studies from technical (sampling methods, duration, prompts), clinical (fields of application, adherence rates, dropouts, intervention effectiveness), and technological (adopted devices) perspectives. Then, we identify the advantages of using information and communications technologies (ICTs) to extend the potential of these approaches to the understanding, assessment, and intervention in depression. Furthermore, we point out the relevant issues that still need to be addressed within this field, and we discuss how EMA and EMI could benefit from the use of sensors and biosensors, along with recent advances in machine learning for affective modelling."
30959445,33.0,"Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis",2019 May;54:280-296.,"Machine learning (ML) algorithms have made a tremendous impact in the field of medical imaging. While medical imaging datasets have been growing in size, a challenge for supervised ML algorithms that is frequently mentioned is the lack of annotated data. As a result, various methods that can learn with less/other types of supervision, have been proposed. We give an overview of semi-supervised, multiple instance, and transfer learning in medical imaging, both in diagnosis or segmentation tasks. We also discuss connections between these learning scenarios, and opportunities for future research. A dataset with the details of the surveyed papers is available via https://figshare.com/articles/Database_of_surveyed_literature_in_Not-so-supervised_a_survey_of_semi-supervised_multi-instance_and_transfer_learning_in_medical_image_analysis_/7479416."
30953518,16.0,Are innovation and new technologies in precision medicine paving a new era in patients centric care?,2019 Apr 5;17(1):114.,"Healthcare is undergoing a transformation, and it is imperative to leverage new technologies to generate new data and support the advent of precision medicine (PM). Recent scientific breakthroughs and technological advancements have improved our understanding of disease pathogenesis and changed the way we diagnose and treat disease leading to more precise, predictable and powerful health care that is customized for the individual patient. Genetic, genomics, and epigenetic alterations appear to be contributing to different diseases. Deep clinical phenotyping, combined with advanced molecular phenotypic profiling, enables the construction of causal network models in which a genomic region is proposed to influence the levels of transcripts, proteins, and metabolites. Phenotypic analysis bears great importance to elucidat the pathophysiology of networks at the molecular and cellular level. Digital biomarkers (BMs) can have several applications beyond clinical trials in diagnostics-to identify patients affected by a disease or to guide treatment. Digital BMs present a big opportunity to measure clinical endpoints in a remote, objective and unbiased manner. However, the use of ""omics"" technologies and large sample sizes have generated massive amounts of data sets, and their analyses have become a major bottleneck requiring sophisticated computational and statistical methods. With the wealth of information for different diseases and its link to intrinsic biology, the challenge is now to turn the multi-parametric taxonomic classification of a disease into better clinical decision-making by more precisely defining a disease. As a result, the big data revolution has provided an opportunity to apply artificial intelligence (AI) and machine learning algorithms to this vast data set. The advancements in digital health opportunities have also arisen numerous questions and concerns on the future of healthcare practices in particular with what regards the reliability of AI diagnostic tools, the impact on clinical practice and vulnerability of algorithms. AI, machine learning algorithms, computational biology, and digital BMs will offer an opportunity to translate new data into actionable information thus, allowing earlier diagnosis and precise treatment options. A better understanding and cohesiveness of the different components of the knowledge network is a must to fully exploit the potential of it."
30950797,19.0,Applications of Machine Learning in Real-Life Digital Health Interventions: Review of the Literature,2019 Apr 5;21(4):e12286.,"Background:                    Machine learning has attracted considerable research interest toward developing smart digital health interventions. These interventions have the potential to revolutionize health care and lead to substantial outcomes for patients and medical professionals.              Objective:                    Our objective was to review the literature on applications of machine learning in real-life digital health interventions, aiming to improve the understanding of researchers, clinicians, engineers, and policy makers in developing robust and impactful data-driven interventions in the health care domain.              Methods:                    We searched the PubMed and Scopus bibliographic databases with terms related to machine learning, to identify real-life studies of digital health interventions incorporating machine learning algorithms. We grouped those interventions according to their target (ie, target condition), study design, number of enrolled participants, follow-up duration, primary outcome and whether this had been statistically significant, machine learning algorithms used in the intervention, and outcome of the algorithms (eg, prediction).              Results:                    Our literature search identified 8 interventions incorporating machine learning in a real-life research setting, of which 3 (37%) were evaluated in a randomized controlled trial and 5 (63%) in a pilot or experimental single-group study. The interventions targeted depression prediction and management, speech recognition for people with speech disabilities, self-efficacy for weight loss, detection of changes in biopsychosocial condition of patients with multiple morbidity, stress management, treatment of phantom limb pain, smoking cessation, and personalized nutrition based on glycemic response. The average number of enrolled participants in the studies was 71 (range 8-214), and the average follow-up study duration was 69 days (range 3-180). Of the 8 interventions, 6 (75%) showed statistical significance (at the P=.05 level) in health outcomes.              Conclusions:                    This review found that digital health interventions incorporating machine learning algorithms in real-life studies can be useful and effective. Given the low number of studies identified in this review and that they did not follow a rigorous machine learning evaluation methodology, we urge the research community to conduct further studies in intervention settings following evaluation principles and demonstrating the potential of machine learning in clinical practice."
30950770,,Identification of novel analgesics through a drug repurposing strategy,2019 Jul 1;9(4):399-415.,"The identification of new indications for approved or failed drugs is a process called drug repositioning or drug repurposing. The motivation includes overcoming the productivity gap that exists in drug development, which is a high-cost-high-risk process. Repositioning also includes rescuing drugs that have safely entered the market but have failed to demonstrate sufficient efficiency for the initial clinical indication. Considering the high prevalence of chronic pain, the lack of sufficient efficacy and the safety issues of current analgesics, repositioning seems to be an attractive approach. This review presents example of drugs that already have been repositioned and highlights new technologies that are available for the identification of additional compounds to stimulate the curiosity of readers for further exploration."
30950746,3.0,Machine Learning Approaches to Analyze Speech-Evoked Neurophysiological Responses,2019 Mar 25;62(3):587-601.,"Purpose Speech-evoked neurophysiological responses are often collected to answer clinically and theoretically driven questions concerning speech and language processing. Here, we highlight the practical application of machine learning (ML)-based approaches to analyzing speech-evoked neurophysiological responses. Method Two categories of ML-based approaches are introduced: decoding models, which generate a speech stimulus output using the features from the neurophysiological responses, and encoding models, which use speech stimulus features to predict neurophysiological responses. In this review, we focus on (a) a decoding model classification approach, wherein speech-evoked neurophysiological responses are classified as belonging to 1 of a finite set of possible speech events (e.g., phonological categories), and (b) an encoding model temporal response function approach, which quantifies the transformation of a speech stimulus feature to continuous neural activity. Results We illustrate the utility of the classification approach to analyze early electroencephalographic (EEG) responses to Mandarin lexical tone categories from a traditional experimental design, and to classify EEG responses to English phonemes evoked by natural continuous speech (i.e., an audiobook) into phonological categories (plosive, fricative, nasal, and vowel). We also demonstrate the utility of temporal response function to predict EEG responses to natural continuous speech from acoustic features. Neural metrics from the 3 examples all exhibit statistically significant effects at the individual level. Conclusion We propose that ML-based approaches can complement traditional analysis approaches to analyze neurophysiological responses to speech signals and provide a deeper understanding of natural speech and language processing using ecologically valid paradigms in both typical and clinical populations."
30950665,1.0,Enabling artificial intelligence in high acuity medical environments,2019 Apr;28(2):120-126.,"Acute patient treatment can heavily profit from AI-based assistive and decision support systems, in terms of improved patient outcome as well as increased efficiency. Yet, only very few applications have been reported because of the limited accessibility of device data due to the lack of adoption of open standards, and the complexity of regulatory/approval requirements for AI-based systems. The fragmentation of data, still being stored in isolated silos, results in limited accessibility for AI in healthcare and machine learning is complicated by the loss of semantics in data conversions. We outline a reference model that addresses the requirements of innovative AI-based research systems as well as the clinical reality. The integration of networked medical devices and Clinical Repositories based on open standards, such as IEEE 11073 SDC and HL7 FHIR, will foster novel assistance and decision support. The reference model will make point-of-care device data available for AI-based approaches. Semantic interoperability between Clinical and Research Repositories will allow correlating patient data, device data, and the patient outcome. Thus, complete workflows in high acuity environments can be analysed. Open semantic interoperability will enable the improvement of patient outcome and the increase of efficiency on a large scale and across clinical applications."
30949431,1.0,Advances in automated tongue diagnosis techniques,2019 Mar;8(1):42-56.,"Tongue diagnosis can be an effective, noninvasive method to perform an auxiliary diagnosis any time anywhere, which can support the global need in the primary healthcare system. This work reviews the recent advances in tongue diagnosis, which is a significant constituent of traditional oriental medicinal technology, and explores the literature to evaluate the works done on the various aspects of computerized tongue diagnosis, namely preprocessing, tongue detection, segmentation, feature extraction, tongue analysis, especially in traditional Chinese medicine (TCM). In spite of huge volume of work done on automatic tongue diagnosis (ATD), there is a lack of adequate survey, especially to combine it with the current diagnosis trends. This paper studies the merits, capabilities, and associated research gaps in current works on ATD systems. After exploring the algorithms used in tongue diagnosis, the current trend and global requirements in health domain motivates us to propose a conceptual framework for the automated tongue diagnostic system on mobile enabled platform. This framework will be able to connect tongue diagnosis with the future point-of-care health system."
30943796,6.0,An overview of thermal necrosis: present and future,2019 Sep;35(9):1555-1562.,"Introduction: Many orthopaedic procedures require drilling of bone, especially fracture repair cases. Bone drilling results in heat generation due to the friction between the bone and the drill bit. A high-level of heat generation kills bone cells. Bone cell death results in resorption of bone around bone screws.Methods: We searched in the literature for data on parameters that influence drilling bone and could lead to thermal necrosis. The points of view of many orthopaedists and neurosurgeons based upon on previous practices and clinical experience are presented.Results: Several potential complications that lead to thermal necrosis are discussed and highlighted.Discussion: Even in the face of growing evidence as to the negative effects of heat induction during drilling, simple and effective methods for monitoring and cooling in real-time are not in widespread usage today. For that purpose, we propose some suggestions for the future of bone drilling, taking note of recent advances in autonomous robotics, intelligent systems and computer simulation techniques.Conclusions: These advances in prevention of thermal necrosis during bone drilling surgery are expected to reduce the risk of patient injury and costs for the health service."
30937909,22.0,Field crop phenomics: enabling breeding for radiation use efficiency and biomass in cereal crops,2019 Sep;223(4):1714-1727.,"Plant phenotyping forms the core of crop breeding, allowing breeders to build on physiological traits and mechanistic science to inform their selection of material for crossing and genetic gain. Recent rapid progress in high-throughput techniques based on machine vision, robotics, and computing (plant phenomics) enables crop physiologists and breeders to quantitatively measure complex and previously intractable traits. By combining these techniques with affordable genomic sequencing and genotyping, machine learning, and genome selection approaches, breeders have an opportunity to make rapid genetic progress. This review focuses on how field-based plant phenomics can enable next-generation physiological breeding in cereal crops for traits related to radiation use efficiency, photosynthesis, and crop biomass. These traits have previously been regarded as difficult and laborious to measure but have recently become a focus as cereal breeders find genetic progress from 'Green Revolution' traits such as harvest index become exhausted. Application of LiDAR, thermal imaging, leaf and canopy spectral reflectance, Chl fluorescence, and machine learning are discussed using wheat and sorghum phenotyping as case studies. A vision of how crop genomics and high-throughput phenotyping could enable the next generation of crop research and breeding is presented."
30935123,9.0,Precision Livestock Farming in Swine Welfare: A Review for Swine Practitioners,2019 Mar 31;9(4):133.,"The burgeoning research and applications of technological advances are launching the development of precision livestock farming. Through sensors (cameras, microphones and accelerometers), images, sounds and movements are combined with algorithms to non-invasively monitor animals to detect their welfare and predict productivity. In turn, this remote monitoring of livestock can provide quantitative and early alerts to situations of poor welfare requiring the stockperson's attention. While swine practitioners' skills include translation of pig data entry into pig health and well-being indices, many do not yet have enough familiarity to advise their clients on the adoption of precision livestock farming practices. This review, intended for swine veterinarians and specialists, (1) includes an introduction to algorithms and machine learning, (2) summarizes current literature on relevant sensors and sensor network systems, and drawing from industry pig welfare audit criteria, (3) explains how these applications can be used to improve swine welfare and meet current pork production stakeholder expectations. Swine practitioners, by virtue of their animal and client advocacy roles, interpretation of benchmarking data, and stewardship in regulatory and traceability programs, can play a broader role as advisors in the transfer of precision livestock farming technology, and its implications to their clients."
30927499,2.0,Systems Metabolic Engineering Meets Machine Learning: A New Era for Data-Driven Metabolic Engineering,2019 Sep;14(9):e1800416.,"The recent increase in high-throughput capacity of 'omics datasets combined with advances and interest in machine learning (ML) have created great opportunities for systems metabolic engineering. In this regard, data-driven modeling methods have become increasingly valuable to metabolic strain design. In this review, the nature of 'omics is discussed and a broad introduction to the ML algorithms combining these datasets into predictive models of metabolism and metabolic rewiring is provided. Next, this review highlights recent work in the literature that utilizes such data-driven methods to inform various metabolic engineering efforts for different classes of application including product maximization, understanding and profiling phenotypes, de novo metabolic pathway design, and creation of robust system-scale models for biotechnology. Overall, this review aims to highlight the potential and promise of using ML algorithms with metabolic engineering and systems biology related datasets."
30926431,11.0,Quality assurance of computer-aided detection and diagnosis in colonoscopy,2019 Jul;90(1):55-63.,"Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field ""deep learning,"" have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice-polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing."
30923883,7.0,"Machine learning concepts, concerns and opportunities for a pediatric radiologist",2019 Apr;49(4):509-516.,"Machine learning, a subfield of artificial intelligence, is a rapidly evolving technology that offers great potential for expanding the quality and value of pediatric radiology. We describe specific types of learning, including supervised, unsupervised and semisupervised. Subsequently, we illustrate two core concepts for the reader: data partitioning and under/overfitting. We also provide an expanded discussion of the challenges of implementing machine learning in children's imaging. These include the requirement for very large data sets, the need to accurately label these images with a relatively small number of pediatric imagers, technical and regulatory hurdles, as well as the opaque character of convolution neural networks. We review machine learning cases in radiology including detection, classification and segmentation. Last, three pediatric radiologists from the Society for Pediatric Radiology Quality and Safety Committee share perspectives for potential areas of development."
30917031,11.0,Stem cells and extracellular vesicles: biological regulators of physiology and disease,2019 Aug 1;317(2):C155-C166.,"Many different subpopulations of subcellular extracellular vesicles (EVs) have been described. EVs are released from all cell types and have been shown to regulate normal physiological homeostasis, as well as pathological states by influencing cell proliferation, differentiation, organ homing, injury and recovery, as well as disease progression. In this review, we focus on the bidirectional actions of vesicles from normal and diseased cells on normal or leukemic target cells; and on the leukemic microenvironment as a whole. EVs from human bone marrow mesenchymal stem cells (MSC) can have a healing effect, reversing the malignant phenotype in prostate and colorectal cancer, as well as mitigating radiation damage to marrow. The role of EVs in leukemia and their bimodal cross talk with the encompassing microenvironment remains to be fully characterized. This may provide insight for clinical advances via the application of EVs as potential therapy and the employment of statistical and machine learning models to capture the pleiotropic effects EVs endow to a dynamic microenvironment, possibly allowing for precise therapeutic intervention."
30912017,2.0,In Silico Drug-Target Profiling,2019;1953:89-103.,"Pharmacological science is trying to establish the link between chemicals, targets, and disease-related phenotypes. A plethora of chemical proteomics and structural data have been generated, thanks to the target-based approach that has dominated drug discovery at the turn of the century. There is an invaluable source of information for in silico target profiling. Prediction is based on the principle of chemical similarity (similar drugs bind similar targets) or on first principles from the biophysics of molecular interactions. In the first case, compound comparison is made through ligand-based chemical similarity search or through classifier-based machine learning approach. The 3D techniques are based on 3D structural descriptors or energy-based scoring scheme to infer a binding affinity of a compound with its putative target. More recently, a new approach based on compound set metric has been proposed in which a query compound is compared with a whole of compounds associated with a target or a family of targets. This chapter reviews the different techniques of in silico target profiling and their main applications such as inference of unwanted targets, drug repurposing, or compound prioritization after phenotypic-based screening campaigns."
30909106,5.0,Computational design for thermostabilization of GPCRs,2019 Apr;55:25-33.,"GPCR superfamily is the largest clinically relevant family of targets in human genome; however, low thermostability and high conformational plasticity of these integral membrane proteins make them notoriously hard to handle in biochemical, biophysical, and structural experiments. Here, we describe the recent advances in computational approaches to design stabilizing mutations for GPCR that take advantage of the structural and sequence conservation properties of the receptors, and employ machine learning on accumulated mutation data for the superfamily. The fast and effective computational tools can provide a viable alternative to existing experimental mutation screening and are poised for further improvements with expansion of thermostability datasets for training the machine learning models. The rapidly growing practical applications of computational stability design streamline GPCR structure determination and may contribute to more efficient drug discovery."
30909105,2.0,Automated discovery of GPCR bioactive ligands,2019 Apr;55:17-24.,"While G-protein-coupled receptors (GPCRs) constitute the largest class of membrane proteins, structures and endogenous ligands of a large portion of GPCRs remain unknown. Because of the involvement of GPCRs in various signaling pathways and physiological roles, the identification of endogenous ligands as well as designing novel drugs is of high interest to the research and medical communities. Along with highlighting the recent advances in structure-based ligand discovery, including docking and molecular dynamics, this article focuses on the latest advances for automating the discovery of bioactive ligands using machine learning. Machine learning is centered around the development and applications of algorithms that can learn from data automatically. Such an approach offers immense opportunities for bioactivity prediction as well as quantitative structure-activity relationship studies. This review describes the most recent and successful applications of machine learning for bioactive ligand discovery, concluding with an outlook on deep learning methods that are capable of automatically extracting salient information from structural data as a promising future direction for rapid and efficient bioactive ligand discovery."
30906397,2.0,Artificial Neural Network: Understanding the Basic Concepts without Mathematics,2018 Sep;17(3):83-89.,"Machine learning is where a machine (i.e., computer) determines for itself how input data is processed and predicts outcomes when provided with new data. An artificial neural network is a machine learning algorithm based on the concept of a human neuron. The purpose of this review is to explain the fundamental concepts of artificial neural networks."
30898903,11.0,Machine learning for data-driven discovery in solid Earth geoscience,2019 Mar 22;363(6433):eaau0323.,"Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth's behavior and by the inaccessibility of nearly all of Earth's subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated."
30898381,23.0,Artificial intelligence in breast imaging,2019 May;74(5):357-366.,"This article reviews current limitations and future opportunities for the application of computer-aided detection (CAD) systems and artificial intelligence in breast imaging. Traditional CAD systems in mammography screening have followed a rules-based approach, incorporating domain knowledge into hand-crafted features before using classical machine learning techniques as a classifier. The first commercial CAD system, ImageChecker M1000, relies on computer vision techniques for pattern recognition. Unfortunately, CAD systems have been shown to adversely affect some radiologists' performance and increase recall rates. The Digital Mammography DREAM Challenge was a multidisciplinary collaboration that provided 640,000 mammography images for teams to help decrease false-positive rates in breast cancer screening. Winning solutions leveraged deep learning's (DL) automatic hierarchical feature learning capabilities and used convolutional neural networks. Start-ups Therapixel and Kheiron Medical Technologies are using DL for breast cancer screening. With increasing use of digital breast tomosynthesis, specific artificial intelligence (AI)-CAD systems are emerging to include iCAD's PowerLook Tomo Detection and ScreenPoint Medical's Transpara. Other AI-CAD systems are focusing on breast diagnostic techniques such as ultrasound and magnetic resonance imaging (MRI). There is a gap in the market for contrast-enhanced spectral mammography AI-CAD tools. Clinical implementation of AI-CAD tools requires testing in scenarios mimicking real life to prove its usefulness in the clinical environment. This requires a large and representative dataset for testing and assessment of the reader's interaction with the tools. A cost-effectiveness assessment should be undertaken, with a large feasibility study carried out to ensure there are no unintended consequences. AI-CAD systems should incorporate explainable AI in accordance with the European Union General Data Protection Regulation (GDPR)."
30898263,15.0,Rise of the Machines: Advances in Deep Learning for Cancer Diagnosis,2019 Mar;5(3):157-169.,"Deep learning refers to a set of computer models that have recently been used to make unprecedented progress in the way computers extract information from images. These algorithms have been applied to tasks in numerous medical specialties, most extensively radiology and pathology, and in some cases have attained performance comparable to human experts. Furthermore, it is possible that deep learning could be used to extract data from medical images that would not be apparent by human analysis and could be used to inform on molecular status, prognosis, or treatment sensitivity. In this review, we outline the current developments and state-of-the-art in applying deep learning for cancer diagnosis, and discuss the challenges in adapting the technology for widespread clinical deployment."
30898208,40.0,Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review,2019 Mar 26;73(11):1317-1335.,"Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with ""big data"" from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care."
30897793,1.0,Sketching the Power of Machine Learning to Decrypt a Neural Systems Model of Behavior,2019 Mar 20;9(3):67.,"Uncovering brain-behavior mechanisms is the ultimate goal of neuroscience. A formidable amount of discoveries has been made in the past 50 years, but the very essence of brain-behavior mechanisms still escapes us. The recent exploitation of machine learning (ML) tools in neuroscience opens new avenues for illuminating these mechanisms. A key advantage of ML is to enable the treatment of large data, combing highly complex processes. This essay provides a glimpse of how ML tools could test a heuristic neural systems model of motivated behavior, the triadic neural systems model, which was designed to understand behavioral transitions in adolescence. This essay previews analytic strategies, using fictitious examples, to demonstrate the potential power of ML to decrypt the neural networks of motivated behavior, generically and across development. Of note, our intent is not to provide a tutorial for these analyses nor a pipeline. The ultimate objective is to relate, as simply as possible, how complex neuroscience constructs can benefit from ML methods for validation and further discovery. By extension, the present work provides a guide that can serve to query the mechanisms underlying the contributions of prefrontal circuits to emotion regulation. The target audience concerns mainly clinical neuroscientists. As a caveat, this broad approach leaves gaps, for which references to comprehensive publications are provided."
30893115,5.0,Prediction of postoperative pulmonary complications,2019 Jun;32(3):443-451.,"Purpose of review:                    Prediction of postoperative pulmonary complications (PPCs) enables individually applied preventive measures and maybe even early treatment if a PPC eventually starts to develop. The purpose of this review is to describe crucial steps in the development and validation of prediction models, examine these steps in the current literature and describe what the future holds for PPC prediction.              Recent findings:                    A systematic search of the medical literature identified 21 articles reporting on prediction models for PPCs. The studies were heterogeneous with regard to design, derivation cohort and whether or not a validation cohort was used. Furthermore, as definitions for PPCs varied substantially, PPC rates were quite different. One-third of the studies had a sufficient sample size for building a prediction model. In most articles, an internal validation step was reported, suggesting a good fit. In the four articles that reported an externally validation step, in three the prognostic model performed less well in external validation. The ARISCAT risk score was the only score that kept sufficient predictive power in external validation, albeit that the sample sizes of the cohorts used may have been too small. Analysis by machine learning could help building new prediction models, as unbiased cluster analyses could uncover clusters of patients with specific underlying pathophysiological mechanisms. Adding biomarkers to the model could optimize identification of biological phenotypes of risk groups.              Summary:                    Many predictive models for PPCs have been reported on. Development of more robust PPC prediction models could be supported by machine learning."
30892723,9.0,Machine Learning for Prediction of Posttraumatic Stress and Resilience Following Trauma: An Overview of Basic Concepts and Recent Advances,2019 Apr;32(2):215-225.,"Posttraumatic stress responses are characterized by a heterogeneity in clinical appearance and etiology. This heterogeneity impacts the field's ability to characterize, predict, and remediate maladaptive responses to trauma. Machine learning (ML) approaches are increasingly utilized to overcome this foundational problem in characterization, prediction, and treatment selection across branches of medicine that have struggled with similar clinical realities of heterogeneity in etiology and outcome, such as oncology. In this article, we review and evaluate ML approaches and applications utilized in the areas of posttraumatic stress, stress pathology, and resilience research, and present didactic information and examples to aid researchers interested in the relevance of ML to their own research. The examined studies exemplify the high potential of ML approaches to build accurate predictive and diagnostic models of posttraumatic stress and stress pathology risk based on diverse sources of available information. The use of ML approaches to integrate high-dimensional data demonstrates substantial gains in risk prediction even when the sources of data are the same as those used in traditional predictive models. This area of research will greatly benefit from collaboration and data sharing among researchers of posttraumatic stress disorder, stress pathology, and resilience."
30890859,1.0,Visual Analytics of Genomic and Cancer Data: A Systematic Review,2019 Mar 13;18:1176935119835546.,"Visual analytics and visualisation can leverage the human perceptual system to interpret and uncover hidden patterns in big data. The advent of next-generation sequencing technologies has allowed the rapid production of massive amounts of genomic data and created a corresponding need for new tools and methods for visualising and interpreting these data. Visualising genomic data requires not only simply plotting of data but should also offer a decision or a choice about what the message should be conveyed in the particular plot; which methodologies should be used to represent the results must provide an easy, clear, and accurate way to the clinicians, experts, or researchers to interact with the data. Genomic data visual analytics is rapidly evolving in parallel with advances in high-throughput technologies such as artificial intelligence (AI) and virtual reality (VR). Personalised medicine requires new genomic visualisation tools, which can efficiently extract knowledge from the genomic data and speed up expert decisions about the best treatment of individual patient's needs. However, meaningful visual analytics of such large genomic data remains a serious challenge. This article provides a comprehensive systematic review and discussion on the tools, methods, and trends for visual analytics of cancer-related genomic data. We reviewed methods for genomic data visualisation including traditional approaches such as scatter plots, heatmaps, coordinates, and networks, as well as emerging technologies using AI and VR. We also demonstrate the development of genomic data visualisation tools over time and analyse the evolution of visualising genomic data."
30890362,22.0,ADMET modeling approaches in drug discovery,2019 May;24(5):1157-1165.,"In silico prediction of ADMET is an important component of pharmaceutical R&D. Last year, the FDA approved 59 new molecular entities, with small molecules comprising 64% of the therapies approved in 2018. Estimation of pharmacokinetic properties in the early phases of drug discovery has been central to guiding hit-to-lead and lead-optimization efforts. Given the outstanding complexity of the current R&D model, drug discovery players have intensely pursued molecular modeling strategies to identify patterns in ADMET data and convert them into knowledge. The field has advanced alongside the progress of chemoinformatics, which has evolved from traditional chemometrics to advanced machine learning methods."
30884989,1.0,A new wave of innovation in Semantic web tools for drug discovery,2019 May;14(5):433-444.,"The use of semantic web technologies to aid drug discovery has gained momentum over recent years. Researchers in this domain have realized that semantic web technologies are key to dealing with the high levels of data for drug discovery. These technologies enable us to represent the data in a formal, structured, interoperable and comparable way, and to tease out undiscovered links between drug data (be it identifying new drug-targets or relevant compounds, or links between specific drugs and diseases). Areas covered: This review focuses on explaining how semantic web technologies are being used to aid advances in drug discovery. The main types of semantic web technologies are explained, outlining how they work and how they can be used in the drug discovery process, with a consideration of how the use of these technologies has progressed from their initial usage. Expert opinion: The increased availability of shared semantic resources (tools, data and importantly the communities) have enabled the application of semantic web technologies to facilitate semantic (context dependent) search across multiple data sources, which can be used by machine learning to produce better predictions by exploiting the semantic links in knowledge graphs and linked datasets."
30883745,8.0,Automated sensing of daily activity: A new lens into development,2019 Apr;61(3):444-464.,"Rapidly maturing technologies for sensing and activity recognition can provide unprecedented access to the complex structure daily activity and interaction, promising new insight into the mechanisms by which experience shapes developmental outcomes. Motion data, autonomic activity, and ""snippets"" of audio and video recordings can be conveniently logged by wearable sensors (Lazer et al., 2009). Machine learning algorithms can process these signals into meaningful markers, from child and parent behavior to outcomes such as depression or teenage drinking. Theoretically motivated aspects of daily activity can be combined and synchronized to examine reciprocal effects between children's behaviors and their environments or internal processes. Captured over longitudinal time, such data provide a new opportunity to study the processes by which individual differences emerge and stabilize. This paper introduces the reader to developments in sensing and activity recognition with implications for developmental phenomena across the lifespan, sketching a framework for leveraging mobile sensors for transactional analyses that bridge micro- and longitudinal- timescales of development. It finishes by detailing resources and best practices to facilitate the next generation of developmentalists to contribute to this emerging area."
30881899,3.0,Personalising medicine in inflammatory bowel disease-current and future perspectives,2019 Jan;8(1):56-69.,"Up to 25% of inflammatory bowel disease (IBD) presents during childhood, often with severe and extensive disease, leading to significant morbidity including delayed growth and nutritional impairment. The classical approach to management has centred on differentiation into Crohn's disease (CD) or ulcerative colitis (UC), with subsequent treatment based on symptoms, results and complications. However, IBD is a heterogeneous condition with substantial variation in phenotype, disease course and outcome, so whilst effective treatment exists one size does not fit all. The ability to predict disease course at diagnosis, alongside tailoring medications based on response gives the potential for a more 'personalised approach'. The move to a pre-emptive strategy to prevent IBD-related complications, whilst simultaneously minimising side effects and long-term toxicity from therapy, particularly in those with relatively indolent disease, has the potential to revolutionise care. In very early-onset IBD, personalised approaches to diagnosis and management have become the standard of treatment enabling clinicians to significantly alter the outcomes of the few children with monogenic disease. However, the promise of discoveries in genomics, microbiome and transcriptomics in paediatric IBD has not yet translated to clinical application for the vast majority of patients. Despite this, the opportunity presents itself to apply data gathered at diagnosis and follow-up to predict which patients are likely to progress to complicated disease, which will respond well and which will require additional therapy. Using complex mathematics and innovative, cutting-edge machine learning (ML) techniques gives the potential to use this data to develop personalised clinical care algorithms to treat patients more effectively, reduce toxicity and improve outcome. In this review, we will consider current management of paediatric IBD, discuss how precision medicine is making inroads into clinical practice already, examine the contemporary studies applying data to stratify patients and explore how future management may be revolutionised by personalisation with clinical, genomic and other multi-omic data."
30881539,4.0,Machine Learning in Relation to Emergency Medicine Clinical and Operational Scenarios: An Overview,2019 Mar;20(2):219-227.,"Health informatics is a vital technology that holds great promise in the healthcare setting. We describe two prominent health informatics tools relevant to emergency care, as well as the historical background and the current state of informatics. We also identify recent research findings and practice changes. The recent advances in machine learning and natural language processing (NLP) are a prominent development in health informatics overall and relevant in emergency medicine (EM). A basic comprehension of machine-learning algorithms is the key to understand the recent usage of artificial intelligence in healthcare. We are using NLP more in clinical use for documentation. NLP has started to be used in research to identify clinically important diseases and conditions. Health informatics has the potential to benefit both healthcare providers and patients. We cover two powerful tools from health informatics for EM clinicians and researchers by describing the previous successes and challenges and conclude with their implications to emergency care."
30878282,3.0,Medical ethics considerations on artificial intelligence,2019 Jun;64:277-282.,"Artificial intelligence (AI) is currently one of the mostly controversial matters of the world. This article discusses AI in terms of the medical ethics issues involved, both existing and potential. Once artificial intelligence is fully developed within electronic systems, it will afford many useful applications in many sectors ranging from banking, agriculture, medical procedures to military operations, especially by decreasing the involvement of humans in critically dangerous activities. Robots as well as computers themselves are embodiments of values inasmuch as they entail actions and choices, but their practical applications are modelled or programmed by the engineers building the systems. AI will need algorithmic procedures to ensure safety in the implementation of such systems. The AI algorithms written could naturally contain errors that may result in unforeseen consequences and unfair outcomes along economic and racial class lines. It is crucial that measures be taken to monitor technological developments ensuring preventative and precautionary safeguards are in place to safeguard the rights of those involved against direct or indirect coercion. While it is the responsibility of AI researchers to ensure that the future impact is more positive than negative, ethicists and philosophers need to be deeply involved in the development of such technologies from the beginning."
