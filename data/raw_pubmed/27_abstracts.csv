pmid,citations,title,date,text
28319238,1.0,Behavioral and neural constraints on hierarchical representations,2017 Mar 1;17(3):13.,"Central to behavior and cognition is the way that sensory stimuli are represented in neural systems. The distributions over such stimuli enjoy rich structure; however, how the brain captures and exploits these regularities is unclear. Here, we consider different sources of perhaps the most prevalent form of structure, namely hierarchies, in one of its most prevalent cases, namely the representation of images. We review experimental approaches across a range of subfields, spanning inference, memory recall, and visual adaptation, to investigate how these constrain hierarchical representations. We also discuss progress in building hierarchical models of the representation of images-this has the potential to clarify how the structure of the world is reflected in biological systems. We suggest there is a need for a closer embedding of recent advances in machine learning and computer vision into the design and interpretation of experiments, notably by utilizing the understanding of the structure of natural scenes and through the creation of hierarchically structured synthetic stimuli."
28315224,1.0,An Overview of Bioinformatics Tools and Resources in Allergy,2017;1592:223-245.,"The rapidly increasing number of characterized allergens has created huge demands for advanced information storage, retrieval, and analysis. Bioinformatics and machine learning approaches provide useful tools for the study of allergens and epitopes prediction, which greatly complement traditional laboratory techniques. The specific applications mainly include identification of B- and T-cell epitopes, and assessment of allergenicity and cross-reactivity. In order to facilitate the work of clinical and basic researchers who are not familiar with bioinformatics, we review in this chapter the most important databases, bioinformatic tools, and methods with relevance to the study of allergens."
28315069,12.0,Toolkits and Libraries for Deep Learning,2017 Aug;30(4):400-405.,"Deep learning is an important new area of machine learning which encompasses a wide range of neural network architectures designed to complete various tasks. In the medical imaging domain, example tasks include organ segmentation, lesion detection, and tumor classification. The most popular network architecture for deep learning for images is the convolutional neural network (CNN). Whereas traditional machine learning requires determination and calculation of features from which the algorithm learns, deep learning approaches learn the important features as well as the proper weighting of those features to make predictions for new data. In this paper, we will describe some of the libraries and tools that are available to aid in the construction and efficient execution of deep learning as applied to medical images."
28302041,,Promises of Machine Learning Approaches in Prediction of Absorption of Compounds,2018;18(3):196-207.,"The Machine Learning (ML) is one of the fastest developing techniques in the prediction and evaluation of important pharmacokinetic properties such as absorption, distribution, metabolism and excretion. The availability of a large number of robust validation techniques for prediction models devoted to pharmacokinetics has significantly enhanced the trust and authenticity in ML approaches. There is a series of prediction models generated and used for rapid screening of compounds on the basis of absorption in last one decade. Prediction of absorption of compounds using ML models has great potential across the pharmaceutical industry as a non-animal alternative to predict absorption. However, these prediction models still have to go far ahead to develop the confidence similar to conventional experimental methods for estimation of drug absorption. Some of the general concerns are selection of appropriate ML methods and validation techniques in addition to selecting relevant descriptors and authentic data sets for the generation of prediction models. The current review explores published models of ML for the prediction of absorption using physicochemical properties as descriptors and their important conclusions. In addition, some critical challenges in acceptance of ML models for absorption are also discussed."
28301734,358.0,Deep Learning in Medical Image Analysis,2017 Jun 21;19:221-248.,"This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement."
28301733,3.0,Big Data Analytics in Chemical Engineering,2017 Jun 7;8:63-85.,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation."
28294138,26.0,Machine learning applications in cell image analysis,2017 Jul;95(6):525-530.,"Machine learning (ML) refers to a set of automatic pattern recognition methods that have been successfully applied across various problem domains, including biomedical image analysis. This review focuses on ML applications for image analysis in light microscopy experiments with typical tasks of segmenting and tracking individual cells, and modelling of reconstructed lineage trees. After describing a typical image analysis pipeline and highlighting challenges of automatic analysis (for example, variability in cell morphology, tracking in presence of clutters) this review gives a brief historical outlook of ML, followed by basic concepts and definitions required for understanding examples. This article then presents several example applications at various image processing stages, including the use of supervised learning methods for improving cell segmentation, and the application of active learning for tracking. The review concludes with remarks on parameter setting and future directions."
28285459,19.0,A Critical Review for Developing Accurate and Dynamic Predictive Models Using Machine Learning Methods in Medicine and Health Care,2017 Apr;41(4):69.,"Recently, Artificial Intelligence (AI) has been used widely in medicine and health care sector. In machine learning, the classification or prediction is a major field of AI. Today, the study of existing predictive models based on machine learning methods is extremely active. Doctors need accurate predictions for the outcomes of their patients' diseases. In addition, for accurate predictions, timing is another significant factor that influences treatment decisions. In this paper, existing predictive models in medicine and health care have critically reviewed. Furthermore, the most famous machine learning methods have explained, and the confusion between a statistical approach and machine learning has clarified. A review of related literature reveals that the predictions of existing predictive models differ even when the same dataset is used. Therefore, existing predictive models are essential, and current methods must be improved."
28283186,42.0,Imaging Genetics and Genomics in Psychiatry: A Critical Review of Progress and Potential,2017 Aug 1;82(3):165-175.,"Imaging genetics and genomics research has begun to provide insight into the molecular and genetic architecture of neural phenotypes and the neural mechanisms through which genetic risk for psychopathology may emerge. As it approaches its third decade, imaging genetics is confronted by many challenges, including the proliferation of studies using small sample sizes and diverse designs, limited replication, problems with harmonization of neural phenotypes for meta-analysis, unclear mechanisms, and evidence that effect sizes may be more modest than originally posited, with increasing evidence of polygenicity. These concerns have encouraged the field to grow in many new directions, including the development of consortia and large-scale data collection projects and the use of novel methods (e.g., polygenic approaches, machine learning) that enhance the quality of imaging genetic studies but also introduce new challenges. We critically review progress in imaging genetics and offer suggestions and highlight potential pitfalls of novel approaches. Ultimately, the strength of imaging genetics and genomics lies in their translational and integrative potential with other research approaches (e.g., nonhuman animal models, psychiatric genetics, pharmacologic challenge) to elucidate brain-based pathways that give rise to the vast individual differences in behavior as well as risk for psychopathology."
28277804,23.0,Cardiac imaging: working towards fully-automated machine analysis & interpretation,2017 Mar;14(3):197-212.,"Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions. Areas covered: This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice. Expert commentary: Fully-automated processing and high-level computer interpretation of cardiac imaging are becoming a reality. Application of machine learning to the vast amounts of quantitative data generated per scan and integration with clinical data also facilitates a move to more patient-specific interpretation. These developments are unlikely to replace interpreting physicians but will provide them with highly accurate tools to detect disease, risk-stratify, and optimize patient-specific treatment. However, with each technological advance, we move further from human dependence and closer to fully-automated machine interpretation."
28272810,57.0,Deep learning for computational chemistry,2017 Jun 15;38(16):1291-1307.,"The rise and fall of artificial neural networks is well documented in the scientific literature of both computer science and computational chemistry. Yet almost two decades later, we are now seeing a resurgence of interest in deep learning, a machine learning algorithm based on multilayer neural networks. Within the last few years, we have seen the transformative impact of deep learning in many domains, particularly in speech recognition and computer vision, to the extent that the majority of expert practitioners in those field are now regularly eschewing prior established models in favor of deep learning models. In this review, we provide an introductory overview into the theory of deep neural networks and their unique properties that distinguish them from traditional machine learning algorithms used in cheminformatics. By providing an overview of the variety of emerging applications of deep neural networks, we highlight its ubiquity and broad applicability to a wide range of challenges in the field, including quantitative structure activity relationship, virtual screening, protein structure prediction, quantum chemistry, materials design, and property prediction. In reviewing the performance of deep neural networks, we observed a consistent outperformance against non-neural networks state-of-the-art models across disparate research topics, and deep neural network-based models often exceeded the ""glass ceiling"" expectations of their respective tasks. Coupled with the maturity of GPU-accelerated computing for training deep neural networks and the exponential growth of chemical data on which to train these networks on, we anticipate that deep learning algorithms will be a valuable tool for computational chemistry. © 2017 Wiley Periodicals, Inc."
28272367,,How Open Data Shapes In Silico Transporter Modeling,2017 Mar 7;22(3):422.,"Chemical compound bioactivity and related data are nowadays easily available from open data sources and the open medicinal chemistry literature for many transmembrane proteins. Computational ligand-based modeling of transporters has therefore experienced a shift from local (quantitative) models to more global, qualitative, predictive models. As the size and heterogeneity of the data set rises, careful data curation becomes even more important. This includes, for example, not only a tailored cutoff setting for the generation of binary classes, but also the proper assessment of the applicability domain. Powerful machine learning algorithms (such as multi-label classification) now allow the simultaneous prediction of multiple related targets. However, the more complex, the less interpretable these models will get. We emphasize that transmembrane transporters are very peculiar, some of which act as off-targets rather than as real drug targets. Thus, careful selection of the right modeling technique is important, as well as cautious interpretation of results. We hope that, as more and more data will become available, we will be able to ameliorate and specify our models, coming closer towards function elucidation and the development of safer medicine."
28268288,1.0,Difficulty understanding speech in noise by the hearing impaired: underlying causes and technological solutions,2016 Aug;2016:89-92.,"A primary complaint of hearing-impaired individuals involves poor speech understanding when background noise is present. Hearing aids and cochlear implants often allow good speech understanding in quiet backgrounds. But hearing-impaired individuals are highly noise intolerant, and existing devices are not very effective at combating background noise. As a result, speech understanding in noise is often quite poor. In accord with the significance of the problem, considerable effort has been expended toward understanding and remedying this issue. Fortunately, our understanding of the underlying issues is reasonably good. In sharp contrast, effective solutions have remained elusive. One solution that seems promising involves a single-microphone machine-learning algorithm to extract speech from background noise. Data from our group indicate that the algorithm is capable of producing vast increases in speech understanding by hearing-impaired individuals. This paper will first provide an overview of the speech-in-noise problem and outline why hearing-impaired individuals are so noise intolerant. An overview of our approach to solving this problem will follow."
28268284,1.0,Intelligent hearing aids: the next revolution,2016 Aug;2016:72-76.,"The first revolution in hearing aids came from nonlinear amplification, which allows better compensation for both soft and loud sounds. The second revolution stemmed from the introduction of digital signal processing, which allows better programmability and more sophisticated algorithms. The third revolution in hearing aids is wireless, which allows seamless connectivity between a pair of hearing aids and with more and more external devices. Each revolution has fundamentally transformed hearing aids and pushed the entire industry forward significantly. Machine learning has received significant attention in recent years and has been applied in many other industries, e.g., robotics, speech recognition, genetics, and crowdsourcing. We argue that the next revolution in hearing aids is machine intelligence. In fact, this revolution is already quietly happening. We will review the development in at least three major areas: applications of machine learning in speech enhancement; applications of machine learning in individualization and customization of signal processing algorithms; applications of machine learning in improving the efficiency and effectiveness of clinical tests. With the advent of the internet of things, the above developments will accelerate. This revolution will bring patient satisfactions to a new level that has never been seen before."
28265788,22.0,A review of supervised machine learning applied to ageing research,2017 Apr;18(2):171-188.,"Broadly speaking, supervised machine learning is the computational task of learning correlations between variables in annotated data (the training set), and using this information to create a predictive model capable of inferring annotations for new data, whose annotations are not known. Ageing is a complex process that affects nearly all animal species. This process can be studied at several levels of abstraction, in different organisms and with different objectives in mind. Not surprisingly, the diversity of the supervised machine learning algorithms applied to answer biological questions reflects the complexities of the underlying ageing processes being studied. Many works using supervised machine learning to study the ageing process have been recently published, so it is timely to review these works, to discuss their main findings and weaknesses. In summary, the main findings of the reviewed papers are: the link between specific types of DNA repair and ageing; ageing-related proteins tend to be highly connected and seem to play a central role in molecular pathways; ageing/longevity is linked with autophagy and apoptosis, nutrient receptor genes, and copper and iron ion transport. Additionally, several biomarkers of ageing were found by machine learning. Despite some interesting machine learning results, we also identified a weakness of current works on this topic: only one of the reviewed papers has corroborated the computational results of machine learning algorithms through wet-lab experiments. In conclusion, supervised machine learning has contributed to advance our knowledge and has provided novel insights on ageing, yet future work should have a greater emphasis in validating the predictions."
28263938,56.0,Analysis of Machine Learning Techniques for Heart Failure Readmissions,2016 Nov;9(6):629-640.,"Background:                    The current ability to predict readmissions in patients with heart failure is modest at best. It is unclear whether machine learning techniques that address higher dimensional, nonlinear relationships among variables would enhance prediction. We sought to compare the effectiveness of several machine learning algorithms for predicting readmissions.              Methods and results:                    Using data from the Telemonitoring to Improve Heart Failure Outcomes trial, we compared the effectiveness of random forests, boosting, random forests combined hierarchically with support vector machines or logistic regression (LR), and Poisson regression against traditional LR to predict 30- and 180-day all-cause readmissions and readmissions because of heart failure. We randomly selected 50% of patients for a derivation set, and a validation set comprised the remaining patients, validated using 100 bootstrapped iterations. We compared C statistics for discrimination and distributions of observed outcomes in risk deciles for predictive range. In 30-day all-cause readmission prediction, the best performing machine learning model, random forests, provided a 17.8% improvement over LR (mean C statistics, 0.628 and 0.533, respectively). For readmissions because of heart failure, boosting improved the C statistic by 24.9% over LR (mean C statistic 0.678 and 0.543, respectively). For 30-day all-cause readmission, the observed readmission rates in the lowest and highest deciles of predicted risk with random forests (7.8% and 26.2%, respectively) showed a much wider separation than LR (14.2% and 16.4%, respectively).              Conclusions:                    Machine learning methods improved the prediction of readmission after hospitalization for heart failure compared with LR and provided the greatest predictive range in observed readmission rates."
28254083,4.0,Classification techniques on computerized systems to predict and/or to detect Apnea: A systematic review,2017 Mar;140:265-274.,"Background and objective:                    Sleep apnea syndrome (SAS), which can significantly decrease the quality of life is associated with a major risk factor of health implications such as increased cardiovascular disease, sudden death, depression, irritability, hypertension, and learning difficulties. Thus, it is relevant and timely to present a systematic review describing significant applications in the framework of computational intelligence-based SAS, including its performance, beneficial and challenging effects, and modeling for the decision-making on multiple scenarios.              Methods:                    This study aims to systematically review the literature on systems for the detection and/or prediction of apnea events using a classification model.              Results:                    Forty-five included studies revealed a combination of classification techniques for the diagnosis of apnea, such as threshold-based (14.75%) and machine learning (ML) models (85.25%). In addition, the ML models, were clustered in a mind map, include neural networks (44.26%), regression (4.91%), instance-based (11.47%), Bayesian algorithms (1.63%), reinforcement learning (4.91%), dimensionality reduction (8.19%), ensemble learning (6.55%), and decision trees (3.27%).              Conclusions:                    A classification model should provide an auto-adaptive and no external-human action dependency. In addition, the accuracy of the classification models is related with the effective features selection. New high-quality studies based on randomized controlled trials and validation of models using a large and multiple sample of data are recommended."
28251040,14.0,Analytic Complexity and Challenges in Identifying Mixtures of Exposures Associated with Phenotypes in the Exposome Era,2017;4(1):22-30.,"Purpose of review:                    Mixtures, or combinations and interactions between multiple environmental exposures, are hypothesized to be causally linked with disease and health-related phenotypes. Established and emerging molecular measurement technologies to assay the exposome, the comprehensive battery of exposures encountered from birth to death, promise a new way of identifying mixtures in disease in the epidemiological setting. In this opinion, we describe the analytic complexity and challenges in identifying mixtures associated with phenotype and disease.              Recent findings:                    Existing and emerging machine-learning methods and data analytic approaches (e.g., ""environment-wide association studies"" [EWASs]), as well as large cohorts may enhance possibilities to identify mixtures of correlated exposures associated with phenotypes; however, the analytic complexity of identifying mixtures is immense.              Summary:                    If the exposome concept is realized, new analytical methods and large sample sizes will be required to ascertain how mixtures are associated with disease. The author recommends documenting prevalent correlated exposures and replicated main effects prior to identifying mixtures."
28242295,17.0,"Analysis of live cell images: Methods, tools and opportunities",2017 Feb 15;115:65-79.,"Advances in optical microscopy, biosensors and cell culturing technologies have transformed live cell imaging. Thanks to these advances live cell imaging plays an increasingly important role in basic biology research as well as at all stages of drug development. Image analysis methods are needed to extract quantitative information from these vast and complex data sets. The aim of this review is to provide an overview of available image analysis methods for live cell imaging, in particular required preprocessing image segmentation, cell tracking and data visualisation methods. The potential opportunities recent advances in machine learning, especially deep learning, and computer vision provide are being discussed. This review includes overview of the different available software packages and toolkits."
28241858,20.0,Machine learning identifies a compact gene set for monitoring the circadian clock in human blood,2017 Feb 28;9(1):19.,"Background:                    The circadian clock and the daily rhythms it produces are crucial for human health, but are often disrupted by the modern environment. At the same time, circadian rhythms may influence the efficacy and toxicity of therapeutics and the metabolic response to food intake. Developing treatments for circadian dysfunction, as well as optimizing the daily timing of treatments for other health conditions, will require a simple and accurate method to monitor the molecular state of the circadian clock.              Methods:                    Here we used a recently developed method called ZeitZeiger to predict circadian time (CT, time of day according to the circadian clock) from genome-wide gene expression in human blood.              Results:                    In cross-validation on 498 samples from 60 individuals across three publicly available datasets, ZeitZeiger predicted CT in single samples with a median absolute error of 2.1 h. The predictor trained on all 498 samples used 15 genes, only two of which are part of the core circadian clock. By then applying ZeitZeiger to 475 additional samples from the same three datasets, we quantified how the circadian clock in the blood was affected by various perturbations to the sleep-wake and light-dark cycles. Finally, we extended ZeitZeiger (1) to handle intra-individual variation by making predictions based on multiple samples taken a known time apart, and (2) to handle inter-individual variation by personalizing predictions based on samples from the respective individual. Each of these strategies improved prediction of CT by ~20%.              Conclusions:                    Our results are an important step towards precision circadian medicine. In addition, our generalizable extensions to ZeitZeiger may be applicable to the growing number of biological datasets that contain multiple observations per individual."
28236531,9.0,Neuroadaptive Bayesian Optimization and Hypothesis Testing,2017 Mar;21(3):155-167.,"Cognitive neuroscientists are often interested in broad research questions, yet use overly narrow experimental designs by considering only a small subset of possible experimental conditions. This limits the generalizability and reproducibility of many research findings. Here, we propose an alternative approach that resolves these problems by taking advantage of recent developments in real-time data analysis and machine learning. Neuroadaptive Bayesian optimization is a powerful strategy to efficiently explore more experimental conditions than is currently possible with standard methodology. We argue that such an approach could broaden the hypotheses considered in cognitive science, improving the generalizability of findings. In addition, Bayesian optimization can be combined with preregistration to cover exploration, mitigating researcher bias more broadly and improving reproducibility."
28232122,23.0,"Functional connectomics from a ""big data"" perspective",2017 Oct 15;160:152-167.,"In the last decade, explosive growth regarding functional connectome studies has been observed. Accumulating knowledge has significantly contributed to our understanding of the brain's functional network architectures in health and disease. With the development of innovative neuroimaging techniques, the establishment of large brain datasets and the increasing accumulation of published findings, functional connectomic research has begun to move into the era of ""big data"", which generates unprecedented opportunities for discovery in brain science and simultaneously encounters various challenging issues, such as data acquisition, management and analyses. Big data on the functional connectome exhibits several critical features: high spatial and/or temporal precision, large sample sizes, long-term recording of brain activity, multidimensional biological variables (e.g., imaging, genetic, demographic, cognitive and clinic) and/or vast quantities of existing findings. We review studies regarding functional connectomics from a big data perspective, with a focus on recent methodological advances in state-of-the-art image acquisition (e.g., multiband imaging), analysis approaches and statistical strategies (e.g., graph theoretical analysis, dynamic network analysis, independent component analysis, multivariate pattern analysis and machine learning), as well as reliability and reproducibility validations. We highlight the novel findings in the application of functional connectomic big data to the exploration of the biological mechanisms of cognitive functions, normal development and aging and of neurological and psychiatric disorders. We advocate the urgent need to expand efforts directed at the methodological challenges and discuss the direction of applications in this field."
28230848,46.0,Computational approaches to fMRI analysis,2017 Feb 23;20(3):304-313.,"Analysis methods in cognitive neuroscience have not always matched the richness of fMRI data. Early methods focused on estimating neural activity within individual voxels or regions, averaged over trials or blocks and modeled separately in each participant. This approach mostly neglected the distributed nature of neural representations over voxels, the continuous dynamics of neural activity during tasks, the statistical benefits of performing joint inference over multiple participants and the value of using predictive models to constrain analysis. Several recent exploratory and theory-driven methods have begun to pursue these opportunities. These methods highlight the importance of computational techniques in fMRI analysis, especially machine learning, algorithmic optimization and parallel computing. Adoption of these techniques is enabling a new generation of experiments and analyses that could transform our understanding of some of the most complex-and distinctly human-signals in the brain: acts of cognition such as thoughts, intentions and memories."
28222333,15.0,Immunoprofiling as a predictor of patient's response to cancer therapy-promises and challenges,2017 Apr;45:60-72.,"Immune cell infiltration is common to many tumors and has been recognized by pathologists for more than 100 years. The application of digital imaging and objective assessment software allowed a concise determination of the type and quantity of immune cells and their location relative to the tumor and, in the case of colon cancer, characterized overall survival better than AJCC TNM staging. Subsequently, expression of PD-L1, by 50% or more tumor cells, identified NSCLC patients with double the response rate to anti-PD-1. Soon, automated staining methods will improve reproducibility of multiplex staining and allow for CLIA standards so that multiplex staining can be used to make clinical decisions. Ultimately, machine-learning algorithms will help interpret data from tissue images and lead to improved delivery of precision medicine."
28212054,136.0,Machine Learning for Medical Imaging,Mar-Apr 2017;37(2):505-515.,"Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works. ©RSNA, 2017."
28211015,31.0,"Fifty years of computer analysis in chest imaging: rule-based, machine learning, deep learning",2017 Mar;10(1):23-32.,"Half a century ago, the term ""computer-aided diagnosis"" (CAD) was introduced in the scientific literature. Pulmonary imaging, with chest radiography and computed tomography, has always been one of the focus areas in this field. In this study, I describe how machine learning became the dominant technology for tackling CAD in the lungs, generally producing better results than do classical rule-based approaches, and how the field is now rapidly changing: in the last few years, we have seen how even better results can be obtained with deep learning. The key differences among rule-based processing, machine learning, and deep learning are summarized and illustrated for various applications of CAD in the chest."
28190189,1.0,Towards Precision in HF Pharmacotherapy,2017 Feb;14(1):1-6.,"Purpose of review:                    Heart failure (HF) is a disease state with great heterogeneity, which complicates the therapeutic process. Identifying more precise HF phenotypes will allow for the development of more targeted therapies and improvement in patient outcomes. This review explores the future for precision medicine in HF treatment.              Recent findings:                    Rather than a continuous disease spectrum with a uniform pathogenesis, HF has phenotypes with different underlying pathophysiologic features. The challenge is to establish clinical phenotypic characterizations to direct therapy. Phenomapping, a process of using machine learning algorithms applied to clinical data sets, has been used to identify phenotypically distinct and clinically meaningful HF groups. As powerful technologies extend our knowledge, future analyses may be able to compile more comprehensive phenotypic profiles using genetic, epigenetic, proteomic, and metabolomic measurements. Identifying clinical characterizations of particular HF patients that would be uniquely or disproportionately responsive to a specific treatment would allow for more direct selection of optimal therapy, reduce trial-and-error prescribing, and help avoid adverse drug reactions."
28183433,41.0,Mechanisms of the Development of Allergy (MeDALL): Introducing novel concepts in allergy phenotypes,2017 Feb;139(2):388-399.,"Asthma, rhinitis, and eczema are complex diseases with multiple genetic and environmental factors interlinked through IgE-associated and non-IgE-associated mechanisms. Mechanisms of the Development of ALLergy (MeDALL; EU FP7-CP-IP; project no: 261357; 2010-2015) studied the complex links of allergic diseases at the clinical and mechanistic levels by linking epidemiologic, clinical, and mechanistic research, including in vivo and in vitro models. MeDALL integrated 14 European birth cohorts, including 44,010 participants and 160 cohort follow-ups between pregnancy and age 20 years. Thirteen thousand children were prospectively followed after puberty by using a newly standardized MeDALL Core Questionnaire. A microarray developed for allergen molecules with increased IgE sensitivity was obtained for 3,292 children. Estimates of air pollution exposure from previous studies were available for 10,000 children. Omics data included those from historical genome-wide association studies (23,000 children) and DNA methylation (2,173), targeted multiplex biomarker (1,427), and transcriptomic (723) studies. Using classical epidemiology and machine-learning methods in 16,147 children aged 4 years and 11,080 children aged 8 years, MeDALL showed the multimorbidity of eczema, rhinitis, and asthma and estimated that only 38% of multimorbidity was attributable to IgE sensitization. MeDALL has proposed a new vision of multimorbidity independent of IgE sensitization, and has shown that monosensitization and polysensitization represent 2 distinct phenotypes. The translational component of MeDALL is shown by the identification of a novel allergic phenotype characterized by polysensitization and multimorbidity, which is associated with the frequency, persistence, and severity of allergic symptoms. The results of MeDALL will help integrate personalized, predictive, preventative, and participatory approaches in allergic diseases."
28154052,4.0,Predicting human behavior: The next frontiers,2017 Feb 3;355(6324):489.,"Machine learning has provided researchers with new tools for understanding human behavior. In this article, we briefly describe some successes in predicting behaviors and describe the challenges over the next few years."
28154050,15.0,Beyond prediction: Using big data for policy problems,2017 Feb 3;355(6324):483-485.,"Machine-learning prediction methods have been extremely productive in applications ranging from medicine to allocating fire and health inspectors in cities. However, there are a number of gaps between making a prediction and making a decision, and underlying assumptions need to be understood in order to optimize data-driven decision-making."
28144827,9.0,Biomarkers for Musculoskeletal Pain Conditions: Use of Brain Imaging and Machine Learning,2017 Jan;19(1):5.,"Chronic musculoskeletal pain condition often shows poor correlations between tissue abnormalities and clinical pain. Therefore, classification of pain conditions like chronic low back pain, osteoarthritis, and fibromyalgia depends mostly on self report and less on objective findings like X-ray or magnetic resonance imaging (MRI) changes. However, recent advances in structural and functional brain imaging have identified brain abnormalities in chronic pain conditions that can be used for illness classification. Because the analysis of complex and multivariate brain imaging data is challenging, machine learning techniques have been increasingly utilized for this purpose. The goal of machine learning is to train specific classifiers to best identify variables of interest on brain MRIs (i.e., biomarkers). This report describes classification techniques capable of separating MRI-based brain biomarkers of chronic pain patients from healthy controls with high accuracy (70-92%) using machine learning, as well as critical scientific, practical, and ethical considerations related to their potential clinical application. Although self-report remains the gold standard for pain assessment, machine learning may aid in the classification of chronic pain disorders like chronic back pain and fibromyalgia as well as provide mechanistic information regarding their neural correlates."
28144341,72.0,Computational methods in drug discovery,2016 Dec 12;12:2694-2718.,"The process for drug discovery and development is challenging, time consuming and expensive. Computer-aided drug discovery (CADD) tools can act as a virtual shortcut, assisting in the expedition of this long process and potentially reducing the cost of research and development. Today CADD has become an effective and indispensable tool in therapeutic development. The human genome project has made available a substantial amount of sequence data that can be used in various drug discovery projects. Additionally, increasing knowledge of biological structures, as well as increasing computer power have made it possible to use computational methods effectively in various phases of the drug discovery and development pipeline. The importance of in silico tools is greater than ever before and has advanced pharmaceutical research. Here we present an overview of computational methods used in different facets of drug discovery and highlight some of the recent successes. In this review, both structure-based and ligand-based drug discovery methods are discussed. Advances in virtual high-throughput screening, protein structure prediction methods, protein-ligand docking, pharmacophore modeling and QSAR techniques are reviewed."
28141576,1.0,Mathematical modelling of the electric sense of fish: the role of multi-frequency measurements and movement,2017 Jan 31;12(2):025002.,"Understanding active electrolocation in weakly electric fish remains a challenging issue. In this article we propose a mathematical formulation of this problem, in terms of partial differential equations. This allows us to detail two algorithms: one for localizing a target using the multi-frequency aspect of the signal, and another one for identifying the shape of this target. Shape recognition is designed in a machine learning point of view, and takes advantage of both the multi-frequency setup and the movement of the fish around its prey. Numerical simulations are shown for the computation of the electric field emitted and sensed by the fish; they are then used as an input for the two algorithms."
28138367,78.0,Machine Learning and Data Mining Methods in Diabetes Research,2017 Jan 8;15:104-116.,"The remarkable advances in biotechnology and health sciences have led to a significant production of data, such as high throughput genetic data and clinical information, generated from large Electronic Health Records (EHRs). To this end, application of machine learning and data mining methods in biosciences is presently, more than ever before, vital and indispensable in efforts to transform intelligently all available information into valuable knowledge. Diabetes mellitus (DM) is defined as a group of metabolic disorders exerting significant pressure on human health worldwide. Extensive research in all aspects of diabetes (diagnosis, etiopathophysiology, therapy, etc.) has led to the generation of huge amounts of data. The aim of the present study is to conduct a systematic review of the applications of machine learning, data mining techniques and tools in the field of diabetes research with respect to a) Prediction and Diagnosis, b) Diabetic Complications, c) Genetic Background and Environment, and e) Health Care and Management with the first category appearing to be the most popular. A wide range of machine learning algorithms were employed. In general, 85% of those used were characterized by supervised learning approaches and 15% by unsupervised ones, and more specifically, association rules. Support vector machines (SVM) arise as the most successful and widely used algorithm. Concerning the type of data, clinical datasets were mainly used. The title applications in the selected articles project the usefulness of extracting valuable knowledge leading to new hypotheses targeting deeper understanding and further investigation in DM."
28133810,34.0,Systems serology for evaluation of HIV vaccine trials,2017 Jan;275(1):262-270.,"The scale and scope of the global epidemic, coupled to challenges with traditional vaccine development approaches, point toward a need for novel methodologies for HIV vaccine research. While the development of vaccines able to induce broadly neutralizing antibodies remains the ultimate goal, to date, vaccines continue to fail to induce these rare humoral immune responses. Conversely, growing evidence across vaccine platforms in both non-human primates and humans points to a role for polyclonal vaccine-induced antibody responses in protection from infection. These candidate vaccines, despite employing disparate viral vectors and immunization strategies, consistently identify a role for functional or non-traditional antibody activities as correlates of immunity. However, the precise mechanism(s) of action of these ""binding"" antibodies, their specific characteristics, and their ability to be selectively induced and/or potentiated to result in complete protection merits parallel investigation to neutralizing antibody-based vaccine design approaches. Ultimately, while neutralizing and functional antibody-based vaccine strategies need not be mutually exclusive, defining the specific characteristics of ""protective"" functional antibodies may provide a target immune profile to potentially induce more robust immunity against HIV. Specifically, one approach to guide the development of functional antibody-based vaccine strategies, termed ""systems serology"", offers an unbiased and comprehensive approach to systematically survey humoral immune responses, capturing the array of functions and humoral response characteristics that may be induced following vaccination with high resolution. Coupled to machine learning tools, large datasets that explore the ""antibody-ome"" offer a means to step back from anticipated correlates and mechanisms of protection and toward a more fundamental understanding of coordinated aspects of humoral immune responses, to more globally differentiate among vaccine candidates, and most critically, to identify the features of humoral immunity that distinguish protective from non-protective responses. Overall, the systematic serological approach described here aimed at broadly capturing the enormous biodiversity in antibody profiles that may emerge following vaccination, complements the existing cutting edge tools in the cellular immunology space that survey vaccine-induced polyfunctional cellular activity by flow cytometry, transcriptional profiling, epigenetic, and metabolomic analysis to offer a means to develop both a more nuanced and a more complete understanding of correlates of protection to support the design of functional vaccine strategies."
28132481,3.0,Tackling the problem of HIV drug resistance,2016;62(3):273-279.,"The virally-encoded HIV-1 protease is an effective target for antiviral drugs, however, treatment for HIV infections is limited by the prevalence of drug resistant viral mutants. In this review, we describe our three-pronged approach to analyze and combat drug resistance. Understanding the molecular basis for resistance due to protease inhibitors is a key initial step in this approach. This knowledge is being employed for the design of new, improved inhibitors with high affinity for resistant mutants as well as wild type enzyme. In parallel with experimental studies of diverse mutants and inhibitory compounds, we are developing efficient algorithms to predict drug resistance phenotype from genotype data. This approach has important practical applications in the clinic where genotyping is recommended for individuals with new infections."
28127429,34.0,Machine learning and systems genomics approaches for multi-omics data,2017 Jan 20;5:2.,"In light of recent advances in biomedical computing, big data science, and precision medicine, there is a mammoth demand for establishing algorithms in machine learning and systems genomics (MLSG), together with multi-omics data, to weigh probable phenotype-genotype relationships. Software frameworks in MLSG are extensively employed to analyze hundreds of thousands of multi-omics data by high-throughput technologies. In this study, we reviewed the MLSG software frameworks and future directions with respect to multi-omics data analysis and integration. Our review was targeted at researching recent approaches and technical solutions for the MLSG software frameworks using multi-omics platforms."
28125523,4.0,Machine Learning-Based Classification of 38 Years of Spine-Related Literature Into 100 Research Topics,2017 Jun 1;42(11):863-870.,"Study design:                    Retrospective review.              Objective:                    To identify the top 100 spine research topics.              Summary of background data:                    Recent advances in ""machine learning,"" or computers learning without explicit instructions, have yielded broad technological advances. Topic modeling algorithms can be applied to large volumes of text to discover quantifiable themes and trends.              Methods:                    Abstracts were extracted from the National Library of Medicine PubMed database from five prominent peer-reviewed spine journals (European Spine Journal [ESJ], The Spine Journal [SpineJ], Spine, Journal of Spinal Disorders and Techniques [JSDT], Journal of Neurosurgery: Spine [JNS]). Each abstract was entered into a latent Dirichlet allocation model specified to discover 100 topics, resulting in each abstract being assigned a probability of belonging in a topic. Topics were named using the five most frequently appearing terms within that topic. Significance of increasing (""hot"") or decreasing (""cold"") topic popularity over time was evaluated with simple linear regression.              Results:                    From 1978 to 2015, 25,805 spine-related research articles were extracted and classified into 100 topics. Top two most published topics included ""clinical, surgeons, guidelines, information, care"" (n = 496 articles) and ""pain, back, low, treatment, chronic"" (424). Top two hot trends included ""disc, cervical, replacement, level, arthroplasty"" (+0.05%/yr, P < 0.001), and ""minimally, invasive, approach, technique"" (+0.05%/yr, P < 0.001). By journal, the most published topics were ESJ-""operative, surgery, postoperative, underwent, preoperative""; SpineJ-""clinical, surgeons, guidelines, information, care""; Spine-""pain, back, low, treatment, chronic""; JNS- ""tumor, lesions, rare, present, diagnosis""; JSDT-""cervical, anterior, plate, fusion, ACDF.""              Conclusion:                    Topics discovered through latent Dirichlet allocation modeling represent unbiased meaningful themes relevant to spine care. Topic dynamics can provide historical context and direction for future research for aspiring investigators and trainees interested in spine careers. Please explore https://singdc.shinyapps.io/spinetopics.              Level of evidence:                    N A."
28125274,61.0,Implementing Machine Learning in Radiology Practice and Research,2017 Apr;208(4):754-760.,"Objective:                    The purposes of this article are to describe concepts that radiologists should understand to evaluate machine learning projects, including common algorithms, supervised as opposed to unsupervised techniques, statistical pitfalls, and data considerations for training and evaluation, and to briefly describe ethical dilemmas and legal risk.              Conclusion:                    Machine learning includes a broad class of computer programs that improve with experience. The complexity of creating, training, and monitoring machine learning indicates that the success of the algorithms will require radiologist involvement for years to come, leading to engagement rather than replacement."
28116551,13.0,Tensor Factorization for Precision Medicine in Heart Failure with Preserved Ejection Fraction,2017 Jun;10(3):305-312.,"Heart failure with preserved ejection fraction (HFpEF) is a heterogeneous clinical syndrome that may benefit from improved subtyping in order to better characterize its pathophysiology and to develop novel targeted therapies. The United States Precision Medicine Initiative comes amid the rapid growth in quantity and modality of clinical data for HFpEF patients ranging from deep phenotypic to trans-omic data. Tensor factorization, a form of machine learning, allows for the integration of multiple data modalities to derive clinically relevant HFpEF subtypes that may have significant differences in underlying pathophysiology and differential response to therapies. Tensor factorization also allows for better interpretability by supporting dimensionality reduction and identifying latent groups of data for meaningful summarization of both features and disease outcomes. In this narrative review, we analyze the modest literature on the application of tensor factorization to related biomedical fields including genotyping and phenotyping. Based on the cited work including work of our own, we suggest multiple tensor factorization formulations capable of integrating the deep phenotypic and trans-omic modalities of data for HFpEF, or accounting for interactions between genetic variants at different omic hierarchies. We encourage extensive experimental studies to tackle challenges in applying tensor factorization for precision medicine in HFpEF, including effectively incorporating existing medical knowledge, properly accounting for uncertainty, and efficiently enforcing sparsity for better interpretability."
28092576,6.0,Multiple-Instance Learning for Medical Image and Video Analysis,2017;10:213-234.,"Multiple-instance learning (MIL) is a recent machine-learning paradigm that is particularly well suited to medical image and video analysis (MIVA) tasks. Based solely on class labels assigned globally to images or videos, MIL algorithms learn to detect relevant patterns locally in images or videos. These patterns are then used for classification at a global level. Because supervision relies on global labels, manual segmentations are not needed to train MIL algorithms, unlike traditional single-instance learning (SIL) algorithms. Consequently, these solutions are attracting increasing interest from the MIVA community: since the term was coined by Dietterich et al. in 1997, 73 research papers about MIL have been published in the MIVA literature. This paper reviews the existing strategies for modeling MIVA tasks as MIL problems, recommends general-purpose MIL algorithms for each type of MIVA tasks, and discusses MIVA-specific MIL algorithms. Various experiments performed in medical image and video datasets are compiled in order to back up these discussions. This meta-analysis shows that, besides being more convenient than SIL solutions, MIL algorithms are also more accurate in many cases. In other words, MIL is the ideal solution for many MIVA tasks. Recent trends are discussed, and future directions are proposed for this emerging paradigm."
28092510,7.0,Unobtrusive and Wearable Systems for Automatic Dietary Monitoring,2017 Sep;64(9):2075-2089.,"The threat of obesity, diabetes, anorexia, and bulimia in our society today has motivated extensive research on dietary monitoring. Standard self-report methods such as 24-h recall and food frequency questionnaires are expensive, burdensome, and unreliable to handle the growing health crisis. Long-term activity monitoring in daily living is a promising approach to provide individuals with quantitative feedback that can encourage healthier habits. Although several studies have attempted automating dietary monitoring using wearable, handheld, smart-object, and environmental systems, it remains an open research problem. This paper aims to provide a comprehensive review of wearable and hand-held approaches from 2004 to 2016. Emphasis is placed on sensor types used, signal analysis and machine learning methods, as well as a benchmark of state-of-the art work in this field. Key issues, challenges, and gaps are highlighted to motivate future work toward development of effective, reliable, and robust dietary monitoring systems."
28087243,74.0,Using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: Methods and applications,2017 Mar;74(Pt A):58-75.,"Deep learning (DL) is a family of machine learning methods that has gained considerable attention in the scientific community, breaking benchmark records in areas such as speech and visual recognition. DL differs from conventional machine learning methods by virtue of its ability to learn the optimal representation from the raw data through consecutive nonlinear transformations, achieving increasingly higher levels of abstraction and complexity. Given its ability to detect abstract and complex patterns, DL has been applied in neuroimaging studies of psychiatric and neurological disorders, which are characterised by subtle and diffuse alterations. Here we introduce the underlying concepts of DL and review studies that have used this approach to classify brain-based disorders. The results of these studies indicate that DL could be a powerful tool in the current search for biomarkers of psychiatric and neurologic disease. We conclude our review by discussing the main promises and challenges of using DL to elucidate brain-based disorders, as well as possible directions for future research."
28060710,5.0,A Review on Methods for Detecting SNP Interactions in High-Dimensional Genomic Data,Mar-Apr 2018;15(2):599-612.,"In this era of genome-wide association studies (GWAS), the quest for understanding the genetic architecture of complex diseases is rapidly increasing more than ever before. The development of high throughput genotyping and next generation sequencing technologies enables genetic epidemiological analysis of large scale data. These advances have led to the identification of a number of single nucleotide polymorphisms (SNPs) responsible for disease susceptibility. The interactions between SNPs associated with complex diseases are increasingly being explored in the current literature. These interaction studies are mathematically challenging and computationally complex. These challenges have been addressed by a number of data mining and machine learning approaches. This paper reviews the current methods and the related software packages to detect the SNP interactions that contribute to diseases. The issues that need to be considered when developing these models are addressed in this review. The paper also reviews the achievements in data simulation to evaluate the performance of these models. Further, it discusses the future of SNP interaction analysis."
28057825,29.0,Machine vision methods for analyzing social interactions,2017 Jan 1;220(Pt 1):25-34.,"Recent developments in machine vision methods for automatic, quantitative analysis of social behavior have immensely improved both the scale and level of resolution with which we can dissect interactions between members of the same species. In this paper, we review these methods, with a particular focus on how biologists can apply them to their own work. We discuss several components of machine vision-based analyses: methods to record high-quality video for automated analyses, video-based tracking algorithms for estimating the positions of interacting animals, and machine learning methods for recognizing patterns of interactions. These methods are extremely general in their applicability, and we review a subset of successful applications of them to biological questions in several model systems with very different types of social behaviors."
28057585,7.0,An Overview of data science uses in bioimage informatics,2017 Feb 15;115:110-118.,"This review aims at providing a practical overview of the use of statistical features and associated data science methods in bioimage informatics. To achieve a quantitative link between images and biological concepts, one typically replaces an object coming from an image (a segmented cell or intracellular object, a pattern of expression or localisation, even a whole image) by a vector of numbers. They range from carefully crafted biologically relevant measurements to features learnt through deep neural networks. This replacement allows for the use of practical algorithms for visualisation, comparison and inference, such as the ones from machine learning or multivariate statistics. While originating mainly, for biology, in high content screening, those methods are integral to the use of data science for the quantitative analysis of microscopy images to gain biological insight, and they are sure to gather more interest as the need to make sense of the increasing amount of acquired imaging data grows more pressing."
28055930,121.0,Deep Learning for Health Informatics,2017 Jan;21(1):4-21.,"With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health."
28055887,19.0,EEG-Based Strategies to Detect Motor Imagery for Control and Rehabilitation,2017 Apr;25(4):392-401.,"Advances in brain-computer interface (BCI) technology have facilitated the detection of Motor Imagery (MI) from electroencephalography (EEG). First, we present three strategies of using BCI to detect MI from EEG: operant conditioning that employed a fixed model, machine learning that employed a subject-specific model computed from calibration, and adaptive strategy that continuously compute the subject-specific model. Second, we review prevailing works that employed the operant conditioning and machine learning strategies. Third, we present our past work on six stroke patients who underwent a BCI rehabilitation clinical trial with averaged accuracies of 79.8% during calibration and 69.5% across 18 online feedback sessions. Finally, we perform an offline study in this paper on our work employing the adaptive strategy. The results yielded significant improvements of 12% (p < 0.001) and 9% (p < 0.001) using all the data and using limited preceding data respectively in the feedback accuracies. The results showed an increase in the amount of training data yielded improvements. Nevertheless, results of using limited preceding data showed a larger part of the improvement was due to the adaptive strategy and changing subject-specific models did not deteriorate the accuracies. Hence the adaptive strategy is effective in addressing the non-stationarity between calibration and feedback sessions."
28042575,13.0,Long Noncoding RNA Identification: Comparing Machine Learning Based Tools for Long Noncoding Transcripts Discrimination,2016;2016:8496165.,"Long noncoding RNA (lncRNA) is a kind of noncoding RNA with length more than 200 nucleotides, which aroused interest of people in recent years. Lots of studies have confirmed that human genome contains many thousands of lncRNAs which exert great influence over some critical regulators of cellular process. With the advent of high-throughput sequencing technologies, a great quantity of sequences is waiting for exploitation. Thus, many programs are developed to distinguish differences between coding and long noncoding transcripts. Different programs are generally designed to be utilised under different circumstances and it is sensible and practical to select an appropriate method according to a certain situation. In this review, several popular methods and their advantages, disadvantages, and application scopes are summarised to assist people in employing a suitable method and obtaining a more reliable result."
28035989,15.0,"Neuroblastoma, a Paradigm for Big Data Science in Pediatric Oncology",2016 Dec 27;18(1):37.,"Pediatric cancers rarely exhibit recurrent mutational events when compared to most adult cancers. This poses a challenge in understanding how cancers initiate, progress, and metastasize in early childhood. Also, due to limited detected driver mutations, it is difficult to benchmark key genes for drug development. In this review, we use neuroblastoma, a pediatric solid tumor of neural crest origin, as a paradigm for exploring ""big data"" applications in pediatric oncology. Computational strategies derived from big data science-network- and machine learning-based modeling and drug repositioning-hold the promise of shedding new light on the molecular mechanisms driving neuroblastoma pathogenesis and identifying potential therapeutics to combat this devastating disease. These strategies integrate robust data input, from genomic and transcriptomic studies, clinical data, and in vivo and in vitro experimental models specific to neuroblastoma and other types of cancers that closely mimic its biological characteristics. We discuss contexts in which ""big data"" and computational approaches, especially network-based modeling, may advance neuroblastoma research, describe currently available data and resources, and propose future models of strategic data collection and analyses for neuroblastoma and other related diseases."
28034409,20.0,Visualizing the knowledge structure and evolution of big data research in healthcare informatics,2017 Feb;98:22-32.,"Background:                    In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.              Methods:                    To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers' production trends in the field and the trend of each paper's co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.              Results:                    By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People's Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).              Conclusion:                    This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions."
28032396,24.0,Computational neuroscience approach to biomarkers and treatments for mental disorders,2017 Apr;71(4):215-237.,"Psychiatry research has long experienced a stagnation stemming from a lack of understanding of the neurobiological underpinnings of phenomenologically defined mental disorders. Recently, the application of computational neuroscience to psychiatry research has shown great promise in establishing a link between phenomenological and pathophysiological aspects of mental disorders, thereby recasting current nosology in more biologically meaningful dimensions. In this review, we highlight recent investigations into computational neuroscience that have undertaken either theory- or data-driven approaches to quantitatively delineate the mechanisms of mental disorders. The theory-driven approach, including reinforcement learning models, plays an integrative role in this process by enabling correspondence between behavior and disorder-specific alterations at multiple levels of brain organization, ranging from molecules to cells to circuits. Previous studies have explicated a plethora of defining symptoms of mental disorders, including anhedonia, inattention, and poor executive function. The data-driven approach, on the other hand, is an emerging field in computational neuroscience seeking to identify disorder-specific features among high-dimensional big data. Remarkably, various machine-learning techniques have been applied to neuroimaging data, and the extracted disorder-specific features have been used for automatic case-control classification. For many disorders, the reported accuracies have reached 90% or more. However, we note that rigorous tests on independent cohorts are critically required to translate this research into clinical applications. Finally, we discuss the utility of the disorder-specific features found by the data-driven approach to psychiatric therapies, including neurofeedback. Such developments will allow simultaneous diagnosis and treatment of mental disorders using neuroimaging, thereby establishing 'theranostics' for the first time in clinical psychiatry."
28011753,75.0,A review on machine learning principles for multi-view biological data integration,2018 Mar 1;19(2):325-340.,"Driven by high-throughput sequencing techniques, modern genomic and clinical studies are in a strong need of integrative machine learning models for better use of vast volumes of heterogeneous information in the deep understanding of biological systems and the development of predictive models. How data from multiple sources (called multi-view data) are incorporated in a learning system is a key step for successful analysis. In this article, we provide a comprehensive review on omics and clinical data integration techniques, from a machine learning perspective, for various analyses such as prediction, clustering, dimension reduction and association. We shall show that Bayesian models are able to use prior information and model measurements with various distributions; tree-based methods can either build a tree with all features or collectively make a final decision based on trees learned from each view; kernel methods fuse the similarity matrices learned from individual views together for a final similarity matrix or learning model; network-based fusion methods are capable of inferring direct and indirect associations in a heterogeneous network; matrix factorization models have potential to learn interactions among features from different views; and a range of deep neural networks can be integrated in multi-modal learning for capturing the complex mechanism of biological systems."
28011145,8.0,A Functional Genomic Meta-Analysis of Clinical Trials in Systemic Sclerosis: Toward Precision Medicine and Combination Therapy,2017 May;137(5):1033-1041.,"Systemic sclerosis is an orphan, systemic autoimmune disease with no FDA-approved treatments. Its heterogeneity and rarity often result in underpowered clinical trials making the analysis and interpretation of associated molecular data challenging. We performed a meta-analysis of gene expression data from skin biopsies of patients with systemic sclerosis treated with five therapies: mycophenolate mofetil, rituximab, abatacept, nilotinib, and fresolimumab. A common clinical improvement criterion of -20% or -5 modified Rodnan skin score was applied to each study. We applied a machine learning approach that captured features beyond differential expression and was better at identifying targets of therapies than the differential expression alone. Regardless of treatment mechanism, abrogation of inflammatory pathways accompanied clinical improvement in multiple studies suggesting that high expression of immune-related genes indicates active and targetable disease. Our framework allowed us to compare different trials and ask if patients who failed one therapy would likely improve on a different therapy, based on changes in gene expression. Genes with high expression at baseline in fresolimumab nonimprovers were downregulated in mycophenolate mofetil improvers, suggesting that immunomodulatory or combination therapy may have benefitted these patients. This approach can be broadly applied to increase tissue specificity and sensitivity of differential expression results."
27999256,15.0,Recent Progress in Machine Learning-Based Methods for Protein Fold Recognition,2016 Dec 16;17(12):2118.,"Knowledge on protein folding has a profound impact on understanding the heterogeneity and molecular function of proteins, further facilitating drug design. Predicting the 3D structure (fold) of a protein is a key problem in molecular biology. Determination of the fold of a protein mainly relies on molecular experimental methods. With the development of next-generation sequencing techniques, the discovery of new protein sequences has been rapidly increasing. With such a great number of proteins, the use of experimental techniques to determine protein folding is extremely difficult because these techniques are time consuming and expensive. Thus, developing computational prediction methods that can automatically, rapidly, and accurately classify unknown protein sequences into specific fold categories is urgently needed. Computational recognition of protein folds has been a recent research hotspot in bioinformatics and computational biology. Many computational efforts have been made, generating a variety of computational prediction methods. In this review, we conduct a comprehensive survey of recent computational methods, especially machine learning-based methods, for protein fold recognition. This review is anticipated to assist researchers in their pursuit to systematically understand the computational recognition of protein folds."
27997811,7.0,Macromolecular target prediction by self-organizing feature maps,2017 Mar;12(3):271-277.,"Rational drug discovery would greatly benefit from a more nuanced appreciation of the activity of pharmacologically active compounds against a diverse panel of macromolecular targets. Already, computational target-prediction models assist medicinal chemists in library screening, de novo molecular design, optimization of active chemical agents, drug re-purposing, in the spotting of potential undesired off-target activities, and in the 'de-orphaning' of phenotypic screening hits. The self-organizing map (SOM) algorithm has been employed successfully for these and other purposes. Areas covered: The authors recapitulate contemporary artificial neural network methods for macromolecular target prediction, and present the basic SOM algorithm at a conceptual level. Specifically, they highlight consensus target-scoring by the employment of multiple SOMs, and discuss the opportunities and limitations of this technique. Expert opinion: Self-organizing feature maps represent a straightforward approach to ligand clustering and classification. Some of the appeal lies in their conceptual simplicity and broad applicability domain. Despite known algorithmic shortcomings, this computational target prediction concept has been proven to work in prospective settings with high success rates. It represents a prototypic technique for future advances in the in silico identification of the modes of action and macromolecular targets of bioactive molecules."
27979318,3.0,Molecular Diagnostics of Ageing and Tackling Age-related Disease,2017 Jan;38(1):67-80.,"As average life expectancy increases there is a greater focus on health-span and, in particular, how to treat or prevent chronic age-associated diseases. Therapies which were able to control 'biological age' with the aim of postponing chronic and costly diseases of old age require an entirely new approach to drug development. Molecular technologies and machine-learning methods have already yielded diagnostics that help guide cancer treatment and cardiovascular procedures. Discovery of valid and clinically informative diagnostics of human biological age (combined with disease-specific biomarkers) has the potential to alter current drug-discovery strategies, aid clinical trial recruitment and maximize healthy ageing. I will review some basic principles that govern the development of 'ageing' diagnostics, how such assays could be used during the drug-discovery or development process. Important logistical and statistical considerations are illustrated by reviewing recent biomarker activity in the field of Alzheimer's disease, as dementia represents the most pressing of priorities for the pharmaceutical industry, as well as the chronic disease in humans most associated with age."
27977088,,"Big Data and Analytics for Infectious Disease Research, Operations, and Policy: Proceedings of a Workshop",,"With the amount of data in the world exploding, big data could generate significant value in the field of infectious disease. The increased use of social media provides an opportunity to improve public health surveillance systems and to develop predictive models. Advances in machine learning and crowdsourcing may also offer the possibility to gather information about disease dynamics, such as contact patterns and the impact of the social environment. New, rapid, point-of-care diagnostics may make it possible to capture not only diagnostic information but also other potentially epidemiologically relevant information in real time. With a wide range of data available for analysis, decision-making and policy-making processes could be improved.                While there are many opportunities for big data to be used for infectious disease research, operations, and policy, many challenges remain before it is possible to capture the full potential of big data. In order to explore some of the opportunities and issues associated with the scientific, policy, and operational aspects of big data in relation to microbial threats and public health, the National Academies of Sciences, Engineering, and Medicine convened a workshop in May 2016. Participants discussed a range of topics including preventing, detecting, and responding to infectious disease threats using big data and related analytics; varieties of data (including demographic, geospatial, behavioral, syndromic, and laboratory) and their broader applications; means to improve their collection, processing, utility, and validation; and approaches that can be learned from other sectors to inform big data strategies for infectious disease research, operations, and policy. This publication summarizes the presentations and discussions from the workshop."
27975231,2.0,Statistical Approaches to Candidate Biomarker Panel Selection,2016;919:463-492.,"The statistical analysis of robust biomarker candidates is a complex process, and is involved in several key steps in the overall biomarker development pipeline (see Fig. 22.1, Chap. 19 ). Initially, data visualization (Sect. 22.1, below) is important to determine outliers and to get a feel for the nature of the data and whether there appear to be any differences among the groups being examined. From there, the data must be pre-processed (Sect. 22.2) so that outliers are handled, missing values are dealt with, and normality is assessed. Once the processed data has been cleaned and is ready for downstream analysis, hypothesis tests (Sect. 22.3) are performed, and proteins that are differentially expressed are identified. Since the number of differentially expressed proteins is usually larger than warrants further investigation (50+ proteins versus just a handful that will be considered for a biomarker panel), some sort of feature reduction (Sect. 22.4) should be performed to narrow the list of candidate biomarkers down to a more reasonable number. Once the list of proteins has been reduced to those that are likely most useful for downstream classification purposes, unsupervised or supervised learning is performed (Sects. 22.5 and 22.6, respectively)."
27966278,12.0,Computational resources and tools for antimicrobial peptides,2017 Jan;23(1):4-12.,"Antimicrobial peptides (AMPs), as evolutionarily conserved components of innate immune system, protect against pathogens including bacteria, fungi, viruses, and parasites. In general, AMPs are relatively small peptides (<10 kDa) with cationic nature and amphipathic structure and have modes of action different from traditional antibiotics. Up to now, there are more than 19 000 AMPs that have been reported, including those isolated from nature sources or by synthesis. They have been considered to be promising substitutes of conventional antibiotics in the quest to address the increasing occurrence of antibiotic resistance. However, most AMPs have modest direct antimicrobial activity, and their mechanisms of action, as well as their structure-activity relationships, are still poorly understood. Computational strategies are invaluable assets to provide insight into the activity of AMPs and thus exploit their potential as a new generation of antimicrobials. This article reviews the advances of AMP databases and computational tools for the prediction and design of new active AMPs. Copyright © 2016 European Peptide Society and John Wiley & Sons, Ltd."
27965365,4.0,Novel tools for quantifying secondary growth,2017 Jan;68(1):89-95.,"Secondary growth occurs in dicotyledons and gymnosperms, and results in an increased girth of plant organs. It is driven primarily by the vascular cambium, which produces thousands of cells throughout the life of several plant species. For instance, even in the small herbaceous model plant Arabidopsis, manual quantification of this massive process is impractical. Here, we provide a comprehensive overview of current methods used to measure radial growth. We discuss the issues and problematics related to its quantification. We highlight recent advances and tools developed for automated cellular phenotyping and its future applications."
27942354,17.0,"Heart Failure: Diagnosis, Severity Estimation and Prediction of Adverse Events Through Machine Learning Techniques",2016 Nov 17;15:26-47.,"Heart failure is a serious condition with high prevalence (about 2% in the adult population in developed countries, and more than 8% in patients older than 75 years). About 3-5% of hospital admissions are linked with heart failure incidents. Heart failure is the first cause of admission by healthcare professionals in their clinical practice. The costs are very high, reaching up to 2% of the total health costs in the developed countries. Building an effective disease management strategy requires analysis of large amount of data, early detection of the disease, assessment of the severity and early prediction of adverse events. This will inhibit the progression of the disease, will improve the quality of life of the patients and will reduce the associated medical costs. Toward this direction machine learning techniques have been employed. The aim of this paper is to present the state-of-the-art of the machine learning methodologies applied for the assessment of heart failure. More specifically, models predicting the presence, estimating the subtype, assessing the severity of heart failure and predicting the presence of adverse events, such as destabilizations, re-hospitalizations, and mortality are presented. According to the authors' knowledge, it is the first time that such a comprehensive review, focusing on all aspects of the management of heart failure, is presented."
27940887,31.0,Machine learning and computer vision approaches for phenotypic profiling,2017 Jan 2;216(1):65-71.,"With recent advances in high-throughput, automated microscopy, there has been an increased demand for effective computational strategies to analyze large-scale, image-based data. To this end, computer vision approaches have been applied to cell segmentation and feature extraction, whereas machine-learning approaches have been developed to aid in phenotypic classification and clustering of data acquired from biological images. Here, we provide an overview of the commonly used computer vision and machine-learning methods for generating and categorizing phenotypic profiles, highlighting the general biological utility of each approach."
27917107,39.0,The Berlin Brain-Computer Interface: Progress Beyond Communication and Control,2016 Nov 21;10:530.,"The combined effect of fundamental results about neurocognitive processes and advancements in decoding mental states from ongoing brain signals has brought forth a whole range of potential neurotechnological applications. In this article, we review our developments in this area and put them into perspective. These examples cover a wide range of maturity levels with respect to their applicability. While we assume we are still a long way away from integrating Brain-Computer Interface (BCI) technology in general interaction with computers, or from implementing neurotechnological measures in safety-critical workplaces, results have already now been obtained involving a BCI as research tool. In this article, we discuss the reasons why, in some of the prospective application domains, considerable effort is still required to make the systems ready to deal with the full complexity of the real world."
27896760,1.0,Opportunities and Challenges of Multiplex Assays: A Machine Learning Perspective,2017;1546:115-122.,"Multiplex assays that allow the simultaneous measurement of multiple analytes in small sample quantities have developed into a widely used technology. Their implementation spans across multiple assay systems and can provide readouts of similar quality as the respective single-plex measures, albeit at far higher throughput. Multiplex assay systems are therefore an important element for biomarker discovery and development strategies but analysis of the derived data can face substantial challenges that may limit the possibility of identifying meaningful biological markers. This chapter gives an overview of opportunities and challenges of multiplexed biomarker analysis, in particular from the perspective of machine learning aimed at identification of predictive biological signatures."
27896759,,Identification and Clinical Translation of Biomarker Signatures: Statistical Considerations,2017;1546:103-114.,"Powerful machine learning tools exist to extract biological patterns for diagnosis or prediction from high-dimensional datasets. Simultaneous advances in high-throughput profiling technologies have led to a rapid acceleration of biomarker discovery investigations across all areas of medicine. However, the translation of biomarker signatures into clinically useful tools has thus far been difficult. In this chapter, several important considerations are discussed that influence such translation in the context of classifier design. These include aspects of variable selection that go beyond classification accuracy, as well as effects of variability on assay stability and sample size. The consideration of such factors may lead to an adaptation of biomarker discovery approaches, aimed at an optimal balance of performance and clinical translatability."
27889391,16.0,Review of fall detection techniques: A data availability perspective,2017 Jan;39:12-22.,"A fall is an abnormal activity that occurs rarely; however, missing to identify falls can have serious health and safety implications on an individual. Due to the rarity of occurrence of falls, there may be insufficient or no training data available for them. Therefore, standard supervised machine learning methods may not be directly applied to handle this problem. In this paper, we present a taxonomy for the study of fall detection from the perspective of availability of fall data. The proposed taxonomy is independent of the type of sensors used and specific feature extraction/selection methods. The taxonomy identifies different categories of classification methods for the study of fall detection based on the availability of their data during training the classifiers. Then, we present a comprehensive literature review within those categories and identify the approach of treating a fall as an abnormal activity to be a plausible research direction. We conclude our paper by discussing several open research problems in the field and pointers for future research."
27886184,28.0,A view of the current and future role of optical coherence tomography in the management of age-related macular degeneration,2017 Jan;31(1):26-44.,"Optical coherence tomography (OCT) has become an established diagnostic technology in the clinical management of age-related macular degeneration (AMD). OCT is being used for primary diagnosis, evaluation of therapeutic efficacy, and long-term monitoring. Computer-based advances in image analysis provide complementary imaging tools such as OCT angiography, further novel automated analysis methods as well as feature detection and prediction of prognosis in disease and therapy by machine learning. In early AMD, pathognomonic features such as drusen, pseudodrusen, and abnormalities of the retinal pigment epithelium (RPE) can be imaged in a qualitative and quantitative way to identify early signs of disease activity and define the risk of progression. In advanced AMD, disease activity can be monitored clearly by qualitative and quantified analyses of fluid pooling, such as intraretinal cystoid fluid, subretinal fluid, and pigment epithelial detachment (PED). Moreover, machine learning methods detect a large spectrum of new biomarkers. Evaluation of treatment efficacy and definition of optimal therapeutic regimens are an important aim in managing neovascular AMD. In atrophic AMD hallmarked by geographic atrophy (GA), advanced spectral domain (SD)-OCT imaging largely replaces conventional fundus autofluorescence (FAF) as it adds insight into the condition of the neurosensory layers and associated alterations at the level of the RPE and choroid. Exploration of imaging features by computerized methods has just begun but has already opened relevant and reliable horizons for the optimal use of OCT imaging for individualized and population-based management of AMD-the leading retinal epidemic of modern times."
27870246,1.0,Materials Informatics: Statistical Modeling in Material Science,2016 Dec;35(11-12):568-579.,"Material informatics is engaged with the application of informatic principles to materials science in order to assist in the discovery and development of new materials. Central to the field is the application of data mining techniques and in particular machine learning approaches, often referred to as Quantitative Structure Activity Relationship (QSAR) modeling, to derive predictive models for a variety of materials-related ""activities"". Such models can accelerate the development of new materials with favorable properties and provide insight into the factors governing these properties. Here we provide a comparison between medicinal chemistry/drug design and materials-related QSAR modeling and highlight the importance of developing new, materials-specific descriptors. We survey some of the most recent QSAR models developed in materials science with focus on energetic materials and on solar cells. Finally we present new examples of material-informatic analyses of solar cells libraries produced from metal oxides using combinatorial material synthesis. Different analyses lead to interesting physical insights as well as to the design of new cells with potentially improved photovoltaic parameters."
27863190,43.0,Human Papillomavirus Drives Tumor Development Throughout the Head and Neck: Improved Prognosis Is Associated With an Immune Response Largely Restricted to the Oropharynx,2016 Dec;34(34):4132-4141.,"Purpose In squamous cell carcinomas of the head and neck (HNSCC), the increasing incidence of oropharyngeal squamous cell carcinomas (OPSCCs) is attributable to human papillomavirus (HPV) infection. Despite commonly presenting at late stage, HPV-driven OPSCCs are associated with improved prognosis compared with HPV-negative disease. HPV DNA is also detectable in nonoropharyngeal (non-OPSCC), but its pathogenic role and clinical significance are unclear. The objectives of this study were to determine whether HPV plays a causal role in non-OPSCC and to investigate whether HPV confers a survival benefit in these tumors. Methods Meta-analysis was used to build a cross-tissue gene-expression signature for HPV-driven cancer. Classifiers trained by machine-learning approaches were used to predict the HPV status of 520 HNSCCs profiled by The Cancer Genome Atlas project. DNA methylation data were similarly used to classify 464 HNSCCs and these analyses were integrated with genomic, histopathology, and survival data to permit a comprehensive comparison of HPV transcript-positive OPSCC and non-OPSCC. Results HPV-driven tumors accounted for 4.1% of non-OPSCCs. Regardless of anatomic site, HPV+ HNSCCs shared highly similar gene expression and DNA methylation profiles; nonkeratinizing, basaloid histopathological features; and lack of TP53 or CDKN2A alterations. Improved overall survival, however, was largely restricted to HPV-driven OPSCCs, which were associated with increased levels of tumor-infiltrating lymphocytes compared with HPV-driven non-OPSCCs. Conclusion Our analysis identified a causal role for HPV in transcript-positive non-OPSCCs throughout the head and neck. Notably, however, HPV-driven non-OPSCCs display a distinct immune microenvironment and clinical behavior compared with HPV-driven OPSCCs."
27862943,18.0,Blood transcriptomic comparison of individuals with and without autism spectrum disorder: A combined-samples mega-analysis,2017 Apr;174(3):181-201.,"Blood-based microarray studies comparing individuals affected with autism spectrum disorder (ASD) and typically developing individuals help characterize differences in circulating immune cell functions and offer potential biomarker signal. We sought to combine the subject-level data from previously published studies by mega-analysis to increase the statistical power. We identified studies that compared ex vivo blood or lymphocytes from ASD-affected individuals and unrelated comparison subjects using Affymetrix or Illumina array platforms. Raw microarray data and clinical meta-data were obtained from seven studies, totaling 626 affected and 447 comparison subjects. Microarray data were processed using uniform methods. Covariate-controlled mixed-effect linear models were used to identify gene transcripts and co-expression network modules that were significantly associated with diagnostic status. Permutation-based gene-set analysis was used to identify functionally related sets of genes that were over- and under-expressed among ASD samples. Our results were consistent with diminished interferon-, EGF-, PDGF-, PI3K-AKT-mTOR-, and RAS-MAPK-signaling cascades, and increased ribosomal translation and NK-cell related activity in ASD. We explored evidence for sex-differences in the ASD-related transcriptomic signature. We also demonstrated that machine-learning classifiers using blood transcriptome data perform with moderate accuracy when data are combined across studies. Comparing our results with those from blood-based studies of protein biomarkers (e.g., cytokines and trophic factors), we propose that ASD may feature decoupling between certain circulating signaling proteins (higher in ASD samples) and the transcriptional cascades which they typically elicit within circulating immune cells (lower in ASD samples). These findings provide insight into ASD-related transcriptional differences in circulating immune cells. © 2016 Wiley Periodicals, Inc."
27862002,4.0,Distributed data networks: a blueprint for Big Data sharing and healthcare analytics,2017 Jan;1387(1):105-111.,"This paper defines the attributes of distributed data networks and outlines the data and analytic infrastructure needed to build and maintain a successful network. We use examples from one successful implementation of a large-scale, multisite, healthcare-related distributed data network, the U.S. Food and Drug Administration-sponsored Sentinel Initiative. Analytic infrastructure-development concepts are discussed from the perspective of promoting six pillars of analytic infrastructure: consistency, reusability, flexibility, scalability, transparency, and reproducibility. This paper also introduces one use case for machine learning algorithm development to fully utilize and advance the portfolio of population health analytics, particularly those using multisite administrative data sources."
27848884,4.0,Understanding the Structural Basis for Inhibition of Cyclin-Dependent Kinases. New Pieces in the Molecular Puzzle,2017;18(9):1104-1111.,"Background:                    Cyclin-dependent kinases (CDKs) comprise an important protein family for development of drugs, mostly aimed for use in treatment of cancer but there is also potential for development of drugs for neurodegenerative diseases and diabetes. Since the early 1990s, structural studies have been carried out on CDKs, in order to determine the structural basis for inhibition of this protein target.              Objective:                    Our goal here is to review recent structural studies focused on CDKs. We concentrate on latest developments in the understanding of the structural basis for inhibition of CDKs, relating structures and ligand-binding information.              Method:                    Protein crystallography has been successfully applied to elucidate over 400 CDK structures. Most of these structures are complexed with inhibitors. We use this richness of structural information to describe the major structural features determining the inhibition of this enzyme.              Results:                    Structures of CDK1, 2, 4-9, 12 13, and 16 have been elucidated. Analysis of these structures in complex with a wide range of different competitive inhibitors, strongly indicate some common features that can be used to guide the development of CDK inhibitors, such as a pattern of hydrogen bonding and the presence of halogen atoms in the ligand structure.              Conclusion:                    Nowadays we have structural information for hundreds of CDKs. Combining the structural and functional information we may say that a pattern of intermolecular hydrogen bonds is of pivotal importance for inhibitor specificity. In addition, machine learning techniques have shown improvements in predicting binding affinity for CDKs."
27842478,1.0,A Review of Computational Methods for Predicting Drug Targets,2018;19(6):562-572.,"Drug discovery and development is not only a time-consuming and labor-intensive process but also full of risk. Identifying targets of small molecules helps evaluate safety of drugs and find new therapeutic applications. The biotechnology measures a wide variety of properties related to drug and targets from different perspectives, thus generating a large body of data. This undoubtedly provides a solid foundation to explore relationships between drugs and targets. A large number of computational techniques have recently been developed for drug target prediction. In this paper, we summarize these computational methods and classify them into structure-based, molecular activity-based, side-effectbased and multi-omics-based predictions according to the used data for inference. The multi-omicsbased methods are further grouped into two types: classifier-based and network-based predictions. Furthermore， the advantages and limitations of each type of methods are discussed. Finally, we point out the future directions of computational predictions for drug targets."
27841450,332.0,Risk factors for suicidal thoughts and behaviors: A meta-analysis of 50 years of research,2017 Feb;143(2):187-232.,"Suicidal thoughts and behaviors (STBs) are major public health problems that have not declined appreciably in several decades. One of the first steps to improving the prevention and treatment of STBs is to establish risk factors (i.e., longitudinal predictors). To provide a summary of current knowledge about risk factors, we conducted a meta-analysis of studies that have attempted to longitudinally predict a specific STB-related outcome. This included 365 studies (3,428 total risk factor effect sizes) from the past 50 years. The present random-effects meta-analysis produced several unexpected findings: across odds ratio, hazard ratio, and diagnostic accuracy analyses, prediction was only slightly better than chance for all outcomes; no broad category or subcategory accurately predicted far above chance levels; predictive ability has not improved across 50 years of research; studies rarely examined the combined effect of multiple risk factors; risk factors have been homogenous over time, with 5 broad categories accounting for nearly 80% of all risk factor tests; and the average study was nearly 10 years long, but longer studies did not produce better prediction. The homogeneity of existing research means that the present meta-analysis could only speak to STB risk factor associations within very narrow methodological limits-limits that have not allowed for tests that approximate most STB theories. The present meta-analysis accordingly highlights several fundamental changes needed in future studies. In particular, these findings suggest the need for a shift in focus from risk factors to machine learning-based risk algorithms. (PsycINFO Database Record"
27830251,2.0,Biomechanisms of Comorbidity: Reviewing Integrative Analyses of Multi-omics Datasets and Electronic Health Records,2016 Nov 10;(1):194-206.,"Objectives:                    Disease comorbidity is a pervasive phenomenon impacting patients' health outcomes, disease management, and clinical decisions. This review presents past, current and future research directions leveraging both phenotypic and molecular information to uncover disease similarity underpinning the biology and etiology of disease comorbidity.              Methods:                    We retrieved ~130 publications and retained 59, ranging from 2006 to 2015, that comprise a minimum number of five diseases and at least one type of biomolecule. We surveyed their methods, disease similarity metrics, and calculation of comorbidities in the electronic health records, if present.              Results:                    Among the surveyed studies, 44% generated or validated disease similarity metrics in context of comorbidity, with 60% being published in the last two years. As inputs, 87% of studies utilized intragenic loci and proteins while 13% employed RNA (mRNA, LncRNA or miRNA). Network modeling was predominantly used (35%) followed by statistics (28%) to impute similarity between these biomolecules and diseases. Studies with large numbers of biomolecules and diseases used network models or naïve overlap of disease-molecule associations, while machine learning, statistics, and information retrieval were utilized in smaller and moderate sized studies. Multiscale computations comprising shared function, network topology, and phenotypes were performed exclusively on proteins.              Conclusion:                    This review highlighted the growing methods for identifying the molecular mechanisms underpinning comorbidities that leverage multiscale molecular information and patterns from electronic health records. The survey unveiled that intergenic polymorphisms have been overlooked for similarity imputation compared to their intragenic counterparts, offering new opportunities to bridge the mechanistic and similarity gaps of comorbidity."
27829350,4.0,Drug-Target Interactions: Prediction Methods and Applications,2018;19(6):537-561.,"Identifying the interactions between drugs and target proteins is a key step in drug discovery. This not only aids to understand the disease mechanism, but also helps to identify unexpected therapeutic activity or adverse side effects of drugs. Hence, drug-target interaction prediction becomes an essential tool in the field of drug repurposing. The availability of heterogeneous biological data on known drug-target interactions enabled many researchers to develop various computational methods to decipher unknown drug-target interactions. This review provides an overview on these computational methods for predicting drug-target interactions along with available webservers and databases for drug-target interactions. Further, the applicability of drug-target interactions in various diseases for identifying lead compounds has been outlined."
27823566,,Parallel Computing for Brain Simulation,2017;17(14):1646-1668.,"Background:                    The human brain is the most complex system in the known universe, it is therefore one of the greatest mysteries. It provides human beings with extraordinary abilities. However, until now it has not been understood yet how and why most of these abilities are produced.              Aims:                    For decades, researchers have been trying to make computers reproduce these abilities, focusing on both understanding the nervous system and, on processing data in a more efficient way than before. Their aim is to make computers process information similarly to the brain. Important technological developments and vast multidisciplinary projects have allowed creating the first simulation with a number of neurons similar to that of a human brain.              Conclusion:                    This paper presents an up-to-date review about the main research projects that are trying to simulate and/or emulate the human brain. They employ different types of computational models using parallel computing: digital models, analog models and hybrid models. This review includes the current applications of these works, as well as future trends. It is focused on various works that look for advanced progress in Neuroscience and still others which seek new discoveries in Computer Science (neuromorphic hardware, machine learning techniques). Their most outstanding characteristics are summarized and the latest advances and future plans are presented. In addition, this review points out the importance of considering not only neurons: Computational models of the brain should also include glial cells, given the proven importance of astrocytes in information processing."
27817211,7.0,Asthma phenotypes in childhood,2017 Jul;13(7):705-713.,"Asthma is no longer thought of as a single disease, but rather a collection of varying symptoms expressing different disease patterns. One of the ongoing challenges is understanding the underlying pathophysiological mechanisms that may be responsible for the varying responses to treatment. Areas Covered: This review provides an overview of our current understanding of the asthma phenotype concept in childhood and describes key findings from both conventional and data-driven methods. Expert Commentary: With the vast amounts of data generated from cohorts, there is hope that we can elucidate distinct pathophysiological mechanisms, or endotypes. In return, this would lead to better patient stratification and disease management, thereby providing true personalised medicine."
27815038,49.0,"Phenotypes in obstructive sleep apnea: A definition, examples and evolution of approaches",2017 Oct;35:113-123.,"Obstructive sleep apnea (OSA) is a complex and heterogeneous disorder and the apnea hypopnea index alone can not capture the diverse spectrum of the condition. Enhanced phenotyping can improve prognostication, patient selection for clinical trials, understanding of mechanisms, and personalized treatments. In OSA, multiple condition characteristics have been termed ""phenotypes."" To help classify patients into relevant prognostic and therapeutic categories, an OSA phenotype can be operationally defined as: ""A category of patients with OSA distinguished from others by a single or combination of disease features, in relation to clinically meaningful attributes (symptoms, response to therapy, health outcomes, quality of life)."" We review approaches to clinical phenotyping in OSA, citing examples of increasing analytic complexity. Although clinical feature based OSA phenotypes with significant prognostic and treatment implications have been identified (e.g., excessive daytime sleepiness OSA), many current categorizations lack association with meaningful outcomes. Recent work focused on pathophysiologic risk factors for OSA (e.g., arousal threshold, craniofacial morphology, chemoreflex sensitivity) appears to capture heterogeneity in OSA, but requires clinical validation. Lastly, we discuss the use of machine learning as a promising phenotyping strategy that can integrate multiple types of data (genomic, molecular, cellular, clinical) to identify unique, meaningful OSA phenotypes."
27814027,19.0,Harnessing Big Data for Systems Pharmacology,2017 Jan 6;57:245-262.,"Systems pharmacology aims to holistically understand mechanisms of drug actions to support drug discovery and clinical practice. Systems pharmacology modeling (SPM) is data driven. It integrates an exponentially growing amount of data at multiple scales (genetic, molecular, cellular, organismal, and environmental). The goal of SPM is to develop mechanistic or predictive multiscale models that are interpretable and actionable. The current explosions in genomics and other omics data, as well as the tremendous advances in big data technologies, have already enabled biologists to generate novel hypotheses and gain new knowledge through computational models of genome-wide, heterogeneous, and dynamic data sets. More work is needed to interpret and predict a drug response phenotype, which is dependent on many known and unknown factors. To gain a comprehensive understanding of drug actions, SPM requires close collaborations between domain experts from diverse fields and integration of heterogeneous models from biophysics, mathematics, statistics, machine learning, and semantic webs. This creates challenges in model management, model integration, model translation, and knowledge integration. In this review, we discuss several emergent issues in SPM and potential solutions using big data technology and analytics. The concurrent development of high-throughput techniques, cloud computing, data science, and the semantic web will likely allow SPM to be findable, accessible, interoperable, reusable, reliable, interpretable, and actionable."
27800282,10.0,Digital Suicide Prevention: Can Technology Become a Game-changer?,2016 Jun 1;13(5-6):16-20.,"Suicide continues to be a leading cause of death and has been recognized as a significant public health issue. Rapid advances in data science can provide us with useful tools for suicide prevention, and help to dynamically assess suicide risk in quantitative data-driven ways. In this article, the authors highlight the most current international research in digital suicide prevention, including the use of machine learning, smartphone applications, and wearable sensor driven systems. The authors also discuss future opportunities for digital suicide prevention, and propose a novel Sensor-driven Mental State Assessment System."
30645788,,The Diagnostic Imagination in Radiology: Part 1,2016 Nov;38(6):39-44.,"*Machines that dream, the restless impulse for technical change that has marked radiology from its beginning and forays into deep neural networks, will no doubt unsettle long-held institu- tional practices in radiology. *A willingness to collaborate and puzzle through machine intelligence has come from those who have not accepted the status quo. A certain form of scientific curiosity has been a guiding principle in their work. *In radiology, machine intelligence has been extremely useful and built into just about every major technical innovation. But it has only been the last several years that a subfield of Al, machine learning, has begun to show remarkably fast development due to faster comput- er processing capabilities and advanced modeling and results emerging from the application of deep learning."
27784247,2.0,Current Trends in Drug Sensitivity Prediction,2016;22(46):6918-6927.,"Cancer cell line panels have proved useful disease models to, among others, identify genomic markers of drug sensitivity and to develop new anticancer drugs. The increasing availability of in vitro sensitivity and cell line profiling data sets raises the question of whether this information could be used, and to which extent, to predict the activity of drugs in cancer cell lines and, ultimately, in patients tumors. Drug sensitivity prediction embraces those approaches aiming at predicting in vitro drug activity on cancer cell lines by integrating genomic and/or chemical information using machine learning models. In this review, we summarize the cytotoxicity assays generally used to determine in vitro activity on cultured cell lines, and revisit the drug sensitivity prediction studies that have leveraged chemical and cell line profiling data from the NCI60, Cancer Cell Line Encyclopedia (CCLE) and Genomics of Drug Sensitivity in Cancer (GDSC) projects. A section outlining current limitations and future perspectives in the field closes the review."
27452181,3.0,Clinical chemistry in higher dimensions: Machine-learning and enhanced prediction from routine clinical chemistry data,2016 Nov;49(16-17):1213-1220.,"Big Data is having an impact on many areas of research, not the least of which is biomedical science. In this review paper, big data and machine learning are defined in terms accessible to the clinical chemistry community. Seven myths associated with machine learning and big data are then presented, with the aim of managing expectation of machine learning amongst clinical chemists. The myths are illustrated with four examples investigating the relationship between biomarkers in liver function tests, enhanced laboratory prediction of hepatitis virus infection, the relationship between bilirubin and white cell count, and the relationship between red cell distribution width and laboratory prediction of anaemia."
27241666,54.0,Big Data and machine learning in radiation oncology: State of the art and future prospects,2016 Nov 1;382(1):110-117.,"Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed."
26670115,,Machine learning and new vital signs monitoring in civilian en route care: A systematic review of the literature and future implications for the military,2016 Nov;81(5 Suppl 2 Proceedings of the 2015 Military Health System Research Symposium):S111-S115.,"Background:                    Although air transport medical services are today an integral part of trauma systems in most developed countries, to date, there are no reviews on recent innovations in civilian en route care. The purpose of this systematic review was to identify potential machine learning and new vital signs monitoring technologies in civilian en route care that could help close civilian and military capability gaps in monitoring and the early detection and treatment of various trauma injuries.              Methods:                    MEDLINE, the Cochrane Database of Systematic Reviews, and citation review of relevant primary and review articles were searched for studies involving civilian en route care, air medical transport, and technologies from January 2005 to November 2015. Data were abstracted on study design, population, year, sponsors, innovation category, details of technologies, and outcomes.              Results:                    Thirteen observational studies involving civilian medical transport met inclusion criteria. Studies either focused on machine learning and software algorithms (n = 5), new vital signs monitoring (n = 6), or both (n = 2). Innovations involved continuous digital acquisition of physiologic data and parameter extraction. Importantly, all studies (n = 13) demonstrated improved outcomes where applicable and potential use during civilian and military en route care. However, almost all studies required further validation in prospective and/or randomized controlled trials.              Conclusion:                    Potential machine learning technologies and monitoring of novel vital signs such as heart rate variability and complexity in civilian en route care could help enhance en route care for our nation's war fighters. In a complex global environment, they could potentially fill capability gaps such as monitoring and the early detection and treatment of various trauma injuries. However, the impact of these innovations and technologies will require further validation before widespread acceptance and prehospital use.              Level of evidence:                    Systematic review, level V."
27479316,13.0,Molecular interaction fingerprint approaches for GPCR drug discovery,2016 Oct;30:59-68.,"Protein-ligand interaction fingerprints (IFPs) are binary 1D representations of the 3D structure of protein-ligand complexes encoding the presence or absence of specific interactions between the binding pocket amino acids and the ligand. Various implementations of IFPs have been developed and successfully applied for post-processing molecular docking results for G Protein-Coupled Receptor (GPCR) ligand binding mode prediction and virtual ligand screening. Novel interaction fingerprint methods enable structural chemogenomics and polypharmacology predictions by complementing the increasing amount of GPCR structural data. Machine learning methods are increasingly used to derive relationships between bioactivity data and fingerprint descriptors of chemical and structural information of binding sites, ligands, and protein-ligand interactions. Factors that influence the application of IFPs include structure preparation, binding site definition, fingerprint similarity assessment, and data processing and these factors pose challenges as well possibilities to optimize interaction fingerprint methods for GPCR drug discovery."
27718010,5.0,"From ""ear"" to there: a review of biorobotic models of auditory processing in lizards",2016 Oct;110(4-5):303-317.,"The peripheral auditory system of lizards has been extensively studied, because of its remarkable directionality. In this paper, we review the research that has been performed on this system using a biorobotic approach. The various robotic implementations developed to date, both wheeled and legged, of the auditory model exhibit strong phonotactic performance for two types of steering mechanisms-a simple threshold decision model and Braitenberg sensorimotor cross-couplings. The Braitenberg approach removed the need for a decision model, but produced relatively inefficient robot trajectories. Introducing various asymmetries in the auditory model reduced the efficiency of the robot trajectories, but successful phonotaxis was maintained. Relatively loud noise distractors degraded the trajectory efficiency and above-threshold noise resulted in unsuccessful phonotaxis. Machine learning techniques were applied to successfully compensate for asymmetries as well as noise distractors. Such techniques were also successfully used to construct a representation of auditory space, which is crucial for sound localisation while remaining stationary as opposed to phonotaxis-based localisation. The peripheral auditory model was furthermore found to adhere to an auditory scaling law governing the variation in frequency response with respect to physical ear separation. Overall, the research to date paves the way towards investigating the more fundamental topic of auditory metres versus auditory maps, and the existing robotic implementations can act as tools to compare the two approaches."
27698038,4.0,A perspective on bridging scales and design of models using low-dimensional manifolds and data-driven model inference,2016 Nov 13;374(2080):20160144.,"Systems in nature capable of collective behaviour are nonlinear, operating across several scales. Yet our ability to account for their collective dynamics differs in physics, chemistry and biology. Here, we briefly review the similarities and differences between mathematical modelling of adaptive living systems versus physico-chemical systems. We find that physics-based chemistry modelling and computational neuroscience have a shared interest in developing techniques for model reductions aiming at the identification of a reduced subsystem or slow manifold, capturing the effective dynamics. By contrast, as relations and kinetics between biological molecules are less characterized, current quantitative analysis under the umbrella of bioinformatics focuses on signal extraction, correlation, regression and machine-learning analysis. We argue that model reduction analysis and the ensuing identification of manifolds bridges physics and biology. Furthermore, modelling living systems presents deep challenges as how to reconcile rich molecular data with inherent modelling uncertainties (formalism, variables selection and model parameters). We anticipate a new generative data-driven modelling paradigm constrained by identified governing principles extracted from low-dimensional manifold analysis. The rise of a new generation of models will ultimately connect biology to quantitative mechanistic descriptions, thereby setting the stage for investigating the character of the model language and principles driving living systems.This article is part of the themed issue 'Multiscale modelling at the physics-chemistry-biology interface'."
27698035,28.0,Big data need big theory too,2016 Nov 13;374(2080):20160153.,"The current interest in big data, machine learning and data analytics has generated the widespread impression that such methods are capable of solving most problems without the need for conventional scientific methods of inquiry. Interest in these methods is intensifying, accelerated by the ease with which digitized data can be acquired in virtually all fields of endeavour, from science, healthcare and cybersecurity to economics, social sciences and the humanities. In multiscale modelling, machine learning appears to provide a shortcut to reveal correlations of arbitrary complexity between processes at the atomic, molecular, meso- and macroscales. Here, we point out the weaknesses of pure big data approaches with particular focus on biology and medicine, which fail to provide conceptual accounts for the processes to which they are applied. No matter their 'depth' and the sophistication of data-driven methods, such as artificial neural nets, in the end they merely fit curves to existing data. Not only do these methods invariably require far larger quantities of data than anticipated by big data aficionados in order to produce statistically reliable results, but they can also fail in circumstances beyond the range of the data used to train them because they are not designed to model the structural characteristics of the underlying system. We argue that it is vital to use theory as a guide to experimental design for maximal efficiency of data collection and to produce reliable predictive models and conceptual knowledge. Rather than continuing to fund, pursue and promote 'blind' big data projects with massive budgets, we call for more funding to be allocated to the elucidation of the multiscale and stochastic processes controlling the behaviour of complex systems, including those of life, medicine and healthcare.This article is part of the themed issue 'Multiscale modelling at the physics-chemistry-biology interface'."
27693712,32.0,Design of efficient computational workflows for in silico drug repurposing,2017 Feb;22(2):210-222.,"Here, we provide a comprehensive overview of the current status of in silico repurposing methods by establishing links between current technological trends, data availability and characteristics of the algorithms used in these methods. Using the case of the computational repurposing of fasudil as an alternative autophagy enhancer, we suggest a generic modular organization of a repurposing workflow. We also review 3D structure-based, similarity-based, inference-based and machine learning (ML)-based methods. We summarize the advantages and disadvantages of these methods to emphasize three current technical challenges. We finish by discussing current directions of research, including possibilities offered by new methods, such as deep learning."
27659841,7.0,Neuroprogression and illness trajectories in bipolar disorder,2017 Mar;17(3):277-285.,"The longitudinal course of bipolar disorder is highly variable, and a subset of patients seems to present a progressive course associated with brain changes and functional impairment. Areas covered: We discuss the theory of neuroprogression in bipolar disorder. This concept considers the systemic stress response that occurs within mood episodes and late-stage deficits in functioning and cognition as well as neuroanatomic changes. We also discuss treatment refractoriness that may take place in some cases of bipolar disorder. We searched PubMed for articles published in any language up to June 4th, 2016. We found 315 abstracts and included 87 studies in our review. Expert commentary: We are of the opinion that the use of specific pharmacological strategies and functional remediation may be potentially useful in bipolar patients at late-stages. New analytic approaches using multimodal data hold the potential to help in identifying signatures of subgroups of patients who will develop a neuroprogressive course."
27656787,3.0,Artificial consciousness and the consciousness-attention dissociation,2016 Oct;45:210-225.,"Artificial Intelligence is at a turning point, with a substantial increase in projects aiming to implement sophisticated forms of human intelligence in machines. This research attempts to model specific forms of intelligence through brute-force search heuristics and also reproduce features of human perception and cognition, including emotions. Such goals have implications for artificial consciousness, with some arguing that it will be achievable once we overcome short-term engineering challenges. We believe, however, that phenomenal consciousness cannot be implemented in machines. This becomes clear when considering emotions and examining the dissociation between consciousness and attention in humans. While we may be able to program ethical behavior based on rules and machine learning, we will never be able to reproduce emotions or empathy by programming such control systems-these will be merely simulations. Arguments in favor of this claim include considerations about evolution, the neuropsychological aspects of emotions, and the dissociation between attention and consciousness found in humans. Ultimately, we are far from achieving artificial consciousness."
27656117,15.0,Pattern Recognition Approaches for Breast Cancer DCE-MRI Classification: A Systematic Review,2016;36(4):449-459.,"We performed a systematic review of several pattern analysis approaches for classifying breast lesions using dynamic, morphological, and textural features in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Several machine learning approaches, namely artificial neural networks (ANN), support vector machines (SVM), linear discriminant analysis (LDA), tree-based classifiers (TC), and Bayesian classifiers (BC), and features used for classification are described. The findings of a systematic review of 26 studies are presented. The sensitivity and specificity are respectively 91 and 83 % for ANN, 85 and 82 % for SVM, 96 and 85 % for LDA, 92 and 87 % for TC, and 82 and 85 % for BC. The sensitivity and specificity are respectively 82 and 74 % for dynamic features, 93 and 60 % for morphological features, 88 and 81 % for textural features, 95 and 86 % for a combination of dynamic and morphological features, and 88 and 84 % for a combination of dynamic, morphological, and other features. LDA and TC have the best performance. A combination of dynamic and morphological features gives the best performance."
27652181,20.0,An overview of topic modeling and its current applications in bioinformatics,2016 Sep 20;5(1):1608.,"Background:                    With the rapid accumulation of biological datasets, machine learning methods designed to automate data analysis are urgently needed. In recent years, so-called topic models that originated from the field of natural language processing have been receiving much attention in bioinformatics because of their interpretability. Our aim was to review the application and development of topic models for bioinformatics.              Description:                    This paper starts with the description of a topic model, with a focus on the understanding of topic modeling. A general outline is provided on how to build an application in a topic model and how to develop a topic model. Meanwhile, the literature on application of topic models to biological data was searched and analyzed in depth. According to the types of models and the analogy between the concept of document-topic-word and a biological object (as well as the tasks of a topic model), we categorized the related studies and provided an outlook on the use of topic models for the development of bioinformatics applications.              Conclusion:                    Topic modeling is a useful method (in contrast to the traditional means of data reduction in bioinformatics) and enhances researchers' ability to interpret biological information. Nevertheless, due to the lack of topic models optimized for specific biological data, the studies on topic modeling in biological data still have a long and challenging road ahead. We believe that topic models are a promising method for various applications in bioinformatics research."
27650405,4.0,Systematic review of blood transcriptome profiling in neuropsychiatric disorders: guidelines for biomarker discovery,2016 Sep;31(5):373-81.,"Introduction:                    The utility of blood for genome-wide gene expression profiling and biomarker discovery has received much attention in patients diagnosed with major neuropsychiatric disorders. While numerous studies have been conducted, statistical rigor and clarity in terms of blood-based biomarker discovery, validation, and testing are needed.              Methods:                    We conducted a systematic review of the literature to investigate methodological approaches and to assess the value of blood transcriptome profiling in research on mental disorders. We were particularly interested in statistical considerations related to machine learning, gene network analyses, and convergence across different disorders.              Results:                    A total of 108 peripheral blood transcriptome studies across 15 disorders were surveyed: 25 studies used a variety of machine learning techniques to assess putative clinical viability of the candidate biomarkers; 11 leveraged a higher-order systems-level perspective to identify gene module-based biomarkers; and nine performed analyses across two or more neuropsychiatric phenotypes. Notably, ~50% of the surveyed studies included fewer than 50 samples (cases and controls), while ~75% included less than 100.              Conclusions:                    Detailed consideration of statistical analysis in the early stages of experimental planning is critical to ensure blood-based biomarker discovery and validation. Statistical guidelines are presented to enhance implementation and reproducibility of machine learning and gene network analyses across independent studies. Future studies capitalizing on larger sample sizes and emerging next-generation technologies set the stage for moving the field forwards. Copyright © 2016 John Wiley & Sons, Ltd."
27649151,39.0,Omics-Based Strategies in Precision Medicine: Toward a Paradigm Shift in Inborn Errors of Metabolism Investigations,2016 Sep 14;17(9):1555.,"The rise of technologies that simultaneously measure thousands of data points represents the heart of systems biology. These technologies have had a huge impact on the discovery of next-generation diagnostics, biomarkers, and drugs in the precision medicine era. Systems biology aims to achieve systemic exploration of complex interactions in biological systems. Driven by high-throughput omics technologies and the computational surge, it enables multi-scale and insightful overviews of cells, organisms, and populations. Precision medicine capitalizes on these conceptual and technological advancements and stands on two main pillars: data generation and data modeling. High-throughput omics technologies allow the retrieval of comprehensive and holistic biological information, whereas computational capabilities enable high-dimensional data modeling and, therefore, accessible and user-friendly visualization. Furthermore, bioinformatics has enabled comprehensive multi-omics and clinical data integration for insightful interpretation. Despite their promise, the translation of these technologies into clinically actionable tools has been slow. In this review, we present state-of-the-art multi-omics data analysis strategies in a clinical context. The challenges of omics-based biomarker translation are discussed. Perspectives regarding the use of multi-omics approaches for inborn errors of metabolism (IEM) are presented by introducing a new paradigm shift in addressing IEM investigations in the post-genomic era."
27641093,17.0,"Computational inference of gene regulatory networks: Approaches, limitations and opportunities",2017 Jan;1860(1):41-52.,"Gene regulatory networks lie at the core of cell function control. In E. coli and S. cerevisiae, the study of gene regulatory networks has led to the discovery of regulatory mechanisms responsible for the control of cell growth, differentiation and responses to environmental stimuli. In plants, computational rendering of gene regulatory networks is gaining momentum, thanks to the recent availability of high-quality genomes and transcriptomes and development of computational network inference approaches. Here, we review current techniques, challenges and trends in gene regulatory network inference and highlight challenges and opportunities for plant science. We provide plant-specific application examples to guide researchers in selecting methodologies that suit their particular research questions. Given the interdisciplinary nature of gene regulatory network inference, we tried to cater to both biologists and computer scientists to help them engage in a dialogue about concepts and caveats in network inference. Specifically, we discuss problems and opportunities in heterogeneous data integration for eukaryotic organisms and common caveats to be considered during network model evaluation. This article is part of a Special Issue entitled: Plant Gene Regulatory Mechanisms and Networks, edited by Dr. Erich Grotewold and Dr. Nathan Springer."
27610328,29.0,Big data and tactical analysis in elite soccer: future challenges and opportunities for sports science,2016 Aug 24;5(1):1410.,"Until recently tactical analysis in elite soccer were based on observational data using variables which discard most contextual information. Analyses of team tactics require however detailed data from various sources including technical skill, individual physiological performance, and team formations among others to represent the complex processes underlying team tactical behavior. Accordingly, little is known about how these different factors influence team tactical behavior in elite soccer. In parts, this has also been due to the lack of available data. Increasingly however, detailed game logs obtained through next-generation tracking technologies in addition to physiological training data collected through novel miniature sensor technologies have become available for research. This leads however to the opposite problem where the shear amount of data becomes an obstacle in itself as methodological guidelines as well as theoretical modelling of tactical decision making in team sports is lacking. The present paper discusses how big data and modern machine learning technologies may help to address these issues and aid in developing a theoretical model for tactical decision making in team sports. As experience from medical applications show, significant organizational obstacles regarding data governance and access to technologies must be overcome first. The present work discusses these issues with respect to tactical analyses in elite soccer and propose a technological stack which aims to introduce big data technologies into elite soccer research. The proposed approach could also serve as a guideline for other sports science domains as increasing data size is becoming a wide-spread phenomenon."
