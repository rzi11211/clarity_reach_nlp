pmid,citations,title,date,text
25666163,52.0,The application of in silico drug-likeness predictions in pharmaceutical research,2015 Jun 23;86:2-10.,"The concept of drug-likeness, established from the analyses of the physiochemical properties or/and structural features of existing small organic drugs or/and drug candidates, has been widely used to filter out compounds with undesirable properties, especially poor ADMET (absorption, distribution, metabolism, excretion, and toxicity) profiles. Here, we summarize various approaches for drug-likeness evaluations, including simple rules/filters based on molecular properties/structures and quantitative prediction models based on sophisticated machine learning methods, and provide a comprehensive review of recent advances in this field. Moreover, the strengths and weaknesses of these approaches are briefly outlined. Finally, the drug-likeness analyses of natural products and traditional Chinese medicines (TCM) are discussed."
25642500,,"Can valid and practical risk-prediction or casemix adjustment models, including adjustment for comorbidity, be generated from English hospital administrative data (Hospital Episode Statistics)? A national observational study",,"Background:                    NHS hospitals collect a wealth of administrative data covering accident and emergency (A&E) department attendances, inpatient and day case activity, and outpatient appointments. Such data are increasingly being used to compare units and services, but adjusting for risk is difficult.              Objectives:                    To derive robust risk-adjustment models for various patient groups, including those admitted for heart failure (HF), acute myocardial infarction, colorectal and orthopaedic surgery, and outcomes adjusting for available patient factors such as comorbidity, using England’s Hospital Episode Statistics (HES) data. To assess if more sophisticated statistical methods based on machine learning such as artificial neural networks (ANNs) outperform traditional logistic regression (LR) for risk prediction. To update and assess for the NHS the Charlson index for comorbidity. To assess the usefulness of outpatient data for these models.              Main outcome measures:                    Mortality, readmission, return to theatre, outpatient non-attendance. For HF patients we considered various readmission measures such as diagnosis-specific and total within a year.              Methods:                    We systematically reviewed studies comparing two or more comorbidity indices. Logistic regression, ANNs, support vector machines and random forests were compared for mortality and readmission. Models were assessed using discrimination and calibration statistics. Competing risks proportional hazards regression and various count models were used for future admissions and bed-days.              Results:                    Our systematic review and empirical analysis suggested that for general purposes comorbidity is currently best described by the set of 30 Elixhauser comorbidities plus dementia. Model discrimination was often high for mortality and poor, or at best moderate, for other outcomes, for example c = 0.62 for readmission and c = 0.73 for death following stroke. Calibration was often good for procedure groups but poorer for diagnosis groups, with overprediction of low risk a common cause. The machine learning methods we investigated offered little beyond LR for their greater complexity and implementation difficulties. For HF, some patient-level predictors differed by primary diagnosis of readmission but not by length of follow-up. Prior non-attendance at outpatient appointments was a useful, strong predictor of readmission. Hospital-level readmission rates for HF did not correlate with readmission rates for non-HF; hospital performance on national audit process measures largely correlated only with HF readmission rates.              Conclusions:                    Many practical risk-prediction or casemix adjustment models can be generated from HES data using LR, though an extra step is often required for accurate calibration. Including outpatient data in readmission models is useful. The three machine learning methods we assessed added little with these data. Readmission rates for HF patients should be divided by diagnosis on readmission when used for quality improvement.              Future work:                    As HES data continue to develop and improve in scope and accuracy, they can be used more, for instance A&E records. The return to theatre metric appears promising and could be extended to other index procedures and specialties. While our data did not warrant the testing of a larger number of machine learning methods, databases augmented with physiological and pathology information, for example, might benefit from methods such as boosted trees. Finally, one could apply the HF readmissions analysis to other chronic conditions.              Funding:                    The National Institute for Health Research Health Services and Delivery Research programme."
25620954,25.0,Movement recognition technology as a method of assessing spontaneous general movements in high risk infants,2015 Jan 9;5:284.,"Preterm birth is associated with increased risks of neurological and motor impairments such as cerebral palsy. The risks are highest in those born at the lowest gestations. Early identification of those most at risk is challenging meaning that a critical window of opportunity to improve outcomes through therapy-based interventions may be missed. Clinically, the assessment of spontaneous general movements is an important tool, which can be used for the prediction of movement impairments in high risk infants. Movement recognition aims to capture and analyze relevant limb movements through computerized approaches focusing on continuous, objective, and quantitative assessment. Different methods of recording and analyzing infant movements have recently been explored in high risk infants. These range from camera-based solutions to body-worn miniaturized movement sensors used to record continuous time-series data that represent the dynamics of limb movements. Various machine learning methods have been developed and applied to the analysis of the recorded movement data. This analysis has focused on the detection and classification of atypical spontaneous general movements. This article aims to identify recent translational studies using movement recognition technology as a method of assessing movement in high risk infants. The application of this technology within pediatric practice represents a growing area of inter-disciplinary collaboration, which may lead to a greater understanding of the development of the nervous system in infants at high risk of motor impairment."
25608213,51.0,A survey of online activity recognition using mobile phones,2015 Jan 19;15(1):2059-85.,"Physical activity recognition using embedded sensors has enabled many context-aware applications in different areas, such as healthcare. Initially, one or more dedicated wearable sensors were used for such applications. However, recently, many researchers started using mobile phones for this purpose, since these ubiquitous devices are equipped with various sensors, ranging from accelerometers to magnetic field sensors. In most of the current studies, sensor data collected for activity recognition are analyzed offline using machine learning tools. However, there is now a trend towards implementing activity recognition systems on these devices in an online manner, since modern mobile phones have become more powerful in terms of available resources, such as CPU, memory and battery. The research on offline activity recognition has been reviewed in several earlier studies in detail. However, work done on online activity recognition is still in its infancy and is yet to be reviewed. In this paper, we review the studies done so far that implement activity recognition systems on mobile phones and use only their on-board sensors. We discuss various aspects of these studies. Moreover, we discuss their limitations and present various recommendations for future research."
25592319,11.0,High-throughput analysis of behavior for drug discovery,2015 Mar 5;750:82-9.,"Drug testing with traditional behavioral assays constitutes a major bottleneck in the development of novel therapies. PsychoGenics developed three comprehensive high-throughput systems, SmartCube(®), NeuroCube(®) and PhenoCube(®) systems, to increase the efficiency of the drug screening and phenotyping in rodents. These three systems capture different domains of behavior, namely, cognitive, motor, circadian, social, anxiety-like, gait and others, using custom-built computer vision software and machine learning algorithms for analysis. This review exemplifies the use of the three systems and explains how they can advance drug screening with their applications to phenotyping of disease models, drug screening, selection of lead candidates, behavior-driven lead optimization, and drug repurposing."
25577381,13.0,Prediction of miRNA targets,2015;1269:207-29.,"Computational methods for miRNA target prediction are currently undergoing extensive review and evaluation. There is still a great need for improvement of these tools and bioinformatics approaches are looking towards high-throughput experiments in order to validate predictions. The combination of large-scale techniques with computational tools will not only provide greater credence to computational predictions but also lead to the better understanding of specific biological questions. Current miRNA target prediction tools utilize probabilistic learning algorithms, machine learning methods and even empirical biologically defined rules in order to build models based on experimentally verified miRNA targets. Large-scale protein downregulation assays and next-generation sequencing (NGS) are now being used to validate methodologies and compare the performance of existing tools. Tools that exhibit greater correlation between computational predictions and protein downregulation or RNA downregulation are considered the state of the art. Moreover, efficiency in prediction of miRNA targets that are concurrently verified experimentally provides additional validity to computational predictions and further highlights the competitive advantage of specific tools and their efficacy in extracting biologically significant results. In this review paper, we discuss the computational methods for miRNA target prediction and provide a detailed comparison of methodologies and features utilized by each specific tool. Moreover, we provide an overview of current state-of-the-art high-throughput methods used in miRNA target prediction."
25566538,3.0,Toward synthesizing executable models in biology,2014 Dec 19;2:75.,"Over the last decade, executable models of biological behaviors have repeatedly provided new scientific discoveries, uncovered novel insights, and directed new experimental avenues. These models are computer programs whose execution mechanistically simulates aspects of the cell's behaviors. If the observed behavior of the program agrees with the observed biological behavior, then the program explains the phenomena. This approach has proven beneficial for gaining new biological insights and directing new experimental avenues. One advantage of this approach is that techniques for analysis of computer programs can be applied to the analysis of executable models. For example, one can confirm that a model agrees with experiments for all possible executions of the model (corresponding to all environmental conditions), even if there are a huge number of executions. Various formal methods have been adapted for this context, for example, model checking or symbolic analysis of state spaces. To avoid manual construction of executable models, one can apply synthesis, a method to produce programs automatically from high-level specifications. In the context of biological modeling, synthesis would correspond to extracting executable models from experimental data. We survey recent results about the usage of the techniques underlying synthesis of computer programs for the inference of biological models from experimental data. We describe synthesis of biological models from curated mutation experiment data, inferring network connectivity models from phosphoproteomic data, and synthesis of Boolean networks from gene expression data. While much work has been done on automated analysis of similar datasets using machine learning and artificial intelligence, using synthesis techniques provides new opportunities such as efficient computation of disambiguating experiments, as well as the ability to produce different kinds of models automatically from biological data."
25561628,3.0,Identification and computational analysis of gene regulatory elements,2015 Jan 5;2015(1):pdb.top083642.,"Over the last two decades, advances in experimental and computational technologies have greatly facilitated genomic research. Next-generation sequencing technologies have made de novo sequencing of large genomes affordable, and powerful computational approaches have enabled accurate annotations of genomic DNA sequences. Charting functional regions in genomes must account for not only the coding sequences, but also noncoding RNAs, repetitive elements, chromatin states, epigenetic modifications, and gene regulatory elements. A mix of comparative genomics, high-throughput biological experiments, and machine learning approaches has played a major role in this truly global effort. Here we describe some of these approaches and provide an account of our current understanding of the complex landscape of the human genome. We also present overviews of different publicly available, large-scale experimental data sets and computational tools, which we hope will prove beneficial for researchers working with large and complex genomes."
25548928,3.0,Advances in protein contact map prediction based on machine learning,2015;11(3):265-70.,"A protein contact map is a simplified, two-dimensional version of the three-dimensional protein structure. Protein contact map is proved to be crucial in forming the three-dimensional structure. Contact map prediction has now become an indispensable and promising intermediate step towards final three-dimensional structure prediction, while directed sequence-structure prediction hits its bottlenecks. In this article, different evaluation scores of prediction efficiency are compared. Next, the state of the art and future perspectives of contact map methods are reviewed and special attention is paid to those relying on machine learning algorithms. Details of neural network based methods as well as a list of machine learning based methods are given. Finally, bottlenecks and potential improvements of contact map predictions are discussed."
25548927,3.0,Some remarks on prediction of protein-protein interaction with machine learning,2015;11(3):254-64.,"Protein-protein interactions (PPIs) play a key role in many cellular processes. Uncovering the PPIs and their function within the cell is a challenge of post-genomic biology and will improve our understanding of disease and help in the development of novel methods for disease diagnosis and forensics. The experimental methods currently used to identify PPIs are both time-consuming and expensive, and high throughput experimental results have shown both high false positive beside false negative information for protein interaction. These obstacles could be overcome by developing computational approaches to predict PPIs and validate the obtained experimental results. In this work, we will describe the recent advances in predicting protein-protein interaction from the following aspects: i) the benchmark dataset construction, ii) the sequence representation approaches, iii) the common machine learning algorithms, and iv) the cross-validation test methods and assessment metrics."
25462637,704.0,Deep learning in neural networks: an overview,2015 Jan;61:85-117.,"In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks."
25462334,16.0,Three-dimensional protein structure prediction: Methods and computational strategies,2014 Dec;53PB:251-276.,"A long standing problem in structural bioinformatics is to determine the three-dimensional (3-D) structure of a protein when only a sequence of amino acid residues is given. Many computational methodologies and algorithms have been proposed as a solution to the 3-D Protein Structure Prediction (3-D-PSP) problem. These methods can be divided in four main classes: (a) first principle methods without database information; (b) first principle methods with database information; (c) fold recognition and threading methods; and (d) comparative modeling methods and sequence alignment strategies. Deterministic computational techniques, optimization techniques, data mining and machine learning approaches are typically used in the construction of computational solutions for the PSP problem. Our main goal with this work is to review the methods and computational strategies that are currently used in 3-D protein prediction."
25461506,10.0,"Computational and experimental single cell biology techniques for the definition of cell type heterogeneity, interplay and intracellular dynamics",2015 Aug;34:9-15.,"Novel technological developments enable single cell population profiling with respect to their spatial and molecular setup. These include single cell sequencing, flow cytometry and multiparametric imaging approaches and open unprecedented possibilities to learn about the heterogeneity, dynamics and interplay of the different cell types which constitute tissues and multicellular organisms. Statistical and dynamic systems theory approaches have been applied to quantitatively describe a variety of cellular processes, such as transcription and cell signaling. Machine learning approaches have been developed to define cell types, their mutual relationships, and differentiation hierarchies shaping heterogeneous cell populations, yielding insights into topics such as, for example, immune cell differentiation and tumor cell type composition. This combination of experimental and computational advances has opened perspectives towards learning predictive multi-scale models of heterogeneous cell populations."
25458128,27.0,Creating a data exchange strategy for radiotherapy research: towards federated databases and anonymised public datasets,2014 Dec;113(3):303-9.,"Disconnected cancer research data management and lack of information exchange about planned and ongoing research are complicating the utilisation of internationally collected medical information for improving cancer patient care. Rapidly collecting/pooling data can accelerate translational research in radiation therapy and oncology. The exchange of study data is one of the fundamental principles behind data aggregation and data mining. The possibilities of reproducing the original study results, performing further analyses on existing research data to generate new hypotheses or developing computational models to support medical decisions (e.g. risk/benefit analysis of treatment options) represent just a fraction of the potential benefits of medical data-pooling. Distributed machine learning and knowledge exchange from federated databases can be considered as one beyond other attractive approaches for knowledge generation within ""Big Data"". Data interoperability between research institutions should be the major concern behind a wider collaboration. Information captured in electronic patient records (EPRs) and study case report forms (eCRFs), linked together with medical imaging and treatment planning data, are deemed to be fundamental elements for large multi-centre studies in the field of radiation therapy and oncology. To fully utilise the captured medical information, the study data have to be more than just an electronic version of a traditional (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability, utilisation of standards, data quality and privacy concerns, data ownership, rights to publish, data pooling architecture and storage. This paper discusses a framework for conceptual packages of ideas focused on a strategic development for international research data exchange in the field of radiation therapy and oncology."
25455254,19.0,Personalized Coaching Systems to support healthy behavior in people with chronic conditions,2014 Dec;24(6):815-26.,"Chronic conditions cannot be cured but daily behavior has a major effect on the severity of secondary problems and quality of life. Changing behavior however requires intensive support in daily life, which is not feasible with a human coach. A new coaching approach - so-called Personal Coaching Systems (PCSs) - use on-body sensing, combined with smart reasoning and context-aware feedback to support users in developing and maintaining a healthier behavior. Three different PCSs will be used to illustrate the different aspects of this approach: (1) Treatment of neck/shoulder pain. EMG patterns of the Trapezius muscles are used to estimate their level of relaxation. Personal vibrotactile feedback is given, to create awareness and enable learning when muscles are insufficiently relaxed. (2) Promoting a healthy activity pattern. Using a 3D accelerometer to measure activity and a smartphone to provide feedback. Timing and content of the feedback are adapted real-time, using machine-learning techniques, to optimize adherence. (3) Management of stress during daily living. The level of stress is quantified using a personal model involving a combination of different sensor signals (EMG, ECG, skin conductance, respiration). Results show that Personal Coaching Systems are feasible and a promising and challenging way forward to coach people with chronic conditions."
25448759,83.0,Machine-learning approaches in drug discovery: methods and applications,2015 Mar;20(3):318-31.,"During the past decade, virtual screening (VS) has evolved from traditional similarity searching, which utilizes single reference compounds, into an advanced application domain for data mining and machine-learning approaches, which require large and representative training-set compounds to learn robust decision rules. The explosive growth in the amount of public domain-available chemical and biological data has generated huge effort to design, analyze, and apply novel learning methodologies. Here, I focus on machine-learning techniques within the context of ligand-based VS (LBVS). In addition, I analyze several relevant VS studies from recent publications, providing a detailed view of the current state-of-the-art in this field and highlighting not only the problematic issues, but also the successes and opportunities for further advances."
25448299,6.0,Text as data: using text-based features for proteins representation and for computational prediction of their characteristics,2015 Mar;74:54-64.,"The current era of large-scale biology is characterized by a fast-paced growth in the number of sequenced genomes and, consequently, by a multitude of identified proteins whose function has yet to be determined. Simultaneously, any known or postulated information concerning genes and proteins is part of the ever-growing published scientific literature, which is expanding at a rate of over a million new publications per year. Computational tools that attempt to automatically predict and annotate protein characteristics, such as function and localization patterns, are being developed along with systems that aim to support the process via text mining. Most work on protein characterization focuses on features derived directly from protein sequence data. Protein-related work that does aim to utilize the literature typically concentrates on extracting specific facts (e.g., protein interactions) from text. In the past few years we have taken a different route, treating the literature as a source of text-based features, which can be employed just as sequence-based protein-features were used in earlier work, for predicting protein subcellular location and possibly also function. We discuss here in detail the overall approach, along with results from work we have done in this area demonstrating the value of this method and its potential use."
25440524,24.0,Applying machine learning techniques for ADME-Tox prediction: a review,2015 Feb;11(2):259-71.,"Introduction:                    Pharmacokinetics involves the study of absorption, distribution, metabolism, excretion and toxicity of xenobiotics (ADME-Tox). In this sense, the ADME-Tox profile of a bioactive compound can impact its efficacy and safety. Moreover, efficacy and safety were considered some of the major causes of clinical failures in the development of new chemical entities. In this context, machine learning (ML) techniques have been often used in ADME-Tox studies due to the existence of compounds with known pharmacokinetic properties available for generating predictive models.              Areas covered:                    This review examines the growth in the use of some ML techniques in ADME-Tox studies, in particular supervised and unsupervised techniques. Also, some critical points (e.g., size of the data set and type of output variable) must be considered during the generation of models that relate ADME-Tox properties and biological activity.              Expert opinion:                    ML techniques have been successfully employed in pharmacokinetic studies, helping the complex process of designing new drug candidates from the use of reliable ML models. An application of this procedure would be the prediction of ADME-Tox properties from studies of quantitative structure-activity relationships or the discovery of new compounds from a virtual screening using filters based on results obtained from ML techniques."
25435482,6.0,Using what you get: dynamic physiologic signatures of critical illness,2015 Jan;31(1):133-64.,"The development and resolution of cardiopulmonary instability take time to become clinically apparent, and the treatments provided take time to have an impact. The characterization of dynamic changes in hemodynamic and metabolic variables is implicit in physiologic signatures. When primary variables are collected with high enough frequency to derive new variables, this data hierarchy can be used to develop physiologic signatures. The creation of physiologic signatures requires no new information; additional knowledge is extracted from data that already exist. It is possible to create physiologic signatures for each stage in the process of clinical decompensation and recovery to improve outcomes."
27485418,12.0,Benchmarking a Wide Range of Chemical Descriptors for Drug-Target Interaction Prediction Using a Chemogenomic Approach,2014 Dec;33(11-12):719-31.,"The identification of drug-target interactions, or interactions between drug candidate compounds and target candidate proteins, is a crucial process in genomic drug discovery. In silico chemogenomic methods are recently recognized as a promising approach for genome-wide scale prediction of drug-target interactions, but the prediction performance depends heavily on the descriptors and similarity measures of drugs and proteins. In this paper, we investigated the performance of various descriptors and similarity measures of drugs and proteins for the drug-target interaction prediction using a chemogenomic approach. We compared the prediction accuracy of 18 chemical descriptors of drugs (e.g., ECFP, FCFP,E-state, CDK, KlekotaRoth, MACCS, PubChem, Dragon, KCF-S, and graph kernels) and 4 descriptors of proteins (e.g., amino acid composition, domain profile, local sequence similarity, and string kernel) on about one hundred thousand drug-target interactions. We examined the combinatorial effects of drug descriptors and protein descriptors using the same benchmark data under several experimental conditions. Large-scale experiments showed that our proposed KCF-S descriptor worked the best in terms of prediction accuracy. The comparative results are expected to be useful for selecting chemical descriptors in various pharmaceutical applications."
25428267,33.0,Acoustic sequences in non-human animals: a tutorial review and prospectus,2016 Feb;91(1):13-52.,"Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise - let alone understand - the significance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be defined and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the fields of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, 'Analysing vocal sequences in animals'. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this field, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different fields, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality."
25423479,22.0,Identifying predictive features in drug response using machine learning: opportunities and challenges,2015;55:15-34.,"This article reviews several techniques from machine learning that can be used to study the problem of identifying a small number of features, from among tens of thousands of measured features, that can accurately predict a drug response. Prediction problems are divided into two categories: sparse classification and sparse regression. In classification, the clinical parameter to be predicted is binary, whereas in regression, the parameter is a real number. Well-known methods for both classes of problems are briefly discussed. These include the SVM (support vector machine) for classification and various algorithms such as ridge regression, LASSO (least absolute shrinkage and selection operator), and EN (elastic net) for regression. In addition, several well-established methods that do not directly fall into machine learning theory are also reviewed, including neural networks, PAM (pattern analysis for microarrays), SAM (significance analysis for microarrays), GSEA (gene set enrichment analysis), and k-means clustering. Several references indicative of the application of these methods to cancer biology are discussed."
25405022,20.0,How machine learning is shaping cognitive neuroimaging,2014 Nov 17;3:28.,"Functional brain images are rich and noisy data that can capture indirect signatures of neural activity underlying cognition in a given experimental setting. Can data mining leverage them to build models of cognition? Only if it is applied to well-posed questions, crafted to reveal cognitive mechanisms. Here we review how predictive models have been used on neuroimaging data to ask new questions, i.e., to uncover new aspects of cognitive organization. We also give a statistical learning perspective on these progresses and on the remaining gaping holes."
25392685,14.0,Text mining in cancer gene and pathway prioritization,2014 Oct 13;13(Suppl 1):69-79.,"Prioritization of cancer implicated genes has received growing attention as an effective way to reduce wet lab cost by computational analysis that ranks candidate genes according to the likelihood that experimental verifications will succeed. A multitude of gene prioritization tools have been developed, each integrating different data sources covering gene sequences, differential expressions, function annotations, gene regulations, protein domains, protein interactions, and pathways. This review places existing gene prioritization tools against the backdrop of an integrative Omic hierarchy view toward cancer and focuses on the analysis of their text mining components. We explain the relatively slow progress of text mining in gene prioritization, identify several challenges to current text mining methods, and highlight a few directions where more effective text mining algorithms may improve the overall prioritization task and where prioritizing the pathways may be more desirable than prioritizing only genes."
25392680,1.0,Optimization of Network Topology in Computer-Aided Detection Schemes Using Phased Searching with NEAT in a Time-Scaled Framework,2014 Oct 13;13(Suppl 1):17-27.,"In the field of computer-aided mammographic mass detection, many different features and classifiers have been tested. Frequently, the relevant features and optimal topology for the artificial neural network (ANN)-based approaches at the classification stage are unknown, and thus determined by trial-and-error experiments. In this study, we analyzed a classifier that evolves ANNs using genetic algorithms (GAs), which combines feature selection with the learning task. The classifier named ""Phased Searching with NEAT in a Time-Scaled Framework"" was analyzed using a dataset with 800 malignant and 800 normal tissue regions in a 10-fold cross-validation framework. The classification performance measured by the area under a receiver operating characteristic (ROC) curve was 0.856 ± 0.029. The result was also compared with four other well-established classifiers that include fixed-topology ANNs, support vector machines (SVMs), linear discriminant analysis (LDA), and bagged decision trees. The results show that Phased Searching outperformed the LDA and bagged decision tree classifiers, and was only significantly outperformed by SVM. Furthermore, the Phased Searching method required fewer features and discarded superfluous structure or topology, thus incurring a lower feature computational and training and validation time requirement. Analyses performed on the network complexities evolved by Phased Searching indicate that it can evolve optimal network topologies based on its complexification and simplification parameter selection process. From the results, the study also concluded that the three classifiers - SVM, fixed-topology ANN, and Phased Searching with NeuroEvolution of Augmenting Topologies (NEAT) in a Time-Scaled Framework - are performing comparably well in our mammographic mass detection scheme."
25360145,51.0,Kernel-based whole-genome prediction of complex traits: a review,2014 Oct 16;5:363.,"Prediction of genetic values has been a focus of applied quantitative genetics since the beginning of the 20th century, with renewed interest following the advent of the era of whole genome-enabled prediction. Opportunities offered by the emergence of high-dimensional genomic data fueled by post-Sanger sequencing technologies, especially molecular markers, have driven researchers to extend Ronald Fisher and Sewall Wright's models to confront new challenges. In particular, kernel methods are gaining consideration as a regression method of choice for genome-enabled prediction. Complex traits are presumably influenced by many genomic regions working in concert with others (clearly so when considering pathways), thus generating interactions. Motivated by this view, a growing number of statistical approaches based on kernels attempt to capture non-additive effects, either parametrically or non-parametrically. This review centers on whole-genome regression using kernel methods applied to a wide range of quantitative traits of agricultural importance in animals and plants. We discuss various kernel-based approaches tailored to capturing total genetic variation, with the aim of arriving at an enhanced predictive performance in the light of available genome annotation information. Connections between prediction machines born in animal breeding, statistics, and machine learning are revisited, and their empirical prediction performance is discussed. Overall, while some encouraging results have been obtained with non-parametric kernels, recovering non-additive genetic variation in a validation dataset remains a challenge in quantitative genetics."
25360109,15.0,Bayesian networks in neuroscience: a survey,2014 Oct 16;8:131.,"Bayesian networks are a type of probabilistic graphical models lie at the intersection between statistics and machine learning. They have been shown to be powerful tools to encode dependence relationships among the variables of a domain under uncertainty. Thanks to their generality, Bayesian networks can accommodate continuous and discrete variables, as well as temporal processes. In this paper we review Bayesian networks and how they can be learned automatically from data by means of structure learning algorithms. Also, we examine how a user can take advantage of these networks for reasoning by exact or approximate inference algorithms that propagate the given evidence through the graphical structure. Despite their applicability in many fields, they have been little used in neuroscience, where they have focused on specific problems, like functional connectivity analysis from neuroimaging data. Here we survey key research in neuroscience where Bayesian networks have been used with different aims: discover associations between variables, perform probabilistic reasoning over the model, and classify new observations with and without supervision. The networks are learned from data of any kind-morphological, electrophysiological, -omics and neuroimaging-, thereby broadening the scope-molecular, cellular, structural, functional, cognitive and medical- of the brain aspects to be studied."
25320644,1.0,Towards predictive resistance models for agrochemicals by combining chemical and protein similarity via proteochemometric modelling,2014 May 15;7(4):119-23.,"Resistance to pesticides is an increasing problem in agriculture. Despite practices such as phased use and cycling of 'orthogonally resistant' agents, resistance remains a major risk to national and global food security. To combat this problem, there is a need for both new approaches for pesticide design, as well as for novel chemical entities themselves. As summarized in this opinion article, a technique termed 'proteochemometric modelling' (PCM), from the field of chemoinformatics, could aid in the quantification and prediction of resistance that acts via point mutations in the target proteins of an agent. The technique combines information from both the chemical and biological domain to generate bioactivity models across large numbers of ligands as well as protein targets. PCM has previously been validated in prospective, experimental work in the medicinal chemistry area, and it draws on the growing amount of bioactivity information available in the public domain. Here, two potential applications of proteochemometric modelling to agrochemical data are described, based on previously published examples from the medicinal chemistry literature."
25307115,8.0,Research review: Functional brain connectivity and child psychopathology--overview and methodological considerations for investigators new to the field,2015 Apr;56(4):400-14.,"Background:                    Functional connectivity MRI is an emerging technique that can be used to investigate typical and atypical brain function in developing and aging populations. Despite some of the current confounds in the field of functional connectivity MRI, the translational potential of the technique available to investigators may eventually be used to improve diagnosis, early disease detection, and therapy monitoring.              Method and scope:                    Based on a comprehensive survey of the literature, this review offers an introduction of resting-state functional connectivity for new investigators to the field of resting-state functional connectivity. We discuss a brief history of the technique, various methods of analysis, the relationship of functional networks to behavior, as well as the translational potential of functional connectivity MRI to investigate neuropsychiatric disorders. We also address some considerations and limitations with data analysis and interpretation.              Conclusions:                    The information provided in this review should serve as a foundation for investigators new to the field of resting-state functional connectivity. The discussion provides a means to better understand functional connectivity and its application to typical and atypical brain function."
25285755,34.0,Integration of biological parts toward the synthesis of a minimal cell,2014 Oct;22:85-91.,"Various approaches are taken to construct synthetic cells in the laboratory, a challenging goal that became experimentally imaginable over the past two decades. The construction of protocells, which explores scenarios of the origin of life, has been the original motivations for such projects. With the advent of the synthetic biology era, bottom-up engineering approaches to synthetic cells are now conceivable. The modular design emerges as the most robust framework to construct a minimal cell from natural molecular components. Although significant advances have been made for each piece making this complex puzzle, the integration of the three fundamental parts, information-metabolism-self-organization, into cell-sized liposomes capable of sustained reproduction has failed so far. Our inability to connect these three elements is also a major limitation in this research area. New methods, such as machine learning coupled to high-throughput techniques, should be exploited to accelerate the cell-free synthesis of complex biochemical systems."
25234433,30.0,Single nucleotide variations: biological impact and theoretical interpretation,2014 Dec;23(12):1650-66.,"Genome-wide association studies (GWAS) and whole-exome sequencing (WES) generate massive amounts of genomic variant information, and a major challenge is to identify which variations drive disease or contribute to phenotypic traits. Because the majority of known disease-causing mutations are exonic non-synonymous single nucleotide variations (nsSNVs), most studies focus on whether these nsSNVs affect protein function. Computational studies show that the impact of nsSNVs on protein function reflects sequence homology and structural information and predict the impact through statistical methods, machine learning techniques, or models of protein evolution. Here, we review impact prediction methods and discuss their underlying principles, their advantages and limitations, and how they compare to and complement one another. Finally, we present current applications and future directions for these methods in biological research and medical genetics."
25223304,22.0,Machine learning for Big Data analytics in plants,2014 Dec;19(12):798-808.,"Rapid advances in high-throughput genomic technology have enabled biology to enter the era of 'Big Data' (large datasets). The plant science community not only needs to build its own Big-Data-compatible parallel computing and data management infrastructures, but also to seek novel analytical paradigms to extract information from the overwhelming amounts of data. Machine learning offers promising computational and analytical solutions for the integrative analysis of large, heterogeneous and unstructured datasets on the Big-Data scale, and is gradually gaining popularity in biology. This review introduces the basic concepts and procedures of machine-learning applications and envisages how machine learning could interface with Big Data technology to facilitate basic research and biotechnology in the plant sciences."
25201114,1.0,Splicing code modeling,2014;825:451-66.,"How do cis and trans elements involved in pre-mRNA splicing come together to form a splicing ""code""? This question has been a driver of much of the research involving RNA biogenesis. The variability of splicing outcome across developmental stages and between tissues coupled with association of splicing defects with numerous diseases highlights the importance of such a code. However, the sheer number of elements involved in splicing regulation and the context-specific manner of their operation have made the derivation of such a code challenging. Recently, machine learning-based methods have been developed to infer computational models for a splicing code. These methods use high-throughput experiments measuring mRNA expression at exonic resolution and binding locations of RNA-binding proteins (RBPs) to infer what the regulatory elements that control the inclusion of a given pre-mRNA segment are. The inferred regulatory models can then be applied to genomic sequences or experimental conditions that have not been measured to predict splicing outcome. Moreover, the models themselves can be interrogated to identify new regulatory mechanisms, which can be subsequently tested experimentally. In this chapter, we survey the current state of this technology, and illustrate how it can be applied by non-computational or RNA splicing experts to study regulation of specific exons by using the AVISPA web tool."
25199597,35.0,Discovering new indicators of fecal pollution,2014 Dec;22(12):697-706.,"Fecal pollution indicators are essential to identify and remediate contamination sources and protect public health. Historically, easily cultured facultative anaerobes such as fecal coliforms, Escherichia coli, or enterococci have been used but these indicators generally provide no information as to their source. More recently, molecular methods have targeted fecal anaerobes, which are much more abundant in humans and other mammals, and some strains appear to be associated with particular host sources. Next-generation sequencing and microbiome studies have created an unprecedented inventory of microbial communities associated with fecal sources, allowing reexamination of which taxonomic groups are best suited as informative indicators. The use of new computational methods, such as oligotyping coupled with well-established machine learning approaches, is providing new insights into patterns of host association. In this review we examine the basis for host-specificity and the rationale for using 16S rRNA gene targets for alternative indicators and highlight two taxonomic groups, Bacteroidales and Lachnospiraceae, which are rich in host-specific bacterial organisms. Finally, we discuss considerations for using alternative indicators for water quality assessments with a particular focus on detecting human sewage sources of contamination."
25187715,23.0,Diagnostic and therapeutic utility of neuroimaging in depression: an overview,2014 Aug 19;10:1509-22.,"A growing number of studies have used neuroimaging to further our understanding of how brain structure and function are altered in major depression. More recently, these techniques have begun to show promise for the diagnosis and treatment of depression, both as aids to conventional methods and as methods in their own right. In this review, we describe recent neuroimaging findings in the field that might aid diagnosis and improve treatment accuracy. Overall, major depression is associated with numerous structural and functional differences in neural systems involved in emotion processing and mood regulation. Furthermore, several studies have shown that the structure and function of these systems is changed by pharmacological and psychological treatments of the condition and that these changes in candidate brain regions might predict clinical response. More recently, ""machine learning"" methods have used neuroimaging data to categorize individual patients according to their diagnostic status and predict treatment response. Despite being mostly limited to group-level comparisons at present, with the introduction of new methods and more naturalistic studies, neuroimaging has the potential to become part of the clinical armamentarium and may improve diagnostic accuracy and inform treatment choice at the patient level."
25183786,20.0,Using high-throughput transcriptomic data for prognosis: a critical overview and perspectives,2014 Sep 1;74(17):4612-21.,"Accurate prognosis and prediction of response to therapy are essential for personalized treatment of cancer. Even though many prognostic gene lists and predictors have been proposed, especially for breast cancer, high-throughput ""omic"" methods have so far not revolutionized clinical practice, and their clinical utility has not been satisfactorily established. Different prognostic gene lists have very few shared genes, the biological meaning of most signatures is unclear, and the published success rates are considered to be overoptimistic. This review examines critically the manner in which prognostic classifiers are derived using machine-learning methods and suggests reasons for the shortcomings and problems listed above. Two approaches that may hold hope for obtaining improved prognosis are presented. Both are based on using existing prior knowledge; one proposes combining molecular ""omic"" predictors with established clinical ones, and the second infers biologically relevant pathway deregulation scores for each tumor from expression data, and uses this representation to study and stratify individual tumors. Approaches such as the second one are referred to in the physics literature as ""phenomenology""; they will, hopefully, play a significant role in future studies of cancer. See all articles in this Cancer Research section, ""Physics in Cancer Research."""
25131220,15.0,Multi-omics analysis of inflammatory bowel disease,2014 Dec;162(2 Pt A):62-8.,"Crohn's disease and ulcerative colitis, known together as inflammatory bowel disease (IBD), are severe autoimmune disorders now causing gut inflammation and ulceration, among other symptoms, in up to 1 in 250 people worldwide. Incidence and prevalence of IBD have been increasing dramatically over the past several decades, although the causes for this increase are still unknown. IBD has both a complex genotype and a complex phenotype, and although it has received substantial attention from the medical research community over recent years, much of the etiology remains unexplained. Genome-wide association studies have identified a rich genetic signature of disease risk in patients with IBD, consisting of at least 163 genetic loci. Many of these loci contain genes directly involved in microbial handling, indicating that the genetic architecture of the disease has been driven by host-microbe interactions. In addition, systematic shifts in gut microbiome structure (enterotype) and function have been observed in patients with IBD. Furthermore, both the host genotype and enterotype are associated with aspects of the disease phenotype, including location of the disease. This provides strong evidence of interactions between host genotype and enterotype; however, there is a lack of published multi-omics data from IBD patients, and a lack of bioinformatics tools for modeling such systems. In this article we discuss, from a computational biologist's point of view, the potential benefits of and the challenges involved in designing and analyzing such multi-omics studies of IBD."
25123745,5.0,Managing large-scale genomic datasets and translation into clinical practice,2014 Aug 15;9(1):212-4.,"Objective:                    To summarize excellent current research in the field of Bioinformatics and Translational Informatics with application in the health domain.              Method:                    We provide a synopsis of the articles selected for the IMIA Yearbook 2014, from which we attempt to derive a synthetic overview of current and future activities in the field. A first step of selection was performed by querying MEDLINE with a list of MeSH descriptors completed by a list of terms adapted to the section. Each section editor evaluated independently the set of 1,851 articles and 15 articles were retained for peer-review.              Results:                    The selection and evaluation process of this Yearbook's section on Bioinformatics and Translational Informatics yielded three excellent articles regarding data management and genome medicine. In the first article, the authors present VEST (Variant Effect Scoring Tool) which is a supervised machine learning tool for prioritizing variants found in exome sequencing projects that are more likely involved in human Mendelian diseases. In the second article, the authors show how to infer surnames of male individuals by crossing anonymous publicly available genomic data from the Y chromosome and public genealogy data banks. The third article presents a statistical framework called iCluster+ that can perform pattern discovery in integrated cancer genomic data. This framework was able to determine different tumor subtypes in colon cancer.              Conclusions:                    The current research activities still attest the continuous convergence of Bioinformatics and Medical Informatics, with a focus this year on large-scale biological, genomic, and Electronic Health Records data. Indeed, there is a need for powerful tools for managing and interpreting complex data, but also a need for user-friendly tools developed for the clinicians in their daily practice. All the recent research and development efforts are contributing to the challenge of impacting clinically the results and even going towards a personalized medicine in the near future."
25112457,16.0,Drug-target interaction prediction via chemogenomic space: learning-based methods,2014 Sep;10(9):1273-87.,"Introduction:                    Identification of the interaction between drugs and target proteins is a crucial task in genomic drug discovery. The in silico prediction is an appropriate alternative for the laborious and costly experimental process of drug-target interaction prediction. Developing a variety of computational methods opens a new direction in analyzing and detecting new drug-target pairs.              Areas covered:                    In this review, we will focus on chemogenomic methods which have established a learning framework for predicting drug-target interactions. Learning-based methods are classified into supervised and semi-supervised, and the supervised learning methods are studied as two separate parts including similarity-based methods and feature-based methods.              Expert opinion:                    In spite of many improvements for pharmacology applications by learning-based methods, there are many over simplification settings in construction of predictive models that may lead to over-optimistic results on drug-target interaction prediction."
25112429,8.0,Extending statistical boosting. An overview of recent methodological developments,2014;53(6):428-35.,"Background:                    Boosting algorithms to simultaneously estimate and select predictor effects in statistical models have gained substantial interest during the last decade.              Objectives:                    This review highlights recent methodological developments regarding boosting algorithms for statistical modelling especially focusing on topics relevant for biomedical research.              Methods:                    We suggest a unified framework for gradient boosting and likelihood-based boosting (statistical boosting) which have been addressed separately in the literature up to now.              Results:                    The methodological developments on statistical boosting during the last ten years can be grouped into three different lines of research: i) efforts to ensure variable selection leading to sparser models, ii) developments regarding different types of predictor effects and how to choose them, iii) approaches to extend the statistical boosting framework to new regression settings.              Conclusions:                    Statistical boosting algorithms have been adapted to carry out unbiased variable selection and automated model choice during the fitting process and can nowadays be applied in almost any regression setting in combination with a large amount of different types of predictor effects."
25106933,15.0,A systematic review of predictive modeling for bronchiolitis,2014 Oct;83(10):691-714.,"Purpose:                    Bronchiolitis is the most common cause of illness leading to hospitalization in young children. At present, many bronchiolitis management decisions are made subjectively, leading to significant practice variation among hospitals and physicians caring for children with bronchiolitis. To standardize care for bronchiolitis, researchers have proposed various models to predict the disease course to help determine a proper management plan. This paper reviews the existing state of the art of predictive modeling for bronchiolitis. Predictive modeling for respiratory syncytial virus (RSV) infection is covered whenever appropriate, as RSV accounts for about 70% of bronchiolitis cases.              Methods:                    A systematic review was conducted through a PubMed search up to April 25, 2014. The literature on predictive modeling for bronchiolitis was retrieved using a comprehensive search query, which was developed through an iterative process. Search results were limited to human subjects, the English language, and children (birth to 18 years).              Results:                    The literature search returned 2312 references in total. After manual review, 168 of these references were determined to be relevant and are discussed in this paper. We identify several limitations and open problems in predictive modeling for bronchiolitis, and provide some preliminary thoughts on how to address them, with the hope to stimulate future research in this domain.              Conclusions:                    Many problems remain open in predictive modeling for bronchiolitis. Future studies will need to address them to achieve optimal predictive models."
25092428,5.0,How can neuroimaging facilitate the diagnosis and stratification of patients with psychosis?,2015 May;25(5):725-32.,"Early diagnosis and treatment of patients with psychosis are associated with improved outcome in terms of future functioning, symptoms and treatment response. Identifying neuroimaging biomarkers for illness onset and treatment response would lead to immediate clinical benefits. In this review we discuss if neuroimaging may be utilised to diagnose patients with psychosis, predict those who will develop the illness in those at high risk, and stratify patients. State-of-the-art developments in the field are critically examined including multicentre studies, longitudinal designs, multimodal imaging and machine learning as well as some of the challenges in utilising future neuroimaging biomarkers in clinical trials. As many of these developments are already being applied in neuroimaging studies of Alzheimer's disease, we discuss what lessons have been learned from this field and how they may be applied to research in psychosis."
25083876,165.0,H3K4me3 breadth is linked to cell identity and transcriptional consistency,2014 Jul 31;158(3):673-88.,"Trimethylation of histone H3 at lysine 4 (H3K4me3) is a chromatin modification known to mark the transcription start sites of active genes. Here, we show that H3K4me3 domains that spread more broadly over genes in a given cell type preferentially mark genes that are essential for the identity and function of that cell type. Using the broadest H3K4me3 domains as a discovery tool in neural progenitor cells, we identify novel regulators of these cells. Machine learning models reveal that the broadest H3K4me3 domains represent a distinct entity, characterized by increased marks of elongation. The broadest H3K4me3 domains also have more paused polymerase at their promoters, suggesting a unique transcriptional output. Indeed, genes marked by the broadest H3K4me3 domains exhibit enhanced transcriptional consistency and [corrected] increased transcriptional levels, and perturbation of H3K4me3 breadth leads to changes in transcriptional consistency. Thus, H3K4me3 breadth contains information that could ensure transcriptional precision at key cell identity/function genes."
25053743,8.0,Kernel methods for large-scale genomic data analysis,2015 Mar;16(2):183-92.,"Machine learning, particularly kernel methods, has been demonstrated as a promising new tool to tackle the challenges imposed by today's explosive data growth in genomics. They provide a practical and principled approach to learning how a large number of genetic variants are associated with complex phenotypes, to help reveal the complexity in the relationship between the genetic markers and the outcome of interest. In this review, we highlight the potential key role it will have in modern genomic data processing, especially with regard to integration with classical methods for gene prioritizing, prediction and data fusion."
25048127,9.0,Machine learning-based methods for prediction of linear B-cell epitopes,2014;1184:217-36.,"B-cell epitope prediction facilitates immunologists in designing peptide-based vaccine, diagnostic test, disease prevention, treatment, and antibody production. In comparison with T-cell epitope prediction, the performance of variable length B-cell epitope prediction is still yet to be satisfied. Fortunately, due to increasingly available verified epitope databases, bioinformaticians could adopt machine learning-based algorithms on all curated data to design an improved prediction tool for biomedical researchers. Here, we have reviewed related epitope prediction papers, especially those for linear B-cell epitope prediction. It should be noticed that a combination of selected propensity scales and statistics of epitope residues with machine learning-based tools formulated a general way for constructing linear B-cell epitope prediction systems. It is also observed from most of the comparison results that the kernel method of support vector machine (SVM) classifier outperformed other machine learning-based approaches. Hence, in this chapter, except reviewing recently published papers, we have introduced the fundamentals of B-cell epitope and SVM techniques. In addition, an example of linear B-cell prediction system based on physicochemical features and amino acid combinations is illustrated in details."
25046016,30.0,Automatic fall monitoring: a review,2014 Jul 18;14(7):12900-36.,"Falls and fall-related injuries are major incidents, especially for elderly people, which often mark the onset of major deterioration of health. More than one-third of home-dwelling people aged 65 or above and two-thirds of those in residential care fall once or more each year. Reliable fall detection, as well as prevention, is an important research topic for monitoring elderly living alone in residential or hospital units. The aim of this study is to review the existing fall detection systems and some of the key research challenges faced by the research community in this field. We categorize the existing platforms into two groups: wearable and ambient devices; the classification methods are divided into rule-based and machine learning techniques. The relative merit and potential drawbacks are discussed, and we also outline some of the outstanding research challenges that emerging new platforms need to address."
25029489,,Kernel association for classification and prediction: a survey,2015 Feb;26(2):208-23.,"Kernel association (KA) in statistical pattern recognition used for classification and prediction have recently emerged in a machine learning and signal processing context. This survey outlines the latest trends and innovations of a kernel framework for big data analysis. KA topics include offline learning, distributed database, online learning, and its prediction. The structural presentation and the comprehensive list of references are geared to provide a useful overview of this evolving field for both specialists and relevant scholars."
25016293,28.0,Text summarization in the biomedical domain: a systematic review of recent research,2014 Dec;52:457-67.,"Objective:                    The amount of information for clinicians and clinical researchers is growing exponentially. Text summarization reduces information as an attempt to enable users to find and understand relevant source texts more quickly and effortlessly. In recent years, substantial research has been conducted to develop and evaluate various summarization techniques in the biomedical domain. The goal of this study was to systematically review recent published research on summarization of textual documents in the biomedical domain.              Materials and methods:                    MEDLINE (2000 to October 2013), IEEE Digital Library, and the ACM digital library were searched. Investigators independently screened and abstracted studies that examined text summarization techniques in the biomedical domain. Information is derived from selected articles on five dimensions: input, purpose, output, method and evaluation.              Results:                    Of 10,786 studies retrieved, 34 (0.3%) met the inclusion criteria. Natural language processing (17; 50%) and a hybrid technique comprising of statistical, Natural language processing and machine learning (15; 44%) were the most common summarization approaches. Most studies (28; 82%) conducted an intrinsic evaluation.              Discussion:                    This is the first systematic review of text summarization in the biomedical domain. The study identified research gaps and provides recommendations for guiding future research on biomedical text summarization.              Conclusion:                    Recent research has focused on a hybrid technique comprising statistical, language processing and machine learning techniques. Further research is needed on the application and evaluation of text summarization in real research or patient care settings."
25008538,1.0,A framework for optimal kernel-based manifold embedding of medical image data,2015 Apr;41:93-107.,"Kernel-based dimensionality reduction is a widely used technique in medical image analysis. To fully unravel the underlying nonlinear manifold the selection of an adequate kernel function and of its free parameters is critical. In practice, however, the kernel function is generally chosen as Gaussian or polynomial and such standard kernels might not always be optimal for a given image dataset or application. In this paper, we present a study on the effect of the kernel functions in nonlinear manifold embedding of medical image data. To this end, we first carry out a literature review on existing advanced kernels developed in the statistics, machine learning, and signal processing communities. In addition, we implement kernel-based formulations of well-known nonlinear dimensional reduction techniques such as Isomap and Locally Linear Embedding, thus obtaining a unified framework for manifold embedding using kernels. Subsequently, we present a method to automatically choose a kernel function and its associated parameters from a pool of kernel candidates, with the aim to generate the most optimal manifold embeddings. Furthermore, we show how the calculated selection measures can be extended to take into account the spatial relationships in images, or used to combine several kernels to further improve the embedding results. Experiments are then carried out on various synthetic and phantom datasets for numerical assessment of the methods. Furthermore, the workflow is applied to real data that include brain manifolds and multispectral images to demonstrate the importance of the kernel selection in the analysis of high-dimensional medical images."
25008281,47.0,Text mining of cancer-related information: review of current status and future directions,2014 Sep;83(9):605-23.,"Purpose:                    This paper reviews the research literature on text mining (TM) with the aim to find out (1) which cancer domains have been the subject of TM efforts, (2) which knowledge resources can support TM of cancer-related information and (3) to what extent systems that rely on knowledge and computational methods can convert text data into useful clinical information. These questions were used to determine the current state of the art in this particular strand of TM and suggest future directions in TM development to support cancer research.              Methods:                    A review of the research on TM of cancer-related information was carried out. A literature search was conducted on the Medline database as well as IEEE Xplore and ACM digital libraries to address the interdisciplinary nature of such research. The search results were supplemented with the literature identified through Google Scholar.              Results:                    A range of studies have proven the feasibility of TM for extracting structured information from clinical narratives such as those found in pathology or radiology reports. In this article, we provide a critical overview of the current state of the art for TM related to cancer. The review highlighted a strong bias towards symbolic methods, e.g. named entity recognition (NER) based on dictionary lookup and information extraction (IE) relying on pattern matching. The F-measure of NER ranges between 80% and 90%, while that of IE for simple tasks is in the high 90s. To further improve the performance, TM approaches need to deal effectively with idiosyncrasies of the clinical sublanguage such as non-standard abbreviations as well as a high degree of spelling and grammatical errors. This requires a shift from rule-based methods to machine learning following the success of similar trends in biological applications of TM. Machine learning approaches require large training datasets, but clinical narratives are not readily available for TM research due to privacy and confidentiality concerns. This issue remains the main bottleneck for progress in this area. In addition, there is a need for a comprehensive cancer ontology that would enable semantic representation of textual information found in narrative reports."
25002826,4.0,Machine learning methods in the computational biology of cancer,2014 Jul 8;470(2167):20140081.,"The objectives of this Perspective paper are to review some recent advances in sparse feature selection for regression and classification, as well as compressed sensing, and to discuss how these might be used to develop tools to advance personalized cancer therapy. As an illustration of the possibilities, a new algorithm for sparse regression is presented and is applied to predict the time to tumour recurrence in ovarian cancer. A new algorithm for sparse feature selection in classification problems is presented, and its validation in endometrial cancer is briefly discussed. Some open problems are also presented."
24998888,55.0,"Machine learning, medical diagnosis, and biomedical engineering research - commentary",2014 Jul 5;13:94.,"A large number of papers are appearing in the biomedical engineering literature that describe the use of machine learning techniques to develop classifiers for detection or diagnosis of disease. However, the usefulness of this approach in developing clinically validated diagnostic techniques so far has been limited and the methods are prone to overfitting and other problems which may not be immediately apparent to the investigators. This commentary is intended to help sensitize investigators as well as readers and reviewers of papers to some potential pitfalls in the development of classifiers, and suggests steps that researchers can take to help avoid these problems. Building classifiers should be viewed not simply as an add-on statistical analysis, but as part and parcel of the experimental process. Validation of classifiers for diagnostic applications should be considered as part of a much larger process of establishing the clinical validity of the diagnostic technique."
24996618,22.0,Host genetic factors predisposing to HIV-associated neurocognitive disorder,2014 Sep;11(3):336-52.,"The success of combination antiretroviral therapy (cART) in transforming the lives of HIV-infected individuals with access to these drugs is tempered by the increasing threat of HIV-associated neurocognitive disorders (HAND) to their overall health and quality of life. Intensive investigations over the past two decades have underscored the role of host immune responses, inflammation, and monocyte-derived macrophages in HAND, but the precise pathogenic mechanisms underlying HAND remain only partially delineated. Complicating research efforts and therapeutic drug development are the sheer complexity of HAND phenotypes, diagnostic imprecision, and the growing intersection of chronic immune activation with aging-related comorbidities. Yet, genetic studies still offer a powerful means of advancing individualized care for HIV-infected individuals at risk. There is an urgent need for 1) longitudinal studies using consistent phenotypic definitions of HAND in HIV-infected subpopulations at very high risk of being adversely impacted, such as children, 2) tissue studies that correlate neuropathological changes in multiple brain regions with genomic markers in affected individuals and with changes at the RNA, epigenomic, and/or protein levels, and 3) genetic association studies using more sensitive subphenotypes of HAND. The NIH Brain Initiative and Human Connectome Project, coupled with rapidly evolving systems biology and machine learning approaches for analyzing high-throughput genetic, transcriptomic and epigenetic data, hold promise for identifying actionable biological processes and gene networks that underlie HAND. This review summarizes the current state of understanding of host genetic factors predisposing to HAND in light of past challenges and suggests some priorities for future research to advance the understanding and clinical management of HAND in the cART era."
24987556,19.0,Big data analysis using modern statistical and machine learning methods in medicine,2014 Jun;18(2):50-7.,"In this article we introduce modern statistical machine learning and bioinformatics approaches that have been used in learning statistical relationships from big data in medicine and behavioral science that typically include clinical, genomic (and proteomic) and environmental variables. Every year, data collected from biomedical and behavioral science is getting larger and more complicated. Thus, in medicine, we also need to be aware of this trend and understand the statistical tools that are available to analyze these datasets. Many statistical analyses that are aimed to analyze such big datasets have been introduced recently. However, given many different types of clinical, genomic, and environmental data, it is rather uncommon to see statistical methods that combine knowledge resulting from those different data types. To this extent, we will introduce big data in terms of clinical data, single nucleotide polymorphism and gene expression studies and their interactions with environment. In this article, we will introduce the concept of well-known regression analyses such as linear and logistic regressions that has been widely used in clinical data analyses and modern statistical models such as Bayesian networks that has been introduced to analyze more complicated data. Also we will discuss how to represent the interaction among clinical, genomic, and environmental data in using modern statistical models. We conclude this article with a promising modern statistical method called Bayesian networks that is suitable in analyzing big data sets that consists with different type of large data from clinical, genomic, and environmental data. Such statistical model form big data will provide us with more comprehensive understanding of human physiology and disease."
24975859,39.0,Evaluation and construction of diagnostic criteria for inclusion body myositis,2014 Jul 29;83(5):426-33.,"Objective:                    To use patient data to evaluate and construct diagnostic criteria for inclusion body myositis (IBM), a progressive disease of skeletal muscle.              Methods:                    The literature was reviewed to identify all previously proposed IBM diagnostic criteria. These criteria were applied through medical records review to 200 patients diagnosed as having IBM and 171 patients diagnosed as having a muscle disease other than IBM by neuromuscular specialists at 2 institutions, and to a validating set of 66 additional patients with IBM from 2 other institutions. Machine learning techniques were used for unbiased construction of diagnostic criteria.              Results:                    Twenty-four previously proposed IBM diagnostic categories were identified. Twelve categories all performed with high (≥97%) specificity but varied substantially in their sensitivities (11%-84%). The best performing category was European Neuromuscular Centre 2013 probable (sensitivity of 84%). Specialized pathologic features and newly introduced strength criteria (comparative knee extension/hip flexion strength) performed poorly. Unbiased data-directed analysis of 20 features in 371 patients resulted in construction of higher-performing data-derived diagnostic criteria (90% sensitivity and 96% specificity).              Conclusions:                    Published expert consensus-derived IBM diagnostic categories have uniformly high specificity but wide-ranging sensitivities. High-performing IBM diagnostic category criteria can be developed directly from principled unbiased analysis of patient data.              Classification of evidence:                    This study provides Class II evidence that published expert consensus-derived IBM diagnostic categories accurately distinguish IBM from other muscle disease with high specificity but wide-ranging sensitivities."
24935820,1.0,Computational validation of the motor contribution to speech perception,2014 Jul;6(3):461-75.,"Action perception and recognition are core abilities fundamental for human social interaction. A parieto-frontal network (the mirror neuron system) matches visually presented biological motion information onto observers' motor representations. This process of matching the actions of others onto our own sensorimotor repertoire is thought to be important for action recognition, providing a non-mediated ""motor perception"" based on a bidirectional flow of information along the mirror parieto-frontal circuits. State-of-the-art machine learning strategies for hand action identification have shown better performances when sensorimotor data, as opposed to visual information only, are available during learning. As speech is a particular type of action (with acoustic targets), it is expected to activate a mirror neuron mechanism. Indeed, in speech perception, motor centers have been shown to be causally involved in the discrimination of speech sounds. In this paper, we review recent neurophysiological and machine learning-based studies showing (a) the specific contribution of the motor system to speech perception and (b) that automatic phone recognition is significantly improved when motor data are used during training of classifiers (as opposed to learning from purely auditory data)."
24895853,15.0,Structural bioinformatics of the interactome,2014;43:193-210.,"The past decade has seen a dramatic expansion in the number and range of techniques available to obtain genome-wide information and to analyze this information so as to infer both the functions of individual molecules and how they interact to modulate the behavior of biological systems. Here, we review these techniques, focusing on the construction of physical protein-protein interaction networks, and highlighting approaches that incorporate protein structure, which is becoming an increasingly important component of systems-level computational techniques. We also discuss how network analyses are being applied to enhance our basic understanding of biological systems and their disregulation, as well as how these networks are being used in drug development."
24857941,10.0,Characterizing EMG data using machine-learning tools,2014 Aug;51:1-13.,"Effective electromyographic (EMG) signal characterization is critical in the diagnosis of neuromuscular disorders. Machine-learning based pattern classification algorithms are commonly used to produce such characterizations. Several classifiers have been investigated to develop accurate and computationally efficient strategies for EMG signal characterization. This paper provides a critical review of some of the classification methodologies used in EMG characterization, and presents the state-of-the-art accomplishments in this field, emphasizing neuromuscular pathology. The techniques studied are grouped by their methodology, and a summary of the salient findings associated with each method is presented."
24834132,22.0,Chemical named entities recognition: a review on approaches and applications,2014 Apr 28;6:17.,"The rapid increase in the flow rate of published digital information in all disciplines has resulted in a pressing need for techniques that can simplify the use of this information. The chemistry literature is very rich with information about chemical entities. Extracting molecules and their related properties and activities from the scientific literature to ""text mine"" these extracted data and determine contextual relationships helps research scientists, particularly those in drug development. One of the most important challenges in chemical text mining is the recognition of chemical entities mentioned in the texts. In this review, the authors briefly introduce the fundamental concepts of chemical literature mining, the textual contents of chemical documents, and the methods of naming chemicals in documents. We sketch out dictionary-based, rule-based and machine learning, as well as hybrid chemical named entity recognition approaches with their applied solutions. We end with an outlook on the pros and cons of these approaches and the types of chemical entities extracted."
24808218,24.0,Predictive monitoring of mobile patients by combining clinical observations with data from wearable sensors,2014 May;18(3):722-30.,"The majority of patients in the hospital are ambulatory and would benefit significantly from predictive and personalized monitoring systems. Such patients are well suited to having their physiological condition monitored using low-power, minimally intrusive wearable sensors. Despite data-collection systems now being manufactured commercially, allowing physiological data to be acquired from mobile patients, little work has been undertaken on the use of the resultant data in a principled manner for robust patient care, including predictive monitoring. Most current devices generate so many false-positive alerts that devices cannot be used for routine clinical practice. This paper explores principled machine learning approaches to interpreting large quantities of continuously acquired, multivariate physiological data, using wearable patient monitors, where the goal is to provide early warning of serious physiological determination, such that a degree of predictive care may be provided. We adopt a one-class support vector machine formulation, proposing a formulation for determining the free parameters of the model using partial area under the ROC curve, a method arising from the unique requirements of performing online analysis with data from patient-worn sensors. There are few clinical evaluations of machine learning techniques in the literature, so we present results from a study at the Oxford University Hospitals NHS Trust devised to investigate the large-scale clinical use of patient-worn sensors for predictive monitoring in a ward with a high incidence of patient mortality. We show that our system can combine routine manual observations made by clinical staff with the continuous data acquired from wearable sensors. Practical considerations and recommendations based on our experiences of this clinical study are discussed, in the context of a framework for personalized monitoring."
24799088,29.0,Big data bioinformatics,2014 Dec;229(12):1896-900.,"Recent technological advances allow for high throughput profiling of biological systems in a cost-efficient manner. The low cost of data generation is leading us to the ""big data"" era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this review, we introduce key concepts in the analysis of big data, including both ""machine learning"" algorithms as well as ""unsupervised"" and ""supervised"" examples of each. We note packages for the R programming language that are available to perform machine learning analyses. In addition to programming based solutions, we review webservers that allow users with limited or no programming background to perform these analyses on large data compendia."
24790546,1.0,A survey on investigating the need for intelligent power-aware load balanced routing protocols for handling critical links in MANETs,2014 Mar 23;2014:138972.,"In mobile ad hoc networks connectivity is always an issue of concern. Due to dynamism in the behavior of mobile nodes, efficiency shall be achieved only with the assumption of good network infrastructure. Presence of critical links results in deterioration which should be detected in advance to retain the prevailing communication setup. This paper discusses a short survey on the specialized algorithms and protocols related to energy efficient load balancing for critical link detection in the recent literature. This paper also suggests a machine learning based hybrid power-aware approach for handling critical nodes via load balancing."
24769242,3.0,Computational modeling of neural plasticity for self-organization of neural networks,2014 Nov;125:43-54.,"Self-organization in biological nervous systems during the lifetime is known to largely occur through a process of plasticity that is dependent upon the spike-timing activity in connected neurons. In the field of computational neuroscience, much effort has been dedicated to building up computational models of neural plasticity to replicate experimental data. Most recently, increasing attention has been paid to understanding the role of neural plasticity in functional and structural neural self-organization, as well as its influence on the learning performance of neural networks for accomplishing machine learning tasks such as classification and regression. Although many ideas and hypothesis have been suggested, the relationship between the structure, dynamics and learning performance of neural networks remains elusive. The purpose of this article is to review the most important computational models for neural plasticity and discuss various ideas about neural plasticity's role. Finally, we suggest a few promising research directions, in particular those along the line that combines findings in computational neuroscience and systems biology, and their synergetic roles in understanding learning, memory and cognition, thereby bridging the gap between computational neuroscience, systems biology and computational intelligence."
24732754,4.0,HIV-associated neuropathogenesis: a systems biology perspective for modeling and therapy,2014 May;119:53-61.,"Despite the development of powerful antiretroviral drugs, HIV-1 associated neurological disorders (HAND) will affect approximately half of those infected with HIV-1. Combined anti-retroviral therapy (cART) targets viral replication and increases T-cell counts, but it does not always control macrophage polarization, brain infection or inflammation. Moreover, it remains difficult to identify those at risk for HAND. New therapies that focus on modulating host immune response by making use of biological pathways could prove to be more effective than cART for the treatment of neuroAIDS. Additionally, while numerous HAND biomarkers have been suggested, they are of little use without methods for appropriate data integration and a systems-level interpretation. Machine learning, could be used to develop multifactorial computational models that provide clinicians and researchers with the ability to identify which factors (in what combination and relative importance) are considered important to outcome."
24723339,14.0,The ENCODE project and perspectives on pathways,2014 May;38(4):275-80.,"The recently completed ENCODE project is a new source of information on metabolic activity, unveiling knowledge about evolution and similarities among species, refuting the myth that most DNA is ""junk"" and has no actual function. With this expansive resource comes a challenge: integrating these new layers of information into our current knowledge of single-nucleotide polymorphisms and previously described metabolic pathways with the aim of discovering new genes and pathways related to human diseases and traits. Further, we must determine which computational methods will be most useful in this pursuit. In this paper, we speculate over the possible methods that will emerge in this new, challenging field."
24718104,35.0,Multivariate data analysis and machine learning in Alzheimer's disease with a focus on structural magnetic resonance imaging,2014;41(3):685-708.,"Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Advances in medical imaging and medical image analysis have provided a means to generate and extract valuable neuroimaging information. Automatic classification techniques provide tools to analyze this information and observe inherent disease-related patterns in the data. In particular, these classifiers have been used to discriminate AD patients from healthy control subjects and to predict conversion from mild cognitive impairment to AD. In this paper, recent studies are reviewed that have used machine learning and multivariate analysis in the field of AD research. The main focus is on studies that used structural magnetic resonance imaging (MRI), but studies that included positron emission tomography and cerebrospinal fluid biomarkers in addition to MRI are also considered. A wide variety of materials and methods has been employed in different studies, resulting in a range of different outcomes. Influential factors such as classifiers, feature extraction algorithms, feature selection methods, validation approaches, and cohort properties are reviewed, as well as key MRI-based and multi-modal based studies. Current and future trends are discussed."
24713999,40.0,Current breathomics--a review on data pre-processing techniques and machine learning in metabolomics breath analysis,2014 Jun;8(2):027105.,"We define breathomics as the metabolomics study of exhaled air. It is a strongly emerging metabolomics research field that mainly focuses on health-related volatile organic compounds (VOCs). Since the amount of these compounds varies with health status, breathomics holds great promise to deliver non-invasive diagnostic tools. Thus, the main aim of breathomics is to find patterns of VOCs related to abnormal (for instance inflammatory) metabolic processes occurring in the human body. Recently, analytical methods for measuring VOCs in exhaled air with high resolution and high throughput have been extensively developed. Yet, the application of machine learning methods for fingerprinting VOC profiles in the breathomics is still in its infancy. Therefore, in this paper, we describe the current state of the art in data pre-processing and multivariate analysis of breathomics data. We start with the detailed pre-processing pipelines for breathomics data obtained from gas-chromatography mass spectrometry and an ion-mobility spectrometer coupled to multi-capillary columns. The outcome of data pre-processing is a matrix containing the relative abundances of a set of VOCs for a group of patients under different conditions (e.g. disease stage, treatment). Independently of the utilized analytical method, the most important question, 'which VOCs are discriminatory?', remains the same. Answers can be given by several modern machine learning techniques (multivariate statistics) and, therefore, are the focus of this paper. We demonstrate the advantages as well the drawbacks of such techniques. We aim to help the community to understand how to profit from a particular method. In parallel, we hope to make the community aware of the existing data fusion methods, as yet unresearched in breathomics."
24713806,19.0,Invadopodia in context,2014;8(3):273-9.,"Invadopodia are dynamic protrusions in motile tumor cells whose function is to degrade extracellular matrix so that cells can enter into new environments. Invadopodia are specifically identified by microscopy as proteolytic invasive protrusions containing TKS5 and cortactin. The increasing complexity in models for the study of invadopodia, including engineered 3D environments, explants, or animal models in vivo, entails a higher level of microenvironment complexity as well as cancer cell heterogeneity. Such experimental setups are rich in information and offer the possibility of contextualizing invadopodia and other motility-related structures. That is, they hold the promise of revealing more realistic microenvironmental conditions under which the invadopodium assembles and functions or in which tumor cells switch to a different cellular phenotype (focal adhesion, lamellipodia, proliferation, and apoptosis). For such an effort, we need a systemic approach to microscopy, which will integrate information from multiple modalities. While the individual technologies needed to achieve this are mostly available, data integration and standardization is not a trivial process. In a systems microscopy approach, microscopy is used to extract information on cell phenotypes and the microenvironment while -omics technologies assess profiles of cancer cell and microenvironment genetic, transcription, translation, and protein makeups. Data are classified and linked via in silico modeling (including statistical and mathematical models and bioinformatics). Computational considerations create predictions to be validated experimentally by perturbing the system through use of genetic manipulations and molecular biology. With such a holistic approach, a deeper understanding of function of invadopodia in vivo will be reached, opening the potential for personalized diagnostics and therapies."
24621527,5.0,Towards more accurate prediction of protein folding rates: a review of the existing Web-based bioinformatics approaches,2015 Mar;16(2):314-24.,"The understanding of protein-folding mechanisms is often considered to be an important goal that will enable structural biologists to discover the mysterious relationship between the sequence, structure and function of proteins. The ability to predict protein-folding rates without the need for actual experimental work will assist the research work of structural biologists in many ways. Many bioinformatics tools have emerged in the past decade, and each has showcased different features. In this article, we review and compare eight web-based prediction tools that are currently available and that predominantly predict the protein-folding rate. The prediction performance, usability and utility, together with the prediction tool development and validation methodologies for these tools, are critically reviewed. This article is presented in a comprehensible manner to assist readers in the process of selecting the most appropriate bioinformatics tools to meet their needs."
24600468,120.0,Common features of microRNA target prediction tools,2014 Feb 18;5:23.,"The human genome encodes for over 1800 microRNAs (miRNAs), which are short non-coding RNA molecules that function to regulate gene expression post-transcriptionally. Due to the potential for one miRNA to target multiple gene transcripts, miRNAs are recognized as a major mechanism to regulate gene expression and mRNA translation. Computational prediction of miRNA targets is a critical initial step in identifying miRNA:mRNA target interactions for experimental validation. The available tools for miRNA target prediction encompass a range of different computational approaches, from the modeling of physical interactions to the incorporation of machine learning. This review provides an overview of the major computational approaches to miRNA target prediction. Our discussion highlights three tools for their ease of use, reliance on relatively updated versions of miRBase, and range of capabilities, and these are DIANA-microT-CDS, miRanda-mirSVR, and TargetScan. In comparison across all miRNA target prediction tools, four main aspects of the miRNA:mRNA target interaction emerge as common features on which most target prediction is based: seed match, conservation, free energy, and site accessibility. This review explains these features and identifies how they are incorporated into currently available target prediction tools. MiRNA target prediction is a dynamic field with increasing attention on development of new analysis tools. This review attempts to provide a comprehensive assessment of these tools in a manner that is accessible across disciplines. Understanding the basis of these prediction methodologies will aid in user selection of the appropriate tools and interpretation of the tool output."
24598104,12.0,Multiparametric Analysis of Screening Data: Growing Beyond the Single Dimension to Infinity and Beyond,2014 Jun;19(5):628-39.,"Advances in instrumentation now allow the development of screening assays that are capable of monitoring multiple readouts such as transcript or protein levels, or even multiple parameters derived from images. Such advances in assay technologies highlight the complex nature of biology and disease. Harnessing this complexity requires integration of all the different parameters that can be measured rather than just monitoring a single dimension as is commonly used. Although some of the methods used to combine multiple measurements, such as principal component analysis, are commonly used for microarray analysis, biologists are not yet using many of the tools that have been developed in other fields to address such issues. Visualization of multiparametric data sets is one of the major challenges in this field, and a depiction of the results in a manner that can be readily interpreted is essential. This article describes a number of assay systems being used to generate such data sets en masse, and the methods being applied to their visualization and analysis. We also discuss some of the challenges of applying methods developed in other fields to biology."
24555973,3.0,The use of intelligent database systems in acute pancreatitis--a systematic review,Jan-Feb 2014;14(1):9-16.,"Introduction:                    Acute pancreatitis (AP) is a complex disease with multiple aetiological factors, wide ranging severity, and multiple challenges to effective triage and management. Databases, data mining and machine learning algorithms (MLAs), including artificial neural networks (ANNs), may assist by storing and interpreting data from multiple sources, potentially improving clinical decision-making.              Aims:                    1) Identify database technologies used to store AP data, 2) collate and categorise variables stored in AP databases, 3) identify the MLA technologies, including ANNs, used to analyse AP data, and 4) identify clinical and non-clinical benefits and obstacles in establishing a national or international AP database.              Methods:                    Comprehensive systematic search of online reference databases. The predetermined inclusion criteria were all papers discussing 1) databases, 2) data mining or 3) MLAs, pertaining to AP, independently assessed by two reviewers with conflicts resolved by a third author.              Results:                    Forty-three papers were included. Three data mining technologies and five ANN methodologies were reported in the literature. There were 187 collected variables identified. ANNs increase accuracy of severity prediction, one study showed ANNs had a sensitivity of 0.89 and specificity of 0.96 six hours after admission--compare APACHE II (cutoff score ≥8) with 0.80 and 0.85 respectively. Problems with databases were incomplete data, lack of clinical data, diagnostic reliability and missing clinical data.              Conclusion:                    This is the first systematic review examining the use of databases, MLAs and ANNs in the management of AP. The clinical benefits these technologies have over current systems and other advantages to adopting them are identified."
24553464,13.0,"Video game play, attention, and learning: how to shape the development of attention and influence learning?",2014 Apr;27(2):185-91.,"Purpose of review:                    The notion that play may facilitate learning has long been touted. Here, we review how video game play may be leveraged for enhancing attentional control, allowing greater cognitive flexibility and learning and in turn new routes to better address developmental disorders.              Recent findings:                    Video games, initially developed for entertainment, appear to enhance the behavior in domains as varied as perception, attention, task switching, or mental rotation. This surprisingly wide transfer may be mediated by enhanced attentional control, allowing increased signal-to-noise ratio and thus more informed decisions.              Summary:                    The possibility of enhancing attentional control through targeted interventions, be it computerized training or self-regulation techniques, is now well established. Embedding such training in video game play is appealing, given the astounding amount of time spent by children and adults worldwide with this media. It holds the promise of increasing compliance in patients and motivation in school children, and of enhancing the use of positive impact games. Yet for all the promises, existing research indicates that not all games are created equal: a better understanding of the game play elements that foster attention and learning as well as of the strategies developed by the players is needed. Computational models from machine learning or developmental robotics provide a rich theoretical framework to develop this work further and address its impact on developmental disorders."
24552264,3.0,An overview of data mining algorithms in drug induced toxicity prediction,2014 Apr;14(4):345-54.,"The growth in chemical diversity has increased the need to adjudicate the toxicity of different chemical compounds raising the burden on the demand of animal testing. The toxicity evaluation requires time consuming and expensive undertaking, leading to the deprivation of the methods employed for screening chemicals pointing towards the need to develop more efficient toxicity assessment systems. Computational approaches have reduced the time as well as the cost for evaluating the toxicity and kinetic behavior of any chemical. The accessibility of a large amount of data and the intense need of turning this data into useful information have attracted the attention towards data mining. Machine Learning, one of the powerful data mining techniques has evolved as the most effective and potent tool for exploring new insights on combinatorial relationships among various experimental data generated. The article accounts on some sophisticated machine learning algorithms like Artificial Neural Networks (ANN), Support Vector Machine (SVM), k-mean clustering and Self Organizing Maps (SOM) with some of the available tools used for classification, sorting and toxicological evaluation of data, clarifying, how data mining and machine learning interact cooperatively to facilitate knowledge discovery. Addressing the association of some commonly used expert systems, we briefly outline some real world applications to consider the crucial role of data set partitioning."
24531111,2.0,Simulation of safety: a review of the state of the art in road safety simulation modelling,2014 May;66:89-103.,"Recent decades have seen considerable growth in computer capabilities, data collection technology and communication mediums. This growth has had considerable impact on our ability to replicate driver behaviour and understand the processes involved in failures in the traffic system. From time to time it is necessary to assess the level of development as a basis of determining how far we have come. This paper sets out to assess the state of the art in the use of computer models to simulate and assess the level of safety in existing and future traffic systems. It reviews developments in the area of road safety simulation models. In particular, it reviews computer models of driver and vehicle behaviour within a road context. It focuses on stochastic numerical models of traffic behaviour and how reliable these are in estimating levels of safety on the traffic network. Models of this type are commonly used in the assessment of traffic systems for capacity, delay and general performance. Adding safety to this assessment regime may allow more comprehensive assessment of future traffic systems. To date the models have focused primarily on vehicular traffic that is, cars and heavy vehicles. It has been shown that these models have potential in measuring the level of conflict on parts of the network and the measure of conflict correlated well with crash statistics. Interest in the prediction of crashes and crash severity is growing and new models are focusing on the continuum of general traffic conditions, conflict, severe conflict, crash and severe crashes. The paper also explores the general data types used to develop, calibrate and validate these models. Recent technological development in in-vehicle data collection, driver simulators and machine learning offers considerable potential for improving the behavioural base, rigour and application of road safety simulation models. The paper closes with some indication of areas of future development."
24509098,39.0,Neural circuits as computational dynamical systems,2014 Apr;25:156-63.,Many recent studies of neurons recorded from cortex reveal complex temporal dynamics. How such dynamics embody the computations that ultimately lead to behavior remains a mystery. Approaching this issue requires developing plausible hypotheses couched in terms of neural dynamics. A tool ideally suited to aid in this question is the recurrent neural network (RNN). RNNs straddle the fields of nonlinear dynamical systems and machine learning and have recently seen great advances in both theory and application. I summarize recent theoretical and technological advances and highlight an example of how RNNs helped to explain perplexing high-dimensional neurophysiological data in the prefrontal cortex.
24508754,15.0,"NeuCube: a spiking neural network architecture for mapping, learning and understanding of spatio-temporal brain data",2014 Apr;52:62-76.,"The brain functions as a spatio-temporal information processing machine. Spatio- and spectro-temporal brain data (STBD) are the most commonly collected data for measuring brain response to external stimuli. An enormous amount of such data has been already collected, including brain structural and functional data under different conditions, molecular and genetic data, in an attempt to make a progress in medicine, health, cognitive science, engineering, education, neuro-economics, Brain-Computer Interfaces (BCI), and games. Yet, there is no unifying computational framework to deal with all these types of data in order to better understand this data and the processes that generated it. Standard machine learning techniques only partially succeeded and they were not designed in the first instance to deal with such complex data. Therefore, there is a need for a new paradigm to deal with STBD. This paper reviews some methods of spiking neural networks (SNN) and argues that SNN are suitable for the creation of a unifying computational framework for learning and understanding of various STBD, such as EEG, fMRI, genetic, DTI, MEG, and NIRS, in their integration and interaction. One of the reasons is that SNN use the same computational principle that generates STBD, namely spiking information processing. This paper introduces a new SNN architecture, called NeuCube, for the creation of concrete models to map, learn and understand STBD. A NeuCube model is based on a 3D evolving SNN that is an approximate map of structural and functional areas of interest of the brain related to the modeling STBD. Gene information is included optionally in the form of gene regulatory networks (GRN) if this is relevant to the problem and the data. A NeuCube model learns from STBD and creates connections between clusters of neurons that manifest chains (trajectories) of neuronal activity. Once learning is applied, a NeuCube model can reproduce these trajectories, even if only part of the input STBD or the stimuli data is presented, thus acting as an associative memory. The NeuCube framework can be used not only to discover functional pathways from data, but also as a predictive system of brain activities, to predict and possibly, prevent certain events. Analysis of the internal structure of a model after training can reveal important spatio-temporal relationships 'hidden' in the data. NeuCube will allow the integration in one model of various brain data, information and knowledge, related to a single subject (personalized modeling) or to a population of subjects. The use of NeuCube for classification of STBD is illustrated in a case study problem of EEG data. NeuCube models result in a better accuracy of STBD classification than standard machine learning techniques. They are robust to noise (so typical in brain data) and facilitate a better interpretation of the results and understanding of the STBD and the brain conditions under which data was collected. Future directions for the use of SNN for STBD are discussed."
24489603,20.0,Machine learning approaches: from theory to application in schizophrenia,2013;2013:867924.,"In recent years, machine learning approaches have been successfully applied for analysis of neuroimaging data, to help in the context of disease diagnosis. We provide, in this paper, an overview of recent support vector machine-based methods developed and applied in psychiatric neuroimaging for the investigation of schizophrenia. In particular, we focus on the algorithms implemented by our group, which have been applied to classify subjects affected by schizophrenia and healthy controls, comparing them in terms of accuracy results with other recently published studies. First we give a description of the basic terminology used in pattern recognition and machine learning. Then we separately summarize and explain each study, highlighting the main features that characterize each method. Finally, as an outcome of the comparison of the results obtained applying the described different techniques, conclusions are drawn in order to understand how much automatic classification approaches can be considered a useful tool in understanding the biological underpinnings of schizophrenia. We then conclude by discussing the main implications achievable by the application of these methods into clinical practice."
24478134,15.0,Probability estimation with machine learning methods for dichotomous and multicategory outcome: theory,2014 Jul;56(4):534-63.,"Probability estimation for binary and multicategory outcome using logistic and multinomial logistic regression has a long-standing tradition in biostatistics. However, biases may occur if the model is misspecified. In contrast, outcome probabilities for individuals can be estimated consistently with machine learning approaches, including k-nearest neighbors (k-NN), bagged nearest neighbors (b-NN), random forests (RF), and support vector machines (SVM). Because machine learning methods are rarely used by applied biostatisticians, the primary goal of this paper is to explain the concept of probability estimation with these methods and to summarize recent theoretical findings. Probability estimation in k-NN, b-NN, and RF can be embedded into the class of nonparametric regression learning machines; therefore, we start with the construction of nonparametric regression estimates and review results on consistency and rates of convergence. In SVMs, outcome probabilities for individuals are estimated consistently by repeatedly solving classification problems. For SVMs we review classification problem and then dichotomous probability estimation. Next we extend the algorithms for estimating probabilities using k-NN, b-NN, and RF to multicategory outcomes and discuss approaches for the multicategory probability estimation problem using SVM. In simulation studies for dichotomous and multicategory dependent variables we demonstrate the general validity of the machine learning methods and compare it with logistic regression. However, each method fails in at least one simulation scenario. We conclude with a discussion of the failures and give recommendations for selecting and tuning the methods. Applications to real data and example code are provided in a companion article (doi:10.1002/bimj.201300077)."
24403917,3.0,GB Virus C/Hepatitis G Virus Envelope Glycoprotein E2: Computational Molecular Features and Immunoinformatics Study,2013 Dec 30;13(12):e15342.,"Introduction:                    GB virus C (GBV-C) or hepatitis G virus (HGV) is an enveloped, RNA positive-stranded flavivirus-like particle. E2 envelope protein of GBV-C plays an important role in virus entry into the cytosol, genotyping and as a marker for diagnosing GBV-C infections. Also, there is discussion on relations between E2 protein and gp41 protein of HIV. The purposes of our study are to multi aspect molecular evaluation of GB virus C E2 protein from its characteristics, mutations, structures and antigenicity which would help to new directions for future researches.              Evidence acquisition:                    Briefly, steps followed here were; retrieving reference sequences of E2 protein, entropy plot evaluation for finding the mutational /conservative regions, analyzing potential Glycosylation, Phosphorylation and Palmitoylation sites, prediction of primary, secondary and tertiary structures, then amino acid distributions and transmembrane topology, prediction of T and B cell epitopes, and finally visualization of epitopes and variations regions in 3D structure.              Results:                    Based on the entropy plot, 3 hypervariable regions (HVR) observed along E2 protein located in residues 133-135, 256-260 and 279-281. Analyzing primary structure of protein sequence revealed basic nature, instability, and low hydrophilicity of this protein. Transmembrane topology prediction showed that residues 257-270 presented outside, while residues 234- 256 and 271-293 were transmembrane regions. Just one N-glycosylation site, 5 potential phosphorylated peptides and two palmitoylation were found. Secondary structure revealed that this protein has 6 α-helix, 12 β-strand 17 Coil structures. Prediction of T-cell epitopes based on HLA-A*02:01 showed that epitope NH3-LLLDFVFVL-COOH is the best antigen icepitope. Comparative analysis for consensus B-cell epitopes regarding transmembrane topology, based on physico-chemical and machine learning approaches revealed that residue 231- 296 (NH2- EARLVPLILLLLWWWVNQLAVLGLPAVEAAVAGEVFAGPALSWCLGLPVVSMILGLANLVLYFRWL-COOH) is most effective and probable B cell epitope for E2 protein.              Conclusions:                    The comprehensive analysis of a protein with important roles has never been easy, and in case of E2 envelope glycoprotein of HGV, there is no much data on its molecular and immunological features, clinical significance and its pathogenic potential in hepatitis or any other GBV-C related diseases. So, results of the present study may explain some structural, physiological and immunological functions of this protein in GBV-C, as well as designing new diagnostic kits and besides, help to better understandingE2 protein characteristic and other members of Flavivirus family, especially HCV."
25756666,1.0,Machine learning in the rational design of antimicrobial peptides,2014;10(3):183-90.,"One of the most important public health issues is the microbial and bacterial resistance to conventional antibiotics by pathogen microorganisms. In recent years, many researches have been focused on the development of new antibiotics. Among these, antimicrobial peptides (AMPs) have raised as a promising alternative to combat antibioticresistant microorganisms. For this reason, many theoretical efforts have been done in the development of new computational tools for the rational design of both better and effective AMPs. In this review, we present an overview of the rational design of AMPs using machine learning techniques and new research fields."
24370382,5.0,Knowledge discovery in clinical decision support systems for pain management: a systematic review,2014 Jan;60(1):1-11.,"Objective:                    The occurrence of pain accounts for billions of dollars in annual medical expenditures; loss of quality of life and decreased worker productivity contribute to indirect costs. As pain is highly subjective, clinical decision support systems (CDSSs) can be critical for improving the accuracy of pain assessment and offering better support for clinical decision-making. This review is focused on computer technologies for pain management that allow CDSSs to obtain knowledge from the clinical data produced by either patients or health care professionals.              Methods and materials:                    A comprehensive literature search was conducted in several electronic databases to identify relevant articles focused on computerised systems that constituted CDSSs and include data or results related to pain symptoms from patients with acute or chronic pain, published between 1992 and 2011 in the English language. In total, thirty-nine studies were analysed; thirty-two were selected from 1245 citations, and seven were obtained from reference tracking.              Results:                    The results highlighted the following clusters of computer technologies: rule-based algorithms, artificial neural networks, nonstandard set theory, and statistical learning algorithms. In addition, several methodologies were found for content processing such as terminologies, questionnaires, and scores. The median accuracy ranged from 53% to 87.5%.              Conclusions:                    Computer technologies that have been applied in CDSSs are important but not determinant in improving the systems' accuracy and the clinical practice, as evidenced by the moderate correlation among the studies. However, these systems play an important role in the design of computerised systems oriented to a patient's symptoms as is required for pain management. Several limitations related to CDSSs were observed: the lack of integration with mobile devices, the reduced use of web-based interfaces, and scarce capabilities for data to be inserted by patients."
24361690,29.0,Progress in computational toxicology,Mar-Apr 2014;69(2):115-40.,"Introduction:                    Computational methods have been widely applied to toxicology across pharmaceutical, consumer product and environmental fields over the past decade. Progress in computational toxicology is now reviewed.              Methods:                    A literature review was performed on computational models for hepatotoxicity (e.g. for drug-induced liver injury (DILI)), cardiotoxicity, renal toxicity and genotoxicity. In addition various publications have been highlighted that use machine learning methods. Several computational toxicology model datasets from past publications were used to compare Bayesian and Support Vector Machine (SVM) learning methods.              Results:                    The increasing amounts of data for defined toxicology endpoints have enabled machine learning models that have been increasingly used for predictions. It is shown that across many different models Bayesian and SVM perform similarly based on cross validation data.              Discussion:                    Considerable progress has been made in computational toxicology in a decade in both model development and availability of larger scale or 'big data' models. The future efforts in toxicology data generation will likely provide us with hundreds of thousands of compounds that are readily accessible for machine learning models. These models will cover relevant chemistry space for pharmaceutical, consumer product and environmental applications."
24361664,36.0,"Non-negative matrix factorization of multimodal MRI, fMRI and phenotypic data reveals differential changes in default mode subnetworks in ADHD",2014 Nov 15;102 Pt 1:207-19.,"In the multimodal neuroimaging framework, data on a single subject are collected from inherently different sources such as functional MRI, structural MRI, behavioral and/or phenotypic information. The information each source provides is not independent; a subset of features from each modality maps to one or more common latent dimensions, which can be interpreted using generative models. These latent dimensions, or ""topics,"" provide a sparse summary of the generative process behind the features for each individual. Topic modeling, an unsupervised generative model, has been used to map seemingly disparate features to a common domain. We use Non-Negative Matrix Factorization (NMF) to infer the latent structure of multimodal ADHD data containing fMRI, MRI, phenotypic and behavioral measurements. We compare four different NMF algorithms and find that the sparsest decomposition is also the most differentiating between ADHD and healthy patients. We identify dimensions that map to interpretable, recognizable dimensions such as motion, default mode network activity, and other such features of the input data. For example, structural and functional graph theory features related to default mode subnetworks clustered with the ADHD-Inattentive diagnosis. Structural measurements of the default mode network (DMN) regions such as the posterior cingulate, precuneus, and parahippocampal regions were all related to the ADHD-Inattentive diagnosis. Ventral DMN subnetworks may have more functional connections in ADHD-I, while dorsal DMN may have less. ADHD topics are dependent upon diagnostic site, suggesting diagnostic differences across geographic locations. We assess our findings in light of the ADHD-200 classification competition, and contrast our unsupervised, nominated topics with previously published supervised learning methods. Finally, we demonstrate the validity of these latent variables as biomarkers by using them for classification of ADHD in 730 patients. Cumulatively, this manuscript addresses how multimodal data in ADHD can be interpreted by latent dimensions."
24351646,52.0,Data mining for wearable sensors in health monitoring systems: a review of recent trends and challenges,2013 Dec 17;13(12):17472-500.,"The past few years have witnessed an increase in the development of wearable sensors for health monitoring systems. This increase has been due to several factors such as development in sensor technology as well as directed efforts on political and stakeholder levels to promote projects which address the need for providing new methods for care given increasing challenges with an aging population. An important aspect of study in such system is how the data is treated and processed. This paper provides a recent review of the latest methods and algorithms used to analyze data from wearable sensors used for physiological monitoring of vital signs in healthcare services. In particular, the paper outlines the more common data mining tasks that have been applied such as anomaly detection, prediction and decision making when considering in particular continuous time series measurements. Moreover, the paper further details the suitability of particular data mining and machine learning methods used to process the physiological data and provides an overview of the properties of the data sets used in experimental validation. Finally, based on this literature review, a number of key challenges have been outlined for data mining methods in health monitoring systems."
24348517,11.0,On protocols and measures for the validation of supervised methods for the inference of biological networks,2013 Dec 3;4:262.,"Networks provide a natural representation of molecular biology knowledge, in particular to model relationships between biological entities such as genes, proteins, drugs, or diseases. Because of the effort, the cost, or the lack of the experiments necessary for the elucidation of these networks, computational approaches for network inference have been frequently investigated in the literature. In this paper, we examine the assessment of supervised network inference. Supervised inference is based on machine learning techniques that infer the network from a training sample of known interacting and possibly non-interacting entities and additional measurement data. While these methods are very effective, their reliable validation in silico poses a challenge, since both prediction and validation need to be performed on the basis of the same partially known network. Cross-validation techniques need to be specifically adapted to classification problems on pairs of objects. We perform a critical review and assessment of protocols and measures proposed in the literature and derive specific guidelines how to best exploit and evaluate machine learning techniques for network inference. Through theoretical considerations and in silico experiments, we analyze in depth how important factors influence the outcome of performance estimation. These factors include the amount of information available for the interacting entities, the sparsity and topology of biological networks, and the lack of experimentally verified non-interacting pairs."
24338557,43.0,Artificial intelligence in medicine and cardiac imaging: harnessing big data and advanced computing to provide personalized medical diagnosis and treatment,2014 Jan;16(1):441.,"Although advances in information technology in the past decade have come in quantum leaps in nearly every aspect of our lives, they seem to be coming at a slower pace in the field of medicine. However, the implementation of electronic health records (EHR) in hospitals is increasing rapidly, accelerated by the meaningful use initiatives associated with the Center for Medicare & Medicaid Services EHR Incentive Programs. The transition to electronic medical records and availability of patient data has been associated with increases in the volume and complexity of patient information, as well as an increase in medical alerts, with resulting ""alert fatigue"" and increased expectations for rapid and accurate diagnosis and treatment. Unfortunately, these increased demands on health care providers create greater risk for diagnostic and therapeutic errors. In the near future, artificial intelligence (AI)/machine learning will likely assist physicians with differential diagnosis of disease, treatment options suggestions, and recommendations, and, in the case of medical imaging, with cues in image interpretation. Mining and advanced analysis of ""big data"" in health care provide the potential not only to perform ""in silico"" research but also to provide ""real time"" diagnostic and (potentially) therapeutic recommendations based on empirical data. ""On demand"" access to high-performance computing and large health care databases will support and sustain our ability to achieve personalized medicine. The IBM Jeopardy! Challenge, which pitted the best all-time human players against the Watson computer, captured the imagination of millions of people across the world and demonstrated the potential to apply AI approaches to a wide variety of subject matter, including medicine. The combination of AI, big data, and massively parallel computing offers the potential to create a revolutionary way of practicing evidence-based, personalized medicine."
24335433,14.0,Biologically inspired intelligent decision making: a commentary on the use of artificial neural networks in bioinformatics,Mar-Apr 2014;5(2):80-95.,"Artificial neural networks (ANNs) are a class of powerful machine learning models for classification and function approximation which have analogs in nature. An ANN learns to map stimuli to responses through repeated evaluation of exemplars of the mapping. This learning approach results in networks which are recognized for their noise tolerance and ability to generalize meaningful responses for novel stimuli. It is these properties of ANNs which make them appealing for applications to bioinformatics problems where interpretation of data may not always be obvious, and where the domain knowledge required for deductive techniques is incomplete or can cause a combinatorial explosion of rules. In this paper, we provide an introduction to artificial neural network theory and review some interesting recent applications to bioinformatics problems."
24323524,12.0,Machine learning applications in proteomics research: how the past can boost the future,2014 Mar;14(4-5):353-66.,"Machine learning is a subdiscipline within artificial intelligence that focuses on algorithms that allow computers to learn solving a (complex) problem from existing data. This ability can be used to generate a solution to a particularly intractable problem, given that enough data are available to train and subsequently evaluate an algorithm on. Since MS-based proteomics has no shortage of complex problems, and since publicly available data are becoming available in ever growing amounts, machine learning is fast becoming a very popular tool in the field. We here therefore present an overview of the different applications of machine learning in proteomics that together cover nearly the entire wet- and dry-lab workflow, and that address key bottlenecks in experiment planning and design, as well as in data processing and analysis."
24320616,99.0,Beyond modules and hubs: the potential of gene coexpression networks for investigating molecular mechanisms of complex brain disorders,2014 Jan;13(1):13-24.,"In a research environment dominated by reductionist approaches to brain disease mechanisms, gene network analysis provides a complementary framework in which to tackle the complex dysregulations that occur in neuropsychiatric and other neurological disorders. Gene-gene expression correlations are a common source of molecular networks because they can be extracted from high-dimensional disease data and encapsulate the activity of multiple regulatory systems. However, the analysis of gene coexpression patterns is often treated as a mechanistic black box, in which looming 'hub genes' direct cellular networks, and where other features are obscured. By examining the biophysical bases of coexpression and gene regulatory changes that occur in disease, recent studies suggest it is possible to use coexpression networks as a multi-omic screening procedure to generate novel hypotheses for disease mechanisms. Because technical processing steps can affect the outcome and interpretation of coexpression networks, we examine the assumptions and alternatives to common patterns of coexpression analysis and discuss additional topics such as acceptable datasets for coexpression analysis, the robust identification of modules, disease-related prioritization of genes and molecular systems and network meta-analysis. To accelerate coexpression research beyond modules and hubs, we highlight some emerging directions for coexpression network research that are especially relevant to complex brain disease, including the centrality-lethality relationship, integration with machine learning approaches and network pharmacology."
24307566,68.0,"Reverse engineering and identification in systems biology: strategies, perspectives and challenges",2013 Dec 4;11(91):20130505.,"The interplay of mathematical modelling with experiments is one of the central elements in systems biology. The aim of reverse engineering is to infer, analyse and understand, through this interplay, the functional and regulatory mechanisms of biological systems. Reverse engineering is not exclusive of systems biology and has been studied in different areas, such as inverse problem theory, machine learning, nonlinear physics, (bio)chemical kinetics, control theory and optimization, among others. However, it seems that many of these areas have been relatively closed to outsiders. In this contribution, we aim to compare and highlight the different perspectives and contributions from these fields, with emphasis on two key questions: (i) why are reverse engineering problems so hard to solve, and (ii) what methods are available for the particular problems arising from systems biology?"
24304044,18.0,Support vector machines for drug discovery,2014 Jan;9(1):93-104.,"Introduction:                    Support vector machines (SVMs) are supervised machine learning algorithms for binary class label prediction and regression-based prediction of property values. In recent years, SVMs have become increasingly popular for drug discovery-relevant applications such as compound classification, the search for novel active compounds and property predictions.              Areas covered:                    The authors provide the readers with a brief introduction of SVM theory and discuss the kernel functions designed for drug discovery applications. The authors also review the different types of SVM applications in drug discovery, looking at particular case studies. Furthermore, the authors discuss the recent hybrid methods developed that incorporate SVM modeling in different ways.              Expert opinion:                    SVMs are currently among the best-performing approaches for chemical and biological property prediction and the computational identification of active compounds. It is anticipated that their use in drug discovery will further increase. Indeed, this will also include the development of SVM-based meta-classifiers that combine different approaches and exploit their individual strengths and complementarity."
24290387,14.0,Motor skill in autism spectrum disorders: a subcortical view,2013;113:207-49.,"The earliest observable symptoms of autism spectrum disorders (ASDs) involve motor behavior. There is a growing awareness of the developmental importance of impaired motor function in ASD and its association with social skill. Compromised motor function requires increased attention, leaving fewer resources available for processing environmental stimuli and learning. This knowledge suggests that the motor system-which we know to be trainable-may be a gateway to improving outcomes of individuals living with ASD. In this review, we suggest a framework borrowed from machine learning to examine where, why, and how motor skills are different in individuals with ASD."
24287119,15.0,An integrative meta-analysis of microRNAs in hepatocellular carcinoma,2013 Dec;11(6):354-67.,"We aimed to shed new light on the roles of microRNAs (miRNAs) in liver cancer using an integrative in silico bioinformatics analysis. A new protocol for target prediction and functional analysis is presented and applied to the 26 highly differentially deregulated miRNAs in hepatocellular carcinoma. This framework comprises: (1) the overlap of prediction results by four out of five target prediction tools, including TargetScan, PicTar, miRanda, DIANA-microT and miRDB (combining machine-learning, alignment, interaction energy and statistical tests in order to minimize false positives), (2) evidence from previous microarray analysis on the expression of these targets, (3) gene ontology (GO) and pathway enrichment analysis of the miRNA targets and their pathways and (4) linking these results to oncogenesis and cancer hallmarks. This yielded new insights into the roles of miRNAs in cancer hallmarks. Here we presented several key targets and hundreds of new targets that are significantly enriched in many new cancer-related hallmarks. In addition, we also revealed some known and new oncogenic pathways for liver cancer. These included the famous MAPK, TGFβ and cell cycle pathways. New insights were also provided into Wnt signaling, prostate cancer, axon guidance and oocyte meiosis pathways. These signaling and developmental pathways crosstalk to regulate stem cell transformation and implicate a role of miRNAs in hepatic stem cell deregulation and cancer development. By analyzing their complete interactome, we proposed new categorization for some of these miRNAs as either tumor-suppressors or oncomiRs with dual roles. Therefore some of these miRNAs may be addressed as therapeutic targets or used as therapeutic agents. Such dual roles thus expand the view of miRNAs as active maintainers of cellular homeostasis."
24273713,30.0,"Towards the identification of imaging biomarkers in schizophrenia, using multivariate pattern classification at a single-subject level",2013 Sep 13;3:279-89.,"Standard univariate analyses of brain imaging data have revealed a host of structural and functional brain alterations in schizophrenia. However, these analyses typically involve examining each voxel separately and making inferences at group-level, thus limiting clinical translation of their findings. Taking into account the fact that brain alterations in schizophrenia expand over a widely distributed network of brain regions, univariate analysis methods may not be the most suited choice for imaging data analysis. To address these limitations, the neuroimaging community has turned to machine learning methods both because of their ability to examine voxels jointly and their potential for making inferences at a single-subject level. This article provides a critical overview of the current and foreseeable applications of machine learning, in identifying imaging-based biomarkers that could be used for the diagnosis, early detection and treatment response of schizophrenia, and could, thus, be of high clinical relevance. We discuss promising future research directions and the main difficulties facing machine learning researchers as far as their potential translation into clinical practice is concerned."
24272434,31.0,Introduction to machine learning,2014;1107:105-28.,"The machine learning field, which can be briefly defined as enabling computers make successful predictions using past experiences, has exhibited an impressive development recently with the help of the rapid increase in the storage capacity and processing power of computers. Together with many other disciplines, machine learning methods have been widely employed in bioinformatics. The difficulties and cost of biological analyses have led to the development of sophisticated machine learning approaches for this application area. In this chapter, we first review the fundamental concepts of machine learning such as feature assessment, unsupervised versus supervised learning and types of classification. Then, we point out the main issues of designing machine learning experiments and their performance evaluation. Finally, we introduce some supervised learning methods."
24262909,13.0,Creating the feedback loop: closed-loop neurostimulation,2014 Jan;25(1):187-204.,"Current DBS therapy delivers a train of electrical pulses at set stimulation parameters. This open-loop design is effective for movement disorders, but therapy may be further optimized by a closed loop design. The technology to record biosignals has outpaced our understanding of their relationship to the clinical state of the whole person. Neuronal oscillations may represent or facilitate the cooperative functioning of brain ensembles, and may provide critical information to customize neuromodulation therapy. This review addresses advances to date, not of the technology per se, but of the strategies to apply neuronal signals to trigger or modulate stimulation systems."
24259662,79.0,Machine learning in cell biology - teaching computers to recognize phenotypes,2013 Dec 15;126(Pt 24):5529-39.,"Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline."
24246494,19.0,An EEG Finger-Print of fMRI deep regional activation,2014 Nov 15;102 Pt 1:128-41.,"This work introduces a general framework for producing an EEG Finger-Print (EFP) which can be used to predict specific brain activity as measured by fMRI at a given deep region. This new approach allows for improved EEG spatial resolution based on simultaneous fMRI activity measurements. Advanced signal processing and machine learning methods were applied on EEG data acquired simultaneously with fMRI during relaxation training guided by on-line continuous feedback on changing alpha/theta EEG measure. We focused on demonstrating improved EEG prediction of activation in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model that is based on time/frequency representation of EEG data from a single electrode, can predict the amygdala related activity significantly better than a traditional theta/alpha activity sampled from the best electrode and about 1/3 of the times, significantly better than a linear combination of frequencies with a pre-defined delay. The far-reaching goal of our approach is to be able to reduce the need for fMRI scanning for probing specific sub-cortical regions such as the amygdala as the basis for brain-training procedures. On the other hand, activity in those regions can be characterized with higher temporal resolution than is obtained by fMRI alone thus revealing additional information about their processing mode."
24245763,,Machine learning and tubercular drug target recognition,2014;20(27):4307-18.,"Tuberculosis (TB) remains to be a global major public-health threat, causing millions of deaths each year. A major difficulty in dealing with TB is that the causative bacterium, Mycobacterium tuberculosis, can persist in host tissue for a long period of time even after treatment. Mycobacterial persistence has become a central research focus for developing next-generation TB drugs. Latest genomic technology has enabled a high-throughput approach for identifying potential TB drug targets. Each gene product can be screened for its uniqueness to the TB metabolism, host-pathogen discrimination, essentiality for survival, and potential for chemical binding, among other properties. However, the exhaustive search for useful drug targets over the entire genome would not be productive as expected in practice. On the other hand, the problem can be formulated as pattern recognition or inductive learning and tackled with rule-based or statistically based learning algorithms. Here we review the perspective that combines machine learning and genomics for drug discovery in tuberculosis."
