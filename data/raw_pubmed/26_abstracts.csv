pmid,citations,title,date,text
28902409,10.0,Binswanger's disease: biomarkers in the inflammatory form of vascular cognitive impairment and dementia,2018 Mar;144(5):634-643.,"Vascular cognitive impairment and dementia (VCID) is a major public health concern because of the increased incidence of vascular disease in the aging population and the impact of vascular disease on Alzheimer's disease. VCID is a heterogeneous group of diseases for which there are no proven treatments. Biomarkers can be used to select more homogeneous populations. Small vessel disease is the most prevalent form of VCID and is the optimal form for treatment trials because there is a progressive course with characteristic pathological changes. Subcortical ischemic vascular disease of the Binswanger type (SIVD-BD) has a characteristic set of features that can be used both to identify patients and to follow treatment. SIVD-BD patients have clinical, neuropsychological, cerebrospinal fluid (CSF) and imaging features that can be used as biomarkers. No one feature is diagnostic, but a multimodal approach defines the SIVD-BD spectrum disorder. The most important features are large white matter lesions with axonal damage, blood-brain barrier disruption as shown by magnetic resonance imaging and CSF, and neuropsychological evidence of executive dysfunction. We have used these features to create a Binswanger Disease Scale and a probability of SIVD-BD, using a machine-learning algorithm. The patients discussed in this review are derived from published studies. Biomarkers not only aid in early diagnosis before the disease process has progressed too far for treatment, but also can indicate response to treatment. Refining the use of biomarkers will allow dementia treatment to enter the era of precision medicine. This article is part of the Special Issue ""Vascular Dementia""."
28893404,2.0,3D Quantitative Chemical Imaging of Tissues by Spectromics,2017 Dec;35(12):1194-1207.,"Mid-infrared (IR), Raman, and X-ray fluorescence (XRF) spectroscopy methods, as well as mass spectrometry (MS), can be used for 3D chemical imaging. These techniques offer an invaluable opportunity to access chemical features of biological samples in a nonsupervised way. The global chemical information they provide enables the exploitation of a large array of chemical species or parameters, so-called 'spectromics'. Extracting chemical data from spectra is critical for the high-quality chemical analysis of biosamples. Furthermore, these are the only currently available techniques that can quantitatively analyze tissue content (e.g., molecular concentrations) and substructures (e.g., cells or blood vessels). The development of chemical-derived biological metadata appears to be a new way to exploit spectral information with machine learning algorithms."
28892073,37.0,Progress and roadblocks in the search for brain-based biomarkers of autism and attention-deficit/hyperactivity disorder,2017 Aug 22;7(8):e1218.,"Children with neurodevelopmental disorders benefit most from early interventions and treatments. The development and validation of brain-based biomarkers to aid in objective diagnosis can facilitate this important clinical aim. The objective of this review is to provide an overview of current progress in the use of neuroimaging to identify brain-based biomarkers for autism spectrum disorder (ASD) and attention-deficit/hyperactivity disorder (ADHD), two prevalent neurodevelopmental disorders. We summarize empirical work that has laid the foundation for using neuroimaging to objectively quantify brain structure and function in ways that are beginning to be used in biomarker development, noting limitations of the data currently available. The most successful machine learning methods that have been developed and applied to date are discussed. Overall, there is increasing evidence that specific features (for example, functional connectivity, gray matter volume) of brain regions comprising the salience and default mode networks can be used to discriminate ASD from typical development. Brain regions contributing to successful discrimination of ADHD from typical development appear to be more widespread, however there is initial evidence that features derived from frontal and cerebellar regions are most informative for classification. The identification of brain-based biomarkers for ASD and ADHD could potentially assist in objective diagnosis, monitoring of treatment response and prediction of outcomes for children with these neurodevelopmental disorders. At present, however, the field has yet to identify reliable and reproducible biomarkers for these disorders, and must address issues related to clinical heterogeneity, methodological standardization and cross-site validation before further progress can be achieved."
28881183,51.0,From machine learning to deep learning: progress in machine intelligence for rational drug discovery,2017 Nov;22(11):1680-1685.,"Machine intelligence, which is normally presented as artificial intelligence, refers to the intelligence exhibited by computers. In the history of rational drug discovery, various machine intelligence approaches have been applied to guide traditional experiments, which are expensive and time-consuming. Over the past several decades, machine-learning tools, such as quantitative structure-activity relationship (QSAR) modeling, were developed that can identify potential biological active molecules from millions of candidate compounds quickly and cheaply. However, when drug discovery moved into the era of 'big' data, machine learning approaches evolved into deep learning approaches, which are a more powerful and efficient way to deal with the massive amounts of data generated from modern drug discovery approaches. Here, we summarize the history of machine learning and provide insight into recently developed deep learning approaches and their applications in rational drug discovery. We suggest that this evolution of machine intelligence now provides a guide for early-stage drug design and discovery in the current big data era."
28879175,4.0,Computer Vision Malaria Diagnostic Systems-Progress and Prospects,2017 Aug 21;5:219.,"Accurate malaria diagnosis is critical to prevent malaria fatalities, curb overuse of antimalarial drugs, and promote appropriate management of other causes of fever. While several diagnostic tests exist, the need for a rapid and highly accurate malaria assay remains. Microscopy and rapid diagnostic tests are the main diagnostic modalities available, yet they can demonstrate poor performance and accuracy. Automated microscopy platforms have the potential to significantly improve and standardize malaria diagnosis. Based on image recognition and machine learning algorithms, these systems maintain the benefits of light microscopy and provide improvements such as quicker scanning time, greater scanning area, and increased consistency brought by automation. While these applications have been in development for over a decade, recently several commercial platforms have emerged. In this review, we discuss the most advanced computer vision malaria diagnostic technologies and investigate several of their features which are central to field use. Additionally, we discuss the technological and policy barriers to implementing these technologies in low-resource settings world-wide."
28867500,1.0,Review of combinations of experimental and computational techniques to identify and understand genes involved in innate immunity and effector-triggered defence,2017 Dec 1;131:120-127.,"The innate immune system includes a first layer of defence that recognises conserved pathogen-associated molecular patterns that are essential for microbial fitness. Resistance (R) gene-based recognition of pathogen effectors, which function in modulation or avoidance of host immunity, activates a second layer of plant defence. In this review, experimental and computational techniques are considered to improve understanding of the plant immune system. Biocomputation contributes to discovery of the molecular genetic basis of host resistance against pathogens. Sequenced genomes have been used to identify R genes in plants. Resistance gene enrichment sequencing based on conserved protein domains has increased the number of R genes with nucleotide-binding site and leucine-rich repeat domains. Network analysis will contribute to an improved understanding of the innate immune system and identify novel genes for partial disease resistance. Machine learning algorithms are expected to become important in defining aspects of the immune system that are less well characterised, including identification of R genes that lack conserved protein domains."
28864356,12.0,Lensless digital holographic microscopy and its applications in biomedicine and environmental monitoring,2018 Mar 1;136:4-16.,"Optical compound microscope has been a major tool in biomedical imaging for centuries. Its performance relies on relatively complicated, bulky and expensive lenses and alignment mechanics. In contrast, the lensless microscope digitally reconstructs microscopic images of specimens without using any lenses, as a result of which it can be made much smaller, lighter and lower-cost. Furthermore, the limited space-bandwidth product of objective lenses in a conventional microscope can be significantly surpassed by a lensless microscope. Such lensless imaging designs have enabled high-resolution and high-throughput imaging of specimens using compact, portable and cost-effective devices to potentially address various point-of-care, global-health and telemedicine related challenges. In this review, we discuss the operation principles and the methods behind lensless digital holographic on-chip microscopy. We also go over various applications that are enabled by cost-effective and compact implementations of lensless microscopy, including some recent work on air quality monitoring, which utilized machine learning for high-throughput and accurate quantification of particulate matter in air. Finally, we conclude with a brief future outlook of this computational imaging technology."
28861708,3.0,Breast cancer cell nuclei classification in histopathology images using deep neural networks,2018 Feb;13(2):179-191.,"Purpose:                    Cell nuclei classification in breast cancer histopathology images plays an important role in effective diagnose since breast cancer can often be characterized by its expression in cell nuclei. However, due to the small and variant sizes of cell nuclei, and heavy noise in histopathology images, traditional machine learning methods cannot achieve desirable recognition accuracy. To address this challenge, this paper aims to present a novel deep neural network which performs representation learning and cell nuclei recognition in an end-to-end manner.              Methods:                    The proposed model hierarchically maps raw medical images into a latent space in which robustness is achieved by employing a stacked denoising autoencoder. A supervised classifier is further developed to improve the discrimination of the model by maximizing inter-subject separability in the latent space. The proposed method involves a cascade model which jointly learns a set of nonlinear mappings and a classifier from the given raw medical images. Such an on-the-shelf learning strategy makes obtaining discriminative features possible, thus leading to better recognition performance.              Results:                    Extensive experiments with benign and malignant breast cancer datasets are conducted to verify the effectiveness of the proposed method. Better performance was obtained when compared with other feature extraction methods, and higher recognition rate was achieved when compared with other seven classification methods.              Conclusions:                    We propose an end-to-end DNN model for cell nuclei and non-nuclei classification of histopathology images. It demonstrates that the proposed method can achieve promising performance in cell nuclei classification, and the proposed method is suitable for the cell nuclei classification task."
28845200,,Nature-Inspired Chemical Reaction Optimisation Algorithms,2017;9(4):411-422.,Nature-inspired meta-heuristic algorithms have dominated the scientific literature in the areas of machine learning and cognitive computing paradigm in the last three decades. Chemical reaction optimisation (CRO) is a population-based meta-heuristic algorithm based on the principles of chemical reaction. A chemical reaction is seen as a process of transforming the reactants (or molecules) through a sequence of reactions into products. This process of transformation is implemented in the CRO algorithm to solve optimisation problems. This article starts with an overview of the chemical reactions and how it is applied to the optimisation problem. A review of CRO and its variants is presented in the paper. Guidelines from the literature on the effective choice of CRO parameters for solution of optimisation problems are summarised.
28841086,152.0,Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning,2017 Nov;12(6):1100-1122.,"Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology's near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior."
28838801,4.0,Optimization of infobutton design and Implementation: A systematic review,2017 Oct;74:10-19.,"Objective:                    Infobuttons are clinical decision tools embedded in the electronic health record that attempt to link clinical data with context sensitive knowledge resources. We systematically reviewed technical approaches that contribute to improved infobutton design, implementation and functionality.              Methods:                    We searched databases including MEDLINE, EMBASE, and the Cochrane Library database from inception to March 1, 2016 for studies describing the use of infobuttons. We selected full review comparative studies, usability studies, and qualitative studies examining infobutton design and implementation. We abstracted usability measures such as user satisfaction, impact, and efficiency, as well as prediction accuracy of infobutton content retrieval algorithms and infobutton adoption/interoperability.              Results:                    We found 82 original research studies on infobuttons. Twelve studies met criteria for detailed abstraction. These studies investigated infobutton interoperability (1 study); tools to help tailor infobutton functionality (1 study); interventions to improve user experience (7 studies); and interventions to improve content retrieval by improving prediction of relevant knowledge resources and information needs (3 studies). In-depth interviews with implementers showed the Health Level Seven (HL7) Infobutton standard to be simple and easy to implement. A usability study demonstrated the feasibility of a tool to help medical librarians tailor infobutton functionality. User experience studies showed that access to resources with which users are familiar increased user satisfaction ratings; and that links to specific subsections of drug monographs increased information seeking efficiency. However, none of the user experience improvements led to increased usage uptake. Recommender systems based on machine learning algorithms outperformed hand-crafted rules in the prediction of relevant resources and clinicians' information needs in a laboratory setting, but no studies were found using these techniques in clinical settings. Improved content indexing in one study led to improved content retrieval across three health care organizations.              Conclusion:                    Best practice technical approaches to ensure optimal infobutton functionality, design and implementation remain understudied. The HL7 Infobutton standard has supported wide adoption of infobutton functionality among clinical information systems and knowledge resources. Limited evidence supports infobutton enhancements such as links to specific subtopics, configuration of optimal resources for specific tasks and users, and improved indexing and content coverage. Further research is needed to investigate user experience improvements to increase infobutton use and effectiveness."
28831921,5.0,In Silico Studies in Drug Research Against Neurodegenerative Diseases,2018;16(6):664-725.,"Background:                    Neurodegenerative diseases such as Alzheimer's disease (AD), amyotrophic lateral sclerosis, Parkinson's disease (PD), spinal cerebellar ataxias, and spinal and bulbar muscular atrophy are described by slow and selective degeneration of neurons and axons in the central nervous system (CNS) and constitute one of the major challenges of modern medicine. Computeraided or in silico drug design methods have matured into powerful tools for reducing the number of ligands that should be screened in experimental assays.              Methods:                    In the present review, the authors provide a basic background about neurodegenerative diseases and in silico techniques in the drug research. Furthermore, they review the various in silico studies reported against various targets in neurodegenerative diseases, including homology modeling, molecular docking, virtual high-throughput screening, quantitative structure activity relationship (QSAR), hologram quantitative structure activity relationship (HQSAR), 3D pharmacophore mapping, proteochemometrics modeling (PCM), fingerprints, fragment-based drug discovery, Monte Carlo simulation, molecular dynamic (MD) simulation, quantum-mechanical methods for drug design, support vector machines, and machine learning approaches.              Results:                    Detailed analysis of the recently reported case studies revealed that the majority of them use a sequential combination of ligand and structure-based virtual screening techniques, with particular focus on pharmacophore models and the docking approach.              Conclusion:                    Neurodegenerative diseases have a multifactorial pathoetiological origin, so scientists have become persuaded that a multi-target therapeutic strategy aimed at the simultaneous targeting of multiple proteins (and therefore etiologies) involved in the development of a disease is recommended in future."
28831290,1.0,An Update on Statistical Boosting in Biomedicine,2017;2017:6083072.,"Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression, and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine."
28830106,12.0,Digital Pharmacovigilance and Disease Surveillance: Combining Traditional and Big-Data Systems for Better Public Health,2016 Dec 1;214(suppl_4):S399-S403.,"The digital revolution has contributed to very large data sets (ie, big data) relevant for public health. The two major data sources are electronic health records from traditional health systems and patient-generated data. As the two data sources have complementary strengths-high veracity in the data from traditional sources and high velocity and variety in patient-generated data-they can be combined to build more-robust public health systems. However, they also have unique challenges. Patient-generated data in particular are often completely unstructured and highly context dependent, posing essentially a machine-learning challenge. Some recent examples from infectious disease surveillance and adverse drug event monitoring demonstrate that the technical challenges can be solved. Despite these advances, the problem of verification remains, and unless traditional and digital epidemiologic approaches are combined, these data sources will be constrained by their intrinsic limits."
28828993,3.0,"General Machine Learning Model, Review, and Experimental-Theoretic Study of Magnolol Activity in Enterotoxigenic Induced Oxidative Stress",2017;17(26):2977-2988.,"This study evaluated the antioxidative effects of magnolol based on the mouse model induced by Enterotoxigenic Escherichia coli (E. coli, ETEC). All experimental mice were equally treated with ETEC suspensions (3.45×109 CFU/ml) after oral administration of magnolol for 7 days at the dose of 0, 100, 300 and 500 mg/kg Body Weight (BW), respectively. The oxidative metabolites and antioxidases for each sample (organism of mouse) were determined: Malondialdehyde (MDA), Nitric Oxide (NO), Glutathione (GSH), Myeloperoxidase (MPO), Catalase (CAT), Superoxide Dismutase (SOD), and Glutathione Peroxidase (GPx). In addition, we also determined the corresponding mRNA expressions of CAT, SOD and GPx as well as the Total Antioxidant Capacity (T-AOC). The experiment was completed with a theoretical study that predicts a series of 79 ChEMBL activities of magnolol with 47 proteins in 18 organisms using a Quantitative Structure- Activity Relationship (QSAR) classifier based on the Moving Averages (MAs) of Rcpi descriptors in three types of experimental conditions (biological activity with specific units, protein target and organisms). Six Machine Learning methods from Weka software were tested and the best QSAR classification model was provided by Random Forest with True Positive Rate (TPR) of 0.701 and Area under Receiver Operating Characteristic (AUROC) of 0.790 (test subset, 10-fold crossvalidation). The model is predicting if the new ChEMBL activities are greater or lower than the average values for the magnolol targets in different organisms."
28812204,8.0,Multivariable Adaptive Artificial Pancreas System in Type 1 Diabetes,2017 Aug 15;17(10):88.,"Purpose of review:                    The review summarizes the current state of the artificial pancreas (AP) systems and introduces various new modules that should be included in future AP systems.              Recent findings:                    A fully automated AP must be able to detect and mitigate the effects of meals, exercise, stress and sleep on blood glucose concentrations. This can only be achieved by using a multivariable approach that leverages information from wearable devices that provide real-time streaming data about various physiological variables that indicate imminent changes in blood glucose concentrations caused by meals, exercise, stress and sleep. The development of a fully automated AP will necessitate the design of multivariable and adaptive systems that use information from wearable devices in addition to glucose sensors and modify the models used in their model-predictive alarm and control systems to adapt to the changes in the metabolic state of the user. These AP systems will also integrate modules for controller performance assessment, fault detection and diagnosis, machine learning and classification to interpret various signals and achieve fault-tolerant control. Advances in wearable devices, computational power, and safe and secure communications are enabling the development of fully automated multivariable AP systems."
28812013,2.0,Intelligent Techniques Using Molecular Data Analysis in Leukaemia: An Opportunity for Personalized Medicine Support System,2017;2017:3587309.,"The use of intelligent techniques in medicine has brought a ray of hope in terms of treating leukaemia patients. Personalized treatment uses patient's genetic profile to select a mode of treatment. This process makes use of molecular technology and machine learning, to determine the most suitable approach to treating a leukaemia patient. Until now, no reviews have been published from a computational perspective concerning the development of personalized medicine intelligent techniques for leukaemia patients using molecular data analysis. This review studies the published empirical research on personalized medicine in leukaemia and synthesizes findings across studies related to intelligence techniques in leukaemia, with specific attention to particular categories of these studies to help identify opportunities for further research into personalized medicine support systems in chronic myeloid leukaemia. A systematic search was carried out to identify studies using intelligence techniques in leukaemia and to categorize these studies based on leukaemia type and also the task, data source, and purpose of the studies. Most studies used molecular data analysis for personalized medicine, but future advancement for leukaemia patients requires molecular models that use advanced machine-learning methods to automate decision-making in treatment management to deliver supportive medical information to the patient in clinical practice."
28807870,7.0,"An open, multi-vendor, multi-field-strength brain MR dataset and analysis of publicly available skull stripping methods agreement",2018 Apr 15;170:482-494.,"This paper presents an open, multi-vendor, multi-field strength magnetic resonance (MR) T1-weighted volumetric brain imaging dataset, named Calgary-Campinas-359 (CC-359). The dataset is composed of images of older healthy adults (29-80 years) acquired on scanners from three vendors (Siemens, Philips and General Electric) at both 1.5 T and 3 T. CC-359 is comprised of 359 datasets, approximately 60 subjects per vendor and magnetic field strength. The dataset is approximately age and gender balanced, subject to the constraints of the available images. It provides consensus brain extraction masks for all volumes generated using supervised classification. Manual segmentation results for twelve randomly selected subjects performed by an expert are also provided. The CC-359 dataset allows investigation of 1) the influences of both vendor and magnetic field strength on quantitative analysis of brain MR; 2) parameter optimization for automatic segmentation methods; and potentially 3) machine learning classifiers with big data, specifically those based on deep learning methods, as these approaches require a large amount of data. To illustrate the utility of this dataset, we compared to the results of a supervised classifier, the results of eight publicly available skull stripping methods and one publicly available consensus algorithm. A linear mixed effects model analysis indicated that vendor (p-value<0.001) and magnetic field strength (p-value<0.001) have statistically significant impacts on skull stripping results."
28798198,11.0,Wearable ballistocardiogram and seismocardiogram systems for health and performance,2018 Feb 1;124(2):452-461.,"Cardiovascular diseases (CVDs) are prevalent in the US, and many forms of CVD primarily affect the mechanical aspects of heart function. Wearable technologies for monitoring the mechanical health of the heart and vasculature could enable proactive management of CVDs through titration of care based on physiological status as well as preventative wellness monitoring to help promote lifestyle choices that reduce the overall risk of developing CVDs. Additionally, such wearable technologies could be used to optimize human performance in austere environments. This review describes our progress in developing wearable ballistocardiogram (BCG)- and seismocardiogram-based systems for monitoring relative changes in cardiac output, contractility, and blood pressure. Our systems use miniature, low-noise accelerometers to measure the movements of the body in response to the heartbeat and novel machine learning algorithms to provide robustness against motion artifacts and sensor misplacement. Moreover, we have mathematically related wearable BCG signals-representing local, cardiogenic movements of a point on the body-to better understood whole body BCG signals, and thereby improved estimation of key health parameters. We validated these systems with experiments in healthy subjects, studies in patients with heart failure, and measurements in austere environments such as water immersion. The systems can be used in future work as a tool for clinicians and physiologists to measure the mechanical aspects of cardiovascular function outside of clinical settings, and to thereby titrate care for patients with CVDs, provide preventative screening, and optimize performance in austere environments by providing real-time in-depth information regarding performance and risk."
28791894,3.0,Decoding HIV resistance: from genotype to therapy,2017 Sep;9(13):1529-1538.,"Genetic variation in HIV poses a major challenge for prevention and treatment of the AIDS pandemic. Resistance occurs by mutations in the target proteins that lower affinity for the drug or alter the protein dynamics, thereby enabling viral replication in the presence of the drug. Due to the prevalence of drug-resistant strains, monitoring the genotype of the infecting virus is recommended. Computational approaches for predicting resistance from genotype data and guiding therapy are discussed. Many prediction methods rely on rules derived from known resistance-associated mutations, however, statistical or machine learning can improve the classification accuracy and assess unknown mutations. Adding classifiers such as information on the atomic structure of the protein can further enhance the predictions."
28751371,2.0,Wearable knee health system employing novel physiological biomarkers,2018 Mar 1;124(3):537-547.,"Knee injuries and chronic disorders, such as arthritis, affect millions of Americans, leading to missed workdays and reduced quality of life. Currently, after an initial diagnosis, there are few quantitative technologies available to provide sensitive subclinical feedback to patients regarding improvements or setbacks to their knee health status; instead, most assessments are qualitative, relying on patient-reported symptoms, performance during functional tests, and physical examinations. Recent advances have been made with wearable technologies for assessing the health status of the knee (and potentially other joints) with the goal of facilitating personalized rehabilitation of injuries and care for chronic conditions. This review describes our progress in developing wearable sensing technologies that enable quantitative physiological measurements and interpretation of knee health status. Our sensing system enables longitudinal quantitative measurements of knee sounds, swelling, and activity context during clinical and field situations. Importantly, we leverage machine-learning algorithms to fuse the low-level signal and feature data of the measured time series waveforms into higher level metrics of joint health. This paper summarizes the engineering validation, baseline physiological experiments, and human subject studies-both cross-sectional and longitudinal-that demonstrate the efficacy of using such systems for robust knee joint health assessment. We envision our sensor system complementing and advancing present-day practices to reduce joint reinjury risk, to optimize rehabilitation recovery time for a quicker return to activity, and to reduce health care costs."
28751369,4.0,Wearable technology for compensatory reserve to sense hypovolemia,2018 Feb 1;124(2):442-451.,"Traditional monitoring technologies fail to provide accurate or early indications of hypovolemia-mediated extremis because physiological systems (as measured by vital signs) effectively compensate until circulatory failure occurs. Hypovolemia is the most life-threatening physiological condition associated with circulatory shock in hemorrhage or sepsis, and it impairs one's ability to sustain physical exertion during heat stress. This review focuses on the physiology underlying the development of a novel noninvasive wearable technology that allows for real-time evaluation of the cardiovascular system's ability to compensate to hypovolemia, or its compensatory reserve, which provides an individualized estimate of impending circulatory collapse. Compensatory reserve is assessed by real-time changes (sampled millions of times per second) in specific features (hundreds of features) of arterial waveform analog signals that can be obtained from photoplethysmography using machine learning and feature extraction techniques. Extensive experimental evidence employing acute reductions in central blood volume (using lower-body negative pressure, blood withdrawal, heat stress, dehydration) demonstrate that compensatory reserve provides the best indicator for early and accurate assessment for compromises in blood pressure, tissue perfusion, and oxygenation in resting human subjects. Engineering challenges exist for the development of a ruggedized wearable system that can measure signals from multiple sites, improve signal-to-noise ratios, be customized for use in austere conditions (e.g., battlefield, patient transport), and be worn during strenuous physical activity."
28738313,42.0,High throughput phenotyping to accelerate crop breeding and monitoring of diseases in the field,2017 Aug;38:184-192.,"Effective implementation of technology that facilitates accurate and high-throughput screening of thousands of field-grown lines is critical for accelerating crop improvement and breeding strategies for higher yield and disease tolerance. Progress in the development of field-based high throughput phenotyping methods has advanced considerably in the last 10 years through technological progress in sensor development and high-performance computing. Here, we review recent advances in high throughput field phenotyping technologies designed to inform the genetics of quantitative traits, including crop yield and disease tolerance. Successful application of phenotyping platforms to advance crop breeding and identify and monitor disease requires: (1) high resolution of imaging and environmental sensors; (2) quality data products that facilitate computer vision, machine learning and GIS; (3) capacity infrastructure for data management and analysis; and (4) automated environmental data collection. Accelerated breeding for agriculturally relevant crop traits is key to the development of improved varieties and is critically dependent on high-resolution, high-throughput field-scale phenotyping technologies that can efficiently discriminate better performing lines within a larger population and across multiple environments."
28729156,3.0,Multivariate pattern analysis utilizing structural or functional MRI-In individuals with musculoskeletal pain and healthy controls: A systematic review,2017 Dec;47(3):418-431.,"Objective:                    The purpose of this systematic review is to systematically review the evidence relating to findings generated by multivariate pattern analysis (MVPA) following structural or functional magnetic resonance imaging (fMRI) to determine if this analysis is able to: a) Discriminate between individuals with musculoskeletal pain and healthy controls, b) Predict pain perception in healthy individuals stimulated with a noxious stimulus compared to those stimulated with a non-noxious stimulus.              Methods:                    MEDLINE, CINAHL, Embase, PEDro, Google Scholar, Cochrane library and Web of Science were systematically screened for relevant literature using different combinations of keywords regarding structural and functional MRI analysed with MVPA, both in individuals with musculoskeletal pain and healthy controls. Reference lists of included articles were hand-searched for additional literature. Eligible articles were assessed on risk of bias and reviewed by two independent researchers.              Results:                    The search query returned 18 articles meeting the inclusion criteria. Methodological quality varied from poor to good. Seven studies investigated the ability of machine-learning algorithms to differentiate patient groups from healthy control participants. Overall, the review demonstrated that MVPA can discriminate between individuals with MSK pain and healthy controls with an overall accuracy ranging from 53% to 94%. Twelve studies utilized healthy control participants (using them as their own controls), during experimental pain paradigms aimed to investigate the ability of machine-learning to differentiate individuals stimulated with noxious stimuli from those stimulated with non-noxious stimuli, with 'pain' detection rates ranging from 60% to 94%. However, significant heterogeneity in patient conditions, study methodology and brain imaging techniques resulted in various findings that make study comparisons and formal conclusions challenging.              Conclusion:                    There is preliminary and emerging evidence that MVPA analyses of structural or functional MRI are able to discriminate between patients and healthy controls, and also discriminate between noxious and non-noxious stimulation. No prospective studies were found in this review to allow determination of the prognostic or diagnostic capabilities or treatment responsiveness of these analyses. Future studies would also benefit from combining various behavioural, genotype and phenotype data into analyses to assist with development of sensitive and specific signatures that could guide future individualized patient treatment options and evaluate how treatments exert their effects."
28728937,24.0,The impact of machine learning techniques in the study of bipolar disorder: A systematic review,2017 Sep;80:538-554.,"Machine learning techniques provide new methods to predict diagnosis and clinical outcomes at an individual level. We aim to review the existing literature on the use of machine learning techniques in the assessment of subjects with bipolar disorder. We systematically searched PubMed, Embase and Web of Science for articles published in any language up to January 2017. We found 757 abstracts and included 51 studies in our review. Most of the included studies used multiple levels of biological data to distinguish the diagnosis of bipolar disorder from other psychiatric disorders or healthy controls. We also found studies that assessed the prediction of clinical outcomes and studies using unsupervised machine learning to build more consistent clinical phenotypes of bipolar disorder. We concluded that given the clinical heterogeneity of samples of patients with BD, machine learning techniques may provide clinicians and researchers with important insights in fields such as diagnosis, personalized treatment and prognosis orientation."
28728899,22.0,Machine learning-enabled discovery and design of membrane-active peptides,2018 Jun 1;26(10):2708-2718.,"Antimicrobial peptides are a class of membrane-active peptides that form a critical component of innate host immunity and possess a diversity of sequence and structure. Machine learning approaches have been profitably employed to efficiently screen sequence space and guide experiment towards promising candidates with high putative activity. In this mini-review, we provide an introduction to antimicrobial peptides and summarize recent advances in machine learning-enabled antimicrobial peptide discovery and design with a focus on a recent work Lee et al. Proc. Natl. Acad. Sci. USA 2016;113(48):13588-13593. This study reports the development of a support vector machine classifier to aid in the design of membrane active peptides. We use this model to discover membrane activity as a multiplexed function in diverse peptide families and provide interpretable understanding of the physicochemical properties and mechanisms governing membrane activity. Experimental validation of the classifier reveals it to have learned membrane activity as a unifying signature of antimicrobial peptides with diverse modes of action. Some of the discriminating rules by which it performs classification are in line with existing ""human learned"" understanding, but it also unveils new previously unknown determinants and multidimensional couplings governing membrane activity. Integrating machine learning with targeted experimentation can guide both antimicrobial peptide discovery and design and new understanding of the properties and mechanisms underpinning their modes of action."
28716627,19.0,Membrane proteins structures: A review on computational modeling tools,2017 Oct;1859(10):2021-2039.,"Background:                    Membrane proteins (MPs) play diverse and important functions in living organisms. They constitute 20% to 30% of the known bacterial, archaean and eukaryotic organisms' genomes. In humans, their importance is emphasized as they represent 50% of all known drug targets. Nevertheless, experimental determination of their three-dimensional (3D) structure has proven to be both time consuming and rather expensive, which has led to the development of computational algorithms to complement the available experimental methods and provide valuable insights.              Scope of review:                    This review highlights the importance of membrane proteins and how computational methods are capable of overcoming challenges associated with their experimental characterization. It covers various MP structural aspects, such as lipid interactions, allostery, and structure prediction, based on methods such as Molecular Dynamics (MD) and Machine-Learning (ML).              Major conclusions:                    Recent developments in algorithms, tools and hybrid approaches, together with the increase in both computational resources and the amount of available data have resulted in increasingly powerful and trustworthy approaches to model MPs.              General significance:                    Even though MPs are elementary and important in nature, the determination of their 3D structure has proven to be a challenging endeavor. Computational methods provide a reliable alternative to experimental methods. In this review, we focus on computational techniques to determine the 3D structure of MP and characterize their binding interfaces. We also summarize the most relevant databases and software programs available for the study of MPs."
28715343,9.0,Deep Belief Networks for Electroencephalography: A Review of Recent Contributions and Future Outlooks,2018 May;22(3):642-652.,"Deep learning, a relatively new branch of machine learning, has been investigated for use in a variety of biomedical applications. Deep learning algorithms have been used to analyze different physiological signals and gain a better understanding of human physiology for automated diagnosis of abnormal conditions. In this paper, we provide an overview of deep learning approaches with a focus on deep belief networks in electroencephalography applications. We investigate the state-of-the-art algorithms for deep belief networks and then cover the application of these algorithms and their performances in electroencephalographic applications. We covered various applications of electroencephalography in medicine, including emotion recognition, sleep stage classification, and seizure detection, in order to understand how deep learning algorithms could be modified to better suit the tasks desired. This review is intended to provide researchers with a broad overview of the currently existing deep belief network methodology for electroencephalography signals, as well as to highlight potential challenges for future research."
28711211,6.0,Advanced Tissue Characterization and Texture Analysis Using Dual-Energy Computed Tomography: Horizons and Emerging Applications,2017 Aug;27(3):533-546.,"In the last article of this issue, advanced analysis capabilities of DECT is reviewed, including spectral Hounsfield unit attenuation curves, virtual monochromatic images, material decomposition maps, tissue effective Z determination, and other advanced post-processing DECT tools, followed by different methods of analysis of the attenuation curves generated using DECT. The article concludes with exciting future horizons and potential applications, such as the use of the rich quantitative data in dual energy CT scans for texture or radiomic analysis and the use of machine learning methods for generation of prediction models using spectral data."
28699534,1.0,Discovery and Development of ATP-Competitive mTOR Inhibitors Using Computational Approaches,2017 Nov 16;23(29):4321-4331.,"The mammalian target of rapamycin (mTOR) is a central controller of cell growth, proliferation, metabolism, and angiogenesis. This protein is an attractive target for new anticancer drug development. Significant progress has been made in hit discovery, lead optimization, drug candidate development and determination of the three-dimensional (3D) structure of mTOR. Computational methods have been applied to accelerate the discovery and development of mTOR inhibitors helping to model the structure of mTOR, screen compound databases, uncover structure-activity relationship (SAR) and optimize the hits, mine the privileged fragments and design focused libraries. Besides, computational approaches were also applied to study protein-ligand interactions mechanisms and in natural product-driven drug discovery. Herein, we survey the most recent progress on the application of computational approaches to advance the discovery and development of compounds targeting mTOR. Future directions in the discovery of new mTOR inhibitors using computational methods are also discussed."
28694872,1.0,"Biomimetic molecular design tools that learn, evolve, and adapt",2017 Jun 29;13:1288-1302.,"A dominant hallmark of living systems is their ability to adapt to changes in the environment by learning and evolving. Nature does this so superbly that intensive research efforts are now attempting to mimic biological processes. Initially this biomimicry involved developing synthetic methods to generate complex bioactive natural products. Recent work is attempting to understand how molecular machines operate so their principles can be copied, and learning how to employ biomimetic evolution and learning methods to solve complex problems in science, medicine and engineering. Automation, robotics, artificial intelligence, and evolutionary algorithms are now converging to generate what might broadly be called in silico-based adaptive evolution of materials. These methods are being applied to organic chemistry to systematize reactions, create synthesis robots to carry out unit operations, and to devise closed loop flow self-optimizing chemical synthesis systems. Most scientific innovations and technologies pass through the well-known ""S curve"", with slow beginning, an almost exponential growth in capability, and a stable applications period. Adaptive, evolving, machine learning-based molecular design and optimization methods are approaching the period of very rapid growth and their impact is already being described as potentially disruptive. This paper describes new developments in biomimetic adaptive, evolving, learning computational molecular design methods and their potential impacts in chemistry, engineering, and medicine."
28693857,5.0,How Not To Drown in Data: A Guide for Biomaterial Engineers,2017 Aug;35(8):743-755.,"High-throughput assays that produce hundreds of measurements per sample are powerful tools for quantifying cell-material interactions. With advances in automation and miniaturization in material fabrication, hundreds of biomaterial samples can be rapidly produced, which can then be characterized using these assays. However, the resulting deluge of data can be overwhelming. To the rescue are computational methods that are well suited to these problems. Machine learning techniques provide a vast array of tools to make predictions about cell-material interactions and to find patterns in cellular responses. Computational simulations allow researchers to pose and test hypotheses and perform experiments in silico. This review describes approaches from these two domains that can be brought to bear on the problem of analyzing biomaterial screening data."
28691131,1.0,A survey of context recognition in surgery,2017 Oct;55(10):1719-1734.,"With the introduction of operating rooms of the future context awareness has gained importance in the surgical environment. This paper organizes and reviews different approaches for recognition of context in surgery. Major electronic research databases were queried to obtain relevant publications submitted between the years 2010 and 2015. Three different types of context were identified: (i) the surgical workflow context, (ii) surgeon's cognitive and (iii) technical state context. A total of 52 relevant studies were identified and grouped based on the type of context detected and sensors used. Different approaches were summarized to provide recommendations for future research. There is still room for improvement in terms of methods used and evaluations performed. Machine learning should be used more extensively to uncover hidden relationships between different properties of the surgeon's state, particularly when performing cognitive context recognition. Furthermore, validation protocols should be improved by performing more evaluations in situ and with a higher number of unique participants. The paper also provides a structured outline of recent context recognition methods to facilitate development of new generation context-aware surgical support systems."
28689314,84.0,Overview of deep learning in medical imaging,2017 Sep;10(3):257-273.,"The use of machine learning (ML) has been increasing rapidly in the medical imaging field, including computer-aided diagnosis (CAD), radiomics, and medical image analysis. Recently, an ML area called deep learning emerged in the computer vision field and became very popular in many fields. It started from an event in late 2012, when a deep-learning approach based on a convolutional neural network (CNN) won an overwhelming victory in the best-known worldwide computer vision competition, ImageNet Classification. Since then, researchers in virtually all fields, including medical imaging, have started actively participating in the explosively growing field of deep learning. In this paper, the area of deep learning in medical imaging is overviewed, including (1) what was changed in machine learning before and after the introduction of deep learning, (2) what is the source of the power of deep learning, (3) two major deep-learning models: a massive-training artificial neural network (MTANN) and a convolutional neural network (CNN), (4) similarities and differences between the two models, and (5) their applications to medical imaging. This review shows that ML with feature input (or feature-based ML) was dominant before the introduction of deep learning, and that the major and essential difference between ML before and after deep learning is the learning of image data directly without object segmentation or feature extraction; thus, it is the source of the power of deep learning, although the depth of the model is an important attribute. The class of ML with image input (or image-based ML) including deep learning has a long history, but recently gained popularity due to the use of the new terminology, deep learning. There are two major models in this class of ML in medical imaging, MTANN and CNN, which have similarities as well as several differences. In our experience, MTANNs were substantially more efficient in their development, had a higher performance, and required a lesser number of training cases than did CNNs. ""Deep learning"", or ML with image input, in medical imaging is an explosively growing, promising field. It is expected that ML with image input will be the mainstream area in the field of medical imaging in the next few decades."
30202589,5.0,Big Data in traumatic brain injury; promise and challenges,2017 Jul 10;2(4):CNC45.,"Traumatic brain injury (TBI) is a spectrum disease of overwhelming complexity, the research of which generates enormous amounts of structured, semi-structured and unstructured data. This resulting big data has tremendous potential to be mined for valuable information regarding the ""most complex disease of the most complex organ"". Big data analyses require specialized big data analytics applications, machine learning and artificial intelligence platforms to reveal associations, trends, correlations and patterns not otherwise realized by current analytical approaches. The intersection of potential data sources between experimental TBI and clinical TBI research presents inherent challenges for setting parameters for the generation of common data elements and to mine existing legacy data that would allow highly translatable big data analyses. In order to successfully utilize big data analyses in TBI, we must be willing to accept the messiness of data, collect and store all data and give up causation for correlation. In this context, coupling the big data approach to established clinical and pre-clinical data sources will transform current practices for triage, diagnosis, treatment and prognosis into highly integrated evidence-based patient care."
28681133,11.0,Innovative Clinical Trial Designs for Precision Medicine in Heart Failure with Preserved Ejection Fraction,2017 Jun;10(3):322-336.,"A major challenge in the care of patients with heart failure and preserved ejection fraction (HFpEF) is the lack of proven therapies due to disappointing results from randomized controlled trials (RCTs). The heterogeneity of the HFpEF syndrome and the use of conventional RCT designs are possible reasons underlying the failure of these trials. There are several factors-including the widespread adoption of electronic health records, decreasing costs of obtaining high-dimensional data, and the availability of a wide variety of potential therapeutics-that have evolved to enable more innovative clinical trial designs in HFpEF. Here, we review the current landscape of HFpEF RCTs and present several innovative RCT designs that could be implemented in HFpEF, including enrichment trials, adaptive trials, umbrella trials, basket trials, and machine learning-based trials (including examples for each). Our hope is that the description of the aforementioned innovative trial designs will stimulate new approaches to clinical trials in HFpEF."
28672838,14.0,Recent Advances in Conotoxin Classification by Using Machine Learning Methods,2017 Jun 25;22(7):1057.,"Conotoxins are disulfide-rich small peptides, which are invaluable peptides that target ion channel and neuronal receptors. Conotoxins have been demonstrated as potent pharmaceuticals in the treatment of a series of diseases, such as Alzheimer's disease, Parkinson's disease, and epilepsy. In addition, conotoxins are also ideal molecular templates for the development of new drug lead compounds and play important roles in neurobiological research as well. Thus, the accurate identification of conotoxin types will provide key clues for the biological research and clinical medicine. Generally, conotoxin types are confirmed when their sequence, structure, and function are experimentally validated. However, it is time-consuming and costly to acquire the structure and function information by using biochemical experiments. Therefore, it is important to develop computational tools for efficiently and effectively recognizing conotoxin types based on sequence information. In this work, we reviewed the current progress in computational identification of conotoxins in the following aspects: (i) construction of benchmark dataset; (ii) strategies for extracting sequence features; (iii) feature selection techniques; (iv) machine learning methods for classifying conotoxins; (v) the results obtained by these methods and the published tools; and (vi) future perspectives on conotoxin classification. The paper provides the basis for in-depth study of conotoxins and drug therapy research."
28670152,116.0,Deep Learning in Medical Imaging: General Overview,Jul-Aug 2017;18(4):570-584.,"The artificial neural network (ANN)-a machine learning technique inspired by the human neuronal synapse system-was introduced in the 1950s. However, the ANN was previously limited in its ability to solve actual problems, due to the vanishing gradient and overfitting problems with training of deep architecture, lack of computing power, and primarily the absence of sufficient data to train the computer system. Interest in this concept has lately resurfaced, due to the availability of big data, enhanced computing power with the current graphics processing units, and novel algorithms to train the deep neural network. Recent studies on this technology suggest its potentially to perform better than humans in some visual and auditory recognition tasks, which may portend its applications in medicine and healthcare, especially in medical imaging, in the foreseeable future. This review article offers perspectives on the history, development, and applications of deep learning technology, particularly regarding its applications in medical imaging."
28667600,2.0,Protein Sorting Prediction,2017;1615:23-57.,"Many computational methods are available for predicting protein sorting in bacteria. When comparing them, it is important to know that they can be grouped into three fundamentally different approaches: signal-based, global-property-based and homology-based prediction. In this chapter, the strengths and drawbacks of each of these approaches is described through many examples of methods that predict secretion, integration into membranes, or subcellular locations in general. The aim of this chapter is to provide a user-level introduction to the field with a minimum of computational theory."
28666178,34.0,A systematic review of gait analysis methods based on inertial sensors and adaptive algorithms,2017 Sep;57:204-210.,"The conventional methods to assess human gait are either expensive or complex to be applied regularly in clinical practice. To reduce the cost and simplify the evaluation, inertial sensors and adaptive algorithms have been utilized, respectively. This paper aims to summarize studies that applied adaptive also called artificial intelligence (AI) algorithms to gait analysis based on inertial sensor data, verifying if they can support the clinical evaluation. Articles were identified through searches of the main databases, which were encompassed from 1968 to October 2016. We have identified 22 studies that met the inclusion criteria. The included papers were analyzed due to their data acquisition and processing methods with specific questionnaires. Concerning the data acquisition, the mean score is 6.1±1.62, what implies that 13 of 22 papers failed to report relevant outcomes. The quality assessment of AI algorithms presents an above-average rating (8.2±1.84). Therefore, AI algorithms seem to be able to support gait analysis based on inertial sensor data. Further research, however, is necessary to enhance and standardize the application in patients, since most of the studies used distinct methods to evaluate healthy subjects."
28663789,32.0,Molecular and phenotypic biomarkers of aging,2017 Jun 9;6:860.,"Individuals of the same age may not age at the same rate. Quantitative biomarkers of aging are valuable tools to measure physiological age, assess the extent of 'healthy aging', and potentially predict health span and life span for an individual. Given the complex nature of the aging process, the biomarkers of aging are multilayered and multifaceted. Here, we review the phenotypic and molecular biomarkers of aging. Identifying and using biomarkers of aging to improve human health, prevent age-associated diseases, and extend healthy life span are now facilitated by the fast-growing capacity of multilevel cross-sectional and longitudinal data acquisition, storage, and analysis, particularly for data related to general human populations. Combined with artificial intelligence and machine learning techniques, reliable panels of biomarkers of aging will have tremendous potential to improve human health in aging societies."
28663166,33.0,Researching Mental Health Disorders in the Era of Social Media: Systematic Review,2017 Jun 29;19(6):e228.,"Background:                    Mental illness is quickly becoming one of the most prevalent public health problems worldwide. Social network platforms, where users can express their emotions, feelings, and thoughts, are a valuable source of data for researching mental health, and techniques based on machine learning are increasingly used for this purpose.              Objective:                    The objective of this review was to explore the scope and limits of cutting-edge techniques that researchers are using for predictive analytics in mental health and to review associated issues, such as ethical concerns, in this area of research.              Methods:                    We performed a systematic literature review in March 2017, using keywords to search articles on data mining of social network data in the context of common mental health disorders, published between 2010 and March 8, 2017 in medical and computer science journals.              Results:                    The initial search returned a total of 5386 articles. Following a careful analysis of the titles, abstracts, and main texts, we selected 48 articles for review. We coded the articles according to key characteristics, techniques used for data collection, data preprocessing, feature extraction, feature selection, model construction, and model verification. The most common analytical method was text analysis, with several studies using different flavors of image analysis and social interaction graph analysis.              Conclusions:                    Despite an increasing number of studies investigating mental health issues using social network data, some common problems persist. Assembling large, high-quality datasets of social media users with mental disorder is problematic, not only due to biases associated with the collection methods, but also with regard to managing consent and selecting appropriate analytics techniques."
28662371,4.0,Modeling food matrix effects on chemical reactivity: Challenges and perspectives,2018;58(16):2814-2828.,"The same chemical reaction may be different in terms of its position of the equilibrium (i.e., thermodynamics) and its kinetics when studied in different foods. The diversity in the chemical composition of food and in its structural organization at macro-, meso-, and microscopic levels, that is, the food matrix, is responsible for this difference. In this viewpoint paper, the multiple, and interconnected ways the food matrix can affect chemical reactivity are summarized. Moreover, mechanistic and empirical approaches to explain and predict the effect of food matrix on chemical reactivity are described. Mechanistic models aim to quantify the effect of food matrix based on a detailed understanding of the chemical and physical phenomena occurring in food. Their applicability is limited at the moment to very simple food systems. Empirical modeling based on machine learning combined with data-mining techniques may represent an alternative, useful option to predict the effect of the food matrix on chemical reactivity and to identify chemical and physical properties to be further tested. In such a way the mechanistic understanding of the effect of the food matrix on chemical reactions can be improved."
28662070,13.0,Machine learning and microsimulation techniques on the prognosis of dementia: A systematic literature review,2017 Jun 29;12(6):e0179804.,"Background:                    Dementia is a complex disorder characterized by poor outcomes for the patients and high costs of care. After decades of research little is known about its mechanisms. Having prognostic estimates about dementia can help researchers, patients and public entities in dealing with this disorder. Thus, health data, machine learning and microsimulation techniques could be employed in developing prognostic estimates for dementia.              Objective:                    The goal of this paper is to present evidence on the state of the art of studies investigating and the prognosis of dementia using machine learning and microsimulation techniques.              Method:                    To achieve our goal we carried out a systematic literature review, in which three large databases-Pubmed, Socups and Web of Science were searched to select studies that employed machine learning or microsimulation techniques for the prognosis of dementia. A single backward snowballing was done to identify further studies. A quality checklist was also employed to assess the quality of the evidence presented by the selected studies, and low quality studies were removed. Finally, data from the final set of studies were extracted in summary tables.              Results:                    In total 37 papers were included. The data summary results showed that the current research is focused on the investigation of the patients with mild cognitive impairment that will evolve to Alzheimer's disease, using machine learning techniques. Microsimulation studies were concerned with cost estimation and had a populational focus. Neuroimaging was the most commonly used variable.              Conclusions:                    Prediction of conversion from MCI to AD is the dominant theme in the selected studies. Most studies used ML techniques on Neuroimaging data. Only a few data sources have been recruited by most studies and the ADNI database is the one most commonly used. Only two studies have investigated the prediction of epidemiological aspects of Dementia using either ML or MS techniques. Finally, care should be taken when interpreting the reported accuracy of ML techniques, given studies' different contexts."
28657906,17.0,Radiogenomics and radiotherapy response modeling,2017 Aug 1;62(16):R179-R206.,"Advances in patient-specific information and biotechnology have contributed to a new era of computational medicine. Radiogenomics has emerged as a new field that investigates the role of genetics in treatment response to radiation therapy. Radiation oncology is currently attempting to embrace these recent advances and add to its rich history by maintaining its prominent role as a quantitative leader in oncologic response modeling. Here, we provide an overview of radiogenomics starting with genotyping, data aggregation, and application of different modeling approaches based on modifying traditional radiobiological methods or application of advanced machine learning techniques. We highlight the current status and potential for this new field to reshape the landscape of outcome modeling in radiotherapy and drive future advances in computational oncology."
28656130,5.0,"Tuberculosis control, and the where and why of artificial intelligence",2017 Jun 21;3(2):00056-2017.,"Countries aiming to reduce their tuberculosis (TB) burden by 2035 to the levels envisaged by the World Health Organization End TB Strategy need to innovate, with approaches such as digital health (electronic and mobile health) in support of patient care, surveillance, programme management, training and communication. Alongside the large-scale roll-out required for such interventions to make a significant impact, products must stay abreast of advancing technology over time. The integration of artificial intelligence into new software promises to make processes more effective and efficient, endowing them with a potential hitherto unimaginable. Users can benefit from artificial intelligence-enabled pattern recognition software for tasks ranging from reading radiographs to adverse event monitoring, sifting through vast datasets to personalise a patient's care plan or to customise training materials. Many experts forecast the imminent transformation of the delivery of healthcare services. We discuss how artificial intelligence and machine learning could revolutionise the management of TB."
28648568,7.0,Application of machine learning classification for structural brain MRI in mood disorders: Critical review from a clinical perspective,2018 Jan 3;80(Pt B):71-80.,"Mood disorders are a highly prevalent group of mental disorders causing substantial socioeconomic burden. There are various methodological approaches for identifying the underlying mechanisms of the etiology, symptomatology, and therapeutics of mood disorders; however, neuroimaging studies have provided the most direct evidence for mood disorder neural substrates by visualizing the brains of living individuals. The prefrontal cortex, hippocampus, amygdala, thalamus, ventral striatum, and corpus callosum are associated with depression and bipolar disorder. Identifying the distinct and common contributions of these anatomical regions to depression and bipolar disorder have broadened and deepened our understanding of mood disorders. However, the extent to which neuroimaging research findings contribute to clinical practice in the real-world setting is unclear. As traditional or non-machine learning MRI studies have analyzed group-level differences, it is not possible to directly translate findings from research to clinical practice; the knowledge gained pertains to the disorder, but not to individuals. On the other hand, a machine learning approach makes it possible to provide individual-level classifications. For the past two decades, many studies have reported on the classification accuracy of machine learning-based neuroimaging studies from the perspective of diagnosis and treatment response. However, for the application of a machine learning-based brain MRI approach in real world clinical settings, several major issues should be considered. Secondary changes due to illness duration and medication, clinical subtypes and heterogeneity, comorbidities, and cost-effectiveness restrict the generalization of the current machine learning findings. Sophisticated classification of clinical and diagnostic subtypes is needed. Additionally, as the approach is inevitably limited by sample size, multi-site participation and data-sharing are needed in the future."
30062151,13.0,Enabling Precision Cardiology Through Multiscale Biology and Systems Medicine,2017 Jun 26;2(3):311-327.,"The traditional paradigm of cardiovascular disease research derives insight from large-scale, broadly inclusive clinical studies of well-characterized pathologies. These insights are then put into practice according to standardized clinical guidelines. However, stagnation in the development of new cardiovascular therapies and variability in therapeutic response implies that this paradigm is insufficient for reducing the cardiovascular disease burden. In this state-of-the-art review, we examine 3 interconnected ideas we put forth as key concepts for enabling a transition to precision cardiology: 1) precision characterization of cardiovascular disease with machine learning methods; 2) the application of network models of disease to embrace disease complexity; and 3) using insights from the previous 2 ideas to enable pharmacology and polypharmacology systems for more precise drug-to-patient matching and patient-disease stratification. We conclude by exploring the challenges of applying a precision approach to cardiology, which arise from a deficit of the required resources and infrastructure, and emerging evidence for the clinical effectiveness of this nascent approach."
28646349,25.0,Modeling Pain Using fMRI: From Regions to Biomarkers,2018 Feb;34(1):208-215.,"Pain is a subjective and complex phenomenon. Its complexity is related to its heterogeneity: multiple component processes, including sensation, affect, and cognition, contribute to pain experience and reporting. These components are likely to be encoded in distributed brain networks that interact to create pain experience and pain-related decision-making. Therefore, to understand pain, we must identify these networks and build models of these interactions that yield testable predictions about pain-related outcomes. We have developed several such models or 'signatures' of pain, by (1) integrating activity across multiple systems, and (2) using pattern-recognition to identify processes related to pain experience. One model, the Neurologic Pain Signature, is sensitive and specific to pain in individuals, involves brain regions that receive nociceptive afferents, and shows little effect of expectation or self-regulation in tests to date. Another, the 'Stimulus Intensity-Independent Pain Signature', explains substantial additional variation in trial-to-trial pain reports. It involves many brain regions that do not show increased activity in proportion to noxious stimulus intensity, including medial and lateral prefrontal cortex, nucleus accumbens, and hippocampus. Responses in this system mediate expectancy and perceived control effects in several studies. Overall, this approach provides a pathway to understanding pain by identifying multiple systems that track different aspects of pain. Such componential models can be combined in unique ways on a subject-by-subject basis to explain an individual's pain experience."
28643174,33.0,Natural Language Processing for EHR-Based Pharmacovigilance: A Structured Review,2017 Nov;40(11):1075-1089.,"The goal of pharmacovigilance is to detect, monitor, characterize and prevent adverse drug events (ADEs) with pharmaceutical products. This article is a comprehensive structured review of recent advances in applying natural language processing (NLP) to electronic health record (EHR) narratives for pharmacovigilance. We review methods of varying complexity and problem focus, summarize the current state-of-the-art in methodology advancement, discuss limitations and point out several promising future directions. The ability to accurately capture both semantic and syntactic structures in clinical narratives becomes increasingly critical to enable efficient and accurate ADE detection. Significant progress has been made in algorithm development and resource construction since 2000. Since 2012, statistical analysis and machine learning methods have gained traction in automation of ADE mining from EHR narratives. Current state-of-the-art methods for NLP-based ADE detection from EHRs show promise regarding their integration into production pharmacovigilance systems. In addition, integrating multifaceted, heterogeneous data sources has shown promise in improving ADE detection and has become increasingly adopted. On the other hand, challenges and opportunities remain across the frontier of NLP application to EHR-based pharmacovigilance, including proper characterization of ADE context, differentiation between off- and on-label drug-use ADEs, recognition of the importance of polypharmacy-induced ADEs, better integration of heterogeneous data sources, creation of shared corpora, and organization of shared-task challenges to advance the state-of-the-art."
28641555,7.0,Supervised Machine Learning Methods Applied to Predict Ligand- Binding Affinity,2017;24(23):2459-2470.,"Background:                    Calculation of ligand-binding affinity is an open problem in computational medicinal chemistry. The ability to computationally predict affinities has a beneficial impact in the early stages of drug development, since it allows a mathematical model to assess protein-ligand interactions. Due to the availability of structural and binding information, machine learning methods have been applied to generate scoring functions with good predictive power.              Objective:                    Our goal here is to review recent developments in the application of machine learning methods to predict ligand-binding affinity.              Method:                    We focus our review on the application of computational methods to predict binding affinity for protein targets. In addition, we also describe the major available databases for experimental binding constants and protein structures. Furthermore, we explain the most successful methods to evaluate the predictive power of scoring functions.              Results:                    Association of structural information with ligand-binding affinity makes it possible to generate scoring functions targeted to a specific biological system. Through regression analysis, this data can be used as a base to generate mathematical models to predict ligandbinding affinities, such as inhibition constant, dissociation constant and binding energy.              Conclusion:                    Experimental biophysical techniques were able to determine the structures of over 120,000 macromolecules. Considering also the evolution of binding affinity information, we may say that we have a promising scenario for development of scoring functions, making use of machine learning techniques. Recent developments in this area indicate that building scoring functions targeted to the biological systems of interest shows superior predictive performance, when compared with other approaches."
28641107,51.0,The Persistence and Transience of Memory,2017 Jun 21;94(6):1071-1084.,"The predominant focus in the neurobiological study of memory has been on remembering (persistence). However, recent studies have considered the neurobiology of forgetting (transience). Here we draw parallels between neurobiological and computational mechanisms underlying transience. We propose that it is the interaction between persistence and transience that allows for intelligent decision-making in dynamic, noisy environments. Specifically, we argue that transience (1) enhances flexibility, by reducing the influence of outdated information on memory-guided decision-making, and (2) prevents overfitting to specific past events, thereby promoting generalization. According to this view, the goal of memory is not the transmission of information through time, per se. Rather, the goal of memory is to optimize decision-making. As such, transience is as important as persistence in mnemonic systems."
28631139,36.0,A Systematic Review of Wearable Patient Monitoring Systems - Current Challenges and Opportunities for Clinical Adoption,2017 Jul;41(7):115.,"The aim of this review is to investigate barriers and challenges of wearable patient monitoring (WPM) solutions adopted by clinicians in acute, as well as in community, care settings. Currently, healthcare providers are coping with ever-growing healthcare challenges including an ageing population, chronic diseases, the cost of hospitalization, and the risk of medical errors. WPM systems are a potential solution for addressing some of these challenges by enabling advanced sensors, wearable technology, and secure and effective communication platforms between the clinicians and patients. A total of 791 articles were screened and 20 were selected for this review. The most common publication venue was conference proceedings (13, 54%). This review only considered recent studies published between 2015 and 2017. The identified studies involved chronic conditions (6, 30%), rehabilitation (7, 35%), cardiovascular diseases (4, 20%), falls (2, 10%) and mental health (1, 5%). Most studies focussed on the system aspects of WPM solutions including advanced sensors, wireless data collection, communication platform and clinical usability based on a specific area or disease. The current studies are progressing with localized sensor-software integration to solve a specific use-case/health area using non-scalable and 'silo' solutions. There is further work required regarding interoperability and clinical acceptance challenges. The advancement of wearable technology and possibilities of using machine learning and artificial intelligence in healthcare is a concept that has been investigated by many studies. We believe future patient monitoring and medical treatments will build upon efficient and affordable solutions of wearable technology."
28624633,4.0,From flamingo dance to (desirable) drug discovery: a nature-inspired approach,2017 Oct;22(10):1489-1502.,"The therapeutic effects of drugs are well known to result from their interaction with multiple intracellular targets. Accordingly, the pharma industry is currently moving from a reductionist approach based on a 'one-target fixation' to a holistic multitarget approach. However, many drug discovery practices are still procedural abstractions resulting from the attempt to understand and address the action of biologically active compounds while preventing adverse effects. Here, we discuss how drug discovery can benefit from the principles of evolutionary biology and report two real-life case studies. We do so by focusing on the desirability principle, and its many features and applications, such as machine learning-based multicriteria virtual screening."
28614702,26.0,De-identification of psychiatric intake records: Overview of 2016 CEGS N-GRID shared tasks Track 1,2017 Nov;75S:S4-S18.,"The 2016 CEGS N-GRID shared tasks for clinical records contained three tracks. Track 1 focused on de-identification of a new corpus of 1000 psychiatric intake records. This track tackled de-identification in two sub-tracks: Track 1.A was a ""sight unseen"" task, where nine teams ran existing de-identification systems, without any modifications or training, on 600 new records in order to gauge how well systems generalize to new data. The best-performing system for this track scored an F1 of 0.799. Track 1.B was a traditional Natural Language Processing (NLP) shared task on de-identification, where 15 teams had two months to train their systems on the new data, then test it on an unannotated test set. The best-performing system from this track scored an F1 of 0.914. The scores for Track 1.A show that unmodified existing systems do not generalize well to new data without the benefit of training data. The scores for Track 1.B are slightly lower than the 2014 de-identification shared task (which was almost identical to 2016 Track 1.B), indicating that these new psychiatric records pose a more difficult challenge to NLP systems. Overall, de-identification is still not a solved problem, though it is important to the future of clinical NLP."
28588416,,Mathematics delivering the advantage: the role of mathematicians in manufacturing and beyond,2017 May;473(2201):20170094.,"Much has been written about the benefits that mathematics can bring to the UK economy and the manufacturing sector in particular, but less on the value of mathematicians and a mathematical training. This article, written from an industry perspective, considers the value of mathematicians to the UK's industrial base and the importance to the UK economy of encouraging young people in the UK to choose to study mathematics at school as a gateway to a wide range of careers. The points are illustrated using examples from the author's 20 years' experience in the security and intelligence and manufacturing sectors."
28585183,15.0,Precision Medicine for Heart Failure with Preserved Ejection Fraction: An Overview,2017 Jun;10(3):233-244.,"There are few proven therapies for heart failure with preserved ejection fraction (HFpEF). The lack of therapies, along with increased recognition of the disorder and its underlying pathophysiology, has led to the acknowledgement that HFpEF is heterogeneous and is not likely to respond to a one-size-fits-all approach. Thus, HFpEF is a prime candidate to benefit from a precision medicine approach. For this reason, we have assembled a compendium of papers on the topic of precision medicine in HFpEF in the Journal of Cardiovascular Translational Research. These papers cover a variety of topics relevant to precision medicine in HFpEF, including automated identification of HFpEF patients; machine learning, novel molecular approaches, genomics, and deep phenotyping of HFpEF; and clinical trial designs that can be used to advance precision medicine in HFpEF. In this introductory article, we provide an overview of precision medicine in HFpEF with the hope that the work described here and in the other papers in this special theme issue will stimulate investigators and clinicians to advance a more targeted approach to HFpEF classification and treatment."
28577131,99.0,Deep Learning for Brain MRI Segmentation: State of the Art and Future Directions,2017 Aug;30(4):449-459.,"Quantitative analysis of brain MRI is routine for many neurological diseases and conditions and relies on accurate segmentation of structures of interest. Deep learning-based segmentation approaches for brain MRI are gaining interest due to their self-learning and generalization ability over large amounts of data. As the deep learning architectures are becoming more mature, they gradually outperform previous state-of-the-art classical machine learning algorithms. This review aims to provide an overview of current deep learning-based segmentation approaches for quantitative brain MRI. First we review the current deep learning architectures used for segmentation of anatomical brain structures and brain lesions. Next, the performance, speed, and properties of deep learning approaches are summarized and discussed. Finally, we provide a critical assessment of the current state and identify likely future developments and trends."
28570593,12.0,A review of active learning approaches to experimental design for uncovering biological networks,2017 Jun 1;13(6):e1005466.,"Various types of biological knowledge describe networks of interactions among elementary entities. For example, transcriptional regulatory networks consist of interactions among proteins and genes. Current knowledge about the exact structure of such networks is highly incomplete, and laboratory experiments that manipulate the entities involved are conducted to test hypotheses about these networks. In recent years, various automated approaches to experiment selection have been proposed. Many of these approaches can be characterized as active machine learning algorithms. Active learning is an iterative process in which a model is learned from data, hypotheses are generated from the model to propose informative experiments, and the experiments yield new data that is used to update the model. This review describes the various models, experiment selection strategies, validation techniques, and successful applications described in the literature; highlights common themes and notable distinctions among methods; and identifies likely directions of future research and open problems in the area."
28552969,15.0,Automatic adventitious respiratory sound analysis: A systematic review,2017 May 26;12(5):e0177926.,"Background:                    Automatic detection or classification of adventitious sounds is useful to assist physicians in diagnosing or monitoring diseases such as asthma, Chronic Obstructive Pulmonary Disease (COPD), and pneumonia. While computerised respiratory sound analysis, specifically for the detection or classification of adventitious sounds, has recently been the focus of an increasing number of studies, a standardised approach and comparison has not been well established.              Objective:                    To provide a review of existing algorithms for the detection or classification of adventitious respiratory sounds. This systematic review provides a complete summary of methods used in the literature to give a baseline for future works.              Data sources:                    A systematic review of English articles published between 1938 and 2016, searched using the Scopus (1938-2016) and IEEExplore (1984-2016) databases. Additional articles were further obtained by references listed in the articles found. Search terms included adventitious sound detection, adventitious sound classification, abnormal respiratory sound detection, abnormal respiratory sound classification, wheeze detection, wheeze classification, crackle detection, crackle classification, rhonchi detection, rhonchi classification, stridor detection, stridor classification, pleural rub detection, pleural rub classification, squawk detection, and squawk classification.              Study selection:                    Only articles were included that focused on adventitious sound detection or classification, based on respiratory sounds, with performance reported and sufficient information provided to be approximately repeated.              Data extraction:                    Investigators extracted data about the adventitious sound type analysed, approach and level of analysis, instrumentation or data source, location of sensor, amount of data obtained, data management, features, methods, and performance achieved.              Data synthesis:                    A total of 77 reports from the literature were included in this review. 55 (71.43%) of the studies focused on wheeze, 40 (51.95%) on crackle, 9 (11.69%) on stridor, 9 (11.69%) on rhonchi, and 18 (23.38%) on other sounds such as pleural rub, squawk, as well as the pathology. Instrumentation used to collect data included microphones, stethoscopes, and accelerometers. Several references obtained data from online repositories or book audio CD companions. Detection or classification methods used varied from empirically determined thresholds to more complex machine learning techniques. Performance reported in the surveyed works were converted to accuracy measures for data synthesis.              Limitations:                    Direct comparison of the performance of surveyed works cannot be performed as the input data used by each was different. A standard validation method has not been established, resulting in different works using different methods and performance measure definitions.              Conclusion:                    A review of the literature was performed to summarise different analysis approaches, features, and methods used for the analysis. The performance of recent studies showed a high agreement with conventional non-automatic identification. This suggests that automated adventitious sound detection or classification is a promising solution to overcome the limitations of conventional auscultation and to assist in the monitoring of relevant diseases."
28545640,120.0,Artificial Intelligence in Precision Cardiovascular Medicine,2017 May 30;69(21):2657-2664.,"Artificial intelligence (AI) is a field of computer science that aims to mimic human thought processes, learning capacity, and knowledge storage. AI techniques have been applied in cardiovascular medicine to explore novel genotypes and phenotypes in existing diseases, improve the quality of patient care, enable cost-effectiveness, and reduce readmission and mortality rates. Over the past decade, several machine-learning techniques have been used for cardiovascular disease diagnosis and prediction. Each problem requires some degree of understanding of the problem, in terms of cardiovascular medicine and statistics, to apply the optimal machine-learning algorithm. In the near future, AI will result in a paradigm shift toward precision cardiovascular medicine. The potential of AI in cardiovascular medicine is tremendous; however, ignorance of the challenges may overshadow its potential clinical impact. This paper gives a glimpse of AI's application in cardiovascular clinical care and discusses its potential role in facilitating precision cardiovascular medicine."
28544059,11.0,Objective assessment of the evolutionary action equation for the fitness effect of missense mutations across CAGI-blinded contests,2017 Sep;38(9):1072-1084.,"A major challenge in genome interpretation is to estimate the fitness effect of coding variants of unknown significance (VUS). Labor, limited understanding of protein functions, and lack of assays generally limit direct experimental assessment of VUS, and make robust and accurate computational approaches a necessity. Often, however, algorithms that predict mutational effect disagree among themselves and with experimental data, slowing their adoption for clinical diagnostics. To objectively assess such methods, the Critical Assessment of Genome Interpretation (CAGI) community organizes contests to predict unpublished experimental data, available only to CAGI assessors. We review here the CAGI performance of evolutionary action (EA) predictions of mutational impact. EA models the fitness effect of coding mutations analytically, as a product of the gradient of the fitness landscape times the perturbation size. In practice, these terms are computed from phylogenetic considerations as the functional sensitivity of the mutated site and as the magnitude of amino acid substitution, respectively, and yield the percentage loss of wild-type activity. In five CAGI challenges, EA consistently performed on par or better than sophisticated machine learning approaches. This objective assessment suggests that a simple differential model of evolution can interpret the fitness effect of coding variations, opening diverse clinical applications."
28540688,,"Machine Learning Techniques in Exploring MicroRNA Gene Discovery, Targets, and Functions",2017;1617:211-224.,"In recent years, the role of miRNAs in post-transcriptional gene regulation has provided new insights into the understanding of several types of cancers and neurological disorders. Although miRNA research has gathered great momentum since its discovery, traditional biological methods for finding miRNA genes and targets continue to remain a huge challenge due to the laborious tasks and extensive time involved. Fortunately, advances in computational methods have yielded considerable improvements in miRNA studies. This literature review briefly discusses recent machine learning-based techniques applied in the discovery of miRNAs, prediction of miRNA targets, and inference of miRNA functions. We also discuss the limitations of how these approaches have been elucidated in previous studies."
28538142,11.0,An Introduction to Infinite HMMs for Single-Molecule Data Analysis,2017 May 23;112(10):2021-2029.,"The hidden Markov model (HMM) has been a workhorse of single-molecule data analysis and is now commonly used as a stand-alone tool in time series analysis or in conjunction with other analysis methods such as tracking. Here, we provide a conceptual introduction to an important generalization of the HMM, which is poised to have a deep impact across the field of biophysics: the infinite HMM (iHMM). As a modeling tool, iHMMs can analyze sequential data without a priori setting a specific number of states as required for the traditional (finite) HMM. Although the current literature on the iHMM is primarily intended for audiences in statistics, the idea is powerful and the iHMM's breadth in applicability outside machine learning and data science warrants a careful exposition. Here, we explain the key ideas underlying the iHMM, with a special emphasis on implementation, and provide a description of a code we are making freely available. In a companion article, we provide an important extension of the iHMM to accommodate complications such as drift."
28530547,4.0,Application of Machine Learning Approaches for Protein-protein Interactions Prediction,2017;13(6):506-514.,"Background:                    Proteomics endeavors to study the structures, functions and interactions of proteins. Information of the protein-protein interactions (PPIs) helps to improve our knowledge of the functions and the 3D structures of proteins. Thus determining the PPIs is essential for the study of the proteomics.              Objective:                    In this review, in order to study the application of machine learning in predicting PPI, some machine learning approaches such as support vector machine (SVM), artificial neural networks (ANNs) and random forest (RF) were selected, and the examples of its applications in PPIs were listed.              Results:                    SVM and RF are two commonly used methods. Nowadays, more researchers predict PPIs by combining more than two methods.              Conclusion:                    This review presents the application of machine learning approaches in predicting PPI. Many examples of success in identification and prediction in the area of PPI prediction have been discussed, and the PPIs research is still in progress."
28527522,4.0,Precision Medicine: Genomic Profiles to Individualize Therapy,2017 Aug;50(4):765-773.,"Precision medicine is the application of genotypic and Omics biomarkers to determine the most appropriate, outcome-driven therapy for individual patients. To determine the best choice of therapy, institutions use significant information technology-enabled data from imaging, electronic medical records, sensors in the clinic/hospitals, and wearable sensors to determine treatment response. With genomic profiling, targets to affect a disease course are continuing to be developed. As clonal mutational prevalence continues to be understood, information can be communicated to patients to inform them that resistance is common, requiring collection of more genetic mutations from patients with further biopsies or blood collection."
28524769,14.0,Machine learning for epigenetics and future medical applications,2017 Jul 3;12(7):505-514.,Understanding epigenetic processes holds immense promise for medical applications. Advances in Machine Learning (ML) are critical to realize this promise. Previous studies used epigenetic data sets associated with the germline transmission of epigenetic transgenerational inheritance of disease and novel ML approaches to predict genome-wide locations of critical epimutations. A combination of Active Learning (ACL) and Imbalanced Class Learning (ICL) was used to address past problems with ML to develop a more efficient feature selection process and address the imbalance problem in all genomic data sets. The power of this novel ML approach and our ability to predict epigenetic phenomena and associated disease is suggested. The current approach requires extensive computation of features over the genome. A promising new approach is to introduce Deep Learning (DL) for the generation and simultaneous computation of novel genomic features tuned to the classification task. This approach can be used with any genomic or biological data set applied to medicine. The application of molecular epigenetic data in advanced machine learning analysis to medicine is the focus of this review.
28520598,19.0,Imaging plus X: multimodal models of neurodegenerative disease,2017 Aug;30(4):371-379.,"Purpose of review:                    This article argues that the time is approaching for data-driven disease modelling to take centre stage in the study and management of neurodegenerative disease. The snowstorm of data now available to the clinician defies qualitative evaluation; the heterogeneity of data types complicates integration through traditional statistical methods; and the large datasets becoming available remain far from the big-data sizes necessary for fully data-driven machine-learning approaches. The recent emergence of data-driven disease progression models provides a balance between imposed knowledge of disease features and patterns learned from data. The resulting models are both predictive of disease progression in individual patients and informative in terms of revealing underlying biological patterns.              Recent findings:                    Largely inspired by observational models, data-driven disease progression models have emerged in the last few years as a feasible means for understanding the development of neurodegenerative diseases. These models have revealed insights into frontotemporal dementia, Huntington's disease, multiple sclerosis, Parkinson's disease and other conditions. For example, event-based models have revealed finer graded understanding of progression patterns; self-modelling regression and differential equation models have provided data-driven biomarker trajectories; spatiotemporal models have shown that brain shape changes, for example of the hippocampus, can occur before detectable neurodegeneration; and network models have provided some support for prion-like mechanistic hypotheses of disease propagation. The most mature results are in sporadic Alzheimer's disease, in large part because of the availability of the Alzheimer's disease neuroimaging initiative dataset. Results generally support the prevailing amyloid-led hypothetical model of Alzheimer's disease, while revealing finer detail and insight into disease progression.              Summary:                    The emerging field of disease progression modelling provides a natural mechanism to integrate different kinds of information, for example from imaging, serum and cerebrospinal fluid markers and cognitive tests, to obtain new insights into progressive diseases. Such insights include fine-grained longitudinal patterns of neurodegeneration, from early stages, and the heterogeneity of these trajectories over the population. More pragmatically, such models enable finer precision in patient staging and stratification, prediction of progression rates and earlier and better identification of at-risk individuals. We argue that this will make disease progression modelling invaluable for recruitment and end-points in future clinical trials, potentially ameliorating the high failure rate in trials of, e.g., Alzheimer's disease therapies. We review the state of the art in these techniques and discuss the future steps required to translate the ideas to front-line application."
28520235,24.0,First Principles Neural Network Potentials for Reactive Simulations of Large Molecular and Condensed Systems,2017 Oct 9;56(42):12828-12840.,"Modern simulation techniques have reached a level of maturity which allows a wide range of problems in chemistry and materials science to be addressed. Unfortunately, the application of first principles methods with predictive power is still limited to rather small systems, and despite the rapid evolution of computer hardware no fundamental change in this situation can be expected. Consequently, the development of more efficient but equally reliable atomistic potentials to reach an atomic level understanding of complex systems has received considerable attention in recent years. A promising new development has been the introduction of machine learning (ML) methods to describe the atomic interactions. Once trained with electronic structure data, ML potentials can accelerate computer simulations by several orders of magnitude, while preserving quantum mechanical accuracy. This Review considers the methodology of an important class of ML potentials that employs artificial neural networks."
28508479,2.0,Prediction of complex traits: Conciliating genetics and statistics,2017 Jun;134(3):178-183.,"This review focuses on methods used to predict complex traits. Main characteristics of prediction approaches are given: the deterministic or stochastic nature of prediction, the objects of prediction, the sources of information and the main statistical methods. Sources of information discussed are the traditional genealogies and phenotypes, nucleotide sequences, expression data and epigenetics marks. Statistical methods are presented as successive degrees of generalization from the definition of the conditional expectation as the prediction rule, to best linear unbiased prediction, then Bayesian and, recently, machine learning methods, including meta-methods. We highlight the contributions of Daniel Gianola to this methodological evolution."
28505027,4.0,Automated surveillance of healthcare-associated infections: state of the art,2017 Aug;30(4):425-431.,"Purpose of review:                    This review describes recent advances in the field of automated surveillance of healthcare-associated infections (HAIs), with a focus on data sources and the development of semiautomated or fully automated algorithms.              Recent findings:                    The availability of high-quality data in electronic health records and a well-designed information technology (IT) infrastructure to access these data are indispensable for successful implementation of automated HAI surveillance. Previous studies have demonstrated that reliance on stand-alone administrative data is generally unsuited as sole case-finding strategy. Recent attempts to combine multiple administrative and clinical data sources in algorithms yielded more reliable results. Current surveillance practices are mostly limited to single healthcare facilities, but future linkage of multiple databases in a network may allow interfacility surveillance. Although prior surveillance algorithms were often straightforward decision trees based on structured data, recent studies have used a wide variety of techniques for case-finding, including logistic regression and various machine learning methods. In the future, natural language processing may enable the use of unstructured narrative data.              Summary:                    Developments in healthcare IT are rapidly changing the landscape of HAI surveillance. The electronic availability and incorporation of routine care data in surveillance algorithms enhances the reliability, efficiency and standardization of surveillance practices."
28497663,2.0,Mortality risk prediction models for coronary artery bypass graft surgery: current scenario and future direction,2017 Dec;58(6):931-942.,"Introduction:                    Many risk prediction models are currently in use for predicting short-term mortality following coronary artery bypass graft (CABG) surgery. This review critically appraised the methods that were used for developing these models to assess their applicability in current practice setting as well as for the necessity of up-gradation.              Evidence acquisition:                    Medline via Ovid was searched for articles published between 1946 and 2016 and EMBASE via Ovid between 1974 and 2016 to identify risk prediction models for CABG. Article selection and data extraction was conducted using the CHARMS checklist for review of prediction model studies. Association between model development methods and model's discrimination was assessed using Kruskal-Wallis one-way analysis of variance and Mann-Whitney U-test.              Evidence synthesis:                    A total of 53 risk prediction models for short-term mortality following CABG were identified. The review found a wide variation in development methodology of risk prediction models in the field. Ambiguous predictor and outcome definition, sub-optimum sample size, inappropriate handling of missing data and inefficient predictor selection technique are major issues identified in the review. Quantitative synthesis in the review showed ""missing value imputation"" and ""adopting machine learning algorithms"" may result in better discrimination power of the models.              Conclusions:                    There are aspects in current risk modeling, where there is room for improvement to reflect current clinical practice. Future risk modelling needs to adopt a standardized approach to defining both outcome and predictor variables, rational treatment of missing data and robust statistical techniques to enhance performance of the mortality risk prediction."
28494725,3.0,Computational Methods for Predicting ncRNA-protein Interactions,2017;13(6):515-525.,"Background:                    RNA-protein interactions (RPIs) play an important role in many cellular processes. In particular, noncoding RNA-protein interactions (ncRPIs) are involved in various gene regulations and human complex diseases. High-throughput experiments have provided a large number of valuable information about ncRPIs, but these experiments are expensive and timeconsuming. Therefore, some computational approaches have been developed to predict ncRPIs efficiently and effectively.              Methods:                    In this work, we will describe the recent advance of predicting ncRPIs from the following aspects: i) the dataset construction; ii) the sequence and structural feature representation, and iii) the machine learning algorithm.              Results:                    The current methods have successfully predicted ncRPIs, but most of them trained and tested on the small benchmark datasets derived from ncRNA-protein complexes in PDB database. The generalization performance and robust of these existing methods need to be further improved.              Conclusion:                    Concomitant with the large numbers of ncRPIs generated by high-throughput technologies, three future directions for predicting ncRPIs with machine learning should be paid attention. One direction is that how to effectively construct the negative sample set. Another is the selection of novel and effective features from the sequences and structures of ncRNAs and proteins. The third is the design of powerful predictor."
28494123,28.0,Ecological momentary interventions for depression and anxiety,2017 Jun;34(6):540-545.,"Ecological momentary interventions (EMIs) are becoming more popular and more powerful resources for the treatment and prevention of depression and anxiety due to advances in technological capacity and analytic sophistication. Previous work has demonstrated that EMIs can be effective at reducing symptoms of depression and anxiety as well as related outcomes of stress and at increasing positive psychological functioning. In this review, we highlight the differences between EMIs and other forms of treatment due to the nature of EMIs to be deeply integrated into the fabric of people's day-to-day lives. EMIs require unique considerations in their design, deployment, and evaluation. Furthermore, given that EMIs have been advanced by changes in technologies and that the use of behavioral intervention technologies for mental health has been increasing, we discuss how technologies and analytics might usher in a new era of EMIs. Future EMIs might reduce user burden and increase intervention personalization and sophistication by leveraging digital sensors and advances in natural language processing and machine learning. Thus, although current EMIs are effective, the EMIs of the future might be more engaging, responsive, and adaptable to different people and different contexts."
28490262,15.0,Methods for safety signal detection in healthcare databases: a literature review,2017 Jun;16(6):721-732.,"With increasing availability, the use of healthcare databases as complementary data source for drug safety signal detection has been explored to circumvent the limitations inherent in spontaneous reporting. Areas covered: To review the methods proposed for safety signal detection in healthcare databases and their performance. Expert opinion: Fifteen different data mining methods were identified. They are based on disproportionality analysis, traditional pharmacoepidemiological designs (e.g. self-controlled designs), sequence symmetry analysis (SSA), sequential statistical testing, temporal association rules, supervised machine learning (SML), and the tree-based scan statistic. When considering the performance of these methods, the self-controlled designs, the SSA, and the SML seemed the most interesting approaches. In the perspective of routine signal detection from healthcare databases, pragmatic aspects such as the need for stakeholders to understand the method in order to be confident in the results must be considered. From this point of view, the SSA could appear as the most suitable method for signal detection in healthcare databases owing to its simple principle and its ability to provide a risk estimate. However, further developments, such as automated prioritization, are needed to help stakeholders handle the multiplicity of signals."
28481395,5.0,A Systematic Review on Machine Learning in Neurosurgery: The Future of Decision-Making in Patient Care,2018;28(2):167-173.,"Current practice of neurosurgery depends on clinical practice guidelines and evidence-based research publications that derive results using statistical methods. However, statistical analysis methods have some limitations such as the inability to analyze nonlinear variables, requiring setting a level of significance, being impractical for analyzing large amounts of data and the possibility of human bias. Machine learning is an emerging method for analyzing massive amounts of complex data which relies on algorithms that allow computers to learn and make accurate predictions. During the past decade, machine learning has been increasingly implemented in medical research as well as neurosurgical publications. This systematical review aimed to assemble the current neurosurgical literature that machine learning has been utilized, and to inform neurosurgeons on this novel method of data analysis."
28456584,44.0,Inference in the age of big data: Future perspectives on neuroscience,2017 Jul 15;155:549-564.,"Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative datasets of unprecedented breadth (e.g., microanatomy, synaptic connections, and optogenetic brain-behavior assays) and size (e.g., cognition, brain imaging, and genetics). While growing data availability and information granularity have been amply discussed, we direct attention to a less explored question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more important to distill neurobiological knowledge from healthy and pathological brain measurements. We argue that large-scale data analysis will use more statistical models that are non-parametric, generative, and mixing frequentist and Bayesian aspects, while supplementing classical hypothesis testing with out-of-sample predictions."
28455151,15.0,Symptom severity prediction from neuropsychiatric clinical records: Overview of 2016 CEGS N-GRID shared tasks Track 2,2017 Nov;75S:S62-S70.,"The second track of the CEGS N-GRID 2016 natural language processing shared tasks focused on predicting symptom severity from neuropsychiatric clinical records. For the first time, initial psychiatric evaluation records have been collected, de-identified, annotated and shared with the scientific community. One-hundred-ten researchers organized in twenty-four teams participated in this track and submitted sixty-five system runs for evaluation. The top ten teams each achieved an inverse normalized macro-averaged mean absolute error score over 0.80. The top performing system employed an ensemble of six different machine learning-based classifiers to achieve a score 0.86. The task resulted to be generally easy with the exception of two specific classes of records: records with very few but crucial positive valence signals, and records describing patients predominantly affected by negative rather than positive valence. Those cases proved to be very challenging for most of the systems. Further research is required to consider the task solved. Overall, the results of this track demonstrate the effectiveness of data-driven approaches to the task of symptom severity classification."
28451550,7.0,Improving the Prediction of Survival in Cancer Patients by Using Machine Learning Techniques: Experience of Gene Expression Data: A Narrative Review,2017 Feb;46(2):165-172.,"Background:                    Today, despite the many advances in early detection of diseases, cancer patients have a poor prognosis and the survival rates in them are low. Recently, microarray technologies have been used for gathering thousands data about the gene expression level of cancer cells. These types of data are the main indicators in survival prediction of cancer. This study highlights the improvement of survival prediction based on gene expression data by using machine learning techniques in cancer patients.              Methods:                    This review article was conducted by searching articles between 2000 to 2016 in scientific databases and e-Journals. We used keywords such as machine learning, gene expression data, survival and cancer.              Results:                    Studies have shown the high accuracy and effectiveness of gene expression data in comparison with clinical data in survival prediction. Because of bewildering and high volume of such data, studies have highlighted the importance of machine learning algorithms such as Artificial Neural Networks (ANN) to find out the distinctive signatures of gene expression in cancer patients. These algorithms improve the efficiency of probing and analyzing gene expression in cancer profiles for survival prediction of cancer.              Conclusion:                    By attention to the capabilities of machine learning techniques in proteomics and genomics applications, developing clinical decision support systems based on these methods for analyzing gene expression data can prevent potential errors in survival estimation, provide appropriate and individualized treatments to patients and improve the prognosis of cancers."
28436837,28.0,"Passive BCI in Operational Environments: Insights, Recent Advances, and Future Trends",2017 Jul;64(7):1431-1436.,"Goal:                    This minireview aims to highlight recent important aspects to consider and evaluate when passive brain-computer interface (pBCI) systems would be developed and used in operational environments, and remarks future directions of their applications.              Methods:                    Electroencephalography (EEG) based pBCI has become an important tool for real-time analysis of brain activity since it could potentially provide covertly-without distracting the user from the main task-and objectively-not affected by the subjective judgment of an observer or the user itself-information about the operator cognitive state.              Results:                    Different examples of pBCI applications in operational environments and new adaptive interface solutions have been presented and described. In addition, a general overview regarding the correct use of machine learning techniques (e.g., which algorithm to use, common pitfalls to avoid, etc.) in the pBCI field has been provided.              Conclusion:                    Despite recent innovations on algorithms and neurotechnology, pBCI systems are not completely ready to enter the market yet, mainly due to limitations of the EEG electrodes technology, and algorithms reliability and capability in real settings.              Significance:                    High complexity and safety critical systems (e.g., airplanes, ATM interfaces) should adapt their behaviors and functionality accordingly to the user' actual mental state. Thus, technologies (i.e., pBCIs) able to measure in real time the user's mental states would result very useful in such ""high risk"" environments to enhance human machine interaction, and so increase the overall safety."
28436590,61.0,Identification of small molecules using accurate mass MS/MS search,2018 Jul;37(4):513-532.,"Tandem mass spectral library search (MS/MS) is the fastest way to correctly annotate MS/MS spectra from screening small molecules in fields such as environmental analysis, drug screening, lipid analysis, and metabolomics. The confidence in MS/MS-based annotation of chemical structures is impacted by instrumental settings and requirements, data acquisition modes including data-dependent and data-independent methods, library scoring algorithms, as well as post-curation steps. We critically discuss parameters that influence search results, such as mass accuracy, precursor ion isolation width, intensity thresholds, centroiding algorithms, and acquisition speed. A range of publicly and commercially available MS/MS databases such as NIST, MassBank, MoNA, LipidBlast, Wiley MSforID, and METLIN are surveyed. In addition, software tools including NIST MS Search, MS-DIAL, Mass Frontier, SmileMS, Mass++, and XCMS2 to perform fast MS/MS search are discussed. MS/MS scoring algorithms and challenges during compound annotation are reviewed. Advanced methods such as the in silico generation of tandem mass spectra using quantum chemistry and machine learning methods are covered. Community efforts for curation and sharing of tandem mass spectra that will allow for faster distribution of scientific discoveries are discussed."
28434256,2.0,A bioinformatics roadmap for the human vaccines project,2017 Jun;16(6):535-544.,"Biomedical research has become a data intensive science in which high throughput experimentation is producing comprehensive data about biological systems at an ever-increasing pace. The Human Vaccines Project is a new public-private partnership, with the goal of accelerating development of improved vaccines and immunotherapies for global infectious diseases and cancers by decoding the human immune system. To achieve its mission, the Project is developing a Bioinformatics Hub as an open-source, multidisciplinary effort with the overarching goal of providing an enabling infrastructure to support the data processing, analysis and knowledge extraction procedures required to translate high throughput, high complexity human immunology research data into biomedical knowledge, to determine the core principles driving specific and durable protective immune responses."
28418201,,"Chemoinformatics at IFP Energies Nouvelles: Applications in the Fields of Energy, Transport, and Environment",2017 Oct;36(10).,"The objective of the present paper is to summarize chemoinformatics based research, and more precisely, the development of quantitative structure property relationships performed at IFP Energies nouvelles (IFPEN) during the last decade. A special focus is proposed on research activities performed in the ""Thermodynamics and Molecular Simulation"" department, i. e. the use of multiscale molecular simulation methods in responses to projects. Molecular simulation techniques can be envisaged to supplement dataset when experimental information lacks, thus the review includes a section dedicated to molecular simulation codes, development of intermolecular potentials, and some of their possible applications. Know-how and feedback from our experiences in terms of machine learning application for thermophysical property predictions are included in a section dealing with methodological aspects. The generic character of chemoinformatics is emphasized through applications in the fields of energy, transport, and environment, with illustrations for three IFPEN business units: ""Transports"", ""Energy Resources"", and ""Processes"". More precisely, the review focus on different challenges such as the prediction of properties for alternative fuels, the prediction of fuel compatibility with polymeric materials, the prediction of properties for surfactants usable in chemical enhanced oil recovery, and the prediction of guest-host interactions between gases and nanoporous materials in the frame of carbon dioxide capture or gas separation activities."
28414186,89.0,A review on neuroimaging-based classification studies and associated feature extraction methods for Alzheimer's disease and its prodromal stages,2017 Jul 15;155:530-548.,"Neuroimaging has made it possible to measure pathological brain changes associated with Alzheimer's disease (AD) in vivo. Over the past decade, these measures have been increasingly integrated into imaging signatures of AD by means of classification frameworks, offering promising tools for individualized diagnosis and prognosis. We reviewed neuroimaging-based studies for AD and mild cognitive impairment classification, selected after online database searches in Google Scholar and PubMed (January, 1985-June, 2016). We categorized these studies based on the following neuroimaging modalities (and sub-categorized based on features extracted as a post-processing step from these modalities): i) structural magnetic resonance imaging [MRI] (tissue density, cortical surface, and hippocampal measurements), ii) functional MRI (functional coherence of different brain regions, and the strength of the functional connectivity), iii) diffusion tensor imaging (patterns along the white matter fibers), iv) fluorodeoxyglucose positron emission tomography (FDG-PET) (metabolic rate of cerebral glucose), and v) amyloid-PET (amyloid burden). The studies reviewed indicate that the classification frameworks formulated on the basis of these features show promise for individualized diagnosis and prediction of clinical progression. Finally, we provided a detailed account of AD classification challenges and addressed some future research directions."
28405937,4.0,New Cardiac Imaging Algorithms to Diagnose Constrictive Pericarditis Versus Restrictive Cardiomyopathy,2017 May;19(5):43.,"Purpose of review:                    Echocardiography is the mainstay in the diagnostic evaluation of constrictive pericarditis (CP) and restrictive cardiomyopathy (RCM), but no single echocardiographic parameter is sufficiently robust to accurately distinguish between the two conditions. The present review summarizes the recent advances in echocardiography that promise to improve its diagnostic performance for this purpose. The role of other imaging modalities such as cardiac computed tomography, magnetic resonance imaging, and invasive hemodynamic assessment in the overall diagnostic approach is also discussed briefly.              Recent findings:                    A recent study has demonstrated improved diagnostic accuracy of echocardiography with integration of multiple conventional echocardiographic parameters in to a step-wise algorithm. Concurrently, the studies using speckle-tracking echocardiography have revealed distinct and disparate patterns of myocardial mechanical abnormalities in CP and RCM with their ability to distinguish between the two conditions. The incorporation of machine-learning algorithms into echocardiography workflow permits easy integration of the wealth of the diagnostic data available and promises to further enhance the diagnostic accuracy of echocardiography. New imaging algorithms are continuously being evolved to permit accurate distinction between CP and RCM. Further research is needed to validate the accuracy of these newer algorithms and to define their place in the overall diagnostic approach for this purpose."
28405579,28.0,A survey of methods and tools to detect recent and strong positive selection,2017 Apr 8;24:7.,"Positive selection occurs when an allele is favored by natural selection. The frequency of the favored allele increases in the population and due to genetic hitchhiking the neighboring linked variation diminishes, creating so-called selective sweeps. Detecting traces of positive selection in genomes is achieved by searching for signatures introduced by selective sweeps, such as regions of reduced variation, a specific shift of the site frequency spectrum, and particular LD patterns in the region. A variety of methods and tools can be used for detecting sweeps, ranging from simple implementations that compute summary statistics such as Tajima's D, to more advanced statistical approaches that use combinations of statistics, maximum likelihood, machine learning etc. In this survey, we present and discuss summary statistics and software tools, and classify them based on the selective sweep signature they detect, i.e., SFS-based vs. LD-based, as well as their capacity to analyze whole genomes or just subgenomic regions. Additionally, we summarize the results of comparisons among four open-source software releases (SweeD, SweepFinder, SweepFinder2, and OmegaPlus) regarding sensitivity, specificity, and execution times. In equilibrium neutral models or mild bottlenecks, both SFS- and LD-based methods are able to detect selective sweeps accurately. Methods and tools that rely on LD exhibit higher true positive rates than SFS-based ones under the model of a single sweep or recurrent hitchhiking. However, their false positive rate is elevated when a misspecified demographic model is used to represent the null hypothesis. When the correct (or similar to the correct) demographic model is used instead, the false positive rates are considerably reduced. The accuracy of detecting the true target of selection is decreased in bottleneck scenarios. In terms of execution time, LD-based methods are typically faster than SFS-based methods, due to the nature of required arithmetic."
28377102,7.0,Can We Predict Gene Expression by Understanding Proximal Promoter Architecture?,2017 Jun;35(6):530-546.,"We review computational predictions of expression from the promoter architecture - the set of transcription factors that can bind the proximal promoter. We focus on spatial expression patterns in animals with complex body plans and many distinct tissue types. This field is ripe for change as functional genomics datasets accumulate for both expression and protein-DNA interactions. While there has been some success in predicting the breadth of expression (i.e., the fraction of tissue types a gene is expressed in), predicting tissue specificity remains challenging. We discuss how progress can be achieved through either machine learning or complementary combinatorial data mining. The likely impact of single-cell expression data is considered. Finally, we discuss the design of artificial promoters as a practical application."
28375728,86.0,Personal Sensing: Understanding Mental Health Using Ubiquitous Sensors and Machine Learning,2017 May 8;13:23-47.,"Sensors in everyday devices, such as our phones, wearables, and computers, leave a stream of digital traces. Personal sensing refers to collecting and analyzing data from sensors embedded in the context of daily life with the aim of identifying human behaviors, thoughts, feelings, and traits. This article provides a critical review of personal sensing research related to mental health, focused principally on smartphones, but also including studies of wearables, social media, and computers. We provide a layered, hierarchical model for translating raw sensor data into markers of behaviors and states related to mental health. Also discussed are research methods as well as challenges, including privacy and problems of dimensionality. Although personal sensing is still in its infancy, it holds great promise as a method for conducting mental health research and as a clinical tool for monitoring at-risk populations and providing the foundation for the next generation of mobile health (or mHealth) interventions."
28366290,18.0,Microbiome Tools for Forensic Science,2017 Sep;35(9):814-823.,"Microbes are present at every crime scene and have been used as physical evidence for over a century. Advances in DNA sequencing and computational approaches have led to recent breakthroughs in the use of microbiome approaches for forensic science, particularly in the areas of estimating postmortem intervals (PMIs), locating clandestine graves, and obtaining soil and skin trace evidence. Low-cost, high-throughput technologies allow us to accumulate molecular data quickly and to apply sophisticated machine-learning algorithms, building generalizable predictive models that will be useful in the criminal justice system. In particular, integrating microbiome and metabolomic data has excellent potential to advance microbial forensics."
28363883,14.0,Methods for Coding Tobacco-Related Twitter Data: A Systematic Review,2017 Mar 31;19(3):e91.,"Background:                    As Twitter has grown in popularity to 313 million monthly active users, researchers have increasingly been using it as a data source for tobacco-related research.              Objective:                    The objective of this systematic review was to assess the methodological approaches of categorically coded tobacco Twitter data and make recommendations for future studies.              Methods:                    Data sources included PsycINFO, Web of Science, PubMed, ABI/INFORM, Communication Source, and Tobacco Regulatory Science. Searches were limited to peer-reviewed journals and conference proceedings in English from January 2006 to July 2016. The initial search identified 274 articles using a Twitter keyword and a tobacco keyword. One coder reviewed all abstracts and identified 27 articles that met the following inclusion criteria: (1) original research, (2) focused on tobacco or a tobacco product, (3) analyzed Twitter data, and (4) coded Twitter data categorically. One coder extracted data collection and coding methods.              Results:                    E-cigarettes were the most common type of Twitter data analyzed, followed by specific tobacco campaigns. The most prevalent data sources were Gnip and Twitter's Streaming application programming interface (API). The primary methods of coding were hand-coding and machine learning. The studies predominantly coded for relevance, sentiment, theme, user or account, and location of user.              Conclusions:                    Standards for data collection and coding should be developed to be able to more easily compare and replicate tobacco-related Twitter results. Additional recommendations include the following: sample Twitter's databases multiple times, make a distinction between message attitude and emotional tone for sentiment, code images and URLs, and analyze user profiles. Being relatively novel and widely used among adolescents and black and Hispanic individuals, Twitter could provide a rich source of tobacco surveillance data among vulnerable populations."
28359573,9.0,"Next-Generation Global Biomonitoring: Large-scale, Automated Reconstruction of Ecological Networks",2017 Jul;32(7):477-487.,"We foresee a new global-scale, ecological approach to biomonitoring emerging within the next decade that can detect ecosystem change accurately, cheaply, and generically. Next-generation sequencing of DNA sampled from the Earth's environments would provide data for the relative abundance of operational taxonomic units or ecological functions. Machine-learning methods would then be used to reconstruct the ecological networks of interactions implicit in the raw NGS data. Ultimately, we envision the development of autonomous samplers that would sample nucleic acids and upload NGS sequence data to the cloud for network reconstruction. Large numbers of these samplers, in a global array, would allow sensitive automated biomonitoring of the Earth's major ecosystems at high spatial and temporal resolution, revolutionising our understanding of ecosystem change."
28356908,5.0,Random Deep Belief Networks for Recognizing Emotions from Speech Signals,2017;2017:1945630.,"Now the human emotions can be recognized from speech signals using machine learning methods; however, they are challenged by the lower recognition accuracies in real applications due to lack of the rich representation ability. Deep belief networks (DBN) can automatically discover the multiple levels of representations in speech signals. To make full of its advantages, this paper presents an ensemble of random deep belief networks (RDBN) method for speech emotion recognition. It firstly extracts the low level features of the input speech signal and then applies them to construct lots of random subspaces. Each random subspace is then provided for DBN to yield the higher level features as the input of the classifier to output an emotion label. All outputted emotion labels are then fused through the majority voting to decide the final emotion label for the input speech signal. The conducted experimental results on benchmark speech emotion databases show that RDBN has better accuracy than the compared methods for speech emotion recognition."
28353229,1.0,The Utility of Multiplex Assays for Identification of Proteomic Signatures in Psychiatry,2017;974:131-138.,"As substantial efforts are being made to identify biological markers of psychiatric illnesses, it is becoming clear that clinically useful accuracy will require larger sets of readouts that potentially span different technological platforms. For discovery of proteomic biomarkers, simultaneous measurement of analytes in small sample quantities has developed into a widely used technology of similar quality as the respective single-plex assays. Multiplex assay systems therefore hold promise for biomarker discovery and development in many complex disease areas including psychiatry. However, analysis of the derived data is subject to substantial challenges that may impede the possibility of obtaining meaningful findings. This chapter discusses potential applications of multiplexed assay technologies during biomarker development and highlights potential challenges for machine learning analysis of derived data."
28353154,2.0,Physiome approach for the analysis of vascular flow reserve in the heart and brain,2017 Jun;469(5-6):613-628.,"This work reviews the key aspects of coronary and neurovascular flow reserves with an emphasis on physiomic modeling characteristics by the use of a variety of numerical approaches. First, we explain the definition of fractional flow reserve (FFR) in coronary artery and introduce its clinical significance. Then, computational researches for obtaining FFR are reviewed, and their clinical outcomes are compared. In the case of cerebrovascular reserve (CVR), in spite of substantial progress in the simulation of cerebral hemodynamics, only a few computational studies exist. Thus, we discuss the limitations of CVR simulation study and suggest the challenging issue to overcome these. Also, the future direction of physiomic researches for the flow reserves in coronary arteries and cerebral arteries is described. Also, we introduce a machine learning algorithm trained by the existing physiomic simulation data of flow reserve and suggest a prospective research direction related to this."
28348527,9.0,Tuning Up the Old Brain with New Tricks: Attention Training via Neurofeedback,2017 Mar 13;9:52.,"Neurofeedback (NF) is a form of biofeedback that uses real-time (RT) modulation of brain activity to enhance brain function and behavioral performance. Recent advances in Brain-Computer Interfaces (BCI) and cognitive training (CT) have provided new tools and evidence that NF improves cognitive functions, such as attention and working memory (WM), beyond what is provided by traditional CT. More published studies have demonstrated the efficacy of NF, particularly for treating attention deficit hyperactivity disorder (ADHD) in children. In contrast, there have been fewer studies done in older adults with or without cognitive impairment, with some notable exceptions. The focus of this review is to summarize current success in RT NF training of older brains aiming to match those of younger brains during attention/WM tasks. We also outline potential future advances in RT brainwave-based NF for improving attention training in older populations. The rapid growth in wireless recording of brain activity, machine learning classification and brain network analysis provides new tools for combating cognitive decline and brain aging in older adults. We optimistically conclude that NF, combined with new neuro-markers (event-related potentials and connectivity) and traditional features, promises to provide new hope for brain and CT in the growing older population."
28344110,5.0,Toward a systematic exploration of nano-bio interactions,2017 May 15;323:66-73.,"Many studies of nanomaterials make non-systematic alterations of nanoparticle physicochemical properties. Given the immense size of the property space for nanomaterials, such approaches are not very useful in elucidating fundamental relationships between inherent physicochemical properties of these materials and their interactions with, and effects on, biological systems. Data driven artificial intelligence methods such as machine learning algorithms have proven highly effective in generating models with good predictivity and some degree of interpretability. They can provide a viable method of reducing or eliminating animal testing. However, careful experimental design with the modelling of the results in mind is a proven and efficient way of exploring large materials spaces. This approach, coupled with high speed automated experimental synthesis and characterization technologies now appearing, is the fastest route to developing models that regulatory bodies may find useful. We advocate greatly increased focus on systematic modification of physicochemical properties of nanoparticles combined with comprehensive biological evaluation and computational analysis. This is essential to obtain better mechanistic understanding of nano-bio interactions, and to derive quantitatively predictive and robust models for the properties of nanomaterials that have useful domains of applicability."
28342697,69.0,Recent publications from the Alzheimer's Disease Neuroimaging Initiative: Reviewing progress toward improved AD clinical trials,2017 Apr;13(4):e1-e85.,"Introduction:                    The Alzheimer's Disease Neuroimaging Initiative (ADNI) has continued development and standardization of methodologies for biomarkers and has provided an increased depth and breadth of data available to qualified researchers. This review summarizes the over 400 publications using ADNI data during 2014 and 2015.              Methods:                    We used standard searches to find publications using ADNI data.              Results:                    (1) Structural and functional changes, including subtle changes to hippocampal shape and texture, atrophy in areas outside of hippocampus, and disruption to functional networks, are detectable in presymptomatic subjects before hippocampal atrophy; (2) In subjects with abnormal β-amyloid deposition (Aβ+), biomarkers become abnormal in the order predicted by the amyloid cascade hypothesis; (3) Cognitive decline is more closely linked to tau than Aβ deposition; (4) Cerebrovascular risk factors may interact with Aβ to increase white-matter (WM) abnormalities which may accelerate Alzheimer's disease (AD) progression in conjunction with tau abnormalities; (5) Different patterns of atrophy are associated with impairment of memory and executive function and may underlie psychiatric symptoms; (6) Structural, functional, and metabolic network connectivities are disrupted as AD progresses. Models of prion-like spreading of Aβ pathology along WM tracts predict known patterns of cortical Aβ deposition and declines in glucose metabolism; (7) New AD risk and protective gene loci have been identified using biologically informed approaches; (8) Cognitively normal and mild cognitive impairment (MCI) subjects are heterogeneous and include groups typified not only by ""classic"" AD pathology but also by normal biomarkers, accelerated decline, and suspected non-Alzheimer's pathology; (9) Selection of subjects at risk of imminent decline on the basis of one or more pathologies improves the power of clinical trials; (10) Sensitivity of cognitive outcome measures to early changes in cognition has been improved and surrogate outcome measures using longitudinal structural magnetic resonance imaging may further reduce clinical trial cost and duration; (11) Advances in machine learning techniques such as neural networks have improved diagnostic and prognostic accuracy especially in challenges involving MCI subjects; and (12) Network connectivity measures and genetic variants show promise in multimodal classification and some classifiers using single modalities are rivaling multimodal classifiers.              Discussion:                    Taken together, these studies fundamentally deepen our understanding of AD progression and its underlying genetic basis, which in turn informs and improves clinical trial design."
28341746,51.0,Leveraging sequence-based faecal microbial community survey data to identify a composite biomarker for colorectal cancer,2018 May;67(5):882-891.,"Objective:                    Colorectal cancer (CRC) is the second leading cause of cancer-associated mortality in the USA. The faecal microbiome may provide non-invasive biomarkers of CRC and indicate transition in the adenoma-carcinoma sequence. Re-analysing raw sequence and metadata from several studies uniformly, we sought to identify a composite and generalisable microbial marker for CRC.              Design:                    Raw 16S rRNA gene sequence data sets from nine studies were processed with two pipelines, (1) QIIME closed reference (QIIME-CR) or (2) a strain-specific method herein termed SS-UP (Strain Select, UPARSE bioinformatics pipeline). A total of 509 samples (79 colorectal adenoma, 195 CRC and 235 controls) were analysed. Differential abundance, meta-analysis random effects regression and machine learning analyses were carried out to determine the consistency and diagnostic capabilities of potential microbial biomarkers.              Results:                    Definitive taxa, including Parvimonas micra ATCC 33270, Streptococcus anginosus and yet-to-be-cultured members of Proteobacteria, were frequently and significantly increased in stools from patients with CRC compared with controls across studies and had high discriminatory capacity in diagnostic classification. Microbiome-based CRC versus control classification produced an area under receiver operator characteristic (AUROC) curve of 76.6% in QIIME-CR and 80.3% in SS-UP. Combining clinical and microbiome markers gave a diagnostic AUROC of 83.3% for QIIME-CR and 91.3% for SS-UP.              Conclusions:                    Despite technological differences across studies and methods, key microbial markers emerged as important in classifying CRC cases and such could be used in a universal diagnostic for the disease. The choice of bioinformatics pipeline influenced accuracy of classification. Strain-resolved microbial markers might prove crucial in providing a microbial diagnostic for CRC."
28324301,10.0,Neuroimaging in Epilepsy,2017 Apr;17(4):32.,"In recent years, the field of neuroimaging has undergone dramatic development. Specifically, of importance for clinicians and researchers managing patients with epilepsies, new methods of brain imaging in search of the seizure-producing abnormalities have been implemented, and older methods have undergone additional refinement. Methodology to predict seizure freedom and cognitive outcome has also rapidly progressed. In general, the image data processing methods are very different and more complicated than even a decade ago. In this review, we identify the recent developments in neuroimaging that are aimed at improved management of epilepsy patients. Advances in structural imaging, diffusion imaging, fMRI, structural and functional connectivity, hybrid imaging methods, quantitative neuroimaging, and machine-learning are discussed. We also briefly summarize the potential new developments that may shape the field of neuroimaging in the near future and may advance not only our understanding of epileptic networks as the source of treatment-resistant seizures but also better define the areas that need to be treated in order to provide the patients with better long-term outcomes."
28319831,7.0,Structure-based prediction of host-pathogen protein interactions,2017 Jun;44:119-124.,"The discovery, validation, and characterization of protein-based interactions from different species are crucial for translational research regarding a variety of pathogens, ranging from the malaria parasite Plasmodium falciparum to HIV-1. Here, we review recent advances in the prediction of host-pathogen protein interfaces using structural information. In particular, we observe that current methods chiefly perform machine learning on sequence and domain information to produce large sets of candidate interactions that are further assessed and pruned to generate final, highly probable sets. Structure-based studies have also emphasized the electrostatic properties and evolutionary transformations of pathogenic interfaces, supplying crucial insight into antigenic determinants and the ways pathogens compete for host protein binding. Advancements in spectroscopic and crystallographic methods complement the aforementioned techniques, permitting the rigorous study of true positives at a molecular level. Together, these approaches illustrate how protein structure on a variety of levels functions coordinately and dynamically to achieve host takeover."
