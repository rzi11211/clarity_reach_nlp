pmid,title,date,text,citations
24231119,Review of machine learning and signal processing techniques for automated electrode selection in high-density microelectrode arrays,2014 Aug;59(4):323-33.,"Recently developed CMOS-based microprobes contain hundreds of electrodes on a single shaft with interelectrode distances as small as 30 Âµm. So far, neuroscientists manually select a subset of those electrodes depending on their appraisal of the ""usefulness"" of the recorded signals, which makes the process subjective but more importantly too time consuming to be useable in practice. The ever-increasing number of recording electrodes on microelectrode probes calls for an automated selection of electrodes containing ""good quality signals"" or ""signals of interest."" This article reviews the different criteria for electrode selection as well as the basic signal processing steps to prepare the data to compute those criteria. We discuss three of them. The first two select the electrodes based on ""signal quality."" The first criterion computes the penalized signal-to-noise ratio (SNR); the second criterion models the neuroscientist's appraisal of signal quality. Last, our most recent work allows the selection of electrodes that capture particular anatomical cell types. The discussed algorithms perform what is called in the literature ""electronic depth control"" in contrast to the mechanical repositioning of the electrode shafts in search of ""good quality signals"" or ""signals of interest.""",
24201027,A review of approaches to identifying patient phenotype cohorts using electronic health records,Mar-Apr 2014;21(2):221-30.,"Objective:                    To summarize literature describing approaches aimed at automatically identifying patients with a common phenotype.              Materials and methods:                    We performed a review of studies describing systems or reporting techniques developed for identifying cohorts of patients with specific phenotypes. Every full text article published in (1) Journal of American Medical Informatics Association, (2) Journal of Biomedical Informatics, (3) Proceedings of the Annual American Medical Informatics Association Symposium, and (4) Proceedings of Clinical Research Informatics Conference within the past 3 years was assessed for inclusion in the review. Only articles using automated techniques were included.              Results:                    Ninety-seven articles met our inclusion criteria. Forty-six used natural language processing (NLP)-based techniques, 24 described rule-based systems, 41 used statistical analyses, data mining, or machine learning techniques, while 22 described hybrid systems. Nine articles described the architecture of large-scale systems developed for determining cohort eligibility of patients.              Discussion:                    We observe that there is a rise in the number of studies associated with cohort identification using electronic medical records. Statistical analyses or machine learning, followed by NLP techniques, are gaining popularity over the years in comparison with rule-based systems.              Conclusions:                    There are a variety of approaches for classifying patients into a particular phenotype. Different techniques and data sources are used, and good performance is reported on datasets at respective institutions. However, no system makes comprehensive use of electronic medical records addressing all of their known weaknesses.",158.0
24199696,"Esophageal cancer staging: past, present, and future",2013 Nov;23(4):461-9.,"TNM cancer staging, conceived 70 years ago, was first applied to the esophagus in 1977. Prior staging was neither data-driven nor harmonized with stomach cancer. Machine-learning analysis of worldwide data addressed these shortcomings in the 7th edition. The 8th edition considers 6 problems in attempting to advance esophageal cancer staging.",18.0
24197932,Applications of alignment-free methods in epigenomics,2014 May;15(3):419-30.,"Epigenetic mechanisms play an important role in the regulation of cell type-specific gene activities, yet how epigenetic patterns are established and maintained remains poorly understood. Recent studies have supported a role of DNA sequences in recruitment of epigenetic regulators. Alignment-free methods have been applied to identify distinct sequence features that are associated with epigenetic patterns and to predict epigenomic profiles. Here, we review recent advances in such applications, including the methods to map DNA sequence to feature space, sequence comparison and prediction models. Computational studies using these methods have provided important insights into the epigenetic regulatory mechanisms.",8.0
24187909,Computational and theoretical methods for protein folding,2013 Dec 3;52(48):8601-24.,"A computational approach is essential whenever the complexity of the process under study is such that direct theoretical or experimental approaches are not viable. This is the case for protein folding, for which a significant amount of data are being collected. This paper reports on the essential role of in silico methods and the unprecedented interplay of computational and theoretical approaches, which is a defining point of the interdisciplinary investigations of the protein folding process. Besides giving an overview of the available computational methods and tools, we argue that computation plays not merely an ancillary role but has a more constructive function in that computational work may precede theory and experiments. More precisely, computation can provide the primary conceptual clues to inspire subsequent theoretical and experimental work even in a case where no preexisting evidence or theoretical frameworks are available. This is cogently manifested in the application of machine learning methods to come to grips with the folding dynamics. These close relationships suggested complementing the review of computational methods within the appropriate theoretical context to provide a self-contained outlook of the basic concepts that have converged into a unified description of folding and have grown in a synergic relationship with their computational counterpart. Finally, the advantages and limitations of current computational methodologies are discussed to show how the smart analysis of large amounts of data and the development of more effective algorithms can improve our understanding of protein folding.",12.0
24182747,Innovation by homologous recombination,2013 Dec;17(6):902-9.,"Swapping fragments among protein homologs can produce chimeric proteins with a wide range of properties, including properties not exhibited by the parents. Computational methods that use information from structures and sequence alignments have been used to design highly functional chimeras and chimera libraries. Recombination has generated proteins with diverse thermostability and mechanical stability, enzyme substrate specificity, and optogenetic properties. Linear regression, Gaussian processes, and support vector machine learning have been used to model sequence-function relationships and predict useful chimeras. These approaches enable engineering of protein chimeras with desired functions, as well as elucidation of the structural basis for these functions.",9.0
24179856,Complex biomarker discovery in neuroimaging data: Finding a needle in a haystack,2013 Aug 7;3:123-31.,"Neuropsychiatric disorders such as schizophrenia, bipolar disorder and Alzheimer's disease are major public health problems. However, despite decades of research, we currently have no validated prognostic or diagnostic tests that can be applied at an individual patient level. Many neuropsychiatric diseases are due to a combination of alterations that occur in a human brain rather than the result of localized lesions. While there is hope that newer imaging technologies such as functional and anatomic connectivity MRI or molecular imaging may offer breakthroughs, the single biomarkers that are discovered using these datasets are limited by their inability to capture the heterogeneity and complexity of most multifactorial brain disorders. Recently, complex biomarkers have been explored to address this limitation using neuroimaging data. In this manuscript we consider the nature of complex biomarkers being investigated in the recent literature and present techniques to find such biomarkers that have been developed in related areas of data mining, statistics, machine learning and bioinformatics.",26.0
24168933,Linking brain imaging signals to visual perception,2013 Nov;30(5-6):229-41.,"The rapid advances in brain imaging technology over the past 20 years are affording new insights into cortical processing hierarchies in the human brain. These new data provide a complementary front in seeking to understand the links between perceptual and physiological states. Here we review some of the challenges associated with incorporating brain imaging data into such ""linking hypotheses,"" highlighting some of the considerations needed in brain imaging data acquisition and analysis. We discuss work that has sought to link human brain imaging signals to existing electrophysiological data and opened up new opportunities in studying the neural basis of complex perceptual judgments. We consider a range of approaches when using human functional magnetic resonance imaging to identify brain circuits whose activity changes in a similar manner to perceptual judgments and illustrate these approaches by discussing work that has studied the neural basis of 3D perception and perceptual learning. Finally, we describe approaches that have sought to understand the information content of brain imaging data using machine learning and work that has integrated multimodal data to overcome the limitations associated with individual brain imaging approaches. Together these approaches provide an important route in seeking to understand the links between physiological and psychological states.",4.0
24158807,Bioinformatics tools for predicting GPCR gene functions,2014;796:205-24.,"The automatic classification of GPCRs by bioinformatics methodology can provide functional information for new GPCRs in the whole 'GPCR proteome' and this information is important for the development of novel drugs. Since GPCR proteome is classified hierarchically, general ways for GPCR function prediction are based on hierarchical classification. Various computational tools have been developed to predict GPCR functions; those tools use not simple sequence searches but more powerful methods, such as alignment-free methods, statistical model methods, and machine learning methods used in protein sequence analysis, based on learning datasets. The first stage of hierarchical function prediction involves the discrimination of GPCRs from non-GPCRs and the second stage involves the classification of the predicted GPCR candidates into family, subfamily, and sub-subfamily levels. Then, further classification is performed according to their protein-protein interaction type: binding G-protein type, oligomerized partner type, etc. Those methods have achieved predictive accuracies of around 90 %. Finally, I described the future subject of research of the bioinformatics technique about functional prediction of GPCR.",2.0
24140287,Drug name recognition in biomedical texts: a machine-learning-based method,2014 May;19(5):610-7.,"Currently, there is an urgent need to develop a technology for extracting drug information automatically from biomedical texts, and drug name recognition is an essential prerequisite for extracting drug information. This article presents a machine-learning-based approach to recognize drug names in biomedical texts. In this approach, a drug name dictionary is first constructed with the external resource of DrugBank and PubMed. Then a semi-supervised learning method, feature coupling generalization, is used to filter this dictionary. Finally, the dictionary look-up and the condition random field method are combined to recognize drug names. Experimental results show that our approach achieves an F-score of 92.54% on the test set of DDIExtraction2011.",5.0
24136011,Improvement of adequate use of warfarin for the elderly using decision tree-based approaches,2014;53(1):47-53.,"Objectives:                    Due to the narrow therapeutic range and high drug-to-drug interactions (DDIs), improving the adequate use of warfarin for the elderly is crucial in clinical practice. This study examines whether the effectiveness of using warfarin among elderly inpatients can be improved when machine learning techniques and data from the laboratory information system are incorporated.              Methods:                    Having employed 288 validated clinical cases in the DDI group and 89 cases in the non-DDI group, we evaluate the prediction performance of seven classification techniques, with and without an Adaptive Boosting (AdaBoost) algorithm. Measures including accuracy, sensitivity, specificity and area under the curve are used to evaluate model performance.              Results:                    Decision tree-based classifiers outperform other investigated classifiers in all evaluation measures. The classifiers supplemented with AdaBoost can generally improve the performance. In addition, weight, congestive heart failure, and gender are among the top three critical variables affecting prediction accuracy for the non-DDI group, while age, ALT, and warfarin doses are the most influential factors for the DDI group.              Conclusion:                    Medical decision support systems incorporating decision tree-based approaches improve predicting performance and thus may serve as a supplementary tool in clinical practice. Information from laboratory tests and inpatients' history should not be ignored because related variables are shown to be decisive in our prediction models, especially when the DDIs exist.",4.0
24126129,"Information-seeking, curiosity, and attention: computational and neural mechanisms",2013 Nov;17(11):585-93.,"Intelligent animals devote much time and energy to exploring and obtaining information, but the underlying mechanisms are poorly understood. We review recent developments on this topic that have emerged from the traditionally separate fields of machine learning, eye movements in natural behavior, and studies of curiosity in psychology and neuroscience. These studies show that exploration may be guided by a family of mechanisms that range from automatic biases toward novelty or surprise to systematic searches for learning progress and information gain in curiosity-driven behavior. In addition, eye movements reflect visual information searching in multiple conditions and are amenable for cellular-level investigations. This suggests that the oculomotor system is an excellent model system for understanding information-sampling mechanisms.",75.0
24116388,Application of machine learning to proteomics data: classification and biomarker identification in postgenomics biology,2013 Dec;17(12):595-610.,"Mass spectrometry is an analytical technique for the characterization of biological samples and is increasingly used in omics studies because of its targeted, nontargeted, and high throughput abilities. However, due to the large datasets generated, it requires informatics approaches such as machine learning techniques to analyze and interpret relevant data. Machine learning can be applied to MS-derived proteomics data in two ways. First, directly to mass spectral peaks and second, to proteins identified by sequence database searching, although relative protein quantification is required for the latter. Machine learning has been applied to mass spectrometry data from different biological disciplines, particularly for various cancers. The aims of such investigations have been to identify biomarkers and to aid in diagnosis, prognosis, and treatment of specific diseases. This review describes how machine learning has been applied to proteomics tandem mass spectrometry data. This includes how it can be used to identify proteins suitable for use as biomarkers of disease and for classification of samples into disease or treatment groups, which may be applicable for diagnostics. It also includes the challenges faced by such investigations, such as prediction of proteins present, protein quantification, planning for the use of machine learning, and small sample sizes.",53.0
24099497,Neuroimaging-based biomarkers in psychiatry: clinical opportunities of a paradigm shift,2013 Sep;58(9):499-508.,"Neuroimaging research has substantiated the functional and structural abnormalities underlying psychiatric disorders but has, thus far, failed to have a significant impact on clinical practice. Recently, neuroimaging-based diagnoses and clinical predictions derived from machine learning analysis have shown significant potential for clinical translation. This review introduces the key concepts of this approach, including how the multivariate integration of patterns of brain abnormalities is a crucial component. We survey recent findings that have potential application for diagnosis, in particular early and differential diagnoses in Alzheimer disease and schizophrenia, and the prediction of clinical response to treatment in depression. We discuss the specific clinical opportunities and the challenges for developing biomarkers for psychiatry in the absence of a diagnostic gold standard. We propose that longitudinal outcomes, such as early diagnosis and prediction of treatment response, offer definite opportunities for progress. We propose that efforts should be directed toward clinically challenging predictions in which neuroimaging may have added value, compared with the existing standard assessment. We conclude that diagnostic and prognostic biomarkers will be developed through the joint application of expert psychiatric knowledge in addition to advanced methods of analysis.",38.0
24096823,Application of machine learning algorithms for clinical predictive modeling: a data-mining approach in SCT,2014 Mar;49(3):332-7.,"Data collected from hematopoietic SCT (HSCT) centers are becoming more abundant and complex owing to the formation of organized registries and incorporation of biological data. Typically, conventional statistical methods are used for the development of outcome prediction models and risk scores. However, these analyses carry inherent properties limiting their ability to cope with large data sets with multiple variables and samples. Machine learning (ML), a field stemming from artificial intelligence, is part of a wider approach for data analysis termed data mining (DM). It enables prediction in complex data scenarios, familiar to practitioners and researchers. Technological and commercial applications are all around us, gradually entering clinical research. In the following review, we would like to expose hematologists and stem cell transplanters to the concepts, clinical applications, strengths and limitations of such methods and discuss current research in HSCT. The aim of this review is to encourage utilization of the ML and DM techniques in the field of HSCT, including prediction of transplantation outcome and donor selection.",21.0
24029388,Multivariate classification of blood oxygen level-dependent FMRI data with diagnostic intention: a clinical perspective,2014 May;35(5):848-55.,"There has been a recent upsurge of reports about applications of pattern-recognition techniques from the field of machine learning to functional MR imaging data as a diagnostic tool for systemic brain disease or psychiatric disorders. Entities studied include depression, schizophrenia, attention deficit hyperactivity disorder, and neurodegenerative disorders like Alzheimer dementia. We review these recent studies which-despite the optimism from some articles-predominantly constitute explorative efforts at the proof-of-concept level. There is some evidence that, in particular, support vector machines seem to be promising. However, the field is still far from real clinical application, and much work has to be done regarding data preprocessing, model optimization, and validation. Reporting standards are proposed to facilitate future meta-analyses or systematic reviews.",24.0
24025554,Five years of designing wireless sensor networks in the DoÃ±ana Biological Reserve (Spain): an applications approach,2013 Sep 10;13(9):12044-69.,"Wireless Sensor Networks (WSNs) are a technology that is becoming very popular for many applications, and environmental monitoring is one of its most important application areas. This technology solves the lack of flexibility of wired sensor installations and, at the same time, reduces the deployment costs. To demonstrate the advantages of WSN technology, for the last five years we have been deploying some prototypes in the DoÃ±ana Biological Reserve, which is an important protected area in Southern Spain. These prototypes not only evaluate the technology, but also solve some of the monitoring problems that have been raised by biologists working in DoÃ±ana. This paper presents a review of the work that has been developed during these five years. Here, we demonstrate the enormous potential of using machine learning in wireless sensor networks for environmental and animal monitoring because this approach increases the amount of useful information and reduces the effort that is required by biologists in an environmental monitoring task.",7.0
24013948,A review of feature reduction techniques in neuroimaging,2014 Apr;12(2):229-44.,"Machine learning techniques are increasingly being used in making relevant predictions and inferences on individual subjects neuroimaging scan data. Previous studies have mostly focused on categorical discrimination of patients and matched healthy controls and more recently, on prediction of individual continuous variables such as clinical scores or age. However, these studies are greatly hampered by the large number of predictor variables (voxels) and low observations (subjects) also known as the curse-of-dimensionality or small-n-large-p problem. As a result, feature reduction techniques such as feature subset selection and dimensionality reduction are used to remove redundant predictor variables and experimental noise, a process which mitigates the curse-of-dimensionality and small-n-large-p effects. Feature reduction is an essential step before training a machine learning model to avoid overfitting and therefore improving model prediction accuracy and generalization ability. In this review, we discuss feature reduction techniques used with machine learning in neuroimaging studies.",95.0
23933976,"A review of recent literature employing electroencephalographic techniques to study the pathophysiology, phenomenology, and treatment response of schizophrenia",2013 Sep;15(9):388.,"Clinical experience and research findings suggest that schizophrenia is a disorder comprised of multiple genetic and neurophysiological subtypes with differential response to treatment. Electroencephalography (EEG) is a non-invasive, inexpensive and useful tool for investigating the neurobiology of schizophrenia and its subtypes. EEG studies elucidate the neurophysiological mechanisms potentially underlying clinical symptomatology. In this review article recent advances in applying EEG to study pathophysiology, phenomenology, and treatment response in schizophrenia are discussed. Investigative strategies employed include: analyzing quantitative EEG (QEEG) spectral power during the resting state and cognitive tasks; applying machine learning methods to identify QEEG indicators of diagnosis and treatment response; and using the event-related brain potential (ERP) technique to characterize the neurocognitive processes underlying clinical symptoms. Studies attempting to validate potential EEG biomarkers of schizophrenia and its symptoms, which could be useful in assessing familial risk and treatment response, are also reviewed.",4.0
23933754,Similarity-based machine learning methods for predicting drug-target interactions: a brief review,2014 Sep;15(5):734-47.,"Computationally predicting drug-target interactions is useful to select possible drug (or target) candidates for further biochemical verification. We focus on machine learning-based approaches, particularly similarity-based methods that use drug and target similarities, which show relationships among drugs and those among targets, respectively. These two similarities represent two emerging concepts, the chemical space and the genomic space. Typically, the methods combine these two types of similarities to generate models for predicting new drug-target interactions. This process is also closely related to a lot of work in pharmacogenomics or chemical biology that attempt to understand the relationships between the chemical and genomic spaces. This background makes the similarity-based approaches attractive and promising. This article reviews the similarity-based machine learning methods for predicting drug-target interactions, which are state-of-the-art and have aroused great interest in bioinformatics. We describe each of these methods briefly, and empirically compare these methods under a uniform experimental setting to explore their advantages and limitations.",69.0
23931841,Beyond identification: emerging and future uses for MALDI-TOF mass spectrometry in the clinical microbiology laboratory,2013 Sep;33(3):611-28.,"The routine use of matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) has revolutionized microorganism identification in the clinical microbiology laboratory. Building from these now common microorganism identification strategies, this review explores future clinical applications of MALDI-TOF MS. This includes practical approaches for laboratorians interested in implementing direct identification processing methods for MALDI-TOF detection of microbes in bloodstream infection (BSI) and urinary tract infection (UTI), as well as, post-analytical approaches for classifying MALDI-TOF spectral data to detect characteristics other and species-level identification (e.g. strain-level classification, typing, and resistance mechanisms).",12.0
23926206,Bioinformatics approaches for improved recombinant protein production in Escherichia coli: protein solubility prediction,2014 Nov;15(6):953-62.,"The solubility of recombinant protein expressed in Escherichia coli often represents the production yield. However, up-to-date, instances of successful production of soluble recombinant proteins in E. coli expression system with high yield remain scarce. This is mainly due to the difficulties in improving the overall production capacity, as most of the well-established strategies usually involve a series of trial and error steps with unguaranteed success. One way to concurrently improve the production yield and minimize the production cost would be incorporating the potency of bioinformatics tools to conduct in silico studies, which forecasts the outcome before actual experimental work. In this article, we review and compare seven prediction tools available, which predict the solubility of protein expressed in E. coli, using the following criteria: prediction performance, usability, utility, prediction tool development and validation methodologies. This comprehensive review will be a valuable resource for researchers with limited prior experience in bioinformatics tools. As such, this will facilitate their choice of appropriate tools for studies related to enhancement of intracellular recombinant protein production in E. coli.",15.0
23921828,Sudden event recognition: a survey,2013 Aug 5;13(8):9966-98.,"Event recognition is one of the most active research areas in video surveillance fields. Advancement in event recognition systems mainly aims to provide convenience, safety and an efficient lifestyle for humanity. A precise, accurate and robust approach is necessary to enable event recognition systems to respond to sudden changes in various uncontrolled environments, such as the case of an emergency, physical threat and a fire or bomb alert. The performance of sudden event recognition systems depends heavily on the accuracy of low level processing, like detection, recognition, tracking and machine learning algorithms. This survey aims to detect and characterize a sudden event, which is a subset of an abnormal event in several video surveillance applications. This paper discusses the following in detail: (1) the importance of a sudden event over a general anomalous event; (2) frameworks used in sudden event recognition; (3) the requirements and comparative studies of a sudden event recognition system and (4) various decision-making approaches for sudden event recognition. The advantages and drawbacks of using 3D images from multiple cameras for real-time application are also discussed. The paper concludes with suggestions for future research directions in sudden event recognition.",1.0
23896182,Scoping review on search queries and social media for disease surveillance: a chronology of innovation,2013 Jul 18;15(7):e147.,"Background:                    The threat of a global pandemic posed by outbreaks of influenza H5N1 (1997) and Severe Acute Respiratory Syndrome (SARS, 2002), both diseases of zoonotic origin, provoked interest in improving early warning systems and reinforced the need for combining data from different sources. It led to the use of search query data from search engines such as Google and Yahoo! as an indicator of when and where influenza was occurring. This methodology has subsequently been extended to other diseases and has led to experimentation with new types of social media for disease surveillance.              Objective:                    The objective of this scoping review was to formally assess the current state of knowledge regarding the use of search queries and social media for disease surveillance in order to inform future work on early detection and more effective mitigation of the effects of foodborne illness.              Methods:                    Structured scoping review methods were used to identify, characterize, and evaluate all published primary research, expert review, and commentary articles regarding the use of social media in surveillance of infectious diseases from 2002-2011.              Results:                    Thirty-two primary research articles and 19 reviews and case studies were identified as relevant. Most relevant citations were peer-reviewed journal articles (29/32, 91%) published in 2010-11 (28/32, 88%) and reported use of a Google program for surveillance of influenza. Only four primary research articles investigated social media in the context of foodborne disease or gastroenteritis. Most authors (21/32 articles, 66%) reported that social media-based surveillance had comparable performance when compared to an existing surveillance program. The most commonly reported strengths of social media surveillance programs included their effectiveness (21/32, 66%) and rapid detection of disease (21/32, 66%). The most commonly reported weaknesses were the potential for false positive (16/32, 50%) and false negative (11/32, 34%) results. Most authors (24/32, 75%) recommended that social media programs should primarily be used to support existing surveillance programs.              Conclusions:                    The use of search queries and social media for disease surveillance are relatively recent phenomena (first reported in 2006). Both the tools themselves and the methodologies for exploiting them are evolving over time. While their accuracy, speed, and cost compare favorably with existing surveillance systems, the primary challenge is to refine the data signal by reducing surrounding noise. Further developments in digital disease surveillance have the potential to improve sensitivity and specificity, passively through advances in machine learning and actively through engagement of users. Adoption, even as supporting systems for existing surveillance, will entail a high level of familiarity with the tools and collaboration across jurisdictions.",61.0
23889055,Recent advances in predicting protein classification and their applications to drug development,2013;13(14):1622-35.,"With the explosion of protein sequences generated in the postgenomic era, the gap between the number of attribute- known proteins and that of uncharacterized ones has become increasingly large. Knowing the key attributes of proteins is a shortcut for prioritizing drug targets and developing novel new drugs. Unfortunately, it is both time-consuming and costly to acquire these kinds of information by purely conducting biological experiments. Therefore, it is highly desired to develop various computational tools for fast and effectively classifying proteins according to their sequence information alone. The process of developing these high throughput tools is generally involved with the following procedures: (1) constructing benchmark datasets; (2) representing a protein sequence with a discrete numerical model; (3) developing or introducing a powerful algorithm or machine learning operator to conduct the prediction; (4) estimating the anticipated accuracy with a proper and objective test method; and (5) establishing a user-friendly web-server accessible to the public. This minireview is focused on the recent progresses in identifying the types of G-protein coupled receptors (GPCRs), subcellular localization of proteins, DNA-binding proteins and their binding sites. All these identification tools may provide very useful informations for in-depth study of drug metabolism.",3.0
23889047,ENZPRED-enzymatic protein class predicting by machine learning,2013;13(14):1674-80.,"Recent times have seen flooding of biological data into the scientific community. Due to increase in large amounts of data from genome and other sequencing projects become available, being diverted on to Insilco approach for data collection and prediction has become a priority also progresses in sequencing technologies have found an exponential function rise in the number of newly found enzymes. Commonly, function of such enzymes is determined by experiments that can be time consuming and costly. As new approaches are needed to determine the functions of the proteins these genes encode. The protein parameters that can be used for an enzyme/ non-enzyme classification includes features of sequences like amino acid composition, dipeptide composition, grand Average of hydropathicity (GRAVY), probability of being in alpha helix, probability of being in beta sheet Probability of being in a turn. We show how large-scale computational analysis can help to address this challenge by help of java and support vector machine library. In this paper, a recently developed machine learning algorithm referred to as the svm library Learning Machine is used to classify protein sequences with six main classes of enzyme data downloaded from a public domain database. Comparative studies on different type of kernel methods like 1.radial basis function, 2.polynomial available in SVM library. Results show that RBF method take less time in training and give more accurate result then other kernel methods to also less training time compared to other kernel methods. The classification accuracy of RBF is also higher than various methods in respect of available sequences data.",1.0
23872922,Prediction of RNA binding proteins comes of age from low resolution to high resolution,2013 Oct;9(10):2417-25.,"Networks of protein-RNA interactions is likely to be larger than protein-protein and protein-DNA interaction networks because RNA transcripts are encoded tens of times more than proteins (e.g. only 3% of human genome coded for proteins), have diverse function and localization, and are controlled by proteins from birth (transcription) to death (degradation). This massive network is evidenced by several recent experimental discoveries of large numbers of previously unknown RNA-binding proteins (RBPs). Meanwhile, more than 400 non-redundant protein-RNA complex structures (at 25% sequence identity or less) have been deposited into the protein databank. These sequences and structural resources for RBPs provide ample data for the development of computational techniques dedicated to RBP prediction, as experimentally determining RNA-binding functions is time-consuming and expensive. This review compares traditional machine-learning based approaches with emerging template-based methods at several levels of prediction resolution ranging from two-state binding/non-binding prediction, to binding residue prediction and protein-RNA complex structure prediction. The analysis indicates that the two approaches are complementary and their combinations may lead to further improvements.",14.0
23848274,A review of protein function prediction under machine learning perspective,2013 Aug;7(2):122-41.,"Protein function prediction is one of the most challenging problems in the post-genomic era. The number of newly identified proteins has been exponentially increasing with the advances of the high-throughput techniques. However, the functional characterization of these new proteins was not incremented in the same proportion. To fill this gap, a large number of computational methods have been proposed in the literature. Early approaches have explored homology relationships to associate known functions to the newly discovered proteins. Nevertheless, these approaches tend to fail when a new protein is considerably different (divergent) from previously known ones. Accordingly, more accurate approaches, that use expressive data representation and explore sophisticate computational techniques are required. Regarding these points, this review provides a comprehensible description of machine learning approaches that are currently applied to protein function prediction problems. We start by defining several problems enrolled in understanding protein function aspects, and describing how machine learning can be applied to these problems. We aim to expose, in a systematical framework, the role of these techniques in protein function inference, sometimes difficult to follow up due to the rapid evolvement of the field. With this purpose in mind, we highlight the most representative contributions, the recent advancements, and provide an insightful categorization and classification of machine learning methods in functional proteomics.",7.0
23829390,"Challenges, issues and trends in fall detection systems",2013 Jul 6;12:66.,"Since falls are a major public health problem among older people, the number of systems aimed at detecting them has increased dramatically over recent years. This work presents an extensive literature review of fall detection systems, including comparisons among various kinds of studies. It aims to serve as a reference for both clinicians and biomedical engineers planning or conducting field investigations. Challenges, issues and trends in fall detection have been identified after the reviewing work. The number of studies using context-aware techniques is still increasing but there is a new trend towards the integration of fall detection into smartphones as well as the use of machine learning methods in the detection algorithm. We have also identified challenges regarding performance under real-life conditions, usability, and user acceptance as well as issues related to power consumption, real-time operations, sensing limitations, privacy and record of real-life falls.",48.0
23818492,Identifying driver mutations from sequencing data of heterogeneous tumors in the era of personalized genome sequencing,2014 Mar;15(2):244-55.,"Distinguishing driver mutations from passenger mutations is critical to the understanding of the molecular mechanisms of carcinogenesis and for identifying prognostic and diagnostic markers as well as therapeutic targets. We reviewed the current approaches and software for identifying driver mutations from passenger mutations including both biology-based approaches and machine-learning-based approaches. We also reviewed approaches to identify driver mutations in the context of pathways or gene sets. Finally, we discussed the challenges of predicting driver mutations considering the complexities of inter- and intra-tumor heterogeneity as well as the evolution and progression of tumors.",14.0
23793982,Survey of encoding and decoding of visual stimulus via FMRI: an image analysis perspective,2014 Mar;8(1):7-23.,"A variety of exciting scientific achievements have been made in the last few decades in brain encoding and decoding via functional magnetic resonance imaging (fMRI). This trend continues to rise in recent years, as evidenced by the increasing number of published papers in this topic and several published survey papers addressing different aspects of research issues. Essentially, these survey articles were mainly from cognitive neuroscience and neuroimaging perspectives, although computational challenges were briefly discussed. To complement existing survey articles, this paper focuses on the survey of the variety of image analysis methodologies, such as neuroimage registration, fMRI signal analysis, ROI (regions of interest) selection, machine learning algorithms, reproducibility analysis, structural and functional connectivity, and natural image analysis, which were employed in previous brain encoding/decoding research works. This paper also provides discussions of potential limitations of those image analysis methodologies and possible future improvements. It is hoped that extensive discussions of image analysis issues could contribute to the advancements of the increasingly important brain encoding/decoding field.",4.0
23787338,Representation learning: a review and new perspectives,2013 Aug;35(8):1798-828.,"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",387.0
23735485,Forecasting respiratory collapse: theory and practice for averting life-threatening infant apneas,2013 Nov 1;189(2):223-31.,"Apnea of prematurity is a common disorder of respiratory control among preterm infants, with potentially serious adverse consequences on infant development. We review the capability for automatically assessing apnea risk and predicting apnea episodes from multimodal physiological measurements, and for using this knowledge to provide timely therapeutic intervention. We also review other, similar clinical domains of respiratory distress assessment and prediction in the hope of gaining useful insights. We propose an algorithmic framework for constructing discriminative feature vectors from physiological measurements, and for building robust and effective statistical models for apnea assessment and prediction.",10.0
23731483,Machine learning and genome annotation: a match meant to be?,2013 May 29;14(5):205.,"By its very nature, genomics produces large, high-dimensional datasets that are well suited to analysis by machine learning approaches. Here, we explain some key aspects of machine learning that make it useful for genome annotation, with illustrative examples from ENCODE.",20.0
23722209,Cellular-resolution connectomics: challenges of dense neural circuit reconstruction,2013 Jun;10(6):501-7.,"Neuronal networks are high-dimensional graphs that are packed into three-dimensional nervous tissue at extremely high density. Comprehensively mapping these networks is therefore a major challenge. Although recent developments in volume electron microscopy imaging have made data acquisition feasible for circuits comprising a few hundreds to a few thousands of neurons, data analysis is massively lagging behind. The aim of this perspective is to summarize and quantify the challenges for data analysis in cellular-resolution connectomics and describe current solutions involving online crowd-sourcing and machine-learning approaches.",81.0
23700999,Evolutionary computation and QSAR research,2013 Jun;9(2):206-25.,"The successful high throughput screening of molecule libraries for a specific biological property is one of the main improvements in drug discovery. The virtual molecular filtering and screening relies greatly on quantitative structure-activity relationship (QSAR) analysis, a mathematical model that correlates the activity of a molecule with molecular descriptors. QSAR models have the potential to reduce the costly failure of drug candidates in advanced (clinical) stages by filtering combinatorial libraries, eliminating candidates with a predicted toxic effect and poor pharmacokinetic profiles, and reducing the number of experiments. To obtain a predictive and reliable QSAR model, scientists use methods from various fields such as molecular modeling, pattern recognition, machine learning or artificial intelligence. QSAR modeling relies on three main steps: molecular structure codification into molecular descriptors, selection of relevant variables in the context of the analyzed activity, and search of the optimal mathematical model that correlates the molecular descriptors with a specific activity. Since a variety of techniques from statistics and artificial intelligence can aid variable selection and model building steps, this review focuses on the evolutionary computation methods supporting these tasks. Thus, this review explains the basic of the genetic algorithms and genetic programming as evolutionary computation approaches, the selection methods for high-dimensional data in QSAR, the methods to build QSAR models, the current evolutionary feature selection methods and applications in QSAR and the future trend on the joint or multi-task feature selection methods.",2.0
23686810,"Automated analysis of diabetic retinopathy images: principles, recent developments, and emerging trends",2013 Aug;13(4):453-9.,"Diabetic retinopathy (DR) is a vision-threatening complication of diabetes. Timely diagnosis and intervention are essential for treatment that reduces the risk of vision loss. A good color retinal (fundus) photograph can be used as a surrogate for face-to-face evaluation of DR. The use of computers to assist or fully automate DR evaluation from retinal images has been studied for many years. Early work showed promising results for algorithms in detecting and classifying DR pathology. Newer techniques include those that adapt machine learning technology to DR image analysis. Challenges remain, however, that must be overcome before fully automatic DR detection and analysis systems become practical clinical tools.",6.0
23651478,A novel integrated framework and improved methodology of computer-aided drug design,2013;13(9):965-88.,"Computer-aided drug design (CADD) is a critical initiating step of drug development, but a single model capable of covering all designing aspects remains to be elucidated. Hence, we developed a drug design modeling framework that integrates multiple approaches, including machine learning based quantitative structure-activity relationship (QSAR) analysis, 3D-QSAR, Bayesian network, pharmacophore modeling, and structure-based docking algorithm. Restrictions for each model were defined for improved individual and overall accuracy. An integration method was applied to join the results from each model to minimize bias and errors. In addition, the integrated model adopts both static and dynamic analysis to validate the intermolecular stabilities of the receptor-ligand conformation. The proposed protocol was applied to identifying HER2 inhibitors from traditional Chinese medicine (TCM) as an example for validating our new protocol. Eight potent leads were identified from six TCM sources. A joint validation system comprised of comparative molecular field analysis, comparative molecular similarity indices analysis, and molecular dynamics simulation further characterized the candidates into three potential binding conformations and validated the binding stability of each protein-ligand complex. The ligand pathway was also performed to predict the ligand ""in"" and ""exit"" from the binding site. In summary, we propose a novel systematic CADD methodology for the identification, analysis, and characterization of drug-like candidates.",38.0
23637855,A plea for neutral comparison studies in computational sciences,2013 Apr 24;8(4):e61562.,"In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of ""new methods"", while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a ""tidy neutral comparison study"". R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/plea2013.",14.0
23622061,Decision making: from neuroscience to psychiatry,2013 Apr 24;78(2):233-48.,"Adaptive behaviors increase the likelihood of survival and reproduction and improve the quality of life. However, it is often difficult to identify optimal behaviors in real life due to the complexity of the decision maker's environment and social dynamics. As a result, although many different brain areas and circuits are involved in decision making, evolutionary and learning solutions adopted by individual decision makers sometimes produce suboptimal outcomes. Although these problems are exacerbated in numerous neurological and psychiatric disorders, their underlying neurobiological causes remain incompletely understood. In this review, theoretical frameworks in economics and machine learning and their applications in recent behavioral and neurobiological studies are summarized. Examples of such applications in clinical domains are also discussed for substance abuse, Parkinson's disease, attention-deficit/hyperactivity disorder, schizophrenia, mood disorders, and autism. Findings from these studies have begun to lay the foundations necessary to improve diagnostics and treatment for various neurological and psychiatric disorders.",38.0
23564629,Evaluating temporal relations in clinical text: 2012 i2b2 Challenge,Sep-Oct 2013;20(5):806-13.,"Background:                    The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions.              Methods:                    The challenge evaluated systems on the information extraction tasks that targeted: (1) clinically significant events, including both clinical concepts such as problems, tests, treatments, and clinical departments, and events relevant to the patient's clinical timeline, such as admissions, transfers between departments, etc; (2) temporal expressions, referring to the dates, times, durations, or frequencies phrases in the clinical text. The values of the extracted temporal expressions had to be normalized to an ISO specification standard; and (3) temporal relations, between the clinical events and temporal expressions. Participants determined pairs of events and temporal expressions that exhibited a temporal relation, and identified the temporal relation between them.              Results:                    For event detection, statistical machine learning (ML) methods consistently showed superior performance. While ML and rule based methods seemed to detect temporal expressions equally well, the best systems overwhelmingly adopted a rule based approach for value normalization. For temporal relation classification, the systems using hybrid approaches that combined ML and heuristics based methods produced the best results.",99.0
23559637,Pattern recognition in bioinformatics,2013 Sep;14(5):633-47.,"Pattern recognition is concerned with the development of systems that learn to solve a given problem using a set of example instances, each represented by a number of features. These problems include clustering, the grouping of similar instances; classification, the task of assigning a discrete label to a given instance; and dimensionality reduction, combining or selecting features to arrive at a more useful representation. The use of statistical pattern recognition algorithms in bioinformatics is pervasive. Classification and clustering are often applied to high-throughput measurement data arising from microarray, mass spectrometry and next-generation sequencing experiments for selecting markers, predicting phenotype and grouping objects or genes. Less explicitly, classification is at the core of a wide range of tools such as predictors of genes, protein function, functional or genetic interactions, etc., and used extensively in systems biology. A course on pattern recognition (or machine learning) should therefore be at the core of any bioinformatics education program. In this review, we discuss the main elements of a pattern recognition course, based on material developed for courses taught at the BSc, MSc and PhD levels to an audience of bioinformaticians, computer scientists and life scientists. We pay attention to common problems and pitfalls encountered in applications and in interpretation of the results obtained.",9.0
23552814,Mathematical modeling of infectious disease dynamics,2013 May 15;4(4):295-306.,"Over the last years, an intensive worldwide effort is speeding up the developments in the establishment of a global surveillance network for combating pandemics of emergent and re-emergent infectious diseases. Scientists from different fields extending from medicine and molecular biology to computer science and applied mathematics have teamed up for rapid assessment of potentially urgent situations. Toward this aim mathematical modeling plays an important role in efforts that focus on predicting, assessing, and controlling potential outbreaks. To better understand and model the contagious dynamics the impact of numerous variables ranging from the micro host-pathogen level to host-to-host interactions, as well as prevailing ecological, social, economic, and demographic factors across the globe have to be analyzed and thoroughly studied. Here, we present and discuss the main approaches that are used for the surveillance and modeling of infectious disease dynamics. We present the basic concepts underpinning their implementation and practice and for each category we give an annotated list of representative works.",49.0
23549108,The role of technology and engineering models in transforming healthcare,2013;6:156-77.,"The healthcare system is in crisis due to challenges including escalating costs, the inconsistent provision of care, an aging population, and high burden of chronic disease related to health behaviors. Mitigating this crisis will require a major transformation of healthcare to be proactive, preventive, patient-centered, and evidence-based with a focus on improving quality-of-life. Information technology, networking, and biomedical engineering are likely to be essential in making this transformation possible with the help of advances, such as sensor technology, mobile computing, machine learning, etc. This paper has three themes: 1) motivation for a transformation of healthcare; 2) description of how information technology and engineering can support this transformation with the help of computational models; and 3) a technical overview of several research areas that illustrate the need for mathematical modeling approaches, ranging from sparse sampling to behavioral phenotyping and early detection. A key tenet of this paper concerns complementing prior work on patient-specific modeling and simulation by modeling neuropsychological, behavioral, and social phenomena. The resulting models, in combination with frequent or continuous measurements, are likely to be key components of health interventions to enhance health and wellbeing and the provision of healthcare.",8.0
23548028,Applied computational techniques on schizophrenia using genetic mutations,2013;13(5):675-84.,"Schizophrenia is a complex disease, with both genetic and environmental influence. Machine learning techniques can be used to associate different genetic variations at different genes with a (schizophrenic or non-schizophrenic) phenotype. Several machine learning techniques were applied to schizophrenia data to obtain the results presented in this study. Considering these data, Quantitative Genotype - Disease Relationships (QDGRs) can be used for disease prediction. One of the best machine learning-based models obtained after this exhaustive comparative study was implemented online; this model is an artificial neural network (ANN). Thus, the tool offers the possibility to introduce Single Nucleotide Polymorphism (SNP) sequences in order to classify a patient with schizophrenia. Besides this comparative study, a method for variable selection, based on ANNs and evolutionary computation (EC), is also presented. This method uses half the number of variables as the original ANN and the variables obtained are among those found in other publications. In the future, QDGR models based on nucleic acid information could be expanded to other diseases.",1.0
23548026,Machine learning and social network analysis applied to Alzheimer's disease biomarkers,2013;13(5):652-62.,"Due to the fact that the number of deaths due Alzheimer is increasing, the scientists have a strong interest in early stage diagnostic of this disease. Alzheimer's patients show different kind of brain alterations, such as morphological, biochemical, functional, etc. Currently, using magnetic resonance imaging techniques is possible to obtain a huge amount of biomarkers; being difficult to appraise which of them can explain more properly how the pathology evolves instead of the normal ageing. Machine Learning methods facilitate an efficient analysis of complex data and can be used to discover which biomarkers are more informative. Moreover, automatic models can learn from historical data to suggest the diagnostic of new patients. Social Network Analysis (SNA) views social relationships in terms of network theory consisting of nodes and connections. The resulting graph-based structures are often very complex; there can be many kinds of connections between the nodes. SNA has emerged as a key technique in modern sociology. It has also gained a significant following in medicine, anthropology, biology, information science, etc., and has become a popular topic of speculation and study. This paper presents a review of machine learning and SNA techniques and then, a new approach to analyze the magnetic resonance imaging biomarkers with these techniques, obtaining relevant relationships that can explain the different phenotypes in dementia, in particular, different stages of Alzheimer's disease.",2.0
23526775,Latest developments in molecular docking: 2010-2011 in review,2013 May;26(5):215-39.,"The aim of docking is to accurately predict the structure of a ligand within the constraints of a receptor binding site and to correctly estimate the strength of binding. We discuss, in detail, methodological developments that occurred in the docking field in 2010 and 2011, with a particular focus on the more difficult, and sometimes controversial, aspects of this promising computational discipline. The main developments in docking in this period, covered in this review, are receptor flexibility, solvation, fragment docking, postprocessing, docking into homology models, and docking comparisons. Several new, or at least newly invigorated, advances occurred in areas such as nonlinear scoring functions, using machine-learning approaches. This review is strongly focused on docking advances in the context of drug design, specifically in virtual screening and fragment-based drug design. Where appropriate, we refer readers to exemplar case studies.",69.0
23506300,The emerging role of photorespiration and non-photorespiratory peroxisomal metabolism in pathogen defence,2013 Jul;15(4):723-36.,"Photorespiration represents one of the major highways of primary plant metabolism and is the most prominent example of metabolic cell organelle integration, since the pathway requires the concerted action of plastidial, peroxisomal, mitochondrial and cytosolic enzymes and organellar transport proteins. Oxygenation of ribulose-1,5-bisphosphate by Rubisco leads to the formation of large amounts of 2-phosphoglycolate, which are recycled to 3-phosphoglycerate by the photorespiratory C2 cycle, concomitant with stoichiometric production rates of H2 O2 in peroxisomes. Apart from its significance for agricultural productivity, a secondary function of photorespiration in pathogen defence has emerged only recently. Here, we summarise literature data supporting the crosstalk between photorespiration and pathogen defence and perform a meta-expression analysis of photorespiratory genes during pathogen attack. Moreover, we screened Arabidopsis proteins newly predicted using machine learning methods to be targeted to peroxisomes, the central H2 O2 -producing organelle of photorespiration, for homologues of known pathogen defence proteins and analysed their expression during pathogen infection. The analyses further support the idea that photorespiration and non-photorespiratory peroxisomal metabolism play multi-faceted roles in pathogen defence beyond metabolism of reactive oxygen species.",23.0
23431257,Prediction of deleterious nonsynonymous single-nucleotide polymorphism for human diseases,2013;2013:675851.,"The identification of genetic variants that are responsible for human inherited diseases is a fundamental problem in human and medical genetics. As a typical type of genetic variation, nonsynonymous single-nucleotide polymorphisms (nsSNPs) occurring in protein coding regions may alter the encoded amino acid, potentially affect protein structure and function, and further result in human inherited diseases. Therefore, it is of great importance to develop computational approaches to facilitate the discrimination of deleterious nsSNPs from neutral ones. In this paper, we review databases that collect nsSNPs and summarize computational methods for the identification of deleterious nsSNPs. We classify the existing methods for characterizing nsSNPs into three categories (sequence based, structure based, and annotation based), and we introduce machine learning models for the prediction of deleterious nsSNPs. We further discuss methods for identifying deleterious nsSNPs in noncoding variants and those for dealing with rare variants.",23.0
23410165,Predicting targeted polypharmacology for drug repositioning and multi- target drug discovery,2013;20(13):1646-61.,"Prediction of polypharmacology of known drugs and new molecules against selected multiple targets is highly useful for finding new therapeutic applications of existing drugs (drug repositioning) and for discovering multi-target drugs with improved therapeutic efficacies by collective regulations of primary therapeutic targets, compensatory signalling and drug resistance mechanisms. In this review, we describe recent progresses in exploration of in-silico methods for predicting polypharmacology of known drugs and new molecules by means of structure-based (molecular docking, binding- site structural similarity, receptor-based pharmacophore searching), expression-based (expression profile/signature similarity disease-drug and drug-drug networks), ligand-based (similarity searching, side-effect similarity, QSAR, machine learning), and fragment-based approaches that have shown promising potential in facilitating drug repositioning and the discovery of multi-target drugs.",16.0
23396601,"Meta-analyses of microarrays of Arabidopsis asymmetric leaves1 (as1), as2 and their modifying mutants reveal a critical role for the ETT pathway in stabilization of adaxial-abaxial patterning and cell division during leaf development",2013 Mar;54(3):418-31.,"It is necessary to use algorithms to analyze gene expression data from DNA microarrays, such as in clustering and machine learning. Previously, we developed the knowledge-based fuzzy adaptive resonance theory (KB-FuzzyART), a clustering algorithm suitable for analyzing gene expression data, to find clues for identifying gene networks. Leaf primordia form around the shoot apical meristem (SAM), which consists of indeterminate stem cells. Upon initiation of leaf development, adaxial-abaxial patterning is crucial for lateral expansion, via cellular proliferation, and the formation of flat symmetric leaves. Many regulatory genes that specify such patterning have been identified. Analysis by the KB-FuzzyART and subsequent molecular and genetic analyses previously showed that ASYMMETRIC LEAVES1 (AS1) and AS2 repress the expression of some abaxial-determinant genes, such as AUXIN RESPONSE FACTOR3 (ARF3)/ETTIN (ETT) and ARF4, which are responsible for defects in leaf adaxial-abaxial polarity in as1 and as2. In the present study, genetic analysis revealed that ARF3/ETT and ARF4 were regulated by modifier genes, BOBBER1 (BOB1) and ELONGATA3 (ELO3), together with AS1-AS2. We analyzed expression arrays with as2 elo3 and as2 bob1, and extracted genes downstream of ARF3/ETT by using KB-FuzzyART and molecular analyses. The results showed that expression of Kip-related protein (KRP) (for inhibitors of cyclin-dependent protein kinases) and Isopentenyltransferase (IPT) (for biosynthesis of cytokinin) genes were controlled by AS1-AS2 through ARF3/ETT and ARF4 functions, which suggests that the AS1-AS2-ETT pathway plays a critical role in controlling the cell division cycle and the biosynthesis of cytokinin around SAM to stabilize leaf development in Arabidopsis thaliana.",12.0
23392334,Bridging paradigms: hybrid mechanistic-discriminative predictive models,2013 Mar;60(3):735-42.,"Many disease processes are extremely complex and characterized by multiple stochastic processes interacting simultaneously. Current analytical approaches have included mechanistic models and machine learning (ML), which are often treated as orthogonal viewpoints. However, to facilitate truly personalized medicine, new perspectives may be required. This paper reviews the use of both mechanistic models and ML in healthcare as well as emerging hybrid methods, which are an exciting and promising approach for biologically based, yet data-driven advanced intelligent systems.",3.0
23382431,Global mapping of infectious disease,2013 Feb 4;368(1614):20120250.,"The primary aim of this review was to evaluate the state of knowledge of the geographical distribution of all infectious diseases of clinical significance to humans. A systematic review was conducted to enumerate cartographic progress, with respect to the data available for mapping and the methods currently applied. The results helped define the minimum information requirements for mapping infectious disease occurrence, and a quantitative framework for assessing the mapping opportunities for all infectious diseases. This revealed that of 355 infectious diseases identified, 174 (49%) have a strong rationale for mapping and of these only 7 (4%) had been comprehensively mapped. A variety of ambitions, such as the quantification of the global burden of infectious disease, international biosurveillance, assessing the likelihood of infectious disease outbreaks and exploring the propensity for infectious disease evolution and emergence, are limited by these omissions. An overview of the factors hindering progress in disease cartography is provided. It is argued that rapid improvement in the landscape of infectious diseases mapping can be made by embracing non-conventional data sources, automation of geo-positioning and mapping procedures enabled by machine learning and information technology, respectively, in addition to harnessing labour of the volunteer 'cognitive surplus' through crowdsourcing.",68.0
23365831,Applications of supervised learning to biological signals: ECG signal quality and systemic vascular resistance,2012;2012:57-60.,"Discovering information encoded in non-invasively recorded biosignals which belies an individual's well-being can help facilitate the development of low-cost unobtrusive medical device technologies, or enable the unsupervised performance of physiological assessments without excessive oversight from trained clinical personnel. Although the unobtrusive or unsupervised nature of such technologies often results in less accurate measures than their invasive or supervised counterparts, this disadvantage is typically outweighed by the ability to monitor larger populations than ever before. The expected consequential benefit will be an improvement in healthcare provision and health outcomes for all. The process of discovering indicators of health in unsupervised or unobtrusive biosignal recordings, or automatically ensuring the validity and quality of such signals, is best realized when following a proven systematic methodology. This paper provides a brief tutorial review of supervised learning, which is a sub-discipline of machine learning, and discusses its application in the development of algorithms to interpret biosignals acquired in unsupervised or semi-supervised environments, with the aim of estimating well-being. Some specific examples in the disparate application areas of telehealth electrocardiogram recording and calculating post-operative systemic vascular resistance are discussed in the context of this systematic approach for information discovery.",1.0
23349097,"Toward a ""structural BLAST"": using structural relationships to infer function",2013 Apr;22(4):359-66.,"We outline a set of strategies to infer protein function from structure. The overall approach depends on extensive use of homology modeling, the exploitation of a wide range of global and local geometric relationships between protein structures and the use of machine learning techniques. The combination of modeling with broad searches of protein structure space defines a ""structural BLAST"" approach to infer function with high genomic coverage. Applications are described to the prediction of protein-protein and protein-ligand interactions. In the context of protein-protein interactions, our structure-based prediction algorithm, PrePPI, has comparable accuracy to high-throughput experiments. An essential feature of PrePPI involves the use of Bayesian methods to combine structure-derived information with non-structural evidence (e.g. co-expression) to assign a likelihood for each predicted interaction. This, combined with a structural BLAST approach significantly expands the range of applications of protein structure in the annotation of protein function, including systems level biological applications where it has previously played little role.",8.0
23335577,Human semi-supervised learning,2013 Jan;5(1):132-72.,"Most empirical work in human categorization has studied learning in either fully supervised or fully unsupervised scenarios. Most real-world learning scenarios, however, are semi-supervised: Learners receive a great deal of unlabeled information from the world, coupled with occasional experiences in which items are directly labeled by a knowledgeable source. A large body of work in machine learning has investigated how learning can exploit both labeled and unlabeled data provided to a learner. Using equivalences between models found in human categorization and machine learning research, we explain how these semi-supervised techniques can be applied to human learning. A series of experiments are described which show that semi-supervised learning models prove useful for explaining human behavior when exposed to both labeled and unlabeled data. We then discuss some machine learning models that do not have familiar human categorization counterparts. Finally, we discuss some challenges yet to be addressed in the use of semi-supervised models for modeling human categorization.",7.0
23321025,A structured approach to predictive modeling of a two-class problem using multidimensional data sets,2013 May 15;61(1):73-85.,"Biological experiments in the post-genome era can generate a staggering amount of complex data that challenges experimentalists to extract meaningful information. Increasingly, the success of an appropriately controlled experiment relies on a robust data analysis pipeline. In this paper, we present a structured approach to the analysis of multidimensional data that relies on a close, two-way communication between the bioinformatician and experimentalist. A sequential approach employing data exploration (visualization, graphical and analytical study), pre-processing, feature reduction and supervised classification using machine learning is presented. This standardized approach is illustrated by an example from a proteomic data analysis that has been used to predict the risk of infectious disease outcome. Strategies for model selection and post hoc model diagnostics are presented and applied to the case illustration. We discuss some of the practical lessons we have learned applying supervised classification to multidimensional data sets, one of which is the importance of feature reduction in achieving optimal modeling performance.",7.0
23101052,Machine Learning Methods in Systematic Reviews: Identifying Quality Improvement Intervention Evaluations [Internet],,"Background:                    Electronic searches typically yield far more citations than are relevant, and reviewers spend a substantial amount of time screening titles and abstracts to identify potential studies eligible for inclusion in a review. This is of particular relevance in complex research fields such as quality improvement. We tested a semiautomated literature screening process applied to the title and abstract screening stage of systematic reviews. A machine learning approach may allow literature reviewers to screen only a fraction of a search output and to use a predictive model to learn and then emulate the reviewersâ decisions. Once learned, the model can apply the selection process to an essentially unlimited number of citations.              Method:                    Two independent literature reviewers screened 1,591 quasi-randomly selected citations in a training dataset used to predict decisions on the remaining citations in a MEDLINE search output of 9,395 citations. We explored different prediction algorithms and tested results against reference samples screened by experts in quality improvement. Qualitative (relevance cutoff determined in ROC curve) and quantitative predictions (probability rank order of citations) were determined.              Results:                    The agreement between independent literature reviewers ranged from Îº= 0.55 to 0.57. Across two reference samples, the predictive performance of the machine learning approach demonstrated 90.1 percent sensitivity, 43.9 percent specificity, and 32.1 percent PPV. This translates to a reduction of 36.1 percent in citation screening if applied. The predictive performance was affected by reviewer disagreements: a subgroup analysis restricted to citations both reviewers agreed on showed a sensitivity of 98.8 percent (specificity 43.9 percent).              Conclusion:                    Machine learning approaches may assist in the title and abstract inclusion screening process in systematic reviews of complex, steadily expanding research fields such as quality improvement. Increased reviewer agreement appeared to be associated with improved predictive performance.",
23071097,Popular computational methods to assess multiprotein complexes derived from label-free affinity purification and mass spectrometry (AP-MS) experiments,2013 Jan;12(1):1-13.,"Advances in sensitivity, resolution, mass accuracy, and throughput have considerably increased the number of protein identifications made via mass spectrometry. Despite these advances, state-of-the-art experimental methods for the study of protein-protein interactions yield more candidate interactions than may be expected biologically owing to biases and limitations in the experimental methodology. In silico methods, which distinguish between true and false interactions, have been developed and applied successfully to reduce the number of false positive results yielded by physical interaction assays. Such methods may be grouped according to: (1) the type of data used: methods based on experiment-specific measurements (e.g., spectral counts or identification scores) versus methods that extract knowledge encoded in external annotations (e.g., public interaction and functional categorisation databases); (2) the type of algorithm applied: the statistical description and estimation of physical protein properties versus predictive supervised machine learning or text-mining algorithms; (3) the type of protein relation evaluated: direct (binary) interaction of two proteins in a cocomplex versus probability of any functional relationship between two proteins (e.g., co-occurrence in a pathway, sub cellular compartment); and (4) initial motivation: elucidation of experimental data by evaluation versus prediction of novel protein-protein interaction, to be experimentally validated a posteriori. This work reviews several popular computational scoring methods and software platforms for protein-protein interactions evaluation according to their methodology, comparative strengths and weaknesses, data representation, accessibility, and availability. The scoring methods and platforms described include: CompPASS, SAINT, Decontaminator, MINT, IntAct, STRING, and FunCoup. References to related work are provided throughout in order to provide a concise but thorough introduction to a rapidly growing interdisciplinary field of investigation.",17.0
23057094,A Pilot Study Using Machine Learning and Domain Knowledge To Facilitate Comparative Effectiveness Review Updating [Internet],,"Background:                    Comparative effectiveness reviews need to be updated frequently to maintain their relevance. Results of earlier screening efforts should be useful in reducing the screening of thousands of newer citations for articles relevant to efficacy/effectiveness and adverse effects (AEs).              Methods:                    We collected 14,700 PubMedÂ® citation classification decisions from a 2007 comparative effectiveness review of interventions to prevent fractures in persons with low bone density (LBD). We also collected 1,307 PubMed citation classification decisions from a 2006 comparative effectiveness review of off-label uses of atypical anti-psychotic drugs (AAP). We first extracted explanatory variables from the MEDLINEÂ® citation related to key concepts, including the intervention, outcome, and study design. We then used the data to empirically derive statistical models (based on sparse generalized linear models with convex penalties [GLMnet] and gradient boosting machine [GBM]) that predicted inclusion in the AAP and LBD reviews. Finally, we evaluated performance on the 11,003 PubMed citations retrieved for the LBD and AAP updated reviews.              Measurements:                    Sensitivity (percentage of relevant citations corrected identified), positive predictive value (PPV, percentage of predicted relevant citations that were truly relevant), and workload reduction (percentage of screening avoided).              Results:                    GLMnet- and GBM-based models performed similarly, with GLMnet (results shown below) performing slightly better. The GLMnet-based model yielded sensitivities of 0.921 and 0.905 and PPVs of 0.185 and 0.102 when predicting articles relevant to the AAP and LBD efficacy/effectiveness analyses respectively (using a threshold of p â¥0.02). GLMnet performed better when identifying AE-relevant articles for the AAP review (sensitivity=0.981) than for the LBD review (0.685). When attempting to maximize sensitivity, GLMnet achieved high sensitivities (0.99 for AAP and 1.0 for LBD) while reducing projected screening by 55.4 percent (1990/3591 articles for AAP) and 63.2 percent (4,454/7,051 for LBD).              Conclusions:                    In this pilot study, we evaluated statistical classifiers that used previous classification decisions and key explanatory variables derived from MEDLINE indexing terms to predict inclusion decisions on two simulated comparative effectiveness review updates. The system achieved higher sensitivity in evaluating efficacy/effectiveness articles than in evaluating LBD AE articles. In the simulation, this prototype system reduced workload associated with screening updated search results for all relevant efficacy/effectiveness and AE articles by more than 50 percent with minimal or no loss of relevant articles. After refinement, these document classification algorithms could help researchers maintain up-to-date reviews.",
23016852,Anti-cancer drug development: computational strategies to identify and target proteins involved in cancer metabolism,2013;19(4):532-77.,"Cancer remains a fundamental burden to public health despite substantial efforts aimed at developing effective chemotherapeutics and significant advances in chemotherapeutic regimens. The major challenge in anti-cancer drug design is to selectively target cancer cells with high specificity. Research into treating malignancies by targeting altered metabolism in cancer cells is supported by computational approaches, which can take a leading role in identifying candidate targets for anti-cancer therapy as well as assist in the discovery and optimisation of anti-cancer agents. Natural products appear to have privileged structures for anti-cancer drug development and the bulk of this particularly valuable chemical space still remains to be explored. In this review we aim to provide a comprehensive overview of current strategies for computer-guided anti-cancer drug development. We start with a discussion of state-of-the art bioinformatics methods applied to the identification of novel anti-cancer targets, including machine learning techniques, the Connectivity Map and biological network analysis. This is followed by an extensive survey of molecular modelling and cheminformatics techniques employed to develop agents targeting proteins involved in the glycolytic, lipid, NAD+, mitochondrial (TCA cycle), amino acid and nucleic acid metabolism of cancer cells. A dedicated section highlights the most promising strategies to develop anti-cancer therapeutics from natural products and the role of metabolism and some of the many targets which are under investigation are reviewed. Recent success stories are reported for all the areas covered in this review. We conclude with a brief summary of the most interesting strategies identified and with an outlook on future directions in anti-cancer drug development.",6.0
23012584,The future of medical diagnostics: large digitized databases,2012 Sep;85(3):363-77.,"The electronic health record mandate within the American Recovery and Reinvestment Act of 2009 will have a far-reaching affect on medicine. In this article, we provide an in-depth analysis of how this mandate is expected to stimulate the production of large-scale, digitized databases of patient information. There is evidence to suggest that millions of patients and the National Institutes of Health will fully support the mining of such databases to better understand the process of diagnosing patients. This data mining likely will reaffirm and quantify known risk factors for many diagnoses. This quantification may be leveraged to further develop computer-aided diagnostic tools that weigh risk factors and provide decision support for health care providers. We expect that creation of these databases will stimulate the development of computer-aided diagnostic support tools that will become an integral part of modern medicine.",10.0
23007437,Ligand- and structure-based pregnane X receptor models,2012;929:359-75.,"The human pregnane X receptor (PXR) is a ligand dependent transcription factor that can be activated by structurally diverse agonists including steroid hormones, bile acids, herbal drugs, and prescription medications. PXR regulates the transcription of several genes involved in xenobiotic detoxification and apoptosis. Activation of PXR has the potential to initiate adverse effects by altering drug pharmacokinetics or perturbing physiological processes. Hence, more reliable prediction of PXR activators would be valuable for pharmaceutical drug discovery to avoid potential toxic effects. Ligand- and protein structure-based computational models for PXR activation have been developed in several studies. There has been limited success with structure-based modeling approaches to predict human PXR activators, which can be attributed to the large and promiscuous site of this protein. Slightly better success has been achieved with ligand-based modeling methods including quantitative structure-activity relationship (QSAR) analysis, pharmacophore modeling and machine learning that use appropriate descriptors to account for the diversity of the ligand classes that bind to PXR. These combined computational approaches using molecular shape information may assist scientists to more confidently identify PXR activators. This chapter reviews the various ligand and structure based methods undertaken to date and their results.",3.0
22952238,Uncovering transcription factor modules using one- and three-dimensional analyses,2012 Sep 7;287(37):30914-21.,"Transcriptional regulation is a critical mediator of many normal cellular processes, as well as disease progression. Transcription factors (TFs) often co-localize at cis-regulatory elements on the DNA, form protein complexes, and collaboratively regulate gene expression. Machine learning and Bayesian approaches have been used to identify TF modules in a one-dimensional context. However, recent studies using high throughput technologies have shown that TF interactions should also be considered in three-dimensional nuclear space. Here, we describe methods for identifying TF modules and discuss how moving from a one-dimensional to a three-dimensional paradigm, along with integrated experimental and computational approaches, can lead to a better understanding of TF association networks.",4.0
22944687,Understanding the substrate specificity of conventional calpains,2012 Sep;393(9):853-71.,"Calpains are intracellular Ca(2+)-dependent Cys proteases that play important roles in a wide range of biological phenomena via the limited proteolysis of their substrates. Genetic defects in calpain genes cause lethality and/or functional deficits in many organisms, including humans. Despite their biological importance, the mechanisms underlying the action of calpains, particularly of their substrate specificities, remain largely unknown. Studies show that certain sequence preferences influence calpain substrate recognition, and some properties of amino acids have been related successfully to substrate specificity and to the calpains' 3D structure. The full spectrum of this substrate specificity, however, has not been clarified using standard sequence analysis algorithms, e.g., the position-specific scoring-matrix method. More advanced bioinformatics techniques were used recently to identify the substrate specificities of calpains and to develop a predictor for calpain cleavage sites, demonstrating the potential of combining empirical data acquisition and machine learning. This review discusses the calpains' substrate specificities, introducing the benefits of bioinformatics applications. In conclusion, machine learning has led to the development of useful predictors for calpain cleavage sites, although the accuracy of the predictions still needs improvement. Machine learning has also elucidated information about the properties of calpains' substrate specificities, including a preference for sequences over secondary structures and the existence of a substrate specificity difference between two similar conventional calpains, which has never been indicated biochemically.",19.0
22842200,Osteoarthritis year 2012 in review: biomarkers,2012 Dec;20(12):1451-64.,"Purpose:                    Biomarkers provide useful diagnostic information by detecting cartilage degradation in osteoarthritis (OA), reflecting disease-relevant biological activity and predicting the course of disease progression. They also serve as surrogate endpoints in the drug discovery process. The aim of this narrative review was to focus on OA biomarker-related papers published between the osteoarthritis research society international (OARSI) 2011 meeting in San Diego and the OARSI 2012 meeting in Barcelona.              Methods:                    The PubMed/MEDLINE and SciVerse Scopus bibliographic databases were searched using the keywords: 'biomarker' and 'osteoarthritis' and/or 'biomarker' and 'proteomics'.              Results:                    Ninety-eight papers were found with the keywords 'biomarker' and 'osteoarthritis'. Fifteen papers were found with the keywords 'biomarker' and 'proteomics'. Review articles were also included. The most relevant published studies focused on extracellular matrix (ECM) molecules in body fluids. Enrichment of the deamidated epitope of cartilage oligomeric matrix protein (D-COMP) suggests that OA disease progression is associated with post-translational modifications that may show specificity for particular joint sites. Fibulin-3 peptides (Fib3-1 and Fib3-2) have been proposed as potential biomarkers of OA along with follistatin-like protein 1 (FSTL1), a new serum biomarker with the capacity to reflect the severity of joint damage. The 'membrane attack complex' (MAC) component of complement has also been implicated in OA.              Conclusion:                    Novel OA biomarkers are needed for sub-clinical disease diagnosis. Proteomic techniques are beginning to yield useful data and deliver new OA biomarkers in serum and urine. Combining biochemical markers with tissue and cell imaging techniques and bioinformatics (i.e., machine learning, clustering, data visualization) may facilitate the development of biomarker combinations enabling earlier detection of OA.",42.0
22830342,Machine learning techniques and drug design,2012;19(25):4289-97.,"The interest in the application of machine learning techniques (MLT) as drug design tools is growing in the last decades. The reason for this is related to the fact that the drug design is very complex and requires the use of hybrid techniques. A brief review of some MLT such as self-organizing maps, multilayer perceptron, bayesian neural networks, counter-propagation neural network and support vector machines is described in this paper. A comparison between the performance of the described methods and some classical statistical methods (such as partial least squares and multiple linear regression) shows that MLT have significant advantages. Nowadays, the number of studies in medicinal chemistry that employ these techniques has considerably increased, in particular the use of support vector machines. The state of the art and the future trends of MLT applications encompass the use of these techniques to construct more reliable QSAR models. The models obtained from MLT can be used in virtual screening studies as well as filters to develop/discovery new chemicals. An important challenge in the drug design field is the prediction of pharmacokinetic and toxicity properties, which can avoid failures in the clinical phases. Therefore, this review provides a critical point of view on the main MLT and shows their potential ability as a valuable tool in drug design.",23.0
22784485,Distinguishing between unipolar depression and bipolar depression: current and future clinical and neuroimaging perspectives,2013 Jan 15;73(2):111-8.,"Differentiating bipolar disorder (BD) from recurrent unipolar depression (UD) is a major clinical challenge. Main reasons for this include the higher prevalence of depressive relative to hypo/manic symptoms during the course of BD illness and the high prevalence of subthreshold manic symptoms in both BD and UD depression. Identifying objective markers of BD might help improve accuracy in differentiating between BD and UD depression, to ultimately optimize clinical and functional outcome for all depressed individuals. Yet, only eight neuroimaging studies to date have directly compared UD and BD depressed individuals. Findings from these studies suggest more widespread abnormalities in white matter connectivity and white matter hyperintensities in BD than UD depression, habenula volume reductions in BD but not UD depression, and differential patterns of functional abnormalities in emotion regulation and attentional control neural circuitry in the two depression types. These findings suggest different pathophysiologic processes, especially in emotion regulation, reward, and attentional control neural circuitry in BD versus UD depression. This review thereby serves as a call to action to highlight the pressing need for more neuroimaging studies, using larger samples sizes, comparing BD and UD depressed individuals. These future studies should also include dimensional approaches, studies of at-risk individuals, and more novel neuroimaging approaches, such as connectivity analysis and machine learning. Ultimately, these approaches might provide biomarkers to identify individuals at future risk for BD versus UD and biological targets for more personalized treatment and new treatment developments for BD and UD depression.",74.0
22776068,Predicting the risk of psychosis onset: advances and prospects,2012 Nov;6(4):368-79.,"Aim:                    To conduct a systematic review of the methods and performance characteristics of models developed for predicting the onset of psychosis.              Methods:                    We performed a comprehensive literature search restricted to English articles and identified using PubMed, Medline and PsychINFO, as well as the reference lists of published studies and reviews. Inclusion criteria included the selection of more than one variable to predict psychosis or schizophrenia onset, and selection of individuals at familial risk or clinical high risk. Eighteen studies met these criteria, and we compared these studies based on the subjects selected, predictor variables used and the choice of statistical or machine learning methods.              Results:                    Quality of life and life functioning as well as structural brain imaging emerged as the most promising predictors of psychosis onset, particularly when they were coupled with appropriate dimensionality reduction methods and predictive model algorithms like the support vector machine (SVM). Balanced accuracy ranged from 100% to 78% in four studies using the SVM, and 67% to 81% in 14 studies using general linear models.              Conclusions:                    Performance of the predictive models improves with quality of life measures, life functioning measures, structural brain imaging data, as well as with the use of methods like SVM. Despite these advances, the overall performance of psychosis predictive models is still modest. In the future, performance can potentially be improved by including genetic variant and new functional imaging data in addition to the predictors that are used currently.",8.0
22752090,Risk estimation and risk prediction using machine-learning methods,2012 Oct;131(10):1639-54.,"After an association between genetic variants and a phenotype has been established, further study goals comprise the classification of patients according to disease risk or the estimation of disease probability. To accomplish this, different statistical methods are required, and specifically machine-learning approaches may offer advantages over classical techniques. In this paper, we describe methods for the construction and evaluation of classification and probability estimation rules. We review the use of machine-learning approaches in this context and explain some of the machine-learning algorithms in detail. Finally, we illustrate the methodology through application to a genome-wide association analysis on rheumatoid arthritis.",31.0
22695048,Hierarchical reinforcement learning and decision making,2012 Dec;22(6):956-62.,"The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings.",39.0
22627698,Extracting biological information with computational analysis of Fourier-transform infrared (FTIR) biospectroscopy datasets: current practices to future perspectives,2012 Jul 21;137(14):3202-15.,"Applying Fourier-transform infrared (FTIR) spectroscopy (or related technologies such as Raman spectroscopy) to biological questions (defined as biospectroscopy) is relatively novel. Potential fields of application include cytological, histological and microbial studies. This potentially provides a rapid and non-destructive approach to clinical diagnosis. Its increase in application is primarily a consequence of developing instrumentation along with computational techniques. In the coming decades, biospectroscopy is likely to become a common tool in the screening or diagnostic laboratory, or even in the general practitioner's clinic. Despite many advances in the biological application of FTIR spectroscopy, there remain challenges in sample preparation, instrumentation and data handling. We focus on the latter, where we identify in the reviewed literature, the existence of four main study goals: Pattern Finding; Biomarker Identification; Imaging; and, Diagnosis. These can be grouped into two frameworks: Exploratory; and, Diagnostic. Existing techniques in Quality Control, Pre-processing, Feature Extraction, Clustering, and Classification are critically reviewed. An aspect that is often visited is that of method choice. Based on the state-of-art, we claim that in the near future research should be focused on the challenges of dataset standardization; building information systems; development and validation of data analysis tools; and, technology transfer. A diagnostic case study using a real-world dataset is presented as an illustration. Many of the methods presented in this review are Machine Learning and Statistical techniques that are extendable to other forms of computer-based biomedical analysis, including mass spectrometry and magnetic resonance.",35.0
22627610,Computational models for prediction of response to antiretroviral therapies,Apr-Jun 2012;14(2):145-53.,"This review describes the state-of-the-art in statistical, machine learning, and expert-advised computational methods for the evaluation and optimization of combination antiretroviral therapy, with respect to the virologic outcomes in HIV-1-infected patients. Currently employed methodologies are based on the paradigm for which mutations present in patient viral genotypes, selected either by treatment or already transmitted to the patient as resistant mutants, are the major drivers of virologic outcomes. Genotypic interpretation systems have been designed with the prime objective of characterizing the resistance to individual drugs, deriving scores from the association of viral genotypes with in vitro phenotypic drug susceptibility or in vivo response to treatment. Nevertheless, the very large range of possible drug combinations and of viral mutational patterns leads to an extremely complex scenario, making prediction of in vivo treatment response extremely challenging. To deal with such complexity, machine learning methods are being increasingly explored, thanks to the availability of exponentially growing HIV data bases in recent years. The combination of genotypic interpretation systems with other laboratory markers, treatment history, past clinical events, and the usage of data-driven techniques has dramatically raised the confidence in predicting virologic outcomes. A few of these systems have been implemented as free web-services, indicating ranks of suitable combination antiretroviral therapy regimens given a patient's clinical background. Future perspectives in the field foresee the extension of therapy optimization systems to newly approved antiretroviral drug targets and the prediction of other clinical outcomes, rather than the sole virologic response.",11.0
22611119,Machine learning approaches for the discovery of gene-gene interactions in disease data,2013 Mar;14(2):251-60.,"Because of the complexity of gene-phenotype relationships machine learning approaches have considerable appeal as a strategy for modelling interactions. A number of such methods have been developed and applied in recent years with some modest success. Progress is hampered by the challenges presented by the complexity of the disease genetic data, including phenotypic and genetic heterogeneity, polygenic forms of inheritance and variable penetrance, combined with the analytical and computational issues arising from the enormous number of potential interactions. We review here recent and current approaches focusing, wherever possible, on applications to real data (particularly in the context of genome-wide association studies) and looking ahead to the further challenges posed by next generation sequencing data.",26.0
22546560,Random forests for genomic data analysis,2012 Jun;99(6):323-9.,"Random forests (RF) is a popular tree-based ensemble machine learning tool that is highly data adaptive, applies to ""large p, small n"" problems, and is able to account for correlation as well as interactions among features. This makes RF particularly appealing for high-dimensional genomic data analysis. In this article, we systematically review the applications and recent progresses of RF for genomic data, including prediction and classification, variable selection, pathway analysis, genetic association and epistasis detection, and unsupervised learning.",143.0
22546255,From theory to bench experiment by computer-assisted drug design,2012;66(3):120-4.,"Tight integration of computer-assisted molecular design with practical realization by medicinal chemistry will be essential for finding next-generation drugs that are optimized for multiple pharmaceutically relevant properties. ETH ZÃ¼rich has established an interdisciplinary research group devoted to exploring the potential of this scientific approach by combining expertise from pharmaceutical chemistry and computer sciences. In this article, some of the group's activities and projects are presented. A current focus is on machine-learning applications aiming at hit and lead structure identification by virtual screening and de novo design. The central concept of 'adaptive fitness landscapes' is highlighted along with practical examples from drug discovery projects.",2.0
22528508,Paradigm shift in toxicity testing and modeling,2012 Sep;14(3):473-80.,"The limitations of traditional toxicity testing characterized by high-cost animal models with low-throughput readouts, inconsistent responses, ethical issues, and extrapolability to humans call for alternative strategies for chemical risk assessment. A new strategy using in vitro human cell-based assays has been designed to identify key toxicity pathways and molecular mechanisms leading to the prediction of an in vivo response. The emergence of quantitative high-throughput screening (qHTS) technology has proved to be an efficient way to decompose complex toxicological end points to specific pathways of targeted organs. In addition, qHTS has made a significant impact on computational toxicology in two aspects. First, the ease of mechanism of action identification brought about by in vitro assays has enhanced the simplicity and effectiveness of machine learning, and second, the high-throughput nature and high reproducibility of qHTS have greatly improved the data quality and increased the quantity of training datasets available for predictive model construction. In this review, the benefits of qHTS routinely used in the US Tox21 program will be highlighted. Quantitative structure-activity relationships models built on traditional in vivo data and new qHTS data will be compared and analyzed. In conjunction with the transition from the pilot phase to the production phase of the Tox21 program, more qHTS data will be made available that will enrich the data pool for predictive toxicology. It is perceivable that new in silico toxicity models based on high-quality qHTS data will achieve unprecedented reliability and robustness, thus becoming a valuable tool for risk assessment and drug discovery.",20.0
22487041,Neural control of dopamine neurotransmission: implications for reinforcement learning,2012 Apr;35(7):1115-23.,"In the past few decades there has been remarkable convergence of machine learning with neurobiological understanding of reinforcement learning mechanisms, exemplified by temporal difference (TD) learning models. The anatomy of the basal ganglia provides a number of potential substrates for instantiation of the TD mechanism. In contrast to the traditional concept of direct and indirect pathway outputs from the striatum, we emphasize that projection neurons of the striatum are branched and individual striatofugal neurons innervate both globus pallidus externa and globus pallidus interna/substantia nigra (GPi/SNr). This suggests that the GPi/SNr has the necessary inputs to operate as the source of a TD signal. We also discuss the mechanism for the timing processes necessary for learning in the TD framework. The TD framework has been particularly successful in analysing electrophysiogical recordings from dopamine (DA) neurons during learning, in terms of reward prediction error. However, present understanding of the neural control of DA release is limited, and hence the neural mechanisms involved are incompletely understood. Inhibition is very conspicuously present among the inputs to the DA neurons, with inhibitory synapses accounting for the majority of synapses on DA neurons. Furthermore, synchronous firing of the DA neuron population requires disinhibition and excitation to occur together in a coordinated manner. We conclude that the inhibitory circuits impinging directly or indirectly on the DA neurons play a central role in the control of DA neuron activity and further investigation of these circuits may provide important insight into the biological mechanisms of reinforcement learning.",12.0
22465077,Machine learning and radiology,2012 Jul;16(5):933-51.,"In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers.",106.0
22458505,Decision tree models for data mining in hit discovery,2012 Apr;7(4):341-52.,"Introduction:                    Decision tree induction (DTI) is a powerful means of modeling data without much prior preparation. Models are readable by humans, robust and easily applied in real-world applications, features that are mutually exclusive in other commonly used machine learning paradigms. While DTI is widely used in disciplines ranging from economics to medicine, they are an intriguing option in pharmaceutical research, especially when dealing with large data stores.              Areas covered:                    This review covers the automated technologies available for creating decision trees and other rules efficiently, even from large datasets such as chemical libraries. The authors discuss the need for properly documented and validated models. Lastly, the authors cover several case studies in hit discovery, drug metabolism and toxicology, and drug surveillance, and compare them with other established techniques.              Expert opinion:                    DTI is a competitive and easy-to-use tool in basic research as well as in hit and drug discovery. Its strengths lie in its ability to handle all sorts of different data formats, the visual nature of the models, and the small computational effort needed for implementation in real-world systems. Limitations include lack of robustness and over-fitted models for certain types of data. As with any modeling technique, proper validation and quality measures are of utmost importance.",4.0
22410950,Finding biomarkers is getting easier,2012 Apr;21(3):631-6.,"Single biomarkers are rarely accurate. Even suites of biomarkers can give conflicting results. Ideally potent combinations of variables are isolated which accurately identify specific analytes and their level of toxicity. The search for such combinations can be done by reducing the thousands of candidate variables to the small number necessary for treatment classification. When the key variables are recognized by machine learning (ML) the results are quite surprising, given the apparent failure of other searching methods to produce good diagnostics. Proteins seem especially useful for portable field tests of a variety of adverse conditions. This review shows how ML, in particular artificial neural networks, can find potent biomarkers embedded in any type of expression data, mainly proteins in this article. A computer does multiple iterations to produce sets of proteins which systematically identify (to near 100% accuracy) the treatment classes of interest. Whether these proteins are useful in actual diagnoses is tested by presenting the computer model with unknown classes. Finding the biomarkers is getting easier but there still must be confirmation, by multivariable statistics and with field studies.",3.0
22401592,Probabilistic logic methods and some applications to biology and medicine,2012 Mar;19(3):316-36.,"For the computational analysis of biological problems-analyzing data, inferring networks and complex models, and estimating model parameters-it is common to use a range of methods based on probabilistic logic constructions, sometimes collectively called machine learning methods. Probabilistic modeling methods such as Bayesian Networks (BN) fall into this class, as do Hierarchical Bayesian Networks (HBN), Probabilistic Boolean Networks (PBN), Hidden Markov Models (HMM), and Markov Logic Networks (MLN). In this review, we describe the most general of these (MLN), and show how the above-mentioned methods are related to MLN and one another by the imposition of constraints and restrictions. This approach allows us to illustrate a broad landscape of constructions and methods, and describe some of the attendant strengths, weaknesses, and constraints of many of these methods. We then provide some examples of their applications to problems in biology and medicine, with an emphasis on genetics. The key concepts needed to picture this landscape of methods are the ideas of probabilistic graphical models, the structures of the graphs, and the scope of the logical language repertoire used (from First-Order Logic [FOL] to Boolean logic.) These concepts are interlinked and together define the nature of each of the probabilistic logic methods. Finally, we discuss the initial applications of MLN to genetics, show the relationship to less general methods like BN, and then mention several examples where such methods could be effective in new applications to specific biological and medical problems.",4.0
22394424,Annual research review: progress in using brain morphometry as a clinical tool for diagnosing psychiatric disorders,2012 May;53(5):519-35.,"Brain morphometry in recent decades has increased our understanding of the neural bases of psychiatric disorders by localizing anatomical disturbances to specific nuclei and subnuclei of the brain. At least some of these disturbances precede the overt expression of clinical symptoms and possibly are endophenotypes that could be used to diagnose an individual accurately as having a specific psychiatric disorder. More accurate diagnoses could significantly reduce the emotional and financial burden of disease by aiding clinicians in implementing appropriate treatments earlier and in tailoring treatment to the individual needs. Several methods, especially those based on machine learning, have been proposed that use anatomical brain measures and gold-standard diagnoses of participants to learn decision rules that classify a person automatically as having one disorder rather than another. We review the general principles and procedures for machine learning, particularly as applied to diagnostic classification, and then review the procedures that have thus far attempted to diagnose psychiatric illnesses automatically using anatomical measures of the brain. We discuss the strengths and limitations of extant procedures and note that the sensitivity and specificity of these procedures in their most successful implementations have approximated 90%. Although these methods have not yet been applied within clinical settings, they provide strong evidence that individual patients can be diagnosed accurately using the spatial pattern of disturbances across the brain.",8.0
22366294,Evaluating the state of the art in coreference resolution for electronic medical records,Sep-Oct 2012;19(5):786-91.,"Background:                    The fifth i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records conducted a systematic review on resolution of noun phrase coreference in medical records. Informatics for Integrating Biology and the Bedside (i2b2) and the Veterans Affair (VA) Consortium for Healthcare Informatics Research (CHIR) partnered to organize the coreference challenge. They provided the research community with two corpora of medical records for the development and evaluation of the coreference resolution systems. These corpora contained various record types (ie, discharge summaries, pathology reports) from multiple institutions.              Methods:                    The coreference challenge provided the community with two annotated ground truth corpora and evaluated systems on coreference resolution in two ways: first, it evaluated systems for their ability to identify mentions of concepts and to link together those mentions. Second, it evaluated the ability of the systems to link together ground truth mentions that refer to the same entity. Twenty teams representing 29 organizations and nine countries participated in the coreference challenge.              Results:                    The teams' system submissions showed that machine-learning and rule-based approaches worked best when augmented with external knowledge sources and coreference clues extracted from document structure. The systems performed better in coreference resolution when provided with ground truth mentions. Overall, the systems struggled in solving coreference resolution for cases that required domain knowledge.",51.0
22339582,"Computational prediction of metabolism: sites, products, SAR, P450 enzyme dynamics, and mechanisms",2012 Mar 26;52(3):617-48.,"Metabolism of xenobiotics remains a central challenge for the discovery and development of drugs, cosmetics, nutritional supplements, and agrochemicals. Metabolic transformations are frequently related to the incidence of toxic effects that may result from the emergence of reactive species, the systemic accumulation of metabolites, or by induction of metabolic pathways. Experimental investigation of the metabolism of small organic molecules is particularly resource demanding; hence, computational methods are of considerable interest to complement experimental approaches. This review provides a broad overview of structure- and ligand-based computational methods for the prediction of xenobiotic metabolism. Current computational approaches to address xenobiotic metabolism are discussed from three major perspectives: (i) prediction of sites of metabolism (SOMs), (ii) elucidation of potential metabolites and their chemical structures, and (iii) prediction of direct and indirect effects of xenobiotics on metabolizing enzymes, where the focus is on the cytochrome P450 (CYP) superfamily of enzymes, the cardinal xenobiotics metabolizing enzymes. For each of these domains, a variety of approaches and their applications are systematically reviewed, including expert systems, data mining approaches, quantitative structure-activity relationships (QSARs), and machine learning-based methods, pharmacophore-based algorithms, shape-focused techniques, molecular interaction fields (MIFs), reactivity-focused techniques, protein-ligand docking, molecular dynamics (MD) simulations, and combinations of methods. Predictive metabolism is a developing area, and there is still enormous potential for improvement. However, it is clear that the combination of rapidly increasing amounts of available ligand- and structure-related experimental data (in particular, quantitative data) with novel and diverse simulation and modeling approaches is accelerating the development of effective tools for prediction of in vivo metabolism, which is reflected by the diverse and comprehensive data sources and methods for metabolism prediction reviewed here. This review attempts to survey the range and scope of computational methods applied to metabolism prediction and also to compare and contrast their applicability and performance.",53.0
22327231,Primary prevention of sudden cardiac death of the young athlete: the controversy about the screening electrocardiogram and its innovative artificial intelligence solution,2012 Mar;33(3):428-33.,"The preparticipation screening for athlete participation in sports typically entails a comprehensive medical and family history and a complete physical examination. A 12-lead electrocardiogram (ECG) can increase the likelihood of detecting cardiac diagnoses such as hypertrophic cardiomyopathy, but this diagnostic test as part of the screening process has engendered considerable controversy. The pro position is supported by argument that international screening protocols support its use, positive diagnosis has multiple benefits, history and physical examination are inadequate, primary prevention is essential, and the cost effectiveness is justified. Although the aforementioned myriad of justifications for routine ECG screening of young athletes can be persuasive, several valid contentions oppose supporting such a policy, namely, that the sudden death incidence is very (too) low, the ECG screening will be too costly, the false-positive rate is too high, resources will be allocated away from other diseases, and manpower is insufficient for its execution. Clinicians, including pediatric cardiologists, have an understandable proclivity for avoiding this prodigious national endeavor. The controversy, however, should not be focused on whether an inexpensive, noninvasive test such as an ECG should be mandated but should instead be directed at just how these tests for young athletes can be performed in the clinical imbroglio of these disease states (with variable genetic penetrance and phenotypic expression) with concomitant fiscal accountability and logistical expediency in this era of economic restraint. This monumental endeavor in any city or region requires two crucial elements well known to business scholars: implementation and execution. The eventual solution for the screening ECG dilemma requires a truly innovative and systematic approach that will liberate us from inadequate conventional solutions. Artificial intelligence, specifically the process termed ""machine learning"" and ""neural networking,"" involves complex algorithms that allow computers to improve the decision-making process based on repeated input of empirical data (e.g., databases and ECGs). These elements all can be improved with a national database, evidence-based medicine, and in the near future, innovation that entails a Kurzweilian artificial intelligence infrastructure with machine learning and neural networking that will construct the ultimate clinical decision-making algorithm.",
22326864,Nonlinear dimensionality reduction and mapping of compound libraries for drug discovery,2012 Apr;34:108-17.,"Visualization of 'chemical space' and compound distributions has received much attraction by medicinal chemists as it may help to intuitively comprehend pharmaceutically relevant molecular features. It has been realized that for meaningful feature extraction from complex multivariate chemical data, such as compound libraries represented by many molecular descriptors, nonlinear projection techniques are required. Recent advances in machine-learning and artificial intelligence have resulted in a transfer of such methods to chemistry. We provide an overview of prominent visualization methods based on nonlinear dimensionality reduction, and highlight applications in drug discovery. Emphasis is on neural network techniques, kernel methods and stochastic embedding approaches, which have been successfully used for ligand-based virtual screening, SAR landscape analysis, combinatorial library design, and screening compound selection.",12.0
22305994,Using Support Vector Machine to identify imaging biomarkers of neurological and psychiatric disease: a critical review,2012 Apr;36(4):1140-52.,"Standard univariate analysis of neuroimaging data has revealed a host of neuroanatomical and functional differences between healthy individuals and patients suffering a wide range of neurological and psychiatric disorders. Significant only at group level however these findings have had limited clinical translation, and recent attention has turned toward alternative forms of analysis, including Support-Vector-Machine (SVM). A type of machine learning, SVM allows categorisation of an individual's previously unseen data into a predefined group using a classification algorithm, developed on a training data set. In recent years, SVM has been successfully applied in the context of disease diagnosis, transition prediction and treatment prognosis, using both structural and functional neuroimaging data. Here we provide a brief overview of the method and review those studies that applied it to the investigation of Alzheimer's disease, schizophrenia, major depression, bipolar disorder, presymptomatic Huntington's disease, Parkinson's disease and autistic spectrum disorder. We conclude by discussing the main theoretical and practical challenges associated with the implementation of this method into the clinic and possible future directions.",289.0
22293396,Chemical shift prediction for protein structure calculation and quality assessment using an optimally parameterized force field,2012 Jan;60:1-28.,"The exquisite sensitivity of chemical shifts as reporters of structural information, and the ability to measure them routinely and accurately, gives great import to formulations that elucidate the structure-chemical-shift relationship. Here we present a new and highly accurate, precise, and robust formulation for the prediction of NMR chemical shifts from protein structures. Our approach, shAIC (shift prediction guided by Akaikes Information Criterion), capitalizes on mathematical ideas and an information-theoretic principle, to represent the functional form of the relationship between structure and chemical shift as a parsimonious sum of smooth analytical potentials which optimally takes into account short-, medium-, and long-range parameters in a nuclei-specific manner to capture potential chemical shift perturbations caused by distant nuclei. shAIC outperforms the state-of-the-art methods that use analytical formulations. Moreover, for structures derived by NMR or structures with novel folds, shAIC delivers better overall results; even when it is compared to sophisticated machine learning approaches. shAIC provides for a computationally lightweight implementation that is unimpeded by molecular size, making it an ideal for use as a force field.",17.0
22286881,Predicting response to antiretroviral treatment by machine learning: the EuResist project,2012;55(2):123-7.,"For a long time, the clinical management of antiretroviral drug resistance was based on sequence analysis of the HIV genome followed by estimating drug susceptibility from the mutational pattern that was detected. The large number of anti-HIV drugs and HIV drug resistance mutations has prompted the development of computer-aided genotype interpretation systems, typically comprising rules handcrafted by experts via careful examination of in vitro and in vivo resistance data. More recently, machine learning approaches have been applied to establish data-driven engines able to indicate the most effective treatments for any patient and virus combination. Systems of this kind, currently including the Resistance Response Database Initiative and the EuResist engine, must learn from the large data sets of patient histories and can provide an objective and accurate estimate of the virological response to different antiretroviral regimens. The EuResist engine was developed by a European consortium of HIV and bioinformatics experts and compares favorably with the most commonly used genotype interpretation systems and HIV drug resistance experts. Next-generation treatment response prediction engines may valuably assist the HIV specialist in the challenging task of establishing effective regimens for patients harboring drug-resistant virus strains. The extensive collection and accurate processing of increasingly large patient data sets are eagerly awaited to further train and translate these systems from prototype engines into real-life treatment decision support tools.",13.0
22281989,Structure-based virtual screening for drug discovery: a problem-centric review,2012 Mar;14(1):133-41.,"Structure-based virtual screening (SBVS) has been widely applied in early-stage drug discovery. From a problem-centric perspective, we reviewed the recent advances and applications in SBVS with a special focus on docking-based virtual screening. We emphasized the researchers' practical efforts in real projects by understanding the ligand-target binding interactions as a premise. We also highlighted the recent progress in developing target-biased scoring functions by optimizing current generic scoring functions toward certain target classes, as well as in developing novel ones by means of machine learning techniques.",117.0
22256868,Genomic approach towards personalized anticancer drug therapy,2012 Jan;13(2):191-9.,"Stratification of patients for multidrug response is a promising strategy for cancer treatment. Genome-based prediction models have great potential for this purpose because the extent of drug sensitivity may be attributed to the heterogeneity of the underlying genetic characteristics of cancer. However, microarray data is difficult to analyze and is not reproducible. Several machine-learning algorithms have therefore been developed in a repeatable manner. Random forests algorithm, which uses an ensemble approach based on classification and regression trees, appears to be superior for predicting multidrug sensitivity. This is because ensemble methods are more effective when there are much more predictors than samples. Here, we review recent advances in the development of classification algorithms using microarray technology for prediction of anticancer sensitivity, discuss the availability of ensemble methods for prediction models, and present data regarding the identification of potential responders to FOLFOX therapy using random forests algorithm.",4.0
22246801,Resolving confusion of tongues in statistics and machine learning: a primer for biologists and bioinformaticians,2012 Feb;12(4-5):543-9.,"Bioinformatics is the field where computational methods from various domains have come together for analysis of biological data. Each domain has introduced its own specific jargon. However, in closely related domains, e.g. machine learning and statistics, concordant and discordant terminology occurs, the later can lead to confusion. This article aims to help solve the confusion of tongues arising from these two closely related domains, which are frequently used in bioinformatics. We provide a short summary of the most commonly applied machine learning and statistical approaches to data analysis in bioinformatics, i.e. classification and statistical hypothesis testing. We explain differences and similarities in common terminology used in various domains, such as precision, recall, sensitivity and true positive rate. This primer can serve as a guide to the terminology used in these fields.",
22128059,Brief review of regression-based and machine learning methods in genetic epidemiology: the Genetic Analysis Workshop 17 experience,2011;35 Suppl 1(Suppl 1):S5-11.,"Genetics Analysis Workshop 17 provided common and rare genetic variants from exome sequencing data and simulated binary and quantitative traits in 200 replicates. We provide a brief review of the machine learning and regression-based methods used in the analyses of these data. Several regression and machine learning methods were used to address different problems inherent in the analyses of these data, which are high-dimension, low-sample-size data typical of many genetic association studies. Unsupervised methods, such as cluster analysis, were used for data segmentation and, subset selection. Supervised learning methods, which include regression-based methods (e.g., generalized linear models, logic regression, and regularized regression) and tree-based methods (e.g., decision trees and random forests), were used for variable selection (selecting genetic and clinical features most associated or predictive of outcome) and prediction (developing models using common and rare genetic variants to accurately predict outcome), with the outcome being case-control status or quantitative trait value. We include a discussion of cross-validation for model selection and assessment, and a description of available software resources for these methods.",40.0
22138323,A review of statistical methods for prediction of proteolytic cleavage,2012 May;13(3):337-49.,"A fundamental component of systems biology, proteolytic cleavage is involved in nearly all aspects of cellular activities: from gene regulation to cell lifecycle regulation. Current sequencing technologies have made it possible to compile large amount of cleavage data and brought greater understanding of the underlying protein interactions. However, the practical impossibility to exhaustively retrieve substrate sequences through experimentation alone has long highlighted the need for efficient computational prediction methods. Such methods must be able to quickly mark substrate candidates and putative cleavage sites for further analysis. Available methods and expected reliability depend heavily on the type and complexity of proteolytic action, as well as the availability of well-labelled experimental data sets: factors varying greatly across enzyme families. For this review, we chose to give a quick overview of the general issues and challenges in cleavage prediction methods followed by a more in-depth presentation of major techniques and implementations, with a focus on two particular families of cysteine proteases: caspases and calpains. Through their respective differences in proteolytic specificity (high for caspases, broader for calpains) and data availability (much lower for calpains), we aimed to illustrate the strengths and limitations of techniques ranging from position-based matrices and decision trees to more flexible machine-learning methods such as hidden Markov models and Support Vector Machines. In addition to a technical overview for each family of algorithms, we tried to provide elements of evaluation and performance comparison across methods.",10.0
22126294,DNA methylation topology: potential of a chromatin landmark for epigenetic drug toxicology,2011 Dec;3(6):761-70.,Targeting chromatin and its basic components through epigenetic drug therapy has become an increased focus in the treatment of complex diseases. This boost calls for the implementation of high-throughput cell-based assays that exploit the increasing knowledge about epigenetic mechanisms and their interventions for genotoxicity testing of epigenetic drugs. 3D quantitative DNA methylation imaging is a novel approach for detecting drug-induced DNA demethylation and concurrent heterochromatin decondensation/reorganization in cells through the analysis of differential nuclear distribution patterns of methylcytosine and gDNA visualized by fluorescence and processed by machine-learning algorithms. Utilizing 3D DNA methylation patterns is a powerful precursor to a series of fully automatable assays that employ chromatin structure and higher organization as novel pharmacodynamic biomarkers for various epigenetic drug actions.,4.0
22075226,Membrane protein structural bioinformatics,2012 Sep;179(3):327-37.,"Despite the increasing number of recently solved membrane protein structures, coverage of membrane protein fold space remains relatively sparse. This necessitates the use of computational strategies to investigate membrane protein structure, allowing us to further our understanding of how membrane proteins carry out their diverse range of functions, while aiding the development of novel predictive tools with which to probe uncharacterised folds. Analysis of known structures, the application of machine learning techniques, molecular dynamics simulations and protein structure prediction have enabled significant advances to be made in the field of membrane protein research. In this communication, the key bioinformatic methods that allow the characterisation of membrane proteins are reviewed, the tools available for the structural analysis of membrane proteins are presented and the contribution these tools have made to expanding our understanding of membrane protein structure, function and stability is discussed.",12.0
22072665,Large-scale automated histology in the pursuit of connectomes,2011 Nov 9;31(45):16125-38.,"How does the brain compute? Answering this question necessitates neuronal connectomes, annotated graphs of all synaptic connections within defined brain areas. Further, understanding the energetics of the brain's computations requires vascular graphs. The assembly of a connectome requires sensitive hardware tools to measure neuronal and neurovascular features in all three dimensions, as well as software and machine learning for data analysis and visualization. We present the state of the art on the reconstruction of circuits and vasculature that link brain anatomy and function. Analysis at the scale of tens of nanometers yields connections between identified neurons, while analysis at the micrometer scale yields probabilistic rules of connection between neurons and exact vascular connectivity.",77.0
21989632,The promise of mHealth: daily activity monitoring and outcome assessments by wearable sensors,Nov-Dec 2011;25(9):788-98.,"Mobile health tools that enable clinicians and researchers to monitor the type, quantity, and quality of everyday activities of patients and trial participants have long been needed to improve daily care, design more clinically meaningful randomized trials of interventions, and establish cost-effective, evidence-based practices. Inexpensive, unobtrusive wireless sensors, including accelerometers, gyroscopes, and pressure-sensitive textiles, combined with Internet-based communications and machine-learning algorithms trained to recognize upper- and lower-extremity movements, have begun to fulfill this need. Continuous data from ankle triaxial accelerometers, for example, can be transmitted from the home and community via WiFi or a smartphone to a remote data analysis server. Reports can include the walking speed and duration of every bout of ambulation, spatiotemporal symmetries between the legs, and the type, duration, and energy used during exercise. For daily care, this readily accessible flow of real-world information allows clinicians to monitor the amount and quality of exercise for risk factor management and compliance in the practice of skills. Feedback may motivate better self-management as well as serve home-based rehabilitation efforts. Monitoring patients with chronic diseases and after hospitalization or the start of new medications for a decline in daily activity may help detect medical complications before rehospitalization becomes necessary. For clinical trials, repeated laboratory-quality assessments of key activities in the community, rather than by clinic testing, self-report, and ordinal scales, may reduce the cost and burden of travel, improve recruitment and retention, and capture more reliable, valid, and responsive ratio-scaled outcome measures that are not mere surrogates for changes in daily impairment, disability, and functioning.",86.0
21926126,Computational prediction of eukaryotic phosphorylation sites,2011 Nov 1;27(21):2927-35.,"Motivation:                    Kinase-mediated phosphorylation is the central mechanism of post-translational modification to regulate cellular responses and phenotypes. Signaling defects associated with protein phosphorylation are linked to many diseases, particularly cancer. Characterizing protein kinases and their substrates enhances our ability to understand and treat such diseases and broadens our knowledge of signaling networks in general. While most or all protein kinases have been identified in well-studied eukaryotes, the sites that they phosphorylate have been only partially elucidated. Experimental methods for identifying phosphorylation sites are resource intensive, so the ability to computationally predict potential sites has considerable value.              Results:                    Many computational techniques for phosphorylation site prediction have been proposed, most of which are available on the web. These techniques differ in several ways, including the machine learning technique used; the amount of sequence information used; whether or not structural information is used in addition to sequence information; whether predictions are made for specific kinases or for kinases in general; and sources of training and testing data. This review summarizes, categorizes and compares the available methods for phosphorylation site prediction, and provides an overview of the challenges that are faced when designing predictors and how they have been addressed. It should therefore be useful both for those wishing to choose a phosphorylation site predictor for their particular biological application, and for those attempting to improve upon established techniques in the future.              Contact:                    brett.trost@usask.ca.",41.0
