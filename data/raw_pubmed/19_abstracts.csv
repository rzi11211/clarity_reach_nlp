pmid,citations,title,date,text
30877639,20.0,An Overview of Scoring Functions Used for Protein-Ligand Interactions in Molecular Docking,2019 Jun;11(2):320-328.,"Currently, molecular docking is becoming a key tool in drug discovery and molecular modeling applications. The reliability of molecular docking depends on the accuracy of the adopted scoring function, which can guide and determine the ligand poses when thousands of possible poses of ligand are generated. The scoring function can be used to determine the binding mode and site of a ligand, predict binding affinity and identify the potential drug leads for a given protein target. Despite intensive research over the years, accurate and rapid prediction of protein-ligand interactions is still a challenge in molecular docking. For this reason, this study reviews four basic types of scoring functions, physics-based, empirical, knowledge-based, and machine learning-based scoring functions, based on an up-to-date classification scheme. We not only discuss the foundations of the four types scoring functions, suitable application areas and shortcomings, but also discuss challenges and potential future study directions."
30872992,16.0,"Machine Learning in Amyotrophic Lateral Sclerosis: Achievements, Pitfalls, and Future Directions",2019 Feb 28;13:135.,"Background: Amyotrophic Lateral Sclerosis (ALS) is a relentlessly progressive neurodegenerative condition with limited therapeutic options at present. Survival from symptom onset ranges from 3 to 5 years depending on genetic, demographic, and phenotypic factors. Despite tireless research efforts, the core etiology of the disease remains elusive and drug development efforts are confounded by the lack of accurate monitoring markers. Disease heterogeneity, late-stage recruitment into pharmaceutical trials, and inclusion of phenotypically admixed patient cohorts are some of the key barriers to successful clinical trials. Machine Learning (ML) models and large international data sets offer unprecedented opportunities to appraise candidate diagnostic, monitoring, and prognostic markers. Accurate patient stratification into well-defined prognostic categories is another aspiration of emerging classification and staging systems. Methods: The objective of this paper is the comprehensive, systematic, and critical review of ML initiatives in ALS to date and their potential in research, clinical, and pharmacological applications. The focus of this review is to provide a dual, clinical-mathematical perspective on recent advances and future directions of the field. Another objective of the paper is the frank discussion of the pitfalls and drawbacks of specific models, highlighting the shortcomings of existing studies and to provide methodological recommendations for future study designs. Results: Despite considerable sample size limitations, ML techniques have already been successfully applied to ALS data sets and a number of promising diagnosis models have been proposed. Prognostic models have been tested using core clinical variables, biological, and neuroimaging data. These models also offer patient stratification opportunities for future clinical trials. Despite the enormous potential of ML in ALS research, statistical assumptions are often violated, the choice of specific statistical models is seldom justified, and the constraints of ML models are rarely enunciated. Conclusions: From a mathematical perspective, the main barrier to the development of validated diagnostic, prognostic, and monitoring indicators stem from limited sample sizes. The combination of multiple clinical, biofluid, and imaging biomarkers is likely to increase the accuracy of mathematical modeling and contribute to optimized clinical trial designs."
30872241,2.0,"Harnessing the Power of Machine Learning in Dementia Informatics Research: Issues, Opportunities, and Challenges",2020;13:113-129.,"Dementia is a chronic and degenerative condition affecting millions globally. The care of patients with dementia presents an ever-continuing challenge to healthcare systems in the 21st century. Medical and health sciences have generated unprecedented volumes of data related to health and wellbeing for patients with dementia due to advances in information technology, such as genetics, neuroimaging, cognitive assessment, free texts, routine electronic health records, etc. Making the best use of these diverse and strategic resources will lead to high-quality care of patients with dementia. As such, machine learning becomes a crucial factor in achieving this objective. The aim of this paper is to provide a state-of-the-art review of machine learning methods applied to health informatics for dementia care. We collate and review the existing scientific methodologies and identify the relevant issues and challenges when faced with big health data. Machine learning has demonstrated promising applications to neuroimaging data analysis for dementia care, while relatively less effort has been made to make use of integrated heterogeneous data via advanced machine learning approaches. We further indicate future potential and research directions in applying advanced machine learning, such as deep learning, to dementia informatics."
30871264,4.0,Computational Approaches in Theranostics: Mining and Predicting Cancer Data,2019 Mar 13;11(3):119.,"The ability to understand the complexity of cancer-related data has been prompted by the applications of (1) computer and data sciences, including data mining, predictive analytics, machine learning, and artificial intelligence, and (2) advances in imaging technology and probe development. Computational modelling and simulation are systematic and cost-effective tools able to identify important temporal/spatial patterns (and relationships), characterize distinct molecular features of cancer states, and address other relevant aspects, including tumor detection and heterogeneity, progression and metastasis, and drug resistance. These approaches have provided invaluable insights for improving the experimental design of therapeutic delivery systems and for increasing the translational value of the results obtained from early and preclinical studies. The big question is: Could cancer theranostics be determined and controlled in silico? This review describes the recent progress in the development of computational models and methods used to facilitate research on the molecular basis of cancer and on the respective diagnosis and optimized treatment, with particular emphasis on the design and optimization of theranostic systems. The current role of computational approaches is providing innovative, incremental, and complementary data-driven solutions for the prediction, simplification, and characterization of cancer and intrinsic mechanisms, and to promote new data-intensive, accurate diagnostics and therapeutics."
30870615,,What does the mind learn? A comparison of human and machine learning representations,2019 Apr;55:97-102.,"We present a brief review of modern machine learning techniques and their use in models of human mental representations, detailing three notable branches: spatial methods, logical methods and artificial neural networks. Each of these branches contains an extensive set of systems, and demonstrate accurate emulations of human learning of categories, concepts and language, despite substantial differences in operation. We suggest that continued applications will allow cognitive researchers the ability to model the complex real-world problems where machine learning has recently been successful, providing more complete behavioural descriptions. This will, however, also require careful consideration of appropriate algorithmic constraints alongside these methods in order to find a combination which captures both the strengths and weaknesses of human cognition."
30867681,4.0,Encodings and models for antimicrobial peptide classification for multi-resistant pathogens,2019 Mar 4;12:7.,"Antimicrobial peptides (AMPs) are part of the inherent immune system. In fact, they occur in almost all organisms including, e.g., plants, animals, and humans. Remarkably, they show effectivity also against multi-resistant pathogens with a high selectivity. This is especially crucial in times, where society is faced with the major threat of an ever-increasing amount of antibiotic resistant microbes. In addition, AMPs can also exhibit antitumor and antiviral effects, thus a variety of scientific studies dealt with the prediction of active peptides in recent years. Due to their potential, even the pharmaceutical industry is keen on discovering and developing novel AMPs. However, AMPs are difficult to verify in vitro, hence researchers conduct sequence similarity experiments against known, active peptides. Unfortunately, this approach is very time-consuming and limits potential candidates to sequences with a high similarity to known AMPs. Machine learning methods offer the opportunity to explore the huge space of sequence variations in a timely manner. These algorithms have, in principal, paved the way for an automated discovery of AMPs. However, machine learning models require a numerical input, thus an informative encoding is very important. Unfortunately, developing an appropriate encoding is a major challenge, which has not been entirely solved so far. For this reason, the development of novel amino acid encodings is established as a stand-alone research branch. The present review introduces state-of-the-art encodings of amino acids as well as their properties in sequence and structure based aggregation. Moreover, albeit a well-chosen encoding is essential, performant classifiers are required, which is reflected by a tendency towards specifically designed models in the literature. Furthermore, we introduce these models with a particular focus on encodings derived from support vector machines and deep learning approaches. Albeit a strong focus has been set on AMP predictions, not all of the mentioned encodings have been elaborated as part of antimicrobial research studies, but rather as general protein or peptide representations."
30866728,4.0,Review of Medical Decision Support and Machine-Learning Methods,2019 Jul;56(4):512-525.,"Machine-learning methods can assist with the medical decision-making processes at the both the clinical and diagnostic levels. In this article, we first review historical milestones and specific applications of computer-based medical decision support tools in both veterinary and human medicine. Next, we take a mechanistic look at 3 archetypal learning algorithms-naive Bayes, decision trees, and neural network-commonly used to power these medical decision support tools. Last, we focus our discussion on the data sets used to train these algorithms and examine methods for validation, data representation, transformation, and feature selection. From this review, the reader should gain some appreciation for how these decision support tools have and can be used in medicine along with insight on their inner workings."
30866425,13.0,Automatic Pulmonary Nodule Detection Applying Deep Learning or Machine Learning Algorithms to the LIDC-IDRI Database: A Systematic Review,2019 Mar 7;9(1):29.,"The aim of this study was to provide an overview of the literature available on machine learning (ML) algorithms applied to the Lung Image Database Consortium Image Collection (LIDC-IDRI) database as a tool for the optimization of detecting lung nodules in thoracic CT scans. This systematic review was compiled according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Only original research articles concerning algorithms applied to the LIDC-IDRI database were included. The initial search yielded 1972 publications after removing duplicates, and 41 of these articles were included in this study. The articles were divided into two subcategories describing their overall architecture. The majority of feature-based algorithms achieved an accuracy >90% compared to the deep learning (DL) algorithms that achieved an accuracy in the range of 82.2%⁻97.6%. In conclusion, ML and DL algorithms are able to detect lung nodules with a high level of accuracy, sensitivity, and specificity using ML, when applied to an annotated archive of CT scans of the lung. However, there is no consensus on the method applied to determine the efficiency of ML algorithms."
30865306,15.0,Precision medicine for the discovery of treatable mechanisms in severe asthma,2019 Sep;74(9):1649-1659.,"Although the complex disease of asthma has been defined as being heterogeneous, the extent of its endophenotypes remains unclear. The pharmacological approach to initiating treatment has, until recently, been based on disease control and severity. The introduction of antibody therapies targeting the Type 2 inflammation pathway for patients with severe asthma has resulted in the recognition of an allergic and an eosinophilic phenotype, which are not mutually exclusive. Concomitantly, molecular phenotyping based on a transcriptomic analysis of bronchial epithelial and sputum cells has identified a Type 2 high inflammation cluster characterized by eosinophilia and recurrent exacerbations, as well as Type 2 low clusters linked with IL-6 trans-signalling, interferon pathways, inflammasome activation and mitochondrial oxidative phosphorylation pathways. Systems biology approaches are establishing the links between these pathways or mechanisms, and clinical and physiologic features. Validation of these pathways contributes to defining endotypes and treatable mechanisms. Precision medicine approaches are necessary to link treatable mechanisms with treatable traits and biomarkers derived from clinical, physiologic and inflammatory features of clinical phenotypes. The deep molecular phenotyping of airway samples along with noninvasive biomarkers linked to bioinformatic and machine learning techniques will enable the rapid detection of molecular mechanisms that transgresses beyond the concept of treatable traits."
30861015,,TECLA: A temperament and psychological type prediction framework from Twitter data,2019 Mar 12;14(3):e0212844.,"Temperament and Psychological Types can be defined as innate psychological characteristics associated with how we relate with the world, and often influence our study and career choices. Furthermore, understanding these features help us manage conflicts, develop leadership, improve teaching and many other skills. Assigning temperament and psychological types is usually made by filling specific questionnaires. However, it is possible to identify temperamental characteristics from a linguistic and behavioral analysis of social media data from a user. Thus, machine-learning algorithms can be used to learn from a user's social media data and infer his/her behavioral type. This paper initially provides a brief historical review of theories on temperament and then brings a survey of research aimed at predicting temperament and psychological types from social media data. It follows with the proposal of a framework to predict temperament and psychological types from a linguistic and behavioral analysis of Twitter data. The proposed framework infers temperament types following the David Keirsey's model, and psychological types based on the MBTI model. Various data modelling and classifiers are used. The results showed that Random Forests with the LIWC technique can predict with 96.46% of accuracy the Artisan temperament, 92.19% the Guardian temperament, 78.68% the Idealist, and 83.82% the Rational temperament. The MBTI results also showed that Random Forests achieved a better performance with an accuracy of 82.05% for the E/I pair, 88.38% for the S/N pair, 80.57% for the T/F pair, and 78.26% for the J/P pair."
30858931,8.0,Artificial intelligence in breast ultrasound,2019 Feb 28;11(2):19-26.,"Artificial intelligence (AI) is gaining extensive attention for its excellent performance in image-recognition tasks and increasingly applied in breast ultrasound. AI can conduct a quantitative assessment by recognizing imaging information automatically and make more accurate and reproductive imaging diagnosis. Breast cancer is the most commonly diagnosed cancer in women, severely threatening women's health, the early screening of which is closely related to the prognosis of patients. Therefore, utilization of AI in breast cancer screening and detection is of great significance, which can not only save time for radiologists, but also make up for experience and skill deficiency on some beginners. This article illustrates the basic technical knowledge regarding AI in breast ultrasound, including early machine learning algorithms and deep learning algorithms, and their application in the differential diagnosis of benign and malignant masses. At last, we talk about the future perspectives of AI in breast ultrasound."
30856578,1.0,Evaluating and comparing remote sensing terrestrial GPP models for their response to climate variability and CO 2 trends,2019 Jun 10;668:696-713.,"Remote sensing (RS)-based models play an important role in estimating and monitoring terrestrial ecosystem gross primary productivity (GPP). Several RS-based GPP models have been developed using different criteria, yet the sensitivities to environmental factors vary among models; thus, a comparison of model sensitivity is necessary for analyzing and interpreting results and for choosing suitable models. In this study, we globally evaluated and compared the sensitivities of 14 RS-based models (2 process-, 4 vegetation-index-, 5 light-use-efficiency, and 3 machine-learning-based models) and benchmarked them against GPP responses to climatic factors measured at flux sites and to elevated CO2 concentrations measured at free-air CO2 enrichment experiment sites. The results demonstrated that the models with relatively high sensitivity to increasing atmospheric CO2 concentrations showed a higher increasing GPP trend. The fundamental difference in the CO2 effect in the models' algorithm either considers the effect of CO2 through changes in greenness indices (nine models) or introduces the influences on photosynthesis (three models). The overall effects of temperature and radiation, in terms of both magnitude and sign, vary among the models, while the models respond relatively consistently to variations in precipitation. Spatially, larger differences among model sensitivity to climatic factors occur in the tropics; at high latitudes, models have a consistent and obvious positive response to variations in temperature and radiation, and precipitation significantly enhances the GPP in mid-latitudes. Compared with the results calculated by flux-site measurements, the model performance differed substantially among different sites. However, the sensitivities of most models are basically within the confidence interval of the flux-site results. In general, the comparison revealed that models differed substantially in the effect of environmental regulations, particularly CO2 fertilization and water stress, on GPP, and none of the models performed consistently better across the different ecosystems and under the various external conditions."
30855159,3.0,Methodological advances in statistical prediction,2019 Dec;31(12):1456-1466.,"Thirty years ago, Dawes, Faust, and Meehl (1989) argued that mental health professionals should routinely use statistical prediction rules to describe and diagnose clients, predict behaviors, and formulate treatment plans. Subsequent research has supported their claim that statistical prediction performs well when compared to clinical judgment. However, many of the things we thought we knew about statistical prediction have changed. The purpose of this literature review is to describe methodological advances in statistical prediction. Three broad areas are covered. First, while statistical prediction rules are valuable for criterion-referenced assessment (e.g., predicting violence, recidivism, treatment outcomes), they are valuable only for some norm-referenced assessment tasks (e.g., diagnosis but not describing personality and psychopathology). Second, statistical prediction is particularly prominent for the prediction of violence and criminal recidivism. Results from this area will be used to describe the validity of traditional clinical judgment, structured professional judgment, and statistical prediction. The results support the use of both structured professional judgment and statistical prediction. The effect of allowing professionals to override statistical predictions consistently led to lower validity. Third, issues in building statistical prediction rules are described, including the assignment of weights to predictors, the emergence of new statistical analyses (e.g., machine learning), and the role of theory. As research has progressed, statistical prediction has become one of the most exciting areas of psychological assessment. (PsycINFO Database Record (c) 2019 APA, all rights reserved)."
30851654,7.0,Backpropagation through time and the brain,2019 Apr;55:82-89.,"It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike."
30850092,6.0,Deep learning can see the unseeable: predicting molecular markers from MRI of brain gliomas,2019 May;74(5):367-373.,"This paper describes state-of-the-art methods for molecular biomarker prediction utilising magnetic resonance imaging. This review paper covers both classical machine learning approaches and deep learning approaches to identifying the predictive features and to perform the actual prediction. In particular, there have been substantial advances in recent years in predicting molecular markers for diffuse gliomas. There are few examples of molecular marker prediction for other brain tumours. Deep learning has contributed significantly to these advances, but suffers from challenges in identifying the features used to make predictions. Tools to better identify and understand those features represent an important area of active research."
30846346,10.0,Multimodal wrist-worn devices for seizure detection and advancing research: Focus on the Empatica wristbands,2019 Jul;153:79-82.,"Wearable automated seizure detection devices offer a high potential to improve seizure management, through continuous ambulatory monitoring, accurate seizure counts, and real-time alerts for prompt intervention. More importantly, these devices can be a life-saving help for people with a higher risk of sudden unexpected death in epilepsy (SUDEP), especially in case of generalized tonic-clonic seizures (GTCS). The Embrace and E4 wristbands (Empatica) are the first commercially available multimodal wristbands that were designed to sense the physiological hallmarks of ongoing GTCS: while Embrace only embeds a machine learning-based detection algorithm, both E4 and Embrace devices are equipped with motion (accelerometers, ACC) and electrodermal activity (EDA) sensors and both the devices received medical clearance (E4 from EU CE, Embrace from EU CE and US FDA). The aim of this contribution is to provide updated evidence of the effectiveness of GTCS detection and monitoring relying on the combination of ACM and EDA sensors. A machine learning algorithm able to recognize ACC and EDA signatures of GTCS-like events has been developed on E4 data, labeled using gold-standard video-EEG examined by epileptologists in clinical centers, and has undergone continuous improvement. While keeping an elevated sensitivity to GTCS (92-100%), algorithm improvements and growing data availability led to lower false alarm rate (FAR) from the initial ˜2 down to 0.2-1 false alarms per day, as showed by retrospective and prospective analyses in inpatient settings. Algorithm adjustment to better discriminate real-life physical activities from GTCS, has brought the initial FAR of ˜6 on outpatient real life settings, down to values comparable to best-case clinical settings (FAR < 0.5), with comparable sensitivity. Moreover, using multimodal sensing, it has been possible not only to detect GTCS but also to quantify seizure-induced autonomic dysfunction, based on automatic features of abnormal motion and EDA. The latter biosignal correlates with the duration of post-ictal generalized EEG suppression, a biomarker observed in 100% of monitored SUDEP cases."
30844756,12.0,"Application of mobile health, telemedicine and artificial intelligence to echocardiography",2019 Jun 1;6(2):R41-R52.,"The intersection of global broadband technology and miniaturized high-capability computing devices has led to a revolution in the delivery of healthcare and the birth of telemedicine and mobile health (mHealth). Rapid advances in handheld imaging devices with other mHealth devices such as smartphone apps and wearable devices are making great strides in the field of cardiovascular imaging like never before. Although these technologies offer a bright promise in cardiovascular imaging, it is far from straightforward. The massive data influx from telemedicine and mHealth including cardiovascular imaging supersedes the existing capabilities of current healthcare system and statistical software. Artificial intelligence with machine learning is the one and only way to navigate through this complex maze of the data influx through various approaches. Deep learning techniques are further expanding their role by image recognition and automated measurements. Artificial intelligence provides limitless opportunity to rigorously analyze data. As we move forward, the futures of mHealth, telemedicine and artificial intelligence are increasingly becoming intertwined to give rise to precision medicine."
30837982,13.0,Comparison of Open-Source Reverse Vaccinology Programs for Bacterial Vaccine Antigen Discovery,2019 Feb 14;10:113.,"Reverse Vaccinology (RV) is a widely used approach to identify potential vaccine candidates (PVCs) by screening the proteome of a pathogen through computational analyses. Since its first application in Group B meningococcus (MenB) vaccine in early 1990's, several software programs have been developed implementing different flavors of the first RV protocol. However, there has been no comprehensive review to date on these different RV tools. We have compared six of these applications designed for bacterial vaccines (NERVE, Vaxign, VaxiJen, Jenner-predict, Bowman-Heinson, and VacSol) against a set of 11 pathogens for which a curated list of known bacterial protective antigens (BPAs) was available. We present results on: (1) the comparison of criteria and programs used for the selection of PVCs (2) computational runtime and (3) performances in terms of fraction of proteome identified as PVC, fraction and enrichment of BPA identified in the set of PVCs. This review demonstrates that none of the programs was able to recall 100% of the tested set of BPAs and that the output lists of proteins are in poor agreement suggesting in the process of prioritize vaccine candidates not to rely on a single RV tool response. Singularly the best balance in terms of fraction of a proteome predicted as good candidate and recall of BPAs has been observed by the machine-learning approach proposed by Bowman (1) and enhanced by Heinson (2). Even though more performing than the other approaches it shows the disadvantage of limited accessibility to non-experts users and strong dependence between results and a-priori training dataset composition. In conclusion we believe that to significantly enhance the performances of next RV methods further studies should focus on the enhancement of accuracy of the existing protein annotation tools and should leverage on the assets of machine-learning techniques applied to biological datasets expanded also through the incorporation and curation of bacterial proteins characterized by negative experimental results."
30837884,7.0,Making Sense of the Epigenome Using Data Integration Approaches,2019 Feb 19;10:126.,"Epigenetic research involves examining the mitotically heritable processes that regulate gene expression, independent of changes in the DNA sequence. Recent technical advances such as whole-genome bisulfite sequencing and affordable epigenomic array-based technologies, allow researchers to measure epigenetic profiles of large cohorts at a genome-wide level, generating comprehensive high-dimensional datasets that may contain important information for disease development and treatment opportunities. The epigenomic profile for a certain disease is often a result of the complex interplay between multiple genetic and environmental factors, which poses an enormous challenge to visualize and interpret these data. Furthermore, due to the dynamic nature of the epigenome, it is critical to determine causal relationships from the many correlated associations. In this review we provide an overview of recent data analysis approaches to integrate various omics layers to understand epigenetic mechanisms of complex diseases, such as obesity and cancer. We discuss the following topics: (i) advantages and limitations of major epigenetic profiling techniques, (ii) resources for standardization, annotation and harmonization of epigenetic data, and (iii) statistical methods and machine learning methods for establishing data-driven hypotheses of key regulatory mechanisms. Finally, we discuss the future directions for data integration that shall facilitate the discovery of epigenetic-based biomarkers and therapies."
30834982,2.0,Spatio-temporal simulation and prediction of land-use change using conventional and machine learning models: a review,2019 Mar 5;191(4):205.,"Spatio-temporal land-use change modeling, simulation, and prediction have become one of the critical issues in the last three decades due to uncertainty, structure, flexibility, accuracy, the ability for improvement, and the capability for integration of available models. Therefore, many types of models such as dynamic, statistical, and machine learning (ML) models have been used in the geographic information system (GIS) environment to fulfill the high-performance requirements of land-use modeling. This paper provides a literature review on models for modeling, simulating, and predicting land-use change to determine the best approach that can realistically simulate land-use changes. Therefore, the general characteristics of conventional and ML models for land-use change are described, and the different techniques used in the design of these models are classified. The strengths and weaknesses of the various dynamic, statistical, and ML models are determined according to the analysis and discussion of the characteristics of these models. The results of the review confirm that ML models are the most powerful models for simulating land-use change because they can include all driving forces of land-use change in the simulation process and simulate linear and non-linear phenomena, which dynamic models and statistical models are unable to do. However, ML models also have limitations. For instance, some ML models are complex, the simulation rules cannot be changed, and it is difficult to understand how ML models work in a system. However, this can be solved via the use of programming languages such as Python, which in turn improve the simulation capabilities of the ML models."
30834534,7.0,Machine learning in plant-pathogen interactions: empowering biological predictions from field scale to genome scale,2020 Oct;228(1):35-41.,"Machine learning (ML) encompasses statistical methods that learn to identify patterns in complex datasets. Here, I review application areas in plant-pathogen interactions that have recently benefited from ML, such as disease monitoring, the discovery of gene regulatory networks, genomic selection for disease resistance and prediction of pathogen effectors. However, achieving robust performance from ML is not trivial and requires knowledge of both the methodology and the biology. I discuss common pitfalls and challenges in using ML approaches. Finally, I highlight future opportunities for ML as a tool for dissecting plant-pathogen interactions using high-throughput data, for example, through integration of diverse data sources and the analysis with higher resolution, such as from individual cells or on elaborate spatial and temporal scales."
30832275,7.0,Pharmacogenomic and Pharmacotranscriptomic Profiling of Childhood Acute Lymphoblastic Leukemia: Paving the Way to Personalized Treatment,2019 Mar 1;10(3):191.,"Personalized medicine is focused on research disciplines which contribute to the individualization of therapy, like pharmacogenomics and pharmacotranscriptomics. Acute lymphoblastic leukemia (ALL) is the most common malignancy of childhood. It is one of the pediatric malignancies with the highest cure rate, but still a lethal outcome due to therapy accounts for 1%⁻3% of deaths. Further improvement of treatment protocols is needed through the implementation of pharmacogenomics and pharmacotranscriptomics. Emerging high-throughput technologies, including microarrays and next-generation sequencing, have provided an enormous amount of molecular data with the potential to be implemented in childhood ALL treatment protocols. In the current review, we summarized the contribution of these novel technologies to the pharmacogenomics and pharmacotranscriptomics of childhood ALL. We have presented data on molecular markers responsible for the efficacy, side effects, and toxicity of the drugs commonly used for childhood ALL treatment, i.e., glucocorticoids, vincristine, asparaginase, anthracyclines, thiopurines, and methotrexate. Big data was generated using high-throughput technologies, but their implementation in clinical practice is poor. Research efforts should be focused on data analysis and designing prediction models using machine learning algorithms. Bioinformatics tools and the implementation of artificial i Lack of association of the CEP72 rs924607 TT genotype with intelligence are expected to open the door wide for personalized medicine in the clinical practice of childhood ALL."
30832207,7.0,Snails In Silico: A Review of Computational Studies on the Conopeptides,2019 Mar 1;17(3):145.,"Marine cone snails are carnivorous gastropods that use peptide toxins called conopeptides both as a defense mechanism and as a means to immobilize and kill their prey. These peptide toxins exhibit a large chemical diversity that enables exquisite specificity and potency for target receptor proteins. This diversity arises in terms of variations both in amino acid sequence and length, and in posttranslational modifications, particularly the formation of multiple disulfide linkages. Most of the functionally characterized conopeptides target ion channels of animal nervous systems, which has led to research on their therapeutic applications. Many facets of the underlying molecular mechanisms responsible for the specificity and virulence of conopeptides, however, remain poorly understood. In this review, we will explore the chemical diversity of conopeptides from a computational perspective. First, we discuss current approaches used for classifying conopeptides. Next, we review different computational strategies that have been applied to understanding and predicting their structure and function, from machine learning techniques for predictive classification to docking studies and molecular dynamics simulations for molecular-level understanding. We then review recent novel computational approaches for rapid high-throughput screening and chemical design of conopeptides for particular applications. We close with an assessment of the state of the field, emphasizing important questions for future lines of inquiry."
30831310,49.0,Ten simple rules for predictive modeling of individual differences in neuroimaging,2019 Jun;193:35-45.,"Establishing brain-behavior associations that map brain organization to phenotypic measures and generalize to novel individuals remains a challenge in neuroimaging. Predictive modeling approaches that define and validate models with independent datasets offer a solution to this problem. While these methods can detect novel and generalizable brain-behavior associations, they can be daunting, which has limited their use by the wider connectivity community. Here, we offer practical advice and examples based on functional magnetic resonance imaging (fMRI) functional connectivity data for implementing these approaches. We hope these ten rules will increase the use of predictive models with neuroimaging data."
30828395,8.0,Radiomics in Oncological PET/CT: a Methodological Overview,2019 Feb;53(1):14-29.,"Radiomics is a medical imaging analysis approach based on computer-vision. Metabolic radiomics in particular analyses the spatial distribution patterns of molecular metabolism on PET images. Measuring intratumoral heterogeneity via image is one of the main targets of radiomics research, and it aims to build a image-based model for better patient management. The workflow of radiomics using texture analysis follows these steps: 1) imaging (image acquisition and reconstruction); 2) preprocessing (segmentation & quantization); 3) quantification (texture matrix design & texture feature extraction); and 4) analysis (statistics and/or machine learning). The parameters or conditions at each of these steps are effect on the results. In statistical testing or modeling, problems such as multiple comparisons, dependence on other variables, and high dimensionality of small sample size data should be considered. Standardization of methodology and harmonization of image quality are one of the most important challenges with radiomics methodology. Even though there are current issues in radiomics methodology, it is expected that radiomics will be clinically useful in personalized medicine for oncology."
30828380,7.0,Current trends in biomarker discovery and analysis tools for traumatic brain injury,2019 Feb 19;13:16.,"Traumatic brain injury (TBI) affects 1.7 million people in the United States each year, causing lifelong functional deficits in cognition and behavior. The complex pathophysiology of neural injury is a primary barrier to developing sensitive and specific diagnostic tools, which consequentially has a detrimental effect on treatment regimens. Biomarkers of other diseases (e.g. cancer) have provided critical insight into disease emergence and progression that lend to developing powerful clinical tools for intervention. Therefore, the biomarker discovery field has recently focused on TBI and made substantial advancements to characterize markers with promise of transforming TBI patient diagnostics and care. This review focuses on these key advances in neural injury biomarkers discovery, including novel approaches spanning from omics-based approaches to imaging and machine learning as well as the evolution of established techniques."
30827579,3.0,Malformations of cortical development: The role of 7-Tesla magnetic resonance imaging in diagnosis,2019 Mar;175(3):157-162.,"Comparison studies between 7T and 1.5 or 3T magnetic resonance imaging (MRI) have demonstrated the added value of ultra-high field (UHF) MRI to better identify, delineate and characterize malformations of cortical development (MCD), and to disambiguate doubtful findings observed at lower field strengths. High resolution structural sequences such as magnetization prepared two rapid acquisition gradient echoes (MP2RAGE), fluid and white matter suppression MP2RAGE (FLAWS), and susceptibility-weighted imaging (SWI) appear to be key to the improvement of MCD diagnosis in clinical practice. 7T MRI offers not only images of high resolution and contrast but also provides many quantitative approaches capable of acting as more efficient probes of microstructure and ameliorating the categorization of MCDs. Post-processing of multiparametric ultra-high resolution and quantitative data may also be used to improve automated detection of MCD via machine learning. Therefore, 7T MRI can be considered as a useful tool in the presurgical evaluation of drug-resistant partial epilepsies, particularly, but not exclusively, in cases of normal appearing conventional MRI. It also opens many perspectives in the fields of in vivo histology and computational anatomy."
30825704,2.0,An integrative computational architecture for object-driven cortex,2019 Apr;55:73-81.,"Computational architecture for object-driven cortex Objects in motion activate multiple cortical regions in every lobe of the human brain. Do these regions represent a collection of independent systems, or is there an overarching functional architecture spanning all of object-driven cortex? Inspired by recent work in artificial intelligence (AI), machine learning, and cognitive science, we consider the hypothesis that these regions can be understood as a coherent network implementing an integrative computational system that unifies the functions needed to perceive, predict, reason about, and plan with physical objects-as in the paradigmatic case of using or making tools. Our proposal draws on a modeling framework that combines multiple AI methods, including causal generative models, hybrid symbolic-continuous planning algorithms, and neural recognition networks, with object-centric, physics-based representations. We review evidence relating specific components of our proposal to the specific regions that comprise object-driven cortex, and lay out future research directions with the goal of building a complete functional and mechanistic account of this system."
30825538,17.0,Stress detection in daily life scenarios using smart phones and wearable sensors: A survey,2019 Apr;92:103139.,"Stress has become a significant cause for many diseases in the modern society. Recently, smartphones, smartwatches and smart wrist-bands have become an integral part of our lives and have reached a widespread usage. This raised the question of whether we can detect and prevent stress with smartphones and wearable sensors. In this survey, we will examine the recent works on stress detection in daily life which are using smartphones and wearable devices. Although there are a number of works related to stress detection in controlled laboratory conditions, the number of studies examining stress detection in daily life is limited. We will divide and investigate the works according to used physiological modality and their targeted environment such as office, campus, car and unrestricted daily life conditions. We will also discuss promising techniques, alleviation methods and research challenges."
30825037,3.0,Towards a Mechanistic-Driven Precision Medicine Approach for Tinnitus,2019 Apr;20(2):115-131.,"In this position review, we propose to establish a path for replacing the empirical classification of tinnitus with a taxonomy from precision medicine. The goal of a classification system is to understand the inherent heterogeneity of individuals experiencing and suffering from tinnitus and to identify what differentiates potential subgroups. Identification of different patient subgroups with distinct audiological, psychophysical, and neurophysiological characteristics will facilitate the management of patients with tinnitus as well as the design and execution of drug development and clinical trials, which, for the most part, have not yielded conclusive results. An alternative outcome of a precision medicine approach in tinnitus would be that additional mechanistic phenotyping might not lead to the identification of distinct drivers in each individual, but instead, it might reveal that each individual may display a quantitative blend of causal factors. Therefore, a precision medicine approach towards identifying these causal factors might not lead to subtyping these patients but may instead highlight causal pathways that can be manipulated for therapeutic gain. These two outcomes are not mutually exclusive, and no matter what the final outcome is, a mechanistic-driven precision medicine approach is a win-win approach for advancing tinnitus research and treatment. Although there are several controversies and inconsistencies in the tinnitus field, which will not be discussed here, we will give a few examples, as to how the field can move forward by exploring the major neurophysiological tinnitus models, mostly by taking advantage of the common features supported by all of the models. Our position stems from the central concept that, as a field, we can and must do more to bring studies of mechanisms into the realm of neuroscience."
30824244,4.0,Recent Developments in the Treatment of Depression,2019 Mar;50(2):257-269.,"The cognitive and behavioral interventions can be as efficacious as antidepressant medications and more enduring, but some patients will be more likely to respond to one than the other. Recent work has focused on developing sophisticated selection algorithms using machine-learning approaches that answer the question, ""What works best for whom?"" Moreover, the vast majority of people suffering from depression reside in low- and middle-income countries where access to either psychotherapy or medications is virtually nonexistent. Great strides have been made in training nonspecialist providers (known as task sharing) to overcome this gap. Finally, recent work growing out of evolutionary psychology suggests that antidepressant medications may suppress symptoms at the expense of prolonging the underlying episode so as to increase the risk of relapse whenever someone tries to stop. We address each of these developments and their cumulative implications."
30821199,2.0,Deep learning for predicting toxicity of chemicals: a mini review,2018;36(4):252-271.,"Humans and wildlife inhabit a world with panoply of natural and synthetic chemicals. Alarmingly, only a limited number of chemicals have undergone comprehensive toxicological evaluation due to limitations of traditional toxicity testing. High-throughput screening assays provide a higher-speed alternative for conventional toxicity testing. Advancement of high-throughput bioassay technology has greatly increased chemical toxicity data volumes in the past decade, pushing toxicology research into a ""big data"" era. However, traditional data analysis methods fail to effectively process large data volumes, presenting both a challenge and an opportunity for toxicologists. Deep learning, a machine learning method leveraging deep neural networks (DNNs), is a proven useful tool for building quantitative structure-activity relationship (QSAR) models for toxicity prediction utilizing these new large datasets. In this mini review, a brief technical background on DNNs is provided, and the current state of chemical toxicity prediction models built with DNNs is reviewed. In addition, relevant toxicity data sources are summarized, possible limitations are discussed, and perspectives on DNN utilization in chemical toxicity prediction are given."
30820462,19.0,Artificial intelligence for precision oncology: beyond patient stratification,2019 Feb 25;3:6.,"The data-driven identification of disease states and treatment options is a crucial challenge for precision oncology. Artificial intelligence (AI) offers unique opportunities for enhancing such predictive capabilities in the lab and the clinic. AI, including its best-known branch of research, machine learning, has significant potential to enable precision oncology well beyond relatively well-known pattern recognition applications, such as the supervised classification of single-source omics or imaging datasets. This perspective highlights key advances and challenges in that direction. Furthermore, it argues that AI's scope and depth of research need to be expanded to achieve ground-breaking progress in precision oncology."
30815669,18.0,Deep learning for cardiovascular medicine: a practical primer,2019 Jul 1;40(25):2058-2073.,"Deep learning (DL) is a branch of machine learning (ML) showing increasing promise in medicine, to assist in data classification, novel disease phenotyping and complex decision making. Deep learning is a form of ML typically implemented via multi-layered neural networks. Deep learning has accelerated by recent advances in computer hardware and algorithms and is increasingly applied in e-commerce, finance, and voice and image recognition to learn and classify complex datasets. The current medical literature shows both strengths and limitations of DL. Strengths of DL include its ability to automate medical image interpretation, enhance clinical decision-making, identify novel phenotypes, and select better treatment pathways in complex diseases. Deep learning may be well-suited to cardiovascular medicine in which haemodynamic and electrophysiological indices are increasingly captured on a continuous basis by wearable devices as well as image segmentation in cardiac imaging. However, DL also has significant weaknesses including difficulties in interpreting its models (the 'black-box' criticism), its need for extensive adjudicated ('labelled') data in training, lack of standardization in design, lack of data-efficiency in training, limited applicability to clinical trials, and other factors. Thus, the optimal clinical application of DL requires careful formulation of solvable problems, selection of most appropriate DL algorithms and data, and balanced interpretation of results. This review synthesizes the current state of DL for cardiovascular clinicians and investigators, and provides technical context to appreciate the promise, pitfalls, near-term challenges, and opportunities for this exciting new area."
30815461,1.0,Blood Pressure Assessment with Differential Pulse Transit Time and Deep Learning: A Proof of Concept,2019 Feb;5(1):23-27.,"Background:                    Modern clinical environments are laden with technology devices continuously gathering physiological data from patients. This is especially true in critical care environments, where life-saving decisions may have to be made on the basis of signals from monitoring devices. Hemodynamic monitoring is essential in dialysis, surgery, and in critically ill patients. For the most severe patients, blood pressure is normally assessed through a catheter, which is an invasive procedure that may result in adverse effects. Blood pressure can also be monitored noninvasively through different methods and these data can be used for the continuous assessment of pressure using machine learning methods. Previous studies have found pulse transit time to be related to blood pressure. In this short paper, we propose to study the feasibility of implementing a data-driven model based on restricted Boltzmann machine artificial neural networks, delivering a first proof of concept for the validity and viability of a method for blood pressure prediction based on these models.              Summary and key messages:                    For the most severe patients (e.g., dialysis, surgery, and the critically ill), blood pressure is normally assessed through invasive catheters. Alternatively, noninvasive methods have also been developed for its monitorization. Data obtained from noninvasive measurements can be used for the continuous assessment of pressure using machine learning methods. In this study, a restricted Boltzmann machine artificial neural network is used to present a first proof of concept for the validity and viability of a method for blood pressure prediction."
30815459,7.0,Societal Issues Concerning the Application of Artificial Intelligence in Medicine,2019 Feb;5(1):11-17.,"Background:                    Medicine is becoming an increasingly data-centred discipline and, beyond classical statistical approaches, artificial intelligence (AI) and, in particular, machine learning (ML) are attracting much interest for the analysis of medical data. It has been argued that AI is experiencing a fast process of commodification. This characterization correctly reflects the current process of industrialization of AI and its reach into society. Therefore, societal issues related to the use of AI and ML should not be ignored any longer and certainly not in the medical domain. These societal issues may take many forms, but they all entail the design of models from a human-centred perspective, incorporating human-relevant requirements and constraints. In this brief paper, we discuss a number of specific issues affecting the use of AI and ML in medicine, such as fairness, privacy and anonymity, explainability and interpretability, but also some broader societal issues, such as ethics and legislation. We reckon that all of these are relevant aspects to consider in order to achieve the objective of fostering acceptance of AI- and ML-based technologies, as well as to comply with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. Our specific goal here is to reflect on how all these topics affect medical applications of AI and ML. This paper includes some of the contents of the ""2nd Meeting of Science and Dialysis: Artificial Intelligence,"" organized in the Bellvitge University Hospital, Barcelona, Spain.              Summary and key messages:                    AI and ML are attracting much interest from the medical community as key approaches to knowledge extraction from data. These approaches are increasingly colonizing ambits of social impact, such as medicine and healthcare. Issues of social relevance with an impact on medicine and healthcare include (although they are not limited to) fairness, explainability, privacy, ethics and legislation."
30815458,5.0,Progress in the Development and Challenges for the Use of Artificial Kidneys and Wearable Dialysis Devices,2019 Feb;5(1):3-10.,"Background:                    Renal transplantation is the treatment of choice for chronic kidney disease (CKD) patients, but the shortage of kidneys and the disabling medical conditions these patients suffer from make dialysis essential for most of them. Since dialysis drastically affects the patients' lifestyle, there are great expectations for the development of wearable artificial kidneys, although their use is currently impeded by major concerns about safety. On the other hand, dialysis patients with hemodynamic instability do not usually tolerate intermittent dialysis therapy because of their inability to adapt to a changing scenario of unforeseen events. Thus, the development of novel wearable dialysis devices and the improvement of clinical tolerance will need contributions from new branches of engineering such as artificial intelligence (AI) and machine learning (ML) for the real-time analysis of equipment alarms, dialysis parameters, and patient-related data with a real-time feedback response. These technologies are endowed with abilities normally associated with human intelligence such as learning, problem solving, human speech understanding, or planning and decision-making. Examples of common applications of AI are visual perception (computer vision), speech recognition, and language translation. In this review, we discuss recent progresses in the area of dialysis and challenges for the use of AI in the development of artificial kidneys.              Summary and key messages:                    Emerging technologies derived from AI, ML, electronics, and robotics will offer great opportunities for dialysis therapy, but much innovation is needed before we achieve a smart dialysis machine able to analyze and understand changes in patient homeostasis and to respond appropriately in real time. Great efforts are being made in the fields of tissue engineering and regenerative medicine to provide alternative cell-based approaches for the treatment of renal failure, including bioartificial renal systems and the implantation of bioengineered kidney constructs."
30810430,11.0,Introduction to artificial intelligence in medicine,2019 Apr;28(2):73-81.,"The term Artificial Intelligence (AI) was coined by John McCarthy in 1956 during a conference held on this subject. However, the possibility of machines being able to simulate human behavior and actually think was raised earlier by Alan Turing who developed the Turing test in order to differentiate humans from machines. Since then, computational power has grown to the point of instant calculations and the ability evaluate new data, according to previously assessed data, in real time. Today, AI is integrated into our daily lives in many forms, such as personal assistants (Siri, Alexa, Google assistant etc.), automated mass transportation, aviation and computer gaming. More recently, AI has also begun to be incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy, opening the path to providing better healthcare overall. Radiological images, pathology slides, and patients' electronic medical records (EMR) are being evaluated by machine learning, aiding in the process of diagnosis and treatment of patients and augmenting physicians' capabilities. Herein we describe the current status of AI in medicine, the way it is used in the different disciplines and future trends."
30808014,28.0,Deep learning for electroencephalogram (EEG) classification tasks: a review,2019 Jun;16(3):031001.,"Objective:                    Electroencephalography (EEG) analysis has been an important tool in neuroscience with applications in neuroscience, neural engineering (e.g. Brain-computer interfaces, BCI's), and even commercial applications. Many of the analytical tools used in EEG studies have used machine learning to uncover relevant information for neural classification and neuroimaging. Recently, the availability of large EEG data sets and advances in machine learning have both led to the deployment of deep learning architectures, especially in the analysis of EEG signals and in understanding the information it may contain for brain functionality. The robust automatic classification of these signals is an important step towards making the use of EEG more practical in many applications and less reliant on trained professionals. Towards this goal, a systematic review of the literature on deep learning applications to EEG classification was performed to address the following critical questions: (1) Which EEG classification tasks have been explored with deep learning? (2) What input formulations have been used for training the deep networks? (3) Are there specific deep learning network structures suitable for specific types of tasks?              Approach:                    A systematic literature review of EEG classification using deep learning was performed on Web of Science and PubMed databases, resulting in 90 identified studies. Those studies were analyzed based on type of task, EEG preprocessing methods, input type, and deep learning architecture.              Main results:                    For EEG classification tasks, convolutional neural networks, recurrent neural networks, deep belief networks outperform stacked auto-encoders and multi-layer perceptron neural networks in classification accuracy. The tasks that used deep learning fell into five general groups: emotion recognition, motor imagery, mental workload, seizure detection, event related potential detection, and sleep scoring. For each type of task, we describe the specific input formulation, major characteristics, and end classifier recommendations found through this review.              Significance:                    This review summarizes the current practices and performance outcomes in the use of deep learning for EEG classification. Practical suggestions on the selection of many hyperparameters are provided in the hope that they will promote or guide the deployment of deep learning to EEG datasets in future research."
30803815,1.0,Machine learning in whole-body MRI: experiences and challenges from an applied study using multicentre data,2019 May;74(5):346-356.,"Machine learning is now being increasingly employed in radiology to assist with tasks such as automatic lesion detection, segmentation, and characterisation. We are currently involved in an National Institute of Health Research (NIHR)-funded project, which aims to develop machine learning methods to improve the diagnostic performance and reduce the radiology reading time of whole-body magnetic resonance imaging (MRI) scans, in patients being staged for cancer (MALIBO study). We describe here the main challenges we have encountered during the course of this project. Data quality and uniformity are the two most important data traits to be considered in clinical trials incorporating machine learning. Robust data pre-processing and machine learning pipelines have been employed in MALIBO, a task facilitated by the now freely available machine learning libraries and toolboxes. Another important consideration for achieving the desired clinical outcome in MALIBO, was to effectively host the resulting machine learning output, along with the clinical images, for reading in a clinical environment. Finally, a range of legal, ethical, and clinical acceptance issues should be considered when attempting to incorporate computer-assisting tools into clinical practice. The road from translating computational methods into potentially useful clinical tools involves an analytical, stepwise adaptation approach, as well as engagement of a multidisciplinary team."
30802231,10.0,Deep Learning Applications in Chest Radiography and Computed Tomography: Current State of the Art,2019 Mar;34(2):75-85.,"Deep learning is a genre of machine learning that allows computational models to learn representations of data with multiple levels of abstraction using numerous processing layers. A distinctive feature of deep learning, compared with conventional machine learning methods, is that it can generate appropriate models for tasks directly from the raw data, removing the need for human-led feature extraction. Medical images are particularly suited for deep learning applications. Deep learning techniques have already demonstrated high performance in the detection of diabetic retinopathy on fundoscopic images and metastatic breast cancer cells on pathologic images. In radiology, deep learning has the opportunity to provide improved accuracy of image interpretation and diagnosis. Many groups are exploring the possibility of using deep learning-based applications to solve unmet clinical needs. In chest imaging, there has been a large effort to develop and apply computer-aided detection systems for the detection of lung nodules on chest radiographs and chest computed tomography. The essential limitation to computer-aided detection is an inability to learn from new information. To overcome these deficiencies, many groups have turned to deep learning approaches with promising results. In addition to nodule detection, interstitial lung disease recognition, lesion segmentation, diagnosis and patient outcomes have been addressed by deep learning approaches. The purpose of this review article was to cover the current state of the art for deep learning approaches and its limitations, and some of the potential impact on the field of radiology, with specific reference to chest imaging."
30799571,37.0,Design Characteristics of Studies Reporting the Performance of Artificial Intelligence Algorithms for Diagnostic Analysis of Medical Images: Results from Recently Published Papers,2019 Mar;20(3):405-410.,"Objective:                    To evaluate the design characteristics of studies that evaluated the performance of artificial intelligence (AI) algorithms for the diagnostic analysis of medical images.              Materials and methods:                    PubMed MEDLINE and Embase databases were searched to identify original research articles published between January 1, 2018 and August 17, 2018 that investigated the performance of AI algorithms that analyze medical images to provide diagnostic decisions. Eligible articles were evaluated to determine 1) whether the study used external validation rather than internal validation, and in case of external validation, whether the data for validation were collected, 2) with diagnostic cohort design instead of diagnostic case-control design, 3) from multiple institutions, and 4) in a prospective manner. These are fundamental methodologic features recommended for clinical validation of AI performance in real-world practice. The studies that fulfilled the above criteria were identified. We classified the publishing journals into medical vs. non-medical journal groups. Then, the results were compared between medical and non-medical journals.              Results:                    Of 516 eligible published studies, only 6% (31 studies) performed external validation. None of the 31 studies adopted all three design features: diagnostic cohort design, the inclusion of multiple institutions, and prospective data collection for external validation. No significant difference was found between medical and non-medical journals.              Conclusion:                    Nearly all of the studies published in the study period that evaluated the performance of AI algorithms for diagnostic analysis of medical images were designed as proof-of-concept technical feasibility studies and did not have the design features that are recommended for robust validation of the real-world clinical performance of AI algorithms."
30790780,2.0,Early seizure detection for closed loop direct neurostimulation devices in epilepsy,2019 Aug;16(4):041001.,"Current treatment concepts for epilepsy are based on continuous drug delivery or electrical stimulation to prevent the occurrence of seizures, exposing the brain and body to a mostly unneeded risk of adverse effects. To address the infrequent occurrence and short duration of epileptic seizures, intelligent implantable closed-loop devices are needed which are based on a refined analysis of ongoing brain activity with highly specific and fast detection algorithms to allow for timely, ictal interventions. Since the development and FDA approval of a first closed loop neurostimulation device relying on simple threshold-based approaches, machine learning approaches became widely available, probably outperformed in the near future by deep convolutional neural networks, which already showed to be extremely successful in pattern recognition in images and partly in signal analysis. Handcrafted features or rules defined by experts become replaced by systematic feature selection procedures and systematic hyperparameter search approaches. Training of these classifiers augments the need of large databases with intracranial EEG recordings, which is partly given by existing databases but potentially can be replaced by continuously transferring data from implanted devices and their publication for research purposes. Already in early design states, the final target hardware must be taken into account for algorithm development. Size, power consumption and, as a consequence, limited computational resources given by low power microcontrollers, FPGAs and ASICS limit the complexity of feature computation, classifier complexity, and the numbers and complexity of layers of deep neuronal networks. Novel approaches for early seizure detection will be a key module for new generations of closed-loop devices together with improved low power implant hardware and will provide together with more efficient intervention paradigms new treatment options for patients with difficult to treat epilepsy."
30785004,8.0,Analyzing biological and artificial neural networks: challenges with opportunities for synergy?,2019 Apr;55:55-64.,"Deep neural networks (DNNs) transform stimuli across multiple processing stages to produce representations that can be used to solve complex tasks, such as object recognition in images. However, a full understanding of how they achieve this remains elusive. The complexity of biological neural networks substantially exceeds the complexity of DNNs, making it even more challenging to understand the representations they learn. Thus, both machine learning and computational neuroscience are faced with a shared challenge: how can we analyze their representations in order to understand how they solve complex tasks? We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs, and in turn, how recently developed techniques for analysis of DNNs can be useful for understanding representations in biological neural networks. We explore opportunities for synergy between the two fields, such as the use of DNNs as in silico model systems for neuroscience, and how this synergy can lead to new hypotheses about the operating principles of biological neural networks."
30783371,5.0,Artificial intelligence in medical imaging of the liver,2019 Feb 14;25(6):672-682.,"Artificial intelligence (AI), particularly deep learning algorithms, is gaining extensive attention for its excellent performance in image-recognition tasks. They can automatically make a quantitative assessment of complex medical image characteristics and achieve an increased accuracy for diagnosis with higher efficiency. AI is widely used and getting increasingly popular in the medical imaging of the liver, including radiology, ultrasound, and nuclear medicine. AI can assist physicians to make more accurate and reproductive imaging diagnosis and also reduce the physicians' workload. This article illustrates basic technical knowledge about AI, including traditional machine learning and deep learning algorithms, especially convolutional neural networks, and their clinical application in the medical imaging of liver diseases, such as detecting and evaluating focal liver lesions, facilitating treatment, and predicting liver treatment response. We conclude that machine-assisted medical services will be a promising solution for future liver medical care. Lastly, we discuss the challenges and future directions of clinical application of deep learning techniques."
30780045,32.0,Continual lifelong learning with neural networks: A review,2019 May;113:54-71.,"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration."
30779785,14.0,Applications of artificial neural networks in health care organizational decision-making: A scoping review,2019 Feb 19;14(2):e0212356.,"Health care organizations are leveraging machine-learning techniques, such as artificial neural networks (ANN), to improve delivery of care at a reduced cost. Applications of ANN to diagnosis are well-known; however, ANN are increasingly used to inform health care management decisions. We provide a seminal review of the applications of ANN to health care organizational decision-making. We screened 3,397 articles from six databases with coverage of Health Administration, Computer Science and Business Administration. We extracted study characteristics, aim, methodology and context (including level of analysis) from 80 articles meeting inclusion criteria. Articles were published from 1997-2018 and originated from 24 countries, with a plurality of papers (26 articles) published by authors from the United States. Types of ANN used included ANN (36 articles), feed-forward networks (25 articles), or hybrid models (23 articles); reported accuracy varied from 50% to 100%. The majority of ANN informed decision-making at the micro level (61 articles), between patients and health care providers. Fewer ANN were deployed for intra-organizational (meso- level, 29 articles) and system, policy or inter-organizational (macro- level, 10 articles) decision-making. Our review identifies key characteristics and drivers for market uptake of ANN for health care organizational decision-making to guide further adoption of this technique."
30774271,12.0,Functional gastrointestinal disorders and gut-brain axis: What does the future hold?,2019 Feb 7;25(5):552-566.,"Despite their high prevalence, lack of understanding of the exact pathophysiology of the functional gastrointestinal disorders has restricted us to symptomatic diagnostic tools and therapies. Complex mechanisms underlying the disturbances in the bidirectional communication between the gastrointestinal tract and the brain have a vital role in the pathogenesis and are key to our understanding of the disease phenomenon. Although we have come a long way in our understanding of these complex disorders with the help of studies on animals especially rodents, there need to be more studies in humans, especially to identify the therapeutic targets. This review study looks at the anatomical features of the gut-brain axis in order to discuss the different factors and underlying molecular mechanisms that may have a role in the pathogenesis of functional gastrointestinal disorders. These molecules and their receptors can be targeted in future for further studies and possible therapeutic interventions. The article also discusses the potential role of artificial intelligence and machine learning and its possible role in our understanding of these scientifically challenging disorders."
30770893,21.0,Deep neural networks in psychiatry,2019 Nov;24(11):1583-1598.,"Machine and deep learning methods, today's core of artificial intelligence, have been applied with increasing success and impact in many commercial and research settings. They are powerful tools for large scale data analysis, prediction and classification, especially in very data-rich environments (""big data""), and have started to find their way into medical applications. Here we will first give an overview of machine learning methods, with a focus on deep and recurrent neural networks, their relation to statistics, and the core principles behind them. We will then discuss and review directions along which (deep) neural networks can be, or already have been, applied in the context of psychiatry, and will try to delineate their future potential in this area. We will also comment on an emerging area that so far has been much less well explored: by embedding semantically interpretable computational models of brain dynamics or behavior into a statistical machine learning context, insights into dysfunction beyond mere prediction and classification may be gained. Especially this marriage of computational models with statistical inference may offer insights into neural and behavioral mechanisms that could open completely novel avenues for psychiatric treatment."
30768796,3.0,Radiation Therapy Quality Assurance Tasks and Tools: The Many Roles of Machine Learning,2020 Jun;47(5):e168-e177.,"The recent explosion in machine learning efforts in the quality assurance (QA) space has produced a variety of proofs-of-concept many with promising results. Expected outcomes of model implementation include improvements in planning time, plan quality, advanced dosimetric QA, predictive machine maintenance, increased safety checks, and developments key for new QA paradigms driven by adaptive planning. In this article, we outline several areas of research and discuss some of the unique challenges each area presents."
30763612,126.0,A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models,2019 Jun;110:12-22.,"Objectives:                    The objective of this study was to compare performance of logistic regression (LR) with machine learning (ML) for clinical prediction modeling in the literature.              Study design and setting:                    We conducted a Medline literature search (1/2016 to 8/2017) and extracted comparisons between LR and ML models for binary outcomes.              Results:                    We included 71 of 927 studies. The median sample size was 1,250 (range 72-3,994,872), with 19 predictors considered (range 5-563) and eight events per predictor (range 0.3-6,697). The most common ML methods were classification trees, random forests, artificial neural networks, and support vector machines. In 48 (68%) studies, we observed potential bias in the validation procedures. Sixty-four (90%) studies used the area under the receiver operating characteristic curve (AUC) to assess discrimination. Calibration was not addressed in 56 (79%) studies. We identified 282 comparisons between an LR and ML model (AUC range, 0.52-0.99). For 145 comparisons at low risk of bias, the difference in logit(AUC) between LR and ML was 0.00 (95% confidence interval, -0.18 to 0.18). For 137 comparisons at high risk of bias, logit(AUC) was 0.34 (0.20-0.47) higher for ML.              Conclusion:                    We found no evidence of superior performance of ML over LR. Improvements in methodology and reporting are needed for studies that compare modeling algorithms."
30762522,5.0,M/EEG-Based Bio-Markers to Predict the MCI and Alzheimer's Disease: A Review From the ML Perspective,2019 Oct;66(10):2924-2935.,"This paper reviews the state-of-the-art neuromarkers development for the prognosis of Alzheimer's disease (AD) and mild cognitive impairment (MCI). The first part of this paper is devoted to reviewing the recently emerged machine learning (ML) algorithms based on electroencephalography (EEG) and magnetoencephalography (MEG) modalities. In particular, the methods are categorized by different types of neuromarkers. The second part of the review is dedicated to a series of investigations that further highlight the differences between these two modalities. First, several source reconstruction methods are reviewed and their source-level performances explored, followed by an objective comparison between EEG and MEG from multiple perspectives. Finally, a number of the most recent reports on classification of MCI/AD during resting state using EEG/MEG are documented to show the up-to-date performance for this well-recognized data collecting scenario. It is noticed that the MEG modality may be particularly effective in distinguishing between subjects with MCI and healthy controls, a high classification accuracy of more than 98% was reported recently; whereas the EEG seems to be performing well in classifying AD and healthy subjects, which also reached around 98% of the accuracy. A number of influential factors have also been raised and suggested for careful considerations while evaluating the ML-based diagnosis systems in the real-world scenarios."
30762223,16.0,Multiparametric MRI and radiomics in prostate cancer: a review,2019 Mar;42(1):3-25.,"Multiparametric MRI (mpMRI) is an imaging modality that combines anatomical MR imaging with one or more functional MRI sequences. It has become a versatile tool for detecting and characterising prostate cancer (PCa). The traditional role of mpMRI was confined to PCa staging, but due to the advanced imaging techniques, its role has expanded to various stages in clinical practises including tumour detection, disease monitor during active surveillance and sequential imaging for patient follow-up. Meanwhile, with the growing speed of data generation and the increasing volume of imaging data, it is highly demanded to apply computerised methods to process mpMRI data and extract useful information. Hence quantitative analysis for imaging data using radiomics has become an emerging paradigm. The application of radiomics approaches in prostate cancer has not only enabled automatic localisation of the disease but also provided a non-invasive solution to assess tumour biology (e.g. aggressiveness and the presence of hypoxia). This article reviews mpMRI and its expanding role in PCa detection, staging and patient management. Following that, an overview of prostate radiomics will be provided, with a special focus on its current applications as well as its future directions."
30760912,44.0,Deep learning and process understanding for data-driven Earth system science,2019 Feb;566(7743):195-204.,"Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning."
30760118,3.0,Preparing next-generation scientists for biomedical big data: artificial intelligence approaches,2019 May 1;16(3):247-257.,"Personalized medicine is being realized by our ability to measure biological and environmental information about patients. Much of these data are being stored in electronic health records yielding big data that presents challenges for its management and analysis. Here, we review several areas of knowledge that are necessary for next-generation scientists to fully realize the potential of biomedical big data. We begin with an overview of big data and its storage and management. We then review statistics and data science as foundational topics followed by a core curriculum of artificial intelligence, machine learning and natural language processing that are needed to develop predictive models for clinical decision making. We end with some specific training recommendations for preparing next-generation scientists for biomedical big data."
30759376,3.0,Potential biomarkers for persistent and neuropathic pain therapy,2019 Jul;199:16-29.,"Persistent, in particular neuropathic pain affects millions of people worldwide. However, the response rate of patients to existing analgesic drugs is less than 50%. There are several possibilities to increase this response rate, such as optimization of the pharmacokinetic and pharmacodynamic properties of analgesics. Another promising approach is to use prognostic biomarkers in patients to determine the optimal pharmacological therapy for each individual. Here, we discuss recent efforts to identify plasma and CSF biomarkers, as well as genetic biomarkers and sensory testing, and how these readouts could be exploited for the prediction of a suitable pharmacological treatment. Collectively, the information on single biomarkers may be stored in knowledge bases and processed by machine-learning and related artificial intelligence techniques, resulting in the optimal pharmacological treatment for individual pain patients. We highlight the potential for biomarker-based individualized pain therapies and discuss biomarker reliability and their utility in clinical practice, as well as limitations of this approach."
30759006,8.0,Improving Detection of Early Chronic Obstructive Pulmonary Disease,2018 Dec;15(Suppl 4):S243-S248.,"Despite being a major cause of morbidity and mortality, chronic obstructive pulmonary disease (COPD) is frequently undiagnosed. Yet the burden of disease among the undiagnosed is significant, as these individuals experience symptoms, exacerbations, and excess mortality compared to those without COPD. The U.S. Preventive Services Task Force recommends against routine screening of asymptomatic individuals with spirometry. Hence, case-finding approaches are needed. A recently developed instrument, the five-item COPD Assessment in Primary Care to Identify Undiagnosed Respiratory Disease and Exacerbation Risk questionnaire plus peak expiratory flow, demonstrates good sensitivity and specificity for distinguishing cases from control subjects and is being studied prospectively in primary care settings to determine its impact on patient outcomes. However, finding the undiagnosed is only half the battle. Mounting evidence suggests significant COPD-like respiratory burden among individuals without airflow obstruction. Many experience dyspnea, mucus production, and exacerbation events and have emphysema and airway abnormalities on computed tomographic (CT) imaging of the chest. However, it is still unclear how to best treat these individuals and which individuals go on to develop spirometric obstruction. These challenges underline the importance of defining what constitutes ""early disease."" A recently proposed definition characterizes early COPD as either: 1) airflow limitation, 2) compatible CT imaging abnormalities, or 3) accelerated forced expiratory volume in 1 second decline in persons younger than 50 years and with greater than a 10 pack-year smoking history. Although it is recognized that this definition does not encompass all individuals who will develop COPD, it is an attempt to identify a group of individuals with most rapid decline to better understand mechanisms of disease development and where disease-modifying interventions are most likely to be successful. Ultimately, leveraging tools such as chest CT imaging, the electronic medical record, and machine learning algorithms may aid in the identification of such individuals."
30748019,9.0,Assessment of Alcohol Use in the Natural Environment,2019 Apr;43(4):564-577.,"The current article critically reviews 3 methodological options for assessing drinking episodes in the natural environment. Ecological momentary assessment (EMA) typically involves using mobile devices to collect self-report data from participants in daily life. This technique is now widely used in alcohol research, but investigators have implemented diverse assessment strategies. This article focuses on ""high-resolution"" EMA protocols that oversample experiences and behaviors within individual drinking episodes. A number of approaches have been used to accomplish this, including using signaled follow-ups tied to drinking initiation, asking participants to log entries before and after individual drinks or drinking episodes, and delivering frequent signaled assessments during periods of the day when alcohol use is most common. Transdermal alcohol sensors (TAS) are devices that are worn continuously and are capable of detecting alcohol eliminated through the skin. These methods are appealing because they do not rely upon drinkers' self-report. Studies using TAS have been appearing with greater frequency over the past several years. New methods are making the use of TAS more tractable by permitting back-translation of transdermal alcohol concentration data to more familiar estimates of blood alcohol concentration or breath alcohol concentration. However, the current generation of devices can have problems with missing data and tend to be relatively insensitive to low-level drinking. An emerging area of research investigates the possibility of using mobile device data and machine learning to passively detect the user's drinking, with promising early findings. EMA, TAS, and sensor-based approaches are all valid, and tend to produce convergent information when used in conjunction with one another. Each has a unique profile of advantages, disadvantages, and threats to validity. Therefore, the nature of the underlying research question must dictate the method(s) investigators select."
30745864,2.0,Probabilistic Encoding Models for Multivariate Neural Data,2019 Jan 28;13:1.,"A key problem in systems neuroscience is to characterize how populations of neurons encode information in their patterns of activity. An understanding of the encoding process is essential both for gaining insight into the origins of perception and for the development of brain-computer interfaces. However, this characterization is complicated by the highly variable nature of neural responses, and thus usually requires probabilistic methods for analysis. Drawing on techniques from statistical modeling and machine learning, we review recent methods for extracting important variables that quantitatively describe how sensory information is encoded in neural activity. In particular, we discuss methods for estimating receptive fields, modeling neural population dynamics, and inferring low dimensional latent structure from a population of neurons, in the context of both electrophysiology and calcium imaging data."
30741241,2.0,Cancer Phenotype Development: A Literature Review,2019;257:468-472.,"EHR-based, computable phenotypes can be leveraged by healthcare organizations and researchers to improve the cohort identification process. The ability to identify patient cohorts using aspects of care and outcomes based on clinical characteristics or diagnostic conditions and/or risk factors presents opportunities to researchers targeting specific populations for drug development and disease interventions. The objective of this review was to summarize the literature describing the development and use of phenotypes for cohort identification of cancer patients. A survey of the literature indexed in PubMed was performed to identify studies using EHR-based phenotypes for use in cancer studies. Specific search criteria were formulated by leveraging a phenotype identification guideline developed by the Phenotypes, Data Standards, and Data Quality Core of the NIH Health Care Systems Research Collaboratory. The final set of articles was examined further to identify 1) the cancer of interest and 2) the different approaches used for phenotype development, validation and implementation. The articles reviewed were specific to breast cancer, colorectal cancer, ovarian cancer, and lung cancer. The approaches taken for phenotype development and validation varied slightly among the relevant publications. Four studies relied on chart review, three utilized machine learning techniques, one took an ontological approach, and one utilized natural language processing (NLP)."
30738835,9.0,The roles of supervised machine learning in systems neuroscience,2019 Apr;175:126-137.,"Over the last several years, the use of machine learning (ML) in neuroscience has been rapidly increasing. Here, we review ML's contributions, both realized and potential, across several areas of systems neuroscience. We describe four primary roles of ML within neuroscience: (1) creating solutions to engineering problems, (2) identifying predictive variables, (3) setting benchmarks for simple models of the brain, and (4) serving itself as a model for the brain. The breadth and ease of its applicability suggests that machine learning should be in the toolbox of most systems neuroscientists."
30738026,1.0,Virtual Reality: Beyond Visualization,2019 Mar 29;431(7):1315-1321.,"Virtual reality (VR) has recently become an affordable technology. A wide range of options are available to access this unique visualization medium, from simple cardboard inserts for smartphones to truly advanced headsets tracked by external sensors. While it is now possible for any research team to gain access to VR, we can still question what it brings to scientific research. Visualization and the ability to navigate complex three-dimensional data are undoubtedly a gateway to many scientific applications; however, we are convinced that data treatment and numerical simulations, especially those mixing interactions with data, human cognition, and automated algorithms will be the future of VR in scientific research. Moreover, VR might soon merit the same level of attention to imaging data as machine learning currently has. In this short perspective, we discuss approaches that employ VR in scientific research based on some concrete examples."
30736374,19.0,The Role of Movement Analysis in Diagnosing and Monitoring Neurodegenerative Conditions: Insights from Gait and Postural Control,2019 Feb 6;9(2):34.,"Quantifying gait and postural control adds valuable information that aids in understanding neurological conditions where motor symptoms predominate and cause considerable functional impairment. Disease-specific clinical scales exist; however, they are often susceptible to subjectivity, and can lack sensitivity when identifying subtle gait and postural impairments in prodromal cohorts and longitudinally to document disease progression. Numerous devices are available to objectively quantify a range of measurement outcomes pertaining to gait and postural control; however, efforts are required to standardise and harmonise approaches that are specific to the neurological condition and clinical assessment. Tools are urgently needed that address a number of unmet needs in neurological practice. Namely, these include timely and accurate diagnosis; disease stratification; risk prediction; tracking disease progression; and decision making for intervention optimisation and maximising therapeutic response (such as medication selection, disease staging, and targeted support). Using some recent examples of research across a range of relevant neurological conditions-including Parkinson's disease, ataxia, and dementia-we will illustrate evidence that supports progress against these unmet clinical needs. We summarise the novel 'big data' approaches that utilise data mining and machine learning techniques to improve disease classification and risk prediction, and conclude with recommendations for future direction."
30733322,4.0,Machine Learning in Nuclear Medicine: Part 1-Introduction,2019 Apr;60(4):451-458.,"This article, the first in a 2-part series, provides an introduction to machine learning (ML) in a nuclear medicine context. This part addresses the history of ML and describes common algorithms, with illustrations of when they can be helpful in nuclear medicine. Part 2 focuses on current contributions of ML to our field, addresses future expectations and limitations, and provides a critical appraisal of what ML can and cannot do."
30730601,11.0,Automated De Novo Drug Design: Are We Nearly There Yet?,2019 Aug 5;58(32):10792-10803.,"Medicinal chemistry and, in particular, drug design have often been perceived as more of an art than a science. The many unknowns of human disease and the sheer complexity of chemical space render decision making in medicinal chemistry exceptionally demanding. Computational models can assist the medicinal chemist in this endeavour. Provided here is an overview of recent examples of automated de novo molecular design, a discussion of the concepts and computational approaches involved, and the daring prediction of some of the possibilities and limitations of drug design using machine intelligence."
30728827,20.0,"Genomic Selection in Aquaculture: Application, Limitations and Opportunities With Special Reference to Marine Shrimp and Pearl Oysters",2019 Jan 23;9:693.,"Within aquaculture industries, selection based on genomic information (genomic selection) has the profound potential to change genetic improvement programs and production systems. Genomic selection exploits the use of realized genomic relationships among individuals and information from genome-wide markers in close linkage disequilibrium with genes of biological and economic importance. We discuss the technical advances, practical requirements, and commercial applications that have made genomic selection feasible in a range of aquaculture industries, with a particular focus on molluscs (pearl oysters, Pinctada maxima) and marine shrimp (Litopenaeus vannamei and Penaeus monodon). The use of low-cost genome sequencing has enabled cost-effective genotyping on a large scale and is of particular value for species without a reference genome or access to commercial genotyping arrays. We highlight the pitfalls and offer the solutions to the genotyping by sequencing approach and the building of appropriate genetic resources to undertake genomic selection from first-hand experience. We describe the potential to capture large-scale commercial phenotypes based on image analysis and artificial intelligence through machine learning, as inputs for calculation of genomic breeding values. The application of genomic selection over traditional aquatic breeding programs offers significant advantages through being able to accurately predict complex polygenic traits including disease resistance; increasing rates of genetic gain; minimizing inbreeding; and negating potential limiting effects of genotype by environment interactions. Further practical advantages of genomic selection through the use of large-scale communal mating and rearing systems are highlighted, as well as presenting rate-limiting steps that impact on attaining maximum benefits from adopting genomic selection. Genomic selection is now at the tipping point where commercial applications can be readily adopted and offer significant short- and long-term solutions to sustainable and profitable aquaculture industries."
30728787,3.0,Quantitative Electroencephalography in Guiding Treatment of Major Depression,2019 Jan 23;9:779.,"This paper reviews significant contributions to the evidence for the use of quantitative electroencephalography features as biomarkers of depression treatment and examines the potential of such technology to guide pharmacotherapy. Frequency band abnormalities such as alpha and theta band abnormalities have shown promise as have combinatorial measures such as cordance (a measure combining alpha and theta power) and the Antidepressant Treatment Response Index in predicting medication treatment response. Nevertheless, studies have been hampered by methodological problems and inconsistencies, and these approaches have ultimately failed to elicit any significant interest in actual clinical practice. More recent machine learning approaches such as the Psychiatric Encephalography Evaluation Registry (PEER) technology and other efforts analyze large datasets to develop variables that may best predict response rather than test a priori hypotheses. PEER is a technology that may go beyond predicting response to a particular antidepressant and help to guide pharmacotherapy."
30723470,22.0,"The Immune System Computes the State of the Body: Crowd Wisdom, Machine Learning, and Immune Cell Reference Repertoires Help Manage Inflammation",2019 Jan 22;10:10.,"Here, we outline an overview of the mammalian immune system that updates and extends the classical clonal selection paradigm. Rather than focusing on strict self-not-self discrimination, we propose that the system orchestrates variable inflammatory responses that maintain the body and its symbiosis with the microbiome while eliminating the threat from pathogenic infectious agents and from tumors. The paper makes four points: The immune system classifies healthy and pathologic states of the body-including both self and foreign elements-by deploying individual lymphocytes as cellular computing machines; immune cells transform input signals from the body into an output of specific immune reactions.Rather than independent clonal responses, groups of individually activated immune-system cells co-react in lymphoid organs to make collective decisions through a type of self-organizing swarm intelligence or crowd wisdom.Collective choices by swarms of immune cells, like those of schools of fish, are modified by relatively small numbers of individual regulators responding to shifting conditions-such collective inflammatory responses are dynamically responsive.Self-reactive autoantibody and T-cell receptor (TCR) repertoires shared by healthy individuals function in a biological version of experience-based supervised machine learning. Immune system decisions are primed by formative experience with training sets of self-antigens encountered during lymphocyte development; these initially trained T cell and B cell repertoires form a Wellness Profile that then guides immune responses to test sets of antigens encountered later. This experience-based machine learning strategy is analogous to that deployed by supervised machine-learning algorithms. We propose experiments to test these ideas. This overview of the immune system bears clinical implications for monitoring wellness and for treating autoimmune disease, cancer, and allograft reactions."
30720708,,Skin Sensitization Testing-What's Next?,2019 Feb 4;20(3):666.,"There is an increasing demand for alternative in vitro methods to replace animal testing, and, to succeed, new methods are required to be at least as accurate as existing in vivo tests. However, skin sensitization is a complex process requiring coordinated and tightly regulated interactions between a variety of cells and molecules. Consequently, there is considerable difficulty in reproducing this level of biological complexity in vitro, and as a result the development of non-animal methods has posed a major challenge. However, with the use of a relevant biological system, the high information content of whole genome expression, and comprehensive bioinformatics, assays for most complex biological processes can be achieved. We propose that the Genomic Allergen Rapid Detection (GARD™) assay, developed to create a holistic data-driven in vitro model with high informational content, could be such an example. Based on the genomic expression of a mature human dendritic cell line and state-of-the-art machine learning techniques, GARD™ can today accurately predict skin sensitizers and correctly categorize skin sensitizing potency. Consequently, by utilizing advanced processing tools in combination with high information genomic or proteomic data, we can take the next step toward alternative methods with the same predictive accuracy as today's in vivo methods-and beyond."
30718453,10.0,Neurodevelopmental heterogeneity and computational approaches for understanding autism,2019 Feb 4;9(1):63.,"In recent years, the emerging field of computational psychiatry has impelled the use of machine learning models as a means to further understand the pathogenesis of multiple clinical disorders. In this paper, we discuss how autism spectrum disorder (ASD) was and continues to be diagnosed in the context of its complex neurodevelopmental heterogeneity. We review machine learning approaches to streamline ASD's diagnostic methods, to discern similarities and differences from comorbid diagnoses, and to follow developmentally variable outcomes. Both supervised machine learning models for classification outcome and unsupervised approaches to identify new dimensions and subgroups are discussed. We provide an illustrative example of how computational analytic methods and a longitudinal design can improve our inferential ability to detect early dysfunctional behaviors that may or may not reach threshold levels for formal diagnoses. Specifically, an unsupervised machine learning approach of anomaly detection is used to illustrate how community samples may be utilized to investigate early autism risk, multidimensional features, and outcome variables. Because ASD symptoms and challenges are not static within individuals across development, computational approaches present a promising method to elucidate subgroups of etiological contributions to phenotype, alternative developmental courses, interactions with biomedical comorbidities, and to predict potential responses to therapeutic interventions."
30717624,3.0,Artificial Intelligence for the Otolaryngologist: A State of the Art Review,2019 Apr;160(4):603-611.,"Objective:                    To provide a state of the art review of artificial intelligence (AI), including its subfields of machine learning and natural language processing, as it applies to otolaryngology and to discuss current applications, future impact, and limitations of these technologies.              Data sources:                    PubMed and Medline search engines.              Review methods:                    A structured search of the current literature was performed (up to and including September 2018). Search terms related to topics of AI in otolaryngology were identified and queried to identify relevant articles.              Conclusions:                    AI is at the forefront of conversation in academic research and popular culture. In recent years, it has been touted for its potential to revolutionize health care delivery. Yet, to date, it has made few contributions to actual medical practice or patient care. Future adoption of AI technologies in otolaryngology practice may be hindered by misconceptions of what AI is and a fear that machine errors may compromise patient care. However, with potential clinical and economic benefits, it is vital for otolaryngologists to understand the principles and scope of AI.              Implications for practice:                    In the coming years, AI is likely to have a major impact on biomedical research and the practice of medicine. Otolaryngologists are key stakeholders in the development and clinical integration of meaningful AI technologies that will improve patient care. High-quality data collection is essential for the development of AI technologies, and otolaryngologists should seek opportunities to collaborate with data scientists to guide them toward the most impactful clinical questions."
30712598,16.0,Prediction of sepsis patients using machine learning approach: A meta-analysis,2019 Mar;170:1-9.,"Study objective:                    Sepsis is a common and major health crisis in hospitals globally. An innovative and feasible tool for predicting sepsis remains elusive. However, early and accurate prediction of sepsis could help physicians with proper treatments and minimize the diagnostic uncertainty. Machine learning models could help to identify potential clinical variables and provide higher performance than existing traditional low-performance models. We therefore performed a meta-analysis of observational studies to quantify the performance of a machine learning model to predict sepsis.              Methods:                    A comprehensive literature search was conducted through the electronic database (e.g. PubMed, Scopus, Google Scholar, EMBASE, etc.) between January 1, 2000, and March 1, 2018. All the studies published in English and reporting the sepsis prediction using machine learning algorithms were considered in this study. Two authors independently extracted valuable information from the included studies. Inclusion and exclusion of studies were based on the Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) guidelines.              Results:                    A total of 7 out of 135 studies met all of our inclusion criteria. For machine learning models, the pooled area under receiving operating curve (SAUROC) for predicting sepsis onset 3 to 4 h before, was 0.89 (95%CI: 0.86-0.92); sensitivity 0.81 (95%CI:0.80-0.81), and specificity 0.72 (95%CI:0.72-0.72) whereas the pooled SAUROC for SIRS, MEWS, and SOFA was 0.70, 0.50, and 0.78. Additionally, diagnostic odd ratio for machine learning, SIRS, MEWS, and SOFA was 15.17 (95%CI: 9.51-24.20), 3.23 (95%CI: 1.52-6.87), 31.99 (95% CI: 1.54-666.74), and 3.75(95%CI: 2.06-6.83).              Conclusion:                    Our study findings suggest that the machine learning approach had a better performance than the existing sepsis scoring systems in predicting sepsis."
30710543,20.0,Artificial Intelligence Transforms the Future of Health Care,2019 Jul;132(7):795-801.,"Life sciences researchers using artificial intelligence (AI) are under pressure to innovate faster than ever. Large, multilevel, and integrated data sets offer the promise of unlocking novel insights and accelerating breakthroughs. Although more data are available than ever, only a fraction is being curated, integrated, understood, and analyzed. AI focuses on how computers learn from data and mimic human thought processes. AI increases learning capacity and provides decision support system at scales that are transforming the future of health care. This article is a review of applications for machine learning in health care with a focus on clinical, translational, and public health applications with an overview of the important role of privacy, data sharing, and genetic information."
30709766,8.0,Thermozymes: Adaptive strategies and tools for their biotechnological applications,2019 Apr;278:372-382.,"In today's scenario of global climate change, there is a colossal demand for sustainable industrial processes and enzymes from thermophiles. Plausibly, thermozymes are an important toolkit, as they are known to be polyextremophilic in nature. Small genome size and diverse molecular conformational modifications have been implicated in devising adaptive strategies. Besides, the utilization of chemical technology and gene editing attributions according to mechanical necessities are the additional key factor for efficacious bioprocess development. Microbial thermozymes have been extensively used in waste management, biofuel, food, paper, detergent, medicinal and pharmaceutical industries. To understand the strength of enzymes at higher temperatures different models utilize X-ray structures of thermostable proteins, machine learning calculations, neural networks, but unified adaptive measures are yet to be totally comprehended. The present review provides a recent updates on thermozymes and various interdisciplinary applications including the aspects of thermophiles bioengineering utilizing synthetic biology and gene editing tools."
30706823,,Computational Approaches as Rational Decision Support Systems for Discovering Next-Generation Antitubercular Agents: Mini-Review,2019;15(5):369-383.,"Tuberculosis, malaria, dengue, chikungunya, leishmaniasis etc. are a large group of neglected tropical diseases that prevail in tropical and subtropical countries, affecting one billion people every year. Minimal funding and grants for research on these scientific problems challenge many researchers to find a different way to reduce the extensive time and cost involved in the drug discovery cycle of these problems. Computer-aided drug design techniques have already been proved successful in the discovery of new molecules rationally by reducing the time and cost involved in the development of drugs. In the current minireview, we are highlighting on the molecular modeling studies published during 2010-2018 for target specific antitubercular agents. This review includes the studies of Structure-Based (SB) and Ligand-Based (LB) modeling and those involving Machine Learning (ML) techniques against different antitubercular targets such as dihydrofolate reductase (DHFR), enoyl Acyl Carrier Protein (ACP) reductase (InhA), catalase-peroxidase (KatG), enzyme antigen 85C, protein tyrosine phosphatases (PtpA and PtpB), dUTPase, thioredoxin reductase (MtTrxR), etc. The information presented in this review will help the researchers to get acquainted with the recent progress in the modeling studies of antitubercular agents."
30706465,6.0,A contemporary review of machine learning in otolaryngology-head and neck surgery,2020 Jan;130(1):45-51.,"One of the key challenges with big data is leveraging the complex network of information to yield useful clinical insights. The confluence of massive amounts of health data and a desire to make inferences and insights on these data has produced a substantial amount of interest in machine-learning analytic methods. There has been a drastic increase in the otolaryngology literature volume describing novel applications of machine learning within the past 5 years. In this timely contemporary review, we provide an overview of popular machine-learning techniques, and review recent machine-learning applications in otolaryngology-head and neck surgery including neurotology, head and neck oncology, laryngology, and rhinology. Investigators have realized significant success in validated models with model sensitivities and specificities approaching 100%. Challenges remain in the implementation of machine-learning algorithms. This may be in part the unfamiliarity of these techniques to clinician leaders on the front lines of patient care. Spreading awareness and confidence in machine learning will follow with further validation and proof-of-value analyses that demonstrate model performance superiority over established methods. We are poised to see a greater influx of machine-learning applications to clinical problems in otolaryngology-head and neck surgery, and it is prudent for providers to understand the potential benefits and limitations of these technologies. Laryngoscope, 130:45-51, 2020."
30706370,,Meta-analysis of the moral brain: patterns of neural engagement assessed using multilevel kernel density analysis,2020 Apr;14(2):534-547.,"The neuroimaging literature in moral cognition has rapidly developed in the last decade with more than 200 publications on the topic. Neuroimaging based models generally agree that limbic regions work with medial prefrontal and temporal regions during moral processing to integrate emotional, social, and cognitive elements into decision-making. However, no quantitative work has been done examining neural response across types of moral cognition tasks. This paper uses Multilevel Kernel Density Analysis (MKDA) to conduct neuroimaging meta-analyses of the moral cognitive literature. MKDA replicated previous findings of the neural correlates of moral cognition: the left amygdala, medial prefrontal cortex, bilateral temporoparietal junction, and posterior cingulate. Random forest algorithms classified neural features as belonging to simple/utilitarian moral dilemmas, explicit/implicit moral tasks, and word/picture moral stimuli tasks; in combination with univariate contrast analyses, these results indicated a distinct pattern of processing for each of the members of these paradigm pairs. Overall, the results emphasize that the task selected for use in a moral cognition neuroimaging study is vital for the elicitation and interpretation of results. It also replicates and re-establishes the neural basis for moral processing, especially important in light of implementation errors in previous meta-analysis."
30696115,13.0,Large-Scale Assessment of Bioinformatics Tools for Lysine Succinylation Sites,2019 Jan 28;8(2):95.,"Lysine succinylation is a form of posttranslational modification of the proteins that play an essential functional role in every aspect of cell metabolism in both prokaryotes and eukaryotes. Aside from experimental identification of succinylation sites, there has been an intense effort geared towards the development of sequence-based prediction through machine learning, due to its promising and essential properties of being highly accurate, robust and cost-effective. In spite of these advantages, there are several problems that are in need of attention in the design and development of succinylation site predictors. Notwithstanding of many studies on the employment of machine learning approaches, few articles have examined this bioinformatics field in a systematic manner. Thus, we review the advancements regarding the current state-of-the-art prediction models, datasets, and online resources and illustrate the challenges and limitations to present a useful guideline for developing powerful succinylation site prediction tools."
30696086,29.0,Machine Learning and Integrative Analysis of Biomedical Big Data,2019 Jan 28;10(2):87.,"Recent developments in high-throughput technologies have accelerated the accumulation of massive amounts of omics data from multiple sources: genome, epigenome, transcriptome, proteome, metabolome, etc. Traditionally, data from each source (e.g., genome) is analyzed in isolation using statistical and machine learning (ML) methods. Integrative analysis of multi-omics and clinical data is key to new biomedical discoveries and advancements in precision medicine. However, data integration poses new computational challenges as well as exacerbates the ones associated with single-omics studies. Specialized computational approaches are required to effectively and efficiently perform integrative analysis of biomedical data acquired from diverse modalities. In this review, we discuss state-of-the-art ML-based approaches for tackling five specific computational challenges associated with integrative analysis: curse of dimensionality, data heterogeneity, missing data, class imbalance and scalability issues."
30690654,10.0,Artificial intelligence and machine learning for human reproduction and embryology presented at ASRM and ESHRE 2018,2019 Apr;36(4):591-600.,"Sixteen artificial intelligence (AI) and machine learning (ML) approaches were reported at the 2018 annual congresses of the American Society for Reproductive Biology (9) and European Society for Human Reproduction and Embryology (7). Nearly every aspect of patient care was investigated, including sperm morphology, sperm identification, identification of empty or oocyte containing follicles, predicting embryo cell stages, predicting blastocyst formation from oocytes, assessing human blastocyst quality, predicting live birth from blastocysts, improving embryo selection, and for developing optimal IVF stimulation protocols. This represents a substantial increase in reports over 2017, where just one abstract each was reported at ASRM (AI) and ESHRE (ML). Our analysis reveals wide variability in how AI and ML methods are described (from not at all or very generic to fully describing the architectural framework) and large variability on accepted dataset sizes (from just 3 patients with 16 follicles in the smallest dataset to 661,060 images of 11,898 human embryos in one of the largest). AI and ML are clearly burgeoning methodologies in human reproduction and embryology and would benefit from early application of reporting standards."
30689359,9.0,Meta-Analysis of Nanoparticle Cytotoxicity via Data-Mining the Literature,2019 Feb 26;13(2):1583-1594.,"Developing predictive modeling frameworks of potential cytotoxicity of engineered nanoparticles is critical for environmental and health risk analysis. The complexity and the heterogeneity of available data on potential risks of nanoparticles, in addition to interdependency of relevant influential attributes, makes it challenging to develop a generalization of nanoparticle toxicity behavior. Lack of systematic approaches to investigate these risks further adds uncertainties and variability to the body of literature and limits generalizability of existing studies. Here, we developed a rigorous approach for assembling published evidence on cytotoxicity of several organic and inorganic nanoparticles and unraveled hidden relationships that were not targeted in the original publications. We used a machine learning approach that employs decision trees together with feature selection algorithms ( e.g., Gain ratio) to analyze a set of published nanoparticle cytotoxicity sample data (2896 samples). The specific studies were selected because they specified nanoparticle-, cell-, and screening method-related attributes. The resultant decision-tree classifiers are sufficiently simple, accurate, and with high prediction power and should be widely applicable to a spectrum of nanoparticle cytotoxicity settings. Among several influential attributes, we show that the cytotoxicity of nanoparticles is primarily predicted from the nanoparticle material chemistry, followed by nanoparticle concentration and size, cell type, and cytotoxicity screening indicator. Overall, our study indicates that following rigorous and transparent methodological experimental approaches, in parallel to continuous addition to this data set developed using our approach, will offer higher predictive power and accuracy and uncover hidden relationships. Results obtained in this study help focus future studies to develop nanoparticles that are safe by design."
30686616,1.0,Reprint of: Mapping human brain lesions and their functional consequences,2019 Apr 15;190:4-13.,"Neuroscience has a long history of inferring brain function by examining the relationship between brain injury and subsequent behavioral impairments. The primary advantage of this method over correlative methods is that it can tell us if a certain brain region is necessary for a given cognitive function. In addition, lesion-based analyses provide unique insights into clinical deficits. In the last decade, statistical voxel-based lesion behavior mapping (VLBM) emerged as a powerful method for understanding the architecture of the human brain. This review illustrates how VLBM improves our knowledge of functional brain architecture, as well as how it is inherently limited by its mass-univariate approach. A wide array of recently developed methods appear to supplement traditional VLBM. This paper provides an overview of these new methods, including the use of specialized imaging modalities, the combination of structural imaging with normative connectome data, as well as multivariate analyses of structural imaging data. We see these new methods as complementing rather than replacing traditional VLBM, providing synergistic tools to answer related questions. Finally, we discuss the potential for these methods to become established in cognitive neuroscience and in clinical applications."
30686613,35.0,A gentle introduction to deep learning in medical image processing,2019 May;29(2):86-101.,"This paper tries to give a gentle introduction to deep learning in medical image processing, proceeding from theoretical foundations to applications. We first discuss general reasons for the popularity of deep learning, including several major breakthroughs in computer science. Next, we start reviewing the fundamental basics of the perceptron and neural networks, along with some fundamental theory that is often omitted. Doing so allows us to understand the reasons for the rise of deep learning in many application domains. Obviously medical image processing is one of these areas which has been largely affected by this rapid progress, in particular in image detection and recognition, image segmentation, image registration, and computer-aided diagnosis. There are also recent trends in physical simulation, modeling, and reconstruction that have led to astonishing results. Yet, some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results. These apparent weaknesses highlight current limitations of deep ()learning. However, we also briefly discuss promising approaches that might be able to resolve these problems in the future."
30686485,,Mapping the Delirium Literature Through Probabilistic Topic Modeling and Network Analysis: A Computational Scoping Review,Mar-Apr 2019;60(2):105-120.,"Background:                    Delirium is an acute confusional state, associated with morbidity and mortality in diverse medically-ill populations. Delirium is recognized, through both professional competencies and instructional materials, as a core topic in consultation psychiatry.              Objective:                    Conduct a computational scoping review of the delirium literature to identify the overall contours of this literature and evolution of the delirium literature over time.              Methods:                    Algorithmic analysis of all research articles on delirium indexed in MEDLINE between 1995 and 2015 using network analysis of citation Medical Subject Headings (MeSH) tags and probabilistic topic modeling of article abstracts.              Results:                    The delirium corpus included 3591 articles in 874 unique journals, of which 95 were primarily psychiatric. The annual delirium publication volume increased from 40 in 1995 to 420 in 2015 and grew as a proportion of total indexed publications from 8.9 to 38.6 per 100,000. The psychiatric journals published 720 of the delirium publications. Articles on treatment of delirium (806) outnumber articles on prevention of delirium (432). Abstract topic modeling and Medical Subject Headings graph community analysis identified similar genres in the delirium literature, including: delirium in geriatric, critically ill, palliative care, and postsurgical patients as well as diagnostic criteria or scales, and clinical risk factors. The genres identified by topic modeling and community analysis were distributed unevenly between psychiatric journals and nonpsychiatric journals.              Conclusion:                    The delirium literature is large and growing. Much of this growth is outside of psychiatric journals. Subtopics of the delirium literature can be algorithmically identified, and these subtopics are distributed unevenly across psychiatric journals."
30684706,6.0,"Machine Learning in Neuro-Oncology: Can Data Analysis from 5,346 Patients Change Decision Making Paradigms?",2019 Jan 23;S1878-8750(19)30141-X.,"Background:                    Machine learning (ML) is an application of artificial intelligence (AI) giving computer systems the ability to learn data, without being explicitly programmed. ML is currently successfully used for optical character recognition, spam filtering, and face recognition. The aim of this study is to review its current application in the field of neuro-oncology.              Methods:                    We conducted a systematic literature review on PubMed and Cochrane Database using a keyword search for the period January 30, 2000-March 31, 2018. Data were clustered for neuro-oncology scope of ML into three categories: patient outcome predictors, imaging analysis, and gene expression.              Results:                    Data from 5,346 patients in 29 studies has been used to develop ML based algorithms (MLBA) in neuro-oncology. MLBA were used to predict outcome in 2,483 patients with a sensitivity range of 78-98% and specificity range of 76-95%. In all studies, MLBA had higher accuracy than conventional ones. MLBA for image analysis showed accuracy diagnosing low grade versus high grade gliomas (HGG) ranging from 80 to 93% and 90% diagnosing HGG versus lymphoma. Seven studies used MLBA to analyze gene expression in neuro-oncology.              Conclusions:                    MLBA in neuro-oncology have shown to predict patients' outcome more accurately than conventional parameters in retrospective analysis. If their high diagnostic accuracy in imaging analysis and detection of somatic mutations is corroborated in prospective studies, tissue diagnosis or liquid biopsy might curtail. Finally, MLBA are promising to help guide targeted therapy, lead to personalized medicine, and open areas of study in the cancer cellular signaling system, not otherwise known."
30684090,12.0,Rheumatoid Arthritis: Atherosclerosis Imaging and Cardiovascular Risk Assessment Using Machine and Deep Learning-Based Tissue Characterization,2019 Jan 25;21(2):7.,"Purpose of the review:                    Rheumatoid arthritis (RA) is a chronic, autoimmune disease which may result in a higher risk of cardiovascular (CV) events and stroke. Tissue characterization and risk stratification of patients with rheumatoid arthritis are a challenging problem. Risk stratification of RA patients using traditional risk factor-based calculators either underestimates or overestimates the CV risk. Advancements in medical imaging have facilitated early and accurate CV risk stratification compared to conventional cardiovascular risk calculators.              Recent finding:                    In recent years, a link between carotid atherosclerosis and rheumatoid arthritis has been widely discussed by multiple studies. Imaging the carotid artery using 2-D ultrasound is a noninvasive, economic, and efficient imaging approach that provides an atherosclerotic plaque tissue-specific image. Such images can help to morphologically characterize the plaque type and accurately measure vital phenotypes such as media wall thickness and wall variability. Intelligence-based paradigms such as machine learning- and deep learning-based techniques not only automate the risk characterization process but also provide an accurate CV risk stratification for better management of RA patients. This review provides a brief understanding of the pathogenesis of RA and its association with carotid atherosclerosis imaged using the B-mode ultrasound technique. Lacunas in traditional risk scores and the role of machine learning-based tissue characterization algorithms are discussed and could facilitate cardiovascular risk assessment in RA patients. The key takeaway points from this review are the following: (i) inflammation is a common link between RA and atherosclerotic plaque buildup, (ii) carotid ultrasound is a better choice to characterize the atherosclerotic plaque tissues in RA patients, and (iii) intelligence-based paradigms are useful for accurate tissue characterization and risk stratification of RA patients."
30682710,20.0,Deep learning in spiking neural networks,2019 Mar;111:47-63.,"In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data."
30677473,13.0,"Novel genetic and epigenetic factors of importance for inter-individual differences in drug disposition, response and toxicity",2019 May;197:122-152.,"Individuals differ substantially in their response to pharmacological treatment. Personalized medicine aspires to embrace these inter-individual differences and customize therapy by taking a wealth of patient-specific data into account. Pharmacogenomic constitutes a cornerstone of personalized medicine that provides therapeutic guidance based on the genomic profile of a given patient. Pharmacogenomics already has applications in the clinics, particularly in oncology, whereas future development in this area is needed in order to establish pharmacogenomic biomarkers as useful clinical tools. In this review we present an updated overview of current and emerging pharmacogenomic biomarkers in different therapeutic areas and critically discuss their potential to transform clinical care. Furthermore, we discuss opportunities of technological, methodological and institutional advances to improve biomarker discovery. We also summarize recent progress in our understanding of epigenetic effects on drug disposition and response, including a discussion of the only few pharmacogenomic biomarkers implemented into routine care. We anticipate, in part due to exciting rapid developments in Next Generation Sequencing technologies, machine learning methods and national biobanks, that the field will make great advances in the upcoming years towards unlocking the full potential of genomic data."
30676989,7.0,Deep Learning: Current and Emerging Applications in Medicine and Technology,2019 May;23(3):906-920.,"Machine learning is enabling researchers to analyze and understand increasingly complex physical and biological phenomena in traditional fields such as biology, medicine, and engineering and emerging fields like synthetic biology, automated chemical synthesis, and biomanufacturing. These fields require new paradigms toward understanding increasingly complex data and converting such data into medical products and services for patients. The move toward deep learning and complex modeling is an attempt to bridge the gap between acquiring massive quantities of complex data, and converting such data into practical insights. Here, we provide an overview of the field of machine learning, its current applications and needs in traditional and emerging fields, and discuss an illustrative attempt at using deep learning to understand swarm behavior of molecular shuttles."
30674262,3.0,Recent Progress in Machine Learning-based Prediction of Peptide Activity for Drug Discovery,2019;19(1):4-16.,"Over the past decades, peptide as a therapeutic candidate has received increasing attention in drug discovery, especially for antimicrobial peptides (AMPs), anticancer peptides (ACPs) and antiinflammatory peptides (AIPs). It is considered that the peptides can regulate various complex diseases which are previously untouchable. In recent years, the critical problem of antimicrobial resistance drives the pharmaceutical industry to look for new therapeutic agents. Compared to organic small drugs, peptide- based therapy exhibits high specificity and minimal toxicity. Thus, peptides are widely recruited in the design and discovery of new potent drugs. Currently, large-scale screening of peptide activity with traditional approaches is costly, time-consuming and labor-intensive. Hence, in silico methods, mainly machine learning approaches, for their accuracy and effectiveness, have been introduced to predict the peptide activity. In this review, we document the recent progress in machine learning-based prediction of peptides which will be of great benefit to the discovery of potential active AMPs, ACPs and AIPs."
30671672,12.0,"Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges and future perspectives",2019 Feb;138(2):109-124.,"In the field of cancer genomics, the broad availability of genetic information offered by next-generation sequencing technologies and rapid growth in biomedical publication has led to the advent of the big-data era. Integration of artificial intelligence (AI) approaches such as machine learning, deep learning, and natural language processing (NLP) to tackle the challenges of scalability and high dimensionality of data and to transform big data into clinically actionable knowledge is expanding and becoming the foundation of precision medicine. In this paper, we review the current status and future directions of AI application in cancer genomics within the context of workflows to integrate genomic analysis for precision cancer care. The existing solutions of AI and their limitations in cancer genetic testing and diagnostics such as variant calling and interpretation are critically analyzed. Publicly available tools or algorithms for key NLP technologies in the literature mining for evidence-based clinical recommendations are reviewed and compared. In addition, the present paper highlights the challenges to AI adoption in digital healthcare with regard to data requirements, algorithmic transparency, reproducibility, and real-world assessment, and discusses the importance of preparing patients and physicians for modern digitized healthcare. We believe that AI will remain the main driver to healthcare transformation toward precision medicine, yet the unprecedented challenges posed should be addressed to ensure safety and beneficial impact to healthcare."
30670469,6.0,Recording and Decoding of Vagal Neural Signals Related to Changes in Physiological Parameters and Biomarkers of Disease,2019 Dec 2;9(12):a034157.,"Our bodies have built-in neural reflexes that continuously monitor organ function and maintain physiological homeostasis. Whereas the field of bioelectronic medicine has mainly focused on the stimulation of neural circuits to treat various conditions, recent studies have started to investigate the possibility of leveraging the sensory arm of these reflexes to diagnose disease states. To accomplish this, neural signals emanating from the body's built-in biosensors and propagating through peripheral nerves must be recorded and decoded to identify the presence or levels of relevant biomarkers of disease. The process of acquiring these signals poses several technical challenges related to the neural interfaces, surgical techniques, and data-processing framework needed to record and analyze them. However, these challenges can be addressed with a rigorous experimental approach and new advances in implantable electrodes, signal processing, and machine learning methods. Outlined in this review are studies decoding vagus nerve activity as it related to inflammatory, metabolic, and cardiopulmonary biomarkers. Successfully decoding peripheral nerve activity related to disease states will not only enable the development of real-time diagnostic devices, but also help advancing truly closed-loop neuromodulation technologies."
30669406,17.0,A Review on a Deep Learning Perspective in Brain Cancer Classification,2019 Jan 18;11(1):111.,"A World Health Organization (WHO) Feb 2018 report has recently shown that mortality rate due to brain or central nervous system (CNS) cancer is the highest in the Asian continent. It is of critical importance that cancer be detected earlier so that many of these lives can be saved. Cancer grading is an important aspect for targeted therapy. As cancer diagnosis is highly invasive, time consuming and expensive, there is an immediate requirement to develop a non-invasive, cost-effective and efficient tools for brain cancer characterization and grade estimation. Brain scans using magnetic resonance imaging (MRI), computed tomography (CT), as well as other imaging modalities, are fast and safer methods for tumor detection. In this paper, we tried to summarize the pathophysiology of brain cancer, imaging modalities of brain cancer and automatic computer assisted methods for brain cancer characterization in a machine and deep learning paradigm. Another objective of this paper is to find the current issues in existing engineering methods and also project a future paradigm. Further, we have highlighted the relationship between brain cancer and other brain disorders like stroke, Alzheimer's, Parkinson's, and Wilson's disease, leukoriaosis, and other neurological disorders in the context of machine learning and the deep learning paradigm."
30667309,8.0,New Frontiers: An Update on Computer-Aided Diagnosis for Breast Imaging in the Age of Artificial Intelligence,2019 Feb;212(2):300-307.,"Objective:                    The purpose of this article is to compare traditional versus machine learning-based computer-aided detection (CAD) platforms in breast imaging with a focus on mammography, to underscore limitations of traditional CAD, and to highlight potential solutions in new CAD systems under development for the future.              Conclusion:                    CAD development for breast imaging is undergoing a paradigm shift based on vast improvement of computing power and rapid emergence of advanced deep learning algorithms, heralding new systems that may hold real potential to improve clinical care."
30664391,8.0,Post-LASIK Ectasia: Twenty Years of a Conundrum,2019;34(2):66-68.,"Corneal ectasia has emerged as a serious complication of laser vision correction (LVC) procedures since the first report by Seiler in 1998. Thereby, its prevention has become a major concern for refractive surgeons. Ectasia occurs due to biomechanical decompensation of the stroma, which may be related to a severe impact on corneal structure (i.e., attempted treatment for high myopia) or the altered biomechanical properties preoperatively. The current understanding is that a combination from those factors determines stability or ectasia progression after LVC. Abnormal corneal topography has been the most important surrogate for lower biomechanical properties, but novel imaging technologies such as tomography and biomechanical assessment have proven to enhance the ability for detecting mild ectatic disease, such as in the eyes with normal topography from patients with clinical ectasia in the fellow eye. Bohac and associates in a retrospective case series analyzed data from 30,167 eyes from 16,732 documented ten eyes (0.033%) of seven patients that developed post-LASIK ectasia. This data supports the concept that the actual incidence of ectasia has decreased from 0.66% reported by Pallikaris in 2001. This has been the result of major development related to the advanced screening strategies. Nevertheless, mysterious cases of ectasia still challenge the field and stimulated research in this field. Ocular allergy and eye rubbing may be a factor that triggered ectasia in such series. Artificial intelligence (AI) and machine-learning algorithms may play a definitive role for further enhancing ectasia risk assessment. Reporting ectasia after LVC is needed."
30664063,5.0,Measurement Variability in Treatment Response Determination for Non-Small Cell Lung Cancer: Improvements Using Radiomics,2019 Mar;34(2):103-115.,"Multimodality imaging measurements of treatment response are critical for clinical practice, oncology trials, and the evaluation of new treatment modalities. The current standard for determining treatment response in non-small cell lung cancer (NSCLC) is based on tumor size using the RECIST criteria. Molecular targeted agents and immunotherapies often cause morphological change without reduction of tumor size. Therefore, it is difficult to evaluate therapeutic response by conventional methods. Radiomics is the study of cancer imaging features that are extracted using machine learning and other semantic features. This method can provide comprehensive information on tumor phenotypes and can be used to assess therapeutic response in this new age of immunotherapy. Delta radiomics, which evaluates the longitudinal changes in radiomics features, shows potential in gauging treatment response in NSCLC. It is well known that quantitative measurement methods may be subject to substantial variability due to differences in technical factors and require standardization. In this review, we describe measurement variability in the evaluation of NSCLC and the emerging role of radiomics."
30659282,40.0,Spatial proteomics: a powerful discovery tool for cell biology,2019 May;20(5):285-302.,"Protein subcellular localization is tightly controlled and intimately linked to protein function in health and disease. Capturing the spatial proteome - that is, the localizations of proteins and their dynamics at the subcellular level - is therefore essential for a complete understanding of cell biology. Owing to substantial advances in microscopy, mass spectrometry and machine learning applications for data analysis, the field is now mature for proteome-wide investigations of spatial cellular regulation. Studies of the human proteome have begun to reveal a complex architecture, including single-cell variations, dynamic protein translocations, changing interaction networks and proteins localizing to multiple compartments. Furthermore, several studies have successfully harnessed the power of comparative spatial proteomics as a discovery tool to unravel disease mechanisms. We are at the beginning of an era in which spatial proteomics finally integrates with cell biology and medical research, thereby paving the way for unbiased systems-level insights into cellular processes. Here, we discuss current methods for spatial proteomics using imaging or mass spectrometry and specifically highlight global comparative applications. The aim of this Review is to survey the state of the field and also to encourage more cell biologists to apply spatial proteomics approaches."
30657889,21.0,A comprehensive review and comparison of existing computational methods for intrinsically disordered protein and region prediction,2019 Jan 18;20(1):330-346.,"Intrinsically disordered proteins and regions are widely distributed in proteins, which are associated with many biological processes and diseases. Accurate prediction of intrinsically disordered proteins and regions is critical for both basic research (such as protein structure and function prediction) and practical applications (such as drug development). During the past decades, many computational approaches have been proposed, which have greatly facilitated the development of this important field. Therefore, a comprehensive and updated review is highly required. In this regard, we give a review on the computational methods for intrinsically disordered protein and region prediction, especially focusing on the recent development in this field. These computational approaches are divided into four categories based on their methodologies, including physicochemical-based method, machine-learning-based method, template-based method and meta method. Furthermore, their advantages and disadvantages are also discussed. The performance of 40 state-of-the-art predictors is directly compared on the target proteins in the task of disordered region prediction in the 10th Critical Assessment of protein Structure Prediction. A more comprehensive performance comparison of 45 different predictors is conducted based on seven widely used benchmark data sets. Finally, some open problems and perspectives are discussed."
30657038,,Recent Advances on Prediction of Human Papillomaviruses Risk Types,2019;20(3):236-243.,"Background:                    Some studies have shown that Human Papillomavirus (HPV) is strongly associated with cervical cancer. As we all know, cervical cancer still remains the fourth most common cancer, affecting women worldwide. Thus, it is both challenging and essential to detect risk types of human papillomaviruses.              Methods:                    In order to discriminate whether HPV type is highly risky or not, many epidemiological and experimental methods have been proposed recently. For HPV risk type prediction, there also have been a few computational studies which are all based on Machine Learning (ML) techniques, but adopt different feature extraction methods. Therefore, we conclude and discuss several classical approaches which have got a better result for the risk type prediction of HPV.              Results:                    This review summarizes the common methods to detect human papillomavirus. The main methods are sequence- derived features, text-based classification, gap-kernel method, ensemble SVM, Word statistical model, position- specific statistical model and mismatch kernel method (SVM). Among these methods, position-specific statistical model get a relatively high accuracy rate (accuracy=97.18%). Word statistical model is also a novel approach, which extracted the information of HPV from the protein ""sequence space"" with word statistical model to predict high-risk types of HPVs (accuracy=95.59%). These methods could potentially be used to improve prediction of highrisk types of HPVs.              Conclusion:                    From the prediction accuracy, we get that the classification results are more accurate by establishing mathematical models. Thus, adopting mathematical methods to predict risk type of HPV will be the main goal of research in the future."
30654138,8.0,An extensive experimental survey of regression methods,2019 Mar;111:11-34.,"Regression is a very relevant problem in machine learning, with many different available approaches. The current work presents a comparison of a large collection composed by 77 popular regression models which belong to 19 families: linear and generalized linear models, generalized additive models, least squares, projection methods, LASSO and ridge regression, Bayesian models, Gaussian processes, quantile regression, nearest neighbors, regression trees and rules, random forests, bagging and boosting, neural networks, deep learning and support vector regression. These methods are evaluated using all the regression datasets of the UCI machine learning repository (83 datasets), with some exceptions due to technical reasons. The experimental work identifies several outstanding regression models: the M5 rule-based model with corrections based on nearest neighbors (cubist), the gradient boosted machine (gbm), the boosting ensemble of regression trees (bstTree) and the M5 regression tree. Cubist achieves the best squared correlation ( R2) in 15.7% of datasets being very near to it, with difference below 0.2 for 89.1% of datasets, and the median of these differences over the dataset collection is very low (0.0192), compared e.g. to the classical linear regression (0.150). However, cubist is slow and fails in several large datasets, while other similar regression models as M5 never fail and its difference to the best R2 is below 0.2 for 92.8% of datasets. Other well-performing regression models are the committee of neural networks (avNNet), extremely randomized regression trees (extraTrees, which achieves the best R2 in 33.7% of datasets), random forest (rf) and ε-support vector regression (svr), but they are slower and fail in several datasets. The fastest regression model is least angle regression lars, which is 70 and 2,115 times faster than M5 and cubist, respectively. The model which requires least memory is non-negative least squares (nnls), about 2 GB, similarly to cubist, while M5 requires about 8 GB. For 97.6% of datasets there is a regression model among the 10 bests which is very near (difference below 0.1) to the best R2, which increases to 100% allowing differences of 0.2. Therefore, provided that our dataset and model collection are representative enough, the main conclusion of this study is that, for a new regression problem, some model in our top-10 should achieve R2 near to the best attainable for that problem."
