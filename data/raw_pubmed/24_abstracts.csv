pmid,citations,title,date,text
29695070,16.0,The Novel Roles of Connexin Channels and Tunneling Nanotubes in Cancer Pathogenesis,2018 Apr 24;19(5):1270.,"Neoplastic growth and cellular differentiation are critical hallmarks of tumor development. It is well established that cell-to-cell communication between tumor cells and ""normal"" surrounding cells regulates tumor differentiation and proliferation, aggressiveness, and resistance to treatment. Nevertheless, the mechanisms that result in tumor growth and spread as well as the adaptation of healthy surrounding cells to the tumor environment are poorly understood. A major component of these communication systems is composed of connexin (Cx)-containing channels including gap junctions (GJs), tunneling nanotubes (TNTs), and hemichannels (HCs). There are hundreds of reports about the role of Cx-containing channels in the pathogenesis of cancer, and most of them demonstrate a downregulation of these proteins. Nonetheless, new data demonstrate that a localized communication via Cx-containing GJs, HCs, and TNTs plays a key role in tumor growth, differentiation, and resistance to therapies. Moreover, the type and downstream effects of signals communicated between the different populations of tumor cells are still unknown. However, new approaches such as artificial intelligence (AI) and machine learning (ML) could provide new insights into these signals communicated between connected cells. We propose that the identification and characterization of these new communication systems and their associated signaling could provide new targets to prevent or reduce the devastating consequences of cancer."
29687000,21.0,Machine Learning in Ultrasound Computer-Aided Diagnostic Systems: A Survey,2018 Mar 4;2018:5137904.,"The ultrasound imaging is one of the most common schemes to detect diseases in the clinical practice. There are many advantages of ultrasound imaging such as safety, convenience, and low cost. However, reading ultrasound imaging is not easy. To support the diagnosis of clinicians and reduce the load of doctors, many ultrasound computer-aided diagnosis (CAD) systems are proposed. In recent years, the success of deep learning in the image classification and segmentation led to more and more scholars realizing the potential of performance improvement brought by utilizing the deep learning in the ultrasound CAD system. This paper summarized the research which focuses on the ultrasound CAD system utilizing machine learning technology in recent years. This study divided the ultrasound CAD system into two categories. One is the traditional ultrasound CAD system which employed the manmade feature and the other is the deep learning ultrasound CAD system. The major feature and the classifier employed by the traditional ultrasound CAD system are introduced. As for the deep learning ultrasound CAD, newest applications are summarized. This paper will be useful for researchers who focus on the ultrasound CAD system."
29681257,5.0,Implementation of novel statistical procedures and other advanced approaches to improve analysis of CASA data,2018 Jun;30(6):860-866.,"Computer-aided sperm analysis (CASA) produces a wealth of data that is frequently ignored. The use of multiparametric statistical methods can help explore these datasets, unveiling the subpopulation structure of sperm samples. In this review we analyse the significance of the internal heterogeneity of sperm samples and its relevance. We also provide a brief description of the statistical tools used for extracting sperm subpopulations from the datasets, namely unsupervised clustering (with non-hierarchical, hierarchical and two-step methods) and the most advanced supervised methods, based on machine learning. The former method has allowed exploration of subpopulation patterns in many species, whereas the latter offering further possibilities, especially considering functional studies and the practical use of subpopulation analysis. We also consider novel approaches, such as the use of geometric morphometrics or imaging flow cytometry. Finally, although the data provided by CASA systems provides valuable information on sperm samples by applying clustering analyses, there are several caveats. Protocols for capturing and analysing motility or morphometry should be standardised and adapted to each experiment, and the algorithms should be open in order to allow comparison of results between laboratories. Moreover, we must be aware of new technology that could change the paradigm for studying sperm motility and morphology."
29679305,2.0,"The New Possibilities from ""Big Data"" to Overlooked Associations Between Diabetes, Biochemical Parameters, Glucose Control, and Osteoporosis",2018 Jun;16(3):320-324.,"Purpose of review:                    To review current practices and technologies within the scope of ""Big Data"" that can further our understanding of diabetes mellitus and osteoporosis from large volumes of data. ""Big Data"" techniques involving supervised machine learning, unsupervised machine learning, and deep learning image analysis are presented with examples of current literature.              Recent findings:                    Supervised machine learning can allow us to better predict diabetes-induced osteoporosis and understand relative predictor importance of diabetes-affected bone tissue. Unsupervised machine learning can allow us to understand patterns in data between diabetic pathophysiology and altered bone metabolism. Image analysis using deep learning can allow us to be less dependent on surrogate predictors and use large volumes of images to classify diabetes-induced osteoporosis and predict future outcomes directly from images. ""Big Data"" techniques herald new possibilities to understand diabetes-induced osteoporosis and ascertain our current ability to classify, understand, and predict this condition."
29676964,12.0,Advanced Imaging Techniques in Evaluation of Colorectal Cancer,May-Jun 2018;38(3):740-765.,"Imaging techniques are clinical decision-making tools in the evaluation of patients with colorectal cancer (CRC). The aim of this article is to discuss the potential of recent advances in imaging for diagnosis, prognosis, therapy planning, and assessment of response to treatment of CRC. Recent developments and new clinical applications of conventional imaging techniques such as virtual colonoscopy, dual-energy spectral computed tomography, elastography, advanced computing techniques (including volumetric rendering techniques and machine learning), magnetic resonance (MR) imaging-based magnetization transfer, and new liver imaging techniques, which may offer additional clinical information in patients with CRC, are summarized. In addition, the clinical value of functional and molecular imaging techniques such as diffusion-weighted MR imaging, dynamic contrast material-enhanced imaging, blood oxygen level-dependent imaging, lymphography with contrast agents, positron emission tomography with different radiotracers, and MR spectroscopy is reviewed, and the advantages and disadvantages of these modalities are evaluated. Finally, the future role of imaging-based analysis of tumor heterogeneity and multiparametric imaging, the development of radiomics and radiogenomics, and future challenges for imaging of patients with CRC are discussed. Online supplemental material is available for this article. Â©RSNA, 2018."
29672663,31.0,Analysis of long noncoding RNAs highlights region-specific altered expression patterns and diagnostic roles in Alzheimer's disease,2019 Mar 25;20(2):598-608.,"Increasing evidence has revealed the multiple roles of long noncoding RNAs (lncRNAs) in neurodevelopment, brain function and aging, and their dysregulation was implicated in many types of neurological diseases. However, expression pattern and diagnostic role of lncRNAs in Alzheimer's disease (AD) remain largely unknown and has gained significant attention. In this study, we performed a comparative analysis for lncRNA expression profiles in four brain regions in brain aging and AD. Our analysis revealed age- and disease-dependent region-specific lncRNA expression patterns in aging and AD. Moreover, we identified a panel of nine lncRNAs (termed LncSigAD9) in a discovery cohort of 114 samples using supervised machine learning and stepwise selection method. The LncSigAD9 was able to differentiate between AD and healthy controls with high diagnostic sensitivity and specificity both in the discovery cohort (86.3 and 89.5%) and the additional independent AD cohort (90.8 and 83.8%). The receiver operating characteristic curves for the LncSigAD9 were 0.863 and 0.939 for discovery and independent cohorts, respectively. Furthermore, the LncSigAD9 demonstrated higher diagnostic performance than nine-minus-one lncRNA signature and mRNA-based signature with a similar number of genes. In silico functional analysis indicated the involvement of lncRNA expression variation in brain development- and metabolism-related biological processes. Taken together, our study highlights the importance of lncRNAs in brain aging and AD, and demonstrated the utility of lncRNAs as a promising biomarker for early AD diagnosis and treatment."
29670029,7.0,Concepts in Light Microscopy of Viruses,2018 Apr 18;10(4):202.,"Viruses threaten humans, livestock, and plants, and are difficult to combat. Imaging of viruses by light microscopy is key to uncover the nature of known and emerging viruses in the quest for finding new ways to treat viral disease and deepening the understanding of virus&ndash;host interactions. Here, we provide an overview of recent technology for imaging cells and viruses by light microscopy, in particular fluorescence microscopy in static and live-cell modes. The review lays out guidelines for how novel fluorescent chemical probes and proteins can be used in light microscopy to illuminate cells, and how they can be used to study virus infections. We discuss advantages and opportunities of confocal and multi-photon microscopy, selective plane illumination microscopy, and super-resolution microscopy. We emphasize the prevalent concepts in image processing and data analyses, and provide an outlook into label-free digital holographic microscopy for virus research."
29662559,13.0,Deep Learning in Nuclear Medicine and Molecular Imaging: Current Perspectives and Future Directions,2018 Apr;52(2):109-118.,"Recent advances in deep learning have impacted various scientific and industrial fields. Due to the rapid application of deep learning in biomedical data, molecular imaging has also started to adopt this technique. In this regard, it is expected that deep learning will potentially affect the roles of molecular imaging experts as well as clinical decision making. This review firstly offers a basic overview of deep learning particularly for image data analysis to give knowledge to nuclear medicine physicians and researchers. Because of the unique characteristics and distinctive aims of various types of molecular imaging, deep learning applications can be different from other fields. In this context, the review deals with current perspectives of deep learning in molecular imaging particularly in terms of development of biomarkers. Finally, future challenges of deep learning application for molecular imaging and future roles of experts in molecular imaging will be discussed."
29659698,10.0,Molecular subtyping of cancer: current status and moving toward clinical applications,2019 Mar 25;20(2):572-584.,"Cancer is a collection of genetic diseases, with large phenotypic differences and genetic heterogeneity between different types of cancers and even within the same cancer type. Recent advances in genome-wide profiling provide an opportunity to investigate global molecular changes during the development and progression of cancer. Meanwhile, numerous statistical and machine learning algorithms have been designed for the processing and interpretation of high-throughput molecular data. Molecular subtyping studies have allowed the allocation of cancer into homogeneous groups that are considered to harbor similar molecular and clinical characteristics. Furthermore, this has helped researchers to identify both actionable targets for drug design as well as biomarkers for response prediction. In this review, we introduce five frequently applied techniques for generating molecular data, which are microarray, RNA sequencing, quantitative polymerase chain reaction, NanoString and tissue microarray. Commonly used molecular data for cancer subtyping and clinical applications are discussed. Next, we summarize a workflow for molecular subtyping of cancer, including data preprocessing, cluster analysis, supervised classification and subtype characterizations. Finally, we identify and describe four major challenges in the molecular subtyping of cancer that may preclude clinical implementation. We suggest that standardized methods should be established to help identify intrinsic subgroup signatures and build robust classifiers that pave the way toward stratified treatment of cancer patients."
29656681,4.0,Progress with modeling activity landscapes in drug discovery,2018 Jul;13(7):605-615.,"Activity landscapes (ALs) are representations and models of compound data sets annotated with a target-specific activity. In contrast to quantitative structure-activity relationship (QSAR) models, ALs aim at characterizing structure-activity relationships (SARs) on a large-scale level encompassing all active compounds for specific targets. The popularity of AL modeling has grown substantially with the public availability of large activity-annotated compound data sets. AL modeling crucially depends on molecular representations and similarity metrics used to assess structural similarity. Areas covered: The concepts of AL modeling are introduced and its basis in quantitatively assessing molecular similarity is discussed. The different types of AL modeling approaches are introduced. AL designs can broadly be divided into three categories: compound-pair based, dimensionality reduction, and network approaches. Recent developments for each of these categories are discussed focusing on the application of mathematical, statistical, and machine learning tools for AL modeling. AL modeling using chemical space networks is covered in more detail. Expert opinion: AL modeling has remained a largely descriptive approach for the analysis of SARs. Beyond mere visualization, the application of analytical tools from statistics, machine learning and network theory has aided in the sophistication of AL designs and provides a step forward in transforming ALs from descriptive to predictive tools. To this end, optimizing representations that encode activity relevant features of molecules might prove to be a crucial step."
29655580,41.0,Canadian Association of Radiologists White Paper on Artificial Intelligence in Radiology,2018 May;69(2):120-135.,"Artificial intelligence (AI) is rapidly moving from an experimental phase to an implementation phase in many fields, including medicine. The combination of improved availability of large datasets, increasing computing power, and advances in learning algorithms has created major performance breakthroughs in the development of AI applications. In the last 5 years, AI techniques known as deep learning have delivered rapidly improving performance in image recognition, caption generation, and speech recognition. Radiology, in particular, is a prime candidate for early adoption of these techniques. It is anticipated that the implementation of AI in radiology over the next decade will significantly improve the quality, value, and depth of radiology's contribution to patient care and population health, and will revolutionize radiologists' workflows. The Canadian Association of Radiologists (CAR) is the national voice of radiology committed to promoting the highest standards in patient-centered imaging, lifelong learning, and research. The CAR has created an AI working group with the mandate to discuss and deliberate on practice, policy, and patient care issues related to the introduction and implementation of AI in imaging. This white paper provides recommendations for the CAR derived from deliberations between members of the AI working group. This white paper on AI in radiology will inform CAR members and policymakers on key terminology, educational needs of members, research and development, partnerships, potential clinical applications, implementation, structure and governance, role of radiologists, and potential impact of AI on radiology in Canada."
29648622,11.0,Deep learning of genomic variation and regulatory network data,2018 May 1;27(R1):R63-R71.,"The human genome is now investigated through high-throughput functional assays, and through the generation of population genomic data. These advances support the identification of functional genetic variants and the prediction of traits (e.g. deleterious variants and disease). This review summarizes lessons learned from the large-scale analyses of genome and exome data sets, modeling of population data and machine-learning strategies to solve complex genomic sequence regions. The review also portrays the rapid adoption of artificial intelligence/deep neural networks in genomics; in particular, deep learning approaches are well suited to model the complex dependencies in the regulatory landscape of the genome, and to provide predictors for genetic variant calling and interpretation."
29637384,12.0,Stacked generalization: an introduction to super learning,2018 May;33(5):459-464.,"Stacked generalization is an ensemble method that allows researchers to combine several different prediction algorithms into one. Since its introduction in the early 1990s, the method has evolved several times into a host of methods among which is the ""Super Learner"". Super Learner uses V-fold cross-validation to build the optimal weighted combination of predictions from a library of candidate algorithms. Optimality is defined by a user-specified objective function, such as minimizing mean squared error or maximizing the area under the receiver operating characteristic curve. Although relatively simple in nature, use of Super Learner by epidemiologists has been hampered by limitations in understanding conceptual and technical details. We work step-by-step through two examples to illustrate concepts and address common concerns."
29629796,7.0,Advances in Pancreatic CT Imaging,2018 Jul;211(1):52-66.,"Objective:                    The purpose of this article is to discuss the advances in CT acquisition and image postprocessing as they apply to imaging the pancreas and to conceptualize the role of radiogenomics and machine learning in pancreatic imaging.              Conclusion:                    CT is the preferred imaging modality for assessment of pancreatic diseases. Recent advances in CT (dual-energy CT, CT perfusion, CT volumetry, and radiogenomics) and emerging computational algorithms (machine learning) have the potential to further increase the value of CT in pancreatic imaging."
29626206,17.0,High-throughput mouse phenomics for characterizing mammalian gene function,2018 Jun;19(6):357-370.,"We are entering a new era of mouse phenomics, driven by large-scale and economical generation of mouse mutants coupled with increasingly sophisticated and comprehensive phenotyping. These studies are generating large, multidimensional gene-phenotype data sets, which are shedding new light on the mammalian genome landscape and revealing many hitherto unknown features of mammalian gene function. Moreover, these phenome resources provide a wealth of disease models and can be integrated with human genomics data as a powerful approach for the interpretation of human genetic variation and its relationship to disease. In the future, the development of novel phenotyping platforms allied to improved computational approaches, including machine learning, for the analysis of phenotype data will continue to enhance our ability to develop a comprehensive and powerful model of mammalian gene-phenotype space."
29618526,236.0,Opportunities and obstacles for deep learning in biology and medicine,2018 Apr;15(141):20170387.,"Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems-patient classification, fundamental biological processes and treatment of patients-and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine."
29614729,20.0,"Imaging, Tracking and Computational Analyses of Virus Entry and Egress with the Cytoskeleton",2018 Mar 31;10(4):166.,"Viruses have a dual nature: particles are &ldquo;passive substances&rdquo; lacking chemical energy transformation, whereas infected cells are &ldquo;active substances&rdquo; turning-over energy. How passive viral substances convert to active substances, comprising viral replication and assembly compartments has been of intense interest to virologists, cell and molecular biologists and immunologists. Infection starts with virus entry into a susceptible cell and delivers the viral genome to the replication site. This is a multi-step process, and involves the cytoskeleton and associated motor proteins. Likewise, the egress of progeny virus particles from the replication site to the extracellular space is enhanced by the cytoskeleton and associated motor proteins. This overcomes the limitation of thermal diffusion, and transports virions and virion components, often in association with cellular organelles. This review explores how the analysis of viral trajectories informs about mechanisms of infection. We discuss the methodology enabling researchers to visualize single virions in cells by fluorescence imaging and tracking. Virus visualization and tracking are increasingly enhanced by computational analyses of virus trajectories as well as in silico modeling. Combined approaches reveal previously unrecognized features of virus-infected cells. Using select examples of complementary methodology, we highlight the role of actin filaments and microtubules, and their associated motors in virus infections. In-depth studies of single virion dynamics at high temporal and spatial resolutions thereby provide deep insight into virus infection processes, and are a basis for uncovering underlying mechanisms of how cells function."
29614377,1.0,Neural representations of time-linked memory,2018 Sep;153(Pt A):57-70.,"Many cognitive processes, such as episodic memory and decision making, rely on the ability to form associations between two events that occur separately in time. The formation of such temporal associations depends on neural representations of three types of information: what has been presented (trace holding), what will follow (temporal expectation), and when the following event will occur (explicit timing). The present review seeks to link these representations with firing patterns of single neurons recorded while rodents and non-human primates associate stimuli, outcomes, and motor responses over time intervals. Across these studies, two distinct firing patterns were observed in the hippocampus, neocortex, and striatum: some neurons change firing rates during or shortly after the stimulus presentation and sustain the firing rate stably or sidlingly during the subsequent intervals (tonic firings). Other neurons transiently change firing rates during a specific moment within the time intervals (phasic firings), and as a group, they form a sequential firing pattern that covers the entire interval. Clever task designs used in some of these studies collectively provide evidence that both tonic and phasic firing responses represent trace holding, temporal expectation, and explicit timing. Subsequently, we applied machine-learning based classification approaches to the two firing patterns within the same dataset collected from rat medial prefrontal cortex during trace eyeblink conditioning. This quantitative analysis revealed that phasic-firing patterns showed greater selectivity for stimulus identity and temporal position than tonic-firing patterns. Our summary illuminates distributed neural representations of temporal association in the forebrain and generates several ideas for future investigations."
29606338,43.0,Deep Learning in Radiology,2018 Nov;25(11):1472-1480.,"As radiology is inherently a data-driven specialty, it is especially conducive to utilizing data processing techniques. One such technique, deep learning (DL), has become a remarkably powerful tool for image processing in recent years. In this work, the Association of University Radiologists Radiology Research Alliance Task Force on Deep Learning provides an overview of DL for the radiologist. This article aims to present an overview of DL in a manner that is understandable to radiologists; to examine past, present, and future applications; as well as to evaluate how radiologists may benefit from this remarkable new tool. We describe several areas within radiology in which DL techniques are having the most significant impact: lesion or disease detection, classification, quantification, and segmentation. The legal and ethical hurdles to implementation are also discussed. By taking advantage of this powerful tool, radiologists can become increasingly more accurate in their interpretations with fewer errors and spend more time to focus on patient care."
29603063,27.0,Deep Learning for Drug Design: an Artificial Intelligence Paradigm for Drug Discovery in the Big Data Era,2018 Mar 30;20(3):58.,"Over the last decade, deep learning (DL) methods have been extremely successful and widely used to develop artificial intelligence (AI) in almost every domain, especially after it achieved its proud record on computational Go. Compared to traditional machine learning (ML) algorithms, DL methods still have a long way to go to achieve recognition in small molecular drug discovery and development. And there is still lots of work to do for the popularization and application of DL for research purpose, e.g., for small molecule drug research and development. In this review, we mainly discussed several most powerful and mainstream architectures, including the convolutional neural network (CNN), recurrent neural network (RNN), and deep auto-encoder networks (DAENs), for supervised learning and nonsupervised learning; summarized most of the representative applications in small molecule drug design; and briefly introduced how DL methods were used in those applications. The discussion for the pros and cons of DL methods as well as the main challenges we need to tackle were also emphasized."
29601896,21.0,Differentiating between bipolar and unipolar depression in functional and structural MRI studies,2019 Apr 20;91:20-27.,"Distinguishing depression in bipolar disorder (BD) from unipolar depression (UD) solely based on clinical clues is difficult, which has led to the exploration of promising neural markers in neuroimaging measures for discriminating between BD depression and UD. In this article, we review structural and functional magnetic resonance imaging (MRI) studies that directly compare UD and BD depression based on neuroimaging modalities including functional MRI studies on regional brain activation or functional connectivity, structural MRI on gray or white matter morphology, and pattern classification analyses using a machine learning approach. Numerous studies have reported distinct functional and structural alterations in emotion- or reward-processing neural circuits between BD depression and UD. Different activation patterns in neural networks including the amygdala, anterior cingulate cortex (ACC), prefrontal cortex (PFC), and striatum during emotion-, reward-, or cognition-related tasks have been reported between BD and UD. A stronger functional connectivity pattern in BD was pronounced in default mode and in frontoparietal networks and brain regions including the PFC, ACC, parietal and temporal regions, and thalamus compared to UD. Gray matter volume differences in the ACC, hippocampus, amygdala, and dorsolateral prefrontal cortex (DLPFC) have been reported between BD and UD, along with a thinner DLPFC in BD compared to UD. BD showed reduced integrity in the anterior part of the corpus callosum and posterior cingulum compared to UD. Several studies performed pattern classification analysis using structural and functional MRI data to distinguish between UD and BD depression using a supervised machine learning approach, which yielded a moderate level of accuracy in classification."
29601321,2.0,Predicting adverse hemodynamic events in critically ill patients,2018 Jun;24(3):196-203.,"Purpose of review:                    The art of predicting future hemodynamic instability in the critically ill has rapidly become a science with the advent of advanced analytical processed based on computer-driven machine learning techniques. How these methods have progressed beyond severity scoring systems to interface with decision-support is summarized.              Recent findings:                    Data mining of large multidimensional clinical time-series databases using a variety of machine learning tools has led to our ability to identify alert artifact and filter it from bedside alarms, display real-time risk stratification at the bedside to aid in clinical decision-making and predict the subsequent development of cardiorespiratory insufficiency hours before these events occur. This fast evolving filed is primarily limited by linkage of high-quality granular to physiologic rationale across heterogeneous clinical care domains.              Summary:                    Using advanced analytic tools to glean knowledge from clinical data streams is rapidly becoming a reality whose clinical impact potential is great."
29600766,,Discovering Synergistic Drug Combination from a Computational Perspective,2018;18(12):965-974.,"Synergistic drug combinations play an important role in the treatment of complex diseases. The identification of effective drug combination is vital to further reduce the side effects and improve therapeutic efficiency. In previous years, in vitro method has been the main route to discover synergistic drug combinations. However, many limitations of time and resource consumption lie within the in vitro method. Therefore, with the rapid development of computational models and the explosive growth of large and phenotypic data, computational methods for discovering synergistic drug combinations are an efficient and promising tool and contribute to precision medicine. It is the key of computational methods how to construct the computational model. Different computational strategies generate different performance. In this review, the recent advancements in computational methods for predicting effective drug combination are concluded from multiple aspects. First, various datasets utilized to discover synergistic drug combinations are summarized. Second, we discussed feature-based approaches and partitioned these methods into two classes including feature-based methods in terms of similarity measure, and feature-based methods in terms of machine learning. Third, we discussed network-based approaches for uncovering synergistic drug combinations. Finally, we analyzed and prospected computational methods for predicting effective drug combinations."
29595111,4.0,Looking for New Inhibitors for the Epidermal Growth Factor Receptor,2018;18(3):219-232.,"Epidermal Growth Factor Receptor (EGFR) is still the main target of the Head and Neck Squamous Cell Cancer (HNSCC) because its overexpression has been detected in more than 90% of this type of cancer. This overexpression is usually linked with more aggressive disease, increased resistance to chemotherapy and radiotherapy, increased metastasis, inhibition of apoptosis, promotion of neoplastic angiogenesis, and, finally, poor prognosis and decreased survival. Due to this reason, the main target in the search of new drugs and inhibitors candidates is to downturn this overexpression. Quantitative Structure-Activity Relationship (QSAR) is one of the most widely used approaches while looking for new and more active inhibitors drugs. In this contest, a lot of authors used this technique, combined with others, to find new drugs or enhance the activity of well-known inhibitors. In this paper, on one hand, we will review the most important QSAR approaches developed in the last fifteen years, spacing from classical 1D approaches until more sophisticated 3D; the first paper is dated 2003 while the last one is from 2017. On the other hand, we will present a completely new QSAR approach aimed at the prediction of new EGFR inhibitors drugs. The model presented here has been developed over a dataset consisting of more than 1000 compounds using various molecular descriptors calculated with the DRAGON 7.0Â© software."
29594137,6.0,Artificial Intelligence for the Artificial Kidney: Pointers to the Future of a Personalized Hemodialysis Therapy,2018 Feb;4(1):1-9.,"Background:                    Current dialysis devices are not able to react when unexpected changes occur during dialysis treatment or to learn about experience for therapy personalization. Furthermore, great efforts are dedicated to develop miniaturized artificial kidneys to achieve a continuous and personalized dialysis therapy, in order to improve the patient's quality of life. These innovative dialysis devices will require a real-time monitoring of equipment alarms, dialysis parameters, and patient-related data to ensure patient safety and to allow instantaneous changes of the dialysis prescription for the assessment of their adequacy. The analysis and evaluation of the resulting large-scale data sets enters the realm of ""big data"" and will require real-time predictive models. These may come from the fields of machine learning and computational intelligence, both included in artificial intelligence, a branch of engineering involved with the creation of devices that simulate intelligent behavior. The incorporation of artificial intelligence should provide a fully new approach to data analysis, enabling future advances in personalized dialysis therapies. With the purpose to learn about the present and potential future impact on medicine from experts in artificial intelligence and machine learning, a scientific meeting was organized in the Hospital Universitari Bellvitge (L'Hospitalet, Barcelona). As an outcome of that meeting, the aim of this review is to investigate artificial intel ligence experiences on dialysis, with a focus on potential barriers, challenges, and prospects for future applications of these technologies.              Summary and key messages:                    Artificial intelligence research on dialysis is still in an early stage, and the main challenge relies on interpretability and/or comprehensibility of data models when applied to decision making. Artificial neural networks and medical decision support systems have been used to make predictions about anemia, total body water, or intradialysis hypotension and are promising approaches for the prescription and monitoring of hemodialysis therapy. Current dialysis machines are continuously improving due to innovative technological developments, but patient safety is still a key challenge. Real-time monitoring systems, coupled with automatic instantaneous biofeedback, will allow changing dialysis prescriptions continuously. The integration of vital sign monitoring with dialysis parameters will produce large data sets that will require the use of data analysis techniques, possibly from the area of machine learning, in order to make better decisions and increase the safety of patients."
29570167,3.0,Advanced Morphologic Analysis for Diagnosing Allograft Rejection: The Case of Cardiac Transplant Rejection,2018 Aug;102(8):1230-1239.,"Allograft rejection remains a significant concern after all solid organ transplants. Although qualitative morphologic analysis with histologic grading of biopsy samples is the main tool employed for diagnosing allograft rejection, this standard has significant limitations in precision and accuracy that affect patient care. The use of endomyocardial biopsy to diagnose cardiac allograft rejection illustrates the significant shortcomings of current approaches for diagnosing allograft rejection. Despite disappointing interobserver variability, concerns about discordance with clinical trajectories, attempts at revising the histologic criteria and efforts to establish new diagnostic tools with imaging and gene expression profiling, no method has yet supplanted endomyocardial biopsy as the diagnostic gold standard. In this context, automated approaches to complex data analysis problems-often referred to as ""machine learning""-represent promising strategies to improve overall diagnostic accuracy. By focusing on cardiac allograft rejection, where tissue sampling is relatively frequent, this review highlights the limitations of the current approach to diagnosing allograft rejection, introduces the basic methodology behind machine learning and automated image feature detection, and highlights the initial successes of these approaches within cardiovascular medicine."
29566172,4.0,Biomedical informatics and machine learning for clinical genomics,2018 May 1;27(R1):R29-R34.,"While tens of thousands of pathogenic variants are used to inform the many clinical applications of genomics, there remains limited information on quantitative disease risk for the majority of variants used in clinical practice. At the same time, rising demand for genetic counselling has prompted a growing need for computational approaches that can help interpret genetic variation. Such tasks include predicting variant pathogenicity and identifying variants that are too common to be penetrant. To address these challenges, researchers are increasingly turning to integrative informatics approaches. These approaches often leverage vast sources of data, including electronic health records and population-level allele frequency databases (e.g. gnomAD), as well as machine learning techniques such as support vector machines and deep learning. In this review, we highlight recent informatics and machine learning approaches that are improving our understanding of pathogenic variation and discuss obstacles that may limit their emerging role in clinical genomics."
29555807,9.0,Blessing of dimensionality: mathematical foundations of the statistical physics of data,2018 Apr 28;376(2118):20170237.,"The concentrations of measure phenomena were discovered as the mathematical background to statistical mechanics at the end of the nineteenth/beginning of the twentieth century and have been explored in mathematics ever since. At the beginning of the twenty-first century, it became clear that the proper utilization of these phenomena in machine learning might transform the curse of dimensionality into the blessing of dimensionality This paper summarizes recently discovered phenomena of measure concentration which drastically simplify some machine learning problems in high dimension, and allow us to correct legacy artificial intelligence systems. The classical concentration of measure theorems state that i.i.d. random points are concentrated in a thin layer near a surface (a sphere or equators of a sphere, an average or median-level set of energy or another Lipschitz function, etc.). The new stochastic separation theorems describe the thin structure of these thin layers: the random points are not only concentrated in a thin layer but are all linearly separable from the rest of the set, even for exponentially large random sets. The linear functionals for separation of points can be selected in the form of the linear Fisher's discriminant. All artificial intelligence systems make errors. Non-destructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behaviour by a simple and robust classifier. The stochastic separation theorems provide us with such classifiers and determine a non-iterative (one-shot) procedure for their construction.This article is part of the theme issue 'Hilbert's sixth problem'."
29551544,3.0,Radiogenomics and IR,2018 May;29(5):706-713.,"Radiogenomics involves the integration of mineable data from imaging phenotypes with genomic and clinical data to establish predictive models using machine learning. As a noninvasive surrogate for a tumor's in vivo genetic profile, radiogenomics may potentially provide data for patient treatment stratification. Radiogenomics may also supersede the shortcomings associated with genomic research, such as the limited availability of high-quality tissue and restricted sampling of tumoral subpopulations. Interventional radiologists are well suited to circumvent these obstacles through advancements in image-guided tissue biopsies and intraprocedural imaging. Comprehensive understanding of the radiogenomic process is crucial for interventional radiologists to contribute to this evolving field."
29545756,18.0,e-Addictology: An Overview of New Technologies for Assessing and Intervening in Addictive Behaviors,2018 Mar 1;9:51.,"Background:                    New technologies can profoundly change the way we understand psychiatric pathologies and addictive disorders. New concepts are emerging with the development of more accurate means of collecting live data, computerized questionnaires, and the use of passive data. Digital phenotyping, a paradigmatic example, refers to the use of computerized measurement tools to capture the characteristics of different psychiatric disorders. Similarly, machine learning-a form of artificial intelligence-can improve the classification of patients based on patterns that clinicians have not always considered in the past. Remote or automated interventions (web-based or smartphone-based apps), as well as virtual reality and neurofeedback, are already available or under development.              Objective:                    These recent changes have the potential to disrupt practices, as well as practitioners' beliefs, ethics and representations, and may even call into question their professional culture. However, the impact of new technologies on health professionals' practice in addictive disorder care has yet to be determined. In the present paper, we therefore present an overview of new technology in the field of addiction medicine.              Method:                    Using the keywords [e-health], [m-health], [computer], [mobile], [smartphone], [wearable], [digital], [machine learning], [ecological momentary assessment], [biofeedback] and [virtual reality], we searched the PubMed database for the most representative articles in the field of assessment and interventions in substance use disorders.              Results:                    We screened 595 abstracts and analyzed 92 articles, dividing them into seven categories: e-health program and web-based interventions, machine learning, computerized adaptive testing, wearable devices and digital phenotyping, ecological momentary assessment, biofeedback, and virtual reality.              Conclusion:                    This overview shows that new technologies can improve assessment and interventions in the field of addictive disorders. The precise role of connected devices, artificial intelligence and remote monitoring remains to be defined. If they are to be used effectively, these tools must be explained and adapted to the different profiles of physicians and patients. The involvement of patients, caregivers and other health professionals is essential to their design and assessment."
29544791,32.0,"Blood vessel segmentation algorithms - Review of methods, datasets and evaluation metrics",2018 May;158:71-91.,"Background:                    Blood vessel segmentation is a topic of high interest in medical image analysis since the analysis of vessels is crucial for diagnosis, treatment planning and execution, and evaluation of clinical outcomes in different fields, including laryngology, neurosurgery and ophthalmology. Automatic or semi-automatic vessel segmentation can support clinicians in performing these tasks. Different medical imaging techniques are currently used in clinical practice and an appropriate choice of the segmentation algorithm is mandatory to deal with the adopted imaging technique characteristics (e.g. resolution, noise and vessel contrast).              Objective:                    This paper aims at reviewing the most recent and innovative blood vessel segmentation algorithms. Among the algorithms and approaches considered, we deeply investigated the most novel blood vessel segmentation including machine learning, deformable model, and tracking-based approaches.              Methods:                    This paper analyzes more than 100 articles focused on blood vessel segmentation methods. For each analyzed approach, summary tables are presented reporting imaging technique used, anatomical region and performance measures employed. Benefits and disadvantages of each method are highlighted.              Discussion:                    Despite the constant progress and efforts addressed in the field, several issues still need to be overcome. A relevant limitation consists in the segmentation of pathological vessels. Unfortunately, not consistent research effort has been addressed to this issue yet. Research is needed since some of the main assumptions made for healthy vessels (such as linearity and circular cross-section) do not hold in pathological tissues, which on the other hand require new vessel model formulations. Moreover, image intensity drops, noise and low contrast still represent an important obstacle for the achievement of a high-quality enhancement. This is particularly true for optical imaging, where the image quality is usually lower in terms of noise and contrast with respect to magnetic resonance and computer tomography angiography.              Conclusion:                    No single segmentation approach is suitable for all the different anatomical region or imaging modalities, thus the primary goal of this review was to provide an up to date source of information about the state of the art of the vessel segmentation algorithms so that the most suitable methods can be chosen according to the specific task."
29536448,4.0,Machine Learning-Based Modeling of Drug Toxicity,2018;1754:247-264.,"Toxicity is an important reason for the failure of drug research and development (R&D). The traditional experimental testings for chemical toxicity profile are costly and time-consuming. Therefore, it is attractive to develop the effective and accurate alternatives, such as in silico prediction models. In this review, we discuss the practical use of some prediction models on three toxicity end points, including acute toxicity, carcinogenicity, and inhibition of the human ether-a-go-go-related gene ion channel (hERG). Special emphasis is put on the machine learning methods for developing in silico models, and their advantages and weaknesses are discussed. We conclude that machine learning methods are valuable for helping the process of designing new candidates with low toxicity in drug R&D studies. In the future, much still needs to be done to understand more completely the biological mechanisms for toxicity and to develop more accurate prediction models to screen compounds."
29536444,,Revisit of Machine Learning Supported Biological and Biomedical Studies,2018;1754:183-204.,"Generally, machine learning includes many in silico methods to transform the principles underlying natural phenomenon to human understanding information, which aim to save human labor, to assist human judge, and to create human knowledge. It should have wide application potential in biological and biomedical studies, especially in the era of big biological data. To look through the application of machine learning along with biological development, this review provides wide cases to introduce the selection of machine learning methods in different practice scenarios involved in the whole biological and biomedical study cycle and further discusses the machine learning strategies for analyzing omics data in some cutting-edge biological studies. Finally, the notes on new challenges for machine learning due to small-sample high-dimension are summarized from the key points of sample unbalance, white box, and causality."
29534053,14.0,Calibration of Minimally Invasive Continuous Glucose Monitoring Sensors: State-of-The-Art and Current Perspectives,2018 Mar 13;8(1):24.,"Minimally invasive continuous glucose monitoring (CGM) sensors are wearable medical devices that provide real-time measurement of subcutaneous glucose concentration. This can be of great help in the daily management of diabetes. Most of the commercially available CGM devices have a wire-based sensor, usually placed in the subcutaneous tissue, which measures a ""raw"" current signal via a glucose-oxidase electrochemical reaction. This electrical signal needs to be translated in real-time to glucose concentration through a calibration process. For such a scope, the first commercialized CGM sensors implemented simple linear regression techniques to fit reference glucose concentration measurements periodically collected by fingerprick. On the one hand, these simple linear techniques required several calibrations per day, with the consequent patient's discomfort. On the other, only a limited accuracy was achieved. This stimulated researchers to propose, over the last decade, more sophisticated algorithms to calibrate CGM sensors, resorting to suitable signal processing, modelling, and machine-learning techniques. This review paper will first contextualize and describe the calibration problem and its implementation in the first generation of CGM sensors, and then present the most recently-proposed calibration algorithms, with a perspective on how these new techniques can influence future CGM products in terms of accuracy improvement and calibration reduction."
29528822,4.0,"Multimedia-enhanced Radiology Reports: Concept, Components, and Challenges",Mar-Apr 2018;38(2):462-482.,"Multimedia-enhanced radiology report (MERR) development is defined and described from an informatics perspective, in which the MERR is seen as a superior information-communicating entity. Recent technical advances, such as the hyperlinking of report text directly to annotated images, improve MERR information content and accessibility compared with text-only reports. The MERR is analyzed by its components, which include hypertext, tables, graphs, embedded images, and their interconnections. The authors highlight the advantages of each component for improving the radiologist's communication of report content information and the user's ability to extract information. Requirements for MERR implementation (eg, integration of picture archiving and communication systems, radiology information systems, and electronic medical record systems) and the authors' initial experiences and challenges in MERR implementation at the National Institutes of Health are reviewed. The transition to MERRs has provided advantages over use of traditional text-only radiology reports because of the capacity to include hyperlinked report text that directs clinicians to image annotations, images, tables, and graphs. A framework is provided for thinking about the MERR from the user's perspective. Additional applications of emerging technologies (eg, artificial intelligence and machine learning) are described in the crafting of what the authors believe is the radiology report of the future. Â©RSNA, 2018."
29521204,5.0,In Silico Chemogenomics Drug Repositioning Strategies for Neglected Tropical Diseases,2019;26(23):4355-4379.,"Only ~1% of all drug candidates against Neglected Tropical Diseases (NTDs) have reached clinical trials in the last decades, underscoring the need for new, safe and effective treatments. In such context, drug repositioning, which allows finding novel indications for approved drugs whose pharmacokinetic and safety profiles are already known, emerging as a promising strategy for tackling NTDs. Chemogenomics is a direct descendent of the typical drug discovery process that involves the systematic screening of chemical compounds against drug targets in high-throughput screening (HTS) efforts, for the identification of lead compounds. However, different to the one-drug-one-target paradigm, chemogenomics attempts to identify all potential ligands for all possible targets and diseases. In this review, we summarize current methodological development efforts in drug repositioning that use state-of-the-art computational ligand- and structure-based chemogenomics approaches. Furthermore, we highlighted the recent progress in computational drug repositioning for some NTDs, based on curation and modeling of genomic, biological, and chemical data. Additionally, we also present in-house and other successful examples and suggest possible solutions to existing pitfalls."
29517418,3.0,SNPs affecting the clinical outcomes of regularly used immunosuppressants,2018 Apr;19(5):495-511.,"Recent studies have suggested that genomic diversity may play a key role in different clinical outcomes, and the importance of SNPs is becoming increasingly clear. In this article, we summarize the bioactivity of SNPs that may affect the sensitivity to or possibility of drug reactions that occur among the signaling pathways of regularly used immunosuppressants, such as glucocorticoids, azathioprine, tacrolimus, mycophenolate mofetil, cyclophosphamide and methotrexate. The development of bioinformatics, including machine learning models, has enabled prediction of the proper immunosuppressant dosage with minimal adverse drug reactions for patients after organ transplantation or for those with autoimmune diseases. This article provides a theoretical basis for the personalized use of immunosuppressants in the future."
29515993,20.0,In Silico Prediction of Chemical Toxicity for Drug Design Using Machine Learning Methods and Structural Alerts,2018 Feb 20;6:30.,"During drug development, safety is always the most important issue, including a variety of toxicities and adverse drug effects, which should be evaluated in preclinical and clinical trial phases. This review article at first simply introduced the computational methods used in prediction of chemical toxicity for drug design, including machine learning methods and structural alerts. Machine learning methods have been widely applied in qualitative classification and quantitative regression studies, while structural alerts can be regarded as a complementary tool for lead optimization. The emphasis of this article was put on the recent progress of predictive models built for various toxicities. Available databases and web servers were also provided. Though the methods and models are very helpful for drug design, there are still some challenges and limitations to be improved for drug safety assessment in the future."
29515569,41.0,Computational Strategies for Dissecting the High-Dimensional Complexity of Adaptive Immune Repertoires,2018 Feb 21;9:224.,"The adaptive immune system recognizes antigens via an immense array of antigen-binding antibodies and T-cell receptors, the immune repertoire. The interrogation of immune repertoires is of high relevance for understanding the adaptive immune response in disease and infection (e.g., autoimmunity, cancer, HIV). Adaptive immune receptor repertoire sequencing (AIRR-seq) has driven the quantitative and molecular-level profiling of immune repertoires, thereby revealing the high-dimensional complexity of the immune receptor sequence landscape. Several methods for the computational and statistical analysis of large-scale AIRR-seq data have been developed to resolve immune repertoire complexity and to understand the dynamics of adaptive immunity. Here, we review the current research on (i) diversity, (ii) clustering and network, (iii) phylogenetic, and (iv) machine learning methods applied to dissect, quantify, and compare the architecture, evolution, and specificity of immune repertoires. We summarize outstanding questions in computational immunology and propose future directions for systems immunology toward coupling AIRR-seq with the computational discovery of immunotherapeutics, vaccines, and immunodiagnostics."
29513400,20.0,The Molecular Industrial Revolution: Automated Synthesis of Small Molecules,2018 Apr 9;57(16):4192-4214.,"Today we are poised for a transition from the highly customized crafting of specific molecular targets by hand to the increasingly general and automated assembly of different types of molecules with the push of a button. Creating machines that are capable of making many different types of small molecules on demand, akin to that which has been achieved on the macroscale with 3D printers, is challenging. Yet important progress is being made toward this objective with two complementary approaches: 1) Automation of customized synthesis routes to different targets by machines that enable the use of many reactions and starting materials, and 2) automation of generalized platforms that make many different targets using common coupling chemistry and building blocks. Continued progress in these directions has the potential to shift the bottleneck in molecular innovation from synthesis to imagination, and thereby help drive a new industrial revolution on the molecular scale."
29508152,13.0,A Systematic Literature Review of Technologies for Suicidal Behavior Prevention,2018 Mar 5;42(4):71.,"Suicide is the second cause of death in young people. The use of technologies as tools facilitates the detection of individuals at risk of suicide thus allowing early intervention and efficacy. Suicide can be prevented in many cases. Technology can help people at risk of suicide and their families. It could prevent situations of risk of suicide with the technological evolution that is increasing. This work is a systematic review of research papers published in the last ten years on technology for suicide prevention. In September 2017, the consultation was carried out in the scientific databases PubMed, ScienceDirect, PsycINFO, The Cochrane Library and Google Scholar. A general search was conducted with the terms ""prevention"" AND ""suicide"" AND ""technology. More specific searches included technologies such as ""Web"", ""mobile"", ""social networks"", and others terms related to technologies. The number of articles found following the methodology proposed was 90, but only 30 are focused on the objective of this work. Most of them were Web technologies (51.61%), mobile solutions (22.58%), social networks (12.90%), machine learning (3.23%) and other technologies (9.68%). According to the results obtained, although there are technological solutions that help the prevention of suicide, much remains to be done in this field. Collaboration among technologists, psychiatrists, patients, and family members is key to advancing the development of new technology-based solutions that can help save lives."
29507784,187.0,"Artificial intelligence in healthcare: past, present and future",2017 Jun 21;2(4):230-243.,"Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI."
29498975,1.0,The State of Technology in Craniosynostosis,2018 Jun;29(4):904-907.,"Introduction:                    Craniosynostosis, the premature fusion of â¥1 cranial sutures, is the leading cause of pediatric skull deformities, affecting 1 of every 2000 to 2500 live births worldwide. Technologies used for the management of craniofacial conditions, specifically in craniosynostosis, have been advancing dramatically. This article highlights the most recent technological advances in craniosynostosis surgery through a systematic review of the literature.              Methods:                    A systematic electronic search was performed using the PubMed database. Search terms used were ""craniosynostosis"" AND ""technology"" OR ""innovation"" OR ""novel.' Two independent reviewers subsequently reviewed the resultant articles based on strict inclusion and exclusion criteria. Selected manuscripts deemed novel by the senior authors were grouped by procedure categories.              Results:                    Following review of the PubMed database, 28 of 536 articles were retained. Of the 28 articles, 20 articles consisting of 21 technologies were deemed as being novel by the senior authors. The technologies were categorized as diagnostic imaging (n = 6), surgical planning (n = 4), cranial vault evaluation (n = 4), machine learning (n = 3), ultrasound pinning (n = 3), and near-infrared spectroscopy (n = 1).              Conclusion:                    Multiple technological advances have impacted the treatment of craniosynostosis. These innovations include improvement in diagnosis and objective measurement of craniosynostosis, preoperative planning, intraoperative procedures, communication between both surgeons and patients, and surgical education."
29497285,18.0,Review of Statistical Learning Methods in Integrated Omics Studies (An Integrated Information Science),2018 Feb 20;12:1177932218759292.,"Integrated omics is becoming a new channel for investigating the complex molecular system in modern biological science and sets a foundation for systematic learning for precision medicine. The statistical/machine learning methods that have emerged in the past decade for integrated omics are not only innovative but also multidisciplinary with integrated knowledge in biology, medicine, statistics, machine learning, and artificial intelligence. Here, we review the nontrivial classes of learning methods from the statistical aspects and streamline these learning methods within the statistical learning framework. The intriguing findings from the review are that the methods used are generalizable to other disciplines with complex systematic structure, and the integrated omics is part of an integrated information science which has collated and integrated different types of information for inferences and decision making. We review the statistical learning methods of exploratory and supervised learning from 42 publications. We also discuss the strengths and limitations of the extended principal component analysis, cluster analysis, network analysis, and regression methods. Statistical techniques such as penalization for sparsity induction when there are fewer observations than the number of features and using Bayesian approach when there are prior knowledge to be integrated are also included in the commentary. For the completeness of the review, a table of currently available software and packages from 23 publications for omics are summarized in the appendix."
29492605,12.0,"Machine learning for medical ultrasound: status, methods, and future opportunities",2018 Apr;43(4):786-799.,"Ultrasound (US) imaging is the most commonly performed cross-sectional diagnostic imaging modality in the practice of medicine. It is low-cost, non-ionizing, portable, and capable of real-time image acquisition and display. US is a rapidly evolving technology with significant challenges and opportunities. Challenges include high inter- and intra-operator variability and limited image quality control. Tremendous opportunities have arisen in the last decade as a result of exponential growth in available computational power coupled with progressive miniaturization of US devices. As US devices become smaller, enhanced computational capability can contribute significantly to decreasing variability through advanced image processing. In this paper, we review leading machine learning (ML) approaches and research directions in US, with an emphasis on recent ML advances. We also present our outlook on future opportunities for ML techniques to further improve clinical workflow and US-based disease diagnosis and characterization."
29490932,43.0,Emerging Technologies for Molecular Diagnosis of Sepsis,2018 Feb 28;31(2):e00089-17.,"Rapid and accurate profiling of infection-causing pathogens remains a significant challenge in modern health care. Despite advances in molecular diagnostic techniques, blood culture analysis remains the gold standard for diagnosing sepsis. However, this method is too slow and cumbersome to significantly influence the initial management of patients. The swift initiation of precise and targeted antibiotic therapies depends on the ability of a sepsis diagnostic test to capture clinically relevant organisms along with antimicrobial resistance within 1 to 3 h. The administration of appropriate, narrow-spectrum antibiotics demands that such a test be extremely sensitive with a high negative predictive value. In addition, it should utilize small sample volumes and detect polymicrobial infections and contaminants. All of this must be accomplished with a platform that is easily integrated into the clinical workflow. In this review, we outline the limitations of routine blood culture testing and discuss how emerging sepsis technologies are converging on the characteristics of the ideal sepsis diagnostic test. We include seven molecular technologies that have been validated on clinical blood specimens or mock samples using human blood. In addition, we discuss advances in machine learning technologies that use electronic medical record data to provide contextual evaluation support for clinical decision-making."
29488902,93.0,A review of classification algorithms for EEG-based brain-computer interfaces: a 10 year update,2018 Jun;15(3):031005.,"Objective:                    Most current electroencephalography (EEG)-based brain-computer interfaces (BCIs) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify EEG signals in BCIs. The time is therefore ripe for an updated review of EEG classification algorithms for BCIs.              Approach:                    We surveyed the BCI and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design BCIs. We synthesize these studies in order to present such algorithms, to report how they were used for BCIs, what were the outcomes, and to identify their pros and cons.              Main results:                    We found that the recently designed classification algorithms for EEG-based BCIs can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple BCI problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art BCI methods.              Significance:                    This paper provides a comprehensive overview of the modern classification algorithms used in EEG-based BCIs, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance EEG classification in BCI."
29487619,104.0,Deep Learning for Computer Vision: A Brief Review,2018 Feb 1;2018:7068349.,"Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein."
29486863,76.0,Machine Learning for Precision Psychiatry: Opportunities and Challenges,2018 Mar;3(3):223-230.,"The nature of mental illness remains a conundrum. Traditional disease categories are increasingly suspected to misrepresent the causes underlying mental disturbance. Yet psychiatrists and investigators now have an unprecedented opportunity to benefit from complex patterns in brain, behavior, and genes using methods from machine learning (e.g., support vector machines, modern neural-network algorithms, cross-validation procedures). Combining these analysis techniques with a wealth of data from consortia and repositories has the potential to advance a biologically grounded redefinition of major psychiatric disorders. Increasing evidence suggests that data-derived subgroups of psychiatric patients can better predict treatment outcomes than DSM/ICD diagnoses can. In a new era of evidence-based psychiatry tailored to single patients, objectively measurable endophenotypes could allow for early disease detection, individualized treatment selection, and dosage adjustment to reduce the burden of disease. This primer aims to introduce clinicians and researchers to the opportunities and challenges in bringing machine intelligence into psychiatric practice."
29483348,8.0,What is the natural measurement unit of temperament: single traits or profiles?,2018 Apr 19;373(1744):20170163.,"There is fundamental doubt about whether the natural unit of measurement for temperament and personality corresponds to single traits or to multi-trait profiles that describe the functioning of a whole person. Biogenetic researchers of temperament usually assume they need to focus on individual traits that differ between individuals. Recent research indicates that a shift of emphasis to understand processes within the individual is crucial for identifying the natural building blocks of temperament. Evolution and development operate on adaptation of whole organisms or persons, not on individual traits or categories. Adaptive functioning generally depends on feedback among many variable processes in ways that are characteristic of complex adaptive systems, not machines with separate parts. Advanced methods of unsupervised machine learning can now be applied to genome-wide association studies and brain imaging in order to uncover the genotypic-phenotypic architecture of traits like temperament, which are strongly influenced by complex interactions, such as genetic epistasis, pleiotropy and gene-environment interactions. We have found that the heritability of temperament can be nearly fully explained by a large number of genetic variants that are unique for multi-trait profiles, not single traits. The implications of this finding for research design and precision medicine are discussed.This article is part of the theme issue 'Diverse perspectives on diversity: multi-disciplinary approaches to taxonomies of individual differences'."
29481793,3.0,Patterns of epileptic seizure occurrence,2019 Jan 15;1703:3-12.,"Background:                    The occurrence of epileptic seizures in seemingly random patterns takes a great toll on persons with epilepsy and their families. Seizure prediction may markedly improve epilepsy management and, therefore, the quality of life of persons with epilepsy.              Methods:                    Literature review.              Results:                    Seizures tend to occur following complex non-random patterns. Circadian oscillators may contribute to the rhythmic patterns of seizure occurrence. Complex mathematical models based on chaos theory try to explain and even predict seizure occurrence. There are several patterns of epileptic seizure occurrence based on seizure location, seizure semiology, and hormonal factors, among others. These patterns are most frequently described for large populations. Inter-individual variability and complex interactions between the rhythmic generators continue to make it more difficult to predict seizures in any individual person. The increasing use of large databases and machine learning techniques may help better define patterns of seizure occurrence in individual patients. Improvements in seizure detection -such as wearable seizure detectors- and in seizure prediction -such as machine learning techniques and artificial as well as neuronal networks- promise to provide further progress in the field of epilepsy and are being applied to closed-loop systems for the treatment of epilepsy.              Conclusions:                    Seizures tend to occur following complex and patient-specific patterns despite their apparently random occurrence. A better understanding of these patterns and current technological advances may allow the implementation of closed-loop detection, prediction, and treatment systems in routine clinical practice."
29477048,4.0,Simulations meet machine learning in structural biology,2018 Apr;49:139-144.,"Classical molecular dynamics (MD) simulations will be able to reach sampling in the second timescale within five years, producing petabytes of simulation data at current force field accuracy. Notwithstanding this, MD will still be in the regime of low-throughput, high-latency predictions with average accuracy. We envisage that machine learning (ML) will be able to solve both the accuracy and time-to-prediction problem by learning predictive models using expensive simulation data. The synergies between classical, quantum simulations and ML methods, such as artificial neural networks, have the potential to drastically reshape the way we make predictions in computational structural biology and drug discovery."
29476532,5.0,Research Imaging of Brain Structure and Function After Concussion,2018 Jun;58(6):827-835.,"Even when concussions are associated with prolonged physical and cognitive sequelae, concussions are typically ""invisible"" on diagnostic brain imaging, indicating that the neuropathology associated with concussion lies under the detection threshold of routine imaging. However, data from brain structural and functional research imaging studies using diffusion tensor imaging, resting-state functional magnetic resonance imaging, and brain perfusion imaging indicate that these imaging sequences have a role in identifying concussion-related neuropathology. These advanced imaging techniques provide insights into concussion neuropathology and might be useful for differentiating concussed patients from healthy controls. In this review article, we provide an overview of research findings from brain structural and functional imaging studies of concussion, and discuss the accuracy of classification models developed via machine-learning algorithms for identifying individual patients with concussion based on imaging data."
29476392,2.0,"Collaborative and Reproducible Research: Goals, Challenges, and Strategies",2018 Jun;31(3):275-282.,"Combining imaging biomarkers with genomic and clinical phenotype data is the foundation of precision medicine research efforts. Yet, biomedical imaging research requires unique infrastructure compared with principally text-driven clinical electronic medical record (EMR) data. The issues are related to the binary nature of the file format and transport mechanism for medical images as well as the post-processing image segmentation and registration needed to combine anatomical and physiological imaging data sources. The SiiM Machine Learning Committee was formed to analyze the gaps and challenges surrounding research into machine learning in medical imaging and to find ways to mitigate these issues. At the 2017 annual meeting, a whiteboard session was held to rank the most pressing issues and develop strategies to meet them. The results, and further reflections, are summarized in this paper."
29467659,10.0,Transfer and Multi-task Learning in QSAR Modeling: Advances and Challenges,2018 Feb 6;9:74.,"Medicinal chemistry projects involve some steps aiming to develop a new drug, such as the analysis of biological targets related to a given disease, the discovery and the development of drug candidates for these targets, performing parallel biological tests to validate the drug effectiveness and side effects. Approaches as quantitative study of activity-structure relationships (QSAR) involve the construction of predictive models that relate a set of descriptors of a chemical compound series and its biological activities with respect to one or more targets in the human body. Datasets used to perform QSAR analyses are generally characterized by a small number of samples and this makes them more complex to build accurate predictive models. In this context, transfer and multi-task learning techniques are very suitable since they take information from other QSAR models to the same biological target, reducing efforts and costs for generating new chemical compounds. Therefore, this review will present the main features of transfer and multi-task learning studies, as well as some applications and its potentiality in drug design projects."
29463985,7.0,Involvement of Machine Learning for Breast Cancer Image Classification: A Survey,2017;2017:3781951.,"Breast cancer is one of the largest causes of women's death in the world today. Advance engineering of natural image classification techniques and Artificial Intelligence methods has largely been used for the breast-image classification task. The involvement of digital image classification allows the doctor and the physicians a second opinion, and it saves the doctors' and physicians' time. Despite the various publications on breast image classification, very few review papers are available which provide a detailed description of breast cancer image classification techniques, feature extraction and selection procedures, classification measuring parameterizations, and image classification findings. We have put a special emphasis on the Convolutional Neural Network (CNN) method for breast image classification. Along with the CNN method we have also described the involvement of the conventional Neural Network (NN), Logic Based classifiers such as the Random Forest (RF) algorithm, Support Vector Machines (SVM), Bayesian methods, and a few of the semisupervised and unsupervised methods which have been used for breast image classification."
29463885,12.0,Machine learning for the meta-analyses of microbial pathogens' volatile signatures,2018 Feb 20;8(1):3360.,"Non-invasive and fast diagnostic tools based on volatolomics hold great promise in the control of infectious diseases. However, the tools to identify microbial volatile organic compounds (VOCs) discriminating between human pathogens are still missing. Artificial intelligence is increasingly recognised as an essential tool in health sciences. Machine learning algorithms based in support vector machines and features selection tools were here applied to find sets of microbial VOCs with pathogen-discrimination power. Studies reporting VOCs emitted by human microbial pathogens published between 1977 and 2016 were used as source data. A set of 18 VOCs is sufficient to predict the identity of 11 microbial pathogens with high accuracy (77%), and precision (62-100%). There is one set of VOCs associated with each of the 11 pathogens which can predict the presence of that pathogen in a sample with high accuracy and precision (86-90%). The implemented pathogen classification methodology supports future database updates to include new pathogen-VOC data, which will enrich the classifiers. The sets of VOCs identified potentiate the improvement of the selectivity of non-invasive infection diagnostics using artificial olfaction devices."
29452923,8.0,Statistical and machine learning approaches to predicting protein-ligand interactions,2018 Apr;49:123-128.,"Data driven computational approaches to predicting protein-ligand binding are currently achieving unprecedented levels of accuracy on held-out test datasets. Up until now, however, this has not led to corresponding breakthroughs in our ability to design novel ligands for protein targets of interest. This review summarizes the current state of the art in this field, emphasizing the recent development of deep neural networks for predicting protein-ligand binding. We explain the major technical challenges that have caused difficulty with predicting novel ligands, including the problems of sampling noise and the challenge of using benchmark datasets that are sufficiently unbiased that they allow the model to extrapolate to new regimes."
29450843,,Detection of Lung Contour with Closed Principal Curve and Machine Learning,2018 Aug;31(4):520-533.,"Radiation therapy plays an essential role in the treatment of cancer. In radiation therapy, the ideal radiation doses are delivered to the observed tumor while not affecting neighboring normal tissues. In three-dimensional computed tomography (3D-CT) scans, the contours of tumors and organs-at-risk (OARs) are often manually delineated by radiologists. The task is complicated and time-consuming, and the manually delineated results will be variable from different radiologists. We propose a semi-supervised contour detection algorithm, which firstly uses a few points of region of interest (ROI) as an approximate initialization. Then the data sequences are achieved by the closed polygonal line (CPL) algorithm, where the data sequences consist of the ordered projection indexes and the corresponding initial points. Finally, the smooth lung contour can be obtained, when the data sequences are trained by the backpropagation neural network model (BNNM). We use the private clinical dataset and the public Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) dataset to measure the accuracy of the presented method, respectively. To the private dataset, experimental results on the initial points which are as low as 15% of the manually delineated points show that the Dice coefficient reaches up to 0.95 and the global error is as low as 1.47 Ã 10-2. The performance of the proposed algorithm is also better than the cubic spline interpolation (CSI) algorithm. While on the public LIDC-IDRI dataset, our method achieves superior segmentation performance with average Dice of 0.83."
29445894,1.0,Sparse QSAR modelling methods for therapeutic and regenerative medicine,2018 Apr;32(4):497-509.,"The quantitative structure-activity relationships method was popularized by Hansch and Fujita over 50 years ago. The usefulness of the method for drug design and development has been shown in the intervening years. As it was developed initially to elucidate which molecular properties modulated the relative potency of putative agrochemicals, and at a time when computing resources were scarce, there is much scope for applying modern mathematical methods to improve the QSAR method and to extending the general concept to the discovery and optimization of bioactive molecules and materials more broadly. I describe research over the past two decades where we have rebuilt the unit operations of the QSAR method using improved mathematical techniques, and have applied this valuable platform technology to new important areas of research and industry such as nanoscience, omics technologies, advanced materials, and regenerative medicine. This paper was presented as the 2017 ACS Herman Skolnik lecture."
29441463,8.0,Cumulative Risk and Impact Modeling on Environmental Chemical and Social Stressors,2018 Mar;5(1):88-99.,"Purpose of review:                    The goal of this review is to identify cumulative modeling methods used to evaluate combined effects of exposures to environmental chemicals and social stressors. The specific review question is: What are the existing quantitative methods used to examine the cumulative impacts of exposures to environmental chemical and social stressors on health?              Recent findings:                    There has been an increase in literature that evaluates combined effects of exposures to environmental chemicals and social stressors on health using regression models; very few studies applied other data mining and machine learning techniques to this problem. The majority of studies we identified used regression models to evaluate combined effects of multiple environmental and social stressors. With proper study design and appropriate modeling assumptions, additional data mining methods may be useful to examine combined effects of environmental and social stressors."
29441154,12.0,e-PTSD: an overview on how new technologies can improve prediction and assessment of Posttraumatic Stress Disorder (PTSD),2018 Feb 6;9(sup1):1424448.,"Background: New technologies may profoundly change our way of understanding psychiatric disorders including posttraumatic stress disorder (PTSD). Imaging and biomarkers, along with technological and medical informatics developments, might provide an answer regarding at-risk patient's identification. Recent advances in the concept of 'digital phenotype', which refers to the capture of characteristics of a psychiatric disorder by computerized measurement tools, is one paradigmatic example. Objective: The impact of the new technologies on health professionals practice in PTSD care remains to be determined. The recent evolutions could disrupt the clinical practices and practitioners in their beliefs, ethics and representations, going as far as questioning their professional culture. In the present paper, we conducted an extensive search to highlight the articles which reflect the potential of these new technologies. Method: We conducted an overview by querying PubMed database with the terms [PTSD] [Posttraumatic stress disorder] AND [Computer] OR [Computerized] OR [Mobile] OR [Automatic] OR [Automated] OR [Machine learning] OR [Sensor] OR [Heart rate variability] OR [HRV] OR [actigraphy] OR [actimetry] OR [digital] OR [motion] OR [temperature] OR [virtual reality]. Results: We summarized the synthesized literature in two categories: prediction and assessment (including diagnostic, screening and monitoring). Two independent reviewers screened, extracted data and quality appraised the sources. Results were synthesized narratively. Conclusions: This overview shows that many studies are underway allowing researchers to start building a PTSD digital phenotype using passive data obtained by biometric sensors. Active data obtained from Ecological Momentary Assessment (EMA) could allow clinicians to assess PTSD patients. The place of connected objects, Artificial Intelligence and remote monitoring of patients with psychiatric pathology remains to be defined. These tools must be explained and adapted to the different profiles of physicians and patients. The involvement of patients, caregivers and health professionals is essential to the design and evaluation of these new tools."
29439490,7.0,Chemical Sensor Systems and Associated Algorithms for Fire Detection: A Review,2018 Feb 11;18(2):553.,"Indoor fire detection using gas chemical sensing has been a subject of investigation since the early nineties. This approach leverages the fact that, for certain types of fire, chemical volatiles appear before smoke particles do. Hence, systems based on chemical sensing can provide faster fire alarm responses than conventional smoke-based fire detectors. Moreover, since it is known that most casualties in fires are produced from toxic emissions rather than actual burns, gas-based fire detection could provide an additional level of safety to building occupants. In this line, since the 2000s, electrochemical cells for carbon monoxide sensing have been incorporated into fire detectors. Even systems relying exclusively on gas sensors have been explored as fire detectors. However, gas sensors respond to a large variety of volatiles beyond combustion products. As a result, chemical-based fire detectors require multivariate data processing techniques to ensure high sensitivity to fires and false alarm immunity. In this paper, we the survey toxic emissions produced in fires and defined standards for fire detection systems. We also review the state of the art of chemical sensor systems for fire detection and the associated signal and data processing algorithms. We also examine the experimental protocols used for the validation of the different approaches, as the complexity of the test measurements also impacts on reported sensitivity and specificity measures. All in all, further research and extensive test under different fire and nuisance scenarios are still required before gas-based fire detectors penetrate largely into the market. Nevertheless, the use of dynamic features and multivariate models that exploit sensor correlations seems imperative."
29438494,6.0,Artificial intelligence in drug combination therapy,2019 Jul 19;20(4):1434-1448.,"Currently, the development of medicines for complex diseases requires the development of combination drug therapies. It is necessary because in many cases, one drug cannot target all necessary points of intervention. For example, in cancer therapy, a physician often meets a patient having a genomic profile including more than five molecular aberrations. Drug combination therapy has been an area of interest for a while, for example the classical work of Loewe devoted to the synergism of drugs was published in 1928-and it is still used in calculations for optimal drug combinations. More recently, over the past several years, there has been an explosion in the available information related to the properties of drugs and the biomedical parameters of patients. For the drugs, hundreds of 2D and 3D molecular descriptors for medicines are now available, while for patients, large data sets related to genetic/proteomic and metabolomics profiles of the patients are now available, as well as the more traditional data relating to the histology, history of treatments, pretreatment state of the organism, etc. Moreover, during disease progression, the genetic profile can change. Thus, the ability to optimize drug combinations for each patient is rapidly moving beyond the comprehension and capabilities of an individual physician. This is the reason, that biomedical informatics methods have been developed and one of the more promising directions in this field is the application of artificial intelligence (AI). In this review, we discuss several AI methods that have been successfully implemented in several instances of combination drug therapy from HIV, hypertension, infectious diseases to cancer. The data clearly show that the combination of rule-based expert systems with machine learning algorithms may be promising direction in this field."
29436887,16.0,Machine learning in autistic spectrum disorder behavioral research: A review and ways forward,2019 Sep;44(3):278-297.,"Autistic Spectrum Disorder (ASD) is a mental disorder that retards acquisition of linguistic, communication, cognitive, and social skills and abilities. Despite being diagnosed with ASD, some individuals exhibit outstanding scholastic, non-academic, and artistic capabilities, in such cases posing a challenging task for scientists to provide answers. In the last few years, ASD has been investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning to improve diagnostic timing, precision, and quality. Machine learning is a multidisciplinary research topic that employs intelligent techniques to discover useful concealed patterns, which are utilized in prediction to improve decision making. Machine learning techniques such as support vector machines, decision trees, logistic regressions, and others, have been applied to datasets related to autism in order to construct predictive models. These models claim to enhance the ability of clinicians to provide robust diagnoses and prognoses of ASD. However, studies concerning the use of machine learning in ASD diagnosis and treatment suffer from conceptual, implementation, and data issues such as the way diagnostic codes are used, the type of feature selection employed, the evaluation measures chosen, and class imbalances in data among others. A more serious claim in recent studies is the development of a new method for ASD diagnoses based on machine learning. This article critically analyses these recent investigative studies on autism, not only articulating the aforementioned issues in these studies but also recommending paths forward that enhance machine learning use in ASD with respect to conceptualization, implementation, and data. Future studies concerning machine learning in autism research are greatly benefitted by such proposals."
29434508,7.0,Quantum machine learning: a classical perspective,2018 Jan;474(2209):20170551.,"Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning (ML) techniques to impressive results in regression, classification, data generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets is motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed up classical ML algorithms. Here we review the literature in quantum ML and discuss perspectives for a mixed readership of classical ML and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in ML are identified as promising directions for the field. Practical questions, such as how to upload classical data into quantum form, will also be addressed."
29431517,9.0,Informatics and machine learning to define the phenotype,2018 Mar;18(3):219-226.,"For the past decade, the focus of complex disease research has been the genotype. From technological advancements to the development of analysis methods, great progress has been made. However, advances in our definition of the phenotype have remained stagnant. Phenotype characterization has recently emerged as an exciting area of informatics and machine learning. The copious amounts of diverse biomedical data that have been collected may be leveraged with data-driven approaches to elucidate trait-related features and patterns. Areas covered: In this review, the authors discuss the phenotype in traditional genetic associations and the challenges this has imposed.Approaches for phenotype refinement that can aid in more accurate characterization of traits are also discussed. Further, the authors highlight promising machine learning approaches for establishing a phenotype and the challenges of electronic health record (EHR)-derived data. Expert commentary: The authors hypothesize that through unsupervised machine learning, data-driven approaches can be used to define phenotypes rather than relying on expert clinician knowledge. Through the use of machine learning and an unbiased set of features extracted from clinical repositories, researchers will have the potential to further understand complex traits and identify patient subgroups. This knowledge may lead to more preventative and precise clinical care."
29430456,1.0,Text-mining analysis of mHealth research,2017 Dec 27;3:53.,"In recent years, because of the advancements in communication and networking technologies, mobile technologies have been developing at an unprecedented rate. mHealth, the use of mobile technologies in medicine, and the related research has also surged parallel to these technological advancements. Although there have been several attempts to review mHealth research through manual processes such as systematic reviews, the sheer magnitude of the number of studies published in recent years makes this task very challenging. The most recent developments in machine learning and text mining offer some potential solutions to address this challenge by allowing analyses of large volumes of texts through semi-automated processes. The objective of this study is to analyze the evolution of mHealth research by utilizing text-mining and natural language processing (NLP) analyses. The study sample included abstracts of 5,644 mHealth research articles, which were gathered from five academic search engines by using search terms such as mobile health, and mHealth. The analysis used the Text Explorer module of JMP Pro 13 and an iterative semi-automated process involving tokenizing, phrasing, and terming. After developing the document term matrix (DTM) analyses such as single value decomposition (SVD), topic, and hierarchical document clustering were performed, along with the topic-informed document clustering approach. The results were presented in the form of word-clouds and trend analyses. There were several major findings regarding research clusters and trends. First, our results confirmed time-dependent nature of terminology use in mHealth research. For example, in earlier versus recent years the use of terminology changed from ""mobile phone"" to ""smartphone"" and from ""applications"" to ""apps"". Second, ten clusters for mHealth research were identified including (I) Clinical Research on Lifestyle Management, (II) Community Health, (III) Literature Review, (IV) Medical Interventions, (V) Research Design, (VI) Infrastructure, (VII) Applications, (VIII) Research and Innovation in Health Technologies, (IX) Sensor-based Devices and Measurement Algorithms, (X) Survey-based Research. Third, the trend analyses indicated the infrastructure cluster as the highest percentage researched area until 2014. The Research and Innovation in Health Technologies cluster experienced the largest increase in numbers of publications in recent years, especially after 2014. This study is unique because it is the only known study utilizing text-mining analyses to reveal the streams and trends for mHealth research. The fast growth in mobile technologies is expected to lead to higher numbers of studies focusing on mHealth and its implications for various healthcare outcomes. Findings of this study can be utilized by researchers in identifying areas for future studies."
29430271,1.0,Point-of-care testing in the early diagnosis of acute pesticide intoxication: The example of paraquat,2018 Jan 19;12(1):011501.,"Acute pesticide intoxication is a common method of suicide globally. This article reviews current diagnostic methods and makes suggestions for future development. In the case of paraquat intoxication, it is characterized by multi-organ failure, causing substantial mortality and morbidity. Early diagnosis may save the life of a paraquat intoxication patient. Conventional paraquat intoxication diagnostic methods, such as symptom review and urine sodium dithionite assay, are time-consuming and impractical in resource-scarce areas where most intoxication cases occur. Several experimental and clinical studies have shown the potential of portable Surface Enhanced Raman Scattering (SERS), paper-based devices, and machine learning for paraquat intoxication diagnosis. Portable SERS and new SERS substrates maintain the sensitivity of SERS while being less costly and more convenient than conventional SERS. Paper-based devices provide the advantages of price and portability. Machine learning algorithms can be implemented as a mobile phone application and facilitate diagnosis in resource-limited areas. Although these methods have not yet met all features of an ideal diagnostic method, the combination and development of these methods offer much promise."
29428074,15.0,Machine learning techniques for breast cancer computer aided diagnosis using different image modalities: A systematic review,2018 Mar;156:25-45.,"Background and objective:                    The high incidence of breast cancer in women has increased significantly in the recent years. Physician experience of diagnosing and detecting breast cancer can be assisted by using some computerized features extraction and classification algorithms. This paper presents the conduction and results of a systematic review (SR) that aims to investigate the state of the art regarding the computer aided diagnosis/detection (CAD) systems for breast cancer.              Methods:                    The SR was conducted using a comprehensive selection of scientific databases as reference sources, allowing access to diverse publications in the field. The scientific databases used are Springer Link (SL), Science Direct (SD), IEEE Xplore Digital Library, and PubMed. Inclusion and exclusion criteria were defined and applied to each retrieved work to select those of interest. From 320 studies retrieved, 154 studies were included. However, the scope of this research is limited to scientific and academic works and excludes commercial interests.              Results:                    This survey provides a general analysis of the current status of CAD systems according to the used image modalities and the machine learning based classifiers. Potential research studies have been discussed to create a more objective and efficient CAD systems."
29427011,4.0,Self-learning computers for surgical planning and prediction of postoperative alignment,2018 Feb;27(Suppl 1):123-128.,"Purpose:                    In past decades, the role of sagittal alignment has been widely demonstrated in the setting of spinal conditions. As several parameters can be affected, identifying the driver of the deformity is the cornerstone of a successful treatment approach. Despite the importance of restoring sagittal alignment for optimizing outcome, this task remains challenging. Self-learning computers and optimized algorithms are of great interest in spine surgery as in that they facilitate better planning and prediction of postoperative alignment. Nowadays, computer-assisted tools are part of surgeons' daily practice; however, the use of such tools remains to be time-consuming.              Methods:                    NARRATIVE REVIEW AND RESULTS: Computer-assisted methods for the prediction of postoperative alignment consist of a three step analysis: identification of anatomical landmark, definition of alignment objectives, and simulation of surgery. Recently, complex rules for the prediction of alignment have been proposed. Even though this kind of work leads to more personalized objectives, the number of parameters involved renders it difficult for clinical use, stressing the importance of developing computer-assisted tools. The evolution of our current technology, including machine learning and other types of advanced algorithms, will provide powerful tools that could be useful in improving surgical outcomes and alignment prediction. These tools can combine different types of advanced technologies, such as image recognition and shape modeling, and using this technique, computer-assisted methods are able to predict spinal shape. The development of powerful computer-assisted methods involves the integration of several sources of information such as radiographic parameters (X-rays, MRI, CT scan, etc.), demographic information, and unusual non-osseous parameters (muscle quality, proprioception, gait analysis data). In using a larger set of data, these methods will aim to mimic what is actually done by spine surgeons, leading to real tailor-made solutions.              Conclusion:                    Integrating newer technology can change the current way of planning/simulating surgery. The use of powerful computer-assisted tools that are able to integrate several parameters and learn from experience can change the traditional way of selecting treatment pathways and counseling patients. However, there is still much work to be done to reach a desired level as noted in other orthopedic fields, such as hip surgery. Many of these tools already exist in non-medical fields and their adaptation to spine surgery is of considerable interest."
29426712,11.0,Getting Momentum: From Biocatalysis to Advanced Synthetic Biology,2018 Mar;43(3):180-198.,"Applied biocatalysis is driven by environmental and economic incentives for using enzymes in the synthesis of various pharmaceutical and industrially important chemicals. Protein engineering is used to tailor the properties of enzymes to catalyze desired chemical transformations, and some engineered enzymes now outperform the best chemocatalytic alternatives by orders of magnitude. Unfortunately, custom engineering of a robust biocatalyst is still a time-consuming process, but an understanding of how enzyme function depends on amino acid sequence will speed up the process. This review demonstrates how recent advances in ultrahigh-throughput screening, mutational scanning, DNA synthesis, metagenomics, and machine learning will soon make it possible to model, predict, and manipulate the relationship between protein sequence and function, accelerating the tailor design of novel biocatalysts."
29426065,18.0,A review of machine learning in obesity,2018 May;19(5):668-685.,"Rich sources of obesity-related data arising from sensors, smartphone apps, electronic medical health records and insurance data can bring new insights for understanding, preventing and treating obesity. For such large datasets, machine learning provides sophisticated and elegant tools to describe, classify and predict obesity-related risks and outcomes. Here, we review machine learning methods that predict and/or classify such as linear and logistic regression, artificial neural networks, deep learning and decision tree analysis. We also review methods that describe and characterize data such as cluster analysis, principal component analysis, network science and topological data analysis. We introduce each method with a high-level overview followed by examples of successful applications. The algorithms were then applied to National Health and Nutrition Examination Survey to demonstrate methodology, utility and outcomes. The strengths and limitations of each method were also evaluated. This summary of machine learning algorithms provides a unique overview of the state of data analysis applied specifically to obesity."
29419402,42.0,Deep Learning in Neuroradiology,2018 Oct;39(10):1776-1784.,"Deep learning is a form of machine learning using a convolutional neural network architecture that shows tremendous promise for imaging applications. It is increasingly being adapted from its original demonstration in computer vision applications to medical imaging. Because of the high volume and wealth of multimodal imaging information acquired in typical studies, neuroradiology is poised to be an early adopter of deep learning. Compelling deep learning research applications have been demonstrated, and their use is likely to grow rapidly. This review article describes the reasons, outlines the basic methods used to train and test deep learning models, and presents a brief overview of current and potential clinical applications with an emphasis on how they are likely to change future neuroradiology practice. Facility with these methods among neuroimaging researchers and clinicians will be important to channel and harness the vast potential of this new method."
29415726,4.0,Advances in intelligent diagnosis methods for pulmonary ground-glass opacity nodules,2018 Feb 7;17(1):20.,"Pulmonary nodule is one of the important lesions of lung cancer, mainly divided into two categories of solid nodules and ground glass nodules. The improvement of diagnosis of lung cancer has significant clinical significance, which could be realized by machine learning techniques. At present, there have been a lot of researches focusing on solid nodules. But the research on ground glass nodules started late, and lacked research results. This paper summarizes the research progress of the method of intelligent diagnosis for pulmonary nodules since 2014. It is described in details from four aspects: nodular signs, data analysis methods, prediction models and system evaluation. This paper aims to provide the research material for researchers of the clinical diagnosis and intelligent analysis of lung cancer, and further improve the precision of pulmonary ground glass nodule diagnosis."
29409736,3.0,Biosignature Discovery for Substance Use Disorders Using Statistical Learning,2018 Feb;24(2):221-235.,"There are limited biomarkers for substance use disorders (SUDs). Traditional statistical approaches are identifying simple biomarkers in large samples, but clinical use cases are still being established. High-throughput clinical, imaging, and 'omic' technologies are generating data from SUD studies and may lead to more sophisticated and clinically useful models. However, analytic strategies suited for high-dimensional data are not regularly used. We review strategies for identifying biomarkers and biosignatures from high-dimensional data types. Focusing on penalized regression and Bayesian approaches, we address how to leverage evidence from existing studies and knowledge bases, using nicotine metabolism as an example. We posit that big data and machine learning approaches will considerably advance SUD biomarker discovery. However, translation to clinical practice, will require integrated scientific efforts."
29405981,9.0,Environmental metabolomics with data science for investigating ecosystem homeostasis,2018 Feb;104:56-88.,"A natural ecosystem can be viewed as the interconnections between complex metabolic reactions and environments. Humans, a part of these ecosystems, and their activities strongly affect the environments. To account for human effects within ecosystems, understanding what benefits humans receive by facilitating the maintenance of environmental homeostasis is important. This review describes recent applications of several NMR approaches to the evaluation of environmental homeostasis by metabolic profiling and data science. The basic NMR strategy used to evaluate homeostasis using big data collection is similar to that used in human health studies. Sophisticated metabolomic approaches (metabolic profiling) are widely reported in the literature. Further challenges include the analysis of complex macromolecular structures, and of the compositions and interactions of plant biomass, soil humic substances, and aqueous particulate organic matter. To support the study of these topics, we also discuss sample preparation techniques and solid-state NMR approaches. Because NMR approaches can produce a number of data with high reproducibility and inter-institution compatibility, further analysis of such data using machine learning approaches is often worthwhile. We also describe methods for data pretreatment in solid-state NMR and for environmental feature extraction from heterogeneously-measured spectroscopic data by machine learning approaches."
29405647,4.0,Deep Generative Models for Molecular Science,2018 Jan;37(1-2).,"Generative deep machine learning models now rival traditional quantum-mechanical computations in predicting properties of new structures, and they come with a significantly lower computational cost, opening new avenues in computational molecular science. In the last few years, a variety of deep generative models have been proposed for modeling molecules, which differ in both their model structure and choice of input features. We review these recent advances within deep generative models for predicting molecular properties, with particular focus on models based on the probabilistic autoencoder (or variational autoencoder, VAE) approach in which the molecular structure is embedded in a latent vector space from which its properties can be predicted and its structure can be restored."
29405289,6.0,Role of artificial intelligence in the care of patients with nonsmall cell lung cancer,2018 Apr;48(4).,"Background:                    Lung cancer is the leading cause of cancer death worldwide. In up to 57% of patients, it is diagnosed at an advanced stage and the 5-year survival rate ranges between 10%-16%. There has been a significant amount of research using machine learning to generate tools using patient data to improve outcomes.              Methods:                    This narrative review is based on research material obtained from PubMed up to Nov 2017. The search terms include ""artificial intelligence,"" ""machine learning,"" ""lung cancer,"" ""Nonsmall Cell Lung Cancer (NSCLC),"" ""diagnosis"" and ""treatment.""              Results:                    Recent studies support the use of computer-aided systems and the use of radiomic features to help diagnose lung cancer earlier. Other studies have looked at machine learning (ML) methods that offer prognostic tools to doctors and help them in choosing personalized treatment options for their patients based on molecular, genetics and histological features. Combining artificial intelligence approaches into health care may serve as a beneficial tool for patients with NSCLC, and this review outlines these benefits and current shortcomings throughout the continuum of care.              Conclusion:                    We present a review of the various applications of ML methods in NSCLC as it relates to improving diagnosis, treatment and outcomes."
29401044,77.0,Machine Learning Approaches for Clinical Psychology and Psychiatry,2018 May 7;14:91-118.,"Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined."
29398494,53.0,Machine Learning in Medical Imaging,2018 Mar;15(3 Pt B):512-520.,"Advances in both imaging and computers have synergistically led to a rapid rise in the potential use of artificial intelligence in various radiological imaging tasks, such as risk assessment, detection, diagnosis, prognosis, and therapy response, as well as in multi-omics disease discovery. A brief overview of the field is given here, allowing the reader to recognize the terminology, the various subfields, and components of machine learning, as well as the clinical potential. Radiomics, an expansion of computer-aided diagnosis, has been defined as the conversion of images to minable data. The ultimate benefit of quantitative radiomics is to (1) yield predictive image-based phenotypes of disease for precision medicine or (2) yield quantitative image-based phenotypes for data mining with other -omics for discovery (ie, imaging genomics). For deep learning in radiology to succeed, note that well-annotated large data sets are needed since deep networks are complex, computer software and hardware are evolving constantly, and subtle differences in disease states are more difficult to perceive than differences in everyday objects. In the future, machine learning in radiology is expected to have a substantial clinical impact with imaging examinations being routinely obtained in clinical practice, providing an opportunity to improve decision support in medical image interpretation. The term of note is decision support, indicating that computers will augment human decision making, making it more effective and efficient. The clinical impact of having computers in the routine clinical practice may allow radiologists to further integrate their knowledge with their clinical colleagues in other medical specialties and allow for precision medicine."
29396321,2.0,Conceptual endophenotypes: A strategy to advance the impact of psychoneuroendocrinology in precision medicine,2018 Mar;89:147-160.,"Psychobiological research has generated a tremendous amount of findings on the psychological, neuroendocrine, molecular and environmental processes that are directly relevant for mental and physical health, but have overwhelmed our capacity to meaningfully absorb, integrate, and utilize this knowledge base. Here, we reflect about suitable strategies to improve the translational success of psychoneuroendocrinological research in the era of precision medicine. Following a strategy advocated by the National Research Council and the tradition of endophenotype-based research, we advance here a new approach, termed ""conceptual endophenotypes"". We define the contextual and formal criteria of conceptual endophenotypes, outline criteria for filtering and selecting information, and describe how conceptual endophenotypes can be validated and implemented at the bedside. As proof-of-concept, we describe some of our findings from research that has adopted this approach in the context of stress-related disorders. We argue that conceptual endophenotypes engineer a bridge between the bench and the bedside. This approach readily lends itself to being continuously developed and implemented. Recent methodological advances, including digital phenotyping, machine learning, grassroots collaboration, and a learning healthcare system, may accelerate the development and implementation of this conceptual endophenotype approach."
29395987,4.0,Humanitarian health computing using artificial intelligence and social media: A narrative literature review,2018 Jun;114:136-142.,"Introduction:                    According to the World Health Organization (WHO), over 130 million people are in constant need of humanitarian assistance due to natural disasters, disease outbreaks, and conflicts, among other factors. These health crises can compromise the resilience of healthcare systems, which are essential for achieving the health objectives of the sustainable development goals (SDGs) of the United Nations (UN). During a humanitarian health crisis, rapid and informed decision making is required. This is often challenging due to information scarcity, limited resources, and strict time constraints. Moreover, the traditional approach to digital health development, which involves a substantial requirement analysis, a feasibility study, and deployment of technology, is ill-suited for many crisis contexts. The emergence of Web 2.0 technologies and social media platforms in the past decade, such as Twitter, has created a new paradigm of massive information and misinformation, in which new technologies need to be developed to aid rapid decision making during humanitarian health crises.              Objective:                    Humanitarian health crises increasingly require the analysis of massive amounts of information produced by different sources, such as social media content, and, hence, they are a prime case for the use of artificial intelligence (AI) techniques to help identify relevant information and make it actionable. To identify challenges and opportunities for using AI in humanitarian health crises, we reviewed the literature on the use of AI techniques to process social media.              Methodology:                    We performed a narrative literature review aimed at identifying examples of the use of AI in humanitarian health crises. Our search strategy was designed to get a broad overview of the different applications of AI in a humanitarian health crisis and their challenges. A total of 1459 articles were screened, and 24 articles were included in the final analysis.              Results:                    Successful case studies of AI applications in a humanitarian health crisis have been reported, such as for outbreak detection. A commonly shared concern in the reviewed literature is the technical challenge of analyzing large amounts of data in real time. Data interoperability, which is essential to data sharing, is also a barrier with regard to the integration of online and traditional data sources. Human and organizational aspects that might be key factors for the adoption of AI and social media remain understudied. There is also a publication bias toward high-income countries, as we identified few examples in low-income countries. Further, we did not identify any examples of certain types of major crisis, such armed conflicts, in which misinformation might be more common.              Conclusions:                    The feasibility of using AI to extract valuable information during a humanitarian health crisis is proven in many cases. There is a lack of research on how to integrate the use of AI into the work-flow and large-scale deployments of humanitarian aid during a health crisis."
29395627,25.0,Challenges and Promises of PET Radiomics,2018 Nov 15;102(4):1083-1089.,"Purpose:                    Radiomics describes the extraction of multiple, otherwise invisible, features from medical images that, with bioinformatic approaches, can be used to provide additional information that can predict underlying tumor biology and behavior.              Methods and materials:                    Radiomic signatures can be used alone or with other patient-specific data to improve tumor phenotyping, treatment response prediction, and prognosis, noninvasively. The data describing 18F-fluorodeoxyglucose positron emission tomography radiomics, often using texture or heterogeneity parameters, are increasing rapidly.              Results:                    In relation to radiation therapy practice, early data have reported the use of radiomic approaches to better define tumor volumes and predict radiation toxicity and treatment response.              Conclusions:                    Although at an early stage of development, with many technical challenges remaining and a need for standardization, promise nevertheless exists that PET radiomics will contribute to personalized medicine, especially with the availability of increased computing power and the development of machine-learning approaches for imaging."
29389679,51.0,Artificial Intelligence in Surgery: Promises and Perils,2018 Jul;268(1):70-76.,"Objective:                    The aim of this review was to summarize major topics in artificial intelligence (AI), including their applications and limitations in surgery. This paper reviews the key capabilities of AI to help surgeons understand and critically evaluate new AI applications and to contribute to new developments.              Summary background data:                    AI is composed of various subfields that each provide potential solutions to clinical problems. Each of the core subfields of AI reviewed in this piece has also been used in other industries such as the autonomous car, social networks, and deep learning computers.              Methods:                    A review of AI papers across computer science, statistics, and medical sources was conducted to identify key concepts and techniques within AI that are driving innovation across industries, including surgery. Limitations and challenges of working with AI were also reviewed.              Results:                    Four main subfields of AI were defined: (1) machine learning, (2) artificial neural networks, (3) natural language processing, and (4) computer vision. Their current and future applications to surgical practice were introduced, including big data analytics and clinical decision support systems. The implications of AI for surgeons and the role of surgeons in advancing the technology to optimize clinical effectiveness were discussed.              Conclusions:                    Surgeons are well positioned to help integrate AI into modern practice. Surgeons should partner with data scientists to capture data across phases of care and to provide clinical context, for AI has the potential to revolutionize the way surgery is taught and practiced with the promise of a future optimized for the highest quality patient care."
29378578,5.0,"Artificial intelligence on the identification of risk groups for osteoporosis, a general review",2018 Jan 29;17(1):12.,"Introduction:                    The goal of this paper is to present a critical review on the main systems that use artificial intelligence to identify groups at risk for osteoporosis or fractures. The systems considered for this study were those that fulfilled the following requirements: range of coverage in diagnosis, low cost and capability to identify more significant somatic factors.              Methods:                    A bibliographic research was done in the databases, PubMed, IEEExplorer Latin American and Caribbean Center on Health Sciences Information (LILACS), Medical Literature Analysis and Retrieval System Online (MEDLINE), Cumulative Index to Nursing and Allied Health Literature (CINAHL), Scopus, Web of Science, and Science Direct searching the terms ""Neural Network"", ""Osteoporosis Machine Learning"" and ""Osteoporosis Neural Network"". Studies with titles not directly related to the research topic and older data that reported repeated strategies were excluded. The search was carried out with the descriptors in German, Spanish, French, Italian, Mandarin, Portuguese and English; but only studies written in English were found to meet the established criteria. Articles covering the period 2000-2017 were selected; however, articles prior to this period with great relevance were included in this study.              Discussion:                    Based on the collected research, it was identified that there are several methods in the use of artificial intelligence to help the screening of risk groups of osteoporosis or fractures. However, such systems were limited to a specific ethnic group, gender or age. For future research, new challenges are presented.              Conclusions:                    It is necessary to develop research with the unification of different databases and grouping of the various attributes and clinical factors, in order to reach a greater comprehensiveness in the identification of risk groups of osteoporosis. For this purpose, the use of any predictive tool should be performed in different populations with greater participation of male patients and inclusion of a larger age range for the ones involved. The biggest challenge is to deal with all the data complexity generated by this unification, developing evidence-based standards for the evaluation of the most significant risk factors."
29377981,28.0,Computational prediction of drug-target interactions using chemogenomic approaches: an empirical survey,2019 Jul 19;20(4):1337-1357.,"Computational prediction of drug-target interactions (DTIs) has become an essential task in the drug discovery process. It narrows down the search space for interactions by suggesting potential interaction candidates for validation via wet-lab experiments that are well known to be expensive and time-consuming. In this article, we aim to provide a comprehensive overview and empirical evaluation on the computational DTI prediction techniques, to act as a guide and reference for our fellow researchers. Specifically, we first describe the data used in such computational DTI prediction efforts. We then categorize and elaborate the state-of-the-art methods for predicting DTIs. Next, an empirical comparison is performed to demonstrate the prediction performance of some representative methods under different scenarios. We also present interesting findings from our evaluation study, discussing the advantages and disadvantages of each method. Finally, we highlight potential avenues for further enhancement of DTI prediction performance as well as related research directions."
29376234,2.0,Multimodal Imaging in Diabetic Macular Edema,Jan-Feb 2018;7(1):22-27.,"Throughout ophthalmic history it has been shown that progress has gone hand in hand with technological breakthroughs. In the past, fluorescein angiography and fundus photographs were the most commonly used imaging modalities in the management of diabetic macular edema (DME). Today, despite the moderate correlation between macular thickness and functional outcomes, spectral domain optical coherence tomography (SD-OCT) has become the DME workhorse in clinical practice. Several SD-OCT biomarkers have been looked at including presence of epiretinal membrane, vitreomacular adhesion, disorganization of the inner retinal layers, central macular thickness, integrity of the ellipsoid layer, and subretinal fluid, among others. Emerging imaging modalities include fundus autofluorescence, macular pigment optical density, fluorescence lifetime imaging ophthalmoscopy, OCT angiography, and adaptive optics. Technological advances in imaging of the posterior segment of the eye have enabled ophthalmologists to develop hypotheses about pathological mechanisms of disease, monitor disease progression, and assess response to treatment. Spectral domain OCT is the most commonly performed imaging modality in the management of DME. However, reliable biomarkers have yet to be identified. Machine learning may provide treatment algorithms based on multimodal imaging."
29375355,5.0,Computational Foundations of Natural Intelligence,2017 Dec 7;11:112.,"New developments in AI and neuroscience are revitalizing the quest to understanding natural intelligence, offering insight about how to equip machines with human-like capabilities. This paper reviews some of the computational principles relevant for understanding natural intelligence and, ultimately, achieving strong AI. After reviewing basic principles, a variety of computational modeling approaches is discussed. Subsequently, I concentrate on the use of artificial neural networks as a framework for modeling cognitive processes. This paper ends by outlining some of the challenges that remain to fulfill the promise of machines that show human-like intelligence."
29374138,15.0,A Shared Vision for Machine Learning in Neuroscience,2018 Feb 14;38(7):1601-1607.,"With ever-increasing advancements in technology, neuroscientists are able to collect data in greater volumes and with finer resolution. The bottleneck in understanding how the brain works is consequently shifting away from the amount and type of data we can collect and toward what we actually do with the data. There has been a growing interest in leveraging this vast volume of data across levels of analysis, measurement techniques, and experimental paradigms to gain more insight into brain function. Such efforts are visible at an international scale, with the emergence of big data neuroscience initiatives, such as the BRAIN initiative (Bargmann et al., 2014), the Human Brain Project, the Human Connectome Project, and the National Institute of Mental Health's Research Domain Criteria initiative. With these large-scale projects, much thought has been given to data-sharing across groups (Poldrack and Gorgolewski, 2014; Sejnowski et al., 2014); however, even with such data-sharing initiatives, funding mechanisms, and infrastructure, there still exists the challenge of how to cohesively integrate all the data. At multiple stages and levels of neuroscience investigation, machine learning holds great promise as an addition to the arsenal of analysis tools for discovering how the brain works."
29367240,4.0,Quantitative approaches to energy and glucose homeostasis: machine learning and modelling for precision understanding and prediction,2018 Jan;15(138):20170736.,"Obesity is a major global public health problem. Understanding how energy homeostasis is regulated, and can become dysregulated, is crucial for developing new treatments for obesity. Detailed recording of individual behaviour and new imaging modalities offer the prospect of medically relevant models of energy homeostasis that are both understandable and individually predictive. The profusion of data from these sources has led to an interest in applying machine learning techniques to gain insight from these large, relatively unstructured datasets. We review both physiological models and machine learning results across a diverse range of applications in energy homeostasis, and highlight how modelling and machine learning can work together to improve predictive ability. We collect quantitative details in a comprehensive mathematical supplement. We also discuss the prospects of forecasting homeostatic behaviour and stress the importance of characterizing stochasticity within and between individuals in order to provide practical, tailored forecasts and guidance to combat the spread of obesity."
29366762,131.0,The rise of deep learning in drug discovery,2018 Jun;23(6):1241-1250.,"Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis."
29366600,3.0,Translational Radiomics: Defining the Strategy Pipeline and Considerations for Application-Part 1: From Methodology to Clinical Implementation,2018 Mar;15(3 Pt B):538-542.,"Enterprise imaging has channeled various technological innovations to the field of clinical radiology, ranging from advanced imaging equipment and postacquisition iterative reconstruction tools to image analysis and computer-aided detection tools. More recently, the advancements in the field of quantitative image analysis coupled with machine learning-based data analytics, classification, and integration have ushered us into the era of radiomics, which has tremendous potential in clinical decision support as well as drug discovery. There are important issues to consider to incorporate radiomics as a clinically applicable system and a commercially viable solution. In this two-part series, we offer insights into the development of the translational pipeline for radiomics from methodology to clinical implementation (Part 1) and from that to enterprise development (Part 2)."
29366598,3.0,Translational Radiomics: Defining the Strategy Pipeline and Considerations for Application-Part 2: From Clinical Implementation to Enterprise,2018 Mar;15(3 Pt B):543-549.,"Enterprise imaging has channeled various technological innovations to the field of clinical radiology, ranging from advanced imaging equipment and postacquisition iterative reconstruction tools to image analysis and computer-aided detection tools. More recently, the advancement in the field of quantitative image analysis coupled with machine learning-based data analytics, classification, and integration has ushered in the era of radiomics, a paradigm shift that holds tremendous potential in clinical decision support as well as drug discovery. However, there are important issues to consider to incorporate radiomics into a clinically applicable system and a commercially viable solution. In this two-part series, we offer insights into the development of the translational pipeline for radiomics from methodology to clinical implementation (Part 1) and from that point to enterprise development (Part 2). In Part 2 of this two-part series, we study the components of the strategy pipeline, from clinical implementation to building enterprise solutions."
29362150,10.0,CT and MR imaging for solid renal mass characterization,2018 Feb;99:40-54.,"As our understanding has expanded that relatively large fraction of incidentally discovered renal masses, especially in small size, are benign or indolent even if malignant, there is growing acceptance of more conservative management including active surveillance for small renal masses. As for advanced renal cell carcinomas (RCCs), nonsurgical and subtype specific treatment options such as immunotherapy and targeted therapy is developing. On these backgrounds, renal mass characterization including differentiation of benign from malignant tumors, RCC subtyping and prediction of RCC aggressiveness is receiving much attention and a variety of imaging techniques and analytic methods are being investigated. In addition to conventional imaging techniques, integration of texture analysis, functional imaging (i.e. diffusion weighted and perfusion imaging) and multivariate diagnostic methods including machine learning have provided promising results for these purposes in research fields, although standardization and external, multi-institutional validations are needed."
29362138,23.0,Texture analysis and machine learning to characterize suspected thyroid nodules and differentiated thyroid cancer: Where do we stand?,2018 Feb;99:1-8.,"In thyroid imaging, ""texture"" refers to the echographic appearence of the parenchyma or a nodule. However, definition of the image characteristics is operator dependent and influenced by the operator's experience. In a more objective texture analysis, a variety of mathematical methods are used to describe image inhomogeneity, allowing assessment of an image by means of quantitative parameters. Moreover, this approach may be used to develop an efficient computer-aided diagnosis (CAD) system to yield a second opinion when differentiating malignant and benign thyroid lesions. The aim of this review is to summarize the available literature data on texture analysis, with and without CAD, in patients with suspected thyroid nodules or differentiated thyroid cancer, and to assess the current state of the approach."
29360430,28.0,Image analysis and machine learning for detecting malaria,2018 Apr;194:36-55.,"Malaria remains a major burden on global health, with roughly 200 million cases worldwide and more than 400,000 deaths per year. Besides biomedical research and political efforts, modern information technology is playing a key role in many attempts at fighting the disease. One of the barriers toward a successful mortality reduction has been inadequate malaria diagnosis in particular. To improve diagnosis, image analysis software and machine learning methods have been used to quantify parasitemia in microscopic blood slides. This article gives an overview of these techniques and discusses the current developments in image analysis and machine learning for microscopic malaria diagnosis. We organize the different approaches published in the literature according to the techniques used for imaging, image preprocessing, parasite detection and cell segmentation, feature computation, and automatic cell classification. Readers will find the different techniques listed in tables, with the relevant articles cited next to them, for both thin and thick blood smear images. We also discussed the latest developments in sections devoted to deep learning and smartphone technology for future malaria diagnosis."
29352978,12.0,Use of multimodality imaging and artificial intelligence for diagnosis and prognosis of early stages of Alzheimer's disease,2018 Apr;194:56-67.,"Alzheimer's disease (AD) is a major neurodegenerative disease and the most common cause of dementia. Currently, no treatment exists to slow down or stop the progression of AD. There is converging belief that disease-modifying treatments should focus on early stages of the disease, that is, the mild cognitive impairment (MCI) and preclinical stages. Making a diagnosis of AD and offering a prognosis (likelihood of converting to AD) at these early stages are challenging tasks but possible with the help of multimodality imaging, such as magnetic resonance imaging (MRI), fluorodeoxyglucose (FDG)-positron emission topography (PET), amyloid-PET, and recently introduced tau-PET, which provides different but complementary information. This article is a focused review of existing research in the recent decade that used statistical machine learning and artificial intelligence methods to perform quantitative analysis of multimodality image data for diagnosis and prognosis of AD at the MCI or preclinical stages. We review the existing work in 3 subareas: diagnosis, prognosis, and methods for handling modality-wise missing data-a commonly encountered problem when using multimodality imaging for prediction or classification. Factors contributing to missing data include lack of imaging equipment, cost, difficulty of obtaining patient consent, and patient drop-off (in longitudinal studies). Finally, we summarize our major findings and provide some recommendations for potential future research directions."
29352006,53.0,Machine learning in cardiovascular medicine: are we there yet?,2018 Jul;104(14):1156-1164.,"Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine."
29348728,11.0,Systems-level mechanisms of action of Panax ginseng: a network pharmacological approach,2018 Jan;42(1):98-106.,"Panax ginseng has been used since ancient times based on the traditional Asian medicine theory and clinical experiences, and currently, is one of the most popular herbs in the world. To date, most of the studies concerning P. ginseng have focused on specific mechanisms of action of individual constituents. However, in spite of many studies on the molecular mechanisms of P. ginseng, it still remains unclear how multiple active ingredients of P. ginseng interact with multiple targets simultaneously, giving the multidimensional effects on various conditions and diseases. In order to decipher the systems-level mechanism of multiple ingredients of P. ginseng, a novel approach is needed beyond conventional reductive analysis. We aim to review the systems-level mechanism of P. ginseng by adopting novel analytical framework-network pharmacology. Here, we constructed a compound-target network of P. ginseng using experimentally validated and machine learning-based prediction results. The targets of the network were analyzed in terms of related biological process, pathways, and diseases. The majority of targets were found to be related with primary metabolic process, signal transduction, nitrogen compound metabolic process, blood circulation, immune system process, cell-cell signaling, biosynthetic process, and neurological system process. In pathway enrichment analysis of targets, mainly the terms related with neural activity showed significant enrichment and formed a cluster. Finally, relative degrees analysis for the target-disease association of P. ginseng revealed several categories of related diseases, including respiratory, psychiatric, and cardiovascular diseases."
