pmid,citations,title,date,text
26636071,2.0,Gradient Matching Methods for Computational Inference in Mechanistic Models for Systems Biology: A Review and Comparative Analysis,2015 Nov 20;3:180.,"Parameter inference in mathematical models of biological pathways, expressed as coupled ordinary differential equations (ODEs), is a challenging problem in contemporary systems biology. Conventional methods involve repeatedly solving the ODEs by numerical integration, which is computationally onerous and does not scale up to complex systems. Aimed at reducing the computational costs, new concepts based on gradient matching have recently been proposed in the computational statistics and machine learning literature. In a preliminary smoothing step, the time series data are interpolated; then, in a second step, the parameters of the ODEs are optimized, so as to minimize some metric measuring the difference between the slopes of the tangents to the interpolants, and the time derivatives from the ODEs. In this way, the ODEs never have to be solved explicitly. This review provides a concise methodological overview of the current state-of-the-art methods for gradient matching in ODEs, followed by an empirical comparative evaluation based on a set of widely used and representative benchmark data."
26634919,25.0,Progress and challenges in bioinformatics approaches for enhancer identification,2016 Nov;17(6):967-979.,"Enhancers are cis-acting DNA elements that play critical roles in distal regulation of gene expression. Identifying enhancers is an important step for understanding distinct gene expression programs that may reflect normal and pathogenic cellular conditions. Experimental identification of enhancers is constrained by the set of conditions used in the experiment. This requires multiple experiments to identify enhancers, as they can be active under specific cellular conditions but not in different cell types/tissues or cellular states. This has opened prospects for computational prediction methods that can be used for high-throughput identification of putative enhancers to complement experimental approaches. Potential functions and properties of predicted enhancers have been catalogued and summarized in several enhancer-oriented databases. Because the current methods for the computational prediction of enhancers produce significantly different enhancer predictions, it will be beneficial for the research community to have an overview of the strategies and solutions developed in this field. In this review, we focus on the identification and analysis of enhancers by bioinformatics approaches. First, we describe a general framework for computational identification of enhancers, present relevant data types and discuss possible computational solutions. Next, we cover over 30 existing computational enhancer identification methods that were developed since 2000. Our review highlights advantages, limitations and potentials, while suggesting pragmatic guidelines for development of more efficient computational enhancer prediction methods. Finally, we discuss challenges and open problems of this topic, which require further consideration."
26626626,29.0,Linking Essential Tremor to the Cerebellum-Neuroimaging Evidence,2016 Jun;15(3):263-75.,"Essential tremor (ET) is the most common pathological tremor disorder in the world, and post-mortem evidence has shown that the cerebellum is the most consistent area of pathology in ET. In the last few years, advanced neuroimaging has tried to confirm this evidence. The aim of the present review is to discuss to what extent the evidence provided by this field of study may be generalised. We performed a systematic literature search combining the terms ET with the following keywords: MRI, VBM, MRS, DTI, fMRI, PET and SPECT. We summarised and discussed each study and placed the results in the context of existing knowledge regarding the cerebellar involvement in ET. A total of 51 neuroimaging studies met our search criteria, roughly divided into 19 structural and 32 functional studies. Despite clinical and methodological differences, both functional and structural imaging studies showed similar findings but without defining a clear topography of neurodegeneration. Indeed, the vast majority of studies found functional and structural abnormalities in several parts of the anterior and posterior cerebellar lobules, but it remains to be established to what degree these neural changes contribute to clinical symptoms of ET. Currently, advanced neuroimaging has confirmed the involvement of the cerebellum in pathophysiological processes of ET, although a high variability in results persists. For this reason, the translation of this knowledge into daily clinical practice is again partially limited, although new advanced multivariate neuroimaging approaches (machine-learning) are proving interesting changes of perspective."
26592808,22.0,Group-regularized individual prediction: theory and application to pain,2017 Jan 15;145(Pt B):274-287.,"Multivariate pattern analysis (MVPA) has become an important tool for identifying brain representations of psychological processes and clinical outcomes using fMRI and related methods. Such methods can be used to predict or 'decode' psychological states in individual subjects. Single-subject MVPA approaches, however, are limited by the amount and quality of individual-subject data. In spite of higher spatial resolution, predictive accuracy from single-subject data often does not exceed what can be accomplished using coarser, group-level maps, because single-subject patterns are trained on limited amounts of often-noisy data. Here, we present a method that combines population-level priors, in the form of biomarker patterns developed on prior samples, with single-subject MVPA maps to improve single-subject prediction. Theoretical results and simulations motivate a weighting based on the relative variances of biomarker-based prediction-based on population-level predictive maps from prior groups-and individual-subject, cross-validated prediction. Empirical results predicting pain using brain activity on a trial-by-trial basis (single-trial prediction) across 6 studies (N=180 participants) confirm the theoretical predictions. Regularization based on a population-level biomarker-in this case, the Neurologic Pain Signature (NPS)-improved single-subject prediction accuracy compared with idiographic maps based on the individuals' data alone. The regularization scheme that we propose, which we term group-regularized individual prediction (GRIP), can be applied broadly to within-person MVPA-based prediction. We also show how GRIP can be used to evaluate data quality and provide benchmarks for the appropriateness of population-level maps like the NPS for a given individual or study."
26587051,,An Overview of Biomolecular Event Extraction from Scientific Documents,2015;2015:571381.,"This paper presents a review of state-of-the-art approaches to automatic extraction of biomolecular events from scientific texts. Events involving biomolecules such as genes, transcription factors, or enzymes, for example, have a central role in biological processes and functions and provide valuable information for describing physiological and pathogenesis mechanisms. Event extraction from biomedical literature has a broad range of applications, including support for information retrieval, knowledge summarization, and information extraction and discovery. However, automatic event extraction is a challenging task due to the ambiguity and diversity of natural language and higher-level linguistic phenomena, such as speculations and negations, which occur in biological texts and can lead to misunderstanding or incorrect interpretation. Many strategies have been proposed in the last decade, originating from different research areas such as natural language processing, machine learning, and statistics. This review summarizes the most representative approaches in biomolecular event extraction and presents an analysis of the current state of the art and of commonly used methods, features, and tools. Finally, current research trends and future perspectives are also discussed."
26581149,48.0,Machine Learning Approaches for Predicting Radiation Therapy Outcomes: A Clinician's Perspective,2015 Dec 1;93(5):1127-35.,"Radiation oncology has always been deeply rooted in modeling, from the early days of isoeffect curves to the contemporary Quantitative Analysis of Normal Tissue Effects in the Clinic (QUANTEC) initiative. In recent years, medical modeling for both prognostic and therapeutic purposes has exploded thanks to increasing availability of electronic data and genomics. One promising direction that medical modeling is moving toward is adopting the same machine learning methods used by companies such as Google and Facebook to combat disease. Broadly defined, machine learning is a branch of computer science that deals with making predictions from complex data through statistical models. These methods serve to uncover patterns in data and are actively used in areas such as speech recognition, handwriting recognition, face recognition, ""spam"" filtering (junk email), and targeted advertising. Although multiple radiation oncology research groups have shown the value of applied machine learning (ML), clinical adoption has been slow due to the high barrier to understanding these complex models by clinicians. Here, we present a review of the use of ML to predict radiation therapy outcomes from the clinician's point of view with the hope that it lowers the ""barrier to entry"" for those without formal training in ML. We begin by describing 7 principles that one should consider when evaluating (or creating) an ML model in radiation oncology. We next introduce 3 popular ML methods--logistic regression (LR), support vector machine (SVM), and artificial neural network (ANN)--and critique 3 seminal papers in the context of these principles. Although current studies are in exploratory stages, the overall methodology has progressively matured, and the field is ready for larger-scale further investigation."
26579514,9.0,Prediction of Genetic Interactions Using Machine Learning and Network Properties,2015 Oct 26;3:172.,"A genetic interaction (GI) is a type of interaction where the effect of one gene is modified by the effect of one or several other genes. These interactions are important for delineating functional relationships among genes and their corresponding proteins, as well as elucidating complex biological processes and diseases. An important type of GI - synthetic sickness or synthetic lethality - involves two or more genes, where the loss of either gene alone has little impact on cell viability, but the combined loss of all genes leads to a severe decrease in fitness (sickness) or cell death (lethality). The identification of GIs is an important problem for it can help delineate pathways, protein complexes, and regulatory dependencies. Synthetic lethal interactions have important clinical and biological significance, such as providing therapeutically exploitable weaknesses in tumors. While near systematic high-content screening for GIs is possible in single cell organisms such as yeast, the systematic discovery of GIs is extremely difficult in mammalian cells. Therefore, there is a great need for computational approaches to reliably predict GIs, including synthetic lethal interactions, in these organisms. Here, we review the state-of-the-art approaches, strategies, and rigorous evaluation methods for learning and predicting GIs, both under general (healthy/standard laboratory) conditions and under specific contexts, such as diseases."
26572668,299.0,Machine Learning in Medicine,2015 Nov 17;132(20):1920-30.,"Spurred by advances in processing power, memory, storage, and an unprecedented wealth of data, computers are being asked to tackle increasingly complex learning tasks, often with astonishing success. Computers have now mastered a popular variant of poker, learned the laws of physics from experimental data, and become experts in video games - tasks that would have been deemed impossible not too long ago. In parallel, the number of companies centered on applying complex data analysis to varying industries has exploded, and it is thus unsurprising that some analytic companies are turning attention to problems in health care. The purpose of this review is to explore what problems in medicine might benefit from such learning approaches and use examples from the literature to introduce basic concepts in machine learning. It is important to note that seemingly large enough medical data sets and adequate learning algorithms have been available for many decades, and yet, although there are thousands of papers applying machine learning algorithms to medical data, very few have contributed meaningfully to clinical care. This lack of impact stands in stark contrast to the enormous relevance of machine learning to many other industries. Thus, part of my effort will be to identify what obstacles there may be to changing the practice of medicine through statistical learning approaches, and discuss how these might be overcome."
26567735,23.0,Frontiers for the Early Diagnosis of AD by Means of MRI Brain Imaging and Support Vector Machines,2016;13(5):509-33.,"The emergence of Alzheimer's Disease (AD) as a consequence of increasing aging population makes urgent the availability of methods for the early and accurate diagnosis. Magnetic Resonance Imaging (MRI) could be used as in vivo, non invasive tool to identify sensitive and specific markers of very early AD progression. In recent years, multivariate pattern analysis (MVPA) and machine- learning algorithms have attracted strong interest within the neuroimaging community, as they allow automatic classification of imaging data with higher performance than univariate statistical analysis. An exhaustive search of PubMed, Web of Science and Medline records was performed in this work, in order to retrieve studies focused on the potential role of MRI in aiding the clinician in early diagnosis of AD by using Support Vector Machines (SVMs) as MVPA automated classification method. A total of 30 studies emerged, published from 2008 to date. This review aims to give a state-of-the-art overview about SVM for the early and differential diagnosis of AD-related pathologies by means of MRI data, starting from preliminary steps such as image pre-processing, feature extraction and feature selection, and ending with classification, validation strategies and extraction of MRI-related biomarkers. The main advantages and drawbacks of the different techniques were explored. Results obtained by the reviewed studies were reported in terms of classification performance and biomarker outcomes, in order to shed light on the parameters that accompany normal and pathological aging. Unresolved issues and possible future directions were finally pointed out."
26566461,8.0,Atopic Dermatitis and Respiratory Allergy: What is the Link,2015;4(4):221-227.,"Understanding the aetiology and progression of atopic dermatitis and respiratory allergy may elucidate early preventative and management strategies aimed towards reducing the global burden of asthma and allergic disease. In this article, we review the current opinion concerning the link between atopic dermatitis and the subsequent progression of respiratory allergies during childhood and into early adolescence. Advances in machine learning and statistical methodology have facilitated the discovery of more refined definitions of phenotypes for identifying biomarkers. Understanding the role of atopic dermatitis in the development of respiratory allergy may ultimately allow us to determine more effective treatment strategies, thus reducing the patient and economic burden associated with these conditions."
26556470,2.0,Applicability of gene expression and systems biology to develop pharmacogenetic predictors; antipsychotic-induced extrapyramidal symptoms as an example,2015 Nov;16(17):1975-88.,"Pharmacogenetics has been driven by a candidate gene approach. The disadvantage of this approach is that is limited by our current understanding of the mechanisms by which drugs act. Gene expression could help to elucidate the molecular signatures of antipsychotic treatments searching for dysregulated molecular pathways and the relationships between gene products, especially protein-protein interactions. To embrace the complexity of drug response, machine learning methods could help to identify gene-gene interactions and develop pharmacogenetic predictors of drug response. The present review summarizes the applicability of the topics presented here (gene expression, network analysis and gene-gene interactions) in pharmacogenetics. In order to achieve this, we present an example of identifying genetic predictors of extrapyramidal symptoms induced by antipsychotic."
26540053,24.0,Computational Prediction of RNA-Binding Proteins and Binding Sites,2015 Nov 3;16(11):26303-17.,"Proteins and RNA interaction have vital roles in many cellular processes such as protein synthesis, sequence encoding, RNA transfer, and gene regulation at the transcriptional and post-transcriptional levels. Approximately 6%-8% of all proteins are RNA-binding proteins (RBPs). Distinguishing these RBPs or their binding residues is a major aim of structural biology. Previously, a number of experimental methods were developed for the determination of protein-RNA interactions. However, these experimental methods are expensive, time-consuming, and labor-intensive. Alternatively, researchers have developed many computational approaches to predict RBPs and protein-RNA binding sites, by combining various machine learning methods and abundant sequence and/or structural features. There are three kinds of computational approaches, which are prediction from protein sequence, prediction from protein structure, and protein-RNA docking. In this paper, we review all existing studies of predictions of RNA-binding sites and RBPs and complexes, including data sets used in different approaches, sequence and structural features used in several predictors, prediction method classifications, performance comparisons, evaluation methods, and future directions."
26512199,20.0,Machine Learning Methods for Predicting HLA-Peptide Binding Activity,2015 Oct 11;9(Suppl 3):21-9.,"As major histocompatibility complexes in humans, the human leukocyte antigens (HLAs) have important functions to present antigen peptides onto T-cell receptors for immunological recognition and responses. Interpreting and predicting HLA-peptide binding are important to study T-cell epitopes, immune reactions, and the mechanisms of adverse drug reactions. We review different types of machine learning methods and tools that have been used for HLA-peptide binding prediction. We also summarize the descriptors based on which the HLA-peptide binding prediction models have been constructed and discuss the limitation and challenges of the current methods. Lastly, we give a future perspective on the HLA-peptide binding prediction method based on network analysis."
26511342,5.0,"Computer-Aided Optimization of Combined Anti-Retroviral Therapy for HIV: New Drugs, New Drug Targets and Drug Resistance",2016;14(2):101-9.,"Background:                    Resistance to antiretroviral drugs is a complex and evolving area with relevant implications in the treatment of human immunodeficiency virus (HIV) infection. Several rules, algorithms and full-fledged computer programs have been developed to assist the HIV specialist in the choice of the best patient-tailored therapy.              Methods:                    Experts' rules and statistical/machine learning algorithms for interpreting HIV drug resistance, along with their program implementations, were retrieved from PubMed and other on-line resources to be critically reviewed in terms of technical approach, performance, usability, update, and evolution (i.e. inclusion of novel drugs or expansion to other viral agents).              Results:                    Several drug resistance prediction algorithms for the nucleotide/nucleoside/non-nucleoside reverse transcriptase, protease and integrase inhibitors as well as coreceptor antagonists are currently available, routinely used, and have been validated thoroughly in independent studies. Computer tools that combine single-drug genotypic/phenotypic resistance interpretation and optimize combination antiretroviral therapy have been also developed and implemented as web applications. Most of the systems have been updated timely to incorporate new drugs and few have recently been expanded to meet a similar need in the Hepatitis C area. Prototype systems aiming at predicting virological response from both virus and patient indicators have been recently developed but they are not yet being routinely used.              Conclusion:                    Computing HIV genotype to predict drug susceptibility in vitro or response to combination antiretroviral therapy in vivo is a continuous and productive research field, translating into successful treatment decision support tools, an essential component of the management of HIV patients."
26509168,5.0,A Review of Computational Methods to Predict the Risk of Rupture of Abdominal Aortic Aneurysms,2015;2015:861627.,"Computational methods have played an important role in health care in recent years, as determining parameters that affect a certain medical condition is not possible in experimental conditions in many cases. Computational fluid dynamics (CFD) methods have been used to accurately determine the nature of blood flow in the cardiovascular and nervous systems and air flow in the respiratory system, thereby giving the surgeon a diagnostic tool to plan treatment accordingly. Machine learning or data mining (MLD) methods are currently used to develop models that learn from retrospective data to make a prediction regarding factors affecting the progression of a disease. These models have also been successful in incorporating factors such as patient history and occupation. MLD models can be used as a predictive tool to determine rupture potential in patients with abdominal aortic aneurysms (AAA) along with CFD-based prediction of parameters like wall shear stress and pressure distributions. A combination of these computer methods can be pivotal in bridging the gap between translational and outcomes research in medicine. This paper reviews the use of computational methods in the diagnosis and treatment of AAA."
26502306,15.0,Diffusion Tensor Imaging of TBI: Potentials and Challenges,2015 Oct;24(5):241-51.,"Neuroimaging plays a critical role in the setting in traumatic brain injury (TBI). Diffusion tensor imaging (DTI) is an advanced magnetic resonance imaging technique that is capable of providing rich information on the brain's neuroanatomic connectome. The purpose of this article is to systematically review the role of DTI and advanced diffusion techniques in the setting of TBI, including diffusion kurtosis imaging (DKI), neurite orientation dispersion and density imaging, diffusion spectrum imaging, and q-ball imaging. We discuss clinical applications of DTI and review the DTI literature as it pertains to TBI. Despite the continued advancements in DTI and related diffusion techniques over the past 20 years, DTI techniques are sensitive for TBI at the group level only and there is insufficient evidence that DTI plays a role at the individual level. We conclude by discussing future directions in DTI research in TBI including the role of machine learning in the pattern classification of TBI."
26499213,9.0,The identification of cis-regulatory elements: A review from a machine learning perspective,2015 Dec;138:6-17.,"The majority of the human genome consists of non-coding regions that have been called junk DNA. However, recent studies have unveiled that these regions contain cis-regulatory elements, such as promoters, enhancers, silencers, insulators, etc. These regulatory elements can play crucial roles in controlling gene expressions in specific cell types, conditions, and developmental stages. Disruption to these regions could contribute to phenotype changes. Precisely identifying regulatory elements is key to deciphering the mechanisms underlying transcriptional regulation. Cis-regulatory events are complex processes that involve chromatin accessibility, transcription factor binding, DNA methylation, histone modifications, and the interactions between them. The development of next-generation sequencing techniques has allowed us to capture these genomic features in depth. Applied analysis of genome sequences for clinical genetics has increased the urgency for detecting these regions. However, the complexity of cis-regulatory events and the deluge of sequencing data require accurate and efficient computational approaches, in particular, machine learning techniques. In this review, we describe machine learning approaches for predicting transcription factor binding sites, enhancers, and promoters, primarily driven by next-generation sequencing data. Data sources are provided in order to facilitate testing of novel methods. The purpose of this review is to attract computational experts and data scientists to advance this field."
26484733,7.0,Building cell models and simulations from microscope images,2016 Mar 1;96:33-39.,"The use of fluorescence microscopy has undergone a major revolution over the past twenty years, both with the development of dramatic new technologies and with the widespread adoption of image analysis and machine learning methods. Many open source software tools provide the ability to use these methods in a wide range of studies, and many molecular and cellular phenotypes can now be automatically distinguished. This article presents the next major challenge in microscopy automation, the creation of accurate models of cell organization directly from images, and reviews the progress that has been made towards this challenge."
26468133,12.0,Toward Big Data Analytics: Review of Predictive Models in Management of Diabetes and Its Complications,2015 Oct 14;10(1):27-34.,"Diabetes is one of the top priorities in medical science and health care management, and an abundance of data and information is available on these patients. Whether data stem from statistical models or complex pattern recognition models, they may be fused into predictive models that combine patient information and prognostic outcome results. Such knowledge could be used in clinical decision support, disease surveillance, and public health management to improve patient care. Our aim was to review the literature and give an introduction to predictive models in screening for and the management of prevalent short- and long-term complications in diabetes. Predictive models have been developed for management of diabetes and its complications, and the number of publications on such models has been growing over the past decade. Often multiple logistic or a similar linear regression is used for prediction model development, possibly owing to its transparent functionality. Ultimately, for prediction models to prove useful, they must demonstrate impact, namely, their use must generate better patient outcomes. Although extensive effort has been put in to building these predictive models, there is a remarkable scarcity of impact studies."
26460190,32.0,Computational prediction of protein interfaces: A review of data driven methods,2015 Nov 30;589(23):3516-26.,"Reliably pinpointing which specific amino acid residues form the interface(s) between a protein and its binding partner(s) is critical for understanding the structural and physicochemical determinants of protein recognition and binding affinity, and has wide applications in modeling and validating protein interactions predicted by high-throughput methods, in engineering proteins, and in prioritizing drug targets. Here, we review the basic concepts, principles and recent advances in computational approaches to the analysis and prediction of protein-protein interfaces. We point out caveats for objectively evaluating interface predictors, and discuss various applications of data-driven interface predictors for improving energy model-driven protein-protein docking. Finally, we stress the importance of exploiting binding partner information in reliably predicting interfaces and highlight recent advances in this emerging direction."
26450107,13.0,Surgical robotics beyond enhanced dexterity instrumentation: a survey of machine learning techniques and their role in intelligent and autonomous surgical actions,2016 Apr;11(4):553-68.,"Purpose:                    Advances in technology and computing play an increasingly important role in the evolution of modern surgical techniques and paradigms. This article reviews the current role of machine learning (ML) techniques in the context of surgery with a focus on surgical robotics (SR). Also, we provide a perspective on the future possibilities for enhancing the effectiveness of procedures by integrating ML in the operating room.              Methods:                    The review is focused on ML techniques directly applied to surgery, surgical robotics, surgical training and assessment. The widespread use of ML methods in diagnosis and medical image computing is beyond the scope of the review. Searches were performed on PubMed and IEEE Explore using combinations of keywords: ML, surgery, robotics, surgical and medical robotics, skill learning, skill analysis and learning to perceive.              Results:                    Studies making use of ML methods in the context of surgery are increasingly being reported. In particular, there is an increasing interest in using ML for developing tools to understand and model surgical skill and competence or to extract surgical workflow. Many researchers begin to integrate this understanding into the control of recent surgical robots and devices.              Conclusion:                    ML is an expanding field. It is popular as it allows efficient processing of vast amounts of data for interpreting and real-time decision making. Already widely used in imaging and diagnosis, it is believed that ML will also play an important role in surgery and interventional treatments. In particular, ML could become a game changer into the conception of cognitive surgical robots. Such robots endowed with cognitive skills would assist the surgical team also on a cognitive level, such as possibly lowering the mental load of the team. For example, ML could help extracting surgical skill, learned through demonstration by human experts, and could transfer this to robotic skills. Such intelligent surgical assistance would significantly surpass the state of the art in surgical robotics. Current devices possess no intelligence whatsoever and are merely advanced and expensive instruments."
26442199,26.0,"Rare disease diagnosis: A review of web search, social media and large-scale data-mining approaches",2015 Sep 16;3(1):e1083145.,"Physicians and the general public are increasingly using web-based tools to find answers to medical questions. The field of rare diseases is especially challenging and important as shown by the long delay and many mistakes associated with diagnoses. In this paper we review recent initiatives on the use of web search, social media and data mining in data repositories for medical diagnosis. We compare the retrieval accuracy on 56 rare disease cases with known diagnosis for the web search tools google.com, pubmed.gov, omim.org and our own search tool findzebra.com. We give a detailed description of IBM's Watson system and make a rough comparison between findzebra.com and Watson on subsets of the Doctor's dilemma dataset. The recall@10 and recall@20 (fraction of cases where the correct result appears in top 10 and top 20) for the 56 cases are found to be be 29%, 16%, 27% and 59% and 32%, 18%, 34% and 64%, respectively. Thus, FindZebra has a significantly (p < 0.01) higher recall than the other 3 search engines. When tested under the same conditions, Watson and FindZebra showed similar recall@10 accuracy. However, the tests were performed on different subsets of Doctors dilemma questions. Advances in technology and access to high quality data have opened new possibilities for aiding the diagnostic process. Specialized search engines, data mining tools and social media are some of the areas that hold promise."
26442103,24.0,A survey about methods dedicated to epistasis detection,2015 Sep 10;6:285.,"During the past decade, findings of genome-wide association studies (GWAS) improved our knowledge and understanding of disease genetics. To date, thousands of SNPs have been associated with diseases and other complex traits. Statistical analysis typically looks for association between a phenotype and a SNP taken individually via single-locus tests. However, geneticists admit this is an oversimplified approach to tackle the complexity of underlying biological mechanisms. Interaction between SNPs, namely epistasis, must be considered. Unfortunately, epistasis detection gives rise to analytic challenges since analyzing every SNP combination is at present impractical at a genome-wide scale. In this review, we will present the main strategies recently proposed to detect epistatic interactions, along with their operating principle. Some of these methods are exhaustive, such as multifactor dimensionality reduction, likelihood ratio-based tests or receiver operating characteristic curve analysis; some are non-exhaustive, such as machine learning techniques (random forests, Bayesian networks) or combinatorial optimization approaches (ant colony optimization, computational evolution system)."
26442053,6.0,Crop improvement using life cycle datasets acquired under field conditions,2015 Sep 22;6:740.,"Crops are exposed to various environmental stresses in the field throughout their life cycle. Modern plant science has provided remarkable insights into the molecular networks of plant stress responses in laboratory conditions, but the responses of different crops to environmental stresses in the field need to be elucidated. Recent advances in omics analytical techniques and information technology have enabled us to integrate data from a spectrum of physiological metrics of field crops. The interdisciplinary efforts of plant science and data science enable us to explore factors that affect crop productivity and identify stress tolerance-related genes and alleles. Here, we describe recent advances in technologies that are key components for data driven crop design, such as population genomics, chronological omics analyses, and computer-aided molecular network prediction. Integration of the outcomes from these technologies will accelerate our understanding of crop phenology under practical field situations and identify key characteristics to represent crop stress status. These elements would help us to genetically engineer ""designed crops"" to prevent yield shortfalls because of environmental fluctuations due to future climate change."
26429153,15.0,Identification of drug candidates and repurposing opportunities through compound-target interaction networks,2015 Dec;10(12):1333-45.,"Introduction:                    System-wide identification of both on- and off-targets of chemical probes provides improved understanding of their therapeutic potential and possible adverse effects, thereby accelerating and de-risking drug discovery process. Given the high costs of experimental profiling of the complete target space of drug-like compounds, computational models offer systematic means for guiding these mapping efforts. These models suggest the most potent interactions for further experimental or pre-clinical evaluation both in cell line models and in patient-derived material.              Areas covered:                    The authors focus here on network-based machine learning models and their use in the prediction of novel compound-target interactions both in target-based and phenotype-based drug discovery applications. While currently being used mainly in complementing the experimentally mapped compound-target networks for drug repurposing applications, such as extending the target space of already approved drugs, these network pharmacology approaches may also suggest completely unexpected and novel investigational probes for drug development.              Expert opinion:                    Although the studies reviewed here have already demonstrated that network-centric modeling approaches have the potential to identify candidate compounds and selective targets in disease networks, many challenges still remain. In particular, these challenges include how to incorporate the cellular context and genetic background into the disease networks to enable more stratified and selective target predictions, as well as how to make the prediction models more realistic for the practical drug discovery and therapeutic applications."
26411473,14.0,Correct machine learning on protein sequences: a peer-reviewing perspective,2016 Sep;17(5):831-40.,"Machine learning methods are becoming increasingly popular to predict protein features from sequences. Machine learning in bioinformatics can be powerful but carries also the risk of introducing unexpected biases, which may lead to an overestimation of the performance. This article espouses a set of guidelines to allow both peer reviewers and authors to avoid common machine learning pitfalls. Understanding biology is necessary to produce useful data sets, which have to be large and diverse. Separating the training and test process is imperative to avoid over-selling method performance, which is also dependent on several hidden parameters. A novel predictor has always to be compared with several existing methods, including simple baseline strategies. Using the presented guidelines will help nonspecialists to appreciate the critical issues in machine learning."
26386547,4.0,Mid-level image representations for real-time heart view plane classification of echocardiograms,2015 Nov 1;66:66-81.,"In this paper, we explore mid-level image representations for real-time heart view plane classification of 2D echocardiogram ultrasound images. The proposed representations rely on bags of visual words, successfully used by the computer vision community in visual recognition problems. An important element of the proposed representations is the image sampling with large regions, drastically reducing the execution time of the image characterization procedure. Throughout an extensive set of experiments, we evaluate the proposed approach against different image descriptors for classifying four heart view planes. The results show that our approach is effective and efficient for the target problem, making it suitable for use in real-time setups. The proposed representations are also robust to different image transformations, e.g., downsampling, noise filtering, and different machine learning classifiers, keeping classification accuracy above 90%. Feature extraction can be performed in 30 fps or 60 fps in some cases. This paper also includes an in-depth review of the literature in the area of automatic echocardiogram view classification giving the reader a through comprehension of this field of study."
26358759,5.0,Bioinformatics approaches to single-cell analysis in developmental biology,2016 Mar;22(3):182-92.,"Individual cells within the same population show various degrees of heterogeneity, which may be better handled with single-cell analysis to address biological and clinical questions. Single-cell analysis is especially important in developmental biology as subtle spatial and temporal differences in cells have significant associations with cell fate decisions during differentiation and with the description of a particular state of a cell exhibiting an aberrant phenotype. Biotechnological advances, especially in the area of microfluidics, have led to a robust, massively parallel and multi-dimensional capturing, sorting, and lysis of single-cells and amplification of related macromolecules, which have enabled the use of imaging and omics techniques on single cells. There have been improvements in computational single-cell image analysis in developmental biology regarding feature extraction, segmentation, image enhancement and machine learning, handling limitations of optical resolution to gain new perspectives from the raw microscopy images. Omics approaches, such as transcriptomics, genomics and epigenomics, targeting gene and small RNA expression, single nucleotide and structural variations and methylation and histone modifications, rely heavily on high-throughput sequencing technologies. Although there are well-established bioinformatics methods for analysis of sequence data, there are limited bioinformatics approaches which address experimental design, sample size considerations, amplification bias, normalization, differential expression, coverage, clustering and classification issues, specifically applied at the single-cell level. In this review, we summarize biological and technological advancements, discuss challenges faced in the aforementioned data acquisition and analysis issues and present future prospects for application of single-cell analyses to developmental biology."
26358617,18.0,Quantitative structure-activity relationship: promising advances in drug discovery platforms,2015 Dec;10(12):1283-300.,"Introduction:                    Quantitative structure-activity relationship (QSAR) modeling is one of the most popular computer-aided tools employed in medicinal chemistry for drug discovery and lead optimization. It is especially powerful in the absence of 3D structures of specific drug targets. QSAR methods have been shown to draw public attention since they were first introduced.              Areas covered:                    In this review, the authors provide a brief discussion of the basic principles of QSAR, model development and model validation. They also highlight the current applications of QSAR in different fields, particularly in virtual screening, rational drug design and multi-target QSAR. Finally, in view of recent controversies, the authors detail the challenges faced by QSAR modeling and the relevant solutions. The aim of this review is to show how QSAR modeling can be applied in novel drug discovery, design and lead optimization.              Expert opinion:                    QSAR should intentionally be used as a powerful tool for fragment-based drug design platforms in the field of drug discovery and design. Although there have been an increasing number of experimentally determined protein structures in recent years, a great number of protein structures cannot be easily obtained (i.e., membrane transport proteins and G-protein coupled receptors). Fragment-based drug discovery, such as QSAR, could be applied further and have a significant role in dealing with these problems. Moreover, along with the development of computer software and hardware, it is believed that QSAR will be increasingly important."
26346869,,Machine learning and statistical methods for the prediction of maximal oxygen uptake: recent advances,2015 Aug 27;8:369-79.,"Maximal oxygen uptake (VO2max) indicates how many milliliters of oxygen the body can consume in a state of intense exercise per minute. VO2max plays an important role in both sport and medical sciences for different purposes, such as indicating the endurance capacity of athletes or serving as a metric in estimating the disease risk of a person. In general, the direct measurement of VO2max provides the most accurate assessment of aerobic power. However, despite a high level of accuracy, practical limitations associated with the direct measurement of VO2max, such as the requirement of expensive and sophisticated laboratory equipment or trained staff, have led to the development of various regression models for predicting VO2max. Consequently, a lot of studies have been conducted in the last years to predict VO2max of various target audiences, ranging from soccer athletes, nonexpert swimmers, cross-country skiers to healthy-fit adults, teenagers, and children. Numerous prediction models have been developed using different sets of predictor variables and a variety of machine learning and statistical methods, including support vector machine, multilayer perceptron, general regression neural network, and multiple linear regression. The purpose of this study is to give a detailed overview about the data-driven modeling studies for the prediction of VO2max conducted in recent years and to compare the performance of various VO2max prediction models reported in related literature in terms of two well-known metrics, namely, multiple correlation coefficient (R) and standard error of estimate. The survey results reveal that with respect to regression methods used to develop prediction models, support vector machine, in general, shows better performance than other methods, whereas multiple linear regression exhibits the worst performance."
26298193,1.0,Learning with hidden variables,2015 Dec;35:110-8.,"Learning and inferring features that generate sensory input is a task continuously performed by cortex. In recent years, novel algorithms and learning rules have been proposed that allow neural network models to learn such features from natural images, written text, audio signals, etc. These networks usually involve deep architectures with many layers of hidden neurons. Here we review recent advancements in this area emphasizing, amongst other things, the processing of dynamical inputs by networks with hidden nodes and the role of single neuron models. These points and the questions they arise can provide conceptual advancements in understanding of learning in the cortex and the relationship between machine learning approaches to learning with hidden nodes and those in cortical circuits."
26293849,7.0,Health Informatics via Machine Learning for the Clinical Management of Patients,2015 Aug 13;10(1):38-43.,"Objectives:                    To review how health informatics systems based on machine learning methods have impacted the clinical management of patients, by affecting clinical practice.              Methods:                    We reviewed literature from 2010-2015 from databases such as Pubmed, IEEE xplore, and INSPEC, in which methods based on machine learning are likely to be reported. We bring together a broad body of literature, aiming to identify those leading examples of health informatics that have advanced the methodology of machine learning. While individual methods may have further examples that might be added, we have chosen some of the most representative, informative exemplars in each case.              Results:                    Our survey highlights that, while much research is taking place in this high-profile field, examples of those that affect the clinical management of patients are seldom found. We show that substantial progress is being made in terms of methodology, often by data scientists working in close collaboration with clinical groups.              Conclusions:                    Health informatics systems based on machine learning are in their infancy and the translation of such systems into clinical management has yet to be performed at scale."
26286210,1.0,Prediction and Dissection of Protein-RNA Interactions by Molecular Descriptors,2016;16(6):604-15.,"Protein-RNA interactions play crucial roles in numerous biological processes. However, detecting the interactions and binding sites between protein and RNA by traditional experiments is still time consuming and labor costing. Thus, it is of importance to develop bioinformatics methods for predicting protein-RNA interactions and binding sites. Accurate prediction of protein-RNA interactions and recognitions will highly benefit to decipher the interaction mechanisms between protein and RNA, as well as to improve the RNA-related protein engineering and drug design. In this work, we summarize the current bioinformatics strategies of predicting protein-RNA interactions and dissecting protein-RNA interaction mechanisms from local structure binding motifs. In particular, we focus on the feature-based machine learning methods, in which the molecular descriptors of protein and RNA are extracted and integrated as feature vectors of representing the interaction events and recognition residues. In addition, the available methods are classified and compared comprehensively. The molecular descriptors are expected to elucidate the binding mechanisms of protein-RNA interaction and reveal the functional implications from structural complementary perspective."
26283676,121.0,"Drug-target interaction prediction: databases, web servers and computational models",2016 Jul;17(4):696-712.,"Identification of drug-target interactions is an important process in drug discovery. Although high-throughput screening and other biological assays are becoming available, experimental methods for drug-target interaction identification remain to be extremely costly, time-consuming and challenging even nowadays. Therefore, various computational models have been developed to predict potential drug-target associations on a large scale. In this review, databases and web servers involved in drug-target identification and drug discovery are summarized. In addition, we mainly introduced some state-of-the-art computational models for drug-target interactions prediction, including network-based method, machine learning-based method and so on. Specially, for the machine learning-based method, much attention was paid to supervised and semi-supervised models, which have essential difference in the adoption of negative samples. Although significant improvements for drug-target interaction prediction have been obtained by many effective computational models, both network-based and machine learning-based methods have their disadvantages, respectively. Furthermore, we discuss the future directions of the network-based drug discovery and network approach for personalized drug discovery based on personalized medicine, genome sequencing, tumor clone-based network and cancer hallmark-based network. Finally, we discussed the new evaluation validation framework and the formulation of drug-target interactions prediction problem by more realistic regression formulation based on quantitative bioactivity data."
26263424,1.0,Data mining and education,Jul-Aug 2015;6(4):333-353.,"An emerging field of educational data mining (EDM) is building on and contributing to a wide variety of disciplines through analysis of data coming from various educational technologies. EDM researchers are addressing questions of cognition, metacognition, motivation, affect, language, social discourse, etc. using data from intelligent tutoring systems, massive open online courses, educational games and simulations, and discussion forums. The data include detailed action and timing logs of student interactions in user interfaces such as graded responses to questions or essays, steps in rich problem solving environments, games or simulations, discussion forum posts, or chat dialogs. They might also include external sensors such as eye tracking, facial expression, body movement, etc. We review how EDM has addressed the research questions that surround the psychology of learning with an emphasis on assessment, transfer of learning and model discovery, the role of affect, motivation and metacognition on learning, and analysis of language data and collaborative learning. For example, we discuss (1) how different statistical assessment methods were used in a data mining competition to improve prediction of student responses to intelligent tutor tasks, (2) how better cognitive models can be discovered from data and used to improve instruction, (3) how data-driven models of student affect can be used to focus discussion in a dialog-based tutoring system, and (4) how machine learning techniques applied to discussion data can be used to produce automated agents that support student learning as they collaborate in a chat room or a discussion board."
26256959,8.0,Cancer Cell Line Panels Empower Genomics-Based Discovery of Precision Cancer Medicine,2015 Sep;56(5):1186-98.,"Since the first human cancer cell line, HeLa, was established in the early 1950s, there has been a steady increase in the number and tumor type of available cancer cell line models. Cancer cell lines have made significant contributions to the development of various chemotherapeutic agents. Recent advances in multi-omics technologies have facilitated detailed characterizations of the genomic, transcriptomic, proteomic, and epigenomic profiles of these cancer cell lines. An increasing number of studies employ the power of a cancer cell line panel to provide predictive biomarkers for targeted and cytotoxic agents, including those that are already used in clinical practice. Different types of statistical and machine learning algorithms have been developed to analyze the large-scale data sets that have been produced. However, much work remains to address the discrepancies in drug assay results from different platforms and the frequent failures to translate discoveries from cell line models to the clinic. Nevertheless, continuous expansion of cancer cell line panels should provide unprecedented opportunities to identify new candidate targeted therapies, particularly for the so-called ""dark matter"" group of cancers, for which pharmacologically tractable driver mutations have not been identified."
26246834,8.0,Advances in Patient Classification for Traditional Chinese Medicine: A Machine Learning Perspective,2015;2015:376716.,"As a complementary and alternative medicine in medical field, traditional Chinese medicine (TCM) has drawn great attention in the domestic field and overseas. In practice, TCM provides a quite distinct methodology to patient diagnosis and treatment compared to western medicine (WM). Syndrome (ZHENG or pattern) is differentiated by a set of symptoms and signs examined from an individual by four main diagnostic methods: inspection, auscultation and olfaction, interrogation, and palpation which reflects the pathological and physiological changes of disease occurrence and development. Patient classification is to divide patients into several classes based on different criteria. In this paper, from the machine learning perspective, a survey on patient classification issue will be summarized on three major aspects of TCM: sign classification, syndrome differentiation, and disease classification. With the consideration of different diagnostic data analyzed by different computational methods, we present the overview for four subfields of TCM diagnosis, respectively. For each subfield, we design a rectangular reference list with applications in the horizontal direction and machine learning algorithms in the longitudinal direction. According to the current development of objective TCM diagnosis for patient classification, a discussion of the research issues around machine learning techniques with applications to TCM diagnosis is given to facilitate the further research for TCM patient classification."
26241930,,Automated negotiation in environmental resource management: Review and assessment,2015 Oct 1;162:148-57.,"Negotiation is an integral part of our daily life and plays an important role in resolving conflicts and facilitating human interactions. Automated negotiation, which aims at capturing the human negotiation process using artificial intelligence and machine learning techniques, is well-established in e-commerce, but its application in environmental resource management remains limited. This is due to the inherent uncertainties and complexity of environmental issues, along with the diversity of stakeholders' perspectives when dealing with these issues. The objective of this paper is to describe the main components of automated negotiation, review and compare machine learning techniques in automated negotiation, and provide a guideline for the selection of suitable methods in the particular context of stakeholders' negotiation over environmental resource issues. We advocate that automated negotiation can facilitate the involvement of stakeholders in the exploration of a plurality of solutions in order to reach a mutually satisfying agreement and contribute to informed decisions in environmental management along with the need for further studies to consolidate the potential of this modeling approach."
26237649,2.0,"Perspectives on Knowledge Discovery Algorithms Recently Introduced in Chemoinformatics: Rough Set Theory, Association Rule Mining, Emerging Patterns, and Formal Concept Analysis",2015 Sep 28;55(9):1781-803.,"Knowledge Discovery in Databases (KDD) refers to the use of methodologies from machine learning, pattern recognition, statistics, and other fields to extract knowledge from large collections of data, where the knowledge is not explicitly available as part of the database structure. In this paper, we describe four modern data mining techniques, Rough Set Theory (RST), Association Rule Mining (ARM), Emerging Pattern Mining (EP), and Formal Concept Analysis (FCA), and we have attempted to give an exhaustive list of their chemoinformatics applications. One of the main strengths of these methods is their descriptive ability. When used to derive rules, for example, in structure-activity relationships, the rules have clear physical meaning. This review has shown that there are close relationships between the methods. Often apparent differences lie in the way in which the problem under investigation has been formulated which can lead to the natural adoption of one or other method. For example, the idea of a structural alert, as a structure which is present in toxic and absent in nontoxic compounds, leads to the natural formulation of an Emerging Pattern search. Despite the similarities between the methods, each has its strengths. RST is useful for dealing with uncertain and noisy data. Its main chemoinformatics applications so far have been in feature extraction and feature reduction, the latter often as input to another data mining method, such as an Support Vector Machine (SVM). ARM has mostly been used for frequent subgraph mining. EP and FCA have both been used to mine both structural and nonstructural patterns for classification of both active and inactive molecules. Since their introduction in the 1980s and 1990s, RST, ARM, EP, and FCA have found wide-ranging applications, with many thousands of citations in Web of Science, but their adoption by the chemoinformatics community has been relatively slow. Advances, both in computer power and in algorithm development, mean that there is the potential to apply these techniques to larger data sets and thus to different problems in the future."
26234511,,Bio-AIMS Collection of Chemoinformatics Web Tools based on Molecular Graph Information and Artificial Intelligence Models,2015;18(8):735-50.,"The molecular information encoding into molecular descriptors is the first step into in silico Chemoinformatics methods in Drug Design. The Machine Learning methods are a complex solution to find prediction models for specific biological properties of molecules. These models connect the molecular structure information such as atom connectivity (molecular graphs) or physical-chemical properties of an atom/group of atoms to the molecular activity (Quantitative Structure - Activity Relationship, QSAR). Due to the complexity of the proteins, the prediction of their activity is a complicated task and the interpretation of the models is more difficult. The current review presents a series of 11 prediction models for proteins, implemented as free Web tools on an Artificial Intelligence Model Server in Biosciences, Bio-AIMS (http://bio-aims.udc.es/TargetPred.php). Six tools predict protein activity, two models evaluate drug - protein target interactions and the other three calculate protein - protein interactions. The input information is based on the protein 3D structure for nine models, 1D peptide amino acid sequence for three tools and drug SMILES formulas for two servers. The molecular graph descriptor-based Machine Learning models could be useful tools for in silico screening of new peptides/proteins as future drug targets for specific treatments."
26234510,2.0,Artificial Neural Network Methods Applied to Drug Discovery for Neglected Diseases,2015;18(8):819-29.,"Among the chemometric tools used in rational drug design, we find artificial neural network methods (ANNs), a statistical learning algorithm similar to the human brain, to be quite powerful. Some ANN applications use biological and molecular data of the training series that are inserted to ensure the machine learning, and to generate robust and predictive models. In drug discovery, researchers use this methodology, looking to find new chemotherapeutic agents for various diseases. The neglected diseases are a group of tropical parasitic diseases that primarily affect poor countries in Africa, Asia, and South America. Current drugs against these diseases cause side effects, are ineffective during the chronic stages of the disease, and are often not available to the needy population, have relative high toxicity, and face developing resistance. Faced with so many problems, new chemotherapeutic agents to treat these infections are much needed. The present review reports on neural network research, which studies new ligands against Chagas' disease, sleeping sickness, malaria, tuberculosis, and leishmaniasis; a few of the neglected diseases."
26233900,3.0,Machine learning in burn care and research: A systematic review of the literature,2015 Dec;41(8):1636-1641.,"Background:                    To date, there are no reviews on machine learning (ML) in burn care. Considering the growth of ML in medicine and the complexities and challenges of burn care, this review specializes on ML applications in burn care. The objective was to examine the features and impact of applications in targeting various aspects of burn care and research.              Methods:                    MEDLINE, the Cochrane Database of Systematic Reviews, ScienceDirect, and citation review of relevant primary and review articles were searched for studies involving burn care/research and machine learning. Data were abstracted on study design, study size, year, population, application of burn care/research, ML technique(s), and algorithm performance.              Results:                    15 retrospective observational studies involving burn patients met inclusion criteria. In total 5105 patients with acute thermal injury, 171 clinical burn wounds, 180 9-mer peptides, and 424 12-mer peptides were included in the studies. Studies focused on burn diagnosis (n=5), aminoglycoside response (n=3), hospital length of stay (n=2), survival/mortality (n=4), burn healing time (n=1), and antimicrobial peptides in burn patients (n=1). Of these 15 studies, 11 used artificial neural networks. Importantly, all studies demonstrated the benefits of ML in burn care/research and superior performance over traditional statistical methods. However, algorithm performance was assessed differently by different authors. Feature selection varied among studies, but studies with similar applications shared specific features including age, gender, presence of inhalation injury, total body surface area burned, and when available, various degrees of burns, infections, and previous histories/conditions of burn patients.              Conclusion:                    A common feature base may be determined for ML in burn care/research, but the impact of ML will require further validation in prospective observational studies and randomized clinical trials, establishment of common performance metrics, and high quality evidence about clinical and economic impacts. Only then can ML applications be advanced and accepted widely in burn care/research."
26233633,13.0,A Review on Carotid Ultrasound Atherosclerotic Tissue Characterization and Stroke Risk Stratification in Machine Learning Framework,2015 Sep;17(9):55.,"Cardiovascular diseases (including stroke and heart attack) are identified as the leading cause of death in today's world. However, very little is understood about the arterial mechanics of plaque buildup, arterial fibrous cap rupture, and the role of abnormalities of the vasa vasorum. Recently, ultrasonic echogenicity characteristics and morphological characterization of carotid plaque types have been shown to have clinical utility in classification of stroke risks. Furthermore, this characterization supports aggressive and intensive medical therapy as well as procedures, including endarterectomy and stenting. This is the first state-of-the-art review to provide a comprehensive understanding of the field of ultrasonic vascular morphology tissue characterization. This paper presents fundamental and advanced ultrasonic tissue characterization and feature extraction methods for analyzing plaque. Additionally, the paper shows how the risk stratification is achieved using machine learning paradigms. More advanced methods need to be developed which can segment the carotid artery walls into multiple regions such as the bulb region and areas both proximal and distal to the bulb. Furthermore, multimodality imaging is needed for validation of such advanced methods for stroke and cardiovascular risk stratification."
26225918,51.0,Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/UTHealth shared task Track 1,2015 Dec;58 Suppl(Suppl):S11-S19.,"The 2014 i2b2/UTHealth Natural Language Processing (NLP) shared task featured four tracks. The first of these was the de-identification track focused on identifying protected health information (PHI) in longitudinal clinical narratives. The longitudinal nature of clinical narratives calls particular attention to details of information that, while benign on their own in separate records, can lead to identification of patients in combination in longitudinal records. Accordingly, the 2014 de-identification track addressed a broader set of entities and PHI than covered by the Health Insurance Portability and Accountability Act - the focus of the de-identification shared task that was organized in 2006. Ten teams tackled the 2014 de-identification task and submitted 22 system outputs for evaluation. Each team was evaluated on their best performing system output. Three of the 10 systems achieved F1 scores over .90, and seven of the top 10 scored over .75. The most successful systems combined conditional random fields and hand-written rules. Our findings indicate that automated systems can be very effective for this task, but that de-identification is not yet a solved problem."
26215861,1.0,Genes Caught In Flagranti: Integrating Renal Transcriptional Profiles With Genotypes and Phenotypes,2015 May;35(3):237-44.,"In the past decade, population genetics has gained tremendous success in identifying genetic variations that are statistically relevant to renal diseases and kidney function. However, it is challenging to interpret the functional relevance of the genetic variations found by population genetics studies. In this review, we discuss studies that integrate multiple levels of data, especially transcriptome profiles and phenotype data, to assign functional roles of genetic variations involved in kidney function. Furthermore, we introduce state-of-the-art machine learning algorithms, Bayesian networks, support vector machines, and Gaussian process regression, which have been applied successfully to integrating genetic, regulatory, and clinical information to predict clinical outcomes. These methods are likely to be deployed successfully in the nephrology field in the near future."
26202261,,Prediction of Cell-Penetrating Peptides,2015;1324:39-58.,"The in silico methods for the prediction of the cell-penetrating peptides are reviewed. Those include the multivariate statistical methods, machine-learning methods such as the artificial neural networks and support vector machines, and molecular modeling techniques including molecular docking and molecular dynamics.The applicability of the methods is demonstrated on the basis of the exemplary cases from the literature."
26201875,115.0,Multi-atlas segmentation of biomedical images: A survey,2015 Aug;24(1):205-219.,"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, et al. (2004), Klein, et al. (2005), and Heckemann, et al. (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of ""atlases"" (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003-2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation."
26185243,227.0,"Machine learning: Trends, perspectives, and prospects",2015 Jul 17;349(6245):255-60.,"Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing."
26172351,24.0,"Segmentation and Image Analysis of Abnormal Lungs at CT: Current Approaches, Challenges, and Future Trends",Jul-Aug 2015;35(4):1056-76.,"The computer-based process of identifying the boundaries of lung from surrounding thoracic tissue on computed tomographic (CT) images, which is called segmentation, is a vital first step in radiologic pulmonary image analysis. Many algorithms and software platforms provide image segmentation routines for quantification of lung abnormalities; however, nearly all of the current image segmentation approaches apply well only if the lungs exhibit minimal or no pathologic conditions. When moderate to high amounts of disease or abnormalities with a challenging shape or appearance exist in the lungs, computer-aided detection systems may be highly likely to fail to depict those abnormal regions because of inaccurate segmentation methods. In particular, abnormalities such as pleural effusions, consolidations, and masses often cause inaccurate lung segmentation, which greatly limits the use of image processing methods in clinical and research contexts. In this review, a critical summary of the current methods for lung segmentation on CT images is provided, with special emphasis on the accuracy and performance of the methods in cases with abnormalities and cases with exemplary pathologic findings. The currently available segmentation methods can be divided into five major classes: (a) thresholding-based, (b) region-based, (c) shape-based, (d) neighboring anatomy-guided, and (e) machine learning-based methods. The feasibility of each class and its shortcomings are explained and illustrated with the most common lung abnormalities observed on CT images. In an overview, practical applications and evolving technologies combining the presented approaches for the practicing radiologist are detailed."
26159433,23.0,The electronic stethoscope,2015 Jul 10;14:66.,"Most heart diseases are associated with and reflected by the sounds that the heart produces. Heart auscultation, defined as listening to the heart sound, has been a very important method for the early diagnosis of cardiac dysfunction. Traditional auscultation requires substantial clinical experience and good listening skills. The emergence of the electronic stethoscope has paved the way for a new field of computer-aided auscultation. This article provides an in-depth study of (1) the electronic stethoscope technology, and (2) the methodology for diagnosis of cardiac disorders based on computer-aided auscultation. The paper is based on a comprehensive review of (1) literature articles, (2) market (state-of-the-art) products, and (3) smartphone stethoscope apps. It covers in depth every key component of the computer-aided system with electronic stethoscope, from sensor design, front-end circuitry, denoising algorithm, heart sound segmentation, to the final machine learning techniques. Our intent is to provide an informative and illustrative presentation of the electronic stethoscope, which is valuable and beneficial to academics, researchers and engineers in the technical field, as well as to medical professionals to facilitate its use clinically. The paper provides the technological and medical basis for the development and commercialization of a real-time integrated heart sound detection, acquisition and quantification system."
26145249,11.0,What was old is new again: using the host response to diagnose infectious disease,2015;15(9):1143-58.,"A century of advances in infectious disease diagnosis and treatment changed the face of medicine. However, challenges continue to develop including multi-drug resistance, globalization that increases pandemic risks and high mortality from severe infections. These challenges can be mitigated through improved diagnostics, focusing on both pathogen discovery and the host response. Here, we review how 'omics' technologies improve sepsis diagnosis, early pathogen identification and personalize therapy. Such host response diagnostics are possible due to the confluence of advanced laboratory techniques (e.g., transcriptomics, metabolomics, proteomics) along with advanced mathematical modeling such as machine learning techniques. The road ahead is promising, but obstacles remain before the impact of such advanced diagnostic modalities is felt at the bedside."
26143394,26.0,Distinguishing Asthma Phenotypes Using Machine Learning Approaches,2015 Jul;15(7):38.,"Asthma is not a single disease, but an umbrella term for a number of distinct diseases, each of which are caused by a distinct underlying pathophysiological mechanism. These discrete disease entities are often labelled as 'asthma endotypes'. The discovery of different asthma subtypes has moved from subjective approaches in which putative phenotypes are assigned by experts to data-driven ones which incorporate machine learning. This review focuses on the methodological developments of one such machine learning technique-latent class analysis-and how it has contributed to distinguishing asthma and wheezing subtypes in childhood. It also gives a clinical perspective, presenting the findings of studies from the past 5 years that used this approach. The identification of true asthma endotypes may be a crucial step towards understanding their distinct pathophysiological mechanisms, which could ultimately lead to more precise prevention strategies, identification of novel therapeutic targets and the development of effective personalized therapies."
26138575,3.0,Role of Open Source Tools and Resources in Virtual Screening for Drug Discovery,2015;18(6):528-43.,"Advancement in chemoinformatics research in parallel with availability of high performance computing platform has made handling of large scale multi-dimensional scientific data for high throughput drug discovery easier. In this study we have explored publicly available molecular databases with the help of open-source based integrated in-house molecular informatics tools for virtual screening. The virtual screening literature for past decade has been extensively investigated and thoroughly analyzed to reveal interesting patterns with respect to the drug, target, scaffold and disease space. The review also focuses on the integrated chemoinformatics tools that are capable of harvesting chemical data from textual literature information and transform them into truly computable chemical structures, identification of unique fragments and scaffolds from a class of compounds, automatic generation of focused virtual libraries, computation of molecular descriptors for structure-activity relationship studies, application of conventional filters used in lead discovery along with in-house developed exhaustive PTC (Pharmacophore, Toxicophores and Chemophores) filters and machine learning tools for the design of potential disease specific inhibitors. A case study on kinase inhibitors is provided as an example."
26134276,115.0,Similarity computation strategies in the microRNA-disease network: a survey,2016 Jan;15(1):55-64.,"Various microRNAs have been demonstrated to play roles in a number of human diseases. Several microRNA-disease network reconstruction methods have been used to describe the association from a systems biology perspective. The key problem for the network is the similarity computation model. In this article, we reviewed the main similarity computation methods and discussed these methods and future works. This survey may prompt and guide systems biology and bioinformatics researchers to build more perfect microRNA-disease associations and may make the network relationship clear for medical researchers."
26108231,18.0,A roadmap to multifactor dimensionality reduction methods,2016 Mar;17(2):293-308.,"Complex diseases are defined to be determined by multiple genetic and environmental factors alone as well as in interactions. To analyze interactions in genetic data, many statistical methods have been suggested, with most of them relying on statistical regression models. Given the known limitations of classical methods, approaches from the machine-learning community have also become attractive. From this latter family, a fast-growing collection of methods emerged that are based on the Multifactor Dimensionality Reduction (MDR) approach. Since its first introduction, MDR has enjoyed great popularity in applications and has been extended and modified multiple times. Based on a literature search, we here provide a systematic and comprehensive overview of these suggested methods. The methods are described in detail, and the availability of implementations is listed. Most recent approaches offer to deal with large-scale data sets and rare variants, which is why we expect these methods to even gain in popularity."
26099739,61.0,Pseudo nucleotide composition or PseKNC: an effective formulation for analyzing genomic sequences,2015 Oct;11(10):2620-34.,"With the avalanche of DNA/RNA sequences generated in the post-genomic age, it is urgent to develop automated methods for analyzing the relationship between the sequences and their functions. Towards this goal, a series of sequence-based methods have been proposed and applied to analyze various character-unknown DNA/RNA sequences in order for in-depth understanding their action mechanisms and processes. Compared with the classical sequence-based methods, the pseudo nucleotide composition or PseKNC approach developed very recently has the following advantages: (1) it can convert length-different DNA/RNA sequences into dimension-fixed digital vectors that can be directly handled by all the existing machine-learning algorithms or operation engines; (2) it can contain the desired features and properties according to the selection or definition of users; (3) it can cover considerable sequence pattern information, both local and global. This minireview is focused on the concept of pseudo nucleotide composition, its development and applications."
26092722,16.0,Spatial and temporal epidemiological analysis in the Big Data era,2015 Nov 1;122(1-2):213-20.,"Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data."
26082714,8.0,Minimal approach to neuro-inspired information processing,2015 Jun 2;9:68.,"To learn and mimic how the brain processes information has been a major research challenge for decades. Despite the efforts, little is known on how we encode, maintain and retrieve information. One of the hypothesis assumes that transient states are generated in our intricate network of neurons when the brain is stimulated by a sensory input. Based on this idea, powerful computational schemes have been developed. These schemes, known as machine-learning techniques, include artificial neural networks, support vector machine and reservoir computing, among others. In this paper, we concentrate on the reservoir computing (RC) technique using delay-coupled systems. Unlike traditional RC, where the information is processed in large recurrent networks of interconnected artificial neurons, we choose a minimal design, implemented via a simple nonlinear dynamical system subject to a self-feedback loop with delay. This design is not intended to represent an actual brain circuit, but aims at finding the minimum ingredients that allow developing an efficient information processor. This simple scheme not only allows us to address fundamental questions but also permits simple hardware implementations. By reducing the neuro-inspired reservoir computing approach to its bare essentials, we find that nonlinear transient responses of the simple dynamical system enable the processing of information with excellent performance and at unprecedented speed. We specifically explore different hardware implementations and, by that, we learn about the role of nonlinearity, noise, system responses, connectivity structure, and the quality of projection onto the required high-dimensional state space. Besides the relevance for the understanding of basic mechanisms, this scheme opens direct technological opportunities that could not be addressed with previous approaches."
26068757,1.0,Practical Applications of Digital Pathology,2015 Apr;22(2):137-41.,"Background:                    Virtual microscopy and advances in machine learning have paved the way for the ever-expanding field of digital pathology. Multiple image-based computing environments capable of performing automated quantitative and morphological analyses are the foundation on which digital pathology is built.              Methods:                    The applications for digital pathology in the clinical setting are numerous and are explored along with the digital software environments themselves, as well as the different analytical modalities specific to digital pathology. Prospective studies, case-control analyses, meta-analyses, and detailed descriptions of software environments were explored that pertained to digital pathology and its use in the clinical setting.              Results:                    Many different software environments have advanced platforms capable of improving digital pathology and potentially influencing clinical decisions.              Conclusions:                    The potential of digital pathology is vast, particularly with the introduction of numerous software environments available for use. With all the digital pathology tools available as well as those in development, the field will continue to advance, particularly in the era of personalized medicine, providing health care professionals with more precise prognostic information as well as helping them guide treatment decisions."
26065052,,Training Students to Extract Value from Big Data: Summary of a Workshop,,"As the availability of high-throughput data-collection technologies, such as information-sensing mobile devices, remote sensing, internet log records, and wireless sensor networks has grown, science, engineering, and business have rapidly transitioned from striving to develop information from scant data to a situation in which the challenge is now that the amount of information exceeds a human's ability to examine, let alone absorb, it. Data sets are increasingly complex, and this potentially increases the problems associated with such concerns as missing information and other quality concerns, data heterogeneity, and differing data formats.                The nation's ability to make use of data depends heavily on the availability of a workforce that is properly trained and ready to tackle high-need areas. Training students to be capable in exploiting big data requires experience with statistical analysis, machine learning, and computational infrastructure that permits the real problems associated with massive data to be revealed and, ultimately, addressed. Analysis of big data requires cross-disciplinary skills, including the ability to make modeling decisions while balancing trade-offs between optimization and approximation, all while being attentive to useful metrics and system robustness. To develop those skills in students, it is important to identify whom to teach, that is, the educational background, experience, and characteristics of a prospective data-science student; what to teach, that is, the technical and practical content that should be taught to the student; and how to teach, that is, the structure and organization of a data-science program.    Training Students to Extract Value from Big Data summarizes a workshop convened in April 2014 by the National Research Council's Committee on Applied and Theoretical Statistics to explore how best to train students to use big data. The workshop explored the need for training and curricula and coursework that should be included. One impetus for the workshop was the current fragmented view of what is meant by analysis of big data, data analytics, or data science. New graduate programs are introduced regularly, and they have their own notions of what is meant by those terms and, most important, of what students need to know to be proficient in data-intensive work. This report provides a variety of perspectives about those elements and about their integration into courses and curricula."
26052377,8.0,Targeted proteomics for biomarker discovery and validation of hepatocellular carcinoma in hepatitis C infected patients,2015 Jun 8;7(10):1312-24.,"Hepatocellular carcinoma (HCC)-related mortality is high because early detection modalities are hampered by inaccuracy, expense and inherent procedural risks. Thus there is an urgent need for minimally invasive, highly specific and sensitive biomarkers that enable early disease detection when therapeutic intervention remains practical. Successful therapeutic intervention is predicated on the ability to detect the cancer early. Similar unmet medical needs abound in most fields of medicine and require novel methodological approaches. Proteomic profiling of body fluids presents a sensitive diagnostic tool for early cancer detection. Here we describe such a strategy of comparative proteomics to identify potential serum-based biomarkers to distinguish high-risk chronic hepatitis C virus infected patients from HCC patients. In order to compensate for the extraordinary dynamic range in serum proteins, enrichment methods that compress the dynamic range without surrendering proteome complexity can help minimize the problems associated with many depletion methods. The enriched serum can be resolved using 2D-difference in-gel electrophoresis and the spots showing statistically significant changes selected for identification by liquid chromatography-tandem mass spectrometry. Subsequent quantitative verification and validation of these candidate biomarkers represent an obligatory and rate-limiting process that is greatly enabled by selected reaction monitoring (SRM). SRM is a tandem mass spectrometry method suitable for identification and quantitation of target peptides within complex mixtures independent on peptide-specific antibodies. Ultimately, multiplexed SRM and dynamic multiple reaction monitoring can be utilized for the simultaneous analysis of a biomarker panel derived from support vector machine learning approaches, which allows monitoring a specific disease state such as early HCC. Overall, this approach yields high probability biomarkers for clinical validation in large patient cohorts and represents a strategy extensible to many diseases."
26037068,9.0,Recent progresses in the exploration of machine learning methods as in-silico ADME prediction tools,2015 Jun 23;86:83-100.,"In-silico methods have been explored as potential tools for assessing ADME and ADME regulatory properties particularly in early drug discovery stages. Machine learning methods, with their ability in classifying diverse structures and complex mechanisms, are well suited for predicting ADME and ADME regulatory properties. Recent efforts have been directed at the broadening of application scopes and the improvement of predictive performance with particular focuses on the coverage of ADME properties, and exploration of more diversified training data, appropriate molecular features, and consensus modeling. Moreover, several online machine learning ADME prediction servers have emerged. Here we review these progresses and discuss the performances, application prospects and challenges of exploring machine learning methods as useful tools in predicting ADME and ADME regulatory properties."
26031189,7.0,"The significance of Ciona intestinalis as a stem organism in integrative studies of functional evolution of the chordate endocrine, neuroendocrine, and nervous systems",2016 Feb 1;227:101-8.,"Ascidians are the closest phylogenetic neighbors to vertebrates and are believed to conserve the evolutionary origin in chordates of the endocrine, neuroendocrine, and nervous systems involving neuropeptides and peptide hormones. Ciona intestinalis harbors various homologs or prototypes of vertebrate neuropeptides and peptide hormones including gonadotropin-releasing hormones (GnRHs), tachykinins (TKs), and calcitonin, as well as Ciona-specific neuropeptides such as Ciona vasopressin, LF, and YFV/L peptides. Moreover, molecular and functional studies on Ciona tachykinin (Ci-TK) have revealed the novel molecular mechanism of inducing oocyte growth via up-regulation of vitellogenesis-associated protease activity, which is expected to be conserved in vertebrates. Furthermore, a series of studies on Ciona GnRH receptor paralogs have verified the species-specific regulation of GnRHergic signaling including unique signaling control via heterodimerization among multiple GnRH receptors. These findings confirm the remarkable significance of ascidians in investigations of the evolutionary processes of the peptidergic systems in chordates, leading to the promising advance in the research on Ciona peptides in the next stage based on the recent development of emerging technologies including genome-editing techniques, peptidomics-based multi-color staining, machine-learning prediction, and next-generation sequencing. These technologies and bioinformatic integration of the resultant ""multi-omics"" data will provide unprecedented insights into the comprehensive understanding of molecular and functional regulatory mechanisms of the Ciona peptides, and will eventually enable the exploration of both conserved and diversified endocrine, neuroendocrine, and nervous systems in the evolutionary lineage of chordates."
26017444,99.0,Probabilistic machine learning and artificial intelligence,2015 May 28;521(7553):452-9.,"How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery."
26017443,13.0,Reinforcement learning improves behaviour from evaluative feedback,2015 May 28;521(7553):445-51.,"Reinforcement learning is a branch of machine learning concerned with using experience gained through interacting with the world and evaluative feedback to improve a system's ability to make behavioural decisions. It has been called the artificial intelligence problem in a microcosm because learning algorithms must act autonomously to perform well and achieve their goals. Partly driven by the increasing availability of rich data, recent years have seen exciting advances in the theory and practice of reinforcement learning, including developments in fundamental technical areas such as generalization, planning, exploration and empirical methodology, leading to increasing applicability to real-life problems."
26014110,12.0,The role of machine learning in neuroimaging for drug discovery and development,2015 Nov;232(21-22):4179-89.,"Neuroimaging has been identified as a potentially powerful probe for the in vivo study of drug effects on the brain with utility across several phases of drug development spanning preclinical and clinical investigations. Specifically, neuroimaging can provide insight into drug penetration and distribution, target engagement, pharmacodynamics, mechanistic action and potential indicators of clinical efficacy. In this review, we focus on machine learning approaches for neuroimaging which enable us to make predictions at the individual level based on the distributed effects across the whole brain. Crucially, these approaches can be trained on data from one study and applied to an independent study and, unlike group-level statistics, can be readily use to assess the generalisability to unseen data. In this review, we present examples and suggestions for how machine learning could help answer fundamental questions spanning the drug discovery pipeline: (1) Who should I recruit for this study? (2) What should I measure and when should I measure it? (3) How does the pharmacological agent behave using an experimental medicine model?, and (4) How does a compound differ from and/or resemble existing compounds? Specifically, we present studies from the literature and we suggest areas for the focus of future development. Further refinement and tailoring of machine learning techniques may help realise their tremendous potential for drug discovery and drug validation."
25986686,4.0,The Role of Water Occlusion for the Definition of a Protein Binding Hot-Spot,2015;15(20):2068-79.,"Biological systems rely on the establishment of interactions between biomolecules, which take place in the aqueous environment of the cell. It was already demonstrated that a small set of residues at the interface, Hot-Spots(HS), contributes significantly to the binding free energy. However, these energetic determinants of affinity and specificity are still not fully understood. Moreover, the contribution of water to their HS character is also poorly characterized. In this review, we have focused on the structural data available that support the occlusion of HS from solvent, and therefore the ""O-ring theory""not only on protein-protein but also on protein-DNA complexes. We also emphasized the use of Solvent Accessible Surface Area (SASA) features in a variety of machine-learning approaches that aim to detect binding HS."
25983676,23.0,Using neurophysiological signals that reflect cognitive or affective state: six recommendations to avoid common pitfalls,2015 Apr 30;9:136.,"Estimating cognitive or affective state from neurophysiological signals and designing applications that make use of this information requires expertise in many disciplines such as neurophysiology, machine learning, experimental psychology, and human factors. This makes it difficult to perform research that is strong in all its aspects as well as to judge a study or application on its merits. On the occasion of the special topic ""Using neurophysiological signals that reflect cognitive or affective state"" we here summarize often occurring pitfalls and recommendations on how to avoid them, both for authors (researchers) and readers. They relate to defining the state of interest, the neurophysiological processes that are expected to be involved in the state of interest, confounding factors, inadvertently ""cheating"" with classification analyses, insight on what underlies successful state estimation, and finally, the added value of neurophysiological measures in the context of an application. We hope that this paper will support the community in producing high quality studies and well-validated, useful applications."
25982977,11.0,Open source tools for large-scale neuroscience,2015 Jun;32:156-63.,"New technologies for monitoring and manipulating the nervous system promise exciting biology but pose challenges for analysis and computation. Solutions can be found in the form of modern approaches to distributed computing, machine learning, and interactive visualization. But embracing these new technologies will require a cultural shift: away from independent efforts and proprietary methods and toward an open source and collaborative neuroscience."
25961077,1.0,Briefing in application of machine learning methods in ion channel prediction,2015;2015:945927.,"In cells, ion channels are one of the most important classes of membrane proteins which allow inorganic ions to move across the membrane. A wide range of biological processes are involved and regulated by the opening and closing of ion channels. Ion channels can be classified into numerous classes and different types of ion channels exhibit different functions. Thus, the correct identification of ion channels and their types using computational methods will provide in-depth insights into their function in various biological processes. In this review, we will briefly introduce and discuss the recent progress in ion channel prediction using machine learning methods."
25960736,,On the Relationship between Variational Level Set-Based and SOM-Based Active Contours,2015;2015:109029.,"Most Active Contour Models (ACMs) deal with the image segmentation problem as a functional optimization problem, as they work on dividing an image into several regions by optimizing a suitable functional. Among ACMs, variational level set methods have been used to build an active contour with the aim of modeling arbitrarily complex shapes. Moreover, they can handle also topological changes of the contours. Self-Organizing Maps (SOMs) have attracted the attention of many computer vision scientists, particularly in modeling an active contour based on the idea of utilizing the prototypes (weights) of a SOM to control the evolution of the contour. SOM-based models have been proposed in general with the aim of exploiting the specific ability of SOMs to learn the edge-map information via their topology preservation property and overcoming some drawbacks of other ACMs, such as trapping into local minima of the image energy functional to be minimized in such models. In this survey, we illustrate the main concepts of variational level set-based ACMs, SOM-based ACMs, and their relationship and review in a comprehensive fashion the development of their state-of-the-art models from a machine learning perspective, with a focus on their strengths and weaknesses."
25958010,16.0,Prediction of cytochrome P450 mediated metabolism,2015 Jun 23;86:61-71.,"Cytochrome P450 enzymes (CYPs) form one of the most important enzyme families involved in the metabolism of xenobiotics. CYPs comprise many isoforms, which catalyze a wide variety of reactions, and potentially, a large number of different metabolites can be formed. However, it is often hard to rationalize what metabolites these enzymes generate. In recent years, many different in silico approaches have been developed to predict binding or regioselective product formation for the different CYP isoforms. These comprise ligand-based methods that are trained on experimental CYP data and structure-based methods that consider how the substrate is oriented in the active site or/and how reactive the part of the substrate that is accessible to the heme group is. We will review key aspects for various approaches that are available to predict binding and site of metabolism (SOM), what outcome can be expected from the predictions, and how they could potentially be improved."
25948244,251.0,Machine learning applications in genetics and genomics,2015 Jun;16(6):321-32.,"The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets."
25944500,5.0,Past and current use of walking measures for children with spina bifida: a systematic review,2015 Aug;96(8):1533-1543.e31.,"Objectives:                    To describe walking measurement in children with spina bifida and to identify patterns in the use of walking measures in this population.              Data sources:                    Seven medical databases-Medline, PubMed, Embase, Scopus, Web of Science, CINAHL, and AMED-were searched from the earliest known record until March 11, 2014. Search terms encompassed 3 themes: (1) children; (2) spina bifida; and (3) walking.              Study selection:                    Articles were included if participants were children with spina bifida aged 1 to 17 years and if walking was measured. Articles were excluded if the assessment was restricted to kinematic, kinetic, or electromyographic analysis of walking. A total of 1751 abstracts were screened by 2 authors independently, and 109 articles were included in this review.              Data extraction:                    Data were extracted using standardized forms. Extracted data included study and participant characteristics and details about the walking measures used, including psychometric properties. Two authors evaluated the methodological quality of articles using a previously published framework that considers sampling method, study design, and psychometric properties of the measures used.              Data synthesis:                    Nineteen walking measures were identified. Ordinal-level rating scales (eg, Hoffer Functional Ambulation Scale) were most commonly used (57% of articles), followed by ratio-level, spatiotemporal measures, such as walking speed (18% of articles). Walking was measured for various reasons relevant to multiple health care disciplines. A machine learning analysis was used to identify patterns in the use of walking measures. The learned classifier predicted whether a spatiotemporal measure was used with 77.1% accuracy. A trend to use spatiotemporal measures in older children and those with lumbar and sacral spinal lesions was identified. Most articles were prospective studies that used samples of convenience and unblinded assessors. Few articles evaluated or considered the psychometric properties of the walking measures used.              Conclusions:                    Despite a demonstrated need to measure walking in children with spina bifida, few valid, reliable, and responsive measures have been established for this population."
25935161,24.0,A comprehensive comparative review of sequence-based predictors of DNA- and RNA-binding residues,2016 Jan;17(1):88-105.,"Motivated by the pressing need to characterize protein-DNA and protein-RNA interactions on large scale, we review a comprehensive set of 30 computational methods for high-throughput prediction of RNA- or DNA-binding residues from protein sequences. We summarize these predictors from several significant perspectives including their design, outputs and availability. We perform empirical assessment of methods that offer web servers using a new benchmark data set characterized by a more complete annotation that includes binding residues transferred from the same or similar proteins. We show that predictors of DNA-binding (RNA-binding) residues offer relatively strong predictive performance but they are unable to properly separate DNA- from RNA-binding residues. We design and empirically assess several types of consensuses and demonstrate that machine learning (ML)-based approaches provide improved predictive performance when compared with the individual predictors of DNA-binding residues or RNA-binding residues. We also formulate and execute first-of-its-kind study that targets combined prediction of DNA- and RNA-binding residues. We design and test three types of consensuses for this prediction and conclude that this novel approach that relies on ML design provides better predictive quality than individual predictors when tested on prediction of DNA- and RNA-binding residues individually. It also substantially improves discrimination between these two types of nucleic acids. Our results suggest that development of a new generation of predictors would benefit from using training data sets that combine both RNA- and DNA-binding proteins, designing new inputs that specifically target either DNA- or RNA-binding residues and pursuing combined prediction of DNA- and RNA-binding residues."
25859202,25.0,Multivariate cross-classification: applying machine learning techniques to characterize abstraction in neural representations,2015 Mar 25;9:151.,"Here we highlight an emerging trend in the use of machine learning classifiers to test for abstraction across patterns of neural activity. When a classifier algorithm is trained on data from one cognitive context, and tested on data from another, conclusions can be drawn about the role of a given brain region in representing information that abstracts across those cognitive contexts. We call this kind of analysis Multivariate Cross-Classification (MVCC), and review several domains where it has recently made an impact. MVCC has been important in establishing correspondences among neural patterns across cognitive domains, including motor-perception matching and cross-sensory matching. It has been used to test for similarity between neural patterns evoked by perception and those generated from memory. Other work has used MVCC to investigate the similarity of representations for semantic categories across different kinds of stimulus presentation, and in the presence of different cognitive demands. We use these examples to demonstrate the power of MVCC as a tool for investigating neural abstraction and discuss some important methodological issues related to its application."
25856482,5.0,The mutual inspirations of machine learning and neuroscience,2015 Apr 8;86(1):25-8.,"Neuroscientists are generating data sets of enormous size, which are matching the complexity of real-world classification tasks. Machine learning has helped data analysis enormously but is often not as accurate as human data analysis. Here, Helmstaedter discusses the challenges and promises of neuroscience-inspired machine learning that lie ahead."
25810317,,Analyzing Medical Image Search Behavior: Semantics and Prediction of Query Results,2015 Oct;28(5):537-46.,"Log files of information retrieval systems that record user behavior have been used to improve the outcomes of retrieval systems, understand user behavior, and predict events. In this article, a log file of the ARRS GoldMiner search engine containing 222,005 consecutive queries is analyzed. Time stamps are available for each query, as well as masked IP addresses, which enables to identify queries from the same person. This article describes the ways in which physicians (or Internet searchers interested in medical images) search and proposes potential improvements by suggesting query modifications. For example, many queries contain only few terms and therefore are not specific; others contain spelling mistakes or non-medical terms that likely lead to poor or empty results. One of the goals of this report is to predict the number of results a query will have since such a model allows search engines to automatically propose query modifications in order to avoid result lists that are empty or too large. This prediction is made based on characteristics of the query terms themselves. Prediction of empty results has an accuracy above 88%, and thus can be used to automatically modify the query to avoid empty result sets for a user. The semantic analysis and data of reformulations done by users in the past can aid the development of better search systems, particularly to improve results for novice users. Therefore, this paper gives important ideas to better understand how people search and how to use this knowledge to improve the performance of specialized medical search engines."
25808539,47.0,"Improvements, trends, and new ideas in molecular docking: 2012-2013 in review",2015 Oct;28(10):581-604.,"Molecular docking is a computational method for predicting the placement of ligands in the binding sites of their receptor(s). In this review, we discuss the methodological developments that occurred in the docking field in 2012 and 2013, with a particular focus on the more difficult aspects of this computational discipline. The main challenges and therefore focal points for developments in docking, covered in this review, are receptor flexibility, solvation, scoring, and virtual screening. We specifically deal with such aspects of molecular docking and its applications as selection criteria for constructing receptor ensembles, target dependence of scoring functions, integration of higher-level theory into scoring, implicit and explicit handling of solvation in the binding process, and comparison and evaluation of docking and scoring methods."
25797506,14.0,Big data in medical science--a biostatistical view,2015 Feb 27;112(9):137-42.,"Background:                    Inexpensive techniques for measurement and data storage now enable medical researchers to acquire far more data than can conveniently be analyzed by traditional methods. The expression ""big data"" refers to quantities on the order of magnitude of a terabyte (1012 bytes); special techniques must be used to evaluate such huge quantities of data in a scientifically meaningful way. Whether data sets of this size are useful and important is an open question that currently confronts medical science.              Methods:                    In this article, we give illustrative examples of the use of analytical techniques for big data and discuss them in the light of a selective literature review. We point out some critical aspects that should be considered to avoid errors when large amounts of data are analyzed.              Results:                    Machine learning techniques enable the recognition of potentially relevant patterns. When such techniques are used, certain additional steps should be taken that are unnecessary in more traditional analyses; for example, patient characteristics should be differentially weighted. If this is not done as a preliminary step before similarity detection, which is a component of many data analysis operations, characteristics such as age or sex will be weighted no higher than any one out of 10 000 gene expression values. Experience from the analysis of conventional observational data sets can be called upon to draw conclusions about potential causal effects from big data sets.              Conclusion:                    Big data techniques can be used, for example, to evaluate observational data derived from the routine care of entire populations, with clustering methods used to analyze therapeutically relevant patient subgroups. Such analyses can provide complementary information to clinical trials of the classic type. As big data analyses become more popular, various statistical techniques for causality analysis in observational data are becoming more widely available. This is likely to be of benefit to medical science, but specific adaptations will have to be made according to the requirements of the applications."
25796619,4.0,"In silico methods for predicting drug-drug interactions with cytochrome P-450s, transporters and beyond",2015 Jun 23;86:46-60.,"Drug-drug interactions (DDIs) are associated with severe adverse effects that may lead to the patient requiring alternative therapeutics and could ultimately lead to drug withdrawal from the market if they are severe. To prevent the occurrence of DDI in the clinic, experimental systems to evaluate drug interaction have been integrated into the various stages of the drug discovery and development process. A large body of knowledge about DDI has also accumulated through these studies and pharmacovigillence systems. Much of this work to date has focused on the drug metabolizing enzymes such as cytochrome P-450s as well as drug transporters, ion channels and occasionally other proteins. This combined knowledge provides a foundation for a hypothesis-driven in silico approach, using either cheminformatics or physiologically based pharmacokinetics (PK) modeling methods to assess DDI potential. Here we review recent advances in these approaches with emphasis on hypothesis-driven mechanistic models for important protein targets involved in PK-based DDI. Recent efforts with other informatics approaches to detect DDI are highlighted. Besides DDI, we also briefly introduce drug interactions with other substances, such as Traditional Chinese Medicines to illustrate how in silico modeling can be useful in this domain. We also summarize valuable data sources and web-based tools that are available for DDI prediction. We finally explore the challenges we see faced by in silico approaches for predicting DDI and propose future directions to make these computational models more reliable, accurate, and publically accessible."
25795924,5.0,Machine learning approaches to analysing textual injury surveillance data: a systematic review,2015 Jun;79:41-9.,"Objective:                    To synthesise recent research on the use of machine learning approaches to mining textual injury surveillance data.              Design:                    Systematic review.              Data sources:                    The electronic databases which were searched included PubMed, Cinahl, Medline, Google Scholar, and Proquest. The bibliography of all relevant articles was examined and associated articles were identified using a snowballing technique.              Selection criteria:                    For inclusion, articles were required to meet the following criteria: (a) used a health-related database, (b) focused on injury-related cases, AND used machine learning approaches to analyse textual data.              Methods:                    The papers identified through the search were screened resulting in 16 papers selected for review. Articles were reviewed to describe the databases and methodology used, the strength and limitations of different techniques, and quality assurance approaches used. Due to heterogeneity between studies meta-analysis was not performed.              Results:                    Occupational injuries were the focus of half of the machine learning studies and the most common methods described were Bayesian probability or Bayesian network based methods to either predict injury categories or extract common injury scenarios. Models were evaluated through either comparison with gold standard data or content expert evaluation or statistical measures of quality. Machine learning was found to provide high precision and accuracy when predicting a small number of categories, was valuable for visualisation of injury patterns and prediction of future outcomes. However, difficulties related to generalizability, source data quality, complexity of models and integration of content and technical knowledge were discussed.              Conclusions:                    The use of narrative text for injury surveillance has grown in popularity, complexity and quality over recent years. With advances in data mining techniques, increased capacity for analysis of large databases, and involvement of computer scientists in the injury prevention field, along with more comprehensive use and description of quality assurance methods in text mining approaches, it is likely that we will see a continued growth and advancement in knowledge of text mining in the injury field."
25789437,36.0,Imaging brain mechanisms in chronic visceral pain,2015 Apr;156 Suppl 1(0 1):S50-S63.,"Chronic visceral pain syndromes are important clinical problems with largely unmet medical needs. Based on the common overlap with other chronic disorders of visceral or somatic pain, mood and affect, and their responsiveness to centrally targeted treatments, an important role of central nervous system in their pathophysiology is likely. A growing number of brain imaging studies in irritable bowel syndrome, functional dyspepsia, and bladder pain syndrome/interstitial cystitis has identified abnormalities in evoked brain responses, resting state activity, and connectivity, as well as in gray and white matter properties. Structural and functional alterations in brain regions of the salience, emotional arousal, and sensorimotor networks, as well as in prefrontal regions, are the most consistently reported findings. Some of these changes show moderate correlations with behavioral and clinical measures. Most recently, data-driven machine-learning approaches to larger data sets have been able to classify visceral pain syndromes from healthy control subjects. Future studies need to identify the mechanisms underlying the altered brain signatures of chronic visceral pain and identify targets for therapeutic interventions."
25783869,1.0,The application of machine learning to the modelling of percutaneous absorption: an overview and guide,2015;26(3):181-204.,"Machine learning (ML) methods have been applied to the analysis of a range of biological systems. This paper reviews the application of these methods to the problem domain of skin permeability and addresses critically some of the key issues. Specifically, ML methods offer great potential in both predictive ability and their ability to provide mechanistic insight to, in this case, the phenomena of skin permeation. However, they are beset by perceptions of a lack of transparency and, often, once a ML or related method has been published there is little impetus from other researchers to adopt such methods. This is usually due to the lack of transparency in some methods and the lack of availability of specific coding for running advanced ML methods. This paper reviews critically the application of ML methods to percutaneous absorption and addresses the key issue of transparency by describing in detail - and providing the detailed coding for - the process of running a ML method (in this case, a Gaussian process regression method). Although this method is applied here to the field of percutaneous absorption, it may be applied more broadly to any biological system."
25773550,20.0,Selecting a dynamic simulation modeling method for health care delivery research-part 2: report of the ISPOR Dynamic Simulation Modeling Emerging Good Practices Task Force,2015 Mar;18(2):147-60.,"In a previous report, the ISPOR Task Force on Dynamic Simulation Modeling Applications in Health Care Delivery Research Emerging Good Practices introduced the fundamentals of dynamic simulation modeling and identified the types of health care delivery problems for which dynamic simulation modeling can be used more effectively than other modeling methods. The hierarchical relationship between the health care delivery system, providers, patients, and other stakeholders exhibits a level of complexity that ought to be captured using dynamic simulation modeling methods. As a tool to help researchers decide whether dynamic simulation modeling is an appropriate method for modeling the effects of an intervention on a health care system, we presented the System, Interactions, Multilevel, Understanding, Loops, Agents, Time, Emergence (SIMULATE) checklist consisting of eight elements. This report builds on the previous work, systematically comparing each of the three most commonly used dynamic simulation modeling methods-system dynamics, discrete-event simulation, and agent-based modeling. We review criteria for selecting the most suitable method depending on 1) the purpose-type of problem and research questions being investigated, 2) the object-scope of the model, and 3) the method to model the object to achieve the purpose. Finally, we provide guidance for emerging good practices for dynamic simulation modeling in the health sector, covering all aspects, from the engagement of decision makers in the model design through model maintenance and upkeep. We conclude by providing some recommendations about the application of these methods to add value to informed decision making, with an emphasis on stakeholder engagement, starting with the problem definition. Finally, we identify areas in which further methodological development will likely occur given the growing ""volume, velocity and variety"" and availability of ""big data"" to provide empirical evidence and techniques such as machine learning for parameter estimation in dynamic simulation models. Upon reviewing this report in addition to using the SIMULATE checklist, the readers should be able to identify whether dynamic simulation modeling methods are appropriate to address the problem at hand and to recognize the differences of these methods from those of other, more traditional modeling approaches such as Markov models and decision trees. This report provides an overview of these modeling methods and examples of health care system problems in which such methods have been useful. The primary aim of the report was to aid decisions as to whether these simulation methods are appropriate to address specific health systems problems. The report directs readers to other resources for further education on these individual modeling methods for system interventions in the emerging field of health care delivery science and implementation."
25773546,18.0,Potential application of machine learning in health outcomes research and some statistical cautions,2015 Mar;18(2):137-40.,"Traditional analytic methods are often ill-suited to the evolving world of health care big data characterized by massive volume, complexity, and velocity. In particular, methods are needed that can estimate models efficiently using very large datasets containing healthcare utilization data, clinical data, data from personal devices, and many other sources. Although very large, such datasets can also be quite sparse (e.g., device data may only be available for a small subset of individuals), which creates problems for traditional regression models. Many machine learning methods address such limitations effectively but are still subject to the usual sources of bias that commonly arise in observational studies. Researchers using machine learning methods such as lasso or ridge regression should assess these models using conventional specification tests."
25769815,39.0,Prediction of drug-ABC-transporter interaction--Recent advances and future challenges,2015 Jun 23;86:17-26.,"With the discovery of P-glycoprotein (P-gp), it became evident that ABC-transporters play a vital role in bioavailability and toxicity of drugs. They prevent intracellular accumulation of toxic compounds, which renders them a major defense mechanism against xenotoxic compounds. Their expression in cells of all major barriers (intestine, blood-brain barrier, blood-placenta barrier) as well as in metabolic organs (liver, kidney) also explains their influence on the ADMET properties of drugs and drug candidates. Thus, in silico models for the prediction of the probability of a compound to interact with P-gp or analogous transporters are of high value in the early phase of the drug discovery process. Within this review, we highlight recent developments in the area, with a special focus on the molecular basis of drug-transporter interaction. In addition, with the recent availability of X-ray structures of several ABC-transporters, also structure-based design methods have been applied and will be addressed."
25765323,31.0,Towards the automatic classification of neurons,2015 May;38(5):307-18.,"The classification of neurons into types has been much debated since the inception of modern neuroscience. Recent experimental advances are accelerating the pace of data collection. The resulting growth of information about morphological, physiological, and molecular properties encourages efforts to automate neuronal classification by powerful machine learning techniques. We review state-of-the-art analysis approaches and the availability of suitable data and resources, highlighting prominent challenges and opportunities. The effective solution of the neuronal classification problem will require continuous development of computational methods, high-throughput data production, and systematic metadata organization to enable cross-laboratory integration."
25764253,8.0,Neuroimaging-based methods for autism identification: a possible translational application?,Oct-Dec 2014;29(4):231-9.,"Classification methods based on machine learning (ML) techniques are becoming widespread analysis tools in neuroimaging studies. They have the potential to enhance the diagnostic power of brain data, by assigning a predictive index, either of pathology or of treatment response, to the single subject's acquisition. ML techniques are currently finding numerous applications in psychiatric illness, in addition to the widely studied neurodegenerative diseases. In this review we give a comprehensive account of the use of classification techniques applied to structural magnetic resonance images in autism spectrum disorders (ASDs). Understanding of these highly heterogeneous neurodevelopmental diseases could greatly benefit from additional descriptors of pathology and predictive indices extracted directly from brain data. A perspective is also provided on the future developments necessary to translate ML methods from the field of ASD research into the clinic."
25759684,26.0,Computational approaches for prediction of pathogen-host protein-protein interactions,2015 Feb 24;6:94.,"Infectious diseases are still among the major and prevalent health problems, mostly because of the drug resistance of novel variants of pathogens. Molecular interactions between pathogens and their hosts are the key parts of the infection mechanisms. Novel antimicrobial therapeutics to fight drug resistance is only possible in case of a thorough understanding of pathogen-host interaction (PHI) systems. Existing databases, which contain experimentally verified PHI data, suffer from scarcity of reported interactions due to the technically challenging and time consuming process of experiments. These have motivated many researchers to address the problem by proposing computational approaches for analysis and prediction of PHIs. The computational methods primarily utilize sequence information, protein structure and known interactions. Classic machine learning techniques are used when there are sufficient known interactions to be used as training data. On the opposite case, transfer and multitask learning methods are preferred. Here, we present an overview of these computational approaches for predicting PHI systems, discussing their weakness and abilities, with future directions."
25756377,16.0,An overview of the prediction of protein DNA-binding sites,2015 Mar 6;16(3):5194-215.,"Interactions between proteins and DNA play an important role in many essential biological processes such as DNA replication, transcription, splicing, and repair. The identification of amino acid residues involved in DNA-binding sites is critical for understanding the mechanism of these biological activities. In the last decade, numerous computational approaches have been developed to predict protein DNA-binding sites based on protein sequence and/or structural information, which play an important role in complementing experimental strategies. At this time, approaches can be divided into three categories: sequence-based DNA-binding site prediction, structure-based DNA-binding site prediction, and homology modeling and threading. In this article, we review existing research on computational methods to predict protein DNA-binding sites, which includes data sets, various residue sequence/structural features, machine learning methods for comparison and selection, evaluation methods, performance comparison of different tools, and future directions in protein DNA-binding site prediction. In particular, we detail the meta-analysis of protein DNA-binding sites. We also propose specific implications that are likely to result in novel prediction methods, increased performance, or practical applications."
25750696,313.0,Machine learning applications in cancer prognosis and prediction,2014 Nov 15;13:8-17.,"Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diagnosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent clinical management of patients. The importance of classifying cancer patients into high or low risk groups has led many research teams, from the biomedical and the bioinformatics field, to study the application of machine learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and treatment of cancerous conditions. In addition, the ability of ML tools to detect key features from complex datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied in cancer research for the development of predictive models, resulting in effective and accurate decision making. Even though it is evident that the use of ML methods can improve our understanding of cancer progression, an appropriate level of validation is needed in order for these methods to be considered in the everyday clinical practice. In this work, we present a review of recent ML approaches employed in the modeling of cancer progression. The predictive models discussed here are based on various supervised ML techniques as well as on different input features and data samples. Given the growing trend on the application of ML methods in cancer research, we present here the most recent publications that employ these techniques as an aim to model cancer risk or patient outcomes."
25733954,12.0,Neuroimaging biomarkers to predict treatment response in schizophrenia: the end of 30 years of solitude?,2014 Dec;16(4):491-503.,"Studies that have used structural magnetic resonance imaging (MRI) suggest that individuals with psychoses have brain alterations, particularly in frontal and temporal cortices, and in the white matter tracts that connect them. Furthermore, these studies suggest that brain alterations may be particularly prominent, already at illness onset, in those individuals more likely to have poorer outcomes (eg, higher number of hospital admissions, and poorer symptom remission, level of functioning, and response to the first treatment with antipsychotic drugs). The fact that, even when present, these brain alterations are subtle and distributed in nature, has limited, until now, the utility of MRI in the clinical management of these disorders. More recently, MRI approaches, such as machine learning, have suggested that these neuroanatomical biomarkers can be used for direct clinical benefits. For example, using support vector machine, MRI data obtained at illness onset have been used to predict, with significant accuracy, whether a specific individual is likely to experience a remission of symptoms later on in the course of the illness. Taken together, this evidence suggests that validated, strong neuroanatomical markers could be used not only to inform tailored intervention strategies in a single individual, but also to allow patient stratification in clinical trials for new treatments."
25733557,13.0,The promise of reverse vaccinology,2015 Mar;7(2):85-9.,"Reverse vaccinology (RV) is a computational approach that aims to identify putative vaccine candidates in the protein coding genome (proteome) of pathogens. RV has primarily been applied to bacterial pathogens to identify proteins that can be formulated into subunit vaccines, which consist of one or more protein antigens. An RV approach based on a filtering method has already been used to construct a subunit vaccine against Neisseria meningitidis serogroup B that is now registered in several countries (Bexsero). Recently, machine learning methods have been used to improve the ability of RV approaches to identify vaccine candidates. Further improvements related to the incorporation of epitope-binding annotation and gene expression data are discussed. In the future, it is envisaged that RV approaches will facilitate rapid vaccine design with less reliance on conventional animal testing and clinical trials in order to curb the threat of antibiotic resistance or newly emerged outbreaks of bacterial origin."
25732094,6.0,A rational model of function learning,2015 Oct;22(5):1193-215.,"Theories of how people learn relationships between continuous variables have tended to focus on two possibilities: one, that people are estimating explicit functions, or two that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, which provide a probabilistic basis for similarity-based function learning, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to define a rational model of human function learning that combines the strengths of both approaches and accounts for a wide variety of experimental results."
25726470,3.0,Computational prediction of riboswitches,2015;553:287-312.,"Riboswitches present a ubiquitous genetic regulatory mechanism for prokaryotes and have been found in HIV1, fungi, plants, and even H. sapiens. We present an overview of approaches to predict riboswitch aptamers and, more generally, RNA conformational switches."
25725305,17.0,Transcriptomics of mRNA and egg quality in farmed fish: Some recent developments and future directions,2015 Sep 15;221:23-30.,"Maternal mRNA transcripts deposited in growing oocytes regulate early development and are under intensive investigation as determinants of egg quality. The research has evolved from single gene studies to microarray and now RNA-Seq analyses in which mRNA expression by virtually every gene can be assessed and related to gamete quality. Such studies have mainly focused on genes changing two- to several-fold in expression between biological states, and have identified scores of candidate genes and a few gene networks whose functioning is related to successful development. However, ever-increasing yields of information from high throughput methods for detecting transcript abundance have far outpaced progress in methods for analyzing the massive quantities of gene expression data, and especially for meaningful relation of whole transcriptome profiles to gamete quality. We have developed a new approach to this problem employing artificial neural networks and supervised machine learning with other novel bioinformatics procedures to discover a previously unknown level of ovarian transcriptome function at which minute changes in expression of a few hundred genes is highly predictive of egg quality. In this paper, we briefly review the progress in transcriptomics of fish egg quality and discuss some future directions for this field of study."
25704908,24.0,Identifying transcriptional cis-regulatory modules in animal genomes,Mar-Apr 2015;4(2):59-84.,"Gene expression is regulated through the activity of transcription factors (TFs) and chromatin-modifying proteins acting on specific DNA sequences, referred to as cis-regulatory elements. These include promoters, located at the transcription initiation sites of genes, and a variety of distal cis-regulatory modules (CRMs), the most common of which are transcriptional enhancers. Because regulated gene expression is fundamental to cell differentiation and acquisition of new cell fates, identifying, characterizing, and understanding the mechanisms of action of CRMs is critical for understanding development. CRM discovery has historically been challenging, as CRMs can be located far from the genes they regulate, have few readily identifiable sequence characteristics, and for many years were not amenable to high-throughput discovery methods. However, the recent availability of complete genome sequences and the development of next-generation sequencing methods have led to an explosion of both computational and empirical methods for CRM discovery in model and nonmodel organisms alike. Experimentally, CRMs can be identified through chromatin immunoprecipitation directed against TFs or histone post-translational modifications, identification of nucleosome-depleted 'open' chromatin regions, or sequencing-based high-throughput functional screening. Computational methods include comparative genomics, clustering of known or predicted TF-binding sites, and supervised machine-learning approaches trained on known CRMs. All of these methods have proven effective for CRM discovery, but each has its own considerations and limitations, and each is subject to a greater or lesser number of false-positive identifications. Experimental confirmation of predictions is essential, although shortcomings in current methods suggest that additional means of validation need to be developed. For further resources related to this article, please visit the WIREs website.              Conflict of interest:                    The authors have declared no conflicts of interest for this article."
28347005,18.0,DNA-Protected Silver Clusters for Nanophotonics,2015 Feb 12;5(1):180-207.,"DNA-protected silver clusters (AgN-DNA) possess unique fluorescence properties that depend on the specific DNA template that stabilizes the cluster. They exhibit peak emission wavelengths that range across the visible and near-IR spectrum. This wide color palette, combined with low toxicity, high fluorescence quantum yields of some clusters, low synthesis costs, small cluster sizes and compatibility with DNA are enabling many applications that employ AgN-DNA. Here we review what is known about the underlying composition and structure of AgN-DNA, and how these relate to the optical properties of these fascinating, hybrid biomolecule-metal cluster nanomaterials. We place AgN-DNA in the general context of ligand-stabilized metal clusters and compare their properties to those of other noble metal clusters stabilized by small molecule ligands. The methods used to isolate pure AgN-DNA for analysis of composition and for studies of solution and single-emitter optical properties are discussed. We give a brief overview of structurally sensitive chiroptical studies, both theoretical and experimental, and review experiments on bringing silver clusters of distinct size and color into nanoscale DNA assemblies. Progress towards using DNA scaffolds to assemble multi-cluster arrays is also reviewed."
25668473,9.0,Toward high-content screening of mitochondrial morphology and membrane potential in living cells,2015 Jun;63:66-70.,"Mitochondria are double membrane organelles involved in various key cellular processes. Governed by dedicated protein machinery, mitochondria move and continuously fuse and divide. These ""mitochondrial dynamics"" are bi-directionally linked to mitochondrial and cell functional state in space and time. Due to the action of the electron transport chain (ETC), the mitochondrial inner membrane displays a inside-negative membrane potential (). The latter is considered a functional readout of mitochondrial ""health"" and required to sustain normal mitochondrial ATP production and mitochondrial fusion. During the last decade, live-cell microscopy strategies were developed for simultaneous quantification of  and mitochondrial morphology. This revealed that ETC dysfunction, changes in  and aberrations in mitochondrial structure often occur in parallel, suggesting they are linked potential targets for therapeutic intervention. Here we discuss how combining high-content and high-throughput strategies can be used for analysis of genetic and/or drug-induced effects at the level of individual organelles, cells and cell populations. This article is part of a Directed Issue entitled: Energy Metabolism Disorders and Therapies."
