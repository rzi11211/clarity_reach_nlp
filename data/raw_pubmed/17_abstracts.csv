pmid,citations,title,date,text
31229078,6.0,Alternative data mining/machine learning methods for the analytical evaluation of food quality and authenticity - A review,2019 Aug;122:25-39.,"In recent years, the variety and volume of data acquired by modern analytical instruments in order to conduct a better authentication of food has dramatically increased. Several pattern recognition tools have been developed to deal with the large volume and complexity of available trial data. The most widely used methods are principal component analysis (PCA), partial least squares-discriminant analysis (PLS-DA), soft independent modelling by class analogy (SIMCA), k-nearest neighbours (kNN), parallel factor analysis (PARAFAC), and multivariate curve resolution-alternating least squares (MCR-ALS). Nevertheless, there are alternative data treatment methods, such as support vector machine (SVM), classification and regression tree (CART) and random forest (RF), that show a great potential and more advantages compared to conventional ones. In this paper, we explain the background of these methods and review and discuss the reported studies in which these three methods have been applied in the area of food quality and authenticity. In addition, we clarify the technical terminology used in this particular area of research."
31226833,2.0,Is It Possible to Predict the Odor of a Molecule on the Basis of its Structure?,2019 Jun 20;20(12):3018.,"The olfactory sense is the dominant sensory perception for many animals. When Richard Axel and Linda B. Buck received the Nobel Prize in 2004 for discovering the G protein-coupled receptors' role in olfactory cells, they highlighted the importance of olfaction to the scientific community. Several theories have tried to explain how cells are able to distinguish such a wide variety of odorant molecules in a complex context in which enantiomers can result in completely different perceptions and structurally different molecules. Moreover, sex, age, cultural origin, and individual differences contribute to odor perception variations that complicate the picture. In this article, recent advances in olfaction theory are presented, and future trends in human olfaction such as structure-based odor prediction and artificial sniffing are discussed at the frontiers of chemistry, physiology, neurobiology, and machine learning."
31222562,3.0,"Overview of image-to-image translation by use of deep neural networks: denoising, super-resolution, modality conversion, and reconstruction in medical imaging",2019 Sep;12(3):235-248.,"Since the advent of deep convolutional neural networks (DNNs), computer vision has seen an extremely rapid progress that has led to huge advances in medical imaging. Every year, many new methods are reported at conferences such as the International Conference on Medical Image Computing and Computer-Assisted Intervention and Machine Learning for Medical Image Reconstruction, or published online at the preprint server arXiv. There is a plethora of surveys on applications of neural networks in medical imaging (see [1] for a relatively recent comprehensive survey). This article does not aim to cover all aspects of the field, but focuses on a particular topic, image-to-image translation. Although the topic may not sound familiar, it turns out that many seemingly irrelevant applications can be understood as instances of image-to-image translation. Such applications include (1) noise reduction, (2) super-resolution, (3) image synthesis, and (4) reconstruction. The same underlying principles and algorithms work for various tasks. Our aim is to introduce some of the key ideas on this topic from a uniform viewpoint. We introduce core ideas and jargon that are specific to image processing by use of DNNs. Having an intuitive grasp of the core ideas of applications of neural networks in medical imaging and a knowledge of technical terms would be of great help to the reader for understanding the existing and future applications. Most of the recent applications which build on image-to-image translation are based on one of two fundamental architectures, called pix2pix and CycleGAN, depending on whether the available training data are paired or unpaired (see Sect. 1.3). We provide codes ([2, 3]) which implement these two architectures with various enhancements. Our codes are available online with use of the very permissive MIT license. We provide a hands-on tutorial for training a model for denoising based on our codes (see Sect. 6). We hope that this article, together with the codes, will provide both an overview and the details of the key algorithms and that it will serve as a basis for the development of new applications."
31222375,6.0,Machine learning approaches for pathologic diagnosis,2019 Aug;475(2):131-138.,"Machine learning techniques, especially deep learning techniques such as convolutional neural networks, have been successfully applied to general image recognitions since their overwhelming performance at the 2012 ImageNet Large Scale Visual Recognition Challenge. Recently, such techniques have also been applied to various medical, including histopathological, images to assist the process of medical diagnosis. In some cases, deep learning-based algorithms have already outperformed experienced pathologists for recognition of histopathological images. However, pathological images differ from general images in some aspects, and thus, machine learning of histopathological images requires specialized learning methods. Moreover, many pathologists are skeptical about the ability of deep learning technology to accurately recognize histopathological images because what the learned neural network recognizes is often indecipherable to humans. In this review, we first introduce various applications incorporating machine learning developed to assist the process of pathologic diagnosis, and then describe machine learning problems related to histopathological image analysis, and review potential ways to solve these problems."
31221831,9.0,Trends and challenges in robot manipulation,2019 Jun 21;364(6446):eaat8414.,"Dexterous manipulation is one of the primary goals in robotics. Robots with this capability could sort and package objects, chop vegetables, and fold clothes. As robots come to work side by side with humans, they must also become human-aware. Over the past decade, research has made strides toward these goals. Progress has come from advances in visual and haptic perception and in mechanics in the form of soft actuators that offer a natural compliance. Most notably, immense progress in machine learning has been leveraged to encapsulate models of uncertainty and to support improvements in adaptive and robust control. Open questions remain in terms of how to enable robots to deal with the most unpredictable agent of all, the human."
31220370,2.0,Machine learning and statistical models for predicting indoor air quality,2019 Sep;29(5):704-726.,"Indoor air quality (IAQ), as determined by the concentrations of indoor air pollutants, can be predicted using either physically based mechanistic models or statistical models that are driven by measured data. In comparison with mechanistic models mostly used in unoccupied or scenario-based environments, statistical models have great potential to explore IAQ captured in large measurement campaigns or in real occupied environments. The present study carried out the first literature review of the use of statistical models to predict IAQ. The most commonly used statistical modeling methods were reviewed and their strengths and weaknesses discussed. Thirty-seven publications, in which statistical models were applied to predict IAQ, were identified. These studies were all published in the past decade, indicating the emergence of the awareness and application of machine learning and statistical modeling in the field of IAQ. The concentrations of indoor particulate matter (PM2.5 and PM10 ) were the most frequently studied parameters, followed by carbon dioxide and radon. The most popular statistical models applied to IAQ were artificial neural networks, multiple linear regression, partial least squares, and decision trees."
31219658,8.0,Current status of artificial intelligence applications in urology and their potential to influence clinical practice,2019 Jun 20.,"Objective:                    To investigate the applications of artificial intelligence (AI) in diagnosis, treatment and outcome predictionin urologic diseases and evaluate its advantages over traditional models and methods.              Materials and methods:                    A literature search was performed after PROSPERO registration (CRD42018103701) and in compliance with Preferred Reported Items for Systematic Reviews and Meta-Analyses (PRISMA) methods. Articles between 1994 and 2018 using the search terms ""urology"", ""artificial intelligence"", ""machine learning"" were included and categorized by the application of AI in urology. Review articles, editorial comments, articles with no full-text access, and nonurologic studies were excluded.              Results:                    Initial search yielded 231 articles, but after excluding duplicates and following full-text review and examination of article references, only 111 articles were included in the final analysis. AI applications in urology include: utilizing radiomic imaging or ultrasonic echo data to improve or automate cancer detection or outcome prediction, utilizing digitized tissue specimen images to automate detection of cancer on pathology slides, and combining patient clinical data, biomarkers, or gene expression to assist disease diagnosis or outcome prediction. Some studies employed AI to plan brachytherapy and radiation treatments while others used video based or robotic automated performance metrics to objectively evaluate surgical skill. Compared to conventional statistical analysis, 71.8% of studies concluded that AI is superior in diagnosis and outcome prediction.              Conclusion:                    AI has been widely adopted in urology. Compared to conventional statistics AI approaches are more accurate in prediction and more explorative for analyzing large data cohorts. With an increasing library of patient data accessible to clinicians, AI may help facilitate evidence-based and individualized patient care."
31217702,6.0,"Critical Care, Critical Data",2019 Jun 12;10:1179597219856564.,"As big data, machine learning, and artificial intelligence continue to penetrate into and transform many facets of our lives, we are witnessing the emergence of these powerful technologies within health care. The use and growth of these technologies has been contingent on the availability of reliable and usable data, a particularly robust resource in critical care medicine where continuous monitoring forms a key component of the infrastructure of care. The response to this opportunity has included the development of open databases for research and other purposes; the development of a collaborative form of clinical data science intended to fully leverage these data resources, and the creation of data-driven applications for purposes such as clinical decision support. Most recently, data levels have reached the thresholds required for the development of robust artificial intelligence features for clinical purposes. The systematic capture and analysis of clinical data in both individuals and populations allows us to begin to move toward precision medicine in the intensive care unit (ICU). In this perspective review, we examine the fundamental role of data as we present the current progress that has been made toward an artificial intelligence (AI)-supported, data-driven precision critical care medicine."
31215361,1.0,Recent Advances in Machine Learning Based Prediction of RNA-protein Interactions,2019;26(8):601-619.,"The interactions between RNAs and proteins play critical roles in many biological processes. Therefore, characterizing these interactions becomes critical for mechanistic, biomedical, and clinical studies. Many experimental methods can be used to determine RNA-protein interactions in multiple aspects. However, due to the facts that RNA-protein interactions are tissuespecific and condition-specific, as well as these interactions are weak and frequently compete with each other, those experimental techniques can not be made full use of to discover the complete spectrum of RNA-protein interactions. To moderate these issues, continuous efforts have been devoted to developing high quality computational techniques to study the interactions between RNAs and proteins. Many important progresses have been achieved with the application of novel techniques and strategies, such as machine learning techniques. Especially, with the development and application of CLIP techniques, more and more experimental data on RNA-protein interaction under specific biological conditions are available. These CLIP data altogether provide a rich source for developing advanced machine learning predictors. In this review, recent progresses on computational predictors for RNA-protein interaction were summarized in the following aspects: dataset, prediction strategies, and input features. Possible future developments were also discussed at the end of the review."
31214847,6.0,Structural Imaging in Parkinson's Disease: New Developments,2019 Jun 18;19(8):50.,"Purpose of review:                    To review the advances in structural imaging for the diagnosis, prognosis, and treatment of Parkinson's disease (PD) during the last 5 years.              Recent findings:                    Structural imaging using high-field MRI (â‰¥ 3 T) and new MR sequences sensitive to iron and nigral pigments have achieved to assess in vivo pathological surrogates useful for PD diagnosis (notably decreased nigral neuromelanin and loss of dorsal nigral hyperintensity, increased nigral iron content, diffusivity, and free-water), prodromal diagnosis (decreased neuromelanin signal in the locus coeruleus), and PD progression (with increasing nigral iron content (increasing R2* rate) and nigral damage (increasing free-water)). Additionally, evaluation of atrophy in small monoaminergic nuclei is useful for prognosis, including cholinergic basal forebrain nuclei atrophy for cognitive impairment. New advances in multimodal structural imaging improve diagnosis, prognosis, and prediction of invasive treatment outcome in PD, and may further benefit from machine learning and large scale longitudinal studies to better identify prognostic subtypes."
31213198,2.0,Analysis of Collagen Spatial Structure Using Multiphoton Microscopy and Machine Learning Methods,2019 Jan;84(Suppl 1):S108-S123.,"Pathogenesis of many diseases is associated with changes in the collagen spatial structure. Traditionally, the 3D structure of collagen in biological tissues is analyzed using histochemistry, immunohistochemistry, magnetic resonance imaging, and X-radiography. At present, multiphoton microscopy (MPM) is commonly used to study the structure of biological tissues. MPM has a high spatial resolution comparable to histological analysis and can be used for direct visualization of collagen spatial structure. Because of a large volume of data accumulated due to the high spatial resolution of MPM, special analytical methods should be used for identification of informative features in the images and quantitative evaluation of relationship between these features and pathological processes resulting in the destruction of collagen structure. Here, we describe current approaches and achievements in the identification of informative features in the MPM images of collagen in biological tissues, as well as the development on this basis of algorithms for computer-aided classification of collagen structures using machine learning as a type of artificial intelligence methods."
31209389,2.0,Completing the picture through correlative characterization,2019 Oct;18(10):1041-1049.,"Natural and manufactured materials rely on complex hierarchical microstructures to deliver a suite of interesting properties. To predict and tailor their performance requires a joined-up knowledge of their multiphase microstructure, interfaces, chemistry and crystallography from the nanoscale to the macroscale. This Perspective reflects on how recent developments in correlative characterization can bring together multiple image modalities and maps of the local chemistry, structure and functionality to form rich multimodal and multiscale correlated datasets. The automated collection and digitization of multidimensional data is an essential part of the picture for developing multiscale modelling and 'big data'-driven machine learning approaches. These are needed to both improve our understanding of existing materials and exploit high-throughput combinatorial synthesis, processing and testing methods to develop materials with bespoke properties."
31207930,11.0,Optimizing Neuro-Oncology Imaging: A Review of Deep Learning Approaches for Glioma Imaging,2019 Jun 14;11(6):829.,"Radiographic assessment with magnetic resonance imaging (MRI) is widely used to characterize gliomas, which represent 80% of all primary malignant brain tumors. Unfortunately, glioma biology is marked by heterogeneous angiogenesis, cellular proliferation, cellular invasion, and apoptosis. This translates into varying degrees of enhancement, edema, and necrosis, making reliable imaging assessment challenging. Deep learning, a subset of machine learning artificial intelligence, has gained traction as a method, which has seen effective employment in solving image-based problems, including those in medical imaging. This review seeks to summarize current deep learning applications used in the field of glioma detection and outcome prediction and will focus on (1) pre- and post-operative tumor segmentation, (2) genetic characterization of tissue, and (3) prognostication. We demonstrate that deep learning methods of segmenting, characterizing, grading, and predicting survival in gliomas are promising opportunities that may enhance both research and clinical activities."
31205413,3.0,Machine Learning-Enhanced T Cell Neoepitope Discovery for Immunotherapy Design,2019 May 23;18:1176935119852081.,"Immune responses mediated by T cells are aimed at specific peptides, designated T cell epitopes, that are recognized when bound to human leukocyte antigen (HLA) molecules. The HLA genes are remarkably polymorphic in the human population allowing a broad and fine-tuned capacity to bind a wide array of peptide sequences. Polymorphisms might generate neoepitopes by impacting the HLA-peptide interaction and potentially alter the level and type of generated T cell responses. Multiple algorithms and tools based on machine learning (ML) have been implemented and are able to predict HLA-peptide binding affinity with considerable accuracy. Challenges in this field include the availability of adequate epitope datasets for training and benchmarking and the development of fully integrated pipelines going from next-generation sequencing to neoepitope prediction and quality analysis metrics. Effectively predicting neoepitopes from in silico data is a demanding task that has been facilitated by ML and will be of great value for the future of personalized immunotherapies against cancer and other diseases."
31203421,8.0,"Why imaging data alone is not enough: AI-based integration of imaging, omics, and clinical data",2019 Dec;46(13):2722-2730.,"Artificial intelligence (AI) is currently regaining enormous interest due to the success of machine learning (ML), and in particular deep learning (DL). Image analysis, and thus radiomics, strongly benefits from this research. However, effectively and efficiently integrating diverse clinical, imaging, and molecular profile data is necessary to understand complex diseases, and to achieve accurate diagnosis in order to provide the best possible treatment. In addition to the need for sufficient computing resources, suitable algorithms, models, and data infrastructure, three important aspects are often neglected: (1) the need for multiple independent, sufficiently large and, above all, high-quality data sets; (2) the need for domain knowledge and ontologies; and (3) the requirement for multiple networks that provide relevant relationships among biological entities. While one will always get results out of high-dimensional data, all three aspects are essential to provide robust training and validation of ML models, to provide explainable hypotheses and results, and to achieve the necessary trust in AI and confidence for clinical applications."
31202770,6.0,Myocardial Mechanics in Patients With Normal LVEF and Diastolic Dysfunction,2020 Jan;13(1 Pt 2):258-271.,"Heart failure with preserved ejection fraction (HFpEF) is a complex clinical entity that is poorly understood yet present in up to 5.5% of the general population. Proven therapies for this disorder are lacking, even though it has a similar prognosis to that of heart failure with reduced ejection fraction (HFrEF). Innovative imaging techniques have provided in-depth understanding of the unique pattern of left ventricular mechanics in patients with HFpEF who progress through preclinical (Stages A to B) and clinical (Stages C to D) American College of Cardiology/American Heart Association heart failure stages. This review highlights the mechanical basis of this disorder from the cellular and myofiber level to chamber dysfunction. As each chamber of the heart is examined, specific biomarkers and echocardiographic parameters with diagnostic and prognostic values are discussed. Finally, novel phenotyping methods including machine learning are reviewed that integrate these mechanics into clinical groups to advise and treat patients."
31200809,8.0,Association mapping in plants in the post-GWAS genomics era,2019;104:75-154.,"With the availability of DNA-based molecular markers during early 1980s and that of sophisticated statistical tools in late 1980s and later, it became possible to identify genomic regions that control a quantitative trait. The two methods used for this purpose included quantitative trait loci (QTL) interval mapping and genome-wide association mapping/studies (GWAS). Both these methods have their own merits and demerits, so that newer approaches were developed in order to deal with the demerits. We have now entered a post-GWAS era, where either the original data on individual genotypes are being used again keeping in view the results of GWAS or else summary statistics obtained through GWAS is subjected to further analysis. The first half of this review briefly deals with the approaches that were used for GWAS, the GWAS results obtained in some major crops (maize, wheat, rice, sorghum and soybean), their utilization for crop improvement and the improvements made to address the limitations of original GWA studies (computational demand, multiple testing and false discovery, rare marker alleles, etc.). These improvements included the development of multi-locus and multi-trait analysis, joint linkage association mapping, etc. Since originally GWA studies were used for mere identification of marker-trait association for marker-assisted selection, the second half of the review is devoted to activities in post-GWAS era, which include different methods that are being used for identification of causal variants and their prioritization (meta-analysis, pathway-based analysis, methylation QTL), functional characterization of candidate signals, gene- and gene-set based association mapping, GWAS using high dimensional data through machine learning, etc. The last section deals with popular resources available for GWAS in plants in the post-GWAS era and the implications of the results of post-GWAS for crop improvement."
31199919,19.0,Pathology Image Analysis Using Segmentation Deep Learning Algorithms,2019 Sep;189(9):1686-1698.,"With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning-based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis."
31198073,,A review on computer-aided recent developments for automatic detection of diabetic retinopathy,2019 Feb;43(2):87-99.,"Diabetic retinopathy is a serious microvascular disorder that might result in loss of vision and blindness. It seriously damages the retinal blood vessels and reduces the light-sensitive inner layer of the eye. Due to the manual inspection of retinal fundus images on diabetic retinopathy to detect the morphological abnormalities in Microaneurysms (MAs), Exudates (EXs), Haemorrhages (HMs), and Inter retinal microvascular abnormalities (IRMA) is very difficult and time consuming process. In order to avoid this, the regular follow-up screening process, and early automatic Diabetic Retinopathy detection are necessary. This paper discusses various methods of analysing automatic retinopathy detection and classification of different grading based on the severity levels. In addition, retinal blood vessel detection techniques are also discussed for the ultimate detection and diagnostic procedure of proliferative diabetic retinopathy. Furthermore, the paper elaborately discussed the systematic review accessed by authors on various publicly available databases collected from different medical sources. In the survey, meta-analysis of several methods for diabetic feature extraction, segmentation and various types of classifiers have been used to evaluate the system performance metrics for the diagnosis of DR. This survey will be helpful for the technical persons and researchers who want to focus on enhancing the diagnosis of a system that would be more powerful in real life."
31194543,24.0,Deep Learning in Chemistry,2019 Jun 24;59(6):2545-2559.,"Machine learning enables computers to address problems by learning from data. Deep learning is a type of machine learning that uses a hierarchical recombination of features to extract pertinent information and then learn the patterns represented in the data. Over the last eight years, its abilities have increasingly been applied to a wide variety of chemical challenges, from improving computational chemistry to drug and materials design and even synthesis planning. This review aims to explain the concepts of deep learning to chemists from any background and follows this with an overview of the diverse applications demonstrated in the literature. We hope that this will empower the broader chemical community to engage with this burgeoning field and foster the growing movement of deep learning accelerated chemistry."
31191325,3.0,Treatment-Resistant Schizophrenia: Insights From Genetic Studies and Machine Learning Approaches,2019 May 29;10:617.,"Schizophrenia (SCZ) is a severe psychiatric disorder affecting approximately 23 million people worldwide. It is considered the eighth leading cause of disability according to the World Health Organization and is associated with a significant reduction in life expectancy. Antipsychotics represent the first-choice treatment in SCZ, but approximately 30% of patients fail to respond to acute treatment. These patients are generally defined as treatment-resistant and are eligible for clozapine treatment. Treatment-resistant patients show a more severe course of the disease, but it has been suggested that treatment-resistant schizophrenia (TRS) may constitute a distinct phenotype that is more than just a more severe form of SCZ. TRS is heritable, and genetics has been shown to play an important role in modulating response to antipsychotics. Important efforts have been put into place in order to better understand the genetic architecture of TRS, with the main goal of identifying reliable predictive markers that might improve the management and quality of life of TRS patients. However, the number of candidate gene and genome-wide association studies specifically focused on TRS is limited, and to date, findings do not allow the disentanglement of its polygenic nature. More recent studies implemented polygenic risk score, gene-based and machine learning methods to explore the genetics of TRS, reporting promising findings. In this review, we present an overview on the genetics of TRS, particularly focusing our discussion on studies implementing polygenic approaches."
31190176,3.0,What can artificial intelligence teach us about the molecular mechanisms underlying disease?,2019 Dec;46(13):2715-2721.,"While molecular imaging with positron emission tomography or single-photon emission computed tomography already reports on tumour molecular mechanisms on a macroscopic scale, there is increasing evidence that there are multiple additional features within medical images that can further improve tumour characterization, treatment prediction and prognostication. Early reports have already revealed the power of radiomics to personalize and improve patient management and outcomes. What remains unclear is how these additional metrics relate to underlying molecular mechanisms of disease. Furthermore, the ability to deal with increasingly large amounts of data from medical images and beyond in a rapid, reproducible and transparent manner is essential for future clinical practice. Here, artificial intelligence (AI) may have an impact. AI encompasses a broad range of 'intelligent' functions performed by computers, including language processing, knowledge representation, problem solving and planning. While rule-based algorithms, e.g. computer-aided diagnosis, have been in use for medical imaging since the 1990s, the resurgent interest in AI is related to improvements in computing power and advances in machine learning (ML). In this review we consider why molecular and cellular processes are of interest and which processes have already been exposed to AI and ML methods as reported in the literature. Non-small-cell lung cancer is used as an exemplar and the focus of this review as the most common tumour type in which AI and ML approaches have been tested and to illustrate some of the concepts."
31181343,3.0,Computational anatomy for multi-organ analysis in medical imaging: A review,2019 Aug;56:44-67.,"The medical image analysis field has traditionally been focused on the development of organ-, and disease-specific methods. Recently, the interest in the development of more comprehensive computational anatomical models has grown, leading to the creation of multi-organ models. Multi-organ approaches, unlike traditional organ-specific strategies, incorporate inter-organ relations into the model, thus leading to a more accurate representation of the complex human anatomy. Inter-organ relations are not only spatial, but also functional and physiological. Over the years, the strategies proposed to efficiently model multi-organ structures have evolved from the simple global modeling, to more sophisticated approaches such as sequential, hierarchical, or machine learning-based models. In this paper, we present a review of the state of the art on multi-organ analysis and associated computation anatomy methodology. The manuscript follows a methodology-based classification of the different techniques available for the analysis of multi-organs and multi-anatomical structures, from techniques using point distribution models to the most recent deep learning-based approaches. With more than 300 papers included in this review, we reflect on the trends and challenges of the field of computational anatomy, the particularities of each anatomical region, and the potential of multi-organ analysis to increase the impact of medical imaging applications on the future of healthcare."
31180806,3.0,The Ultimate Guide to Bacterial Swarming: An Experimental Model to Study the Evolution of Cooperative Behavior,2019 Sep 8;73:293-312.,"Cooperation has fascinated biologists since Darwin. How did cooperative behaviors evolve despite the fitness cost to the cooperator? Bacteria have cooperative behaviors that make excellent models to take on this age-old problem from both proximate (molecular) and ultimate (evolutionary) angles. We delve into Pseudomonas aeruginosa swarming, a phenomenon where billions of bacteria move cooperatively across distances of centimeters in a matter of a few hours. Experiments with swarming have unveiled a strategy called metabolic prudence that stabilizes cooperation, have showed the importance of spatial structure, and have revealed a regulatory network that integrates environmental stimuli and direct cooperative behavior, similar to a machine learning algorithm. The study of swarming elucidates more than proximate mechanisms: It exposes ultimate mechanisms valid to all scales, from cells in cancerous tumors to animals in large communities."
31177872,,An update on advanced dual-energy CT for head and neck cancer imaging,2019 Jul;19(7):633-644.,"Introduction: Dual-energy-computed tomography (DECT) is an advanced form of computed tomography (CT) that enables spectral tissue characterization beyond what is possible with conventional CT scans. DECT can improve non-invasive diagnostic evaluation of the neck, especially for the evaluation of head and neck cancer. Areas covered: This article is a review of current applications of DECT for the evaluation of head and neck cancer, focusing largely on squamous cell carcinoma (HNSCC). The article will begin with a brief overview of principles and different approaches for DECT scanning. This will be followed by a review of different DECT applications in diagnostic imaging and radiation oncology, practical and workflow considerations, and various emerging advanced applications for tumor analysis, including the use of DECT datasets for radiomics and machine learning applications. Expert opinion: Using a multi-parametric approach, different DECT reconstructions can be used to improve diagnostic evaluation and surveillance of head and neck cancer, including improving visibility of HNSCC, determination of tumor boundaries and extent, and invasion of critical organs such as the thyroid cartilage. In the future, the large amount of quantitative information on DECT scans may be leveraged for improving radiomic and machine learning models for tumor characterization."
31175395,1.0,Physician centred imaging interpretation is dying out - why should I be a nuclear medicine physician?,2019 Dec;46(13):2708-2714.,"Radiomics, machine learning, and, more generally, artificial intelligence (AI) provide unique tools to improve the performances of nuclear medicine in all aspects. They may help rationalise the operational organisation of imaging departments, optimise resource allocations, and improve image quality while decreasing radiation exposure and maintaining qualitative accuracy. There is already convincing data that show AI detection, and interpretation algorithms can perform with equal or higher diagnostic accuracy in various specific indications than experts in the field. Preliminary data strongly suggest that AI will be able to process imaging data and information well beyond what is visible to the human eye, and it will be able to integrate features to provide signatures that may further drive personalised medicine. As exciting as these prospects are, they currently remain essentially projects with a long way to go before full validation and routine clinical implementation. AI uses a language that is totally unfamiliar to nuclear medicine physicians, who have not been trained to manage the highly complex concepts that rely primarily on mathematics, computer sciences, and engineering. Nuclear medicine physicians are mostly familiar with biology, pharmacology, and physics, yet, considering the disruptive nature of AI in medicine, we need to start acquiring the knowledge that will keep us in the position of being actors and not merely witnesses of the wonders developed by other stakeholders in front of our incredulous eyes. This will allow us to remain a useful and valid interface between the image, the data, and the patients and free us to pursue other, one might say nobler tasks, such as treating, caring and communicating with our patients or conducting research and development."
31174387,22.0,A Structure-Based Drug Discovery Paradigm,2019 Jun 6;20(11):2783.,"Structure-based drug design is becoming an essential tool for faster and more cost-efficient lead discovery relative to the traditional method. Genomic, proteomic, and structural studies have provided hundreds of new targets and opportunities for future drug discovery. This situation poses a major problem: the necessity to handle the ""big data"" generated by combinatorial chemistry. Artificial intelligence (AI) and deep learning play a pivotal role in the analysis and systemization of larger data sets by statistical machine learning methods. Advanced AI-based sophisticated machine learning tools have a significant impact on the drug discovery process including medicinal chemistry. In this review, we focus on the currently available methods and algorithms for structure-based drug design including virtual screening and de novo drug design, with a special emphasis on AI- and deep-learning-based methods used for drug discovery."
31173851,4.0,"A review on brain tumor diagnosis from MRI images: Practical implications, key achievements, and lessons learned",2019 Sep;61:300-318.,"The successful early diagnosis of brain tumors plays a major role in improving the treatment outcomes and thus improving patient survival. Manually evaluating the numerous magnetic resonance imaging (MRI) images produced routinely in the clinic is a difficult process. Thus, there is a crucial need for computer-aided methods with better accuracy for early tumor diagnosis. Computer-aided brain tumor diagnosis from MRI images consists of tumor detection, segmentation, and classification processes. Over the past few years, many studies have focused on traditional or classical machine learning techniques for brain tumor diagnosis. Recently, interest has developed in using deep learning techniques for diagnosing brain tumors with better accuracy and robustness. This study presents a comprehensive review of traditional machine learning techniques and evolving deep learning techniques for brain tumor diagnosis. This review paper identifies the key achievements reflected in the performance measurement metrics of the applied algorithms in the three diagnosis processes. In addition, this study discusses the key findings and draws attention to the lessons learned as a roadmap for future research."
31173849,7.0,Machine learning in resting-state fMRI analysis,2019 Dec;64:101-121.,"Machine learning techniques have gained prominence for the analysis of resting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, we present an overview of various unsupervised and supervised machine learning applications to rs-fMRI. We offer a methodical taxonomy of machine learning methods in resting-state fMRI. We identify three major divisions of unsupervised learning methods with regard to their applications to rs-fMRI, based on whether they discover principal modes of variation across space, time or population. Next, we survey the algorithms and rs-fMRI feature representations that have driven the success of supervised subject-level predictions. The goal is to provide a high-level overview of the burgeoning field of rs-fMRI from the perspective of machine learning applications."
31173533,2.0,Biomicrofluidic Systems for Hematologic Cancer Research and Clinical Applications,2019 Oct;24(5):457-476.,"A persistent challenge in developing personalized treatments for hematologic cancers is the lack of patient specific, physiologically relevant disease models to test investigational drugs in clinical trials and to select therapies in a clinical setting. Biomicrofluidic systems and organ-on-a-chip technologies have the potential to change how researchers approach the fundamental study of hematologic cancers and select clinical treatment for individual patient. Here, we review microfluidics cell-based technology with application toward studying hematologic tumor microenvironments (TMEs) for the purpose of drug discovery and clinical treatment selection. We provide an overview of state-of-the-art microfluidic systems designed to address questions related to hematologic TMEs and drug development. Given the need to develop personalized treatment platforms involving this technology, we review pharmaceutical drugs and different modes of immunotherapy for hematologic cancers, followed by key considerations for developing a physiologically relevant microfluidic companion diagnostic tool for mimicking different hematologic TMEs for testing with different drugs in clinical trials. Opportunities lie ahead for engineers to revolutionize conventional drug discovery strategies of hematologic cancers, including integrating cell-based microfluidics technology with machine learning and automation techniques, which may stimulate pharma and regulatory bodies to promote research and applications of microfluidics technology for drug development."
31171259,12.0,The Future of Cardiovascular Computed Tomography: Advanced Analytics and Clinical Insights,2019 Jun;12(6):1058-1072.,"Cardiovascular computed tomography (CCT) has undergone rapid maturation over the last decade and is now of proven clinical utility in the diagnosis and management of coronary artery disease, in guiding structural heart disease intervention, and in the diagnosis and treatment of congenital heart disease. The next decade will undoubtedly witness further advances in hardware and advanced analytics that will potentially see an increasingly core role for CCT at the center of clinical cardiovascular practice. In coronary artery disease assessment this may be via improved hemodynamic adjudication, and shear stress analysis using computational flow dynamics, more accurate and robust plaque characterization with spectral or photon-counting CT, or advanced quantification of CT data via artificial intelligence, machine learning, and radiomics. In structural heart disease, CCT is already pivotal to procedural planning with adjudication of gradients before and following intervention, whereas in congenital heart disease CCT is already used to support clinical decision making from neonates to adults, often with minimal radiation dose. In both these areas the role of computational flow dynamics, advanced tissue printing, and image modelling has the potential to revolutionize the way these complex conditions are managed, and CCT is likely to become an increasingly critical enabler across the whole advancing field of cardiovascular medicine."
31166761,8.0,Artificial Intelligence in Musculoskeletal Imaging: Current Status and Future Directions,2019 Sep;213(3):506-513.,"OBJECTIVE. The objective of this article is to show how artificial intelligence (AI) has impacted different components of the imaging value chain thus far as well as to describe its potential future uses. CONCLUSION. The use of AI has the potential to greatly enhance every component of the imaging value chain. From assessing the appropriateness of imaging orders to helping predict patients at risk for fracture, AI can increase the value that musculoskeletal imagers provide to their patients and to referring clinicians by improving image quality, patient centricity, imaging efficiency, and diagnostic accuracy."
31163504,5.0,"Artificial Intelligence in Musculoskeletal Imaging: Review of Current Literature, Challenges, and Trends",2019 Jun;23(3):304-311.,"Artificial intelligence (AI) has gained major attention with a rapid increase in the number of published articles, mostly recently. This review provides a general understanding of how AI can or will be useful to the musculoskeletal radiologist. After a brief technical background on AI, machine learning, and deep learning, we illustrate, through examples from the musculoskeletal literature, potential AI applications in the various steps of the radiologist's workflow, from managing the request to communication of results. The implementation of AI solutions does not go without challenges and limitations. These are also discussed, as well as the trends and perspectives."
31162525,1.0,Roles of membrane transporters: connecting the dots from sequence to phenotype,2019 Sep 24;124(2):201-208.,"Background:                    Plant membrane transporters are involved in diverse cellular processes underpinning plant physiology, such as nutrient acquisition, hormone movement, resource allocation, exclusion or sequestration of various solutes from cells and tissues, and environmental and developmental signalling. A comprehensive characterization of transporter function is therefore key to understanding and improving plant performance.              Scope and conclusions:                    In this review, we focus on the complexities involved in characterizing transporter function and the impact that this has on current genomic annotations. Specific examples are provided that demonstrate why sequence homology alone cannot be relied upon to annotate and classify transporter function, and to show how even single amino acid residue variations can influence transporter activity and specificity. Misleading nomenclature of transporters is often a source of confusion in transporter characterization, especially for people new to or outside the field. Here, to aid researchers dealing with interpretation of large data sets that include transporter proteins, we provide examples of transporters that have been assigned names that misrepresent their cellular functions. Finally, we discuss the challenges in connecting transporter function at the molecular level with physiological data, and propose a solution through the creation of new databases. Further fundamental in-depth research on specific transport (and other) proteins is still required; without it, significant deficiencies in large-scale data sets and systems biology approaches will persist. Reliable characterization of transporter function requires integration of data at multiple levels, from amino acid residue sequence annotation to more in-depth biochemical, structural and physiological studies."
31159603,1.0,Towards Patient-centered Diagnosis of Pediatric Obstructive Sleep Apnea-A Review of Biomedical Engineering Strategies,2019 Jul;16(7):617-629.,"Introduction:                    Obstructive sleep apnea in children has a prevalence of 5%. Polysomnography is considered to be the gold standard for diagnosis and stratification of the condition. However, it is resource-intensive, expensive and uncomfortable for children and their families.              Areas covered:                    We focus this review on technical developments in sensor technology, materials and predictive analytics for translation to (i) patient comfort and compliance in the laboratory and (ii) validation of home sleep apnea testing in children. Key developments in adult polysomnography that could be considered for adoption in children are also highlighted. This review is organized by Sleep, Cardiovascular, Oximetry, Position, Effort, and Respiratory (SCOPER) parameters of interest.              Expert opinion:                    In the past decade, improvements in respiratory sensors and signal processing strategies have transitioned sleep apnea testing in adults from the laboratory to home, thus reducing costs and improving access. Unfortunately, such benefits have not been observed for children principally due to the lack of high-quality studies. The increasing cost of diagnosis of sleep apnea in children needs urgent attention. Recent technical developments as described in this review have the potential to support further evaluation of home sleep apnea testing while improving the current circumstances of in-lab polysomnography for children."
31158512,3.0,Ontology mapping for semantically enabled applications,2019 Oct;24(10):2068-2075.,"In this review, we provide a summary of recent progress in ontology mapping (OM) at a crucial time when biomedical research is under a deluge of an increasing amount and variety of data. This is particularly important for realising the full potential of semantically enabled or enriched applications and for meaningful insights, such as drug discovery, using machine-learning technologies. We discuss challenges and solutions for better ontology mappings, as well as how to select ontologies before their application. In addition, we describe tools and algorithms for ontology mapping, including evaluation of tool capability and quality of mappings. Finally, we outline the requirements for an ontology mapping service (OMS) and the progress being made towards implementation of such sustainable services."
31158511,4.0,Unbiased data analytic strategies to improve biomarker discovery in precision medicine,2019 Sep;24(9):1735-1748.,"Omics technologies promised improved biomarker discovery for precision medicine. The foremost problem of discovered biomarkers is irreproducibility between patient cohorts. From a data analytics perspective, the main reason for these failures is bias in statistical approaches and overfitting resulting from batch effects and confounding factors. The keys to reproducible biomarker discovery are: proper study design, unbiased data preprocessing and quality control analyses, and a knowledgeable application of statistics and machine learning algorithms. In this review, we discuss study design and analysis considerations and suggest standards from an expert point-of-view to promote unbiased decision-making in biomarker discovery in precision medicine."
31157465,1.0,Distance versus Capillary Flow Dynamics-Based Detection Methods on a Microfluidic Paper-Based Analytical Device (Î¼PAD),2019 Oct 11;25(57):13070-13077.,"In recent years, there has been high interest in paper-based microfluidic sensors or microfluidic paper-based analytical devices (Î¼PADs) towards low-cost, portable, and easy-to-use sensing for chemical and biological targets. Î¼PAD allows spontaneous liquid flow without any external or internal pumping, as well as an innate filtration capability. Although both optical (colorimetric and fluorescent) and electrochemical detection have been demonstrated on Î¼PADs, several limitations still remain, such as the need for additional equipment, vulnerability to ambient lighting perturbation, and inferior sensitivity. Herein, alternative detection methods on Î¼PADs are reviewed to resolve these issues, including relatively well studied distance-based measurements and the newer capillary flow dynamics-based method. Detection principles, assay performance, strengths, and weaknesses are explained for these methods, along with their potential future applications towards point-of-care medical diagnostics and other field-based applications."
31153774,30.0,The Heterogeneity Problem: Approaches to Identify Psychiatric Subtypes,2019 Jul;23(7):584-601.,"The imprecise nature of psychiatric nosology restricts progress towards characterizing and treating mental health disorders. One issue is the 'heterogeneity problem': different causal mechanisms may relate to the same disorder, and multiple outcomes of interest can occur within one individual. Our review tackles this heterogeneity problem, providing considerations, concepts, and approaches for investigators examining human cognition and mental health. We highlight the difficulty of pure dimensional approaches due to 'the curse of dimensionality'. Computationally, we consider supervised and unsupervised statistical approaches to identify putative subtypes within a population. However, we emphasize that subtype identification should be linked to a particular outcome or question. We conclude with novel hybrid approaches that can identify subtypes tied to outcomes, and may help advance precision diagnostic and treatment tools."
31153580,2.0,Diagnostic performance of fractional flow reserve derived from coronary CT angiography for detection of lesion-specific ischemia: A multi-center study and meta-analysis,2019 Jul;116:90-97.,"Purpose:                    To evaluate the diagnostic performance of coronary computed tomography angiography derived fractional flow reserve (CT-FFR) with invasive fractional flow reserve (FFR) in patients with coronary artery disease"" before ""with invasive fractional flow reserve serving as the reference standard.              Materials and methods:                    CT-FFR values based on a machine learning algorithm (cFFRML) in 183 vessels of 136 patients from four centers were measured with invasive FFR as reference standard. The diagnostic performance from our multicenter study was combined into a meta-analysis following a literature search in Web of Science, PubMed, Cochrane library to identify studies comparing diagnostic performance of coronary computed tomography angiography (CCTA) and CT-FFR. Sensitivity, specificity, accuracy were analyzed on both per-vessel and per-patient basis for intermediate lesions and by algorithm.              Results:                    Our multicenter study demonstrated sensitivities, specificities, and accuracies of cFFRML and CCTA of 0.85, 0.94, 0.90, and 0.95, 0.28, 0.55 on a per-vessel basis, respectively. For our meta-analysis, pooled sensitivities, specificities, and accuracies of CT-FFR and CCTA were 0.85, 0.82, 0.82, and 0.85, 0.57, 0.65 with AUC of 0.86 (95%CI: 0.83Ëœ0.89) and 0.83 (95%CI: 0.79Ëœ0.86) on a per-vessel basis, respectively. The sensitivity, specificity and accuracy for intermediate lesions using cFFRML were 0.84, 0.92, and 0.89. No significant difference was found among different algorithms of CT-FFR (P < 0.001).              Conslusion:                    This multicenter study with meta-analysis showed that CT-FFR had a high diagnostic accuracy in determining ischemia-specific lesions and intermediate lesions. There was no significant difference when comparing the combined diagnostic performance of different algorithms of CT-FFR with invasive FFR as the reference standard."
31153403,2.0,Computer-aided diagnosis of congestive heart failure using ECG signals - A review,2019 Jun;62:95-104.,"The heart muscle pumps blood to vital organs, which is indispensable for human life. Congestive heart failure (CHF) is characterized by the inability of the heart to pump blood adequately throughout the body without an increase in intracardiac pressure. The symptoms include lung and peripheral congestion, leading to breathing difficulty and swollen limbs, dizziness from reduced delivery of blood to the brain, as well as arrhythmia. Coronary artery disease, myocardial infarction, and medical co-morbidities such as kidney disease, diabetes, and high blood pressure all take a toll on the heart and can impair myocardial function. CHF prevalence is growing worldwide. It afflicts millions of people globally, and is a leading cause of death. Hence, proper diagnosis, monitoring and management are imperative. The importance of an objective CHF diagnostic tool cannot be overemphasized. Standard diagnostic tests for CHF include chest X-ray, magnetic resonance imaging (MRI), nuclear imaging, echocardiography, and invasive angiography. However, these methods are costly, time-consuming, and they can be operator-dependent. Electrocardiography (ECG) is inexpensive and widely accessible, but ECG changes are typically not specific for CHF diagnosis. A properly designed computer-aided detection (CAD) system for CHF, based on the ECG, would potentially reduce subjectivity and provide quantitative assessment for informed decision-making. Herein, we review existing CAD for automatic CHF diagnosis, and highlight the development of an ECG-based CAD diagnostic system that employs deep learning algorithms to automatically detect CHF."
31153155,3.0,Lumbar spondylolisthesis: modern registries and the development of artificial intelligence,2019 Jun 1;30(6):729-735.,"OBJECTIVEThere are a wide variety of comparative treatment options in neurosurgery that do not lend themselves to traditional randomized controlled trials. The object of this article was to examine how clinical registries might be used to generate new evidence to support a particular treatment option when comparable options exist. Lumbar spondylolisthesis is used as an example.METHODSThe authors reviewed the literature examining the comparative effectiveness of decompression alone versus decompression with fusion for lumbar stenosis with degenerative spondylolisthesis. Modern data acquisition for the creation of registries was also reviewed with an eye toward how artificial intelligence for the treatment of lumbar spondylolisthesis might be explored.RESULTSCurrent randomized controlled trials differ on the importance of adding fusion when performing decompression for lumbar spondylolisthesis. Standardized approaches to extracting data from the electronic medical record as well as the ability to capture radiographic imaging and incorporate patient-reported outcomes (PROs) will ultimately lead to the development of modern, structured, data-filled registries that will lay the foundation for machine learning.CONCLUSIONSThere is a growing realization that patient experience, satisfaction, and outcomes are essential to improving the overall quality of spine care. There is a need to use practical, validated PRO tools in the quest to optimize outcomes within spine care. Registries will be designed to contain robust clinical data in which predictive analytics can be generated to develop and guide data-driven personalized spine care."
31151893,9.0,A Road Map for Translational Research on Artificial Intelligence in Medical Imaging: From the 2018 National Institutes of Health/RSNA/ACR/The Academy Workshop,2019 Sep;16(9 Pt A):1179-1189.,"Advances in machine learning in medical imaging are occurring at a rapid pace in research laboratories both at academic institutions and in industry. Important artificial intelligence (AI) tools for diagnostic imaging include algorithms for disease detection and classification, image optimization, radiation reduction, and workflow enhancement. Although advances in foundational research are occurring rapidly, translation to routine clinical practice has been slower. In August 2018, the National Institutes of Health assembled multiple relevant stakeholders at a public meeting to discuss the current state of knowledge, infrastructure gaps, and challenges to wider implementation. The conclusions of that meeting are summarized in two publications that identify and prioritize initiatives to accelerate foundational and translational research in AI for medical imaging. This publication summarizes key priorities for translational research developed at the workshop including: (1) creating structured AI use cases, defining and highlighting clinical challenges potentially solvable by AI; (2) establishing methods to encourage data sharing for training and testing AI algorithms to promote generalizability to widespread clinical practice and mitigate unintended bias; (3) establishing tools for validation and performance monitoring of AI algorithms to facilitate regulatory approval; and (4) developing standards and common data elements for seamless integration of AI tools into existing clinical workflows. An important goal of the resulting road map is to grow an ecosystem, facilitated by professional societies, industry, and government agencies, that will allow robust collaborations between practicing clinicians and AI researchers to advance foundational and translational research relevant to medical imaging."
31151223,17.0,In-Vivo and Ex-Vivo Tissue Analysis through Hyperspectral Imaging Techniques: Revealing the Invisible Features of Cancer,2019 May 30;11(6):756.,"In contrast to conventional optical imaging modalities, hyperspectral imaging (HSI) is able to capture much more information from a certain scene, both within and beyond the visual spectral range (from 400 to 700 nm). This imaging modality is based on the principle that each material provides different responses to light reflection, absorption, and scattering across the electromagnetic spectrum. Due to these properties, it is possible to differentiate and identify the different materials/substances presented in a certain scene by their spectral signature. Over the last two decades, HSI has demonstrated potential to become a powerful tool to study and identify several diseases in the medical field, being a non-contact, non-ionizing, and a label-free imaging modality. In this review, the use of HSI as an imaging tool for the analysis and detection of cancer is presented. The basic concepts related to this technology are detailed. The most relevant, state-of-the-art studies that can be found in the literature using HSI for cancer analysis are presented and summarized, both in-vivo and ex-vivo. Lastly, we discuss the current limitations of this technology in the field of cancer detection, together with some insights into possible future steps in the improvement of this technology."
31151154,1.0,Innovative MRI Techniques in Neuroimaging Approaches for Cerebrovascular Diseases and Vascular Cognitive Impairment,2019 May 30;20(11):2656.,"Cognitive impairment and dementia are recognized as major threats to public health. Many studies have shown the important role played by challenges to the cerebral vasculature and the neurovascular unit. To investigate the structural and functional characteristics of the brain, MRI has proven an invaluable tool for visualizing the internal organs of patients and analyzing the parameters related to neuronal activation and blood flow in vivo. Different strategies of imaging can be combined to obtain various parameters: (i) measures of cortical and subcortical structures (cortical thickness, subcortical structures volume); (ii) evaluation of microstructural characteristics of the white matter (fractional anisotropy, mean diffusivity); (iii) neuronal activation and synchronicity to identify functional networks across different regions (functional connectivity between specific regions, graph measures of specific nodes); and (iv) structure of the cerebral vasculature and its efficacy in irrorating the brain (main vessel diameter, cerebral perfusion). The high amount of data obtainable from multi-modal sources calls for methods of advanced analysis, like machine-learning algorithms that allow the discrimination of the most informative features, to comprehensively characterize the cerebrovascular network into specific and sensitive biomarkers. By using the same techniques of human imaging in pre-clinical research, we can also investigate the mechanisms underlying the pathophysiological alterations identified in patients by imaging, with the chance of looking for molecular mechanisms to recover the pathology or hamper its progression."
31149787,12.0,Promising Artificial Intelligence-Machine Learning-Deep Learning Algorithms in Ophthalmology,May-Jun 2019;8(3):264-272.,"The lifestyle of modern society has changed significantly with the emergence of artificial intelligence (AI), machine learning (ML), and deep learning (DL) technologies in recent years. Artificial intelligence is a multidimensional technology with various components such as advanced algorithms, ML and DL. Together, AI, ML, and DL are expected to provide automated devices to ophthalmologists for early diagnosis and timely treatment of ocular disorders in the near future. In fact, AI, ML, and DL have been used in ophthalmic setting to validate the diagnosis of diseases, read images, perform corneal topographic mapping and intraocular lens calculations. Diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are the 3 most common causes of irreversible blindness on a global scale. Ophthalmic imaging provides a way to diagnose and objectively detect the progression of a number of pathologies including DR, AMD, glaucoma, and other ophthalmic disorders. There are 2 methods of imaging used as diagnostic methods in ophthalmic practice: fundus digital photography and optical coherence tomography (OCT). Of note, OCT has become the most widely used imaging modality in ophthalmology settings in the developed world. Changes in population demographics and lifestyle, extension of average lifespan, and the changing pattern of chronic diseases such as obesity, diabetes, DR, AMD, and glaucoma create a rising demand for such images. Furthermore, the limitation of availability of retina specialists and trained human graders is a major problem in many countries. Consequently, given the current population growth trends, it is inevitable that analyzing such images is time-consuming, costly, and prone to human error. Therefore, the detection and treatment of DR, AMD, glaucoma, and other ophthalmic disorders through unmanned automated applications system in the near future will be inevitable. We provide an overview of the potential impact of the current AI, ML, and DL methods and their applications on the early detection and treatment of DR, AMD, glaucoma, and other ophthalmic diseases."
31145684,,Regression analysis for detecting epileptic seizure with different feature extracting strategies,2019 Dec 18;64(6):619-642.,"Due to the excitability of neurons in the brain, a neurological disorder is produced known as epilepsy. The brain activity of patients suffering from epilepsy is monitored through electroencephalography (EEG). The multivariate nature of features from time domain, frequency domain, complexity and wavelet entropy based, and the statistical features were extracted from healthy and epileptic subjects using the Bonn University database and seizure and non-seizure intervals using the CHB MIT database. The robust machine learning regression methods based on regression, support vector regression (SVR), regression tree (RT), ensemble regression, Gaussian process regression (GPR) were employed for detecting and predicting epileptic seizures. Performance was measured in terms of root mean square error (RMSE), squared error, mean square error (MSE) and mean absolute error (MAE). Moreover, detailed optimization was performed using a RT to predict the selected features from each feature category. A deeper analysis was conducted on features and tree regression methods where optimal RMSE and MSE results were obtained. The best optimal performance was obtained using the ensemble boosted regression tree (BRT) and exponential GPR with an RMSE of 0.47, an MSE (0.22), an R Square (RS) (0.25) and an MAE (0.30) using the Bonn University database and support vector machine (SVM) fine Gaussian with RMSE (0.63634), RS (0.03), MSE (0.40493) and MAE (0.31744); squared exponential GPR and rational quadratic GPR with an RMSE of 0.63841, an RS (0.03), an MSE (0.40757) and an MAE (0.3472) was obtained using the CHB MIT database. A further deeper analysis for the prediction of selected features was performed on an RT to compute the optimal feasible point, observed and estimated function values, function evaluation time, objective function evaluation time and overall elapsed time."
31144777,1.0,Operational framework and training standard requirements for AI-empowered robotic surgery,2020 Oct;16(5):1-13.,"Background:                    For autonomous robot-delivered surgeries to ever become a feasible option, we recommend the combination of human-centered artificial intelligence (AI) and transparent machine learning (ML), with integrated Gross anatomy models. This can be supplemented with medical imaging data of cadavers for performance evaluation.              Methods:                    We reviewed technological advances and state-of-the-art documented developments. We undertook a literature search on surgical robotics and skills, tracing agent studies, relevant frameworks, and standards for AI. This embraced transparency aspects of AI.              Conclusion:                    We recommend ""a procedure/skill template"" for teaching AI that can be used by a surgeon. Similar existing methodologies show that when such a metric-based approach is used for training surgeons, cardiologists, and anesthetists, it results in a >40% error reduction in objectively assessed intraoperative procedures. The integration of Explainable AI and ML, and novel tissue characterization sensorics to tele-operated robotic-assisted procedures with medical imaged cadavers, provides robotic guidance and refines tissue classifications at a molecular level."
31144539,,Quantitative assessment of the activity of antituberculosis drugs and regimens,2019 Jun;17(6):449-457.,"Introduction: Identification of optimal drug doses and drug combinations is crucial for optimized treatment of tuberculosis. Areas covered: An unprecedented level of research activity involving multiple approaches is seeking to improve tuberculosis treatment. This report is a review of the quantitative methods currently used on clinical data sets to identify drug exposure targets and optimal drug combinations for tuberculosis treatment. A high-level summary of the methods, including the strengths and weaknesses of each method and potential methodological improvements is presented. Methods incorporating data generated from multiple sources such as in vitro and clinical studies, and their potential to provide better estimates of pharmacokinetic/pharmacodynamic (PK/PD) targets, are discussed. PK/PD relationships identified are compared between different studies and data analysis methods. Expert opinion: The relationships between drug exposures and tuberculosis treatment outcomes are complex and require analytical methods capable of handling the multidimensional nature of the relationships. The choice of a method is guided by its complexity, interpretability of results, and type of data available."
31144302,14.0,Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice,2019 Oct;249(2):143-150.,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence-based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM-Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. Â© 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd."
31140082,8.0,Applications of deep learning for the analysis of medical data,2019 Jun;42(6):492-504.,"Over the past decade, deep learning has demonstrated superior performances in solving many problems in various fields of medicine compared with other machine learning methods. To understand how deep learning has surpassed traditional machine learning techniques, in this review, we briefly explore the basic learning algorithms underlying deep learning. In addition, the procedures for building deep learning-based classifiers for seizure electroencephalograms and gastric tissue slides are described as examples to demonstrate the simplicity and effectiveness of deep learning applications. Finally, we review the clinical applications of deep learning in radiology, pathology, and drug discovery, where deep learning has been actively adopted. Considering the great advantages of deep learning techniques, deep learning will be increasingly and widely utilized in a wide variety of different areas in medicine in the coming decades."
31138507,8.0,Identification of senescent cells in multipotent mesenchymal stromal cell cultures: Current methods and future directions,2019 Aug;21(8):803-819.,"Regardless of their tissue of origin, multipotent mesenchymal stromal cells (MSCs) are commonly expanded in vitro for several population doublings to achieve a sufficient number of cells for therapy. Prolonged MSC expansion has been shown to result in phenotypical, morphological and gene expression changes in MSCs, which ultimately lead to the state of senescence. The presence of senescent cells in therapeutic MSC batches is undesirable because it reduces their viability, differentiation potential and trophic capabilities. Additionally, senescent cells acquire senescence-activated secretory phenotype, which may not only induce apoptosis in the neighboring host cells following MSC transplantation, but also trigger local inflammatory reactions. This review outlines the current and promising new methodologies for the identification of senescent cells in MSC cultures, with a particular emphasis on non-destructive and label-free methodologies. Technologies allowing identification of individual senescent cells, based on new surface markers, offer potential advantage for targeted senescent cell removal using new-generation senolytic agents, and subsequent production of therapeutic MSC batches fully devoid of senescent cells. Methods or a combination of methods that are non-destructive and label-free, for example, involving cell size and spectroscopic measurements, could be the best way forward because they do not modify the cells of interest, thus maximizing the final output of therapeutic-grade MSC cultures. The further incorporation of machine learning methods has also recently shown promise in facilitating, automating and enhancing the analysis of these measured data."
32903302,,Robots in laparoscopic surgery: current and future status,2019 May 29;1:12.,"In this paper, we focus on robots used for laparoscopic surgery, which is one of the most active areas for research and development of surgical robots. We introduce research and development of laparoscope-holder robots, master-slave robots and hand-held robotic forceps. Then, we discuss future directions for surgical robots. For robot hardware, snake like flexible mechanisms for single-port access surgery (SPA) and NOTES (Natural Orifice Transluminal Endoscopic Surgery) and applications of soft robotics are actively used. On the software side, research such as automation of surgical procedures using machine learning is one of the hot topics."
31134293,2.0,Computational approaches and machine learning for individual-level treatment predictions,2021 May;238(5):1231-1239.,"Rationale:                    The impact of neuroscience-based approaches for psychiatry on pragmatic clinical decision-making has been limited. Although neuroscience has provided insights into basic mechanisms of neural function, these insights have not improved the ability to generate better assessments, prognoses, diagnoses, or treatment of psychiatric conditions.              Objectives:                    To integrate the emerging findings in machine learning and computational psychiatry to address the question: what measures that are not derived from the patient's self-assessment or the assessment by a trained professional can be used to make more precise predictions about the individual's current state, the individual's future disease trajectory, or the probability to respond to a particular intervention?              Results:                    Currently, the ability to use individual differences to predict differential outcomes is very modest possibly related to the fact that the effect sizes of interventions are small. There is emerging evidence of genetic and neuroimaging-based heterogeneity of psychiatric disorders, which contributes to imprecise predictions. Although the use of machine learning tools to generate clinically actionable predictions is still in its infancy, these approaches may identify subgroups enabling more precise predictions. In addition, computational psychiatry might provide explanatory disease models based on faulty updating of internal values or beliefs.              Conclusions:                    There is a need for larger studies, clinical trials using machine learning, or computational psychiatry model parameters predictions as actionable outcomes, comparing alternative explanatory computational models, and using translational approaches that apply similar paradigms and models in humans and animals."
31133758,76.0,Deep learning for cellular image analysis,2019 Dec;16(12):1233-1246.,"Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field's progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs' experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application."
31132292,,Machine Learning in the Detection of the Glaucomatous Disc and Visual Field,2019;34(4):232-242.,"Glaucoma is the leading cause of irreversible blindness worldwide. Early detection is of utmost importance as there is abundant evidence that early treatment prevents disease progression, preserves vision, and improves patients' long-term quality of life. The structure and function thresholds that alert to the diagnosis of glaucoma can be obtained entirely via digital means, and as such, screening is well suited to benefit from artificial intelligence and specifically machine learning. This paper reviews the concepts and current literature on the use of machine learning for detection of the glaucomatous disc and visual field."
31131957,3.0,Meta-analysis of massively parallel reporter assays enables prediction of regulatory function across cell types,2019 Sep;40(9):1299-1313.,"Deciphering the potential of noncoding loci to influence gene regulation has been the subject of intense research, with important implications in understanding genetic underpinnings of human diseases. Massively parallel reporter assays (MPRAs) can measure regulatory activity of thousands of DNA sequences and their variants in a single experiment. With increasing number of publically available MPRA data sets, one can now develop data-driven models which, given a DNA sequence, predict its regulatory activity. Here, we performed a comprehensive meta-analysis of several MPRA data sets in a variety of cellular contexts. We first applied an ensemble of methods to predict MPRA output in each context and observed that the most predictive features are consistent across data sets. We then demonstrate that predictive models trained in one cellular context can be used to predict MPRA output in another, with loss of accuracy attributed to cell-type-specific features. Finally, we show that our approach achieves top performance in the Fifth Critical Assessment of Genome Interpretation ""Regulation Saturation"" Challenge for predicting effects of single-nucleotide variants. Overall, our analysis provides insights into how MPRA data can be leveraged to highlight functional regulatory regions throughout the genome and can guide effective design of future experiments by better prioritizing regions of interest."
31131140,8.0,Analyzing and Visualizing Knowledge Structures of Health Informatics from 1974 to 2018: A Bibliometric and Social Network Analysis,2019 Apr;25(2):61-72.,"Objectives:                    This paper aims to provide a theoretical clarification of the health informatics field by conducting a quantitative review analysis of the health informatics literature. And this paper aims to map scientific networks; to uncover the explicit and hidden patterns, knowledge structures, and sub-structures in scientific networks; to track the flow and burst of scientific topics; and to discover what effects they have on the scientific growth of health informatics.              Methods:                    This study was a quantitative literature review of the health informatics field, employing text mining and bibliometric research methods. This paper reviews 30,115 articles with health informatics as their topic, which are indexed in the Web of Science Core Collection Database from 1974 to 2018. This study analyzed and mapped four networks: author co-citation network, co-occurring author keywords and keywords plus, co-occurring subject categories, and country co-citation network. We used CiteSpace 5.3 and VOSviewer to analyze data, and we used Gephi 0.9.2 and VOSviewer to visualize the networks.              Results:                    This study found that the three major themes of the literature from 1974 to 2018 were the utilization of computer science in healthcare, the impact of health informatics on patient safety and the quality of healthcare, and decision support systems. The study found that, since 2016, health informatics has entered a new era to provide predictive, preventative, personalized, and participatory healthcare systems.              Conclusions:                    This study found that the future strands of research may be patient-generated health data, deep learning algorithms, quantified self and self-tracking tools, and Internet of Things based decision support systems."
31129651,1.0,Controversies in diagnosis: contemporary debates in the diagnostic safety literature,2020 Jan 28;7(1):3-9.,"Since the 2015 publication of the National Academy of Medicine's (NAM) Improving Diagnosis in Health Care (Improving Diagnosis in Health Care. In: Balogh EP, Miller BT, Ball JR, editors. Improving Diagnosis in Health Care. Washington (DC): National Academies Press, 2015.), literature in diagnostic safety has grown rapidly. This update was presented at the annual international meeting of the Society to Improve Diagnosis in Medicine (SIDM). We focused our literature search on articles published between 2016 and 2018 using keywords in Pubmed and the Agency for Healthcare Research and Quality (AHRQ)'s Patient Safety Network's running bibliography of diagnostic error literature (Diagnostic Errors Patient Safety Network: Agency for Healthcare Research and Quality; Available from: https://psnet.ahrq.gov/search?topic=Diagnostic-Errors&f_topicIDs=407). Three key topics emerged from our review of recent abstracts in diagnostic safety. First, definitions of diagnostic error and related concepts are evolving since the NAM's report. Second, medical educators are grappling with new approaches to teaching clinical reasoning and diagnosis. Finally, the potential of artificial intelligence (AI) to advance diagnostic excellence is coming to fruition. Here we present contemporary debates around these three topics in a pro/con format."
31123497,12.0,The role of emotions in cancer patients' decision-making,2019 Mar 28;13:914.,"Introduction:                    Despite the attempt to make decisions based on evidence, doctors still have to consider patients' choices which often involve other factors. In particular, emotions seem to influence the way that options and the surrounding information are interpreted and used.              Objective:                    The objective of the present review is to provide a brief overview of research on decision making and cancer with a specific focus on the role of emotions.              Method:                    Thirty-nine studies were identified and analysed. Most of the studies investigated anxiety and fear. Worry was the other psychological factor that, together with anxiety, played a crucial role in cancer-related decision-making.              Results:                    The roles of fear, anxiety and worry were described for detection behaviour, diagnosis, choice about prevention and curative treatments and help-seeking behaviour. Results were inconsistent among the studies. Results stressed that cognitive appraisal and emotional arousal (emotion's intensity level) interact in shaping the decision. Moderate levels of anxiety and worry improved decision-making, while low and high levels tended to have no effect or a hindering effect on decision making. Moderating factors played an under-investigated role.              Conclusions:                    Decision making is a complex non-linear process that is affected by several factors, such as, for example, personal knowledge, past experiences, individual differences and certainly emotions. Research studies should investigate further potential moderators of the effect of emotions on cancer-related choice. Big data and machine learning could be a good opportunity to test the interaction between a large amount of factors that is not feasible in traditional research. New technologies such as eHealth and virtual reality can offer support for the regulation of emotions and decision making."
31119599,18.0,A Brief History of Protein Sorting Prediction,2019 Jun;38(3):200-216.,"Ever since the signal hypothesis was proposed in 1971, the exact nature of signal peptides has been a focus point of research. The prediction of signal peptides and protein subcellular location from amino acid sequences has been an important problem in bioinformatics since the dawn of this research field, involving many statistical and machine learning technologies. In this review, we provide a historical account of how position-weight matrices, artificial neural networks, hidden Markov models, support vector machines and, lately, deep learning techniques have been used in the attempts to predict where proteins go. Because the secretory pathway was the first one to be studied both experimentally and through bioinformatics, our main focus is on the historical development of prediction methods for signal peptides that target proteins for secretion; prediction methods to identify targeting signals for other cellular compartments are treated in less detail."
31118821,84.0,Taiwan's National Health Insurance Research Database: past and future,2019 May 3;11:349-358.,"Taiwan's National Health Insurance Research Database (NHIRD) exemplifies a population-level data source for generating real-world evidence to support clinical decisions and health care policy-making. Like with all claims databases, there have been some validity concerns of studies using the NHIRD, such as the accuracy of diagnosis codes and issues around unmeasured confounders. Endeavors to validate diagnosed codes or to develop methodologic approaches to address unmeasured confounders have largely increased the reliability of NHIRD studies. Recently, Taiwan's Ministry of Health and Welfare (MOHW) established a Health and Welfare Data Center (HWDC), a data repository site that centralizes the NHIRD and about 70 other health-related databases for data management and analyses. To strengthen the protection of data privacy, investigators are required to conduct on-site analysis at an HWDC through remote connection to MOHW servers. Although the tight regulation of this on-site analysis has led to inconvenience for analysts and has increased time and costs required for research, the HWDC has created opportunities for enriched dimensions of study by linking across the NHIRD and other databases. In the near future, researchers will have greater opportunity to distill knowledge from the NHIRD linked to hospital-based electronic medical records databases containing unstructured patient-level information by using artificial intelligence techniques, including machine learning and natural language processes. We believe that NHIRD with multiple data sources could represent a powerful research engine with enriched dimensions and could serve as a guiding light for real-world evidence-based medicine in Taiwan."
31118321,,Evaluation of Chatbot Prototypes for Taking the Virtual Patient's History,2019;260:73-80.,"In medical education Virtual Patients (VP) are often applied to train students in different scenarios such as recording the patient's medical history or deciding a treatment option. Usually, such interactions are predefined by software logic and databases following strict rules. At this point, Natural Language Processing/Machine Learning (NLP/ML) algorithms could help to increase the overall flexibility, since most of the rules can derive directly from training data. This would allow a more sophisticated and individual conversation between student and VP. One type of technology that is heavily based on such algorithmic advances are chatbots or conversational agents. Therefore, a literature review is carried out to give insight into existing educational ideas with such agents. Besides, different prototypes are implemented for the scenario of taking the patient's medical history, responding with the classified intent of a generic anamnestic question. Although the small number of questions (n=109) leads to a high SD during evaluation, all scores (recall, precision, f1) reach already a level above 80% (micro-averaged). This shows a first promising step to use these prototypes for taking the medical history of a VP."
31116112,,The promise and perils of 'Big Data': focus on spondyloarthritis,2019 Jul;31(4):355-361.,"Purpose of review:                    This review will describe the available large-scale data sources to study spondyloarthritis (SpA), enumerate approaches to identify SpA and its disease-related manifestations and outcomes, and will outline existing and future methods to collect novel data types [e.g. patient-reported outcomes (PRO), passive data from wearables and biosensors].              Recent findings:                    In addition to traditional clinic visit-based SpA registries, newer data sources, such as health plan claims data, single and multispecialty electronic health record (EHR) based registries, patient registries and linkages between data sources, have catalyzed the breadth and depth of SpA research. Health activity tracker devices and PRO collected via PROMIS instruments have been shown to have good validity when assessed in SpA patients as compared to legacy disease-specific instruments. In certain cases, machine learning outperforms traditional methods to identify SpA and its associated manifestations in EHR and claims data, and may predict disease flare.              Summary:                    Although caution remains in the application of newer data sources and methods including the important need for replication, the availability of new data sources, health tracker devices and analytic methods holds great promise to catalyze SpA research."
31115885,,A Review of Microarray Datasets: Where to Find Them and Specific Characteristics,2019;1986:65-85.,"The advent of DNA microarray datasets has stimulated a new line of research both in bioinformatics and in machine learning. This type of data is used to collect information from tissue and cell samples regarding gene expression differences that could be useful for disease diagnosis or for distinguishing specific types of tumor. Microarray data classification is a difficult challenge for machine learning researchers due to its high number of features and the small sample sizes. This chapter is devoted to reviewing the microarray databases most frequently used in the literature. We also make the interested reader aware of the problematic of data characteristics in this domain, such as the imbalance of the data, their complexity, and the so-called dataset shift."
31114635,1.0,"Innovative strategies for annotating the ""relationSNP"" between variants and molecular phenotypes",2019 May 14;12:10.,"Characterizing how variation at the level of individual nucleotides contributes to traits and diseases has been an area of growing interest since the completion of sequencing the first human genome. Our understanding of how a single nucleotide polymorphism (SNP) leads to a pathogenic phenotype on a genome-wide scale is a fruitful endeavor for anyone interested in developing diagnostic tests, therapeutics, or simply wanting to understand the etiology of a disease or trait. To this end, many datasets and algorithms have been developed as resources/tools to annotate SNPs. One of the most common practices is to annotate coding SNPs that affect the protein sequence. Synonymous variants are often grouped as one type of variant, however there are in fact many tools available to dissect their effects on gene expression. More recently, large consortiums like ENCODE and GTEx have made it possible to annotate non-coding regions. Although annotating variants is a common technique among human geneticists, the constant advances in tools and biology surrounding SNPs requires an updated summary of what is known and the trajectory of the field. This review will discuss the history behind SNP annotation, commonly used tools, and newer strategies for SNP annotation. Additionally, we will comment on the caveats that distinguish approaches from one another, along with gaps in the current state of knowledge, and potential future directions. We do not intend for this to be a comprehensive review for any specific area of SNP annotation, but rather it will be an excellent resource for those unfamiliar with computational tools used to functionally characterize SNPs. In summary, this review will help illustrate how each SNP annotation method impacts the way in which the genetic and molecular etiology of a disease is explored in-silico."
31112393,9.0,Applications and limitations of machine learning in radiation oncology,2019 Aug;92(1100):20190001.,"Machine learning approaches to problem-solving are growing rapidly within healthcare, and radiation oncology is no exception. With the burgeoning interest in machine learning comes the significant risk of misaligned expectations as to what it can and cannot accomplish. This paper evaluates the role of machine learning and the problems it solves within the context of current clinical challenges in radiation oncology. The role of learning algorithms within the workflow for external beam radiation therapy are surveyed, considering simulation imaging, multimodal fusion, image segmentation, treatment planning, quality assurance, and treatment delivery and adaptation. For each aspect, the clinical challenges faced, the learning algorithms proposed, and the successes and limitations of various approaches are analyzed. It is observed that machine learning has largely thrived on reproducibly mimicking conventional human-driven solutions with more efficiency and consistency. On the other hand, since algorithms are generally trained using expert opinion as ground truth, machine learning is of limited utility where problems or ground truths are not well-defined, or if suitable measures of correctness are not available. As a result, machines may excel at replicating, automating and standardizing human behaviour on manual chores, meanwhile the conceptual clinical challenges relating to definition, evaluation, and judgement remain in the realm of human intelligence and insight."
31111616,4.0,Quantitative cardiac MRI,2020 Mar;51(3):693-711.,"Cardiac MRI has become an indispensable imaging modality in the investigation of patients with suspected heart disease. It has emerged as the gold standard test for cardiac function, volumes, and mass and allows noninvasive tissue characterization and the assessment of myocardial perfusion. Quantitative MRI already has a key role in the development and incorporation of machine learning in clinical imaging, potentially offering major improvements in both workflow efficiency and diagnostic accuracy. As the clinical applications of a wide range of quantitative cardiac MRI techniques are being explored and validated, we are expanding our capabilities for earlier detection, monitoring, and risk stratification of disease, potentially guiding personalized management decisions in various cardiac disease models. In this article we review established and emerging quantitative techniques, their clinical applications, highlight novel advances, and appraise their clinical diagnostic potential. Level of Evidence: 2 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2020;51:693-711."
31111458,7.0,A primer in artificial intelligence in cardiovascular medicine,2019 Sep;27(9):392-402.,"Driven by recent developments in computational power, algorithms and web-based storage resources, machine learning (ML)-based artificial intelligence (AI) has quickly gained ground as the solution for many technological and societal challenges. AI education has become very popular and is oversubscribed at Dutch universities. Major investments were made in 2018 to develop and build the first AI-driven hospitals to improve patient care and reduce healthcare costs. AI has the potential to greatly enhance traditional statistical analyses in many domains and has been demonstrated to allow the discovery of 'hidden' information in highly complex datasets. As such, AI can also be of significant value in the diagnosis and treatment of cardiovascular disease, and the first applications of AI in the cardiovascular field are promising. However, many professionals in the cardiovascular field involved in patient care, education or science are unaware of the basics behind AI and the existing and expected applications in their field. In this review, we aim to introduce the broad cardiovascular community to the basics of modern ML-based AI and explain several of the commonly used algorithms. We also summarise their initial and future applications relevant to the cardiovascular field."
31109150,4.0,Gap Junction Channels of Innexins and Connexins: Relations and Computational Perspectives,2019 May 19;20(10):2476.,"Gap junction (GJ) channels in invertebrates have been used to understand cell-to-cell communication in vertebrates. GJs are a common form of intercellular communication channels which connect the cytoplasm of adjacent cells. Dysregulation and structural alteration of the gap junction-mediated communication have been proven to be associated with a myriad of symptoms and tissue-specific pathologies. Animal models relying on the invertebrate nervous system have exposed a relationship between GJs and the formation of electrical synapses during embryogenesis and adulthood. The modulation of GJs as a therapeutic and clinical tool may eventually provide an alternative for treating tissue formation-related diseases and cell propagation. This review concerns the similarities between Hirudo medicinalis innexins and human connexins from nucleotide and protein sequence level perspectives. It also sets forth evidence of computational techniques applied to the study of proteins, sequences, and molecular dynamics. Furthermore, we propose machine learning techniques as a method that could be used to study protein structure, gap junction inhibition, metabolism, and drug development."
33137726,,Computer vision and machine learning in science fiction,2019 May 22;4(30):eaax7421.,Science fiction has a cautionary view of computer vision and machine learning.
31108201,,"Single-cell approaches to cell competition: High-throughput imaging, machine learning and simulations",2020 Jun;63:60-68.,"Cell competition is a quality control mechanism in tissues that results in the elimination of less fit cells. Over the past decade, the phenomenon of cell competition has been identified in many physiological and pathological contexts, driven either by biochemical signaling or by mechanical forces within the tissue. In both cases, competition has generally been characterized based on the elimination of loser cells at the population level, but significantly less attention has been focused on determining how single-cell dynamics and interactions regulate population-wide changes. In this review, we describe quantitative strategies and outline the outstanding challenges in understanding the single cell rules governing tissue-scale competition dynamics. We propose quantitative metrics to characterize single cell behaviors in competition and use them to distinguish the types and outcomes of competition. We describe how such metrics can be measured experimentally using a novel combination of high-throughput imaging and machine learning algorithms. We outline the experimental challenges to quantify cell fate dynamics with high-statistical precision, and describe the utility of computational modeling in testing hypotheses not easily accessible in experiments. In particular, cell-based modeling approaches that combine mechanical interaction of cells with decision-making rules for cell fate choices provide a powerful framework to understand and reverse-engineer the diverse rules of cell competition."
31107871,6.0,Animal models of chemotherapy-induced peripheral neuropathy: A machine-assisted systematic review and meta-analysis,2019 May 20;17(5):e3000243.,"We report a systematic review and meta-analysis of research using animal models of chemotherapy-induced peripheral neuropathy (CIPN). We systematically searched 5 online databases in September 2012 and updated the search in November 2015 using machine learning and text mining to reduce the screening for inclusion workload and improve accuracy. For each comparison, we calculated a standardised mean difference (SMD) effect size, and then combined effects in a random-effects meta-analysis. We assessed the impact of study design factors and reporting of measures to reduce risks of bias. We present power analyses for the most frequently reported behavioural tests; 337 publications were included. Most studies (84%) used male animals only. The most frequently reported outcome measure was evoked limb withdrawal in response to mechanical monofilaments. There was modest reporting of measures to reduce risks of bias. The number of animals required to obtain 80% power with a significance level of 0.05 varied substantially across behavioural tests. In this comprehensive summary of the use of animal models of CIPN, we have identified areas in which the value of preclinical CIPN studies might be increased. Using both sexes of animals in the modelling of CIPN, ensuring that outcome measures align with those most relevant in the clinic, and the animal's pain contextualised ethology will likely improve external validity. Measures to reduce risk of bias should be employed to increase the internal validity of studies. Different outcome measures have different statistical power, and this can refine our approaches in the modelling of CIPN."
31104157,2.0,Connected Health Technology for Cardiovascular Disease Prevention and Management,2019 May 18;21(6):29.,"Purpose of the review:                    Advances in computing power and wireless technologies have reshaped our approach to patient monitoring. Medical grade sensors and apps that were once restricted to hospitals and specialized clinic are now widely available. Here, we review the current evidence supporting the use of connected health technologies for the prevention and management of cardiovascular disease in an effort to highlight gaps and future opportunities for innovation.              Recent findings:                    Initial studies in connected health for cardiovascular disease prevention and management focused primarily on activity tracking and blood pressure monitoring but have since expanded to include a full panoply of novel sensors and pioneering smartphone apps with targeted interventions in diet, lipid management and risk assessment, smoking cessation, cardiac rehabilitation, heart failure, and arrhythmias. While outfitting patients with sensors and devices alone is infrequently a lasting solution, monitoring programs that include personalized insights based on patient-level data are more likely to lead to improved outcomes. Advances in this space have been driven by patients and researchers while healthcare systems remain slow to fully integrate and adequately adapt these new technologies into their workflows. Cardiovascular disease prevention and management continue to be key focus areas for clinicians and researchers in the connected health space. Exciting progress has been made though studies continue to suffer from small sample size and limited follow-up. Efforts that combine home patient monitoring, engagement, and personalized feedback are the most promising. Ultimately, combining patient-level ambulatory sensor data, electronic health records, and genomics using machine learning analytics will bring precision medicine closer to reality."
31099675,4.0,"The Impact of Big Data Research on Practice, Policy, and Cancer Care",2019 Jan;39:e167-e175.,"The concept of ""big data"" research-the aggregation and analysis of biologic, clinical, administrative, and other data sources to drive new advances in biomedical knowledge-has been embraced by the cancer research enterprise. Although much of the conversation has concentrated on the amalgamation of basic biologic data (e.g., genomics, metabolomics, tumor tissue), new opportunities to extend potential contributions of big data to clinical practice and policy abound. This article examines these opportunities through discussion of three major data sources: aggregated clinical trial data, administrative data (including insurance claims data), and data from electronic health records. We will discuss the benefits of data use to answer key oncology practice and policy research questions, along with limitations inherent in these complex data sources. Finally, the article will discuss overarching themes across data types and offer next steps for the research, practice, and policy communities. The use of multiple sources of big data has the promise of improving knowledge and providing more accurate data for clinicians and policy decision makers. In the future, optimization of machine learning may allow for current limitations of big data analyses to be attenuated, thereby resulting in improved patient care and outcomes."
31095701,13.0,Challenges in IBD Research: Precision Medicine,2019 May 16;25(Suppl 2):S31-S39.,"Precision medicine is part of five focus areas of the Challenges in IBD research document, which also includes preclinical human IBD mechanisms, environmental triggers, novel technologies, and pragmatic clinical research. The Challenges in IBD Research document provides a comprehensive overview of current gaps in inflammatory bowel diseases (IBD) research and delivers actionable approaches to address them. It is the result of a multidisciplinary input from scientists, clinicians, patients, and funders, and represents a valuable resource for patient centric research prioritization. In particular, the precision medicine section is focused on highlighting the main gap areas that must be addressed to get closer to treatments tailored to the biological and clinical characteristics of each patient, which is the aim of precision medicine. The main gaps were identified in: 1) understanding and predicting the natural history of IBD: disease susceptibility, activity, and behavior; 2) predicting disease course and treatment response; and 3) optimizing current and developing new molecular technologies. Suggested approaches to bridge these gaps include prospective longitudinal cohort studies to identify and validate precision biomarkers for prognostication of disease course, and prediction and monitoring of treatment response. To achieve this, harmonization across studies is key as well as development of standardized methods and infrastructure. The implementation of state-of-the-art molecular technologies, systems biology and machine learning approaches for multi-omics and clinical data integration and analysis will be also fundamental. Finally, randomized biomarker-stratified trials will be critical to evaluate the clinical utility of validated signatures and biomarkers in improving patient outcomes and cost-effective care."
31092914,31.0,A new era: artificial intelligence and machine learning in prostate cancer,2019 Jul;16(7):391-403.,"Artificial intelligence (AI) - the ability of a machine to perform cognitive tasks to achieve a particular goal based on provided data - is revolutionizing and reshaping our health-care systems. The current availability of ever-increasing computational power, highly developed pattern recognition algorithms and advanced image processing software working at very high speeds has led to the emergence of computer-based systems that are trained to perform complex tasks in bioinformatics, medical imaging and medical robotics. Accessibility to 'big data' enables the 'cognitive' computer to scan billions of bits of unstructured information, extract the relevant information and recognize complex patterns with increasing confidence. Computer-based decision-support systems based on machine learning (ML) have the potential to revolutionize medicine by performing complex tasks that are currently assigned to specialists to improve diagnostic accuracy, increase efficiency of throughputs, improve clinical workflow, decrease human resource costs and improve treatment choices. These characteristics could be especially helpful in the management of prostate cancer, with growing applications in diagnostic imaging, surgical interventions, skills training and assessment, digital pathology and genomics. Medicine must adapt to this changing world, and urologists, oncologists, radiologists and pathologists, as high-volume users of imaging and pathology, need to understand this burgeoning science and acknowledge that the development of highly accurate AI-based decision-support applications of ML will require collaboration between data scientists, computer researchers and engineers."
31090273,7.0,Genomic data mining for functional annotation of human long noncoding RNAs,2019 Jun;20(6):476-487.,"Life may have begun in an RNA world, which is supported by increasing evidence of the vital role that RNAs perform in biological systems. In the human genome, most genes actually do not encode proteins; they are noncoding RNA genes. The largest class of noncoding genes is known as long noncoding RNAs (lncRNAs), which are transcripts greater in length than 200 nucleotides, but with no protein-coding capacity. While some lncRNAs have been demonstrated to be key regulators of gene expression and 3D genome organization, most lncRNAs are still uncharacterized. We thus propose several data mining and machine learning approaches for the functional annotation of human lncRNAs by leveraging the vast amount of data from genetic and genomic studies. Recent results from our studies and those of other groups indicate that genomic data mining can give insights into lncRNA functions and provide valuable information for experimental studies of candidate lncRNAs associated with human disease."
31089906,13.0,Artificial Intelligence in Cardiovascular Medicine,2019 May 14;21(6):25.,"Purpose of review:                    The ripples of artificial intelligence are being felt in various sectors of human life. Machine learning, a subset of artificial intelligence, extracts information from large databases of information and is gaining traction in various fields of cardiology. In this review, we highlight noteworthy examples of machine learning utilization in echocardiography, nuclear cardiology, computed tomography, and magnetic resonance imaging over the past year.              Recent findings:                    In the past year, machine learning (ML) has expanded its boundaries in cardiology with several positive results. Some studies have integrated clinical and imaging information to further augment the accuracy of these ML algorithms. All the studies mentioned in this review have clearly demonstrated superior results of ML in relation to conventional approaches for identifying obstructions or predicting major adverse events in reference to conventional approaches. As the influx of data arriving from gradually evolving technologies in health care and wearable devices continues to be more complex, ML may serve as the bridge to transcend the gap between health care and patients in the future. In order to facilitate a seamless transition between both, a few issues must be resolved for a successful implementation of ML in health care."
31083923,2.0,Use of Magnetic Resonance Imaging and Artificial Intelligence in Studies of Diagnosis of Parkinson's Disease,2019 Jun 19;10(6):2658-2667.,"Parkinson's disease (PD) is a common neurodegenerative disorder. It has a delitescent onset and a slow progress. The clinical manifestations of PD in patients are highly heterogeneous. Thus, PD diagnosis process is complex and mainly depends on the professional knowledge and experience of the physician. Magnetic resonance imaging (MRI) could detect the small changes in the brain of PD patients, and quantitative analysis of brain MRI may improve the clinical diagnosis efficiency. However, due to the complexity of clinical courses in PD and the high dimensionality in multimodal MRI data, traditional mathematical analysis could not effectively extract the huge information in them. Up to now, the accuracy of PD diagnosis in large sample size is still unsatisfying. As artificial intelligence (AI) is becoming more mature, varieties of statistical models and machine learning (ML) algorithms have been used for quantitative imaging data analysis to explore a diagnostic result. This review aims to state an overview of existing research recently that used statistical ML/AI methods to perform quantitative analysis of MR image data for the study of PD diagnosis. First we review the recent research in three subareas: diagnosis, differential diagnosis, and subtyping of PD. Then we described the overall workflow from MR image to classification result. Finally, we summarized a critical assessment of the current research and provide some recommendations for likely future research developments and trends."
33467340,,The Development of a Personalised Training Framework: Implementation of Emerging Technologies for Performance,2019 May 16;4(2):25.,"Over the last decade, there has been considerable interest in the individualisation of athlete training, including the use of genetic information, alongside more advanced data capture and analysis techniques. Here, we explore the evidence for, and practical use of, a number of these emerging technologies, including the measurement and quantification of epigenetic changes, microbiome analysis and the use of cell-free DNA, along with data mining and machine learning. In doing so, we develop a theoretical model for the use of these technologies in an elite sport setting, allowing the coach to better answer six key questions: (1) To what training will my athlete best respond? (2) How well is my athlete adapting to training? (3) When should I change the training stimulus (i.e., has the athlete reached their adaptive ceiling for this training modality)? (4) How long will it take for a certain adaptation to occur? (5) How well is my athlete tolerating the current training load? (6) What load can my athlete handle today? Special consideration is given to whether such an individualised training framework will outperform current methods as well as the challenges in implementing this approach."
31079952,2.0,Putting machine learning into motion: applications in cardiovascular imaging,2020 Jan;75(1):33-37.,"Heart and circulatory diseases cause a quarter of all deaths in the UK and cardiac imaging offers an effective tool for early diagnosis and risk-stratification to improve premature death and disability. This domain of radiology is unique in that assessing flow and motion is essential for understanding and quantifying normal physiology and disease processes. Conventional image interpretation relies on manual analysis but this often fails to capture important prognostic features in the complex disturbances of cardiovascular physiology. Machine learning (ML) in cardiovascular imaging promises to be a transformative tool and addresses an unmet need for patient-specific management, accurate prediction of future events, and the discovery of tractable molecular mechanisms of disease. This review discusses the potential of ML across every aspect of image analysis including efficient acquisition, segmentation and motion tracking, disease classification, prediction tasks and modelling of genotype-phenotype interactions; however, significant challenges remain in access to high-quality data at scale, robust validation, and clinical interpretability."
31078331,4.0,Uncovering Ecological Patterns with Convolutional Neural Networks,2019 Aug;34(8):734-745.,"Using remotely sensed imagery to identify biophysical components across landscapes is an important avenue of investigation for ecologists studying ecosystem dynamics. With high-resolution remotely sensed imagery, algorithmic utilization of image context is crucial for accurate identification of biophysical components at large scales. In recent years, convolutional neural networks (CNNs) have become ubiquitous in image processing, and are rapidly becoming more common in ecology. Because the quantity of high-resolution remotely sensed imagery continues to rise, CNNs are increasingly essential tools for large-scale ecosystem analysis. We discuss here the conceptual advantages of CNNs, demonstrate how they can be used by ecologists through distinct examples of their application, and provide a walkthrough of how to use them for ecological applications."
31078239,1.0,Artificial Intelligence for the Treatment of Lumbar Spondylolisthesis,2019 Jul;30(3):383-389.,Multiple registries are currently collecting patient-specific data on lumbar spondylolisthesis including outcomes data. The collection of imaging diagnostics data along with comparative outcomes data following decompression versus decompression and fusion treatments for degenerative spondylolisthesis represents an enormous opportunity for modern machine-learning analytics research.
33178922,,"Artificial intelligence in oncology, its scope and future prospects with specific reference to radiation oncology",2019 May 13;1(1):20180031.,"Objective:                    Artificial intelligence (AI) seems to be bridging the gap between the acquisition of data and its meaningful interpretation. These approaches, have shown outstanding capabilities, outperforming most classification and regression methods to date and the ability to automatically learn the most suitable data representation for the task at hand and present it for better correlation. This article tries to sensitize the practising radiation oncologists to understand where the potential role of AI lies and what further can be achieved with it.              Methods and materials:                    Contemporary literature was searched and the available literature was sorted and an attempt at writing a comprehensive non-systematic review was made.              Results:                    The article addresses various areas in oncology, especially in the field of radiation oncology, where the work based on AI has been done. Whether it's the screening modalities, or diagnosis or the prognostic assays, AI has come with more accurately defining results and survival of patients. Various steps and protocols in radiation oncology are now using AI-based methods, like in the steps of planning, segmentation and delivery of radiation. Benefit of AI across all the platforms of health sector may lead to a more refined and personalized medicine in near future.              Conclusion:                    AI with the use of machine learning and artificial neural networks has come up with faster and more accurate solutions for the problems faced by oncologist. The uses of AI,are likely to get increased exponentially . However, concerns regarding demographic discrepancies in relation to patients, disease and their natural history and reports of manipulation of AI, the ultimate responsibility will rest on the treating physicians."
31076049,3.0,Intensive Care Unit Telemedicine: Innovations and Limitations,2019 Jul;35(3):497-509.,"Intensive care unit (ICU) telemedicine is an established entity that has the ability to not only improve the effectiveness, efficiency, and safety of critical care, but to also serve as a tool to combat staffing shortages and resource-limited environments. Several areas for future innovation exist within the field, including the use of advanced practice providers, robust inclusion in medical education, and concurrent application of advanced machine learning. The globalization of critical care services will also likely be predominantly delivered by ICU telemedicine. Limitations faced by the field include technical issues, financial concerns, and organizational elements."
31076048,3.0,"Intensive Care Unit Telemedicine in the Era of Big Data, Artificial Intelligence, and Computer Clinical Decision Support Systems",2019 Jul;35(3):483-495.,"This article examines the history of the telemedicine intensive care unit (tele-ICU), the current state of clinical decision support systems (CDSS) in the tele-ICU, applications of machine learning (ML) algorithms to critical care, and opportunities to integrate ML with tele-ICU CDSS. The enormous quantities of data generated by tele-ICU systems is a major driver in the development of the large, comprehensive, heterogeneous, and granular data sets necessary to develop generalizable ML CDSS algorithms, and deidentification of these data sets expands opportunities for ML CDSS research."
31074639,,"Social Profiling: A Review, Taxonomy, and Challenges",2019 Jul;22(7):433-450.,"Social media has taken an important place in the routine life of people. Every single second, users from all over the world are sharing interests, emotions, and other useful information that leads to the generation of huge volumes of user-generated data. Profiling users by extracting attribute information from social media data has been gaining importance with the increasing user-generated content over social media platforms. Meeting the user's satisfaction level for information collection is becoming more challenging and difficult. This is because of too much noise generated, which affects the process of information collection due to explosively increasing online data. Social profiling is an emerging approach to overcome the challenges faced in meeting user's demands by introducing the concept of personalized search while keeping in consideration user profiles generated using social network data. This study reviews and classifies research inferring users social profile attributes from social media data as individual and group profiling. The existing techniques along with utilized data sources, the limitations, and challenges are highlighted. The prominent approaches adopted include Machine Learning, Ontology, and Fuzzy logic. Social media data from Twitter and Facebook have been used by most of the studies to infer the social attributes of users. The studies show that user social attributes, including age, gender, home location, wellness, emotion, opinion, relation, influence, and so on, still need to be explored. This review gives researchers insights of the current state of literature and challenges for inferring user profile attributes using social media data."
31074158,,Orthodontics in the era of big data analytics,2019 May;22 Suppl 1:8-13.,"The objective of this report was to provide an overview of the current landscape of big data analytics in the healthcare sector, introduce various approaches of machine learning and discuss potential implications in the field of orthodontics. With the increasing availability of data from various sources, the traditional analytical methods may not be conducive anymore for examining clinical outcomes. Machine-learning approaches, which are algorithms trained to identify patterns in large data sets, are ideally suited to facilitate data-driven decision making. The field of orthodontics is particularly ripe for embracing the big data analytics platform to improve decision making in clinical practice. The availability of omics data, state-of-the-art imaging and potential for establishing large clinical data repositories have favourably positioned the specialty of orthodontics to deliver personalized and precision orthodontic care. Specifically, we discuss about next-generation sequencing, radiomics in the context of CBCT imaging, and how centralized data repositories can enable real-time data pooling from multiple sources."
31071473,5.0,Precision diagnostics based on machine learning-derived imaging signatures,2019 Dec;64:49-61.,"The complexity of modern multi-parametric MRI has increasingly challenged conventional interpretations of such images. Machine learning has emerged as a powerful approach to integrating diverse and complex imaging data into signatures of diagnostic and predictive value. It has also allowed us to progress from group comparisons to imaging biomarkers that offer value on an individual basis. We review several directions of research around this topic, emphasizing the use of machine learning in personalized predictions of clinical outcome, in breaking down broad umbrella diagnostic categories into more detailed and precise subtypes, and in non-invasively estimating cancer molecular characteristics. These methods and studies contribute to the field of precision medicine, by introducing more specific diagnostic and predictive biomarkers of clinical outcome, therefore pointing to better matching of treatments to patients."
31069725,10.0,"Complex Mixtures, Complex Analyses: an Emphasis on Interpretable Results",2019 Jun;6(2):53-61.,"Purpose of review:                    The purpose of this review is to outline the main questions in environmental mixtures research and provide a non-technical explanation of novel or advanced methods to answer these questions.              Recent findings:                    Machine learning techniques are now being incorporated into environmental mixture research to overcome issues with traditional methods. Though some methods perform well on specific tasks, no method consistently outperforms all others in complex mixture analyses, largely because different methods were developed to answer different research questions. We discuss four main questions in environmental mixtures research: (1) Are there specific exposure patterns in the study population? (2) Which are the toxic agents in the mixture? (3) Are mixture members acting synergistically? And, (4) what is the overall effect of the mixture? We emphasize the importance of robust methods and interpretable results over predictive accuracy. We encourage collaboration with computer scientists, data scientists, and biostatisticians in future mixture method development."
31067608,6.0,Recent methodology progress of deep learning for RNA-protein interaction prediction,2019 Nov;10(6):e1544.,"Interactions between RNAs and proteins play essential roles in many important biological processes. Benefitting from the advances of next generation sequencing technologies, hundreds of RNA-binding proteins (RBP) and their associated RNAs have been revealed, which enables the large-scale prediction of RNA-protein interactions using machine learning methods. Till now, a wide range of computational tools and pipelines have been developed, including deep learning models, which have achieved remarkable performance on the identification of RNA-protein binding affinities and sites. In this review, we provide an overview of the successful implementation of various deep learning approaches for predicting RNA-protein interactions, mainly focusing on the prediction of RNA-protein interaction pairs and RBP-binding sites on RNAs. Furthermore, we discuss the advantages and disadvantages of these approaches, and highlight future perspectives on how to design better deep learning models. Finally, we suggest some promising future directions of computational tasks in the study of RNA-protein interactions, especially the interactions between noncoding RNAs and proteins. This article is categorized under: RNA Interactions with Proteins and Other Molecules > Protein-RNA Interactions: Functional Implications RNA Evolution and Genomics > Computational Analyses of RNA RNA Interactions with Proteins and Other Molecules > Protein-RNA Recognition."
31066697,30.0,Natural Language Processing of Clinical Notes on Chronic Diseases: Systematic Review,2019 Apr 27;7(2):e12239.,"Background:                    Novel approaches that complement and go beyond evidence-based medicine are required in the domain of chronic diseases, given the growing incidence of such conditions on the worldwide population. A promising avenue is the secondary use of electronic health records (EHRs), where patient data are analyzed to conduct clinical and translational research. Methods based on machine learning to process EHRs are resulting in improved understanding of patient clinical trajectories and chronic disease risk prediction, creating a unique opportunity to derive previously unknown clinical insights. However, a wealth of clinical histories remains locked behind clinical narratives in free-form text. Consequently, unlocking the full potential of EHR data is contingent on the development of natural language processing (NLP) methods to automatically transform clinical text into structured clinical data that can guide clinical decisions and potentially delay or prevent disease onset.              Objective:                    The goal of the research was to provide a comprehensive overview of the development and uptake of NLP methods applied to free-text clinical notes related to chronic diseases, including the investigation of challenges faced by NLP methodologies in understanding clinical narratives.              Methods:                    Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed and searches were conducted in 5 databases using ""clinical notes,"" ""natural language processing,"" and ""chronic disease"" and their variations as keywords to maximize coverage of the articles.              Results:                    Of the 2652 articles considered, 106 met the inclusion criteria. Review of the included papers resulted in identification of 43 chronic diseases, which were then further classified into 10 disease categories using the International Classification of Diseases, 10th Revision. The majority of studies focused on diseases of the circulatory system (n=38) while endocrine and metabolic diseases were fewest (n=14). This was due to the structure of clinical records related to metabolic diseases, which typically contain much more structured data, compared with medical records for diseases of the circulatory system, which focus more on unstructured data and consequently have seen a stronger focus of NLP. The review has shown that there is a significant increase in the use of machine learning methods compared to rule-based approaches; however, deep learning methods remain emergent (n=3). Consequently, the majority of works focus on classification of disease phenotype with only a handful of papers addressing extraction of comorbidities from the free text or integration of clinical notes with structured data. There is a notable use of relatively simple methods, such as shallow classifiers (or combination with rule-based methods), due to the interpretability of predictions, which still represents a significant issue for more complex methods. Finally, scarcity of publicly available data may also have contributed to insufficient development of more advanced methods, such as extraction of word embeddings from clinical notes.              Conclusions:                    Efforts are still required to improve (1) progression of clinical NLP methods from extraction toward understanding; (2) recognition of relations among entities rather than entities in isolation; (3) temporal extraction to understand past, current, and future clinical events; (4) exploitation of alternative sources of clinical knowledge; and (5) availability of large-scale, de-identified clinical corpora."
31065266,2.0,Gastroenterology Meets Machine Learning: Status Quo and Quo Vadis,2019 Apr 2;2019:1870975.,"Machine learning has undergone a transition phase from being a pure statistical tool to being one of the main drivers of modern medicine. In gastroenterology, this technology is motivating a growing number of studies that rely on these innovative methods to deal with critical issues related to this practice. Hence, in the light of the burgeoning research on the use of machine learning in gastroenterology, a systematic review of the literature is timely. In this work, we present the results gleaned through a systematic review of prominent gastroenterology literature using machine learning techniques. Based on the analysis of 88 journal articles, we delimit the scope of application, we discuss current limitations including bias, lack of transparency, accountability, and data availability, and we put forward future avenues."
31063735,8.0,Assessing exposure to outdoor air pollution for epidemiological studies: Model-based and personal sampling strategies,2019 Jun;143(6):2002-2006.,"Epidemiologic studies have found air pollution to be causally linked to respiratory health including the exacerbation and development of childhood asthma. Accurately characterizing exposure is paramount in these studies to ensure valid estimates of health effects. Here, we provide a brief overview of the evolution of air pollution exposure assessment ranging from the use of ground-based, single-site air monitoring stations for population-level estimates to recent advances in spatiotemporal models, which use advanced machine learning algorithms and satellite-based data to accurately estimate individual-level daily exposures at high spatial resolutions. In addition, we review recent advances in sensor technology that enable the use of personal monitoring in epidemiologic studies, long-considered the ""holy grail"" of air pollution exposure assessment. Finally, we highlight key advantages and uses of each approach including the generalizability and public health relevance of air pollution models and the accuracy of personal monitors that are useful to guide personalized prevention strategies. Investigators and clinicians interested in the effects of air pollution on allergic disease and asthma should carefully consider the pros and cons of each approach to guide their application in research and practice."
31062835,2.0,Advancing Alzheimer's Disease Treatment: Lessons from CTAD 2018,2019;6(3):198-203.,"The 2018 Clinical Trials on Alzheimer's Disease (CTAD) conference showcased recent successes and failures in trials of Alzheimer's disease treatments. More importantly, the conference provided opportunities for investigators to share what they have learned from those studies with the goal of designing future trials with a greater likelihood of success. Data from studies of novel and non-amyloid treatment approaches were also shared, including neuroprotective and regenerative strategies and those that target neuroinflammation and synaptic function. New tools to improve the efficiency and productivity of clinical trials were described, including biomarkers and machine learning algorithms for predictive modeling."
31058383,8.0,A roadmap to integrate astrocytes into Systems Neuroscience,2020 Jan;68(1):5-26.,"Systems neuroscience is still mainly a neuronal field, despite the plethora of evidence supporting the fact that astrocytes modulate local neural circuits, networks, and complex behaviors. In this article, we sought to identify which types of studies are necessary to establish whether astrocytes, beyond their well-documented homeostatic and metabolic functions, perform computations implementing mathematical algorithms that sub-serve coding and higher-brain functions. First, we reviewed Systems-like studies that include astrocytes in order to identify computational operations that these cells may perform, using Ca2+ transients as their encoding language. The analysis suggests that astrocytes may carry out canonical computations in a time scale of subseconds to seconds in sensory processing, neuromodulation, brain state, memory formation, fear, and complex homeostatic reflexes. Next, we propose a list of actions to gain insight into the outstanding question of which variables are encoded by such computations. The application of statistical analyses based on machine learning, such as dimensionality reduction and decoding in the context of complex behaviors, combined with connectomics of astrocyte-neuronal circuits, is, in our view, fundamental undertakings. We also discuss technical and analytical approaches to study neuronal and astrocytic populations simultaneously, and the inclusion of astrocytes in advanced modeling of neural circuits, as well as in theories currently under exploration such as predictive coding and energy-efficient coding. Clarifying the relationship between astrocytic Ca2+ and brain coding may represent a leap forward toward novel approaches in the study of astrocytes in health and disease."
31057526,22.0,Application of Machine Learning in Microbiology,2019 Apr 18;10:827.,"Microorganisms are ubiquitous and closely related to people's daily lives. Since they were first discovered in the 19th century, researchers have shown great interest in microorganisms. People studied microorganisms through cultivation, but this method is expensive and time consuming. However, the cultivation method cannot keep a pace with the development of high-throughput sequencing technology. To deal with this problem, machine learning (ML) methods have been widely applied to the field of microbiology. Literature reviews have shown that ML can be used in many aspects of microbiology research, especially classification problems, and for exploring the interaction between microorganisms and the surrounding environment. In this study, we summarize the application of ML in microbiology."
31055238,3.0,"Big data in nanoscale connectomics, and the greed for training labels",2019 Apr;55:180-187.,"The neurosciences have developed methods that outpace most other biomedical fields in terms of acquired bytes. We review how the information content and analysis challenge of such data indicates that electron microscopy (EM)-based connectomics is an especially hard problem. Here, as in many other current machine learning applications, the need for excessive amounts of labelled data while utilizing only a small fraction of available raw image data for algorithm training illustrates the still fundamental gap between artificial and biological intelligence. Substantial improvements of label and energy efficiency in machine learning may be required to address the formidable challenge of acquiring the nanoscale connectome of a human brain."
31054502,10.0,"Radiological images and machine learning: Trends, perspectives, and prospects",2019 May;108:354-370.,"The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently."
