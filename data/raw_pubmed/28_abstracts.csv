pmid,citations,title,date,text
27603023,302.0,Using MetaboAnalyst 3.0 for Comprehensive Metabolomics Data Analysis,2016 Sep 7;55:14.10.1-14.10.91.,"MetaboAnalyst (http://www.metaboanalyst.ca) is a comprehensive Web application for metabolomic data analysis and interpretation. MetaboAnalyst handles most of the common metabolomic data types from most kinds of metabolomics platforms (MS and NMR) for most kinds of metabolomics experiments (targeted, untargeted, quantitative). In addition to providing a variety of data processing and normalization procedures, MetaboAnalyst also supports a number of data analysis and data visualization tasks using a range of univariate, multivariate methods such as PCA (principal component analysis), PLS-DA (partial least squares discriminant analysis), heatmap clustering and machine learning methods. MetaboAnalyst also offers a variety of tools for metabolomic data interpretation including MSEA (metabolite set enrichment analysis), MetPA (metabolite pathway analysis), and biomarker selection via ROC (receiver operating characteristic) curve analysis, as well as time series and power analysis. This unit provides an overview of the main functional modules and the general workflow of the latest version of MetaboAnalyst (MetaboAnalyst 3.0), followed by eight detailed protocols. © 2016 by John Wiley & Sons, Inc."
27599991,43.0,The Next Era: Deep Learning in Pharmaceutical Research,2016 Nov;33(11):2594-603.,"Over the past decade we have witnessed the increasing sophistication of machine learning algorithms applied in daily use from internet searches, voice recognition, social network software to machine vision software in cameras, phones, robots and self-driving cars. Pharmaceutical research has also seen its fair share of machine learning developments. For example, applying such methods to mine the growing datasets that are created in drug discovery not only enables us to learn from the past but to predict a molecule's properties and behavior in future. The latest machine learning algorithm garnering significant attention is deep learning, which is an artificial neural network with multiple hidden layers. Publications over the last 3 years suggest that this algorithm may have advantages over previous machine learning methods and offer a slight but discernable edge in predictive performance. The time has come for a balanced review of this technique but also to apply machine learning methods such as deep learning across a wider array of endpoints relevant to pharmaceutical research for which the datasets are growing such as physicochemical property prediction, formulation prediction, absorption, distribution, metabolism, excretion and toxicity (ADME/Tox), target prediction and skin permeation, etc. We also show that there are many potential applications of deep learning beyond cheminformatics. It will be important to perform prospective testing (which has been carried out rarely to date) in order to convince skeptics that there will be benefits from investing in this technique."
27592382,26.0,Feature selection methods for big data bioinformatics: A survey from the search perspective,2016 Dec 1;111:21-31.,"This paper surveys main principles of feature selection and their recent applications in big data bioinformatics. Instead of the commonly used categorization into filter, wrapper, and embedded approaches to feature selection, we formulate feature selection as a combinatorial optimization or search problem and categorize feature selection methods into exhaustive search, heuristic search, and hybrid methods, where heuristic search methods may further be categorized into those with or without data-distilled feature ranking measures."
27586752,,Learnability of prosodic boundaries: Is infant-directed speech easier?,2016 Aug;140(2):1239.,"This study explores the long-standing hypothesis that the acoustic cues to prosodic boundaries in infant-directed speech (IDS) make those boundaries easier to learn than those in adult-directed speech (ADS). Three cues (pause duration, nucleus duration, and pitch change) were investigated, by means of a systematic review of the literature, statistical analyses of a corpus of Japanese, and machine learning experiments. The review of previous work revealed that the effect of register on boundary cues is less well established than previously thought, and that results often vary across studies for certain cues. Statistical analyses run on a large database of mother-child and mother-interviewer interactions showed that the duration of a pause and the duration of the syllable nucleus preceding the boundary are two cues which are enhanced in IDS, while f0 change is actually degraded in IDS. Supervised and unsupervised machine learning techniques applied to these acoustic cues revealed that IDS boundaries were consistently better classified than ADS ones, regardless of the learning method used. The role of the cues examined in this study and the importance of these findings in the more general context of early linguistic structure acquisition is discussed."
29560871,7.0,The Clinical Added Value of Imaging: A Perspective From Outcome Prediction,2016 Sep;1(5):423-432.,"Objective measures of psychiatric health would be of benefit in clinical practice. Despite considerable research in the area of psychiatric neuroimaging outcome prediction, translating putative neuroimaging markers (neuromarkers) of a disorder into clinical practice has proven challenging. We reviewed studies that used neuroimaging measures to predict treatment response and disease outcomes in major depressive disorder, substance use, autism spectrum disorder, psychosis, and dementia. The majority of studies sought to predict psychiatric outcomes rather than develop a specific biological index of future disease trajectory. Studies varied widely with respect to sample size and quantification of out-of-sample prediction model performance. Many studies were able to predict psychiatric outcomes with moderate accuracy, with neuroimaging data often augmenting the prediction compared to clinical or psychometric data alone. We make recommendations for future research with respect to methods that can increase the generalizability and reproducibility of predictions. Large sample sizes in conjunction with machine learning methods, such as feature selection, cross-validation, and random label permutation, provide significant improvement to and quantification of generalizability. Further refinement of neuroimaging protocols and analysis methods will likely facilitate the clinical applicability of predictive imaging markers in psychiatry. Such clinically relevant neuromarkers need not necessarily be grounded in the pathophysiology of the disease, but identifying these neuromarkers may suggest targets for future research into disease mechanisms. The ability of imaging prediction models to augment clinical judgments will ultimately depend on the personal and economic costs and benefits to the patient."
29560870,13.0,Using Electroencephalography for Treatment Guidance in Major Depressive Disorder,2016 Sep;1(5):411-422.,"Given the high prevalence of treatment-resistant depression and the long delays in finding effective treatments via trial and error, valid biomarkers of treatment outcome with the ability to guide treatment selection represent one of the most important unmet needs in mood disorders. A large body of research has investigated, for this purpose, biomarkers derived from electroencephalography (EEG), using resting state EEG or evoked potentials. Most studies have focused on specific EEG features (or combinations thereof), whereas more recently machine-learning approaches have been used to define the EEG features with the best predictive abilities without a priori hypotheses. While reviewing these different approaches, we have focused on the predictor characteristics and the quality of the supporting evidence."
27568202,15.0,Connectivity Changes in Parkinson's Disease,2016 Oct;16(10):91.,"Parkinson's disease (PD) is a chronic and progressive movement disorder of the central nervous system characterized by widespread alterations in several non-motor aspects such as mood, sleep, olfactory, and cognition in addition to motor dysfunctions. Advanced neuroimaging using functional connectivity reconstruction of the human brain has provided a vast knowledge on the pathophysiological mechanisms underlying this disorder, but this, however, does not cover the overall inter-/intra-individual variability of PD phenotypes. The present review is aimed at discussing to what extent the evidence provided by group-based neuroimaging analysis in this field of study (using seed-based, network-based, or graph theory approaches) may be generalized. In particular, we summarized the literature on the application of resting-state functional connectivity studies to explore different neural correlates of motor and non-motor symptoms of PD and the neural mechanisms involved in treatment effects: effects of levodopa or deep brain stimulation. The lesson learnt from one decade of studies provides consistent evidence on the role of the altered communication between the striato-frontal pathways as a marker of PD-related motor degeneration, whereas in the non-motor domain, several missing pieces of a complex puzzle are provided. However, the main target is to present a new era of intelligent neuroimaging applications, where automated multivariate analysis of functional connectivity data may be used for moving from group-level statistical results to personalized predictions in a clinical setting. Although in its relative infancy, the evidence gathered so far suggests a new era of clinical neuroimaging is starting."
27559342,1.0,Data Mining and Pattern Recognition Models for Identifying Inherited Diseases: Challenges and Implications,2016 Aug 10;7:136.,"Data mining and pattern recognition methods reveal interesting findings in genetic studies, especially on how the genetic makeup is associated with inherited diseases. Although researchers have proposed various data mining models for biomedical approaches, there remains a challenge in accurately prioritizing the single nucleotide polymorphisms (SNP) associated with the disease. In this commentary, we review the state-of-art data mining and pattern recognition models for identifying inherited diseases and deliberate the need of binary classification- and scoring-based prioritization methods in determining causal variants. While we discuss the pros and cons associated with these methods known, we argue that the gene prioritization methods and the protein interaction (PPI) methods in conjunction with the K nearest neighbors' could be used in accurately categorizing the genetic factors in disease causation."
27555308,160.0,Looking for a Signal in the Noise: Revisiting Obesity and the Microbiome,2016 Aug 23;7(4):e01018-16.,"Two recent studies have reanalyzed previously published data and found that when data sets were analyzed independently, there was limited support for the widely accepted hypothesis that changes in the microbiome are associated with obesity. This hypothesis was reconsidered by increasing the number of data sets and pooling the results across the individual data sets. The preferred reporting items for systematic reviews and meta-analyses guidelines were used to identify 10 studies for an updated and more synthetic analysis. Alpha diversity metrics and the relative risk of obesity based on those metrics were used to identify a limited number of significant associations with obesity; however, when the results of the studies were pooled by using a random-effect model, significant associations were observed among Shannon diversity, the number of observed operational taxonomic units, Shannon evenness, and obesity status. They were not observed for the ratio of Bacteroidetes and Firmicutes or their individual relative abundances. Although these tests yielded small P values, the difference between the Shannon diversity indices of nonobese and obese individuals was 2.07%. A power analysis demonstrated that only one of the studies had sufficient power to detect a 5% difference in diversity. When random forest machine learning models were trained on one data set and then tested by using the other nine data sets, the median accuracy varied between 33.01 and 64.77% (median, 56.68%). Although there was support for a relationship between the microbial communities found in human feces and obesity status, this association was relatively weak and its detection is confounded by large interpersonal variation and insufficient sample sizes.              Importance:                    As interest in the human microbiome grows, there is an increasing number of studies that can be used to test numerous hypotheses across human populations. The hypothesis that variation in the gut microbiota can explain or be used to predict obesity status has received considerable attention and is frequently mentioned as an example of the role of the microbiome in human health. Here we assessed this hypothesis by using 10 independent studies and found that although there is an association, it is smaller than can be detected by most microbiome studies. Furthermore, we directly tested the ability to predict obesity status on the basis of the composition of an individual's microbiome and found that the median classification accuracy is between 33.01 and 64.77%. This type of analysis can be used to design future studies and expanded to explore other hypotheses."
27537363,2.0,Biomechanical Studies on Patterns of Cranial Bone Fracture Using the Immature Porcine Model,2017 Feb 1;139(2).,"This review was prepared for the American Society of Mechanical Engineers Lissner Medal. It specifically discusses research performed in the Orthopaedic Biomechanics Laboratories on pediatric cranial bone mechanics and patterns of fracture in collaboration with the Forensic Anthropology Laboratory at Michigan State University. Cranial fractures are often an important element seen by forensic anthropologists during the investigation of pediatric trauma cases litigated in courts. While forensic anthropologists and forensic biomechanists are often called on to testify in these cases, there is little basic science developed in support of their testimony. The following is a review of studies conducted in the above laboratories and supported by the National Institute of Justice to begin an understanding of the mechanics and patterns of pediatric cranial bone fracture. With the lack of human pediatric specimens, the studies utilize an immature porcine model. Because much case evidence involves cranial bone fracture, the studies described below focus on determining input loading based on the resultant bone fracture pattern. The studies involve impact to the parietal bone, the most often fractured cranial bone, and begin with experiments on entrapped heads, progressing to those involving free-falling heads. The studies involve head drops onto different types and shapes of interfaces with variations of impact energy. The studies show linear fractures initiating from sutural boundaries, away from the impact site, for flat surface impacts, in contrast to depressed fractures for more focal impacts. The results have been incorporated into a ""Fracture Printing Interface (FPI),"" using machine learning and pattern recognition algorithms. The interface has been used to help interpret mechanisms of injury in pediatric death cases collected from medical examiner offices. The ultimate aim of this program of study is to develop a ""Human Fracture Printing Interface"" that can be used by forensic investigators in determining mechanisms of pediatric cranial bone fracture."
27529225,11.0,Deep Artificial Neural Networks and Neuromorphic Chips for Big Data Analysis: Pharmaceutical and Bioinformatics Applications,2016 Aug 11;17(8):1313.,"Over the past decade, Deep Artificial Neural Networks (DNNs) have become the state-of-the-art algorithms in Machine Learning (ML), speech recognition, computer vision, natural language processing and many other tasks. This was made possible by the advancement in Big Data, Deep Learning (DL) and drastically increased chip processing abilities, especially general-purpose graphical processing units (GPGPUs). All this has created a growing interest in making the most of the potential offered by DNNs in almost every field. An overview of the main architectures of DNNs, and their usefulness in Pharmacology and Bioinformatics are presented in this work. The featured applications are: drug design, virtual screening (VS), Quantitative Structure-Activity Relationship (QSAR) research, protein structure prediction and genomics (and other omics) data mining. The future need of neuromorphic hardware for DNNs is also discussed, and the two most advanced chips are reviewed: IBM TrueNorth and SpiNNaker. In addition, this review points out the importance of considering not only neurons, as DNNs and neuromorphic chips should also include glial cells, given the proven importance of astrocytes, a type of glial cell which contributes to information processing in the brain. The Deep Artificial Neuron-Astrocyte Networks (DANAN) could overcome the difficulties in architecture design, learning process and scalability of the current ML methods."
27528421,6.0,Computer-aided diagnosis of breast cancer using cytological images: A systematic review,2016 Oct;48(5):461-74.,"Cytological evaluation by microscopic image-based characterization [imprint cytology (IC) and fine needle aspiration cytology (FNAC)] plays an integral role in primary screening/detection of breast cancer. The sensitivity of IC and FNAC as a screening tool is dependent on the image quality and the pathologist's level of expertise. Computer-aided diagnosis (CAD) is used to assists the pathologists by developing various machine learning and image processing algorithms. This study reviews the various manual and computer-aided techniques used so far in breast cytology. Diagnostic applications were studied to estimate the role of CAD in breast cancer diagnosis. This paper presents an overview of image processing and pattern recognition techniques that have been used to address several issues in breast cytology-based CAD including slide preparation, staining, microscopic imaging, pre-processing, segmentation, feature extraction and diagnostic classification. This review provides better insights to readers regarding the state of the art the knowledge on CAD-based breast cancer diagnosis to date."
27525223,3.0,Computerized techniques pave the way for drug-drug interaction prediction and interpretation,2016;6(2):71-8.,"Introduction:                    Health care industry also patients penalized by medical errors that are inevitable but highly preventable. Vast majority of medical errors are related to adverse drug reactions, while drug-drug interactions (DDIs) are the main cause of adverse drug reactions (ADRs). DDIs and ADRs have mainly been reported by haphazard case studies. Experimental in vivo and in vitro researches also reveals DDI pairs. Laboratory and experimental researches are valuable but also expensive and in some cases researchers may suffer from limitations.              Methods:                    In the current investigation, the latest published works were studied to analyze the trend and pattern of the DDI modelling and the impacts of machine learning methods. Applications of computerized techniques were also investigated for the prediction and interpretation of DDIs.              Results:                    Computerized data-mining in pharmaceutical sciences and related databases provide new key transformative paradigms that can revolutionize the treatment of diseases and hence medical care. Given that various aspects of drug discovery and pharmacotherapy are closely related to the clinical and molecular/biological information, the scientifically sound databases (e.g., DDIs, ADRs) can be of importance for the success of pharmacotherapy modalities.              Conclusion:                    A better understanding of DDIs not only provides a robust means for designing more effective medicines but also grantees patient safety."
27525157,1.0,New Evaluation Vector through the Stanford Mobile Inquiry-Based Learning Environment (SMILE) for Participatory Action Research,2016 Jul;22(3):164-71.,"Objectives:                    This article reviews an evaluation vector model driven from a participatory action research leveraging a collective inquiry system named SMILE (Stanford Mobile Inquiry-based Learning Environment).              Methods:                    SMILE has been implemented in a diverse set of collective inquiry generation and analysis scenarios including community health care-specific professional development sessions and community-based participatory action research projects. In each scenario, participants are given opportunities to construct inquiries around physical and emotional health-related phenomena in their own community.              Results:                    Participants formulated inquiries as well as potential clinical treatments and hypothetical scenarios to address health concerns or clarify misunderstandings or misdiagnoses often found in their community practices. From medical universities to rural village health promotion organizations, all participatory inquiries and potential solutions can be collected and analyzed. The inquiry and solution sets represent an evaluation vector which helps educators better understand community health issues at a much deeper level.              Conclusions:                    SMILE helps collect problems that are most important and central to their community health concerns. The evaluation vector, consisting participatory and collective inquiries and potential solutions, helps the researchers assess the participants' level of understanding on issues around health concerns and practices while helping the community adequately formulate follow-up action plans. The method used in SMILE requires much further enhancement with machine learning and advanced data visualization."
27518905,6.0,Imaging and machine learning techniques for diagnosis of Alzheimer's disease,2016 Dec 1;27(8):857-870.,"Alzheimer's disease (AD) is a common health problem in elderly people. There has been considerable research toward the diagnosis and early detection of this disease in the past decade. The sensitivity of biomarkers and the accuracy of the detection techniques have been defined to be the key to an accurate diagnosis. This paper presents a state-of-the-art review of the research performed on the diagnosis of AD based on imaging and machine learning techniques. Different segmentation and machine learning techniques used for the diagnosis of AD are reviewed including thresholding, supervised and unsupervised learning, probabilistic techniques, Atlas-based approaches, and fusion of different image modalities. More recent and powerful classification techniques such as the enhanced probabilistic neural network of Ahmadlou and Adeli should be investigated with the goal of improving the diagnosis accuracy. A combination of different image modalities can help improve the diagnosis accuracy rate. Research is needed on the combination of modalities to discover multi-modal biomarkers."
27502048,28.0,Ten problems and solutions when predicting individual outcome from lesion site after stroke,2017 Jan 15;145(Pt B):200-208.,"In this paper, we consider solutions to ten of the challenges faced when trying to predict an individual's functional outcome after stroke on the basis of lesion site. A primary goal is to find lesion-outcome associations that are consistently observed in large populations of stroke patients because consistent associations maximise confidence in future individualised predictions. To understand and control multiple sources of inter-patient variability, we need to systematically investigate each contributing factor and how each factor depends on other factors. This requires very large cohorts of patients, who differ from one another in typical and measurable ways, including lesion site, lesion size, functional outcome and time post stroke (weeks to decades). These multivariate investigations are complex, particularly when the contributions of different variables interact with one another. Machine learning algorithms can help to identify the most influential variables and indicate dependencies between different factors. Multivariate lesion analyses are needed to understand how the effect of damage to one brain region depends on damage or preservation in other brain regions. Such data-led investigations can reveal predictive relationships between lesion site and outcome. However, to understand and improve the predictions we need explanatory models of the neural networks and degenerate pathways that support functions of interest. This will entail integrating the results of lesion analyses with those from functional imaging (fMRI, MEG), transcranial magnetic stimulation (TMS) and diffusor tensor imaging (DTI) studies of healthy participants and patients."
27501063,14.0,"""Is voice a marker for Autism spectrum disorder? A systematic review and meta-analysis""",2017 Mar;10(3):384-407.,"Individuals with Autism Spectrum Disorder (ASD) tend to show distinctive, atypical acoustic patterns of speech. These behaviors affect social interactions and social development and could represent a non-invasive marker for ASD. We systematically reviewed the literature quantifying acoustic patterns in ASD. Search terms were: (prosody OR intonation OR inflection OR intensity OR pitch OR fundamental frequency OR speech rate OR voice quality OR acoustic) AND (autis* OR Asperger). Results were filtered to include only: empirical studies quantifying acoustic features of vocal production in ASD, with a sample size >2, and the inclusion of a neurotypical comparison group and/or correlations between acoustic measures and severity of clinical features. We identified 34 articles, including 30 univariate studies and 15 multivariate machine-learning studies. We performed meta-analyses of the univariate studies, identifying significant differences in mean pitch and pitch range between individuals with ASD and comparison participants (Cohen's d of 0.4-0.5 and discriminatory accuracy of about 61-64%). The multivariate studies reported higher accuracies than the univariate studies (63-96%). However, the methods used and the acoustic features investigated were too diverse for performing meta-analysis. We conclude that multivariate studies of acoustic patterns are a promising but yet unsystematic avenue for establishing ASD markers. We outline three recommendations for future studies: open data, open methods, and theory-driven research. Autism Res 2017, 10: 384-407. © 2016 International Society for Autism Research, Wiley Periodicals, Inc."
27501026,41.0,"Machine learning for large-scale wearable sensor data in Parkinson's disease: Concepts, promises, pitfalls, and futures",2016 Sep;31(9):1314-26.,"For the treatment and monitoring of Parkinson's disease (PD) to be scientific, a key requirement is that measurement of disease stages and severity is quantitative, reliable, and repeatable. The last 50 years in PD research have been dominated by qualitative, subjective ratings obtained by human interpretation of the presentation of disease signs and symptoms at clinical visits. More recently, ""wearable,"" sensor-based, quantitative, objective, and easy-to-use systems for quantifying PD signs for large numbers of participants over extended durations have been developed. This technology has the potential to significantly improve both clinical diagnosis and management in PD and the conduct of clinical studies. However, the large-scale, high-dimensional character of the data captured by these wearable sensors requires sophisticated signal processing and machine-learning algorithms to transform it into scientifically and clinically meaningful information. Such algorithms that ""learn"" from data have shown remarkable success in making accurate predictions for complex problems in which human skill has been required to date, but they are challenging to evaluate and apply without a basic understanding of the underlying logic on which they are based. This article contains a nontechnical tutorial review of relevant machine-learning algorithms, also describing their limitations and how these can be overcome. It discusses implications of this technology and a practical road map for realizing the full potential of this technology in PD research and practice. © 2016 International Parkinson and Movement Disorder Society."
27491648,89.0,Deep Learning in Drug Discovery,2016 Jan;35(1):3-14.,"Artificial neural networks had their first heyday in molecular informatics and drug discovery approximately two decades ago. Currently, we are witnessing renewed interest in adapting advanced neural network architectures for pharmaceutical research by borrowing from the field of ""deep learning"". Compared with some of the other life sciences, their application in drug discovery is still limited. Here, we provide an overview of this emerging field of molecular informatics, present the basic concepts of prominent deep learning methods and offer motivation to explore these techniques for their usefulness in computer-assisted drug discovery and design. We specifically emphasize deep neural networks, restricted Boltzmann machine networks and convolutional networks."
27488403,1.0,Progress in Biomedical Knowledge Discovery: A 25-year Retrospective,2016 Aug 2;Suppl 1(Suppl 1):S117-29.,"Objectives:                    We sought to explore, via a systematic review of the literature, the state of the art of knowledge discovery in biomedical databases as it existed in 1992, and then now, 25 years later, mainly focused on supervised learning.              Methods:                    We performed a rigorous systematic search of PubMed and latent Dirichlet allocation to identify themes in the literature and trends in the science of knowledge discovery in and between time periods and compare these trends. We restricted the result set using a bracket of five years previous, such that the 1992 result set was restricted to articles published between 1987 and 1992, and the 2015 set between 2011 and 2015. This was to reflect the current literature available at the time to researchers and others at the target dates of 1992 and 2015. The search term was framed as: Knowledge Discovery OR Data Mining OR Pattern Discovery OR Pattern Recognition, Automated.              Results:                    A total 538 and 18,172 documents were retrieved for 1992 and 2015, respectively. The number and type of data sources increased dramatically over the observation period, primarily due to the advent of electronic clinical systems. The period 1992- 2015 saw the emergence of new areas of research in knowledge discovery, and the refinement and application of machine learning approaches that were nascent or unknown in 1992.              Conclusions:                    Over the 25 years of the observation period, we identified numerous developments that impacted the science of knowledge discovery, including the availability of new forms of data, new machine learning algorithms, and new application domains. Through a bibliometric analysis we examine the striking changes in the availability of highly heterogeneous data resources, the evolution of new algorithmic approaches to knowledge discovery, and we consider from legal, social, and political perspectives possible explanations of the growth of the field. Finally, we reflect on the achievements of the past 25 years to consider what the next 25 years will bring with regard to the availability of even more complex data and to the methods that could be, and are being now developed for the discovery of new knowledge in biomedical data."
27474269,249.0,Deep learning for computational biology,2016 Jul 29;12(7):878.,"Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology."
27473064,178.0,Deep learning in bioinformatics,2017 Sep 1;18(5):851-869.,"In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies."
27470504,8.0,A review on host-pathogen interactions: classification and prediction,2016 Oct;35(10):1581-99.,"The research on host-pathogen interactions is an ever-emerging and evolving field. Every other day a new pathogen gets discovered, along with comes the challenge of its prevention and cure. As the intelligent human always vies for prevention, which is better than cure, understanding the mechanisms of host-pathogen interactions gets prior importance. There are many mechanisms involved from the pathogen as well as the host sides while an interaction happens. It is a vis-a-vis fight of the counter genes and proteins from both sides. Who wins depends on whether a host gets an infection or not. Moreover, a higher level of complexity arises when the pathogens evolve and become resistant to a host's defense mechanisms. Such pathogens pose serious challenges for treatment. The entire human population is in danger of such long-lasting persistent infections. Some of these infections even increase the rate of mortality. Hence there is an immediate emergency to understand how the pathogens interact with their host for successful invasion. It may lead to discovery of appropriate preventive measures, and the development of rational therapeutic measures and medication against such infections and diseases. This review, a state-of-the-art updated scenario of host-pathogen interaction research, has been done by keeping in mind this urgency. It covers the biological and computational aspects of host-pathogen interactions, classification of the methods by which the pathogens interact with their hosts, different machine learning techniques for prediction of host-pathogen interactions, and future scopes of this research field."
27451435,5.0,Natural language processing in pathology: a scoping review,2016 Jul 22;jclinpath-2016-203872.,"Background:                    Encoded pathology data are key for medical registries and analyses, but pathology information is often expressed as free text.              Objective:                    We reviewed and assessed the use of NLP (natural language processing) for encoding pathology documents.              Materials and methods:                    Papers addressing NLP in pathology were retrieved from PubMed, Association for Computing Machinery (ACM) Digital Library and Association for Computational Linguistics (ACL) Anthology. We reviewed and summarised the study objectives; NLP methods used and their validation; software implementations; the performance on the dataset used and any reported use in practice.              Results:                    The main objectives of the 38 included papers were encoding and extraction of clinically relevant information from pathology reports. Common approaches were word/phrase matching, probabilistic machine learning and rule-based systems. Five papers (13%) compared different methods on the same dataset. Four papers did not specify the method(s) used. 18 of the 26 studies that reported F-measure, recall or precision reported values of over 0.9. Proprietary software was the most frequently mentioned category (14 studies); General Architecture for Text Engineering (GATE) was the most applied architecture overall. Practical system use was reported in four papers. Most papers used expert annotation validation.              Conclusions:                    Different methods are used in NLP research in pathology, and good performances, that is, high precision and recall, high retrieval/removal rates, are reported for all of these. Lack of validation and of shared datasets precludes performance comparison. More comparative analysis and validation are needed to provide better insight into the performance and merits of these methods."
27446888,2.0,Pre-Adult MRI of Brain Cancer and Neurological Injury: Multivariate Analyses,2016 Jun 23;4:65.,"Brain cancer and neurological injuries, such as stroke, are life-threatening conditions for which further research is needed to overcome the many challenges associated with providing optimal patient care. Multivariate analysis (MVA) is a class of pattern recognition technique involving the processing of data that contains multiple measurements per sample. MVA can be used to address a wide variety of neuroimaging challenges, including identifying variables associated with patient outcomes; understanding an injury's etiology, development, and progression; creating diagnostic tests; assisting in treatment monitoring; and more. Compared to adults, imaging of the developing brain has attracted less attention from MVA researchers, however, remarkable MVA growth has occurred in recent years. This paper presents the results of a systematic review of the literature focusing on MVA technologies applied to brain injury and cancer in neurological fetal, neonatal, and pediatric magnetic resonance imaging (MRI). With a wide variety of MRI modalities providing physiologically meaningful biomarkers and new biomarker measurements constantly under development, MVA techniques hold enormous potential toward combining available measurements toward improving basic research and the creation of technologies that contribute to improving patient care."
27440790,4.0,"Digital technology in respiratory diseases: Promises, (no) panacea and time for a new paradigm",2016 May;13(2):189-91.,"In a world where digital technology has revolutionized the way we work, shop and manage our finances it is unsurprising that digital systems are suggested as potential solutions to delivering clinically and cost-effective care for an aging population with one or more long-term conditions. However, recent evidence suggesting that telehealth may not be quite the panacea that was promised, has led to discussions on the mechanisms and role of digital technology in respiratory care. Implementation in rural and remote settings offers significant benefits in terms of convenient access to care, but is contingent on technical and organizational infrastructure. Telemonitoring systems rely on algorithms to detect deterioration and trigger alerts; machine learning may enable telemonitoring of the future to develop personalized systems that are sensitive to clinical status whilst reducing false alerts. By providing access to information, offering convenient and flexible modes of communication and enabling the transfer of monitoring data to support professional assessment, telehealth can support self-management. At present, all too often, expensive 'off the shelf' systems are purchased and given to clinicians to use. It is time for the paradigm to shift. As clinicians we should identify the specific challenges we face in delivering care, and expect flexible systems that can be customized to individual patients' requirements and adapted to our diverse healthcare contexts."
27436868,57.0,Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges,2017 Jun 14;38(23):1805-1814.,"Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning."
27435734,6.0,Walking through the statistical black boxes of plant breeding,2016 Oct;129(10):1933-49.,"The main statistical procedures in plant breeding are based on Gaussian process and can be computed through mixed linear models. Intelligent decision making relies on our ability to extract useful information from data to help us achieve our goals more efficiently. Many plant breeders and geneticists perform statistical analyses without understanding the underlying assumptions of the methods or their strengths and pitfalls. In other words, they treat these statistical methods (software and programs) like black boxes. Black boxes represent complex pieces of machinery with contents that are not fully understood by the user. The user sees the inputs and outputs without knowing how the outputs are generated. By providing a general background on statistical methodologies, this review aims (1) to introduce basic concepts of machine learning and its applications to plant breeding; (2) to link classical selection theory to current statistical approaches; (3) to show how to solve mixed models and extend their application to pedigree-based and genomic-based prediction; and (4) to clarify how the algorithms of genome-wide association studies work, including their assumptions and limitations."
27423409,119.0,Image analysis and machine learning in digital pathology: Challenges and opportunities,2016 Oct;33:170-175.,"With the rise in whole slide scanner technology, large numbers of tissue slides are being scanned and represented and archived digitally. While digital pathology has substantial implications for telepathology, second opinions, and education there are also huge research opportunities in image computing with this new source of ""big data"". It is well known that there is fundamental prognostic data embedded in pathology images. The ability to mine ""sub-visual"" image features from digital pathology slide images, features that may not be visually discernible by a pathologist, offers the opportunity for better quantitative modeling of disease appearance and hence possibly improved prediction of disease aggressiveness and patient outcome. However the compelling opportunities in precision medicine offered by big digital pathology data come with their own set of computational challenges. Image analysis and computer assisted detection and diagnosis tools previously developed in the context of radiographic images are woefully inadequate to deal with the data density in high resolution digitized whole slide images. Additionally there has been recent substantial interest in combining and fusing radiologic imaging and proteomics and genomics based measurements with features extracted from digital pathology images for better prognostic prediction of disease aggressiveness and patient outcome. Again there is a paucity of powerful tools for combining disease specific features that manifest across multiple different length scales. The purpose of this review is to discuss developments in computational image analysis tools for predictive modeling of digital pathology images from a detection, segmentation, feature extraction, and tissue classification perspective. We discuss the emergence of new handcrafted feature approaches for improved predictive modeling of tissue appearance and also review the emergence of deep learning schemes for both object detection and tissue classification. We also briefly review some of the state of the art in fusion of radiology and pathology images and also combining digital pathology derived image measurements with molecular ""omics"" features for better predictive modeling. The review ends with a brief discussion of some of the technical and computational challenges to be overcome and reflects on future opportunities for the quantitation of histopathology."
27423136,17.0,Protein function in precision medicine: deep understanding with machine learning,2016 Aug;590(15):2327-41.,"Precision medicine and personalized health efforts propose leveraging complex molecular, medical and family history, along with other types of personal data toward better life. We argue that this ambitious objective will require advanced and specialized machine learning solutions. Simply skimming some low-hanging results off the data wealth might have limited potential. Instead, we need to better understand all parts of the system to define medically relevant causes and effects: how do particular sequence variants affect particular proteins and pathways? How do these effects, in turn, cause the health or disease-related phenotype? Toward this end, deeper understanding will not simply diffuse from deeper machine learning, but from more explicit focus on understanding protein function, context-specific protein interaction networks, and impact of variation on both."
27406289,48.0,"Machine learning, statistical learning and the future of biological research in psychiatry",2016 Sep;46(12):2455-65.,"Psychiatric research has entered the age of 'Big Data'. Datasets now routinely involve thousands of heterogeneous variables, including clinical, neuroimaging, genomic, proteomic, transcriptomic and other 'omic' measures. The analysis of these datasets is challenging, especially when the number of measurements exceeds the number of individuals, and may be further complicated by missing data for some subjects and variables that are highly correlated. Statistical learning-based models are a natural extension of classical statistical approaches but provide more effective methods to analyse very large datasets. In addition, the predictive capability of such models promises to be useful in developing decision support systems. That is, methods that can be introduced to clinical settings and guide, for example, diagnosis classification or personalized treatment. In this review, we aim to outline the potential benefits of statistical learning methods in clinical research. We first introduce the concept of Big Data in different environments. We then describe how modern statistical learning models can be used in practice on Big Datasets to extract relevant information. Finally, we discuss the strengths of using statistical learning in psychiatric studies, from both research and practical clinical points of view."
27383691,3.0,Alien Mindscapes-A Perspective on the Search for Extraterrestrial Intelligence,2016 Sep;16(9):661-76.,"Advances in planetary and space sciences, astrobiology, and life and cognitive sciences, combined with developments in communication theory, bioneural computing, machine learning, and big data analysis, create new opportunities to explore the probabilistic nature of alien life. Brought together in a multidisciplinary approach, they have the potential to support an integrated and expanded Search for Extraterrestrial Intelligence (SETI (1) ), a search that includes looking for life as we do not know it. This approach will augment the odds of detecting a signal by broadening our understanding of the evolutionary and systemic components in the search for extraterrestrial intelligence (ETI), provide more targets for radio and optical SETI, and identify new ways of decoding and coding messages using universal markers.              Key words:                    SETI-Astrobiology-Coevolution of Earth and life-Planetary habitability and biosignatures. Astrobiology 16, 661-676."
27379211,13.0,Big Data Analytics for Prostate Radiotherapy,2016 Jun 14;6:149.,"Radiation therapy is a first-line treatment option for localized prostate cancer and radiation-induced normal tissue damage are often the main limiting factor for modern radiotherapy regimens. Conversely, under-dosing of target volumes in an attempt to spare adjacent healthy tissues limits the likelihood of achieving local, long-term control. Thus, the ability to generate personalized data-driven risk profiles for radiotherapy outcomes would provide valuable prognostic information to help guide both clinicians and patients alike. Big data applied to radiation oncology promises to deliver better understanding of outcomes by harvesting and integrating heterogeneous data types, including patient-specific clinical parameters, treatment-related dose-volume metrics, and biological risk factors. When taken together, such variables make up the basis for a multi-dimensional space (the ""RadoncSpace"") in which the presented modeling techniques search in order to identify significant predictors. Herein, we review outcome modeling and big data-mining techniques for both tumor control and radiotherapy-induced normal tissue effects. We apply many of the presented modeling approaches onto a cohort of hypofractionated prostate cancer patients taking into account different data types and a large heterogeneous mix of physical and biological parameters. Cross-validation techniques are also reviewed for the refinement of the proposed framework architecture and checking individual model performance. We conclude by considering advanced modeling techniques that borrow concepts from big data analytics, such as machine learning and artificial intelligence, before discussing the potential future impact of systems radiobiology approaches."
27362387,20.0,Machine Learning Techniques in Clinical Vision Sciences,2017 Jan;42(1):1-15.,"This review presents and discusses the contribution of machine learning techniques for diagnosis and disease monitoring in the context of clinical vision science. Many ocular diseases leading to blindness can be halted or delayed when detected and treated at its earliest stages. With the recent developments in diagnostic devices, imaging and genomics, new sources of data for early disease detection and patients' management are now available. Machine learning techniques emerged in the biomedical sciences as clinical decision-support techniques to improve sensitivity and specificity of disease detection and monitoring, increasing objectively the clinical decision-making process. This manuscript presents a review in multimodal ocular disease diagnosis and monitoring based on machine learning approaches. In the first section, the technical issues related to the different machine learning approaches will be present. Machine learning techniques are used to automatically recognize complex patterns in a given dataset. These techniques allows creating homogeneous groups (unsupervised learning), or creating a classifier predicting group membership of new cases (supervised learning), when a group label is available for each case. To ensure a good performance of the machine learning techniques in a given dataset, all possible sources of bias should be removed or minimized. For that, the representativeness of the input dataset for the true population should be confirmed, the noise should be removed, the missing data should be treated and the data dimensionally (i.e., the number of parameters/features and the number of cases in the dataset) should be adjusted. The application of machine learning techniques in ocular disease diagnosis and monitoring will be presented and discussed in the second section of this manuscript. To show the clinical benefits of machine learning in clinical vision sciences, several examples will be presented in glaucoma, age-related macular degeneration, and diabetic retinopathy, these ocular pathologies being the major causes of irreversible visual impairment."
27349830,10.0,Cardiac image modelling: Breadth and depth in heart disease,2016 Oct;33:38-43.,"With the advent of large-scale imaging studies and big health data, and the corresponding growth in analytics, machine learning and computational image analysis methods, there are now exciting opportunities for deepening our understanding of the mechanisms and characteristics of heart disease. Two emerging fields are computational analysis of cardiac remodelling (shape and motion changes due to disease) and computational analysis of physiology and mechanics to estimate biophysical properties from non-invasive imaging. Many large cohort studies now underway around the world have been specifically designed based on non-invasive imaging technologies in order to gain new information about the development of heart disease from asymptomatic to clinical manifestations. These give an unprecedented breadth to the quantification of population variation and disease development. Also, for the individual patient, it is now possible to determine biophysical properties of myocardial tissue in health and disease by interpreting detailed imaging data using computational modelling. For these population and patient-specific computational modelling methods to develop further, we need open benchmarks for algorithm comparison and validation, open sharing of data and algorithms, and demonstration of clinical efficacy in patient management and care. The combination of population and patient-specific modelling will give new insights into the mechanisms of cardiac disease, in particular the development of heart failure, congenital heart disease, myocardial infarction, contractile dysfunction and diastolic dysfunction."
27346545,38.0,Computational neuroimaging strategies for single patient predictions,2017 Jan 15;145(Pt B):180-199.,"Neuroimaging increasingly exploits machine learning techniques in an attempt to achieve clinically relevant single-subject predictions. An alternative to machine learning, which tries to establish predictive links between features of the observed data and clinical variables, is the deployment of computational models for inferring on the (patho)physiological and cognitive mechanisms that generate behavioural and neuroimaging responses. This paper discusses the rationale behind a computational approach to neuroimaging-based single-subject inference, focusing on its potential for characterising disease mechanisms in individual subjects and mapping these characterisations to clinical predictions. Following an overview of two main approaches - Bayesian model selection and generative embedding - which can link computational models to individual predictions, we review how these methods accommodate heterogeneity in psychiatric and neurological spectrum disorders, help avoid erroneous interpretations of neuroimaging data, and establish a link between a mechanistic, model-based approach and the statistical perspectives afforded by machine learning."
27345524,196.0,Long non-coding RNAs and complex diseases: from experimental results to computational models,2017 Jul 1;18(4):558-576.,"LncRNAs have attracted lots of attentions from researchers worldwide in recent decades. With the rapid advances in both experimental technology and computational prediction algorithm, thousands of lncRNA have been identified in eukaryotic organisms ranging from nematodes to humans in the past few years. More and more research evidences have indicated that lncRNAs are involved in almost the whole life cycle of cells through different mechanisms and play important roles in many critical biological processes. Therefore, it is not surprising that the mutations and dysregulations of lncRNAs would contribute to the development of various human complex diseases. In this review, we first made a brief introduction about the functions of lncRNAs, five important lncRNA-related diseases, five critical disease-related lncRNAs and some important publicly available lncRNA-related databases about sequence, expression, function, etc. Nowadays, only a limited number of lncRNAs have been experimentally reported to be related to human diseases. Therefore, analyzing available lncRNA-disease associations and predicting potential human lncRNA-disease associations have become important tasks of bioinformatics, which would benefit human complex diseases mechanism understanding at lncRNA level, disease biomarker detection and disease diagnosis, treatment, prognosis and prevention. Furthermore, we introduced some state-of-the-art computational models, which could be effectively used to identify disease-related lncRNAs on a large scale and select the most promising disease-related lncRNAs for experimental validation. We also analyzed the limitations of these models and discussed the future directions of developing computational models for lncRNA research."
27340949,3.0,Multiscale modeling of brain dynamics: from single neurons and networks to mathematical tools,2016 Sep;8(5):438-58.,"The extreme complexity of the brain naturally requires mathematical modeling approaches on a large variety of scales; the spectrum ranges from single neuron dynamics over the behavior of groups of neurons to neuronal network activity. Thus, the connection between the microscopic scale (single neuron activity) to macroscopic behavior (emergent behavior of the collective dynamics) and vice versa is a key to understand the brain in its complexity. In this work, we attempt a review of a wide range of approaches, ranging from the modeling of single neuron dynamics to machine learning. The models include biophysical as well as data-driven phenomenological models. The discussed models include Hodgkin-Huxley, FitzHugh-Nagumo, coupled oscillators (Kuramoto oscillators, Rössler oscillators, and the Hindmarsh-Rose neuron), Integrate and Fire, networks of neurons, and neural field equations. In addition to the mathematical models, important mathematical methods in multiscale modeling and reconstruction of the causal connectivity are sketched. The methods include linear and nonlinear tools from statistics, data analysis, and time series analysis up to differential equations, dynamical systems, and bifurcation theory, including Granger causal connectivity analysis, phase synchronization connectivity analysis, principal component analysis (PCA), independent component analysis (ICA), and manifold learning algorithms such as ISOMAP, and diffusion maps and equation-free techniques. WIREs Syst Biol Med 2016, 8:438-458. doi: 10.1002/wsbm.1348 For further resources related to this article, please visit the WIREs website."
27322705,4.0,Machine learning approaches in MALDI-MSI: clinical applications,2016 Jul;13(7):685-96.,"Introduction:                    Despite the unquestionable advantages of Matrix-Assisted Laser Desorption/Ionization Mass Spectrometry Imaging in visualizing the spatial distribution and the relative abundance of biomolecules directly on-tissue, the yielded data is complex and high dimensional. Therefore, analysis and interpretation of this huge amount of information is mathematically, statistically and computationally challenging.              Areas covered:                    This article reviews some of the challenges in data elaboration with particular emphasis on machine learning techniques employed in clinical applications, and can be useful in general as an entry point for those who want to study the computational aspects. Several characteristics of data processing are described, enlightening advantages and disadvantages. Different approaches for data elaboration focused on clinical applications are also provided. Practical tutorial based upon Orange Canvas and Weka software is included, helping familiarization with the data processing. Expert commentary: Recently, MALDI-MSI has gained considerable attention and has been employed for research and diagnostic purposes, with successful results. Data dimensionality constitutes an important issue and statistical methods for information-preserving data reduction represent one of the most challenging aspects. The most common data reduction methods are characterized by collecting independent observations into a single table. However, the incorporation of relational information can improve the discriminatory capability of the data."
27315762,61.0,What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated,2016 Jul;20(7):512-534.,"We update complementary learning systems (CLS) theory, which holds that intelligent agents must possess two learning systems, instantiated in mammalians in neocortex and hippocampus. The first gradually acquires structured knowledge representations while the second quickly learns the specifics of individual experiences. We broaden the role of replay of hippocampal memories in the theory, noting that replay allows goal-dependent weighting of experience statistics. We also address recent challenges to the theory and extend it by showing that recurrent activation of hippocampal traces can support some forms of generalization and that neocortical learning can be rapid for information that is consistent with known structure. Finally, we note the relevance of the theory to the design of artificial intelligent agents, highlighting connections between neuroscience and machine learning."
27306552,16.0,Transforming the care of atrial fibrillation with mobile health,2016 Oct;47(1):45-50.,"Atrial fibrillation (AF) is a multifaceted and highly variable disease that is often difficult to manage within the traditional health-care model. The conventional model of regular or pre-scheduled appointments with physicians or allied health professionals is poorly suited to the unpredictable and often urgent clinical needs of patients with AF. Mobile health (mHealth) has the potential to dramatically transform the delivery and quality of AF care. In this brief review, we summarize the current limitations and evidence gaps in treating patients with AF. We then describe the current mHealth landscape, changes in telehealth coverage and reimbursement, and recent technological advances of smartphones, mobile applications, and connected wearable devices. We also describe important barriers and challenges, such as clinical management of large volumes of data, application of predictive analytics/machine learning, and the need for high-quality randomized clinical trials."
27295548,33.0,A renaissance of neural networks in drug discovery,2016 Aug;11(8):785-95.,"Introduction:                    Neural networks are becoming a very popular method for solving machine learning and artificial intelligence problems. The variety of neural network types and their application to drug discovery requires expert knowledge to choose the most appropriate approach.              Areas covered:                    In this review, the authors discuss traditional and newly emerging neural network approaches to drug discovery. Their focus is on backpropagation neural networks and their variants, self-organizing maps and associated methods, and a relatively new technique, deep learning. The most important technical issues are discussed including overfitting and its prevention through regularization, ensemble and multitask modeling, model interpretation, and estimation of applicability domain. Different aspects of using neural networks in drug discovery are considered: building structure-activity models with respect to various targets; predicting drug selectivity, toxicity profiles, ADMET and physicochemical properties; characteristics of drug-delivery systems and virtual screening.              Expert opinion:                    Neural networks continue to grow in importance for drug discovery. Recent developments in deep learning suggests further improvements may be gained in the analysis of large chemical data sets. It's anticipated that neural networks will be more widely used in drug discovery in the future, and applied in non-traditional areas such as drug delivery systems, biologically compatible materials, and regenerative medicine."
27282231,4.0,A review of the applications of data mining and machine learning for the prediction of biomedical properties of nanoparticles,2016 Aug;132:93-103.,"This article presents a comprehensive review of applications of data mining and machine learning for the prediction of biomedical properties of nanoparticles of medical interest. The papers reviewed here present the results of research using these techniques to predict the biological fate and properties of a variety of nanoparticles relevant to their biomedical applications. These include the influence of particle physicochemical properties on cellular uptake, cytotoxicity, molecular loading, and molecular release in addition to manufacturing properties like nanoparticle size, and polydispersity. Overall, the results are encouraging and suggest that as more systematic data from nanoparticles becomes available, machine learning and data mining would become a powerful aid in the design of nanoparticles for biomedical applications. There is however the challenge of great heterogeneity in nanoparticles, which will make these discoveries more challenging than for traditional small molecule drug design."
27271051,127.0,"Characterization of PET/CT images using texture analysis: the past, the present… any future?",2017 Jan;44(1):151-165.,"After seminal papers over the period 2009 - 2011, the use of texture analysis of PET/CT images for quantification of intratumour uptake heterogeneity has received increasing attention in the last 4 years. Results are difficult to compare due to the heterogeneity of studies and lack of standardization. There are also numerous challenges to address. In this review we provide critical insights into the recent development of texture analysis for quantifying the heterogeneity in PET/CT images, identify issues and challenges, and offer recommendations for the use of texture analysis in clinical research. Numerous potentially confounding issues have been identified, related to the complex workflow for the calculation of textural features, and the dependency of features on various factors such as acquisition, image reconstruction, preprocessing, functional volume segmentation, and methods of establishing and quantifying correspondences with genomic and clinical metrics of interest. A lack of understanding of what the features may represent in terms of the underlying pathophysiological processes and the variability of technical implementation practices makes comparing results in the literature challenging, if not impossible. Since progress as a field requires pooling results, there is an urgent need for standardization and recommendations/guidelines to enable the field to move forward. We provide a list of correct formulae for usual features and recommendations regarding implementation. Studies on larger cohorts with robust statistical analysis and machine learning approaches are promising directions to evaluate the potential of this approach."
27224846,18.0,Using online social networks to track a pandemic: A systematic review,2016 Aug;62:1-11.,"Background:                    The popularity and proliferation of online social networks (OSNs) have created massive social interaction among users that generate an extensive amount of data. An OSN offers a unique opportunity for studying and understanding social interaction and communication among far larger populations now more than ever before. Recently, OSNs have received considerable attention as a possible tool to track a pandemic because they can provide an almost real-time surveillance system at a less costly rate than traditional surveillance systems.              Methods:                    A systematic literature search for studies with the primary aim of using OSN to detect and track a pandemic was conducted. We conducted an electronic literature search for eligible English articles published between 2004 and 2015 using PUBMED, IEEExplore, ACM Digital Library, Google Scholar, and Web of Science. First, the articles were screened on the basis of titles and abstracts. Second, the full texts were reviewed. All included studies were subjected to quality assessment.              Result:                    OSNs have rich information that can be utilized to develop an almost real-time pandemic surveillance system. The outcomes of OSN surveillance systems have demonstrated high correlations with the findings of official surveillance systems. However, the limitation in using OSN to track pandemic is in collecting representative data with sufficient population coverage. This challenge is related to the characteristics of OSN data. The data are dynamic, large-sized, and unstructured, thus requiring advanced algorithms and computational linguistics.              Conclusions:                    OSN data contain significant information that can be used to track a pandemic. Different from traditional surveys and clinical reports, in which the data collection process is time consuming at costly rates, OSN data can be collected almost in real time at a cheaper cost. Additionally, the geographical and temporal information can provide exploratory analysis of spatiotemporal dynamics of infectious disease spread. However, on one hand, an OSN-based surveillance system requires comprehensive adoption, enhanced geographical identification system, and advanced algorithms and computational linguistics to eliminate its limitations and challenges. On the other hand, OSN is probably to never replace traditional surveillance, but it can offer complementary data that can work best when integrated with traditional data."
27213397,1.0,Contextualising Water Use in Residential Settings: A Survey of Non-Intrusive Techniques and Approaches,2016 May 20;16(5):738.,"Water monitoring in households is important to ensure the sustainability of fresh water reserves on our planet. It provides stakeholders with the statistics required to formulate optimal strategies in residential water management. However, this should not be prohibitive and appliance-level water monitoring cannot practically be achieved by deploying sensors on every faucet or water-consuming device of interest due to the higher hardware costs and complexity, not to mention the risk of accidental leakages that can derive from the extra plumbing needed. Machine learning and data mining techniques are promising techniques to analyse monitored data to obtain non-intrusive water usage disaggregation. This is because they can discern water usage from the aggregated data acquired from a single point of observation. This paper provides an overview of water usage disaggregation systems and related techniques adopted for water event classification. The state-of-the art of algorithms and testbeds used for fixture recognition are reviewed and a discussion on the prominent challenges and future research are also included."
27207370,5.0,Bioimage Informatics for Big Data,2016;219:263-72.,"Bioimage informatics is a field wherein high-throughput image informatics methods are used to solve challenging scientific problems related to biology and medicine. When the image datasets become larger and more complicated, many conventional image analysis approaches are no longer applicable. Here, we discuss two critical challenges of large-scale bioimage informatics applications, namely, data accessibility and adaptive data analysis. We highlight case studies to show that these challenges can be tackled based on distributed image computing as well as machine learning of image examples in a multidimensional environment."
27171499,21.0,Discovery and Optimization of Materials Using Evolutionary Approaches,2016 May 25;116(10):6107-32.,"Materials science is undergoing a revolution, generating valuable new materials such as flexible solar panels, biomaterials and printable tissues, new catalysts, polymers, and porous materials with unprecedented properties. However, the number of potentially accessible materials is immense. Artificial evolutionary methods such as genetic algorithms, which explore large, complex search spaces very efficiently, can be applied to the identification and optimization of novel materials more rapidly than by physical experiments alone. Machine learning models can augment experimental measurements of materials fitness to accelerate identification of useful and novel materials in vast materials composition or property spaces. This review discusses the problems of large materials spaces, the types of evolutionary algorithms employed to identify or optimize materials, and how materials can be represented mathematically as genomes, describes fitness landscapes and mutation operators commonly employed in materials evolution, and provides a comprehensive summary of published research on the use of evolutionary methods to generate new catalysts, phosphors, and a range of other materials. The review identifies the potential for evolutionary methods to revolutionize a wide range of manufacturing, medical, and materials based industries."
27168345,20.0,Neurobiological markers predicting treatment response in anxiety disorders: A systematic review and implications for clinical application,2016 Jul;66:143-62.,"Anxiety disorders constitute the largest group of mental disorders with a high individual and societal burden. Neurobiological markers of treatment response bear potential to improve response rates by informing stratified medicine approaches. A systematic review was performed on the current evidence of the predictive value of genetic, neuroimaging and other physiological markers for treatment response (pharmacological and/or psychotherapeutic treatment) in anxiety disorders. Studies published until March 2015 were selected through search in PubMed, Web of Science, PsycINFO, Embase, and CENTRAL. Sixty studies were included, among them 27 on genetic, 17 on neuroimaging and 16 on other markers. Preliminary evidence was found for the functional 5-HTTLPR/rs25531 genotypes, anterior cingulate cortex function and cardiovascular flexibility to modulate treatment outcome. Studies varied considerably in methodological quality. Application of more stringent study methodology, predictions on the individual patient level and cross-validation in independent samples are recommended to set the next stage of biomarker research and to avoid flawed conclusions in the emerging field of ""Mental Health Predictomics""."
27157417,1.0,Probing the Hypothesis of SAR Continuity Restoration by the Removal of Activity Cliffs Generators in QSAR,2016;22(33):5043-5056.,"In this work we report the first attempt to study the effect of activity cliffs over the generalization ability of machine learning (ML) based QSAR classifiers, using as study case a previously reported diverse and noisy dataset focused on drug induced liver injury (DILI) and more than 40 ML classification algorithms. Here, the hypothesis of structure-activity relationship (SAR) continuity restoration by activity cliffs removal is tested as a potential solution to overcome such limitation. Previously, a parallelism was established between activity cliffs generators (ACGs) and instances that should be misclassified (ISMs), a related concept from the field of machine learning. Based on this concept we comparatively studied the classification performance of multiple machine learning classifiers as well as the consensus classifier derived from predictive classifiers obtained from training sets including or excluding ACGs. The influence of the removal of ACGs from the training set over the virtual screening performance was also studied for the respective consensus classifiers algorithms. In general terms, the removal of the ACGs from the training process slightly decreased the overall accuracy of the ML classifiers and multi-classifiers, improving their sensitivity (the weakest feature of ML classifiers trained with ACGs) but decreasing their specificity. Although these results do not support a positive effect of the removal of ACGs over the classification performance of ML classifiers, the ""balancing effect"" of ACG removal demonstrated to positively influence the virtual screening performance of multi-classifiers based on valid base ML classifiers. Specially, the early recognition ability was significantly favored after ACGs removal. The results presented and discussed in this work represent the first step towards the application of a remedial solution to the activity cliffs problem in QSAR studies."
27157416,9.0,Data-driven Approach to Detect and Predict Adverse Drug Reactions,2016;22(23):3498-526.,"Background:                    Many factors that directly or indirectly cause adverse drug reaction (ADRs) varying from pharmacological, immunological and genetic factors to ethnic, age, gender, social factors as well as drug and disease related ones. On the other hand, advanced methods of statistics, machine learning and data mining allow the users to more effectively analyze the data for descriptive and predictive purposes. The fast changes in this field make it difficult to follow the research progress and context on ADR detection and prediction.              Methods:                    A large amount of articles on ADRs in the last twenty years is collected. These articles are grouped by recent data types used to study ADRs: omics, social media and electronic medical records (EMRs), and reviewed in terms of the problem addressed, the datasets used and methods.              Results:                    Corresponding three tables are established providing brief information on the research for ADRs detection and prediction.              Conclusion:                    The data-driven approach has shown to be powerful in ADRs detection and prediction. The review helps researchers and pharmacists to have a quick overview on the current status of ADRs detection and prediction."
27130797,43.0,IBM Watson: How Cognitive Computing Can Be Applied to Big Data Challenges in Life Sciences Research,2016 Apr;38(4):688-701.,"Life sciences researchers are under pressure to innovate faster than ever. Big data offer the promise of unlocking novel insights and accelerating breakthroughs. Ironically, although more data are available than ever, only a fraction is being integrated, understood, and analyzed. The challenge lies in harnessing volumes of data, integrating the data from hundreds of sources, and understanding their various formats. New technologies such as cognitive computing offer promise for addressing this challenge because cognitive solutions are specifically designed to integrate and analyze big datasets. Cognitive solutions can understand different types of data such as lab values in a structured database or the text of a scientific publication. Cognitive solutions are trained to understand technical, industry-specific content and use advanced reasoning, predictive modeling, and machine learning techniques to advance research faster. Watson, a cognitive computing technology, has been configured to support life sciences research. This version of Watson includes medical literature, patents, genomics, and chemical and pharmacological data that researchers would typically use in their work. Watson has also been developed with specific comprehension of scientific terminology so it can make novel connections in millions of pages of text. Watson has been applied to a few pilot studies in the areas of drug target identification and drug repurposing. The pilot results suggest that Watson can accelerate identification of novel drug candidates and novel drug targets by harnessing the potential of big data."
27119951,18.0,Big Data and Machine Learning in Plastic Surgery: A New Frontier in Surgical Innovation,2016 May;137(5):890e-897e.,"Medical decision-making is increasingly based on quantifiable data. From the moment patients come into contact with the health care system, their entire medical history is recorded electronically. Whether a patient is in the operating room or on the hospital ward, technological advancement has facilitated the expedient and reliable measurement of clinically relevant health metrics, all in an effort to guide care and ensure the best possible clinical outcomes. However, as the volume and complexity of biomedical data grow, it becomes challenging to effectively process ""big data"" using conventional techniques. Physicians and scientists must be prepared to look beyond classic methods of data processing to extract clinically relevant information. The purpose of this article is to introduce the modern plastic surgeon to machine learning and computational interpretation of large data sets. What is machine learning? Machine learning, a subfield of artificial intelligence, can address clinically relevant problems in several domains of plastic surgery, including burn surgery; microsurgery; and craniofacial, peripheral nerve, and aesthetic surgery. This article provides a brief introduction to current research and suggests future projects that will allow plastic surgeons to explore this new frontier of surgical science."
27114900,15.0,Computational analysis in epilepsy neuroimaging: A survey of features and methods,2016 Feb 23;11:515-529.,"Epilepsy affects 65 million people worldwide, a third of whom have seizures that are resistant to anti-epileptic medications. Some of these patients may be amenable to surgical therapy or treatment with implantable devices, but this usually requires delineation of discrete structural or functional lesion(s), which is challenging in a large percentage of these patients. Advances in neuroimaging and machine learning allow semi-automated detection of malformations of cortical development (MCDs), a common cause of drug resistant epilepsy. A frequently asked question in the field is what techniques currently exist to assist radiologists in identifying these lesions, especially subtle forms of MCDs such as focal cortical dysplasia (FCD) Type I and low grade glial tumors. Below we introduce some of the common lesions encountered in patients with epilepsy and the common imaging findings that radiologists look for in these patients. We then review and discuss the computational techniques introduced over the past 10 years for quantifying and automatically detecting these imaging findings. Due to large variations in the accuracy and implementation of these studies, specific techniques are traditionally used at individual centers, often guided by local expertise, as well as selection bias introduced by the varying prevalence of specific patient populations in different epilepsy centers. We discuss the need for a multi-institutional study that combines features from different imaging modalities as well as computational techniques to definitively assess the utility of specific automated approaches to epilepsy imaging. We conclude that sharing and comparing these different computational techniques through a common data platform provides an opportunity to rigorously test and compare the accuracy of these tools across different patient populations and geographical locations. We propose that these kinds of tools, quantitative imaging analysis methods and open data platforms for aggregating and sharing data and algorithms, can play a vital role in reducing the cost of care, the risks of invasive treatments, and improve overall outcomes for patients with epilepsy."
27113568,10.0,Use of systems biology to decipher host-pathogen interaction networks and predict biomarkers,2016 Jul;22(7):600-6.,"In systems biology, researchers aim to understand complex biological systems as a whole, which is often achieved by mathematical modelling and the analyses of high-throughput data. In this review, we give an overview of medical applications of systems biology approaches with special focus on host-pathogen interactions. After introducing general ideas of systems biology, we focus on (1) the detection of putative biomarkers for improved diagnosis and support of therapeutic decisions, (2) network modelling for the identification of regulatory interactions between cellular molecules to reveal putative drug targets and (3) module discovery for the detection of phenotype-specific modules in molecular interaction networks. Biomarker detection applies supervised machine learning methods utilizing high-throughput data (e.g. single nucleotide polymorphism (SNP) detection, RNA-seq, proteomics) and clinical data. We demonstrate structural analysis of molecular networks, especially by identification of disease modules as a novel strategy, and discuss possible applications to host-pathogen interactions. Pioneering work was done to predict molecular host-pathogen interactions networks based on dual RNA-seq data. However, currently this network modelling is restricted to a small number of genes. With increasing number and quality of databases and data repositories, the prediction of large-scale networks will also be feasible that can used for multidimensional diagnosis and decision support for prevention and therapy of diseases. Finally, we outline further perspective issues such as support of personalized medicine with high-throughput data and generation of multiscale host-pathogen interaction models."
27110292,47.0,Machine-learning scoring functions to improve structure-based binding affinity prediction and virtual screening,Nov-Dec 2015;5(6):405-424.,"Docking tools to predict whether and how a small molecule binds to a target can be applied if a structural model of such target is available. The reliability of docking depends, however, on the accuracy of the adopted scoring function (SF). Despite intense research over the years, improving the accuracy of SFs for structure-based binding affinity prediction or virtual screening has proven to be a challenging task for any class of method. New SFs based on modern machine-learning regression models, which do not impose a predetermined functional form and thus are able to exploit effectively much larger amounts of experimental data, have recently been introduced. These machine-learning SFs have been shown to outperform a wide range of classical SFs at both binding affinity prediction and virtual screening. The emerging picture from these studies is that the classical approach of using linear regression with a small number of expert-selected structural features can be strongly improved by a machine-learning approach based on nonlinear regression allied with comprehensive data-driven feature selection. Furthermore, the performance of classical SFs does not grow with larger training datasets and hence this performance gap is expected to widen as more training data becomes available in the future. Other topics covered in this review include predicting the reliability of a SF on a particular target class, generating synthetic data to improve predictive performance and modeling guidelines for SF development. WIREs Comput Mol Sci 2015, 5:405-424. doi: 10.1002/wcms.1225 For further resources related to this article, please visit the WIREs website."
27097559,31.0,Areas of controversy in neuroprogression in bipolar disorder,2016 Aug;134(2):91-103.,"Objective:                    We aimed to review clinical features and biological underpinnings related to neuroprogression in bipolar disorder (BD). Also, we discussed areas of controversy and future research in the field.              Method:                    We systematically reviewed the extant literature pertaining to neuroprogression and BD by searching PubMed and EMBASE for articles published up to March 2016.              Results:                    A total of 114 studies were included. Neuroimaging and clinical evidence from cross-sectional and longitudinal studies show that a subset of patients with BD presents a neuroprogressive course with brain changes and unfavorable outcomes. Risk factors associated with these unfavorable outcomes are number of mood episodes, early trauma, and psychiatric and clinical comorbidity.              Conclusion:                    Illness trajectories are largely variable, and illness progression is not a general rule in BD. The number of manic episodes seems to be the clinical marker more robustly associated with neuroprogression in BD. However, the majority of the evidence came from cross-sectional studies that are prone to bias. Longitudinal studies may help to identify signatures of neuroprogression and integrate findings from the field of neuroimaging, neurocognition, and biomarkers."
27090952,38.0,Computational Analysis of Behavior,2016 Jul 8;39:217-36.,"In this review, we discuss the emerging field of computational behavioral analysis-the use of modern methods from computer science and engineering to quantitatively measure animal behavior. We discuss aspects of experiment design important to both obtaining biologically relevant behavioral data and enabling the use of machine vision and learning techniques for automation. These two goals are often in conflict. Restraining or restricting the environment of the animal can simplify automatic behavior quantification, but it can also degrade the quality or alter important aspects of behavior. To enable biologists to design experiments to obtain better behavioral measurements, and computer scientists to pinpoint fruitful directions for algorithm improvement, we review known effects of artificial manipulation of the animal on behavior. We also review machine vision and learning techniques for tracking, feature extraction, automated behavior classification, and automated behavior discovery, the assumptions they make, and the types of data they work best with."
27072840,6.0,Regional infarction identification from cardiac CT images: a computer-aided biomechanical approach,2016 Sep;11(9):1573-83.,"Purpose:                    Regional infarction identification is important for heart disease diagnosis and management, and myocardial deformation has been shown to be effective for this purpose. Although tagged and strain-encoded MR images can provide such measurements, they are uncommon in clinical routine. On the contrary, cardiac CT images are more available with lower costs, but they only provide motion of cardiac boundaries and additional constraints are required to obtain the myocardial strains. The goal of this study is to verify the potential of contrast-enhanced CT images on computer-aided regional infarction identification.              Methods:                    We propose a biomechanical approach combined with machine learning algorithms. A hyperelastic biomechanical model is used with deformable image registration to estimate 3D myocardial strains from CT images. The regional strains and CT image intensities are input to a classifier for regional infarction identification. Cross-validations on ten canine image sequences with artificially induced infarctions were used to study the performances of using different feature combinations and machine learning algorithms.              Results:                    Radial strain, circumferential strain, first principal strain, and image intensity were shown to be discriminative features. The highest identification accuracy ([Formula: see text] %) was achieved when combining radial strain with image intensity. Random forests gave better results than support vector machines on less discriminative features. Random forests also performed better when all strains were used together.              Conclusion:                    Although CT images cannot directly measure myocardial deformation, with the use of a biomechanical model, the estimated strains can provide promising identification results especially when combined with CT image intensity."
27053448,3.0,"Outcome modeling techniques for prostate cancer radiotherapy: Data, models, and validation",2016 Mar;32(3):512-20.,"Prostate cancer is a frequently diagnosed malignancy worldwide and radiation therapy is a first-line approach in treating localized as well as locally advanced cases. The limiting factor in modern radiotherapy regimens is dose to normal structures, an excess of which can lead to aberrant radiation-induced toxicities. Conversely, dose reduction to spare adjacent normal structures risks underdosing target volumes and compromising local control. As a result, efforts aimed at predicting the effects of radiotherapy could invaluably optimize patient treatments by mitigating such toxicities and simultaneously maximizing biochemical control. In this work, we review the types of data, frameworks and techniques used for prostate radiotherapy outcome modeling. Consideration is given to clinical and dose-volume metrics, such as those amassed by the QUANTEC initiative, and also to newer methods for the integration of biological and genetic factors to improve prediction performance. We furthermore highlight trends in machine learning that may help to elucidate the complex pathophysiological mechanisms of tumor control and radiation-induced normal tissue side effects."
27039698,11.0,Using neuroimaging to help predict the onset of psychosis,2017 Jan 15;145(Pt B):209-217.,"The aim of this review is to assess the potential for neuroimaging measures to facilitate prediction of the onset of psychosis. Research in this field has mainly involved people at 'ultra-high risk' (UHR) of psychosis, who have a very high risk of developing a psychotic disorder within a few years of presentation to mental health services. The review details the key findings and developments in this area to date and examines the methodological and logistical challenges associated with making predictions in an individual subject in a clinical setting."
27017830,,Computational Analysis and Simulation of Empathic Behaviors: a Survey of Empathy Modeling with Behavioral Signal Processing Framework,2016 May;18(5):49.,"Empathy is an important psychological process that facilitates human communication and interaction. Enhancement of empathy has profound significance in a range of applications. In this paper, we review emerging directions of research on computational analysis of empathy expression and perception as well as empathic interactions, including their simulation. We summarize the work on empathic expression analysis by the targeted signal modalities (e.g., text, audio, and facial expressions). We categorize empathy simulation studies into theory-based emotion space modeling or application-driven user and context modeling. We summarize challenges in computational study of empathy including conceptual framing and understanding of empathy, data availability, appropriate use and validation of machine learning techniques, and behavior signal processing. Finally, we propose a unified view of empathy computation and offer a series of open problems for future research."
27014079,23.0,Predicting Essential Genes and Proteins Based on Machine Learning and Network Topological Features: A Comprehensive Review,2016 Mar 8;7:75.,"Essential proteins/genes are indispensable to the survival or reproduction of an organism, and the deletion of such essential proteins will result in lethality or infertility. The identification of essential genes is very important not only for understanding the minimal requirements for survival of an organism, but also for finding human disease genes and new drug targets. Experimental methods for identifying essential genes are costly, time-consuming, and laborious. With the accumulation of sequenced genomes data and high-throughput experimental data, many computational methods for identifying essential proteins are proposed, which are useful complements to experimental methods. In this review, we show the state-of-the-art methods for identifying essential genes and proteins based on machine learning and network topological features, point out the progress and limitations of current methods, and discuss the challenges and directions for further research."
27012503,177.0,Single subject prediction of brain disorders in neuroimaging: Promises and pitfalls,2017 Jan 15;145(Pt B):137-165.,"Neuroimaging-based single subject prediction of brain disorders has gained increasing attention in recent years. Using a variety of neuroimaging modalities such as structural, functional and diffusion MRI, along with machine learning techniques, hundreds of studies have been carried out for accurate classification of patients with heterogeneous mental and neurodegenerative disorders such as schizophrenia and Alzheimer's disease. More than 500 studies have been published during the past quarter century on single subject prediction focused on a multiple brain disorders. In the first part of this study, we provide a survey of more than 200 reports in this field with a focus on schizophrenia, mild cognitive impairment (MCI), Alzheimer's disease (AD), depressive disorders, autism spectrum disease (ASD) and attention-deficit hyperactivity disorder (ADHD). Detailed information about those studies such as sample size, type and number of extracted features and reported accuracy are summarized and discussed. To our knowledge, this is by far the most comprehensive review of neuroimaging-based single subject prediction of brain disorders. In the second part, we present our opinion on major pitfalls of those studies from a machine learning point of view. Common biases are discussed and suggestions are provided. Moreover, emerging trends such as decentralized data sharing, multimodal brain imaging, differential diagnosis, disease subtype classification and deep learning are also discussed. Based on this survey, there is extensive evidence showing the great potential of neuroimaging data for single subject prediction of various disorders. However, the main bottleneck of this exciting field is still the limited sample size, which could be potentially addressed by modern data sharing models such as the ones discussed in this paper. Emerging big data technologies and advanced data-intensive machine learning methodologies such as deep learning have coincided with an increasing need for accurate, robust and generalizable single subject prediction of brain disorders during an exciting time. In this report, we survey the past and offer some opinions regarding the road ahead."
27011184,46.0,"MicroRNAs as Biomarkers for Diagnosis, Prognosis and Theranostics in Prostate Cancer",2016 Mar 22;17(3):421.,"Prostate cancer (PC) includes several phenotypes, from indolent to highly aggressive cancer. Actual diagnostic and prognostic tools have several limitations, and there is a need for new biomarkers to stratify patients and assign them optimal therapies by taking into account potential genetic and epigenetic differences. MicroRNAs (miRNAs) are small sequences of non-coding RNA regulating specific genes involved in the onset and development of PC. Stable miRNAs have been found in biofluids, such as serum and plasma; thus, the measurement of PC-associated miRNAs is emerging as a non-invasive tool for PC detection and monitoring. In this study, we conduct an in-depth literature review focusing on miRNAs that may contribute to the diagnosis and prognosis of PC. The role of miRNAs as a potential theranostic tool in PC is discussed. Using a meta-analysis approach, we found a group of 29 miRNAs with diagnostic properties and a group of seven miRNAs with prognostic properties, which were found already expressed in both biofluids and PC tissues. We tested the two miRNA groups on The Cancer Genome Atlas dataset of PC tissue samples with a machine-learning approach. Our results suggest that these 29 miRNAs should be considered as potential panel of biomarkers for the diagnosis of PC, both as in vivo non-invasive test and ex vivo confirmation test."
27007977,101.0,Applications of Deep Learning in Biomedicine,2016 May 2;13(5):1445-54.,"Increases in throughput and installed base of biomedical research equipment led to a massive accumulation of -omics data known to be highly variable, high-dimensional, and sourced from multiple often incompatible data platforms. While this data may be useful for biomarker identification and drug discovery, the bulk of it remains underutilized. Deep neural networks (DNNs) are efficient algorithms based on the use of compositional layers of neurons, with advantages well matched to the challenges -omics data presents. While achieving state-of-the-art results and even surpassing human accuracy in many challenging tasks, the adoption of deep learning in biomedicine has been comparatively slow. Here, we discuss key features of deep learning that may give this approach an edge over other machine learning methods. We then consider limitations and review a number of applications of deep learning in biomedical studies demonstrating proof of concept and practical utility."
26996942,6.0,Current Approaches in Computational Drug Resistance Prediction in HIV,2016;14(4):307-15.,"Background:                    Today a broad range of antiretroviral drug regimens are applicable for the successful suppression of virus replication in human immunodeficiency virus (HIV) infected people. However, there still remains an obstacle in therapy: the high mutation rate of the HI virus under drug pressure leads to resistant variants causing failure of permanent and effective treatment. Therefore, resistance testing is therefore inevitable to administer appropriate antiviral drugs to infected patients.              Methods:                    By means of current high-throughput sequencing technologies, computational models have recently constituted important assistance in drug resistance prediction and can guide the choice of medical treatment. Several machine learning algorithms, e.g. support-vector machines, random forests, as well as statistical methods have been already applied to genotypic data and structural information to predict drug resistance.              Results:                    In this review, we provide an overview of existing approaches in computational drug resistance prediction in HIV. We further highlight the challenges and limitations of current methods, e.g. time complexity and prediction of non-B subtypes.              Conclusion:                    Moreover, we give a perspective on multi-label and multi-instance classification techniques that potentially tackle the problem of cross-resistances among drugs."
26996613,104.0,Metabolic Burden: Cornerstones in Synthetic Biology and Metabolic Engineering Applications,2016 Aug;34(8):652-664.,"Engineering cell metabolism for bioproduction not only consumes building blocks and energy molecules (e.g., ATP) but also triggers energetic inefficiency inside the cell. The metabolic burdens on microbial workhorses lead to undesirable physiological changes, placing hidden constraints on host productivity. We discuss cell physiological responses to metabolic burdens, as well as strategies to identify and resolve the carbon and energy burden problems, including metabolic balancing, enhancing respiration, dynamic regulatory systems, chromosomal engineering, decoupling cell growth with production phases, and co-utilization of nutrient resources. To design robust strains with high chances of success in industrial settings, novel genome-scale models (GSMs), (13)C-metabolic flux analysis (MFA), and machine-learning approaches are needed for weighting, standardizing, and predicting metabolic costs."
26995379,13.0,Rationale and methodology of a collaborative learning project in congenital cardiac care,2016 Apr;174:129-37.,"Background:                    Collaborative learning is a technique through which individuals or teams learn together by capitalizing on one another's knowledge, skills, resources, experience, and ideas. Clinicians providing congenital cardiac care may benefit from collaborative learning given the complexity of the patient population and team approach to patient care.              Rationale and development:                    Industrial system engineers first performed broad-based time-motion and process analyses of congenital cardiac care programs at 5 Pediatric Heart Network core centers. Rotating multidisciplinary team site visits to each center were completed to facilitate deep learning and information exchange. Through monthly conference calls and an in-person meeting, we determined that duration of mechanical ventilation following infant cardiac surgery was one key variation that could impact a number of clinical outcomes. This was underscored by one participating center's practice of early extubation in the majority of its patients. A consensus clinical practice guideline using collaborative learning was developed and implemented by multidisciplinary teams from the same 5 centers. The 1-year prospective initiative was completed in May 2015, and data analysis is under way.              Conclusion:                    Collaborative learning that uses multidisciplinary team site visits and information sharing allows for rapid structured fact-finding and dissemination of expertise among institutions. System modeling and machine learning approaches objectively identify and prioritize focused areas for guideline development. The collaborative learning framework can potentially be applied to other components of congenital cardiac care and provide a complement to randomized clinical trials as a method to rapidly inform and improve the care of children with congenital heart disease."
26979668,4.0,On the convergence of nanotechnology and Big Data analysis for computer-aided diagnosis,2016 Apr;11(8):959-82.,"An overview is provided of the challenges involved in building computer-aided diagnosis systems capable of precise medical diagnostics based on integration and interpretation of data from different sources and formats. The availability of massive amounts of data and computational methods associated with the Big Data paradigm has brought hope that such systems may soon be available in routine clinical practices, which is not the case today. We focus on visual and machine learning analysis of medical data acquired with varied nanotech-based techniques and on methods for Big Data infrastructure. Because diagnosis is essentially a classification task, we address the machine learning techniques with supervised and unsupervised classification, making a critical assessment of the progress already made in the medical field and the prospects for the near future. We also advocate that successful computer-aided diagnosis requires a merge of methods and concepts from nanotechnology and Big Data analysis."
26962757,8.0,Propensity score analysis with missing data,2016 Sep;21(3):427-45.,"Propensity score analysis is a method that equates treatment and control groups on a comprehensive set of measured confounders in observational (nonrandomized) studies. A successful propensity score analysis reduces bias in the estimate of the average treatment effect in a nonrandomized study, making the estimate more comparable with that obtained from a randomized experiment. This article reviews and discusses an important practical issue in propensity analysis, in which the baseline covariates (potential confounders) and the outcome have missing values (incompletely observed). We review the statistical theory of propensity score analysis and estimation methods for propensity scores with incompletely observed covariates. Traditional logistic regression and modern machine learning methods (e.g., random forests, generalized boosted modeling) as estimation methods for incompletely observed covariates are reviewed. Balance diagnostics and equating methods for incompletely observed covariates are briefly described. Using an empirical example, the propensity score estimation methods for incompletely observed covariates are illustrated and compared. (PsycINFO Database Record"
26952574,11.0,Collective-Intelligence Recommender Systems: Advancing Computer Tailoring for Health Behavior Change Into the 21st Century,2016 Mar 7;18(3):e42.,"Background:                    What is the next frontier for computer-tailored health communication (CTHC) research? In current CTHC systems, study designers who have expertise in behavioral theory and mapping theory into CTHC systems select the variables and develop the rules that specify how the content should be tailored, based on their knowledge of the targeted population, the literature, and health behavior theories. In collective-intelligence recommender systems (hereafter recommender systems) used by Web 2.0 companies (eg, Netflix and Amazon), machine learning algorithms combine user profiles and continuous feedback ratings of content (from themselves and other users) to empirically tailor content. Augmenting current theory-based CTHC with empirical recommender systems could be evaluated as the next frontier for CTHC.              Objective:                    The objective of our study was to uncover barriers and challenges to using recommender systems in health promotion.              Methods:                    We conducted a focused literature review, interviewed subject experts (n=8), and synthesized the results.              Results:                    We describe (1) limitations of current CTHC systems, (2) advantages of incorporating recommender systems to move CTHC forward, and (3) challenges to incorporating recommender systems into CTHC. Based on the evidence presented, we propose a future research agenda for CTHC systems.              Conclusions:                    We promote discussion of ways to move CTHC into the 21st century by incorporation of recommender systems."
26941766,11.0,Changing the Game: Using Integrative Genomics to Probe Virulence Mechanisms of the Stem Rust Pathogen Puccinia graminis f. sp. tritici,2016 Feb 24;7:205.,"The recent resurgence of wheat stem rust caused by new virulent races of Puccinia graminis f. sp. tritici (Pgt) poses a threat to food security. These concerns have catalyzed an extensive global effort toward controlling this disease. Substantial research and breeding programs target the identification and introduction of new stem rust resistance (Sr) genes in cultivars for genetic protection against the disease. Such resistance genes typically encode immune receptor proteins that recognize specific components of the pathogen, known as avirulence (Avr) proteins. A significant drawback to deploying cultivars with single Sr genes is that they are often overcome by evolution of the pathogen to escape recognition through alterations in Avr genes. Thus, a key element in achieving durable rust control is the deployment of multiple effective Sr genes in combination, either through conventional breeding or transgenic approaches, to minimize the risk of resistance breakdown. In this situation, evolution of pathogen virulence would require changes in multiple Avr genes in order to bypass recognition. However, choosing the optimal Sr gene combinations to deploy is a challenge that requires detailed knowledge of the pathogen Avr genes with which they interact and the virulence phenotypes of Pgt existing in nature. Identifying specific Avr genes from Pgt will provide screening tools to enhance pathogen virulence monitoring, assess heterozygosity and propensity for mutation in pathogen populations, and confirm individual Sr gene functions in crop varieties carrying multiple effective resistance genes. Toward this goal, much progress has been made in assembling a high quality reference genome sequence for Pgt, as well as a Pan-genome encompassing variation between multiple field isolates with diverse virulence spectra. In turn this has allowed prediction of Pgt effector gene candidates based on known features of Avr genes in other plant pathogens, including the related flax rust fungus. Upregulation of gene expression in haustoria and evidence for diversifying selection are two useful parameters to identify candidate Avr genes. Recently, we have also applied machine learning approaches to agnostically predict candidate effectors. Here, we review progress in stem rust pathogenomics and approaches currently underway to identify Avr genes recognized by wheat Sr genes."
26936700,30.0,"Vision 20/20: Magnetic resonance imaging-guided attenuation correction in PET/MRI: Challenges, solutions, and opportunities",2016 Mar;43(3):1130-55.,"Attenuation correction is an essential component of the long chain of data correction techniques required to achieve the full potential of quantitative positron emission tomography (PET) imaging. The development of combined PET/magnetic resonance imaging (MRI) systems mandated the widespread interest in developing novel strategies for deriving accurate attenuation maps with the aim to improve the quantitative accuracy of these emerging hybrid imaging systems. The attenuation map in PET/MRI should ideally be derived from anatomical MR images; however, MRI intensities reflect proton density and relaxation time properties of biological tissues rather than their electron density and photon attenuation properties. Therefore, in contrast to PET/computed tomography, there is a lack of standardized global mapping between the intensities of MRI signal and linear attenuation coefficients at 511 keV. Moreover, in standard MRI sequences, bones and lung tissues do not produce measurable signals owing to their low proton density and short transverse relaxation times. MR images are also inevitably subject to artifacts that degrade their quality, thus compromising their applicability for the task of attenuation correction in PET/MRI. MRI-guided attenuation correction strategies can be classified in three broad categories: (i) segmentation-based approaches, (ii) atlas-registration and machine learning methods, and (iii) emission/transmission-based approaches. This paper summarizes past and current state-of-the-art developments and latest advances in PET/MRI attenuation correction. The advantages and drawbacks of each approach for addressing the challenges of MR-based attenuation correction are comprehensively described. The opportunities brought by both MRI and PET imaging modalities for deriving accurate attenuation maps and improving PET quantification will be elaborated. Future prospects and potential clinical applications of these techniques and their integration in commercial systems will also be discussed."
26924521,1.0,Recent advances in quantitative high throughput and high content data analysis,2016;11(4):415-23.,"Introduction:                    High throughput screening has become a basic technique with which to explore biological systems. Advances in technology, including increased screening capacity, as well as methods that generate multiparametric readouts, are driving the need for improvements in the analysis of data sets derived from such screens.              Areas covered:                    This article covers the recent advances in the analysis of high throughput screening data sets from arrayed samples, as well as the recent advances in the analysis of cell-by-cell data sets derived from image or flow cytometry application. Screening multiple genomic reagents targeting any given gene creates additional challenges and so methods that prioritize individual gene targets have been developed. The article reviews many of the open source data analysis methods that are now available and which are helping to define a consensus on the best practices to use when analyzing screening data.              Expert opinion:                    As data sets become larger, and more complex, the need for easily accessible data analysis tools will continue to grow. The presentation of such complex data sets, to facilitate quality control monitoring and interpretation of the results will require the development of novel visualizations. In addition, advanced statistical and machine learning algorithms that can help identify patterns, correlations and the best features in massive data sets will be required. The ease of use for these tools will be important, as they will need to be used iteratively by laboratory scientists to improve the outcomes of complex analyses."
26911811,78.0,Extracting information from the text of electronic medical records to improve case detection: a systematic review,2016 Sep;23(5):1007-15.,"Background:                    Electronic medical records (EMRs) are revolutionizing health-related research. One key issue for study quality is the accurate identification of patients with the condition of interest. Information in EMRs can be entered as structured codes or unstructured free text. The majority of research studies have used only coded parts of EMRs for case-detection, which may bias findings, miss cases, and reduce study quality. This review examines whether incorporating information from text into case-detection algorithms can improve research quality.              Methods:                    A systematic search returned 9659 papers, 67 of which reported on the extraction of information from free text of EMRs with the stated purpose of detecting cases of a named clinical condition. Methods for extracting information from text and the technical accuracy of case-detection algorithms were reviewed.              Results:                    Studies mainly used US hospital-based EMRs, and extracted information from text for 41 conditions using keyword searches, rule-based algorithms, and machine learning methods. There was no clear difference in case-detection algorithm accuracy between rule-based and machine learning methods of extraction. Inclusion of information from text resulted in a significant improvement in algorithm sensitivity and area under the receiver operating characteristic in comparison to codes alone (median sensitivity 78% (codes + text) vs 62% (codes), P = .03; median area under the receiver operating characteristic 95% (codes + text) vs 88% (codes), P = .025).              Conclusions:                    Text in EMRs is accessible, especially with open source information extraction algorithms, and significantly improves case detection when combined with codes. More harmonization of reporting within EMR studies is needed, particularly standardized reporting of algorithm accuracy metrics like positive predictive value (precision) and sensitivity (recall)."
26906507,162.0,Computational psychiatry as a bridge from neuroscience to clinical applications,2016 Mar;19(3):404-13.,"Translating advances in neuroscience into benefits for patients with mental illness presents enormous challenges because it involves both the most complex organ, the brain, and its interaction with a similarly complex environment. Dealing with such complexities demands powerful techniques. Computational psychiatry combines multiple levels and types of computation with multiple types of data in an effort to improve understanding, prediction and treatment of mental illness. Computational psychiatry, broadly defined, encompasses two complementary approaches: data driven and theory driven. Data-driven approaches apply machine-learning methods to high-dimensional data to improve classification of disease, predict treatment outcomes or improve treatment selection. These approaches are generally agnostic as to the underlying mechanisms. Theory-driven approaches, in contrast, use models that instantiate prior knowledge of, or explicit hypotheses about, such mechanisms, possibly at multiple levels of analysis and abstraction. We review recent advances in both approaches, with an emphasis on clinical applications, and highlight the utility of combining them."
26898163,25.0,Painful Issues in Pain Prediction,2016 Apr;39(4):212-220.,"How perception of pain emerges from neural activity is largely unknown. Identifying a neural 'pain signature' and deriving a way to predict perceived pain from brain activity would have enormous basic and clinical implications. Researchers are increasingly turning to functional brain imaging, often applying machine-learning algorithms to infer that pain perception occurred. Yet, such sophisticated analyses are fraught with interpretive difficulties. Here, we highlight some common and troublesome problems in the literature, and suggest methods to ensure researchers draw accurate conclusions from their results. Since functional brain imaging is increasingly finding practical applications with real-world consequences, it is critical to interpret brain scans accurately, because decisions based on neural data will only be as good as the science behind them."
26873661,9.0,"The recent progress in proteochemometric modelling: focusing on target descriptors, cross-term descriptors and application scope",2017 Jan;18(1):125-136.,"As an extension of the conventional quantitative structure activity relationship models, proteochemometric (PCM) modelling is a computational method that can predict the bioactivity relations between multiple ligands and multiple targets. Traditional PCM modelling includes three essential elements: descriptors (including target descriptors, ligand descriptors and cross-term descriptors), bioactivity data and appropriate learning functions that link the descriptors to the bioactivity data. Since its appearance, PCM modelling has developed rapidly over the past decade by taking advantage of the progress of different descriptors and machine learning techniques, along with the increasing amounts of available bioactivity data. Specifically, the new emerging target descriptors and cross-term descriptors not only significantly increased the performance of PCM modelling but also expanded its application scope from traditional protein-ligand interaction to more abundant interactions, including protein-peptide, protein-DNA and even protein-protein interactions. In this review, target descriptors and cross-term descriptors, as well as the corresponding application scope, are intensively summarized. Additionally, we look forward to seeing PCM modelling extend into new application scopes, such as Target-Catalyst-Ligand systems, with the further development of descriptors, machine learning techniques and increasing amounts of available bioactivity data."
26868042,4.0,"Expanding perspectives on cognition in humans, animals, and machines",2016 Apr;37:85-91.,"Over the past decade neuroscience has been attacking the problem of cognition with increasing vigor. Yet, what exactly is cognition, beyond a general signifier of anything seemingly complex the brain does? Here, we briefly review attempts to define, describe, explain, build, enhance and experience cognition. We highlight perspectives including psychology, molecular biology, computation, dynamical systems, machine learning, behavior and phenomenology. This survey of the landscape reveals not a clear target for explanation but a pluralistic and evolving scene with diverse opportunities for grounding future research. We argue that rather than getting to the bottom of it, over the next century, by deconstructing and redefining cognition, neuroscience will and should expand rather than merely reduce our concept of the mind."
26867072,10.0,A Biobehavioral Framework to Address the Emerging Challenge of Multimorbidity,2016 Apr;78(3):281-9.,"Multimorbidity, the co-occurrence of multiple physical or psychological illnesses, is prevalent particularly among older adults. The number of Americans with multiple chronic diseases is projected to increase from 57 million in 2000 to 81 million in 2020. However, behavioral medicine and health psychology, while focusing on the co-occurrence of psychological/psychiatric disorders with primary medical morbidities, have historically tended to ignore the co-occurrence of primary medical comorbidities, such as diabetes and cancer, and their biopsychosocial implications. This approach may hinder our ecologically valid understanding of the etiology, prevention, and treatment for individual patients with multimorbidity. In this selective review, we propose a heuristic behavioral framework for the etiology of multimorbidity. More acknowledgment and systematic research on multiple, co-existing disorders in behavioral medicine are consistent with the biopsychosocial model's emphasis on treating the ""whole person,"" which means not considering any single illness, its symptoms, risk factors, or mechanisms, in isolation. As systems analytics, big data, machine learning, and mixed-model trajectory analyses, among others, come online and become more widely available, we may be able to tackle multimorbidity more holistically, efficiently, and satisfactorily."
26846174,3.0,Models and Data Sources Used in Systems Medicine. A Systematic Literature Review,2016;55(2):107-13.,"Background:                    Systems medicine is a new approach for the development and selection of treatment strategies for patients with complex diseases. It is often referred to as the application of systems biology methods for decision making in patient care. For systems medicine computer applications, many different data sources have to be integrated and included into models. This is a challenging task for Medical Informatics since the approach exceeds traditional systems like Electronic Health Records. To prioritize research activities for systems medicine applications, it is necessary to get an overview over modelling methods and data sources already used in this field.              Objectives:                    We performed a systematic literature review with the objective to capture current use of 1) modelling methods and 2) data sources in systems medicine related research projects.              Methods:                    We queried the MEDLINE and ScienceDirect databases for papers associated with the search term systems medicine and related terms. Papers were screened and assessed in full text in a two-step process according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement guidelines.              Results:                    The queries returned 698 articles of which 34 papers were finally included into the study. A multitude of modelling approaches such as machine learning and network analysis was identified and classified. Since these approaches are also used in other domains, no methods specific for systems medicine could be identified. Omics data are the most widely used data types followed by clinical data. Most studies only include a rather limited number of data sources.              Conclusions:                    Currently, many different modelling approaches are used in systems medicine. Thus, highly flexible modular solutions are necessary for systems medicine clinical applications. However, the number of data sources included into the models is limited and most projects currently focus on prognosis. To leverage the potential of systems medicine further, it will be necessary to focus on treatment strategies for patients and consider a broader range of data."
26834576,3.0,"Multivariate Analyses Applied to Healthy Neurodevelopment in Fetal, Neonatal, and Pediatric MRI",2016 Jan 21;9:163.,"Multivariate analysis (MVA) is a class of statistical and pattern recognition techniques that involve the processing of data that contains multiple measurements per sample. MVA can be used to address a wide variety of neurological medical imaging related challenges including the evaluation of healthy brain development, the automated analysis of brain tissues and structures through image segmentation, evaluating the effects of genetic and environmental factors on brain development, evaluating sensory stimulation's relationship with functional brain activity and much more. Compared to adult imaging, pediatric, neonatal and fetal imaging have attracted less attention from MVA researchers, however, recent years have seen remarkable MVA research growth in pre-adult populations. This paper presents the results of a systematic review of the literature focusing on MVA applied to healthy subjects in fetal, neonatal and pediatric magnetic resonance imaging (MRI) of the brain. While the results of this review demonstrate considerable interest from the scientific community in applications of MVA technologies in brain MRI, the field is still young and significant research growth will continue into the future."
26819529,11.0,Diagnosing gastrointestinal illnesses using fecal headspace volatile organic compounds,2016 Jan 28;22(4):1639-49.,"Volatile organic compounds (VOCs) emitted from stool are the components of the smell of stool representing the end products of microbial activity and metabolism that can be used to diagnose disease. Despite the abundance of hydrogen, carbon dioxide, and methane that have already been identified in human flatus, the small portion of trace gases making up the VOCs emitted from stool include organic acids, alcohols, esters, heterocyclic compounds, aldehydes, ketones, and alkanes, among others. These are the gases that vary among individuals in sickness and in health, in dietary changes, and in gut microbial activity. Electronic nose devices are analytical and pattern recognition platforms that can utilize mass spectrometry or electrochemical sensors to detect these VOCs in gas samples. When paired with machine-learning and pattern recognition algorithms, this can identify patterns of VOCs, and thus patterns of smell, that can be used to identify disease states. In this review, we provide a clinical background of VOC identification, electronic nose development, and review gastroenterology applications toward diagnosing disease by the volatile headspace analysis of stool."
26814169,32.0,Use of machine learning approaches for novel drug discovery,2016;11(3):225-39.,"Introduction:                    The use of computational tools in the early stages of drug development has increased in recent decades. Machine learning (ML) approaches have been of special interest, since they can be applied in several steps of the drug discovery methodology, such as prediction of target structure, prediction of biological activity of new ligands through model construction, discovery or optimization of hits, and construction of models that predict the pharmacokinetic and toxicological (ADMET) profile of compounds.              Areas covered:                    This article presents an overview on some applications of ML techniques in drug design. These techniques can be employed in ligand-based drug design (LBDD) and structure-based drug design (SBDD) studies, such as similarity searches, construction of classification and/or prediction models of biological activity, prediction of secondary structures and binding sites docking and virtual screening.              Expert opinion:                    Successful cases have been reported in the literature, demonstrating the efficiency of ML techniques combined with traditional approaches to study medicinal chemistry problems. Some ML techniques used in drug design are: support vector machine, random forest, decision trees and artificial neural networks. Currently, an important application of ML techniques is related to the calculation of scoring functions used in docking and virtual screening assays from a consensus, combining traditional and ML techniques in order to improve the prediction of binding sites and docking solutions."
26806341,13.0,Computer vision for high content screening,2016;51(2):102-9.,"High Content Screening (HCS) technologies that combine automated fluorescence microscopy with high throughput biotechnology have become powerful systems for studying cell biology and drug screening. These systems can produce more than 100 000 images per day, making their success dependent on automated image analysis. In this review, we describe the steps involved in quantifying microscopy images and different approaches for each step. Typically, individual cells are segmented from the background using a segmentation algorithm. Each cell is then quantified by extracting numerical features, such as area and intensity measurements. As these feature representations are typically high dimensional (>500), modern machine learning algorithms are used to classify, cluster and visualize cells in HCS experiments. Machine learning algorithms that learn feature representations, in addition to the classification or clustering task, have recently advanced the state of the art on several benchmarking tasks in the computer vision community. These techniques have also recently been applied to HCS image analysis."
26800334,3.0,Prototype-based models in machine learning,Mar-Apr 2016;7(2):92-111.,"An overview is given of prototype-based models in machine learning. In this framework, observations, i.e., data, are stored in terms of typical representatives. Together with a suitable measure of similarity, the systems can be employed in the context of unsupervised and supervised analysis of potentially high-dimensional, complex datasets. We discuss basic schemes of competitive vector quantization as well as the so-called neural gas approach and Kohonen's topology-preserving self-organizing map. Supervised learning in prototype systems is exemplified in terms of learning vector quantization. Most frequently, the familiar Euclidean distance serves as a dissimilarity measure. We present extensions of the framework to nonstandard measures and give an introduction to the use of adaptive distances in relevance learning."
26776761,8.0,Information and communication technology solutions for outdoor navigation in dementia,2016 Jun;12(6):695-707.,"Introduction:                    Information and communication technology (ICT) is potentially mature enough to empower outdoor and social activities in dementia. However, actual ICT-based devices have limited functionality and impact, mainly limited to safety. What is an ideal operational framework to enhance this field to support outdoor and social activities?              Methods:                    Review of literature and cross-disciplinary expert discussion.              Results:                    A situation-aware ICT requires a flexible fine-tuning by stakeholders of system usability and complexity of function, and of user safety and autonomy. It should operate by artificial intelligence/machine learning and should reflect harmonized stakeholder values, social context, and user residual cognitive functions. ICT services should be proposed at the prodromal stage of dementia and should be carefully validated within the life space of users in terms of quality of life, social activities, and costs.              Discussion:                    The operational framework has the potential to produce ICT and services with high clinical impact but requires substantial investment."
26759786,22.0,Studying depression using imaging and machine learning methods,2015 Nov 10;10:115-23.,"Depression is a complex clinical entity that can pose challenges for clinicians regarding both accurate diagnosis and effective timely treatment. These challenges have prompted the development of multiple machine learning methods to help improve the management of this disease. These methods utilize anatomical and physiological data acquired from neuroimaging to create models that can identify depressed patients vs. non-depressed patients and predict treatment outcomes. This article (1) presents a background on depression, imaging, and machine learning methodologies; (2) reviews methodologies of past studies that have used imaging and machine learning to study depression; and (3) suggests directions for future depression-related studies."
26728066,3.0,Predicting Subcellular Localization of Proteins by Bioinformatic Algorithms,2017;404:129-158.,"When predicting the subcellular localization of proteins from their amino acid sequences, there are basically three approaches: signal-based, global property-based, and homology-based. Each of these has its advantages and drawbacks, and it is important when comparing methods to know which approach was used. Various statistical and machine learning algorithms are used with all three approaches, and various measures and standards are employed when reporting the performances of the developed methods. This chapter presents a number of available methods for prediction of sorting signals and subcellular localization, but rather than providing a checklist of which predictors to use, it aims to function as a guide for critical assessment of prediction methods."
26696900,10.0,Prediction of Druggable Proteins Using Machine Learning and Systems Biology: A Mini-Review,2015 Dec 8;6:366.,"The emergence of -omics technologies has allowed the collection of vast amounts of data on biological systems. Although, the pace of such collection has been exponential, the impact of these data remains small on many critical biomedical applications such as drug development. Limited resources, high costs, and low hit-to-lead ratio have led researchers to search for more cost effective methodologies. A possible alternative is to incorporate computational methods of potential drug target prediction early during drug discovery workflow. Computational methods based on systems approaches have the advantage of taking into account the global properties of a molecule not limited to its sequence, structure or function. Machine learning techniques are powerful tools that can extract relevant information from massive and noisy data sets. In recent years the scientific community has explored the combined power of these fields to propose increasingly accurate and low cost methods to propose interesting drug targets. In this mini-review, we describe promising approaches based on the simultaneous use of systems biology and machine learning to access gene and protein druggability. Moreover, we discuss the state-of-the-art of this emerging and interdisciplinary field, discussing data sources, algorithms and the performance of the different methodologies. Finally, we indicate interesting avenues of research and some remaining open challenges."
26690804,14.0,"""Look at my classifier's result"": Disentangling unresponsive from (minimally) conscious patients",2017 Jan 15;145(Pt B):288-303.,"Given the fact that clinical bedside examinations can have a high rate of misdiagnosis, machine learning techniques based on neuroimaging and electrophysiological measurements are increasingly being considered for comatose patients and patients with unresponsive wakefulness syndrome, a minimally conscious state or locked-in syndrome. Machine learning techniques have the potential to move from group-level statistical results to personalized predictions in a clinical setting. They have been applied for the purpose of (1) detecting changes in brain activation during functional tasks, equivalent to a behavioral command-following test and (2) estimating signs of consciousness by analyzing measurement data obtained from multiple subjects in resting state. In this review, we provide a comprehensive overview of the literature on both approaches and discuss the translation of present findings to clinical practice. We found that most studies struggle with the difficulty of establishing a reliable behavioral assessment and fluctuations in the patient's levels of arousal. Both these factors affect the training and validation of machine learning methods to a considerable degree. In studies involving more than 50 patients, small to moderate evidence was found for the presence of signs of consciousness or good outcome, where one study even showed strong evidence for good outcome."
26690135,20.0,Bioinformatics Mining and Modeling Methods for the Identification of Disease Mechanisms in Neurodegenerative Disorders,2015 Dec 7;16(12):29179-206.,"Since the decoding of the Human Genome, techniques from bioinformatics, statistics, and machine learning have been instrumental in uncovering patterns in increasing amounts and types of different data produced by technical profiling technologies applied to clinical samples, animal models, and cellular systems. Yet, progress on unravelling biological mechanisms, causally driving diseases, has been limited, in part due to the inherent complexity of biological systems. Whereas we have witnessed progress in the areas of cancer, cardiovascular and metabolic diseases, the area of neurodegenerative diseases has proved to be very challenging. This is in part because the aetiology of neurodegenerative diseases such as Alzheimer´s disease or Parkinson´s disease is unknown, rendering it very difficult to discern early causal events. Here we describe a panel of bioinformatics and modeling approaches that have recently been developed to identify candidate mechanisms of neurodegenerative diseases based on publicly available data and knowledge. We identify two complementary strategies-data mining techniques using genetic data as a starting point to be further enriched using other data-types, or alternatively to encode prior knowledge about disease mechanisms in a model based framework supporting reasoning and enrichment analysis. Our review illustrates the challenges entailed in integrating heterogeneous, multiscale and multimodal information in the area of neurology in general and neurodegeneration in particular. We conclude, that progress would be accelerated by increasing efforts on performing systematic collection of multiple data-types over time from each individual suffering from neurodegenerative disease. The work presented here has been driven by project AETIONOMY; a project funded in the course of the Innovative Medicines Initiative (IMI); which is a public-private partnership of the European Federation of Pharmaceutical Industry Associations (EFPIA) and the European Commission (EC)."
26689499,13.0,Providing data science support for systems pharmacology and its implications to drug discovery,2016;11(3):241-56.,"Introduction:                    The conventional one-drug-one-target-one-disease drug discovery process has been less successful in tracking multi-genic, multi-faceted complex diseases. Systems pharmacology has emerged as a new discipline to tackle the current challenges in drug discovery. The goal of systems pharmacology is to transform huge, heterogeneous, and dynamic biological and clinical data into interpretable and actionable mechanistic models for decision making in drug discovery and patient treatment. Thus, big data technology and data science will play an essential role in systems pharmacology.              Areas covered:                    This paper critically reviews the impact of three fundamental concepts of data science on systems pharmacology: similarity inference, overfitting avoidance, and disentangling causality from correlation. The authors then discuss recent advances and future directions in applying the three concepts of data science to drug discovery, with a focus on proteome-wide context-specific quantitative drug target deconvolution and personalized adverse drug reaction prediction.              Expert opinion:                    Data science will facilitate reducing the complexity of systems pharmacology modeling, detecting hidden correlations between complex data sets, and distinguishing causation from correlation. The power of data science can only be fully realized when integrated with mechanism-based multi-scale modeling that explicitly takes into account the hierarchical organization of biological systems from nucleic acid to proteins, to molecular interaction networks, to cells, to tissues, to patients, and to populations."
26677181,,Systems Medicine in Pharmaceutical Research and Development,2016;1386:87-104.,"The development of new drug therapies requires substantial and ever increasing investments from the pharmaceutical company. Ten years ago, the average time from early target identification and optimization until initial market authorization of a new drug compound took more than 10 years and involved costs in the order of one billion US dollars. Recent studies indicate even a significant growth of costs in the meanwhile, mainly driven by the increasing complexity of diseases addressed by pharmaceutical research.Modeling and simulation are proven approaches to handle highly complex systems; hence, systems medicine is expected to control the spiral of complexity of diseases and increasing costs. Today, the main focus of systems medicine applications in industry is on mechanistic modeling. Biological mechanisms are represented by explicit equations enabling insight into the cooperation of all relevant mechanisms. Mechanistic modeling is widely accepted in pharmacokinetics, but prediction from cell behavior to patients is rarely possible due to lacks in our understanding of the controlling mechanisms. Data-driven modeling aims to compensate these lacks by the use of advanced statistical and machine learning methods. Future progress in pharmaceutical research and development will require integrated hybrid modeling technologies allowing realization of the benefits of both mechanistic and data-driven modeling. In this chapter, we sketch typical industrial application areas for both modeling techniques and derive the requirements for future technology development."
26674745,12.0,Standardized data collection to build prediction models in oncology: a prototype for rectal cancer,2016 Jan;12(1):119-36.,"The advances in diagnostic and treatment technology are responsible for a remarkable transformation in the internal medicine concept with the establishment of a new idea of personalized medicine. Inter- and intra-patient tumor heterogeneity and the clinical outcome and/or treatment's toxicity's complexity, justify the effort to develop predictive models from decision support systems. However, the number of evaluated variables coming from multiple disciplines: oncology, computer science, bioinformatics, statistics, genomics, imaging, among others could be very large thus making traditional statistical analysis difficult to exploit. Automated data-mining processes and machine learning approaches can be a solution to organize the massive amount of data, trying to unravel important interaction. The purpose of this paper is to describe the strategy to collect and analyze data properly for decision support and introduce the concept of an 'umbrella protocol' within the framework of 'rapid learning healthcare'."
26651918,106.0,Machine Learning for High-Throughput Stress Phenotyping in Plants,2016 Feb;21(2):110-124.,"Advances in automated and high-throughput imaging technologies have resulted in a deluge of high-resolution images and sensor data of plants. However, extracting patterns and features from this large corpus of data requires the use of machine learning (ML) tools to enable data assimilation and feature identification for stress phenotyping. Four stages of the decision cycle in plant stress phenotyping and plant breeding activities where different ML approaches can be deployed are (i) identification, (ii) classification, (iii) quantification, and (iv) prediction (ICQP). We provide here a comprehensive overview and user-friendly taxonomy of ML tools to enable the plant community to correctly and easily apply the appropriate ML tools and best-practice guidelines for various biotic and abiotic stress traits."
26651007,7.0,Functional neuroimaging of psychotherapeutic processes in anxiety and depression: from mechanisms to predictions,2016 Jan;29(1):25-31.,"Purpose of review:                    The review provides an update of functional neuroimaging studies that identify neural processes underlying psychotherapy and predict outcomes following psychotherapeutic treatment in anxiety and depressive disorders. Following current developments in this field, studies were classified as 'mechanistic' or 'predictor' studies (i.e., informing neurobiological models about putative mechanisms versus aiming to provide predictive information).              Recent findings:                    Mechanistic evidence points toward a dual-process model of psychotherapy in anxiety disorders with abnormally increased limbic activation being decreased, while prefrontal activity is increased. Partly overlapping findings are reported for depression, albeit with a stronger focus on prefrontal activation following treatment. No studies directly comparing neural pathways of psychotherapy between anxiety and depression were detected. Consensus is accumulating for an overarching role of the anterior cingulate cortex in modulating treatment response across disorders. When aiming to quantify clinical utility, the need for single-subject predictions is increasingly recognized and predictions based on machine learning approaches show high translational potential.              Summary:                    Present findings encourage the search for predictors providing clinically meaningful information for single patients. However, independent validation as a crucial prerequisite for clinical use is still needed. Identifying nonresponders a priori creates the need for alternative treatment options that can be developed based on an improved understanding of those neural mechanisms underlying effective interventions."
26648850,21.0,Control Capabilities of Myoelectric Robotic Prostheses by Hand Amputees: A Scientific Research and Market Overview,2015 Nov 30;9:162.,"Hand amputation can dramatically affect the capabilities of a person. Cortical reorganization occurs in the brain, but the motor and somatosensorial cortex can interact with the remnant muscles of the missing hand even many years after the amputation, leading to the possibility to restore the capabilities of hand amputees through myoelectric prostheses. Myoelectric hand prostheses with many degrees of freedom are commercially available and recent advances in rehabilitation robotics suggest that their natural control can be performed in real life. The first commercial products exploiting pattern recognition to recognize the movements have recently been released, however the most common control systems are still usually unnatural and must be learned through long training. Dexterous and naturally controlled robotic prostheses can become reality in the everyday life of amputees but the path still requires many steps. This mini-review aims to improve the situation by giving an overview of the advancements in the commercial and scientific domains in order to outline the current and future chances in this field and to foster the integration between market and scientific research."
26640765,9.0,"Multivariate analyses applied to fetal, neonatal and pediatric MRI of neurodevelopmental disorders",2015 Oct 3;9:532-44.,"Multivariate analysis (MVA) is a class of statistical and pattern recognition methods that involve the processing of data that contains multiple measurements per sample. MVA can be used to address a wide variety of medical neuroimaging-related challenges including identifying variables associated with a measure of clinical importance (i.e. patient outcome), creating diagnostic tests, assisting in characterizing developmental disorders, understanding disease etiology, development and progression, assisting in treatment monitoring and much more. Compared to adults, imaging of developing immature brains has attracted less attention from MVA researchers. However, remarkable MVA research growth has occurred in recent years. This paper presents the results of a systematic review of the literature focusing on MVA technologies applied to neurodevelopmental disorders in fetal, neonatal and pediatric magnetic resonance imaging (MRI) of the brain. The goal of this manuscript is to provide a concise review of the state of the scientific literature on studies employing brain MRI and MVA in a pre-adult population. Neurological developmental disorders addressed in the MVA research contained in this review include autism spectrum disorder, attention deficit hyperactivity disorder, epilepsy, schizophrenia and more. While the results of this review demonstrate considerable interest from the scientific community in applications of MVA technologies in pediatric/neonatal/fetal brain MRI, the field is still young and considerable research growth remains ahead of us."
