pmid,citations,title,date,text
31383477,13.0,Data-driven modeling and prediction of blood glucose dynamics: Machine learning applications in type 1 diabetes,2019 Jul;98:109-134.,"Background:                    Diabetes mellitus (DM) is a metabolic disorder that causes abnormal blood glucose (BG) regulation that might result in short and long-term health complications and even death if not properly managed. Currently, there is no cure for diabetes. However, self-management of the disease, especially keeping BG in the recommended range, is central to the treatment. This includes actively tracking BG levels and managing physical activity, diet, and insulin intake. The recent advancements in diabetes technologies and self-management applications have made it easier for patients to have more access to relevant data. In this regard, the development of an artificial pancreas (a closed-loop system), personalized decision systems, and BG event alarms are becoming more apparent than ever. Techniques such as predicting BG (modeling of a personalized profile), and modeling BG dynamics are central to the development of these diabetes management technologies. The increased availability of sufficient patient historical data has paved the way for the introduction of machine learning and its application for intelligent and improved systems for diabetes management. The capability of machine learning to solve complex tasks with dynamic environment and knowledge has contributed to its success in diabetes research.              Motivation:                    Recently, machine learning and data mining have become popular, with their expanding application in diabetes research and within BG prediction services in particular. Despite the increasing and expanding popularity of machine learning applications in BG prediction services, updated reviews that map and materialize the current trends in modeling options and strategies are lacking within the context of BG prediction (modeling of personalized profile) in type 1 diabetes.              Objective:                    The objective of this review is to develop a compact guide regarding modeling options and strategies of machine learning and a hybrid system focusing on the prediction of BG dynamics in type 1 diabetes. The review covers machine learning approaches pertinent to the controller of an artificial pancreas (closed-loop systems), modeling of personalized profiles, personalized decision support systems, and BG alarm event applications. Generally, the review will identify, assess, analyze, and discuss the current trends of machine learning applications within these contexts.              Method:                    A rigorous literature review was conducted between August 2017 and February 2018 through various online databases, including Google Scholar, PubMed, ScienceDirect, and others. Additionally, peer-reviewed journals and articles were considered. Relevant studies were first identified by reviewing the title, keywords, and abstracts as preliminary filters with our selection criteria, and then we reviewed the full texts of the articles that were found relevant. Information from the selected literature was extracted based on predefined categories, which were based on previous research and further elaborated through brainstorming among the authors.              Results:                    The initial search was done by analyzing the title, abstract, and keywords. A total of 624 papers were retrieved from DBLP Computer Science (25), Diabetes Technology and Therapeutics (31), Google Scholar (193), IEEE (267), Journal of Diabetes Science and Technology (31), PubMed/Medline (27), and ScienceDirect (50). After removing duplicates from the list, 417 records remained. Then, we independently assessed and screened the articles based on the inclusion and exclusion criteria, which eliminated another 204 papers, leaving 213 relevant papers. After a full-text assessment, 55 articles were left, which were critically analyzed. The inter-rater agreement was measured using a Cohen Kappa test, and disagreements were resolved through discussion.              Conclusion:                    Due to the complexity of BG dynamics, it remains difficult to achieve a universal model that produces an accurate prediction in every circumstance (i.e., hypo/eu/hyperglycemia events). Recently, machine learning techniques have received wider attention and increased popularity in diabetes research in general and BG prediction in particular, coupled with the ever-growing availability of a self-collected health data. The state-of-the-art demonstrates that various machine learning techniques have been tested to predict BG, such as recurrent neural networks, feed-forward neural networks, support vector machines, self-organizing maps, the Gaussian process, genetic algorithm and programs, deep neural networks, and others, using various group of input parameters and training algorithms. The main limitation of the current approaches is the lack of a well-defined approach to estimate carbohydrate intake, which is mainly done manually by individual users and is prone to an error that can severely affect the predictive performance. Moreover, a universal approach has not been established to estimate and quantify the approximate effect of physical activities, stress, and infections on the BG level. No researchers have assessed model predictive performance during stress and infection incidences in a free-living condition, which should be considered in future studies. Furthermore, a little has been done regarding model portability that can capture inter- and intra-variability among patients. It seems that the effect of time lags between the CGM readings and the actual BG levels is not well covered. However, in general, we foresee that these developments might foster the advancement of next-generation BG prediction algorithms, which will make a great contribution in the effort to develop the long-awaited, so-called artificial pancreas (a closed-loop system)."
31383376,6.0,Artificial Intelligence for Drug Toxicity and Safety,2019 Sep;40(9):624-635.,"Interventional pharmacology is one of medicine's most potent weapons against disease. These drugs, however, can result in damaging side effects and must be closely monitored. Pharmacovigilance is the field of science that monitors, detects, and prevents adverse drug reactions (ADRs). Safety efforts begin during the development process, using in vivo and in vitro studies, continue through clinical trials, and extend to postmarketing surveillance of ADRs in real-world populations. Future toxicity and safety challenges, including increased polypharmacy and patient diversity, stress the limits of these traditional tools. Massive amounts of newly available data present an opportunity for using artificial intelligence (AI) and machine learning to improve drug safety science. Here, we explore recent advances as applied to preclinical drug safety and postmarketing surveillance with a specific focus on machine and deep learning (DL) approaches."
33733101,,Artificial Intelligence Based Approaches to Identify Molecular Determinants of Exceptional Health and Life Span-An Interdisciplinary Workshop at the National Institute on Aging,2019 Aug 6;2:12.,"Artificial intelligence (AI) has emerged as a powerful approach for integrated analysis of the rapidly growing volume of multi-omics data, including many research and clinical tasks such as prediction of disease risk and identification of potential therapeutic targets. However, the potential for AI to facilitate the identification of factors contributing to human exceptional health and life span and their translation into novel interventions for enhancing health and life span has not yet been realized. As researchers on aging acquire large scale data both in human cohorts and model organisms, emerging opportunities exist for the application of AI approaches to untangle the complex physiologic process(es) that modulate health and life span. It is expected that efficient and novel data mining tools that could unravel molecular mechanisms and causal pathways associated with exceptional health and life span could accelerate the discovery of novel therapeutics for healthy aging. Keeping this in mind, the National Institute on Aging (NIA) convened an interdisciplinary workshop titled ""Contributions of Artificial Intelligence to Research on Determinants and Modulation of Health Span and Life Span"" in August 2018. The workshop involved experts in the fields of aging, comparative biology, cardiology, cancer, and computational science/AI who brainstormed ideas on how AI can be leveraged for the analyses of large-scale data sets from human epidemiological studies and animal/model organisms to close the current knowledge gaps in processes that drive exceptional life and health span. This report summarizes the discussions and recommendations from the workshop on future application of AI approaches to advance our understanding of human health and life span."
31377227,10.0,"Deep learning in drug discovery: opportunities, challenges and future prospects",2019 Oct;24(10):2017-2032.,"Artificial Intelligence (AI) is an area of computer science that simulates the structures and operating principles of the human brain. Machine learning (ML) belongs to the area of AI and endeavors to develop models from exposure to training data. Deep Learning (DL) is another subset of AI, where models represent geometric transformations over many different layers. This technology has shown tremendous potential in areas such as computer vision, speech recognition and natural language processing. More recently, DL has also been successfully applied in drug discovery. Here, I analyze several relevant DL applications and case studies, providing a detailed view of the current state-of-the-art in drug discovery and highlighting not only the problematic issues, but also the successes and opportunities for further advances."
31374225,8.0,Machine learning and data mining frameworks for predicting drug response in cancer: An overview and a novel in silico screening process based on association rule mining,2019 Nov;203:107395.,"A major challenge in cancer treatment is predicting the clinical response to anti-cancer drugs on a personalized basis. The success of such a task largely depends on the ability to develop computational resources that integrate big ""omic"" data into effective drug-response models. Machine learning is both an expanding and an evolving computational field that holds promise to cover such needs. Here we provide a focused overview of: 1) the various supervised and unsupervised algorithms used specifically in drug response prediction applications, 2) the strategies employed to develop these algorithms into applicable models, 3) data resources that are fed into these frameworks and 4) pitfalls and challenges to maximize model performance. In this context we also describe a novel in silico screening process, based on Association Rule Mining, for identifying genes as candidate drivers of drug response and compare it with relevant data mining frameworks, for which we generated a web application freely available at: https://compbio.nyumc.org/drugs/. This pipeline explores with high efficiency large sample-spaces, while is able to detect low frequency events and evaluate statistical significance even in the multidimensional space, presenting the results in the form of easily interpretable rules. We conclude with future prospects and challenges of applying machine learning based drug response prediction in precision medicine."
31372505,28.0,Artificial intelligence and machine learning in clinical development: a translational perspective,2019 Jul 26;2:69.,"Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, computing power to identify clinically meaningful patterns in the data using efficient artificial intelligence and machine-learning algorithms, and regulators embracing this change through new collaborations. This perspective summarizes insights, recent developments, and recommendations for infusing actionable computational evidence into clinical development and health care from academy, biotechnology industry, nonprofit foundations, regulators, and technology corporations. Analysis and learning from publically available biomedical and clinical trial data sets, real-world evidence from sensors, and health records by machine-learning architectures are discussed. Strategies for modernizing the clinical development process by integration of AI- and ML-based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration are outlined. We conclude by discussing applications and impact of digital algorithmic evidence to improve medical care for patients."
31371969,3.0,Toward a personalized therapy for panic disorder: preliminary considerations from a work in progress,2019 Jul 11;15:1957-1970.,"Although several treatment options for panic disorder (PD) are available, the best intervention for each individual patient remains uncertain and the use of a more personalized therapeutic approach in PD is required. In clinical practice, clinicians combine general scientific information and personal experience in the decision-making process to choose a tailored treatment for each patient. In this sense, clinicians already use a somehow personalized medicine strategy. However, the influence of their interpretative personal models may lead to bias related to personal convictions, not sufficiently grounded on scientific evidence. Hence, an effort to give some advice based on the science of personalized medicine could have positive effects on clinicians' decisions. Based on a narrative review of meta-analyses, systematic reviews, and experimental studies, we proposed a first-step attempt of evidence-based personalized therapy for PD. We focused on some phenomenological profiles, encompassing symptoms during/outside panic attacks, related patterns of physiological functions, and some aspects of physical health, which might be worth considering when developing treatment plans for patients with PD. We considered respiratory, cardiac, vestibular, and derealization/depersonalization profiles, with related implications for treatment. Given the extensiveness of the topic, we considered only medications and some somatic interventions. Our proposal should be considered neither exhaustive nor conclusive, as it is meant as a very preliminary step toward a future, robust evidence-based personalized therapy for PD. Clearly much more work is needed to achieve this goal, and recent technological advances, such as wearable devices, big data platforms, and the application of machine learning techniques, may help obtain reliable findings. We believe that combining the efforts of different research groups in this work in progress can lead to largely shared conclusions in the near future."
31371027,9.0,Machine learning and glioma imaging biomarkers,2020 Jan;75(1):20-32.,"Aim:                    To review how machine learning (ML) is applied to imaging biomarkers in neuro-oncology, in particular for diagnosis, prognosis, and treatment response monitoring.              Materials and methods:                    The PubMed and MEDLINE databases were searched for articles published before September 2018 using relevant search terms. The search strategy focused on articles applying ML to high-grade glioma biomarkers for treatment response monitoring, prognosis, and prediction.              Results:                    Magnetic resonance imaging (MRI) is typically used throughout the patient pathway because routine structural imaging provides detailed anatomical and pathological information and advanced techniques provide additional physiological detail. Using carefully chosen image features, ML is frequently used to allow accurate classification in a variety of scenarios. Rather than being chosen by human selection, ML also enables image features to be identified by an algorithm. Much research is applied to determining molecular profiles, histological tumour grade, and prognosis using MRI images acquired at the time that patients first present with a brain tumour. Differentiating a treatment response from a post-treatment-related effect using imaging is clinically important and also an area of active study (described here in one of two Special Issue publications dedicated to the application of ML in glioma imaging).              Conclusion:                    Although pioneering, most of the evidence is of a low level, having been obtained retrospectively and in single centres. Studies applying ML to build neuro-oncology monitoring biomarker models have yet to show an overall advantage over those using traditional statistical methods. Development and validation of ML models applied to neuro-oncology require large, well-annotated datasets, and therefore multidisciplinary and multi-centre collaborations are necessary."
31366471,11.0,Artificial intelligence for microscopy: what you should know,2019 Aug 30;47(4):1029-1040.,"Artificial Intelligence based on Deep Learning (DL) is opening new horizons in biomedical research and promises to revolutionize the microscopy field. It is now transitioning from the hands of experts in computer sciences to biomedical researchers. Here, we introduce recent developments in DL applied to microscopy, in a manner accessible to non-experts. We give an overview of its concepts, capabilities and limitations, presenting applications in image segmentation, classification and restoration. We discuss how DL shows an outstanding potential to push the limits of microscopy, enhancing resolution, signal and information content in acquired data. Its pitfalls are discussed, along with the future directions expected in this field."
31366110,,Antibiotic-Resistant Septicemia in Pediatric Oncology Patients Associated with Post-Therapeutic Neutropenic Fever,2019 Jul 30;8(3):106.,"Death in cancer patients can be caused by the progression of tumors, their malignity, or other associated conditions such as sepsis, which is a multiphasic host response to a pathogen that can be significantly amplified by endogenous factors. Its incidence is continuously rising, which reflects the increasing number of sick patients at a higher risk of infection, especially those that are elderly, pediatric, or immunosuppressed. Sepsis appears to be directly associated with oncological treatment and fatal septic shock. Patients with a cancer diagnosis face a much higher risk of infections after being immunosuppressed by chemotherapy, radiotherapy, or anti-inflammatory therapy, especially caused by non-pathogenic, Gram-negative, and multidrug-resistant pathogens. There is a notorious difference between the incidence and mortality rates related to sepsis in pediatric oncologic patients between developed and developing countries: they are much higher in developing countries, where investment for diagnosis and treatment resources, infrastructure, medical specialists, cancer-related control programs, and post-therapeutic care is insufficient. This situation not only limits but also reduces the life expectancy of treated pediatric oncologic patients, and demands higher costs from the healthcare systems. Therefore, efforts must aim to limit the progression of sepsis conditions, applying the most recommended therapeutic regimens as soon as the initial risk factors are clinically evident-or even before they are, as when taking advantage of machine learning prediction systems to analyze data."
31362904,,"If machines can learn, who needs scientists?",2019 Sep;306:162-166.,"Machine learning has been used in NMR in for decades, but recent developments signal explosive growth is on the horizon. An obstacle to the application of machine learning in NMR is the relative paucity of available training data, despite the existence of numerous public NMR data repositories. Other challenges include the problem of interpreting the results of a machine learning algorithm, and incorporating machine learning into hypothesis-driven research. This perspective imagines the potential of machine learning in NMR and speculates on possible approaches to the hurdles."
31362571,5.0,Probing the 3D architecture of the plant nucleus with microscopy approaches: challenges and solutions,2019 Dec;10(1):181-212.,"The eukaryotic cell nucleus is a central organelle whose architecture determines genome function at multiple levels. Deciphering nuclear organizing principles influencing cellular responses and identity is a timely challenge. Despite many similarities between plant and animal nuclei, plant nuclei present intriguing specificities. Complementary to molecular and biochemical approaches, 3D microscopy is indispensable for resolving nuclear architecture. However, novel solutions are required for capturing cell-specific, sub-nuclear and dynamic processes. We provide a pointer for utilising high-to-super-resolution microscopy and image processing to probe plant nuclear architecture in 3D at the best possible spatial and temporal resolution and at quantitative and cell-specific levels. High-end imaging and image-processing solutions allow the community now to transcend conventional practices and benefit from continuously improving approaches. These promise to deliver a comprehensive, 3D view of plant nuclear architecture and to capture spatial dynamics of the nuclear compartment in relation to cellular states and responses. Abbreviations: 3D and 4D: Three and Four dimensional; AI: Artificial Intelligence; ant: antipodal nuclei (ant); CLSM: Confocal Laser Scanning Microscopy; CTs: Chromosome Territories; DL: Deep Learning; DLIm: Dynamic Live Imaging; ecn: egg nucleus; FACS: Fluorescence-Activated Cell Sorting; FISH: Fluorescent In Situ Hybridization; FP: Fluorescent Proteins (GFP, RFP, CFP, YFP, mCherry); FRAP: Fluorescence Recovery After Photobleaching; GPU: Graphics Processing Unit; KEEs: KNOT Engaged Elements; INTACT: Isolation of Nuclei TAgged in specific Cell Types; LADs: Lamin-Associated Domains; ML: Machine Learning; NA: Numerical Aperture; NADs: Nucleolar Associated Domains; PALM: Photo-Activated Localization Microscopy; Pixel: Picture element; pn: polar nuclei; PSF: Point Spread Function; RHF: Relative Heterochromatin Fraction; SIM: Structured Illumination Microscopy; SLIm: Static Live Imaging; SMC: Spore Mother Cell; SNR: Signal to Noise Ratio; SRM: Super-Resolution Microscopy; STED: STimulated Emission Depletion; STORM: STochastic Optical Reconstruction Microscopy; syn: synergid nuclei; TADs: Topologically Associating Domains; Voxel: Volumetric pixel."
31362251,6.0,The effect of short-term exposure to the natural environment on depressive mood: A systematic review and meta-analysis,2019 Oct;177:108606.,"Background:                    Research suggests that exposure to the natural environment can improve mood, however, current reviews are limited in scope and there is little understanding of moderators.              Objective:                    To conduct a comprehensive systematic review and meta-analysis of the evidence for the effect of short-term exposure to the natural environment on depressive mood.              Methods:                    Five databases were systematically searched for relevant studies published up to March 2018. Risk of bias was evaluated using the Cochrane Risk of Bias (ROB) tool 1.0 and the Risk of Bias in Non-Randomised Studies of Interventions (ROBINS-I) tool where appropriate. The Grades of Recommendation, Assessment, Development, and Evaluation (GRADE) approach was used to assess the quality of evidence overall. A random-effects meta-analysis was performed. 20 potential moderators of the effect size were coded and the machine learning-based MetaForest algorithm was used to identify relevant moderators. These were then entered into a meta-regression.              Results:                    33 studies met the inclusion criteria. Effect sizes ranged from -2.30 to 0.84, with an unweighted mean effect size of Mg=-0.29,SD=0.60. However, there was significant residual heterogeneity between studies and risk of bias was high. Type of natural environment, type of built environment, gender mix of the sample, and region of study origin, among others, were identified as relevant moderators but were not significant when entered in a meta-regression. The quality of evidence was rated very low to low. An assessment of publication bias was inconclusive.              Conclusions:                    A small effect was found for reduction in depressive mood following exposure to the natural environment. However, the high risk of bias and low quality of studies limits confidence in the results. The variation in effect size also remains largely unexplained. It is recommended that future studies make use of reporting guidelines and aim to reduce the potential for bias where possible."
31355445,20.0,"Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association",2019 Nov;249(3):286-294.,"In this white paper, experts from the Digital Pathology Association (DPA) define terminology and concepts in the emerging field of computational pathology, with a focus on its application to histology images analyzed together with their associated patient data to extract information. This review offers a historical perspective and describes the potential clinical benefits from research and applications in this field, as well as significant obstacles to adoption. Best practices for implementing computational pathology workflows are presented. These include infrastructure considerations, acquisition of training data, quality assessments, as well as regulatory, ethical, and cyber-security concerns. Recommendations are provided for regulators, vendors, and computational pathology practitioners in order to facilitate progress in the field. © 2019 The Authors. The Journal of Pathology published by John Wiley & Sons Ltd on behalf of Pathological Society of Great Britain and Ireland."
31354718,4.0,Computational Methodologies for the in vitro and in situ Quantification of Neutrophil Extracellular Traps,2019 Jul 10;10:1562.,"Neutrophil extracellular traps (NETs) are a neutrophil defensive mechanism where chromatin is expelled together with antimicrobial proteins in response to a number of stimuli. Even though beneficial in many cases, their dysfunction has been implicated in many diseases, such as rheumatoid arthritis and cancer. Accurate quantification of NETs is of utmost importance for correctly studying their role in various diseases, especially when considering them as therapeutic targets. Unfortunately, NET quantification has a number of limitations. However, recent developments in computational methodologies for quantifying NETs have vastly improved the ability to study NETs. Methods range from using ImageJ to user friendly applications and to more sophisticated machine-learning approaches. These various methods are reviewed and discussed in this review."
31354276,5.0,Machine learning techniques in a structural and functional MRI diagnostic approach in schizophrenia: a systematic review,2019 Jun 19;15:1605-1627.,"Background:                    Diagnosis of schizophrenia (SCZ) is made exclusively clinically, since specific biomarkers that can predict the disease accurately remain unknown. Machine learning (ML) represents a promising approach that could support clinicians in the diagnosis of mental disorders.              Objectives:                    A systematic review, according to the PRISMA statement, was conducted to evaluate its accuracy to distinguish SCZ patients from healthy controls.              Methods:                    We systematically searched PubMed, Embase, MEDLINE, PsychINFO and the Cochrane Library through December 2018 using generic terms for ML techniques and SCZ without language or time restriction. Thirty-five studies were included in this review: eight of them used structural neuroimaging, twenty-six used functional neuroimaging and one both, with a minimum accuracy >60% (most of them 75-90%). Sensitivity, Specificity and accuracy were extracted from each publication or obtained directly from authors.              Results:                    Support vector machine, the most frequent technique, if associated with other ML techniques achieved accuracy close to 100%. The prefrontal and temporal cortices appeared to be the most useful brain regions for the diagnosis of SCZ. ML analysis can efficiently detect significantly altered brain connectivity in patients with SCZ (eg, default mode network, visual network, sensorimotor network, frontoparietal network and salience network).              Conclusion:                    The greater accuracy demonstrated by these predictive models and the new models resulting from the integration of multiple ML techniques will be increasingly decisive for early diagnosis and evaluation of the treatment response and to establish the prognosis of patients with SCZ. To achieve a real benefit for patients, the future challenge is to reach an accurate diagnosis not only through clinical evaluation but also with the aid of ML algorithms."
31353422,,Artificial Intelligence in Reproductive Urology,2019 Jul 29;20(9):52.,"Purpose of review:                    The promise of artificial intelligence (AI) in medicine has been widely theorized over the past couple of decades. It has only been with technological advances over the past few years that physicians and computer scientists have started discovering its true clinical potential. Reproductive urology is a sub-discipline that AI could be of great contribution, as current predictive models and subjectivity within the field have several limitations. We review the literature to summarize recent AI applications in reproductive urology.              Recent findings:                    Early AI applications in reproductive urology focused on predicting semen parameters based on questionnaires that identify potential environmental factors and/or lifestyle habits impacting male fertility. AI has shown success in predicting the patient subpopulation most likely to need a genetic workup for azoospermia. With recent advances in image processing, automated sperm detection is a reality. Semen analyses, once a laboratory-only diagnostic test, have moved into health consumer homes with the advent of AI. AI's prospects in medicine are considerable and there is strong potential for AI within reproductive urology. Research in identifying the factors that can affect reproductive success either naturally or with assisted reproduction is of paramount importance to move the field forward."
31352462,1.0,New Technologies for Outcome Measures in Retinal Disease: Review from the European Vision Institute Special Interest Focus Group,2020;63(2):77-87.,"Novel diagnostic tools to measure retinal function and structure are rapidly being developed and introduced into clinical use. Opportunities exist to use these informative and robust measures as endpoints for clinical trials to determine efficacy and to monitor safety of therapeutic interventions. In order to inform researchers and clinician-scientists about these new diagnostic tools, a workshop was organized by the European Vision Institute. Invited speakers highlighted the recent advances in state-of-the-art technologies for outcome measures in the field of retina. This review highlights the workshop's presentations in the context of published literature."
31351213,5.0,Artificial intelligence for assisting diagnostics and assessment of Parkinson's disease-A review,2019 Sep;184:105442.,"Artificial intelligence, specifically machine learning, has found numerous applications in computer-aided diagnostics, monitoring and management of neurodegenerative movement disorders of parkinsonian type. These tasks are not trivial due to high inter-subject variability and similarity of clinical presentations of different neurodegenerative disorders in the early stages. This paper aims to give a comprehensive, high-level overview of applications of artificial intelligence through machine learning algorithms in kinematic analysis of movement disorders, specifically Parkinson's disease (PD). We surveyed papers published between January 2007 and January 2019, within online databases, including PubMed and Science Direct, with a focus on the most recently published studies. The search encompassed papers dealing with the implementation of machine learning algorithms for diagnosis and assessment of PD using data describing motion of upper and lower extremities. This systematic review presents an overview of 48 relevant studies published in the abovementioned period, which investigate the use of artificial intelligence for diagnostics, therapy assessment and progress prediction in PD based on body kinematics. Different machine learning algorithms showed promising results, particularly for early PD diagnostics. The investigated publications demonstrated the potentials of collecting data from affordable and globally available devices. However, to fully exploit artificial intelligence technologies in the future, more widespread collaboration is advised among medical institutions, clinicians and researchers, to facilitate aligning of data collection protocols, sharing and merging of data sets."
31350037,5.0,Technology approaches to digital health literacy,2019 Oct 15;293:294-296.,"Digital health literacy is an extension of health literacy and uses the same operational definition, but in the context of technology. Technology solutions have the potential to both promote health literacy or be a barrier. To be effective, health technology solutions should go beyond building literacy and numeracy skills to functional and critical skills, such as navigating the healthcare system, communication with healthcare providers, and shared decision making. New and emerging technologies are highlighted: AI/machine learning, voice first, remote patient monitoring, wearables, and apps and web sites. Health technology represents enormous promise in the building of digital health literacy skills and improved health outcomes in patients with cardiovascular and other chronic conditions. This is a promise, however, that is yet to be fulfilled. TOPICS: Hypertension, Rehabilitation, Metabolic Syndrome, Health Policy, Risk Factor."
31349617,3.0,Computer Methods for Automatic Locomotion and Gesture Tracking in Mice and Small Animals for Neuroscience Applications: A Survey,2019 Jul 25;19(15):3274.,"Neuroscience has traditionally relied on manually observing laboratory animals in controlled environments. Researchers usually record animals behaving freely or in a restrained manner and then annotate the data manually. The manual annotation is not desirable for three reasons; (i) it is time-consuming, (ii) it is prone to human errors, and (iii) no two human annotators will 100% agree on annotation, therefore, it is not reproducible. Consequently, automated annotation for such data has gained traction because it is efficient and replicable. Usually, the automatic annotation of neuroscience data relies on computer vision and machine learning techniques. In this article, we have covered most of the approaches taken by researchers for locomotion and gesture tracking of specific laboratory animals, i.e. rodents. We have divided these papers into categories based upon the hardware they use and the software approach they take. We have also summarized their strengths and weaknesses."
31348869,1.0,Artificial Intelligence in Drug Treatment,2020 Jan 6;60:353-369.,"The most common applications of artificial intelligence (AI) in drug treatment have to do with matching patients to their optimal drug or combination of drugs, predicting drug-target or drug-drug interactions, and optimizing treatment protocols. This review outlines some of the recently developed AI methods aiding the drug treatment and administration process. Selection of the best drug(s) for a patient typically requires the integration of patient data, such as genetics or proteomics, with drug data, like compound chemical descriptors, to score the therapeutic efficacy of drugs. The prediction of drug interactions often relies on similarity metrics, assuming that drugs with similar structures or targets will have comparable behavior or may interfere with each other. Optimizing the dosage schedule for administration of drugs is performed using mathematical models to interpret pharmacokinetic and pharmacodynamic data. The recently developed and powerful models for each of these tasks are addressed, explained, and analyzed here."
31347226,7.0,Magnetic resonance fingerprinting review part 2: Technique and directions,2020 Apr;51(4):993-1007.,"Magnetic resonance fingerprinting (MRF) is a general framework to quantify multiple MR-sensitive tissue properties with a single acquisition. There have been numerous advances in MRF in the years since its inception. In this work we highlight some of the recent technical developments in MRF, focusing on sequence optimization, modifications for reconstruction and pattern matching, new methods for partial volume analysis, and applications of machine and deep learning. Level of Evidence: 2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:993-1007."
31344945,,Small Genomes and Big Data: Adaptation of Plastid Genomics to the High-Throughput Era,2019 Jul 24;9(8):299.,"Plastid genome sequences are becoming more readily available with the increase in high-throughput sequencing, and whole-organelle genetic data is available for algae and plants from across the diversity of photosynthetic eukaryotes. This has provided incredible opportunities for studying species which may not be amenable to in vivo study or genetic manipulation or may not yet have been cultured. Research into plastid genomes has pushed the limits of what can be deduced from genomic information, and in particular genomic information obtained from public databases. In this Review, we discuss how research into plastid genomes has benefitted enormously from the explosion of publicly available genome sequence. We describe two case studies in how using publicly available gene data has supported previously held hypotheses about plastid traits from lineage-restricted experiments across algal and plant diversity. We propose how this approach could be used across disciplines for inferring functional and biological characteristics from genomic approaches, including integration of new computational and bioinformatic approaches such as machine learning. We argue that the techniques developed to gain the maximum possible insight from plastid genomes can be applied across the eukaryotic tree of life."
31344655,2.0,Engineering approaches for characterizing soft tissue mechanical properties: A review,2019 Oct;69:127-140.,"From cancer diagnosis to detailed characterization of arterial wall biomechanics, the elastic property of tissues is widely studied as an early sign of disease onset. The fibrous structural features of tissues are a direct measure of its health and functionality. Alterations in the structural features of tissues are often manifested as local stiffening and are early signs for diagnosing a disease. These elastic properties are measured ex vivo in conventional mechanical testing regimes, however, the heterogeneous microstructure of tissues can be accurately resolved over relatively smaller length scales with enhanced spatial resolution using techniques such as micro-indentation, microelectromechanical (MEMS) based cantilever sensors and optical catheters which also facilitate in vivo assessment of mechanical properties. In this review, we describe several probing strategies (qualitative and quantitative) based on the spatial scale of mechanical assessment and also discuss the potential use of machine learning techniques to compute the mechanical properties of soft tissues. This work details state of the art advancement in probing strategies, associated challenges toward quantitative characterization of tissue biomechanics both from an engineering and clinical standpoint."
31344297,2.0,Quantitation of Femtomolar-Level Protein Biomarkers Using a Simple Microbubbling Digital Assay and Bright-Field Smartphone Imaging,2019 Sep 23;58(39):13922-13928.,"Quantitating ultra-low concentrations of protein biomarkers is critical for early disease diagnosis and treatment. However, most current point-of-care (POC) assays are limited in sensitivity. Herein, we introduce an ultra-sensitive and facile microbubbling assay for the quantification of protein biomarkers with a digital-readout method that requires only a smartphone camera. We used machine learning to develop a smartphone application for automated image analysis to facilitate accurate and robust counting. Using this method, post-prostatectomy surveillance of prostate specific antigen (PSA) can be achieved with a detection limit (LOD) of 2.1 fm (0.060 pg mL-1 ), and early pregnancy detection using βhCG can be achieved with a of 0.034 mIU mL-1 (2.84 pg mL-1 ). This work provides the proof-of-principle of the microbubbling assay with a digital readout as an ultra-sensitive technology with minimal requirement for power and accessories, facilitating future POC applications."
31344143,6.0,Bone age assessment with various machine learning techniques: A systematic literature review and meta-analysis,2019 Jul 25;14(7):e0220242.,"Background:                    The assessment of bone age and skeletal maturity and its comparison to chronological age is an important task in the medical environment for the diagnosis of pediatric endocrinology, orthodontics and orthopedic disorders, and legal environment in what concerns if an individual is a minor or not when there is a lack of documents. Being a time-consuming activity that can be prone to inter- and intra-rater variability, the use of methods which can automate it, like Machine Learning techniques, is of value.              Objective:                    The goal of this paper is to present the state of the art evidence, trends and gaps in the research related to bone age assessment studies that make use of Machine Learning techniques.              Method:                    A systematic literature review was carried out, starting with the writing of the protocol, followed by searches on three databases: Pubmed, Scopus and Web of Science to identify the relevant evidence related to bone age assessment using Machine Learning techniques. One round of backward snowballing was performed to find additional studies. A quality assessment was performed on the selected studies to check for bias and low quality studies, which were removed. Data was extracted from the included studies to build summary tables. Lastly, a meta-analysis was performed on the performances of the selected studies.              Results:                    26 studies constituted the final set of included studies. Most of them proposed automatic systems for bone age assessment and investigated methods for bone age assessment based on hand and wrist radiographs. The samples used in the studies were mostly comprehensive or bordered the age of 18, and the data origin was in most of cases from United States and West Europe. Few studies explored ethnic differences.              Conclusions:                    There is a clear focus of the research on bone age assessment methods based on radiographs whilst other types of medical imaging without radiation exposure (e.g. magnetic resonance imaging) are not much explored in the literature. Also, socioeconomic and other aspects that could influence in bone age were not addressed in the literature. Finally, studies that make use of more than one region of interest for bone age assessment are scarce."
31343790,9.0,Artificial intelligence in the interpretation of breast cancer on MRI,2020 May;51(5):1310-1324.,"Advances in both imaging and computers have led to the rise in the potential use of artificial intelligence (AI) in various tasks in breast imaging, going beyond the current use in computer-aided detection to include diagnosis, prognosis, response to therapy, and risk assessment. The automated capabilities of AI offer the potential to enhance the diagnostic expertise of clinicians, including accurate demarcation of tumor volume, extraction of characteristic cancer phenotypes, translation of tumoral phenotype features to clinical genotype implications, and risk prediction. The combination of image-specific findings with the underlying genomic, pathologic, and clinical features is becoming of increasing value in breast cancer. The concurrent emergence of newer imaging techniques has provided radiologists with greater diagnostic tools and image datasets to analyze and interpret. Integrating an AI-based workflow within breast imaging enables the integration of multiple data streams into powerful multidisciplinary applications that may lead the path to personalized patient-specific medicine. In this article we describe the goals of AI in breast cancer imaging, in particular MRI, and review the literature as it relates to the current application, potential, and limitations in breast cancer. Level of Evidence: 3 Technical Efficacy: Stage 3 J. Magn. Reson. Imaging 2020;51:1310-1324."
31341467,,Speech Technology Progress Based on New Machine Learning Paradigm,2019 Jun 25;2019:4368036.,"Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing."
31340696,10.0,Results of the European Group for the Study of Resistant Depression (GSRD) - basis for further research and clinical practice,2019 Jul;20(6):427-448.,"Objectives: The overview outlines two decades of research from the European Group for the Study of Resistant Depression (GSRD) that fundamentally impacted evidence-based algorithms for diagnostics and psychopharmacotherapy of treatment-resistant depression (TRD). Methods: The GSRD staging model characterising response, non-response and resistance to antidepressant (AD) treatment was applied to 2762 patients in eight European countries. Results: In case of non-response, dose escalation and switching between different AD classes did not show superiority over continuation of original AD treatment. Predictors for TRD were symptom severity, duration of the current major depressive episode (MDE), suicidality, psychotic and melancholic features, comorbid anxiety and personality disorders, add-on treatment, non-response to the first AD, adverse effects, high occupational level, recurrent disease course, previous hospitalisations, positive family history of MDD, early age of onset and novel associations of single nucleoid polymorphisms (SNPs) within the PPP3CC, ST8SIA2, CHL1, GAP43 and ITGB3 genes and gene pathways associated with neuroplasticity, intracellular signalling and chromatin silencing. A prediction model reaching accuracy of above 0.7 highlighted symptom severity, suicidality, comorbid anxiety and lifetime MDEs as the most informative predictors for TRD. Applying machine-learning algorithms, a signature of three SNPs of the BDNF, PPP3CC and HTR2A genes and lacking melancholia predicted treatment response. Conclusions: The GSRD findings offer a unique and balanced perspective on TRD representing foundation for further research elaborating on specific clinical and genetic hypotheses and treatment strategies within appropriate study-designs, especially interaction-based models and randomized controlled trials."
31330861,16.0,A Guide for Using Deep Learning for Complex Trait Genomic Prediction,2019 Jul 20;10(7):553.,"Deep learning (DL) has emerged as a powerful tool to make accurate predictions from complex data such as image, text, or video. However, its ability to predict phenotypic values from molecular data is less well studied. Here, we describe the theoretical foundations of DL and provide a generic code that can be easily modified to suit specific needs. DL comprises a wide variety of algorithms which depend on numerous hyperparameters. Careful optimization of hyperparameter values is critical to avoid overfitting. Among the DL architectures currently tested in genomic prediction, convolutional neural networks (CNNs) seem more promising than multilayer perceptrons (MLPs). A limitation of DL is in interpreting the results. This may not be relevant for genomic prediction in plant or animal breeding but can be critical when deciding the genetic risk to a disease. Although DL technologies are not ""plug-and-play"", they are easily implemented using Keras and TensorFlow public software. To illustrate the principles described here, we implemented a Keras-based code in GitHub."
31327699,1.0,A Narrative Review of Analytics in Pediatric Cardiac Anesthesia and Critical Care Medicine,2020 Feb;34(2):479-482.,"Congenital heart disease (CHD) is one of the most common birth anomalies, and the care of children with CHD has improved over the past 4 decades. However, children with CHD who undergo general anesthesia remain at increased risk for morbidity and mortality. The proliferation of electronic health record systems and sophisticated patient monitors affords the opportunity to capture and analyze large amounts of CHD patient data, and the application of novel, effective analytics methods to these data can enable clinicians to enhance their care of pediatric CHD patients. This narrative review covers recent efforts to leverage analytics in pediatric cardiac anesthesia and critical care to improve the care of children with CHD."
31326236,9.0,Insights into Computational Drug Repurposing for Neurodegenerative Disease,2019 Aug;40(8):565-576.,"Computational drug repurposing has the ability to remarkably reduce drug development time and cost in an era where these factors are prohibitively high. Several examples of successful repurposed drugs exist in fields such as oncology, diabetes, leprosy, inflammatory bowel disease, among others, however computational drug repurposing in neurodegenerative disease has presented several unique challenges stemming from the lack of validation methods and difficulty in studying heterogenous diseases of aging. Here, we examine existing approaches to computational drug repurposing, including molecular, clinical, and biophysical methods, and propose data sources and methods to advance computational drug repurposing in neurodegenerative disease using Alzheimer's disease as an example."
31326235,19.0,Artificial Intelligence for Clinical Trial Design,2019 Aug;40(8):577-591.,"Clinical trials consume the latter half of the 10 to 15 year, 1.5-2.0 billion USD, development cycle for bringing a single new drug to market. Hence, a failed trial sinks not only the investment into the trial itself but also the preclinical development costs, rendering the loss per failed clinical trial at 800 million to 1.4 billion USD. Suboptimal patient cohort selection and recruiting techniques, paired with the inability to monitor patients effectively during trials, are two of the main causes for high trial failure rates: only one of 10 compounds entering a clinical trial reaches the market. We explain how recent advances in artificial intelligence (AI) can be used to reshape key steps of clinical trial design towards increasing trial success rates."
31324413,22.0,Meta-Analysis Reveals Reproducible Gut Microbiome Alterations in Response to a High-Fat Diet,2019 Aug 14;26(2):265-272.e4.,"Multiple research groups have shown that diet impacts the gut microbiome; however, variability in experimental design and quantitative assessment have made it challenging to assess the degree to which similar diets have reproducible effects across studies. Through an unbiased subject-level meta-analysis framework, we re-analyzed 27 dietary studies including 1,101 samples from rodents and humans. We demonstrate that a high-fat diet (HFD) reproducibly changes gut microbial community structure. Finer taxonomic analysis revealed that the most reproducible signals of a HFD are Lactococcus species, which we experimentally demonstrate to be common dietary contaminants. Additionally, a machine-learning approach defined a signature that predicts the dietary intake of mice and demonstrated that phylogenetic and gene-centric transformations of this model can be translated to humans. Together, these results demonstrate the utility of microbiome meta-analyses in identifying robust and reproducible features for mechanistic studies in preclinical models."
31324321,,Diagnosis of seizures and encephalopathy using conventional EEG and amplitude integrated EEG,2019;162:363-400.,"Seizures are more common in the neonatal period than at any other time of life, partly due to the relative hyperexcitability of the neonatal brain. Brain monitoring of sick neonates in the NICU using either conventional electroencephalography or amplitude integrated EEG is essential to accurately detect seizures. Treatment of seizures is important, as evidence increasingly indicates that seizures damage the brain in addition to that caused by the underlying etiology. Prompt treatment has been shown to reduce seizure burden with the potential to ameliorate seizure-mediated damage. Neonatal encephalopathy most commonly caused by a hypoxia-ischemia results in an alteration of mental status and problems such as seizures, hypotonia, apnea, and feeding difficulties. Confirmation of encephalopathy with EEG monitoring can act as an important adjunct to other investigations and the clinical examination, particularly when considering treatment strategies such as therapeutic hypothermia. Brain monitoring also provides useful early prognostic indicators to clinicians. Recent use of machine learning in algorithms to continuously monitor the neonatal EEG, detect seizures, and grade encephalopathy offers the exciting prospect of real-time decision support in the NICU in the very near future."
31322128,1.0,Computer-Aided Detection for Breast Cancer Screening in Clinical Settings: Scoping Review,2019 Jul 18;7(3):e12660.,"Background:                    With the growth of machine learning applications, the practice of medicine is evolving. Computer-aided detection (CAD) is a software technology that has become widespread in radiology practices, particularly in breast cancer screening for improving detection rates at earlier stages. Many studies have investigated the diagnostic accuracy of CAD, but its implementation in clinical settings has been largely overlooked.              Objective:                    The aim of this scoping review was to summarize recent literature on the adoption and implementation of CAD during breast cancer screening by radiologists and to describe barriers and facilitators for CAD use.              Methods:                    The MEDLINE database was searched for English, peer-reviewed articles that described CAD implementation, including barriers or facilitators, in breast cancer screening and were published between January 2010 and March 2018. Articles describing the diagnostic accuracy of CAD for breast cancer detection were excluded. The search returned 526 citations, which were reviewed in duplicate through abstract and full-text screening. Reference lists and cited references in the included studies were reviewed.              Results:                    A total of nine articles met the inclusion criteria. The included articles showed that there is a tradeoff between the facilitators and barriers for CAD use. Facilitators for CAD use were improved breast cancer detection rates, increased profitability of breast imaging, and time saved by replacing double reading. Identified barriers were less favorable perceptions of CAD compared to double reading by radiologists, an increase in recall rates of patients for further testing, increased costs, and unclear effect on patient outcomes.              Conclusions:                    There is a gap in the literature between CAD's well-established diagnostic accuracy and its implementation and use by radiologists. Generally, the perceptions of radiologists have not been considered and details of implementation approaches for adoption of CAD have not been reported. The cost-effectiveness of CAD has not been well established for breast cancer screening in various populations. Further research is needed on how to best facilitate CAD in radiology practices in order to optimize patient outcomes, and the views of radiologists need to be better considered when advancing CAD use."
31320024,3.0,Impact of Artificial Intelligence on Interventional Cardiology: From Decision-Making Aid to Advanced Interventional Procedure Assistance,2019 Jul 22;12(14):1293-1303.,"Access to big data analyzed by supercomputers using advanced mathematical algorithms (i.e., deep machine learning) has allowed for enhancement of cognitive output (i.e., visual imaging interpretation) to previously unseen levels and promises to fundamentally change the practice of medicine. This field, known as ""artificial intelligence"" (AI), is making significant progress in areas such as automated clinical decision making, medical imaging analysis, and interventional procedures, and has the potential to dramatically influence the practice of interventional cardiology. The unique nature of interventional cardiology makes it an ideal target for the development of AI-based technologies designed to improve real-time clinical decision making, streamline workflow in the catheterization laboratory, and standardize catheter-based procedures through advanced robotics. This review provides an introduction to AI by highlighting its scope, potential applications, and limitations in interventional cardiology."
31319964,4.0,Reviewing ensemble classification methods in breast cancer,2019 Aug;177:89-112.,"Context:                    Ensemble methods consist of combining more than one single technique to solve the same task. This approach was designed to overcome the weaknesses of single techniques and consolidate their strengths. Ensemble methods are now widely used to carry out prediction tasks (e.g. classification and regression) in several fields, including that of bioinformatics. Researchers have particularly begun to employ ensemble techniques to improve research into breast cancer, as this is the most frequent type of cancer and accounts for most of the deaths among women.              Objective and method:                    The goal of this study is to analyse the state of the art in ensemble classification methods when applied to breast cancer as regards 9 aspects: publication venues, medical tasks tackled, empirical and research types adopted, types of ensembles proposed, single techniques used to construct the ensembles, validation framework adopted to evaluate the proposed ensembles, tools used to build the ensembles, and optimization methods used for the single techniques. This paper was undertaken as a systematic mapping study.              Results:                    A total of 193 papers that were published from the year 2000 onwards, were selected from four online databases: IEEE Xplore, ACM digital library, Scopus and PubMed. This study found that of the six medical tasks that exist, the diagnosis medical task was that most frequently researched, and that the experiment-based empirical type and evaluation-based research type were the most dominant approaches adopted in the selected studies. The homogeneous type was that most widely used to perform the classification task. With regard to single techniques, this mapping study found that decision trees, support vector machines and artificial neural networks were those most frequently adopted to build ensemble classifiers. In the case of the evaluation framework, the Wisconsin Breast Cancer dataset was the most frequently used by researchers to perform their experiments, while the most noticeable validation method was k-fold cross-validation. Several tools are available to perform experiments related to ensemble classification methods, such as Weka and R Software. Few researchers took into account the optimisation of the single technique of which their proposed ensemble was composed, while the grid search method was that most frequently adopted to tune the parameter settings of a single classifier.              Conclusion:                    This paper reports an in-depth study of the application of ensemble methods as regards breast cancer. Our results show that there are several gaps and issues and we, therefore, provide researchers in the field of breast cancer research with recommendations. Moreover, after analysing the papers found in this systematic mapping study, we discovered that the majority report positive results concerning the accuracy of ensemble classifiers when compared to the single classifiers. In order to aggregate the evidence reported in literature, it will, therefore, be necessary to perform a systematic literature review and meta-analysis in which an in-depth analysis could be conducted so as to confirm the superiority of ensemble classifiers over the classical techniques."
31319675,,Unsupervised Learning Techniques for the Investigation of Chronic Rhinosinusitis,2019 Dec;128(12):1170-1176.,"Objectives:                    This article reviews the principles of unsupervised learning, a novel technique which has increasingly been reported as a tool for the investigation of chronic rhinosinusitis (CRS). It represents a paradigm shift from the traditional approach to investigating CRS based upon the clinically recognized phenotypes of ""with polyps"" and ""without polyps"" and instead relies upon the application of complex mathematical models to derive subgroups which can then be further examined. This review article reports on the principles which underlie this investigative technique and some of the published examples in CRS.              Methods:                    This review summarizes the different types of unsupervised learning techniques which have been described and briefly expounds upon their useful applications. A literature review of studies which have unsupervised learning is then presented to provide a practical guide to its uses and some of the new directions of investigations suggested by their findings.              Results:                    The commonest unsupervised learning technique applied to rhinology research is cluster analysis, which can be further subdivided into hierarchical and non-hierarchical approaches. The mathematical principles which underpin these approaches are explained within this article. Studies which have used these techniques can be broadly divided into those which have used clinical data only and that which includes biomarkers. Studies which include biomarkers adhere closely to the established canon of CRS disease phenotypes, while those that use clinical data may diverge from the typical ""polyp versus non-polyp"" phenotypes and reflect subgroups of patients who share common symptom modifiers.              Summary:                    Artificial intelligence is increasingly influential in health care research and machine learning techniques have been reported in the investigation of CRS, promising several interesting new avenues for research. However, when critically appraising studies which use this technique, the reader needs to be au fait with the limitations and appropriate uses of its application."
31319078,2.0,Bending the Artificial Intelligence Curve for Radiology: Informatics Tools From ACR and RSNA,2019 Oct;16(10):1464-1470.,"Artificial intelligence (AI) will reshape radiology over the coming years. The radiology community has a strong history of embracing new technology for positive change, and AI is no exception. As with any new technology, rapid, successful implementation faces several challenges that will require creation and adoption of new integration technology. Use cases important to real-world application of AI are described, including clinical registries, AI research, AI product validation, and computer assistance for radiology reporting. Furthermore, the informatics technologies required for successful implementation of the use cases are described, including open Computer-Assisted Radiologist Decision Support, ACR Assist, ACR Data Science Institute use cases, common data elements (radelement.org), RadLex (radlex.org), LOINC/RSNA RadLex Playbook (loinc.org), and Radiology Report Templates (radreport.org)."
31314262,,Signal Processing: False Alarm Reduction,,"Modern patient monitoring systems in intensive care produce frequent false alarms that can lead to reduced standard of care. In this case study we demonstrate the development of an algorithm which uses a data fusion and machine learning approach to reduce the number of false alarms, while avoiding the suppression of a significant number of true alarms."
31314250,,Prediction Modeling Methodology,,"In the previous chapter, you have learned how to prepare your data before you start the process of generating a predictive model. In this chapter, you will learn how to make a predictive model using very common regression techniques and how to evaluate the performance of a model. In the next chapter we will then look at more advanced machine learning techniques that have become increasingly popular in recent years."
31314240,,Diving Deeper into Models,,"Pre-requisites to better understand the chapter: knowledge of the major steps and procedures of developing a clinical prediction model.    Logical position of the chapter with respect to the previous chapter: in the last chapters, you have learned how to develop and validate a clinical prediction model. You have been learning logistic regression as main algorithm to build the model. However, several different more complex algorithms can be used to build a clinical prediction model. In this chapter, the main machine learning based algorithms will be presented to you.    Learning objectives: you will be presented with the definitions of: machine learning, supervised and unsupervised learning. The major algorithms for the last two categories will be introduced."
31313637,6.0,Mapping the Passions: Toward a High-Dimensional Taxonomy of Emotional Experience and Expression,2019 Jul;20(1):69-90.,"What would a comprehensive atlas of human emotions include? For 50 years, scientists have sought to map emotion-related experience, expression, physiology, and recognition in terms of the ""basic six""-anger, disgust, fear, happiness, sadness, and surprise. Claims about the relationships between these six emotions and prototypical facial configurations have provided the basis for a long-standing debate over the diagnostic value of expression (for review and latest installment in this debate, see Barrett et al., p. 1). Building on recent empirical findings and methodologies, we offer an alternative conceptual and methodological approach that reveals a richer taxonomy of emotion. Dozens of distinct varieties of emotion are reliably distinguished by language, evoked in distinct circumstances, and perceived in distinct expressions of the face, body, and voice. Traditional models-both the basic six and affective-circumplex model (valence and arousal)-capture a fraction of the systematic variability in emotional response. In contrast, emotion-related responses (e.g., the smile of embarrassment, triumphant postures, sympathetic vocalizations, blends of distinct expressions) can be explained by richer models of emotion. Given these developments, we discuss why tests of a basic-six model of emotion are not tests of the diagnostic value of facial expression more generally. Determining the full extent of what facial expressions can tell us, marginally and in conjunction with other behavioral and contextual cues, will require mapping the high-dimensional, continuous space of facial, bodily, and vocal signals onto richly multifaceted experiences using large-scale statistical modeling and machine-learning methods."
31313504,6.0,Hybrid modeling frameworks of tumor development and treatment,2020 Jan;12(1):e1461.,"Tumors are complex multicellular heterogeneous systems comprised of components that interact with and modify one another. Tumor development depends on multiple factors: intrinsic, such as genetic mutations, altered signaling pathways, or variable receptor expression; and extrinsic, such as differences in nutrient supply, crosstalk with stromal or immune cells, or variable composition of the surrounding extracellular matrix. Tumors are also characterized by high cellular heterogeneity and dynamically changing tumor microenvironments. The complexity increases when this multiscale, multicomponent system is perturbed by anticancer treatments. Modeling such complex systems and predicting how tumors will respond to therapies require mathematical models that can handle various types of information and combine diverse theoretical methods on multiple temporal and spatial scales, that is, hybrid models. In this update, we discuss the progress that has been achieved during the last 10 years in the area of the hybrid modeling of tumors. The classical definition of hybrid models refers to the coupling of discrete descriptions of cells with continuous descriptions of microenvironmental factors. To reflect on the direction that the modeling field has taken, we propose extending the definition of hybrid models to include of coupling two or more different mathematical frameworks. Thus, in addition to discussing recent advances in discrete/continuous modeling, we also discuss how these two mathematical descriptions can be coupled with theoretical frameworks of optimal control, optimization, fluid dynamics, game theory, and machine learning. All these methods will be illustrated with applications to tumor development and various anticancer treatments. This article is characterized under: Analytical and Computational Methods > Computational Methods Translational, Genomic, and Systems Medicine > Therapeutic Methods Models of Systems Properties and Processes > Organ, Tissue, and Physiological Models."
31312621,,The Future of Robotic Surgery in Pediatric Urology: Upcoming Technology and Evolution Within the Field,2019 Jul 2;7:259.,"Since the introduction of the Da Vinci Surgical System (Intuitive Surgical, Inc., Sunnyvale, CA) in 1999, the market for robot assisted laparoscopic surgery has grown with urology. The initial surgical advantage seen in adults was for robotic prostatectomy, and over time this expanded to the pediatric population with robotic pyeloplasty. The introduction of three-dimensional visualization, tremor elimination, a 4th arm, and 7-degree range of motion allowed a significant operator advantage over laparoscopy, especially for anastomotic suturing. After starting with pyeloplasty, the use of robotic technology with pediatric urology has expanded to include ureteral reimplantation and even more complex reconstructive procedures, such as enterocystoplasty, appendicovesicostomy, and bladder neck reconstruction. However, limitations of the Da Vinci Surgical Systems still exist despite its continued technological advances over multiple generations in the past 20 years. Due to the smaller pediatric market, less focus appears to have been placed on the development of the smaller 5 mm instruments. As pediatric urology continues to utilize robotic technology for minimally invasive surgery, there is hope that additional pediatric-friendly instruments and components will be developed, either by Intuitive Surgical or one of the new robotic platforms in development that are working to address many of the shortcomings of current systems. These new robotic platforms include improved haptic feedback systems, flexible scopes, easier maneuverability, and even adaptive machine learning concepts to bring robotic assisted laparoscopic surgery to the next level. In this report, we review the present and upcoming technological advances of the current Da Vinci surgical systems as well as various new robotic platforms, each offering a unique set of technological advantages. As technology progresses, the understanding of and access to these new robotic platforms will help guide pediatric urologists into the next forefront of minimally invasive surgery."
31312276,8.0,The application of convolutional neural network to stem cell biology,2019 Jul 5;39:14.,"Induced pluripotent stem cells (iPSC) are one the most prominent innovations of medical research in the last few decades. iPSCs can be easily generated from human somatic cells and have several potential uses in regenerative medicine, disease modeling, drug screening, and precision medicine. However, further innovation is still required to realize their full potential. Machine learning is an algorithm that learns from large datasets for pattern formation and classification. Deep learning, a form of machine learning, uses a multilayered neural network that mimics human neural circuit structure. Deep neural networks can automatically extract features from an image, although classical machine learning methods still require feature extraction by a human expert. Deep learning technology has developed recently; in particular, the accuracy of an image classification task by using a convolutional neural network (CNN) has exceeded that of humans since 2015. CNN is now used to address several tasks including medical issues. We believe that CNN would also have a great impact on the research of stem cell biology. iPSCs are utilized after their differentiation to specific cells, which are characterized by molecular techniques such as immunostaining or lineage tracing. Each cell shows a characteristic morphology; thus, a morphology-based identification system of cell type by CNN would be an alternative technique. The development of CNN enables the automation of identifying cell types from phase contrast microscope images without molecular labeling, which will be applied to several researches and medical science. Image classification is a strong field among deep learning tasks, and several medical tasks will be solved by deep learning-based programs in the future."
31311655,8.0,Integrative Approaches to Cancer Immunotherapy,2019 Jul;5(7):400-410.,"Cancer immunotherapy aims to arm patients with cancer-fighting immunity. Many new cancer-specific immunotherapeutic drugs have gained approval in the past several years, demonstrating immunotherapy's efficacy and promise as an anticancer modality. Despite these successes, several outstanding questions remain for cancer immunotherapy, including how to make immunotherapy more efficacious in a broader range of cancer types and patients, and how to predict which patients will respond or not respond to therapy. We present a case for integrative systems approaches that will answer these questions. This involves applying mechanistic and statistical modeling, establishing consistent and widely adopted experimental tools to generate systems-level data, and creating sustained mechanisms of support. If implemented, these approaches will lead to major advances in cancer treatment."
31311203,4.0,A Survey on Recent Trends and Open Issues in Energy Efficiency of 5G,2019 Jul 15;19(14):3126.,"The rapidly increasing interest from various verticals for the upcoming 5th generation (5G) networks expect the network to support higher data rates and have an improved quality of service. This demand has been met so far by employing sophisticated transmission techniques including massive Multiple Input Multiple Output (MIMO), millimeter wave (mmWave) bands as well as bringing the computational power closer to the users via advanced baseband processing units at the base stations. Future evolution of the networks has also been assumed to open many new business horizons for the operators and the need of not only a resource efficient but also an energy efficient ecosystem has greatly been felt. The deployment of small cells has been envisioned as a promising answer for handling the massive heterogeneous traffic, but the adverse economic and environmental impacts cannot be neglected. Given that 10% of the world's energy consumption is due to the Information and Communications Technology (ICT) industry, energy-efficiency has thus become one of the key performance indicators (KPI). Various avenues of optimization, game theory and machine learning have been investigated for enhancing power allocation for downlink and uplink channels, as well as other energy consumption/saving approaches. This paper surveys the recent works that address energy efficiency of the radio access as well as the core of wireless networks, and outlines related challenges and open issues."
31308553,42.0,Machine-learning-guided directed evolution for protein engineering,2019 Aug;16(8):687-694.,"Protein engineering through machine-learning-guided directed evolution enables the optimization of protein functions. Machine-learning approaches predict how sequence maps to function in a data-driven manner without requiring a detailed model of the underlying physics or biological pathways. Such methods accelerate directed evolution by learning from the properties of characterized variants and using that information to select sequences that are likely to exhibit improved properties. Here we introduce the steps required to build machine-learning sequence-function models and to use those models to guide engineering, making recommendations at each stage. This review covers basic concepts relevant to the use of machine learning for protein engineering, as well as the current literature and applications of this engineering paradigm. We illustrate the process with two case studies. Finally, we look to future opportunities for machine learning to enable the discovery of unknown protein functions and uncover the relationship between protein sequence and function."
31304857,3.0,A Review of Machine Learning Techniques for Keratoconus Detection and Refractive Surgery Screening,2019;34(4):317-326.,"Various machine learning techniques have been developed for keratoconus detection and refractive surgery screening. These techniques utilize inputs from a range of corneal imaging devices and are built with automated decision trees, support vector machines, and various types of neural networks. In general, these techniques demonstrate very good differentiation of normal and keratoconic eyes, as well as good differentiation of normal and form fruste keratoconus. However, it is difficult to directly compare these studies, as keratoconus represents a wide spectrum of disease. More importantly, no public dataset exists for research purposes. Despite these challenges, machine learning in keratoconus detection and refractive surgery screening is a burgeoning field of study, with significant potential for continued advancement as imaging devices and techniques become more sophisticated."
31304340,5.0,Reflecting health: smart mirrors for personalized medicine,2018 Nov 8;1:62.,"Inexpensive embedded computing and the related Internet of Things technologies enable the recent development of smart products that can respond to human needs and improve everyday tasks in an attempt to make traditional environments more ""intelligent"". Several projects have augmented mirrors for a range of smarter applications in automobiles and homes. The opportunity to apply smart mirror technology to healthcare to predict and to monitor aspects of health and disease is a natural but mostly underdeveloped idea. We envision that smart mirrors comprising a combination of intelligent hardware and software could identify subtle, yet clinically relevant changes in physique and appearance. Similarly, a smart mirror could record and evaluate body position and motion to identify posture and movement issues, as well as offer feedback for corrective actions. Successful development and implementation of smart mirrors for healthcare applications will require overcoming new challenges in engineering, machine learning, computer vision, and biomedical research. This paper examines the potential uses of smart mirrors in healthcare and explores how this technology might benefit users in various medical environments. We also provide a brief description of the state-of-the-art, including a functional prototype concept developed by our group, and highlight the directions to make this device more mainstream in health-related applications."
31304333,15.0,Machine learning and medical education,2018 Sep 27;1:54.,Artificial intelligence (AI) driven by machine learning (ML) algorithms is a branch in computer science that is rapidly gaining popularity within the healthcare sector. Recent regulatory approvals of AI-driven companion diagnostics and other products are glimmers of a future in which these tools could play a key role by defining the way medicine will be practiced. Educating the next generation of medical professionals with the right ML techniques will enable them to become part of this emerging data science revolution.
31297148,14.0,Cardiac tissue engineering: state-of-the-art methods and outlook,2019 Jun 28;13:57.,"The purpose of this review is to assess the state-of-the-art fabrication methods, advances in genome editing, and the use of machine learning to shape the prospective growth in cardiac tissue engineering. Those interdisciplinary emerging innovations would move forward basic research in this field and their clinical applications. The long-entrenched challenges in this field could be addressed by novel 3-dimensional (3D) scaffold substrates for cardiomyocyte (CM) growth and maturation. Stem cell-based therapy through genome editing techniques can repair gene mutation, control better maturation of CMs or even reveal its molecular clock. Finally, machine learning and precision control for improvements of the construct fabrication process and optimization in tissue-specific clonal selections with an outlook of cardiac tissue engineering are also presented."
31295267,16.0,Machine and deep learning meet genome-scale metabolic modeling,2019 Jul 11;15(7):e1007084.,"Omic data analysis is steadily growing as a driver of basic and applied molecular biology research. Core to the interpretation of complex and heterogeneous biological phenotypes are computational approaches in the fields of statistics and machine learning. In parallel, constraint-based metabolic modeling has established itself as the main tool to investigate large-scale relationships between genotype, phenotype, and environment. The development and application of these methodological frameworks have occurred independently for the most part, whereas the potential of their integration for biological, biomedical, and biotechnological research is less known. Here, we describe how machine learning and constraint-based modeling can be combined, reviewing recent works at the intersection of both domains and discussing the mathematical and practical aspects involved. We overlap systematic classifications from both frameworks, making them accessible to nonexperts. Finally, we delineate potential future scenarios, propose new joint theoretical frameworks, and suggest concrete points of investigation for this joint subfield. A multiview approach merging experimental and knowledge-driven omic data through machine learning methods can incorporate key mechanistic information in an otherwise biologically-agnostic learning process."
31294972,36.0,Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery,2019 Sep 25;119(18):10520-10594.,"Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design."
31293616,26.0,A Review and Tutorial of Machine Learning Methods for Microbiome Host Trait Prediction,2019 Jun 25;10:579.,"With the growing importance of microbiome research, there is increasing evidence that host variation in microbial communities is associated with overall host health. Advancement in genetic sequencing methods for microbiomes has coincided with improvements in machine learning, with important implications for disease risk prediction in humans. One aspect specific to microbiome prediction is the use of taxonomy-informed feature selection. In this review for non-experts, we explore the most commonly used machine learning methods, and evaluate their prediction accuracy as applied to microbiome host trait prediction. Methods are described at an introductory level, and R/Python code for the analyses is provided."
31287638,,Automated analysis of free-text comments and dashboard representations in patient experience surveys: a multimethod co-design study,,"Background:                    Patient experience surveys (PESs) often include informative free-text comments, but with no way of systematically, efficiently and usefully analysing and reporting these. The National Cancer Patient Experience Survey (CPES), used to model the approach reported here, generates > 70,000 free-text comments annually.              Main aim:                    To improve the use and usefulness of PES free-text comments in driving health service changes that improve the patient experience.              Secondary aims:                    (1) To structure CPES free-text comments using rule-based information retrieval (IR) (‘text engineering’), drawing on health-care domain-specific gazetteers of terms, with in-built transferability to other surveys and conditions; (2) to display the results usefully for health-care professionals, in a digital toolkit dashboard display that drills down to the original free text; (3) to explore the usefulness of interdisciplinary mixed stakeholder co-design and consensus-forming approaches in technology development, ensuring that outputs have meaning for all; and (4) to explore the usefulness of Normalisation Process Theory (NPT) in structuring outputs for implementation and sustainability.              Design:                    A scoping review, rapid review and surveys with stakeholders in health care (patients, carers, health-care providers, commissioners, policy-makers and charities) explored clinical dashboard design/patient experience themes. The findings informed the rules for the draft rule-based IR [developed using half of the 2013 Wales CPES (WCPES) data set] and prototype toolkit dashboards summarising PES data. These were refined following mixed stakeholder, concept-mapping workshops and interviews, which were structured to enable consensus-forming ‘co-design’ work. IR validation used the second half of the WCPES, with comparison against its manual analysis; transferability was tested using further health-care data sets. A discrete choice experiment (DCE) explored which toolkit features were preferred by health-care professionals, with a simple cost–benefit analysis. Structured walk-throughs with NHS managers in Wessex, London and Leeds explored usability and general implementation into practice.              Key outcomes:                    A taxonomy of ranked PES themes, a checklist of key features recommended for digital clinical toolkits, rule-based IR validation and transferability scores, usability, and goal-oriented, cost–benefit and marketability results. The secondary outputs were a survey, scoping and rapid review findings, and concordance and discordance between stakeholders and methods.              Results:                    (1) The surveys, rapid review and workshops showed that stakeholders differed in their understandings of the patient experience and priorities for change, but that they reached consensus on a shortlist of 19 themes; six were considered to be core; (2) the scoping review and one survey explored the clinical toolkit design, emphasising that such toolkits should be quick and easy to use, and embedded in workflows; the workshop discussions, the DCE and the walk-throughs confirmed this and foregrounded other features to form the toolkit design checklist; and (3) the rule-based IR, developed using noun and verb phrases and lookup gazetteers, was 86% accurate on the WCPES, but needs modification to improve this and to be accurate with other data sets. The DCE and the walk-through suggest that the toolkit would be well accepted, with a favourable cost–benefit ratio, if implemented into practice with appropriate infrastructure support.              Limitations:                    Small participant numbers and sampling bias across component studies. The scoping review studies mostly used top-down approaches and focused on professional dashboards. The rapid review of themes had limited scope, with no second reviewer. The IR needs further refinement, especially for transferability. New governance restrictions further limit immediate use.              Conclusions:                    Using a multidisciplinary, mixed stakeholder, use of co-design, proof of concept was shown for an automated display of patient experience free-text comments in a way that could drive health-care improvements in real time. The approach is easily modified for transferable application.              Future work:                    Further exploration is needed of implementation into practice, transferable uses and technology development co-design approaches.              Funding:                    The National Institute for Health Research Health Services and Delivery Research programme."
31288140,4.0,Machine learning-based coronary artery disease diagnosis: A comprehensive review,2019 Aug;111:103346.,"Coronary artery disease (CAD) is the most common cardiovascular disease (CVD) and often leads to a heart attack. It annually causes millions of deaths and billions of dollars in financial losses worldwide. Angiography, which is invasive and risky, is the standard procedure for diagnosing CAD. Alternatively, machine learning (ML) techniques have been widely used in the literature as fast, affordable, and noninvasive approaches for CAD detection. The results that have been published on ML-based CAD diagnosis differ substantially in terms of the analyzed datasets, sample sizes, features, location of data collection, performance metrics, and applied ML techniques. Due to these fundamental differences, achievements in the literature cannot be generalized. This paper conducts a comprehensive and multifaceted review of all relevant studies that were published between 1992 and 2019 for ML-based CAD diagnosis. The impacts of various factors, such as dataset characteristics (geographical location, sample size, features, and the stenosis of each coronary artery) and applied ML techniques (feature selection, performance metrics, and method) are investigated in detail. Finally, the important challenges and shortcomings of ML-based CAD diagnosis are discussed."
31282778,,Iterative processes: a review of semi-supervised machine learning in rehabilitation science,2020 Jul;15(5):515-520.,"Purpose: To define semi-supervised machine learning (SSML) and explore current and potential applications of this analytic strategy in rehabilitation research.Method: We conducted a scoping review using PubMed, GoogleScholar and Medline. Studies were included if they: (1) described a semi-supervised approach to apply machine learning algorithms during data analysis and (2) examined constructs encompassed by the International Classification of Functioning, Disability and Health (ICF). The first two authors reviewed identified articles and recorded study and participant characteristics. The ICF domain used in each study was also identified.Results: After combining information from the eight studies, we established that SSML was a feasible approach for analysis of complex data in rehabilitation research. We also determined that semi-supervised approaches may be more accurate than supervised machine learning approaches.Conclusions: A semi-supervised approach to machine learning has potential to enhance our understanding of complex data sets in rehabilitation science. SSML mirrors the iterative process of rehabilitation, making this approach ideal for calibrating devices, classifying activities or identifying just-in-time interventions. Rehabilitation scientists who are interested in conducting SSML should collaborate with data scientists to advance the application of this approach within our field.Implications for rehabilitationSemi-supervised machine learning applications may be a feasible approach for analyses of complex data sets in rehabilitation research.Semi-supervised machine learning approaches uses a combination of labelled and unlabelled data to produce accurate predictive models, thereby requiring less user-input data than other machine learning approaches (i.e., supervised, unsupervised), reducing resource cost and user-burden.Semi-supervised machine learning is an iterative process that, when applied to rehabilitation assessment and outcomes, could produce accurate personalized models for treatment.Rehabilitation researchers and data scientists should collaborate to implement semi-supervised machine learning approaches in rehabilitation research, optimizing the power of large datasets that are becoming more readily available within the field (e.g., EEG signals, sensors, smarthomes)."
31281846,6.0,Human Systems Biology and Metabolic Modelling: A Review-From Disease Metabolism to Precision Medicine,2019 Jun 9;2019:8304260.,"In cell and molecular biology, metabolism is the only system that can be fully simulated at genome scale. Metabolic systems biology offers powerful abstraction tools to simulate all known metabolic reactions in a cell, therefore providing a snapshot that is close to its observable phenotype. In this review, we cover the 15 years of human metabolic modelling. We show that, although the past five years have not experienced large improvements in the size of the gene and metabolite sets in human metabolic models, their accuracy is rapidly increasing. We also describe how condition-, tissue-, and patient-specific metabolic models shed light on cell-specific changes occurring in the metabolic network, therefore predicting biomarkers of disease metabolism. We finally discuss current challenges and future promising directions for this research field, including machine/deep learning and precision medicine. In the omics era, profiling patients and biological processes from a multiomic point of view is becoming more common and less expensive. Starting from multiomic data collected from patients and N-of-1 trials where individual patients constitute different case studies, methods for model-building and data integration are being used to generate patient-specific models. Coupled with state-of-the-art machine learning methods, this will allow characterizing each patient's disease phenotype and delivering precision medicine solutions, therefore leading to preventative medicine, reduced treatment, and in silico clinical trials."
31281326,3.0,Systems and Synthetic Biology of Forest Trees: A Bioengineering Paradigm for Woody Biomass Feedstocks,2019 Jun 20;10:775.,"Fast-growing forest plantations are sustainable feedstocks of plant biomass that can serve as alternatives to fossil carbon resources for materials, chemicals, and energy. Their ability to efficiently harvest light energy and carbon from the atmosphere and sequester this into metabolic precursors for lignocellulosic biopolymers and a wide range of plant specialized metabolites make them excellent biochemical production platforms and living biorefineries. Their large sizes have facilitated multi-omics analyses and systems modeling of key biological processes such as lignin biosynthesis in trees. High-throughput 'omics' approaches have also been applied in segregating tree populations where genetic variation creates abundant genetic perturbations of system components allowing construction of systems genetics models linking genes and pathways to complex trait variation. With this information in hand, it is now possible to start using synthetic biology and genome editing techniques in a bioengineering approach based on a deeper understanding and rational design of biological parts, devices, and integrated systems. However, the complexity of the biology and interacting components will require investment in big data informatics, machine learning, and intuitive visualization to fully explore multi-dimensional patterns and identify emergent properties of biological systems. Predictive systems models could be tested rapidly through high-throughput synthetic biology approaches and multigene editing. Such a bioengineering paradigm, together with accelerated genomic breeding, will be crucial for the development of a new generation of woody biorefinery crops."
31280916,7.0,"Artificial Intelligence and Arthroplasty at a Single Institution: Real-World Applications of Machine Learning to Big Data, Value-Based Care, Mobile Health, and Remote Patient Monitoring",2019 Oct;34(10):2204-2209.,"Background:                    Driven by the recent ubiquity of big data and computing power, we established the Machine Learning Arthroplasty Laboratory (MLAL) to examine and apply artificial intelligence (AI) to musculoskeletal medicine.              Methods:                    In this review, we discuss the 2 core objectives of the MLAL as they relate to the practice and progress of orthopedic surgery: (1) patient-specific, value-based care and (2) human movement.              Results:                    We developed and validated several machine learning-based models for primary lower extremity arthroplasty that preoperatively predict patient-specific, risk-adjusted value metrics, including cost, length of stay, and discharge disposition, to provide improved expectation management, preoperative planning, and potential financial arbitration. Additionally, we leveraged passive, ubiquitous mobile technologies to build a small data registry of human movement surrounding TKA that permits remote patient monitoring to evaluate therapy compliance, outcomes, opioid intake, mobility, and joint range of motion.              Conclusion:                    The rapid rate with which we in arthroplasty are acquiring and storing continuous data, whether passively or actively, demands an advanced processing approach: AI. By carefully studying AI techniques with the MLAL, we have applied this evolving technique as a first step that may directly improve patient outcomes and practice of orthopedics."
31280350,8.0,"Artificial intelligence, machine (deep) learning and radio(geno)mics: definitions and nuclear medicine imaging applications",2019 Dec;46(13):2630-2637.,"Techniques from the field of artificial intelligence, and more specifically machine (deep) learning methods, have been core components of most recent developments in the field of medical imaging. They are already being exploited or are being considered to tackle most tasks, including image reconstruction, processing (denoising, segmentation), analysis and predictive modelling. In this review we introduce and define these key concepts and discuss how the techniques from this field can be applied to nuclear medicine imaging applications with a particular focus on radio(geno)mics."
31279913,1.0,Artificial Intelligence in Aortic Surgery: The Rise of the Machine,2019 Winter;31(4):635-637.,"The first concept of Artificial Intelligence (AI) came into attention during 1920s and currently it is rapidly being integrated in our daily clinical practice. The use of AI has evolved from basic image-based analysis into complex decisions related to different surgical procedure. AI has been very widely used in the cardiology field, however the use of such machine-led decisions has been limited and explored at slower pace in surgical practice. The use of AI in cardiac surgery is still at its infancy but growing dramatically to reflect the changes in the clinical decision making process for better patient outcomes. The machine-led but human controlled algorithms will soon be taking over most of the decision making processes in cardiac surgery. This review article focuses on the practice of AI in aortic surgery and the future of such technology-led decision making pathways on patient outcomes, surgeon's learning skills and adaptability."
31278512,1.0,Practical issues in assessing nailfold capillaroscopic images: a summary,2019 Sep;38(9):2343-2354.,"Nailfold capillaroscopy (NC) is a highly sensitive, safe, and non-invasive technique to assess involvement rate of microvascularity in dermatomyositis and systemic sclerosis. A large number of studies have focused on NC pattern description, classification, and scoring system validation, but minimal information has been published on the accuracy and precision of the measurement. The objective of this review article is to identify different factors affecting the reliability and validity of the assessment in NC. Several factors can affect the reliability of the examination, e.g., physiological artifacts, the nailfold imaging instrument, human factors, and the assessment rules and standards. It is impossible to avoid all artifacts, e.g., skin transparency, physically injured fingers, and skin pigmentation. However, minimization of the impact of some of these artifacts by considering some protocols before the examination and by using specialized tools, training, guidelines, and software can help to reduce errors in the measurement and assessment of NC images. Establishing guidelines and instructions for automatic characterization and measurement based on machine learning techniques also may reduce ambiguities and the assessment time."
31277828,14.0,Machine Learning for Health Services Researchers,2019 Jul;22(7):808-815.,"Background:                    Machine learning is increasingly used to predict healthcare outcomes, including cost, utilization, and quality.              Objective:                    We provide a high-level overview of machine learning for healthcare outcomes researchers and decision makers.              Methods:                    We introduce key concepts for understanding the application of machine learning methods to healthcare outcomes research. We first describe current standards to rigorously learn an estimator, which is an algorithm developed through machine learning to predict a particular outcome. We include steps for data preparation, estimator family selection, parameter learning, regularization, and evaluation. We then compare 3 of the most common machine learning methods: (1) decision tree methods that can be useful for identifying how different subpopulations experience different risks for an outcome; (2) deep learning methods that can identify complex nonlinear patterns or interactions between variables predictive of an outcome; and (3) ensemble methods that can improve predictive performance by combining multiple machine learning methods.              Results:                    We demonstrate the application of common machine methods to a simulated insurance claims dataset. We specifically include statistical code in R and Python for the development and evaluation of estimators for predicting which patients are at heightened risk for hospitalization from ambulatory care-sensitive conditions.              Conclusions:                    Outcomes researchers should be aware of key standards for rigorously evaluating an estimator developed through machine learning approaches. Although multiple methods use machine learning concepts, different approaches are best suited for different research problems."
31276247,6.0,Machine learning in breast MRI,2020 Oct;52(4):998-1018.,"Machine-learning techniques have led to remarkable advances in data extraction and analysis of medical imaging. Applications of machine learning to breast MRI continue to expand rapidly as increasingly accurate 3D breast and lesion segmentation allows the combination of radiologist-level interpretation (eg, BI-RADS lexicon), data from advanced multiparametric imaging techniques, and patient-level data such as genetic risk markers. Advances in breast MRI feature extraction have led to rapid dataset analysis, which offers promise in large pooled multiinstitutional data analysis. The object of this review is to provide an overview of machine-learning and deep-learning techniques for breast MRI, including supervised and unsupervised methods, anatomic breast segmentation, and lesion segmentation. Finally, it explores the role of machine learning, current limitations, and future applications to texture analysis, radiomics, and radiogenomics. Level of Evidence: 3 Technical Efficacy Stage: 2 J. Magn. Reson. Imaging 2019. J. Magn. Reson. Imaging 2020;52:998-1018."
31272605,5.0,Artificial intelligence in cytopathology: a review of the literature and overview of commercial landscape,Jul-Aug 2019;8(4):230-241.,"Artificial intelligence (AI) has made impressive strides recently in interpreting complex images, thanks to improvements in deep learning techniques and increasing computational power. Researchers have started applying these advanced techniques to pathology images, although most efforts have been focused on histopathology. Cytopathology, however, remains the original field of pathology for which AI models for clinical use were successfully commercialized, to assist with automating Papanicolaou test screening. Recent AI efforts have focused on whole slide images of both gynecologic and non-gynecologic cytopathology. This review summarizes the literature and commercial landscape of AI as applied to cytopathology."
31271668,6.0,"Medication management needs information and communications technology-based approaches, including telehealth and artificial intelligence",2020 Oct;86(10):2000-2007.,"Life expectancy is rising in most parts of the world as is the prevalence of chronic diseases. Suboptimal adherence to long-term medications is still rather the norm than the exception, although it is well known that suboptimal adherence compromises the therapeutic effectiveness. Information and communications technology provides new concepts for improving adherence to medications. These so-called telehealth concepts or services help to implement closed-loop healthcare paradigms and to establish collaborative care networks involving all stakeholders relevant to optimising the overall medication therapy. Together with data from Electronic Health Records and Electronic Medical Records, these networks pave the way to data-driven decision support systems. Recent advances in machine learning, predictive analytics, and artificial intelligence allow further steps towards fully autonomous telehealth systems. This might bring advances in the future: disburden healthcare professionals from repetitive tasks, enable them to timely react to critical situations, and offer a comprehensive overview of the patients' medication status. Advanced analytics can help to assess whether patients have taken their medications as prescribed, to improve adherence via automatic reminders. Ultimately, all relevant data sources need to be collated into a basis for data-driven methods, with the goal to assist healthcare professionals in guiding patients to obtain the best possible health status, with a reasonable resource utilisation and a risk-adjusted safety and privacy approach. This paper summarises the state-of-the-art of telehealth and artificial intelligence applications in medication management. It focuses on 3 major aspects: latest technologies, current applications, and patient related issues."
31270976,36.0,Reproducibility and Generalizability in Radiomics Modeling: Possible Strategies in Radiologic and Statistical Perspectives,2019 Jul;20(7):1124-1137.,"Radiomics, which involves the use of high-dimensional quantitative imaging features for predictive purposes, is a powerful tool for developing and testing medical hypotheses. Radiologic and statistical challenges in radiomics include those related to the reproducibility of imaging data, control of overfitting due to high dimensionality, and the generalizability of modeling. The aims of this review article are to clarify the distinctions between radiomics features and other omics and imaging data, to describe the challenges and potential strategies in reproducibility and feature selection, and to reveal the epidemiological background of modeling, thereby facilitating and promoting more reproducible and generalizable radiomics research."
31270636,10.0,Current Approaches to the Use of Artificial Intelligence for Injury Risk Assessment and Performance Prediction in Team Sports: a Systematic Review,2019 Jul 3;5(1):28.,"Background:                    The application of artificial intelligence (AI) opens an interesting perspective for predicting injury risk and performance in team sports. A better understanding of the techniques of AI employed and of the sports that are using AI is clearly warranted. The purpose of this study is to identify which AI approaches have been applied to investigate sport performance and injury risk and to find out which AI techniques each sport has been using.              Methods:                    Systematic searches through the PubMed, Scopus, and Web of Science online databases were conducted for articles reporting AI techniques or methods applied to team sports athletes.              Results:                    Fifty-eight studies were included in the review with 11 AI techniques or methods being applied in 12 team sports. Pooled sample consisted of 6456 participants (97% male, 25 ± 8 years old; 3% female, 21 ± 10 years old) with 76% of them being professional athletes. The AI techniques or methods most frequently used were artificial neural networks, decision tree classifier, support vector machine, and Markov process with good performance metrics for all of them. Soccer, basketball, handball, and volleyball were the team sports with more applications of AI.              Conclusions:                    The results of this review suggest a prevalent application of AI methods in team sports based on the number of published studies. The current state of development in the area proposes a promising future with regard to AI use in team sports. Further evaluation research based on prospective methods is warranted to establish the predictive performance of specific AI techniques and methods."
31268707,5.0,Opportunities and Challenges in Phenotypic Screening for Neurodegenerative Disease Research,2020 Mar 12;63(5):1823-1840.,"Toxic misfolded proteins potentially underly many neurodegenerative diseases, but individual targets which regulate these proteins and their downstream detrimental effects are often unknown. Phenotypic screening is an unbiased method to screen for novel targets and therapeutic molecules and span the range from primitive model organisms such as Sacchaomyces cerevisiae, which allow for high-throughput screening to patient-derived cell-lines that have a close connection to the disease biology but are limited in screening capacity. This perspective will review current phenotypic models, as well as the chemical screening strategies most often employed. Advances in in 3D cell cultures, high-content screens, robotic microscopy, CRISPR screening, and use of machine learning methods to process the enormous amount of data generated by these screens are certain to change the paradigm for phenotypic screening and will be discussed."
31267627,21.0,The Crucial Role of Methodology Development in Directed Evolution of Selective Enzymes,2020 Aug 3;59(32):13204-13231.,"Directed evolution of stereo-, regio-, and chemoselective enzymes constitutes a unique way to generate biocatalysts for synthetically interesting transformations in organic chemistry and biotechnology. In order for this protein engineering technique to be efficient, fast, and reliable, and also of relevance to synthetic organic chemistry, methodology development was and still is necessary. Following a description of early key contributions, this review focuses on recent developments. It includes optimization of molecular biological methods for gene mutagenesis and the design of efficient strategies for their application, resulting in notable reduction of the screening effort (bottleneck of directed evolution). When aiming for laboratory evolution of selectivity and activity, second-generation versions of Combinatorial Active-Site Saturation Test (CAST) and Iterative Saturation Mutagenesis (ISM), both involving saturation mutagenesis (SM) at sites lining the binding pocket, have emerged as preferred approaches, aided by in silico methods such as machine learning. The recently proposed Focused Rational Iterative Site-specific Mutagenesis (FRISM) constitutes a fusion of rational design and directed evolution. On-chip solid-phase chemical gene synthesis for rapid library construction enhances library quality notably by eliminating undesired amino acid bias, the future of directed evolution?"
33178948,2.0,Balancing accuracy and interpretability of machine learning approaches for radiation treatment outcomes modeling,2019 Jul 4;1(1):20190021.,"Radiation outcomes prediction (ROP) plays an important role in personalized prescription and adaptive radiotherapy. A clinical decision may not only depend on an accurate radiation outcomes' prediction, but also needs to be made based on an informed understanding of the relationship among patients' characteristics, radiation response and treatment plans. As more patients' biophysical information become available, machine learning (ML) techniques will have a great potential for improving ROP. Creating explainable ML methods is an ultimate task for clinical practice but remains a challenging one. Towards complete explainability, the interpretability of ML approaches needs to be first explored. Hence, this review focuses on the application of ML techniques for clinical adoption in radiation oncology by balancing accuracy with interpretability of the predictive model of interest. An ML algorithm can be generally classified into an interpretable (IP) or non-interpretable (NIP) (""black box"") technique. While the former may provide a clearer explanation to aid clinical decision-making, its prediction performance is generally outperformed by the latter. Therefore, great efforts and resources have been dedicated towards balancing the accuracy and the interpretability of ML approaches in ROP, but more still needs to be done. In this review, current progress to increase the accuracy for IP ML approaches is introduced, and major trends to improve the interpretability and alleviate the ""black box"" stigma of ML in radiation outcomes modeling are summarized. Efforts to integrate IP and NIP ML approaches to produce predictive models with higher accuracy and interpretability for ROP are also discussed."
31262850,1.0,To Combine or Not Combine: Drug Interactions and Tools for Their Analysis. Reflections from the EORTC-PAMM Course on Preclinical and Early-phase Clinical Pharmacology,2019 Jul;39(7):3303-3309.,"Combination therapies are used in the clinic to achieve cure, better efficacy and to circumvent resistant disease in patients. Initial assessment of the effect of such combinations, usually of two agents, is frequently performed using in vitro assays. In this review, we give a short summary of the types of analyses that were presented during the Preclinical and Early-phase Clinical Pharmacology Course of the Pharmacology and Molecular Mechanisms Group, European Organization for Research and Treatment on Cancer, that can be used to determine the efficacy of drug combinations. The effect of a combination treatment can be calculated using mathematical equations based on either the Loewe additivity or Bliss independence model, or a combination of both, such as Chou and Talalay's median-drug effect model. Interactions can be additive, synergistic (more than additive), or antagonistic (less than additive). Software packages CalcuSyn (also available as CompuSyn) and Combenefit are designed to calculate the extent of the combined effects. Interestingly, the application of machine-learning methods in the prediction of combination treatments, which can include pharmacogenomic, genetic, metabolomic and proteomic profiles, might contribute to further refinement of combination regimens. However, more research is needed to apply appropriate rules of machine learning methods to ensure correct predictive models."
31261187,5.0,Artificial intelligence for pediatric ophthalmology,2019 Sep;30(5):337-346.,"Purpose of review:                    Despite the impressive results of recent artificial intelligence applications to general ophthalmology, comparatively less progress has been made toward solving problems in pediatric ophthalmology using similar techniques. This article discusses the unique needs of pediatric patients and how artificial intelligence techniques can address these challenges, surveys recent applications to pediatric ophthalmology, and discusses future directions.              Recent findings:                    The most significant advances involve the automated detection of retinopathy of prematurity, yielding results that rival experts. Machine learning has also been applied to the classification of pediatric cataracts, prediction of postoperative complications following cataract surgery, detection of strabismus and refractive error, prediction of future high myopia, and diagnosis of reading disability. In addition, machine learning techniques have been used for the study of visual development, vessel segmentation in pediatric fundus images, and ophthalmic image synthesis.              Summary:                    Artificial intelligence applications could significantly benefit clinical care by optimizing disease detection and grading, broadening access to care, furthering scientific discovery, and improving clinical efficiency. These methods need to match or surpass physician performance in clinical trials before deployment with patients. Owing to the widespread use of closed-access data sets and software implementations, it is difficult to directly compare the performance of these approaches, and reproducibility is poor. Open-access data sets and software could alleviate these issues and encourage further applications to pediatric ophthalmology."
31260168,1.0,Agent-based models of inflammation in translational systems biology: A decade later,2019 Nov;11(6):e1460.,"Agent-based modeling is a rule-based, discrete-event, and spatially explicit computational modeling method that employs computational objects that instantiate the rules and interactions among the individual components (""agents"") of system. Agent-based modeling is well suited to translating into a computational model the knowledge generated from basic science research, particularly with respect to translating across scales the mechanisms of cellular behavior into aggregated cell population dynamics manifesting at the tissue and organ level. This capacity has made agent-based modeling an integral method in translational systems biology (TSB), an approach that uses multiscale dynamic computational modeling to explicitly represent disease processes in a clinically relevant fashion. The initial work in the early 2000s using agent-based models (ABMs) in TSB focused on examining acute inflammation and its intersection with wound healing; the decade since has seen vast growth in both the application of agent-based modeling to a wide array of disease processes as well as methodological advancements in the use and analysis of ABM. This report presents an update on an earlier review of ABMs in TSB and presents examples of exciting progress in the modeling of various organs and diseases that involve inflammation. This review also describes developments that integrate the use of ABMs with cutting-edge technologies such as high-performance computing, machine learning, and artificial intelligence, with a view toward the future integration of these methodologies. This article is categorized under: Translational, Genomic, and Systems Medicine > Translational Medicine Models of Systems Properties and Processes > Mechanistic Models Models of Systems Properties and Processes > Organ, Tissue, and Physiological Models Models of Systems Properties and Processes > Organismal Models."
31258772,6.0,"Tracking knowledge evolution, hotspots and future directions of emerging technologies in cancers research: a bibliometrics review",2019 Jun 2;10(12):2643-2653.,"Due to various environmental pollution issues, cancers have become the ""first killer"" of human beings in the 21st century and their control has become a global strategy of human health. The increasing development of emerging information technologies has provided opportunities for prevention, early detection, diagnosis, intervention, prognosis, nursing, and rehabilitation of cancers. In recent years, the literature associated with emerging technologies in cancer has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of this field. To explore the dynamic knowledge evolution of emerging information technologies in cancer literature, we comprehensively analyzed the development status and research hotspots in this field from bibliometrics perspective. We collected 7,136 articles (2000-2017) from the Web of Science database and visually displayed the dynamic knowledge evolution process via the analysis on time-sequence changes, spatial distribution, knowledge base, and hotspots. Much institutional cooperation occurs in this field, and research groups are relatively concentrated. BMC Bioinformatics, PLOS One, Journal of Urology, Scientific Reports, and Bioinformatics are the top five journals in this field. Research hotspots are mainly concentrated in two dimensions: the disease dimension (e.g., cancer, breast cancer, and prostate cancer), and the technical dimension (e.g., robotics, machine learning, data mining, and etc.). The emerging technologies in cancer research is fast ascending and promising. This study also provides researchers with panoramic knowledge of this field, as well as research hotspots and future directions."
31257740,,Digital Medicine in Thyroidology: A New Era of Managing Thyroid Disease,2019 Jun;34(2):124-131.,"Digital medicine has the capacity to affect all aspects of medicine, including disease prediction, prevention, diagnosis, treatment, and post-treatment management. In the field of thyroidology, researchers are also investigating potential applications of digital technology for the thyroid disease. Recent studies using artificial intelligence (AI)/machine learning (ML) have reported reasonable performance for the classification of thyroid nodules based on ultrasonographic (US) images. AI/ML-based methods have also shown good diagnostic accuracy for distinguishing between benign and malignant thyroid lesions based on cytopathologic findings. Assistance from AI/ML methods could overcome the limitations of conventional thyroid US and fine-needle aspiration cytology. A web-based database has been developed for thyroid cancer care. In addition to its role as a nationwide registry of thyroid cancer, it is expected to serve as a clinical platform to facilitate better thyroid cancer care and as a research platform providing comprehensive disease-specific big data. Evidence has been found that biosignal monitoring with wearable devices may predict thyroid dysfunction. This real-world thyroid function monitoring could aid in the management and early detection of thyroid dysfunction. In the thyroidology field, research involving the range of digital medicine technologies and their clinical applications is expected to be even more active in the future."
31257314,4.0,Utilization of Artificial Intelligence in Echocardiography,2019 Jul 25;83(8):1623-1629.,"Echocardiography has a central role in the diagnosis and management of cardiovascular disease. Precise and reliable echocardiographic assessment is required for clinical decision-making. Even if the development of new technologies (3-dimentional echocardiography, speckle-tracking, semi-automated analysis, etc.), the final decision on analysis is strongly dependent on operator experience. Diagnostic errors are a major unresolved problem. Moreover, not only can cardiologists differ from one another in image interpretation, but also the same observer may come to different findings when a reading is repeated. Daily high workloads in clinical practice may lead to this error, and all cardiologists require precise perception in this field. Artificial intelligence (AI) has the potential to improve analysis and interpretation of medical images to a new stage compared with previous algorithms. From our comprehensive review, we believe AI has the potential to improve accuracy of diagnosis, clinical management, and patient care. Although there are several concerns about the required large dataset and ""black box"" algorithm, AI can provide satisfactory results in this field. In the future, it will be necessary for cardiologists to adapt their daily practice to incorporate AI in this new stage of echocardiography."
31256388,4.0,Emerging Methods to Objectively Assess Pruritus in Atopic Dermatitis,2019 Sep;9(3):407-420.,"Introduction:                    Atopic dermatitis (AD) is an inflammatory skin disease with a chronic, relapsing course. Clinical features of AD vary by age, duration, and severity but can include papules, vesicles, erythema, exudate, xerosis, scaling, and lichenification. However, the most defining and universal symptom of AD is pruritus. Pruritus or itch, defined as an unpleasant urge to scratch, is problematic for many reasons, particularly its negative impact on quality of life. Despite the profoundly negative impact of pruritus on patients with AD, clinicians and researchers lack standardized and validated methods to objectively measure pruritus. The purpose of this review is to discuss emerging methods to assess pruritus in AD by describing objective patient-centered tools developed or enhanced over the last decade that can be utilized by clinicians and researchers alike.              Methods:                    This review is based on a literature search in Medline, Embase, and Web of Science databases. The search was performed in February 2019. The keywords were used ""pruritus,"" ""itch,"" ""atopic dermatitis,"" ""eczema,"" ""measurements,"" ""tools,"" ""instruments,"" ""accelerometer,"" ""wrist actigraphy,"" ""smartwatch,"" ""transducer,"" ""vibration,"" ""brain mapping,"" ""magnetic resonance imaging,"" and ""positron emission tomography."" Only articles written in English were included, and no restrictions were set on study type. To focus on emerging methods, prioritization was given to results from the last decade (2009-2019).              Results:                    The search yielded 49 results in PubMed, 134 results in Embase, and 85 results in Web of Science. Each result was independently reviewed in a standardized manner by two of the authors (M.S., K.L.), and disagreements between reviewers were resolved by consensus. Relevant findings were categorized into the following sections: video surveillance, acoustic surveillance, wrist actigraphy, smart devices, vibration transducers, and neurological imaging. Examples are provided along with descriptions of how each technology works, instances of use in research or clinical practice, and as applicable, reports of validation studies and correlation with other methods.              Conclusion:                    The variety of new and improved methods to evaluate pruritus in AD is welcomed by clinicians, researchers, and patients alike. Future directions include next-generation smart devices as well as exploring new territories, such as identifying biomarkers that correlate to itch and machine-learning programs to identify itch processing in the brain. As these efforts continue, it will be essential to remain patient-centered by developing techniques that minimize discomfort, respect privacy, and provide accurate data that can be used to better manage itch in AD."
31255749,6.0,Assessing the effectiveness of artificial intelligence methods for melanoma: A retrospective review,2019 Nov;81(5):1176-1180.,"Background:                    Artificial intelligence methods for the classification of melanoma have been studied extensively. However, few studies compare these methods under the same standards.              Objective:                    To seek the best artificial intelligence method for diagnosis of melanoma.              Methods:                    The contrast test used 2200 dermoscopic images. Image segmentations, feature extractions, and classifications were performed in sequence for evaluation of traditional machine learning algorithms. The recent popular convolutional neural network frameworks were used for transfer learning training classification.              Results:                    The region growing algorithm has the best segmentation performance, with an intersection over union of 70.06% and a false-positive rate of 17.67%. Classification performance was better with logistic regression, with a sensitivity of 76.36% and a specificity of 87.04%. The Inception V3 model (Google, Mountain View, CA) worked best in deep learning algorithms: the accuracy was 93.74%, the sensitivity was 94.36%, and the specificity was 85.64%.              Limitations:                    There was no division in the severity of melanoma samples used in this experiment. The data set was relatively small for deep learning.              Conclusion:                    The performance of traditional machine learning is satisfactory for the small data set of melanoma dermoscopic images, and the potential for deep learning in the future big data era is enormous."
31254491,3.0,Imaging Quality Control in the Era of Artificial Intelligence,2019 Sep;16(9 Pt B):1259-1266.,"The advent of artificial intelligence (AI) promises to have a transformational impact on quality in medicine, including in radiology. However, experience has shown that quality tools alone are often not sufficient to bring about consistent excellent performance. Specifically, rather than assuming outcome targets are consistently met, in quality control, managers assume that wide variation is likely present unless proven otherwise with objective performance data. In this article, we discuss what we consider to be the eight essential elements required to achieve comprehensive process control, necessary to deliver consistent quality in radiology: a process control framework, performance measures, performance standards and targets, monitoring applications, prediction models, optimization models, feedback mechanisms, and accountability mechanisms. We consider these elements to be universally applicable, including in the application of AI-based models. We also discuss how the lack of specific elements of a quality control program can hinder widespread quality control efforts. We illustrate the concept using the example of a CT radiation dose optimization and process control program previously developed by one of the authors and provide several examples of how AI-based tools might be used for quality control in radiology."
31254036,11.0,Next generation research applications for hybrid PET/MR and PET/CT imaging using deep learning,2019 Dec;46(13):2700-2707.,"Introduction:                    Recently there have been significant advances in the field of machine learning and artificial intelligence (AI) centered around imaging-based applications such as computer vision. In particular, the tremendous power of deep learning algorithms, primarily based on convolutional neural network strategies, is becoming increasingly apparent and has already had direct impact on the fields of radiology and nuclear medicine. While most early applications of computer vision to radiological imaging have focused on classification of images into disease categories, it is also possible to use these methods to improve image quality. Hybrid imaging approaches, such as PET/MRI and PET/CT, are ideal for applying these methods.              Methods:                    This review will give an overview of the application of AI to improve image quality for PET imaging directly and how the additional use of anatomic information from CT and MRI can lead to further benefits. For PET, these performance gains can be used to shorten imaging scan times, with improvement in patient comfort and motion artifacts, or to push towards lower radiotracer doses. It also opens the possibilities for dual tracer studies, more frequent follow-up examinations, and new imaging indications. How to assess quality and the potential effects of bias in training and testing sets will be discussed.              Conclusion:                    Harnessing the power of these new technologies to extract maximal information from hybrid PET imaging will open up new vistas for both research and clinical applications with associated benefits in patient care."
31253449,5.0,Artificial Intelligence and Machine Learning in Lower Extremity Arthroplasty: A Review,2019 Oct;34(10):2201-2203.,"Background:                    Driven by the rapid development of big data and processing power, artificial intelligence and machine learning (ML) applications are poised to expand orthopedic surgery frontiers. Lower extremity arthroplasty is uniquely positioned to most dramatically benefit from ML applications given its central role in alternative payment models and the value equation.              Methods:                    In this report, we discuss the origins and model specifics behind machine learning, consider its progression into healthcare, and present some of its most recent advances and applications in arthroplasty.              Results:                    A narrative review of artificial intelligence and ML developments is summarized with specific applications to lower extremity arthroplasty, with specific lessons learned from osteoarthritis gait models, joint-specific imaging analysis, and value-based payment models.              Conclusion:                    The advancement and employment of ML provides an opportunity to provide data-driven, high performance medicine that can rapidly improve the science, economics, and delivery of lower extremity arthroplasty."
31249591,14.0,Inferring Interaction Networks From Multi-Omics Data,2019 Jun 12;10:535.,"A major goal in systems biology is a comprehensive description of the entirety of all complex interactions between different types of biomolecules-also referred to as the interactome-and how these interactions give rise to higher, cellular and organism level functions or diseases. Numerous efforts have been undertaken to define such interactomes experimentally, for example yeast-two-hybrid based protein-protein interaction networks or ChIP-seq based protein-DNA interactions for individual proteins. To complement these direct measurements, genome-scale quantitative multi-omics data (transcriptomics, proteomics, metabolomics, etc.) enable researchers to predict novel functional interactions between molecular species. Moreover, these data allow to distinguish relevant functional from non-functional interactions in specific biological contexts. However, integration of multi-omics data is not straight forward due to their heterogeneity. Numerous methods for the inference of interaction networks from homogeneous functional data exist, but with the advent of large-scale paired multi-omics data a new class of methods for inferring comprehensive networks across different molecular species began to emerge. Here we review state-of-the-art techniques for inferring the topology of interaction networks from functional multi-omics data, encompassing graphical models with multiple node types and quantitative-trait-loci (QTL) based approaches. In addition, we will discuss Bayesian aspects of network inference, which allow for leveraging already established biological information such as known protein-protein or protein-DNA interactions, to guide the inference process."
31248976,6.0,How accurate are suicide risk prediction models? Asking the right questions for clinical practice,2019 Aug;22(3):125-128.,"Prediction models assist in stratifying and quantifying an individual's risk of developing a particular adverse outcome, and are widely used in cardiovascular and cancer medicine. Whether these approaches are accurate in predicting self-harm and suicide has been questioned. We searched for systematic reviews in the suicide risk assessment field, and identified three recent reviews that have examined current tools and models derived using machine learning approaches. In this clinical review, we present a critical appraisal of these reviews, and highlight three major limitations that are shared between them. First, structured tools are not compared with unstructured assessments routine in clinical practice. Second, they do not sufficiently consider a range of performance measures, including negative predictive value and calibration. Third, the potential role of these models as clinical adjuncts is not taken into consideration. We conclude by presenting the view that the current role of prediction models for self-harm and suicide is currently not known, and discuss some methodological issues and implications of some machine learning and other analytic techniques for clinical utility."
31246909,5.0,A review of early warning systems for prompt detection of patients at risk for clinical decline,2019 Jul;87(1S Suppl 1):S67-S73.,"Early Warning Scores (EWS) are a composite evaluation of a patient's basic physiology, changes of which are the first indicators of clinical decline and are used to prompt further patient assessment and when indicated intervention. These are sometimes referred to as ""track and triggers systems"" with tracking meant to denote periodic observation of physiology and trigger being a predetermined response criteria. This review article examines the most widely used EWS, with special attention paid to those used in military and trauma populations.The earliest EWS is the Modified Early Earning Score (MEWS). In MEWS, points are allocated to vital signs based on their degree of abnormality, and summed to yield an aggregate score. A score above a threshold would elicit a clinical response such as a rapid response team. Modified Early Earning Score was subsequently followed up with the United Kingdom's National Early Warning Score, the electronic cardiac arrest triage score, and the 10 Signs of Vitality score, among others.Severity of illness indicators have been in military and civilian trauma populations, such as the Revised Trauma Score, Injury Severity Score, and Trauma and Injury Severity. The sequential organ failure assessment score and its attenuated version quick sequential organ failure assessment were developed to aggressively identify patients near septic shock.Effective EWS have certain characteristics. First, they should accurately capture vital signs information. Second, almost all data should be derived electronically rather than manually. Third, the measurements should take into consideration multiple organ systems. Finally, information that goes into an EWS must be captured in a timely manner. Future trends include the use of machine learning to detect subtle changes in physiology and the inclusion of data from biomarkers. As EWS improve, they will be more broadly used in both military and civilian environments. LEVEL OF EVIDENCE: Review article, level I."
31246376,2.0,A review of approaches for analysing obstructive sleep apnoea-related patterns in pulse oximetry data,2020 May;25(5):475-485.,"Overnight pulse oximetry allows the relatively non-invasive estimation of peripheral blood haemoglobin oxygen saturations (SpO2 ), and forms part of the typical polysomnogram (PSG) for investigation of obstructive sleep apnoea (OSA). While the raw SpO2 signal can provide detailed information about OSA-related pathophysiology, this information is typically summarized with simple statistics such as the oxygen desaturation index (ODI, number of desaturations per hour). As such, this study reviews the technical methods for quantifying OSA-related patterns in oximetry data. The technical methods described in literature can be broadly grouped into four categories: (i) Describing the detailed characteristics of desaturations events; (ii) Time series statistics; (iii) Analysis of power spectral distribution (i.e. frequency domain analysis); and (d) Non-linear analysis. These are described and illustrated with examples of oximetry traces. The utilization of these techniques is then described in two applications. First, the application of detailed oximetry analysis allows the accurate automated classification of PSG-defined OSA. Second, quantifications which better characterize the severity of desaturation events are better predictors of OSA-related epidemiological outcomes than standard clinical metrics. Finally, methodological considerations and further applications and opportunities are considered."
31243141,5.0,From observing to predicting single-cell structure and function with high-throughput/high-content microscopy,2019 Jul 3;63(2):197-208.,"In the past 15 years, cell-based microscopy has evolved its focus from observing cell function to aiming to predict it. In particular-powered by breakthroughs in computer vision, large-scale image analysis and machine learning-high-throughput and high-content microscopy imaging have enabled to uniquely harness single-cell information to systematically discover and annotate genes and regulatory pathways, uncover systems-level interactions and causal links between cellular processes, and begin to clarify and predict causal cellular behaviour and decision making. Here we review these developments, discuss emerging trends in the field, and describe how single-cell 'omics and single-cell microscopy are imminently in an intersecting trajectory. The marriage of these two fields will make possible an unprecedented understanding of cell and tissue behaviour and function."
31241024,,Fuzzy Classification Methods Based Diagnosis of Parkinson's disease from Speech Test Cases,2019;12(2):100-120.,"Background:                    Together with the Alzheimer's disease, Parkinson's disease is considered as one of the two serious known neurodegenerative diseases. Physicians find it hard to predict whether a given patient has already developed or is expected to develop the Parkinson's disease in the future. To overcome this difficulty, it is possible to develop a computing model, which analyzes the data related to a given patient and predicts with acceptable accuracy when he/she is anticipated to develop the Parkinson's disease.              Objectives:                    This paper contributes an attractive prediction framework based on some machine learning approaches for distinguishing people with Parkinsonism from healthy individuals.              Methods:                    Several fuzzy classifiers such as Inductive Fuzzy Classifier, Fuzzy Rough Classifier and two types of neuro-fuzzy classifiers have been employed.              Results:                    The fuzzy classifiers utilized in this study have been tested using the ""Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set"" of 40 subjects available on the UCI repository.              Conclusion:                    The results achieved show that FURIA, MLP- Bagging - SGD, genfis2 and scg1 performed the best among the fuzzy rough, WEKA, adaptive neuro-fuzzy and neuro-fuzzy classifiers, respectively. The worst performance belongs to nearest neighborhood, IBK, genfis3 and scg3 among the formerly mentioned classifiers. The results reported in this paper are better in comparison to the results reported in Sakar et al., where the same dataset was used, with utilization of different classifiers. This demonstrates the applicability and effectiveness of the fuzzy classifiers used in this study as compared to the non-fuzzy classifiers used by Sakar et al."
31240330,32.0,"Radiomics in nuclear medicine: robustness, reproducibility, standardization, and how to avoid data analysis traps and replication crisis",2019 Dec;46(13):2638-2655.,"Radiomics in nuclear medicine is rapidly expanding. Reproducibility of radiomics studies in multicentre settings is an important criterion for clinical translation. We therefore performed a meta-analysis to investigate reproducibility of radiomics biomarkers in PET imaging and to obtain quantitative information regarding their sensitivity to variations in various imaging and radiomics-related factors as well as their inherent sensitivity. Additionally, we identify and describe data analysis pitfalls that affect the reproducibility and generalizability of radiomics studies. After a systematic literature search, 42 studies were included in the qualitative synthesis, and data from 21 were used for the quantitative meta-analysis. Data concerning measurement agreement and reliability were collected for 21 of 38 different factors associated with image acquisition, reconstruction, segmentation and radiomics-specific processing steps. Variations in voxel size, segmentation and several reconstruction parameters strongly affected reproducibility, but the level of evidence remained weak. Based on the meta-analysis, we also assessed inherent sensitivity to variations of 110 PET image biomarkers. SUVmean and SUVmax were found to be reliable, whereas image biomarkers based on the neighbourhood grey tone difference matrix and most biomarkers based on the size zone matrix were found to be highly sensitive to variations, and should be used with care in multicentre settings. Lastly, we identify 11 data analysis pitfalls. These pitfalls concern model validation and information leakage during model development, but also relate to reporting and the software used for data analysis. Avoiding such pitfalls is essential for minimizing bias in the results and to enable reproduction and validation of radiomics studies."
31239834,,A Review of Characterization Approaches for Smallholder Farmers: Towards Predictive Farm Typologies,2019 May 22;2019:6121467.,"Characterization of smallholder farmers has been conducted in various researches by using machine learning algorithms, participatory and expert-based methods. All approaches used end up with the development of some subgroups known as farm typologies. The main purpose of this paper is to highlight the main approaches used to characterize smallholder farmers, presenting the pros and cons of the approaches. By understanding the nature and key advantages of the reviewed approaches, the paper recommends a hybrid approach towards having predictive farm typologies. Search of relevant research articles published between 2007 and 2018 was done on ScienceDirect and Google Scholar. By using a generated search query, 20 research articles related to characterization of smallholder farmers were retained. Cluster-based algorithms appeared to be the mostly used in characterizing smallholder farmers. However, being highly unpredictable and inconsistent, use of clustering methods calls in for a discussion on how well the developed farm typologies can be used to predict future trends of the farmers. A thorough discussion is presented and recommends use of supervised models to validate unsupervised models. In order to achieve predictive farm typologies, three stages in characterization are recommended as tested in smallholder dairy farmers datasets: (a) develop farm types from a comparative analysis of more than two unsupervised learning algorithms by using training models, (b) assess the training models' robustness in predicting farm types for a testing dataset, and (c) assess the predictive power of the developed farm types from each algorithm by predicting the trend of several response variables."
31235303,2.0,Artificial Intelligence: Can Information be Transformed into Intelligence in Surgical Education?,2019 Aug;29(3):339-350.,"Artificial intelligence (AI) is being rapidly integrated into various medical applications. Although early application of AI has been achieved in image-based, as well as statistical computational models, translation into procedure-based specialties such as surgery may take longer to achieve. A potential application of AI in surgical education is as a teaching coach or mentor that interacts with the used via virtual and/or augmented reality. The question arises as to whether machines will achieve the wisdom and intelligence of human educators."
31232610,2.0,Clinical pharmacology of old age,2019 Aug;12(8):749-755.,"Introduction: With the majority of elderly persons consuming multiple drugs, inappropriate drug use is a major issue in geriatric medicine. Areas covered: We reviewed PubMed, Embase, and Cochrane from inception to 1 May 2019 for potentially inappropriate use of medications, polypharmacy, and age-dependent changes in pharmacokinetics and pharmacodynamics. We selected to highlight new aspects that have emerged in recent years: appropriate monitoring of drug adherence and the introduction of Big Data analysis in advancing geriatric pharmacology. Expert opinion: There are major gaps in the pharmacological treatment of the elderly. Most drugs were designed and tested in adults, with no pharmacokinetic and pharmacodynamic data on changes in old age. This void must be corrected through systematic and well-designed research programs. Potentially inappropriate use of medications (PIM) in the elderly is a serious issue in advanced age. Analysis of PIM shows relatively low predictive value in real life medicine. Most physicians continue to prescribe to the elderly medicines which should not be given at all, or not combined. Polypharmacy is a complex issue in old age, and in many cases treating physicians are not conducting critical assessment of the need for numerous medications."
31230937,2.0,Best practices in digital health literacy,2019 Oct 1;292:277-279.,"The connection between health literacy and health outcomes includes access and utilization of healthcare services, patient/provider interaction and self-care. Digital approaches can be designed to simplify or expand on a concept, test for understanding, and do not have a time constraint. New technologies, such as artificial intelligence and machine learning, virtual and augmented reality, and blockchain can move the role of technology beyond data collection to a more integrated system. Rather than being a passive participant, digital solutions provide the opportunity for the individual to be an active participant in their health. These solutions can be delivered in a way that builds or enhances the individual's belief that the plan will be successful and more confidence that they can stick with it. Digital solutions allow for the delivery of multi-media education, such as videos, voice, and print, at different reading levels, in multiple languages, using formal and informal teaching methods. By giving the patient a greater voice and empowering them to be active participants in their care, they can develop their decision making and shared decision making skills. The first step in our health literacy instructional model is to address the emotional state of the person. Once the emotional state has been addressed, and an engagement strategy has been deployed the final phase is the delivery of an educational solution. While a clear definition of health literacy and an instructional model are important, further research must be done to continually determine more effective ways to incorporate health technology in the process of improving health outcomes."
31229952,9.0,EULAR points to consider for the use of big data in rheumatic and musculoskeletal diseases,2020 Jan;79(1):69-76.,"Background:                    Tremendous opportunities for health research have been unlocked by the recent expansion of big data and artificial intelligence. However, this is an emergent area where recommendations for optimal use and implementation are needed. The objective of these European League Against Rheumatism (EULAR) points to consider is to guide the collection, analysis and use of big data in rheumatic and musculoskeletal disorders (RMDs).              Methods:                    A multidisciplinary task force of 14 international experts was assembled with expertise from a range of disciplines including computer science and artificial intelligence. Based on a literature review of the current status of big data in RMDs and in other fields of medicine, points to consider were formulated. Levels of evidence and strengths of recommendations were allocated and mean levels of agreement of the task force members were calculated.              Results:                    Three overarching principles and 10 points to consider were formulated. The overarching principles address ethical and general principles for dealing with big data in RMDs. The points to consider cover aspects of data sources and data collection, privacy by design, data platforms, data sharing and data analyses, in particular through artificial intelligence and machine learning. Furthermore, the points to consider state that big data is a moving field in need of adequate reporting of methods and benchmarking, careful data interpretation and implementation in clinical practice.              Conclusion:                    These EULAR points to consider discuss essential issues and provide a framework for the use of big data in RMDs."
31229667,3.0,Role of deep learning in infant brain MRI analysis,2019 Dec;64:171-189.,"Deep learning algorithms and in particular convolutional networks have shown tremendous success in medical image analysis applications, though relatively few methods have been applied to infant MRI data due numerous inherent challenges such as inhomogenous tissue appearance across the image, considerable image intensity variability across the first year of life, and a low signal to noise setting. This paper presents methods addressing these challenges in two selected applications, specifically infant brain tissue segmentation at the isointense stage and presymptomatic disease prediction in neurodevelopmental disorders. Corresponding methods are reviewed and compared, and open issues are identified, namely low data size restrictions, class imbalance problems, and lack of interpretation of the resulting deep learning solutions. We discuss how existing solutions can be adapted to approach these issues as well as how generative models seem to be a particularly strong contender to address them."
