pmid,title,text,date,citations
31815397,Deep Learning in Omics Data Analysis and Precision Medicine,"The rise of omics techniques has resulted in an explosion of molecular data in modern biomedical research. Together with information from medical images and clinical data, the field of omics has driven the implementation of personalized medicine. Biomedical and omics datasets are complex and heterogeneous, and extracting meaningful knowledge from this vast amount of information is by far the most important challenge for bioinformatics and machine learning researchers. In this context, there is an increasing interest in the potential of deep learning (DL) methods to create predictive models and to identify complex patterns from these large datasets. This chapter provides an overview of the main applications of DL methods in biomedical research, with focus on omics data analysis and precision medicine applications. DL algorithms and the most popular architectures are introduced first. This is followed by a review of some of the main applications and problems approached by DL in omics data and medical image analysis. Finally, implementations for improving the diagnosis, treatment, and classification of complex diseases are discussed.",,
31816343,Supervised and unsupervised algorithms for bioinformatics and data science,"Bioinformatics refers to an ever evolving huge field of research based on millions of algorithms, designated to several data banks. Such algorithms are either supervised or unsupervised. In this article, a detailed overview of the supervised and unsupervised techniques is presented with the aid of examples. The aim of this article is to provide the readers with the basic understanding of the state of the art models, which are key ingredients of explainable machine learning in the field of bioinformatics.",2020 Mar;151:14-22.,
31815564,Perspectives on the current developments with neuromodulation for the treatment of epilepsy,"Introduction: As deep brain stimulation revolutionized the treatment of movement disorders in the late 80s, neuromodulation in the treatment of epilepsy will undoubtedly undergo transformative changes in the years to come with the exponential growth of technological development moving into mainstream practice; the appearance of companies such as Facebook, Google, Neuralink within the realm of brain-computer interfaces points to this trend.Areas covered: This perspective piece will talk about the history of brain stimulation in epilepsy, current-approved treatments, technical developments and the future of neurostimulation.Expert opinion: Further understanding of the brain alongside machine learning and innovative technology will be the future of neuromodulation for the treatment of epilepsy. All of these innovations and advances should pave the way toward overcoming the vexing underutilization of surgery in the therapeutic armamentarium against medically refractory seizures, given the implicit advantage of a neuromodulatory rather than neurodestructive approach.",2020 Feb;20(2):189-194.,1.0
31815179,Bridging the gap between military prolonged field care monitoring and exploration spaceflight: the compensatory reserve,"The concept of prolonged field care (PFC), or medical care applied beyond doctrinal planning timelines, is the top priority capability gap across the US Army. PFC is the idea that combat medics must be prepared to provide medical care to serious casualties in the field without the support of robust medical infrastructure or resources in the event of delayed medical evacuation. With limited resources, significant distances to travel before definitive care, and an inability to evacuate in a timely fashion, medical care during exploration spaceflight constitutes the ultimate example PFC. One of the main capability gaps for PFC in both military and spaceflight settings is the need for technologies for individualized monitoring of a patient's physiological status. A monitoring capability known as the compensatory reserve measurement (CRM) meets such a requirement. CRM is a small, portable, wearable technology that uses a machine learning and feature extraction-based algorithm to assess real-time changes in hundreds of specific features of arterial waveforms. Future development and advancement of CRM still faces engineering challenges to develop ruggedized wearable sensors that can measure waveforms for determining CRM from multiple sites on the body and account for less than optimal conditions (sweat, water, dirt, blood, movement, etc.). We show here the utility of a military wearable technology, CRM, which can be translated to space exploration.",2019 Dec 4;5:29.,1.0
31813895,Analysis Method of the Ion Current-Time Waveform Obtained from Low Aspect Ratio Solid-state Nanopores,"Low aspect ratio nanopores are expected to be applied to the detection of viruses and bacteria because of their high spatial resolution. Multiphysics simulations have revealed that the ion current-time waveform obtained from low aspect ratio nanopores contains information on not only the volume of viruses and bacteria, but also the structure, surface charge, and flow dynamics. Analysis using machine learning extracts information about these analytes from the ion current-time waveform. The combination of low aspect ratio nanopores, multiphysics simulation, and machine learning has made it possible to distinguish different types of viruses and bacteria with high accuracy.",2020 Feb 10;36(2):161-175.,
31812921,Recent advances on constraint-based models by integrating machine learning,"Research that meaningfully integrates constraint-based modeling with machine learning is at its infancy but holds much promise. Here, we consider where machine learning has been implemented within the constraint-based modeling reconstruction framework and highlight the need to develop approaches that can identify meaningful features from large-scale data and connect them to biological mechanisms to establish causality to connect genotype to phenotype. We motivate the construction of iterative integrative schemes where machine learning can fine-tune the input constraints in a constraint-based model or contrarily, constraint-based model simulation results are analyzed by machine learning and reconciled with experimental data. This can iteratively refine a constraint-based model until there is consistency between experimental data, machine learning results, and constraint-based model simulations.",2020 Aug;64:85-91.,3.0
31811431,Deep learning: definition and perspectives for thoracic imaging,"Relevance and penetration of machine learning in clinical practice is a recent phenomenon with multiple applications being currently under development. Deep learning-and especially convolutional neural networks (CNNs)-is a subset of machine learning, which has recently entered the field of thoracic imaging. The structure of neural networks, organized in multiple layers, allows them to address complex tasks. For several clinical situations, CNNs have demonstrated superior performance as compared with classical machine learning algorithms and in some cases achieved comparable or better performance than clinical experts. Chest radiography, a high-volume procedure, is a natural application domain because of the large amount of stored images and reports facilitating the training of deep learning algorithms. Several algorithms for automated reporting have been developed. The training of deep learning algorithm CT images is more complex due to the dimension, variability, and complexity of the 3D signal. The role of these methods is likely to increase in clinical practice as a complement of the radiologist's expertise. The objective of this review is to provide definitions for understanding the methods and their potential applications for thoracic imaging. KEY POINTS: • Deep learning outperforms other machine learning techniques for number of tasks in radiology. • Convolutional neural network is the most popular deep learning architecture in medical imaging. • Numerous deep learning algorithms are being currently developed; some of them may become part of clinical routine in the near future.",2020 Apr;30(4):2021-2030.,7.0
31810541,Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases,"Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.",2019 Aug;33(4):101429.,
31809864,Functional Neuroimaging in the New Era of Big Data,"The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.",2019 Aug;17(4):393-401.,2.0
31806421,The pathogenesis of systemic lupus erythematosus: Harnessing big data to understand the molecular basis of lupus,"Systemic lupus erythematosus (SLE) is a chronic, systemic autoimmune disease that causes damage to multiple organ systems. Despite decades of research and available murine models that capture some aspects of the human disease, new treatments for SLE lag behind other autoimmune diseases such as Rheumatoid Arthritis and Crohn's disease. Big data genomic assays have transformed our understanding of SLE by providing important insights into the molecular heterogeneity of this multigenic disease. Gene wide association studies have demonstrated more than 100 risk loci, supporting a model of multiple genetic hits increasing SLE risk in a non-linear fashion, and providing evidence of ancestral diversity in susceptibility loci. Epigenetic studies to determine the role of methylation, acetylation and non-coding RNAs have provided new understanding of the modulation of gene expression in SLE patients and identified new drug targets and biomarkers for SLE. Gene expression profiling has led to a greater understanding of the role of myeloid cells in the pathogenesis of SLE, confirmed roles for T and B cells in SLE, promoted clinical trials based on the prominent interferon signature found in SLE patients, and identified candidate biomarkers and cellular signatures to further drug development and drug repurposing. Gene expression studies are advancing our understanding of the underlying molecular heterogeneity in SLE and providing hope that patient stratification will expedite new therapies based on personal molecular signatures. Although big data analyses present unique interpretation challenges, both computationally and biologically, advances in machine learning applications may facilitate the ability to predict changes in SLE disease activity and optimize therapeutic strategies.",2020 Jun;110:102359.,16.0
31804782,Assessing the Accuracy of Machine-Assisted Abstract Screening With DistillerAI: A User Study [Internet],"Background:                    Web applications that employ natural language processing technologies such as text mining and text classification to support systematic reviewers during abstract screening have become more user friendly and more common. Such semi-automated screening tools can increase efficiency by reducing the number of abstracts needed to screen or by replacing one screener after adequately training the algorithm of the machine. Savings in workload between 30 percent and 70 percent might be possible with the use of such tools. The goal of our project was to conduct a case study to explore a screening approach that temporarily replaces a human screener with a semi-automated screening tool.              Methods:                    To address our objective, we evaluated the accuracy of a machine-assisted screening approach using an Agency for Healthcare Research and Quality comparative effectiveness review as the reference standard. We chose DistillerAI as a semi-automated screening tool for our project, applying its naïve Bayesian machine-learning option. Five teams screened the same 2,472 abstracts in parallel, using the machine-assisted approach. Each team trained DistillerAI with 300 randomly selected abstracts that the team screened dually. For the remaining 2,172 abstracts, DistillerAI replaced one human screener in each team and provided predictions about the relevance of records. We used a prediction score of 0.5 (i.e., inconclusive) or greater to classify a record as an inclusion. A single reviewer also screened all remaining abstracts. A second human screener resolved conflicts between the single reviewer and DistillerAI. We compared the decisions of the machine-assisted approach, single-reviewer screening (i.e., no machine assistance), and screening with DistillerAI alone (i.e., no human involvement after training) against the reference standard and calculated sensitivities, specificities, and the area under the receiver operating characteristics curve. In addition, we determined the interrater agreement, the proportion of included abstracts, and the number of conflicts between human screeners and DistillerAI.              Results:                    The mean sensitivity of the machine-assisted screening approach across the five screening teams was 78 percent (95% confidence interval [CI], 66% to 90%), and the mean specificity was 95 percent (95% CI, 92% to 97%). By comparison, the sensitivity of single-reviewer screening was also 78 percent (95% CI, 66% to 89%); the sensitivity of DistillerAI alone was 14 percent (95% CI, 0% to 31%). Specificities for single-reviewer screening and DistillerAI alone were 94 percent (95% CI, 91% to 97%) and 98 percent (95% CI, 97% to 100%), respectively. Machine-assisted screening and single-reviewer screening had similar areas under the curve (0.87 and 0.86, respectively); by contrast, the area under the curve for DistillerAI alone was just slightly better than chance (0.56). The interrater agreement between human screeners and DistillerAI with a prevalence-adjusted kappa was 0.85 (95% CI, 0.84 to 0.86).              Discussion:                    Findings of our study indicate that the accuracy of DistillerAI is not yet adequate to replace a human screener temporarily during abstract screening. The approach that we tested missed too many relevant studies and created too many conflicts between human screeners and DistillerAI. Rapid reviews, which do not require detecting the totality of the relevant evidence, may find semi-automation tools to have greater utility than traditional systematic reviews.",,
31803127,Quantifying the Metabolic Signature of Multiple Sclerosis by in vivo Proton Magnetic Resonance Spectroscopy: Current Challenges and Future Outlook in the Translation From Proton Signal to Diagnostic Biomarker,"Proton magnetic resonance spectroscopy (1H-MRS) offers a growing variety of methods for querying potential diagnostic biomarkers of multiple sclerosis in living central nervous system tissue. For the past three decades, 1H-MRS has enabled the acquisition of a rich dataset suggestive of numerous metabolic alterations in lesions, normal-appearing white matter, gray matter, and spinal cord of individuals with multiple sclerosis, but this body of information is not free of seeming internal contradiction. The use of 1H-MRS signals as diagnostic biomarkers depends on reproducible and generalizable sensitivity and specificity to disease state that can be confounded by a multitude of influences, including experiment group classification and demographics; acquisition sequence; spectral quality and quantifiability; the contribution of macromolecules and lipids to the spectroscopic baseline; spectral quantification pipeline; voxel tissue and lesion composition; T 1 and T 2 relaxation; B1 field characteristics; and other features of study design, spectral acquisition and processing, and metabolite quantification about which the experimenter may possess imperfect or incomplete information. The direct comparison of 1H-MRS data from individuals with and without multiple sclerosis poses a special challenge in this regard, as several lines of evidence suggest that experimental cohorts may differ significantly in some of these parameters. We review the existing findings of in vivo 1H-MRS on central nervous system metabolic abnormalities in multiple sclerosis and its subtypes within the context of study design, spectral acquisition and processing, and metabolite quantification and offer an outlook on technical considerations, including the growing use of machine learning, by future investigations into diagnostic biomarkers of multiple sclerosis measurable by 1H-MRS.",2019 Nov 15;10:1173.,5.0
31799938,Counting Bites With Bits: Expert Workshop Addressing Calorie and Macronutrient Intake Monitoring,"Background:                    Conventional diet assessment approaches such as the 24-hour self-reported recall are burdensome, suffer from recall bias, and are inaccurate in estimating energy intake. Wearable sensor technology, coupled with advanced algorithms, is increasingly showing promise in its ability to capture behaviors that provide useful information for estimating calorie and macronutrient intake.              Objective:                    This paper aimed to summarize current technological approaches to monitoring energy intake on the basis of expert opinion from a workshop panel and to make recommendations to advance technology and algorithms to improve estimation of energy expenditure.              Methods:                    A 1-day invitational workshop sponsored by the National Science Foundation was held at Northwestern University. A total of 30 participants, including population health researchers, engineers, and intervention developers, from 6 universities and the National Institutes of Health participated in a panel discussing the state of evidence with regard to monitoring calorie intake and eating behaviors.              Results:                    Calorie monitoring using technological approaches can be characterized into 3 domains: (1) image-based sensing (eg, wearable and smartphone-based cameras combined with machine learning algorithms); (2) eating action unit (EAU) sensors (eg, to measure feeding gesture and chewing rate); and (3) biochemical measures (eg, serum and plasma metabolite concentrations). We discussed how each domain functions, provided examples of promising solutions, and highlighted potential challenges and opportunities in each domain. Image-based sensor research requires improved ground truth (context and known information about the foods), accurate food image segmentation and recognition algorithms, and reliable methods of estimating portion size. EAU-based domain research is limited by the understanding of when their systems (device and inference algorithm) succeed and fail, need for privacy-protecting methods of capturing ground truth, and uncertainty in food categorization. Although an exciting novel technology, the challenges of biochemical sensing range from a lack of adaptability to environmental effects (eg, temperature change) and mechanical impact, instability of wearable sensor performance over time, and single-use design.              Conclusions:                    Conventional approaches to calorie monitoring rely predominantly on self-reports. These approaches can gain contextual information from image-based and EAU-based domains that can map automatically captured food images to a food database and detect proxies that correlate with food volume and caloric intake. Although the continued development of advanced machine learning techniques will advance the accuracy of such wearables, biochemical sensing provides an electrochemical analysis of sweat using soft bioelectronics on human skin, enabling noninvasive measures of chemical compounds that provide insight into the digestive and endocrine systems. Future computing-based researchers should focus on reducing the burden of wearable sensors, aligning data across multiple devices, automating methods of data annotation, increasing rigor in studying system acceptability, increasing battery lifetime, and rigorously testing validity of the measure. Such research requires moving promising technological solutions from the controlled laboratory setting to the field.",2019 Dec 4;21(12):e14904.,1.0
31799423,"Integrating machine learning and multiscale modeling-perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences","Fueled by breakthrough technology developments, the biological, biomedical, and behavioral sciences are now collecting more data than ever before. There is a critical need for time- and cost-efficient strategies to analyze and interpret these data to advance human health. The recent rise of machine learning as a powerful technique to integrate multimodality, multifidelity data, and reveal correlations between intertwined phenomena presents a special opportunity in this regard. However, machine learning alone ignores the fundamental laws of physics and can result in ill-posed problems or non-physical solutions. Multiscale modeling is a successful strategy to integrate multiscale, multiphysics data and uncover mechanisms that explain the emergence of function. However, multiscale modeling alone often fails to efficiently combine large datasets from different sources and different levels of resolution. Here we demonstrate that machine learning and multiscale modeling can naturally complement each other to create robust predictive models that integrate the underlying physics to manage ill-posed problems and explore massive design spaces. We review the current literature, highlight applications and opportunities, address open questions, and discuss potential challenges and limitations in four overarching topical areas: ordinary differential equations, partial differential equations, data-driven approaches, and theory-driven approaches. Towards these goals, we leverage expertise in applied mathematics, computer science, computational biology, biophysics, biomechanics, engineering mechanics, experimentation, and medicine. Our multidisciplinary perspective suggests that integrating machine learning and multiscale modeling can provide new insights into disease mechanisms, help identify new targets and treatment strategies, and inform decision making for the benefit of human health.",2019 Nov 25;2:115.,24.0
31799422,Challenges of developing a digital scribe to reduce clinical documentation burden,"Clinicians spend a large amount of time on clinical documentation of patient encounters, often impacting quality of care and clinician satisfaction, and causing physician burnout. Advances in artificial intelligence (AI) and machine learning (ML) open the possibility of automating clinical documentation with digital scribes, using speech recognition to eliminate manual documentation by clinicians or medical scribes. However, developing a digital scribe is fraught with problems due to the complex nature of clinical environments and clinical conversations. This paper identifies and discusses major challenges associated with developing automated speech-based documentation in clinical settings: recording high-quality audio, converting audio to transcripts using speech recognition, inducing topic structure from conversation data, extracting medical concepts, generating clinically meaningful summaries of conversations, and obtaining clinical data for AI and ML algorithms.",2019 Nov 22;2:114.,5.0
31798635,Machine Learning for Cancer Immunotherapies Based on Epitope Recognition by T Cell Receptors,"In the last years, immunotherapies have shown tremendous success as treatments for multiple types of cancer. However, there are still many obstacles to overcome in order to increase response rates and identify effective therapies for every individual patient. Since there are many possibilities to boost a patient's immune response against a tumor and not all can be covered, this review is focused on T cell receptor-mediated therapies. CD8+ T cells can detect and destroy malignant cells by binding to peptides presented on cell surfaces by MHC (major histocompatibility complex) class I molecules. CD4+ T cells can also mediate powerful immune responses but their peptide recognition by MHC class II molecules is more complex, which is why the attention has been focused on CD8+ T cells. Therapies based on the power of T cells can, on the one hand, enhance T cell recognition by introducing TCRs that preferentially direct T cells to tumor sites (so called TCR-T therapy) or through vaccination to induce T cells in vivo. On the other hand, T cell activity can be improved by immune checkpoint inhibition or other means that help create a microenvironment favorable for cytotoxic T cell activity. The manifold ways in which the immune system and cancer interact with each other require not only the use of large omics datasets from gene, to transcript, to protein, and to peptide but also make the application of machine learning methods inevitable. Currently, discovering and selecting suitable TCRs is a very costly and work intensive in vitro process. To facilitate this process and to additionally allow for highly personalized therapies that can simultaneously target multiple patient-specific antigens, especially neoepitopes, breakthrough computational methods for predicting antigen presentation and TCR binding are urgently required. Particularly, potential cross-reactivity is a major consideration since off-target toxicity can pose a major threat to patient safety. The current speed at which not only datasets grow and are made available to the public, but also at which new machine learning methods evolve, is assuring that computational approaches will be able to help to solve problems that immunotherapies are still facing.",2019 Nov 19;10:1141.,3.0
31797321,Hybrid PET/MR imaging in myocardial inflammation post-myocardial infarction,"Hybrid PET/MR imaging is an emerging imaging modality combining positron emission tomography (PET) and magnetic resonance imaging (MRI) in the same system. Since the introduction of clinical PET/MRI in 2011, it has had some impact (e.g., imaging the components of inflammation in myocardial infarction), but its role could be much greater. Many opportunities remain unexplored and will be highlighted in this review. The inflammatory process post-myocardial infarction has many facets at a cellular level which may affect the outcome of the patient, specifically the effects on adverse left ventricular remodeling, and ultimately prognosis. The goal of inflammation imaging is to track the process non-invasively and quantitatively to determine the best therapeutic options for intervention and to monitor those therapies. While PET and MRI, acquired separately, can image aspects of inflammation, hybrid PET/MRI has the potential to advance imaging of myocardial inflammation. This review contains a description of hybrid PET/MRI, its application to inflammation imaging in myocardial infarction and the challenges, constraints, and opportunities in designing data collection protocols. Finally, this review explores opportunities in PET/MRI: improved registration, partial volume correction, machine learning, new approaches in the development of PET and MRI pulse sequences, and the use of novel injection strategies.",2020 Dec;27(6):2083-2099.,5.0
31796060,Text-mining clinically relevant cancer biomarkers for curation into the CIViC database,"Background:                    Precision oncology involves analysis of individual cancer samples to understand the genes and pathways involved in the development and progression of a cancer. To improve patient care, knowledge of diagnostic, prognostic, predisposing, and drug response markers is essential. Several knowledgebases have been created by different groups to collate evidence for these associations. These include the open-access Clinical Interpretation of Variants in Cancer (CIViC) knowledgebase. These databases rely on time-consuming manual curation from skilled experts who read and interpret the relevant biomedical literature.              Methods:                    To aid in this curation and provide the greatest coverage for these databases, particularly CIViC, we propose the use of text mining approaches to extract these clinically relevant biomarkers from all available published literature. To this end, a group of cancer genomics experts annotated sentences that discussed biomarkers with their clinical associations and achieved good inter-annotator agreement. We then used a supervised learning approach to construct the CIViCmine knowledgebase.              Results:                    We extracted 121,589 relevant sentences from PubMed abstracts and PubMed Central Open Access full-text papers. CIViCmine contains over 87,412 biomarkers associated with 8035 genes, 337 drugs, and 572 cancer types, representing 25,818 abstracts and 39,795 full-text publications.              Conclusions:                    Through integration with CIVIC, we provide a prioritized list of curatable clinically relevant cancer biomarkers as well as a resource that is valuable to other knowledgebases and precision cancer analysts in general. All data is publically available and distributed with a Creative Commons Zero license. The CIViCmine knowledgebase is available at http://bionlp.bcgsc.ca/civicmine/.",2019 Dec 3;11(1):78.,3.0
31795240,A Survey of Teleceptive Sensing for Wearable Assistive Robotic Devices,"Teleception is defined as sensing that occurs remotely, with no physical contact with the object being sensed. To emulate innate control systems of the human body, a control system for a semi- or fully autonomous assistive device not only requires feedforward models of desired movement, but also the environmental or contextual awareness that could be provided by teleception. Several recent publications present teleception modalities integrated into control systems and provide preliminary results, for example, for performing hand grasp prediction or endpoint control of an arm assistive device; and gait segmentation, forward prediction of desired locomotion mode, and activity-specific control of a prosthetic leg or exoskeleton. Collectively, several different approaches to incorporating teleception have been used, including sensor fusion, geometric segmentation, and machine learning. In this paper, we summarize the recent and ongoing published work in this promising new area of research.",2019 Nov 28;19(23):5238.,2.0
31795151,Estimating Biomechanical Time-Series with Wearable Sensors: A Systematic Review of Machine Learning Techniques,"Wearable sensors have the potential to enable comprehensive patient characterization and optimized clinical intervention. Critical to realizing this vision is accurate estimation of biomechanical time-series in daily-life, including joint, segment, and muscle kinetics and kinematics, from wearable sensor data. The use of physical models for estimation of these quantities often requires many wearable devices making practical implementation more difficult. However, regression techniques may provide a viable alternative by allowing the use of a reduced number of sensors for estimating biomechanical time-series. Herein, we review 46 articles that used regression algorithms to estimate joint, segment, and muscle kinematics and kinetics. We present a high-level comparison of the many different techniques identified and discuss the implications of our findings concerning practical implementation and further improving estimation accuracy. In particular, we found that several studies report the incorporation of domain knowledge often yielded superior performance. Further, most models were trained on small datasets in which case nonparametric regression often performed best. No models were open-sourced, and most were subject-specific and not validated on impaired populations. Future research should focus on developing open-source algorithms using complementary physics-based and machine learning techniques that are validated in clinically impaired populations. This approach may further improve estimation performance and reduce barriers to clinical adoption.",2019 Nov 28;19(23):5227.,1.0
31791160,Clinical utility of pre-endoscopy risk scores in upper gastrointestinal bleeding,"Introduction: Acute upper-gastrointestinal bleeding (AUGIB) is a common medical emergency, with an incidence of 103-172 per 100,000 in the United Kingdom (UK) and mortality of 2% to 10%. Early and accurate prediction of the severity of an AUGIB episode may help guide management, including in or outpatient management, level of care required, and timing of endoscopy. This article aims to address the clinical utility of the various pre-endoscopic risk assessment tools used in AUGIB.Areas covered: The authors undertook a literature review of the current evidence on the pre-endoscopic risk assessment scores. Additional the authors discuss the recently published novel risk assessment scores.Expert opinion: The evidence shows that GBS is the most clinically useful risk assessment score in correctly identifying very low-risk patients suitable for outpatient management. At present, research is ongoing to assess machine learning in the assessment of patients presenting with AUGIB. More research is needed but it shows promise for the future.",2019 Dec;13(12):1161-1167.,1.0
31790958,"Evolution, current challenges, and future possibilities in the objective assessment of aesthetic outcome of breast cancer locoregional treatment","The Breast Cancer overall survival rate has raised impressively in the last 20 years mainly due to improved screening and effectiveness of treatments. This increase in survival paralleled the awareness over the long-lasting impact of the side effects of treatments on patient quality of life, emphasizing the motto ""a longer but better life for breast cancer patients"". In breast cancer more strikingly than in other cancers, besides the side effects of systemic treatments, there is the visible impact of surgery and radiotherapy on patients' body image. This has sparked interest on the development of tools for the aesthetic evaluation of Breast Cancer locoregional treatments, which evolved from manual, subjective approaches to computerized, automated solutions. However, although studied for almost four decades, past solutions were not mature enough to become a standard. Recent advancements in machine learning have inspired trends toward deep-learning-based medical image analysis, also bringing new promises to the field of aesthetic assessment of locoregional treatments. In this paper, a review and discussion of the previous state-of-the-art methods in the field is conducted and the extracted knowledge is used to understand the evolution and current challenges. The aim of this paper is to delve into the current opportunities as well as motivate and guide future research in the aesthetic assessment of Breast Cancer locoregional treatments.",2020 Feb;49:123-130.,3.0
31789703,Retinal vessel changes in cerebrovascular disease,"Purpose of review:                    The retina is growingly recognized as a window into cerebrovascular and systemic vascular conditions. The utility of noninvasive retinal vessel biomarkers in cerebrovascular risk assessment has expanded due to advances in retinal imaging techniques and machine learning-based digital analysis. The purpose of this review is to underscore the latest evidence linking retinal vascular abnormalities with stroke and vascular-related cognitive disorders; to highlight modern developments in retinal vascular imaging modalities and software-based vasculopathy quantification.              Recent findings:                    Longitudinal studies undertaken for extended periods indicate that retinal vascular changes can predict cerebrovascular disorders (CVD). Cerebrovascular ties to dementia provoked recent explorations of retinal vessel imaging tools for conceivable early cognitive decline detection. Innovative biomedical engineering technologies and advanced dynamic and functional retinal vascular imaging methods have recently been added to the armamentarium, allowing an unbiased and comprehensive analysis of the retinal vasculature. Improved artificial intelligence-based deep learning algorithms have boosted the application of retinal imaging as a clinical and research tool to screen, risk stratify, and monitor with precision CVD and vascular cognitive impairment.              Summary:                    Mounting evidence supports the use of quantitative retinal vessel analysis in predicting CVD, from clinical stroke to neuroimaging markers of stroke and neurodegeneration.",2020 Feb;33(1):87-92.,2.0
31789682,Possibility of Deep Learning in Medical Imaging Focusing Improvement of Computed Tomography Image Quality,"Deep learning (DL), part of a broader family of machine learning methods, is based on learning data representations rather than task-specific algorithms. Deep learning can be used to improve the image quality of clinical scans with image noise reduction. We review the ability of DL to reduce the image noise, present the advantages and disadvantages of computed tomography image reconstruction, and examine the potential value of new DL-based computed tomography image reconstruction.",Mar/Apr 2020;44(2):161-167.,3.0
31790164,Performance and Usability of Machine Learning for Screening in Systematic Reviews: A Comparative Evaluation of Three Tools [Internet],"Background:                    Machine learning tools can expedite systematic review (SR) completion by reducing manual screening workloads, yet their adoption has been slow. Evidence of their reliability and usability may improve their acceptance within the SR community. We explored the performance of three tools when used to: (a) eliminate irrelevant records (Automated Simulation) and (b) complement the work of a single reviewer (Semi-automated Simulation). We evaluated the usability of each tool.              Methods:                    We subjected three SRs to two retrospective screening simulations. In each tool (Abstrackr, DistillerSR, and RobotAnalyst), we screened a 200-record training set and downloaded the predicted relevance of the remaining records. We calculated the proportion missed and the workload and time savings compared to dual independent screening. To test usability, eight research staff undertook a screening exercise in each tool and completed a survey, including the System Usability Scale (SUS).              Results:                    Using Abstrackr, DistillerSR, and RobotAnalyst respectively, the median (range) proportion missed was 5 (0 to 28) percent, 97 (96 to 100) percent, and 70 (23 to 100) percent in the Automated Simulation and 1 (0 to 2) percent, 2 (0 to 7) percent, and 2 (0 to 4) percent in the Semi-automated Simulation. The median (range) workload savings was 90 (82 to 93) percent, 99 (98 to 99) percent, and 85 (85 to 88) percent for the Automated Simulation and 40 (32 to 43) percent, 49 (48 to 49 percent), and 35 (34 to 38 percent) for the Semi-automated Simulation. The median (range) time savings was 154 (91 to 183), 185 (95 to 201), and 157 (86 to 172) hours for the Automated Simulation and 61 (42 to 82), 92 (46 to 100), and 64 (37 to 71) hours for the Semi-automated Simulation. Abstrackr identified 33–90% of records erroneously excluded by a single reviewer, while RobotAnalyst performed less well and DistillerSR provided no relative advantage. Based on reported SUS scores, Abstrackr fell in the usable, DistillerSR the marginal, and RobotAnalyst the unacceptable usability range. Usability depended on six interdependent properties: user friendliness, qualities of the user interface, features and functions, trustworthiness, ease and speed of obtaining predictions, and practicality of the export file(s).              Conclusions:                    The workload and time savings afforded in the Automated Simulation came with increased risk of erroneously excluding relevant records. Supplementing a single reviewer’s decisions with relevance predictions (Semi-automated Simulation) improved upon the proportion missed in some cases, but performance varied by tool and SR. Designing tools based on reviewers’ self-identified preferences may improve their compatibility with present workflows.",,
33404545,Engineering Plant Cytochrome P450s for Enhanced Synthesis of Natural Products: Past Achievements and Future Perspectives,"Cytochrome P450s (P450s) are the most versatile catalysts and are widely used by plants to synthesize a vast array of structurally diverse specialized metabolites that not only play essential ecological roles but also constitute a valuable resource for the development of new drugs. To accelerate the metabolic engineering of these high-value metabolites, it is imperative to identify and characterize pathway P450s, and to further improve their activities through protein engineering. In this review, we focus on P450 engineering and summarize the major strategies for enhancing the stability and activity of P450s and successful cases of P450 engineering. Studies in which the functions of P450s were altered to create de novo metabolic pathways or novel compounds are discussed as well. We also overview emerging tools, specifically DNA synthesis, machine learning, and de novo protein design, as well as the evolutionary patterns of P450s unveiled from a massive number of DNA sequences that could be integrated to accelerate the engineering of these enzymes. These approaches would greatly aid in the exploitation of plant-specialized metabolites or derivatives for various uses including medical applications.",2019 Dec 3;1(1):100012.,
31786740,Evolving the pulmonary nodules diagnosis from classical approaches to deep learning-aided decision support: three decades' development course and future prospect,"Purpose:                    Lung cancer is the commonest cause of cancer deaths worldwide, and its mortality can be reduced significantly by performing early diagnosis and screening. Since the 1960s, driven by the pressing needs to accurately and effectively interpret the massive volume of chest images generated daily, computer-assisted diagnosis of pulmonary nodule has opened up new opportunities to relax the limitation from physicians' subjectivity, experiences and fatigue. And the fair access to the reliable and affordable computer-assisted diagnosis will fight the inequalities in incidence and mortality between populations. It has been witnessed that significant and remarkable advances have been achieved since the 1980s, and consistent endeavors have been exerted to deal with the grand challenges on how to accurately detect the pulmonary nodules with high sensitivity at low false-positive rate as well as on how to precisely differentiate between benign and malignant nodules. There is a lack of comprehensive examination of the techniques' development which is evolving the pulmonary nodules diagnosis from classical approaches to machine learning-assisted decision support. The main goal of this investigation is to provide a comprehensive state-of-the-art review of the computer-assisted nodules detection and benign-malignant classification techniques developed over three decades, which have evolved from the complicated ad hoc analysis pipeline of conventional approaches to the simplified seamlessly integrated deep learning techniques. This review also identifies challenges and highlights opportunities for future work in learning models, learning algorithms and enhancement schemes for bridging current state to future prospect and satisfying future demand.              Conclusion:                    It is the first literature review of the past 30 years' development in computer-assisted diagnosis of lung nodules. The challenges indentified and the research opportunities highlighted in this survey are significant for bridging current state to future prospect and satisfying future demand. The values of multifaceted driving forces and multidisciplinary researches are acknowledged that will make the computer-assisted diagnosis of pulmonary nodules enter into the main stream of clinical medicine and raise the state-of-the-art clinical applications as well as increase both welfares of physicians and patients. We firmly hold the vision that fair access to the reliable, faithful, and affordable computer-assisted diagnosis for early cancer diagnosis would fight the inequalities in incidence and mortality between populations, and save more lives.",2020 Jan;146(1):153-185.,7.0
31786504,Ethical considerations in artificial intelligence,"With artificial intelligence (AI) precipitously perched at the apex of the hype curve, the promise of transforming the disparate fields of healthcare, finance, journalism, and security and law enforcement, among others, is enormous. For healthcare - particularly radiology - AI is anticipated to facilitate improved diagnostics, workflow, and therapeutic planning and monitoring. And, while it is also causing some trepidation among radiologists regarding its uncertain impact on the demand and training of our current and future workforce, most of us welcome the potential to harness AI for transformative improvements in our ability to diagnose disease more accurately and earlier in the populations we serve.",2020 Jan;122:108768.,2.0
31786416,Machine learning with multiparametric magnetic resonance imaging of the breast for early prediction of response to neoadjuvant chemotherapy,"In patients with locally advanced breast cancer undergoing neoadjuvant chemotherapy (NAC), some patients achieve a complete pathologic response (pCR), some achieve a partial response, and some do not respond at all or even progress. Accurate prediction of treatment response has the potential to improve patient care by improving prognostication, enabling de-escalation of toxic treatment that has little benefit, facilitating upfront use of novel targeted therapies, and avoiding delays to surgery. Visual inspection of a patient's tumor on multiparametric MRI is insufficient to predict that patient's response to NAC. However, machine learning and deep learning approaches using a mix of qualitative and quantitative MRI features have recently been applied to predict treatment response early in the course of or even before the start of NAC. This is a novel field but the data published so far has shown promising results. We provide an overview of the machine learning and deep learning models developed to date, as well as discuss some of the challenges to clinical implementation.",2020 Feb;49:115-122.,3.0
31783696,Artificial Intelligence (AI) in Rare Diseases: Is the Future Brighter?,"The amount of data collected and managed in (bio)medicine is ever-increasing. Thus, there is a need to rapidly and efficiently collect, analyze, and characterize all this information. Artificial intelligence (AI), with an emphasis on deep learning, holds great promise in this area and is already being successfully applied to basic research, diagnosis, drug discovery, and clinical trials. Rare diseases (RDs), which are severely underrepresented in basic and clinical research, can particularly benefit from AI technologies. Of the more than 7000 RDs described worldwide, only 5% have a treatment. The ability of AI technologies to integrate and analyze data from different sources (e.g., multi-omics, patient registries, and so on) can be used to overcome RDs' challenges (e.g., low diagnostic rates, reduced number of patients, geographical dispersion, and so on). Ultimately, RDs' AI-mediated knowledge could significantly boost therapy development. Presently, there are AI approaches being used in RDs and this review aims to collect and summarize these advances. A section dedicated to congenital disorders of glycosylation (CDG), a particular group of orphan RDs that can serve as a potential study model for other common diseases and RDs, has also been included.",2019 Nov 27;10(12):978.,7.0
33386099,Proteochemometrics - recent developments in bioactivity and selectivity modeling,"Proteochemometrics is a machine learning based modeling approach relying on a combination of ligand and protein descriptors. With ongoing developments in machine learning and increases in public data the technique is more frequently applied in early drug discovery, typically in ligand-target binding prediction. Common applications include improvements to single target quantitative structure-activity relationship models, protein selectivity and promiscuity modeling, and large-scale deep learning approaches. The increase in predictive power using proteochemometrics is observed in multi-target bioactivity modeling, opening the door to more extensive studies covering whole protein families. On top of that, with deep learning fueling more complex and larger scale models, proteochemometrics allows faster and higher quality computational models supporting the design, make, test cycle.",2019 Dec;32-33:89-98.,
33386098,Selecting machine-learning scoring functions for structure-based virtual screening,"Interest in docking technologies has grown parallel to the ever increasing number and diversity of 3D models for macromolecular therapeutic targets. Structure-Based Virtual Screening (SBVS) aims at leveraging these experimental structures to discover the necessary starting points for the drug discovery process. It is now established that Machine Learning (ML) can strongly enhance the predictive accuracy of scoring functions for SBVS by exploiting large datasets from targets, molecules and their associations. However, with greater choice, the question of which ML-based scoring function is the most suitable for prospective use on a given target has gained importance. Here we analyse two approaches to select an existing scoring function for the target along with a third approach consisting in generating a scoring function tailored to the target. These analyses required discussing the limitations of popular SBVS benchmarks, the alternatives to benchmark scoring functions for SBVS and how to generate them or use them using freely-available software.",2019 Dec;32-33:81-87.,1.0
33386097,Practical considerations for active machine learning in drug discovery,"Active machine learning enables the automated selection of the most valuable next experiments to improve predictive modelling and hasten active retrieval in drug discovery. Although a long established theoretical concept and introduced to drug discovery approximately 15 years ago, the deployment of active learning technology in the discovery pipelines across academia and industry remains slow. With the recent re-discovered enthusiasm for artificial intelligence as well as improved flexibility of laboratory automation, active learning is expected to surge and become a key technology for molecular optimizations. This review recapitulates key findings from previous active learning studies to highlight the challenges and opportunities of applying adaptive machine learning to drug discovery. Specifically, considerations regarding implementation, infrastructural integration, and expected benefits are discussed. By focusing on these practical aspects of active learning, this review aims at providing insights for scientists planning to implement active learning workflows in their discovery pipelines.",2019 Dec;32-33:73-79.,
33386095,On failure modes in molecule generation and optimization,"There has been a wave of generative models for molecules triggered by advances in the field of Deep Learning. These generative models are often used to optimize chemical compounds towards particular properties or a desired biological activity. The evaluation of generative models remains challenging and suggested performance metrics or scoring functions often do not cover all relevant aspects of drug design projects. In this work, we highlight some unintended failure modes in molecular generation and optimization and how these evade detection by current performance metrics.",2019 Dec;32-33:55-63.,
33386093,The art of atom descriptor design,"This review provides an overview of descriptions of atoms applied to the understanding of phenomena like chemical reactivity and selectivity, pKa values, Site of Metabolism prediction, or hydrogen bond strengths, but also the substitution of quantum mechanical calculations by machine learning models for energies, forces or even spectrosocopic properties and finally the fast calculation of atomic charges for force field parametrization. The descriptor space ranges from derivatives of the wavefunctions or electron density via quantum mechanics derived descriptors to classical descriptions of atoms and their embedding in a molecule. The common denominator for all approaches is the thorough understanding of the physics of the chemical problem that guided the design of the atom descriptor. Quantum mechanics (QM) and machine learning (ML) finally are converging to a new discipline, namely QM/ML.",2019 Dec;32-33:37-43.,1.0
31782286,Global perspective on carotid intima-media thickness and plaque: should the current measurement guidelines be revisited?,"Carotid intima-media thickness (cIMT) and carotid plaque (CP) currently act as risk predictors for CVD/Stroke risk assessment. Over 2000 articles have been published that cover either use cIMT/CP or alterations of cIMT/CP and additional image-based phenotypes to associate cIMT related markers with CVD/Stroke risk. These articles have shown variable results, which likely reflect a lack of standardization in the tools for measurement, risk stratification, and risk assessment. Guidelines for cIMT/CP measurement are influenced by major factors like the atherosclerosis disease itself, conventional risk factors, 10-year measurement tools, types of CVD/Stroke risk calculators, incomplete validation of measurement tools, and the fast pace of computer technology advancements. This review discusses the following major points: 1) the American Society of Echocardiography and Mannheim guidelines for cIMT/CP measurements; 2) forces that influence the guidelines; and 3) calculators for risk stratification and assessment under the influence of advanced intelligence methods. The review also presents the knowledge-based learning strategies such as machine and deep learning which may play a future role in CVD/stroke risk assessment. We conclude that both machine learning and non-machine learning strategies will flourish for current and 10-year CVD/Stroke risk prediction as long as they integrate image-based phenotypes with conventional risk factors.",2019 Dec;38(6):451-465.,5.0
31781215,Artificial Intelligence in Interventional Radiology: A Literature Review and Future Perspectives,"The term ""artificial intelligence"" (AI) includes computational algorithms that can perform tasks considered typical of human intelligence, with partial to complete autonomy, to produce new beneficial outputs from specific inputs. The development of AI is largely based on the introduction of artificial neural networks (ANN) that allowed the introduction of the concepts of ""computational learning models,"" machine learning (ML) and deep learning (DL). AI applications appear promising for radiology scenarios potentially improving lesion detection, segmentation, and interpretation with a recent application also for interventional radiology (IR) practice, including the ability of AI to offer prognostic information to both patients and physicians about interventional oncology procedures. This article integrates evidence-reported literature and experience-based perceptions to assist not only residents and fellows who are training in interventional radiology but also practicing colleagues who are approaching to locoregional mini-invasive treatments.",2019 Nov 3;2019:6153041.,2.0
31781015,Clinical Risk Score for Predicting Recurrence Following a Cerebral Ischemic Event,"Introduction: Recurrent stroke has a higher rate of death and disability. A number of risk scores have been developed to predict short-term and long-term risk of stroke following an initial episode of stroke or transient ischemic attack (TIA) with limited clinical utilities. In this paper, we review different risk score models and discuss their validity and clinical utilities. Methods: The PubMed bibliographic database was searched for original research articles on the various risk scores for risk of stroke following an initial episode of stroke or TIA. The validation of the models was evaluated by examining the internal and external validation process as well as statistical methodology, the study power, as well as the accuracy and metrics such as sensitivity and specificity. Results: Different risk score models have been derived from different study populations. Validation studies for these risk scores have produced conflicting results. Currently, ABCD2 score with diffusion weighted imaging (DWI) and Recurrence Risk Estimator at 90 days (RRE-90) are the two acceptable models for short-term risk prediction whereas Essen Stroke Risk Score (ESRS) and Stroke Prognosis Instrument-II (SPI-II) can be useful for prediction of long-term risk. Conclusion: The clinical risk scores that currently exist for predicting short-term and long-term risk of recurrent cerebral ischemia are limited in their performance and clinical utilities. There is a need for a better predictive tool which can overcome the limitations of current predictive models. Application of machine learning methods in combination with electronic health records may provide platform for development of new-generation predictive tools.",2019 Nov 12;10:1106.,4.0
31779139,Machine Learning Approaches for the Prioritization of Genomic Variants Impacting Pre-mRNA Splicing,"Defects in pre-mRNA splicing are frequently a cause of Mendelian disease. Despite the advent of next-generation sequencing, allowing a deeper insight into a patient's variant landscape, the ability to characterize variants causing splicing defects has not progressed with the same speed. To address this, recent years have seen a sharp spike in the number of splice prediction tools leveraging machine learning approaches, leaving clinical geneticists with a plethora of choices for in silico analysis. In this review, some basic principles of machine learning are introduced in the context of genomics and splicing analysis. A critical comparative approach is then used to describe seven recent machine learning-based splice prediction tools, revealing highly diverse approaches and common caveats. We find that, although great progress has been made in producing specific and sensitive tools, there is still much scope for personalized approaches to prediction of variant impact on splicing. Such approaches may increase diagnostic yields and underpin improvements to patient care.",2019 Nov 26;8(12):1513.,4.0
31779133,A Survey on Machine-Learning Techniques for UAV-Based Communications,"Unmanned aerial vehicles (UAVs) will be an integral part of the next generation wireless communication networks. Their adoption in various communication-based applications is expected to improve coverage and spectral efficiency, as compared to traditional ground-based solutions. However, this new degree of freedom that will be included in the network will also add new challenges. In this context, the machine-learning (ML) framework is expected to provide solutions for the various problems that have already been identified when UAVs are used for communication purposes. In this article, we provide a detailed survey of all relevant research works, in which ML techniques have been used on UAV-based communications for improving various design and functional aspects such as channel modeling, resource management, positioning, and security.",2019 Nov 26;19(23):5170.,3.0
31777668,Artificial Intelligence Applications in Type 2 Diabetes Mellitus Care: Focus on Machine Learning Methods,"Objectives:                    The incidence of type 2 diabetes mellitus has increased significantly in recent years. With the development of artificial intelligence applications in healthcare, they are used for diagnosis, therapeutic decision making, and outcome prediction, especially in type 2 diabetes mellitus. This study aimed to identify the artificial intelligence (AI) applications for type 2 diabetes mellitus care.              Methods:                    This is a review conducted in 2018. We searched the PubMed, Web of Science, and Embase scientific databases, based on a combination of related mesh terms. The article selection process was based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). Finally, 31 articles were selected after inclusion and exclusion criteria were applied. Data gathering was done by using a data extraction form. Data were summarized and reported based on the study objectives.              Results:                    The main applications of AI for type 2 diabetes mellitus care were screening and diagnosis in different stages. Among all of the reviewed AI methods, machine learning methods with 71% (n = 22) were the most commonly applied techniques. Many applications were in multi method forms (23%). Among the machine learning algorithms applications, support vector machine (21%) and naive Bayesian (19%) were the most commonly used methods. The most important variables that were used in the selected studies were body mass index, fasting blood sugar, blood pressure, HbA1c, triglycerides, low-density lipoprotein, high-density lipoprotein, and demographic variables.              Conclusions:                    It is recommended to select optimal algorithms by testing various techniques. Support vector machine and naive Bayesian might achieve better performance than other applications due to the type of variables and targets in diabetes-related outcomes classification.",2019 Oct;25(4):248-261.,5.0
31776737,Artificial intelligence and robotics: a combination that is changing the operating room,"Purpose:                    The aim of the current narrative review was to summarize the available evidence in the literature on artificial intelligence (AI) methods that have been applied during robotic surgery.              Methods:                    A narrative review of the literature was performed on MEDLINE/Pubmed and Scopus database on the topics of artificial intelligence, autonomous surgery, machine learning, robotic surgery, and surgical navigation, focusing on articles published between January 2015 and June 2019. All available evidences were analyzed and summarized herein after an interactive peer-review process of the panel.              Literature review:                    The preliminary results of the implementation of AI in clinical setting are encouraging. By providing a readout of the full telemetry and a sophisticated viewing console, robot-assisted surgery can be used to study and refine the application of AI in surgical practice. Machine learning approaches strengthen the feedback regarding surgical skills acquisition, efficiency of the surgical process, surgical guidance and prediction of postoperative outcomes. Tension-sensors on the robotic arms and the integration of augmented reality methods can help enhance the surgical experience and monitor organ movements.              Conclusions:                    The use of AI in robotic surgery is expected to have a significant impact on future surgical training as well as enhance the surgical experience during a procedure. Both aim to realize precision surgery and thus to increase the quality of the surgical care. Implementation of AI in master-slave robotic surgery may allow for the careful, step-by-step consideration of autonomous robotic surgery.",2020 Oct;38(10):2359-2366.,4.0
31772395,Intraoperative hypotension and its prediction,"Intraoperative hypotension (IOH) very commonly accompanies general anaesthesia in patients undergoing major surgical procedures. The development of IOH is unwanted, since it is associated with adverse outcomes such as acute kidney injury and myocardial injury, stroke and mortality. Although the definition of IOH is variable, harm starts to occur below a mean arterial pressure (MAP) threshold of 65 mmHg. The odds of adverse outcome increase for increasing duration and/or magnitude of IOH below this threshold, and even short periods of IOH seem to be associated with adverse outcomes. Therefore, reducing the hypotensive burden by predicting and preventing IOH through proactive appropriate treatment may potentially improve patient outcome. In this review article, we summarise the current state of the prediction of IOH by the use of so-called machine-learning algorithms. Machine-learning algorithms that use high-fidelity data from the arterial pressure waveform, may be used to reveal 'traits' that are unseen by the human eye and are associated with the later development of IOH. These algorithms can use large datasets for 'training', and can subsequently be used by clinicians for haemodynamic monitoring and guiding therapy. A first clinically available application, the hypotension prediction index (HPI), is aimed to predict an impending hypotensive event, and additionally, to guide appropriate treatment by calculated secondary variables to asses preload (dynamic preload variables), contractility (dP/dtmax), and afterload (dynamic arterial elastance, Eadyn). In this narrative review, we summarise the current state of the prediction of hypotension using such novel, automated algorithms and we will highlight HPI and the secondary variables provided to identify the probable origin of the (impending) hypotensive event.",2019 Nov;63(11):877-885.,4.0
31771246,Wearable-Sensor-based Detection and Prediction of Freezing of Gait in Parkinson's Disease: A Review,"Freezing of gait (FOG) is a serious gait disturbance, common in mid- and late-stage Parkinson's disease, that affects mobility and increases fall risk. Wearable sensors have been used to detect and predict FOG with the ultimate aim of preventing freezes or reducing their effect using gait monitoring and assistive devices. This review presents and assesses the state of the art of FOG detection and prediction using wearable sensors, with the intention of providing guidance on current knowledge, and identifying knowledge gaps that need to be filled and challenges to be considered in future studies. This review searched the Scopus, PubMed, and Web of Science databases to identify studies that used wearable sensors to detect or predict FOG episodes in Parkinson's disease. Following screening, 74 publications were included, comprising 68 publications detecting FOG, seven predicting FOG, and one in both categories. Details were extracted regarding participants, walking task, sensor type and body location, detection or prediction approach, feature extraction and selection, classification method, and detection and prediction performance. The results showed that increasingly complex machine-learning algorithms combined with diverse feature sets improved FOG detection. The lack of large FOG datasets and highly person-specific FOG manifestation were common challenges. Transfer learning and semi-supervised learning were promising for FOG detection and prediction since they provided person-specific tuning while preserving model generalization.",2019 Nov 24;19(23):5141.,7.0
31767810,Deep biomarkers of aging and longevity: from research to applications,"Multiple recent advances in machine learning enabled computer systems to exceed human performance in many tasks including voice, text, and speech recognition and complex strategy games. Aging is a complex multifactorial process driven by and resulting in the many minute changes transpiring at every level of the human organism. Deep learning systems trained on the many measurable features changing in time can generalize and learn the many biological processes on the population and individual levels. The deep age predictors can help advance aging research by establishing causal relationships in non-linear systems. Deep aging clocks can be used for identification of novel therapeutic targets, evaluating the efficacy of the various interventions, data quality control, data economics, prediction of health trajectories, mortality, and many other applications. Here we present the current state of development of the deep aging clocks in the context of the pharmaceutical research and development and clinical applications.",2019 Nov 25;11(22):10771-10780.,7.0
31767194,"The digital surgeon: How big data, automation, and artificial intelligence will change surgical practice","Exponential growth in computing power, data storage, and sensing technology has led to a world in which we can both capture and analyze incredible amounts of data. The evolution of machine learning has further advanced the ability of computers to develop insights from massive data sets that are beyond the capacity of human analysis. The convergence of computational power, data storage, connectivity, and Artificial Intelligence (AI) has led to health technologies that, to date, have focused on diagnostic areas such as radiology and pathology. The question remains how the digital revolution will translate in the realm of surgery. There are three main areas where the authors believe that AI could impact surgery in the near future: enhancement of training modalities, cognitive enhancement of the surgeon, and procedural automation. While the promise of Big Data, AI, and Automation is high, there have been unanticipated missteps in the use of such technologies that are worth considering as we evaluate how such technologies could/should be adopted in surgical practice. Surgeons must be prepared to adopt smarter training modalities, supervise the learning of machines that can enhance cognitive function, and ultimately oversee autonomous surgery without allowing for a decay in the surgeon's operating skills.",2020 Jan;55S:47-50.,3.0
31766686,Electronic Tongues for Inedible Media,"""Electronic tongues"", ""taste sensors"", and similar devices (further named as ""multisensor systems"", or MSS) have been studied and applied mostly for the analysis of edible analytes. This is not surprising, since the MSS development was sometimes inspired by the mainstream idea that they could substitute human gustatory tests. However, the basic principle behind multisensor systems-a combination of an array of cross-sensitive chemical sensors for liquid analysis and a machine learning engine for multivariate data processing-does not imply any limitations on the application of such systems for the analysis of inedible media. This review deals with the numerous MSS applications for the analysis of inedible analytes, among other things, for agricultural and medical purposes.",2019 Nov 22;19(23):5113.,1.0
31762579,A Review of Machine Learning Approaches in Assisted Reproductive Technologies,"Introduction:                    Assisted reproductive technologies (ART) are recent improvements in infertility treatment. However, there is no significant increase in pregnancy rates with the aid of ART. Costly and complex process of ART's makes them as challenging issues. Computational prediction models could predict treatment outcome, before the start of an ART cycle.              Aim:                    This review provides an overview on machine learning-based prediction models in ART.              Methods:                    This article was executed based on a literature review through scientific databases search such as PubMed, Scopus, Web of Science and Google Scholar.              Results:                    We identified 20 papers reporting on machine learning-based prediction models in IVF or ICSI settings. All of the models were validated only by internal validation. Therefore, external validation of the models and the impact analysis of them were the missing parts of the all studies.              Conclusion:                    Machine learning-based prediction models provide a clinical decision support tool for both clinicians and patients and lead to improvement in ART success rates.",2019 Sep;27(3):205-211.,5.0
31761063,Imaging of Central Nervous System Tumors Based on the 2016 World Health Organization Classification,"The 2016 World Health Organization Classification of Tumors of the Central Nervous System (CNS) incorporated well-established molecular markers known to drive tumorigenesis and tumor behavior into the existing classification of CNS tumors based on histopathologic appearance. This integrated classification system has led to a major restructuring of the diffuse gliomas. In addition, it resulted in the categorization of medulloblastomas into four distinct molecular subgroups. Radiogenomic studies have revealed key imaging differences between certain genetic groups and may aid in the diagnosis, longitudinal assessment of treatment response, and evaluation of tumor recurrence in patients with brain tumors.",2020 Feb;38(1):95-113.,3.0
31759575,Widespread Morphometric Abnormalities in Major Depression: Neuroplasticity and Potential for Biomarker Development,Major depression is common and debilitating. Identifying neurobiological subtypes that comprise the disorder and predict clinical outcome are key challenges. Genetic and environmental factors leading to major depression are expressed in neural structure and function. Volumetric decreases in gray matter have been demonstrated in corticolimbic circuits involved in emotion regulation. MR imaging observable abnormalities reflect cytoarchitectonic alterations within a local neuroendocrine milieu with systemic effects. Multivariate pattern analysis offers the potential to identify the neurobiological subtypes and predictors of clinical outcome. It is essential to characterize disease heterogeneity by incorporating data-driven inductive and symptom-based deductive approaches in an iterative process.,2020 Feb;30(1):85-95.,
31759574,Neuroimaging in Schizophrenia,"Schizophrenia is a chronic psychotic disorder with a lifetime prevalence of about 1%. Onset is typically in adolescence or early adulthood; characteristic symptoms include positive symptoms, negative symptoms, and impairments in cognition. Neuroimaging studies have shown substantive evidence of brain structural, functional, and neurochemical alterations that are more pronounced in the association cortex and subcortical regions. These abnormalities are not sufficiently specific to be of diagnostic value, but there may be a role for imaging techniques to provide predictions of outcome. Incorporating multimodal imaging datasets using machine learning approaches may offer better diagnostic and predictive value in schizophrenia.",2020 Feb;30(1):73-83.,4.0
31756920,"Bubbles, Foam Formation, Stability and Consumer Perception of Carbonated Drinks: A Review of Current, New and Emerging Technologies for Rapid Assessment and Control","Quality control, mainly focused on the assessment of bubble and foam-related parameters, is critical in carbonated beverages, due to their relationship with the chemical components as well as their influence on sensory characteristics such as aroma release, mouthfeel, and perception of tastes and aromas. Consumer assessment and acceptability of carbonated beverages are mainly based on carbonation, foam, and bubbles, as a flat carbonated beverage is usually perceived as low quality. This review focuses on three beverages: beer, sparkling water, and sparkling wine. It explains the characteristics of foam and bubble formation, and the traditional methods, as well as emerging technologies based on robotics and computer vision, to assess bubble and foam-related parameters. Furthermore, it explores the most common methods and the use of advanced techniques using an artificial intelligence approach to assess sensory descriptors both for descriptive analysis and consumers' acceptability. Emerging technologies, based on the combination of robotics, computer vision, and machine learning as an approach to artificial intelligence, have been developed and applied for the assessment of beer and, to a lesser extent, sparkling wine. This, has the objective of assessing the final products quality using more reliable, accurate, affordable, and less time-consuming methods. However, despite carbonated water being an important product, due to its increasing consumption, more research needs to focus on exploring more efficient, repeatable, and accurate methods to assess carbonation and bubble size, distribution and dynamics.",2019 Nov 20;8(12):596.,3.0
31756390,Data generation and network reconstruction strategies for single cell transcriptomic profiles of CRISPR-mediated gene perturbations,"Recent advances in single-cell RNA-sequencing (scRNA-seq) in combination with CRISPR/Cas9 technologies have enabled the development of methods for large-scale perturbation studies with transcriptional readouts. These methods are highly scalable and have the potential to provide a wealth of information on the biological networks that underlie cellular response. Here we discuss how to overcome several key challenges to generate and analyse data for the confident reconstruction of models of the underlying cellular network. Some challenges are generic, and apply to analysing any single-cell transcriptomic data, while others are specific to combined single-cell CRISPR/Cas9 data, in particular barcode swapping, knockdown efficiency, multiplicity of infection and potential confounding factors. We also provide a curated collection of published data sets to aid the development of analysis strategies. Finally, we discuss several network reconstruction approaches, including co-expression networks and Bayesian networks, as well as their limitations, and highlight the potential of Nested Effects Models for network reconstruction from scRNA-seq data. This article is part of a Special Issue entitled: Transcriptional Profiles and Regulatory Gene Networks edited by Dr. Dr. Federico Manuel Giorgi and Dr. Shaun Mahony.",2020 Jun;1863(6):194441.,1.0
31755816,Sepsis 2019: What Surgeons Need to Know,"The definition of sepsis continues to be as dynamic as the management strategies used to treat this. Sepsis-3 has replaced the earlier systemic inflammatory response syndrome (SIRS)-based diagnoses with the rapid Sequential Organ Failure Assessment (SOFA) score assisting in predicting overall prognosis with regards to mortality. Surgeons have an important role in ensuring adequate source control while recognizing the threat of carbapenem-resistance in gram-negative organisms. Rapid diagnostic tests are being used increasingly for the early identification of multi-drug-resistant organisms (MDROs), with a key emphasis on the multidisciplinary alert of results. Novel, higher generation antibiotic agents have been developed for resistance in ESKCAPE (Enterococcus faecium, Staphylococcus aureus, Klebsiella pneumoniae, Acinetobacter baumannii, Pseudomonas aeruginosa, and Enterobacter species) organisms while surgeons have an important role in the prevention of spread. The Study to Optimize Peritoneal Infection Therapy (STOP-IT) trial has challenged the previous paradigm of length of antibiotic treatment whereas biomarkers such as procalcitonin are playing a prominent role in individualizing therapy. Several novel therapies for refractory septic shock, while still investigational, are gaining prominence rapidly (such as vitamin C) whereas others await further clinical trials. Management strategies presented as care bundles continue to be updated by the Surviving Sepsis Campaign, yet still remain controversial in its global adoption. We have broadened our temporal and epidemiologic perspective of sepsis by understanding it both as an acute, time-sensitive, life-threatening illness to a chronic condition that increases the risk of mortality up to five years post-discharge. Artificial intelligence, machine learning, and bedside scoring systems can assist the clinician in predicting post-operative sepsis. The public health role of the surgeon is key. This includes collaboration and multi-disciplinary antibiotic stewardship at a hospital level. It also requires controlling pharmaceutical sales and the unregulated dispensing of antibiotic agents globally through policy initiatives to control emerging resistance through prevention.",2020 Apr;21(3):195-204.,
31755802,An overview of deep learning algorithms and water exchange in colonoscopy in improving adenoma detection,"Introduction: Among the Gastrointestinal (GI) Endoscopy Editorial Board top 10 topics in advances in endoscopy in 2018, water exchange colonoscopy and artificial intelligence were both considered important advances. Artificial intelligence holds the potential to increase and water exchange significantly increases adenoma detection.Areas covered: The authors searched MEDLINE (1998-2019) using the following medical subject terms: water-aided, water-assisted and water exchange colonoscopy, adenoma, artificial intelligence, deep learning, computer-assisted detection, and neural networks. Additional related studies were manually searched from the reference lists of publications. Only fully published journal articles in English were reviewed. The latest date of the search was Aug10, 2019. Artificial intelligence, machine learning, and deep learning contribute to the promise of real-time computer-aided detection diagnosis. By emphasizing near-complete suction of infused water during insertion, water exchange provides salvage cleaning and decreases cleaning-related multi-tasking distractions during withdrawal, increasing adenoma detection. The review will address how artificial intelligence and water exchange can complement each other in improving adenoma detection during colonoscopy.Expert opinion: In 5 years, research on artificial intelligence will likely achieve real-time application and evaluation of factors contributing to quality colonoscopy. Better understanding and more widespread use of water exchange will be possible.",2019 Dec;13(12):1153-1160.,1.0
31754741,"Highlights of the special scientific sessions of the 46th Annual Scientific Meeting of the International Skeletal Society (ISS) 2019, Vancouver, Canada","This paper summarizes the highlights of the Scientific Sessions of the 46th Annual Scientific Meeting of the International Skeletal Society (ISS) which was hosted in Vancouver, Canada, in September 2019.",2020 Feb;49(2):333-335.,
31753325,A machine learning approach to predict surgical learning curves,"Background:                    Contemporary surgical training programs rely on the repetition of selected surgical motor tasks. Such methodology is inherently open ended with no control on the time taken to attain a set level of proficiency, given the trainees' intrinsic differences in initial skill levels and learning abilities. Hence, an efficient training program should aim at tailoring the surgical training protocols to each trainee. In this regard, a predictive model using information from the initial learning stage to predict learning curve characteristics should facilitate the whole surgical training process.              Methods:                    This paper analyzes learning curve data to train a multivariate supervised machine learning model. One factor is extracted to define the trainees' learning ability. An unsupervised machine learning model is also utilized for trainee classification. When established, the model can predict robustly the learning curve characteristics based on the first few trials.              Results:                    We show that the information present in the first 10 trials of surgical tasks can be utilized to predict the number of trials required to achieve proficiency (R2=0.72) and the final performance level (R2=0.89). Furthermore, only a single factor, learning index, is required to describe the learning process and to classify learners with unique learning characteristics.              Conclusion:                    Using machine learning models, we show, for the first time, that the first few trials contain sufficient information to predict learning curve characteristics and that a single factor can capture the complex learning behavior. Using such models holds the potential for personalization of training regimens, leading to greater efficiency and lower costs.",2020 Feb;167(2):321-327.,1.0
31749705,Applications of Deep-Learning in Exploiting Large-Scale and Heterogeneous Compound Data in Industrial Pharmaceutical Research,"In recent years, the development of high-throughput screening (HTS) technologies and their establishment in an industrialized environment have given scientists the possibility to test millions of molecules and profile them against a multitude of biological targets in a short period of time, generating data in a much faster pace and with a higher quality than before. Besides the structure activity data from traditional bioassays, more complex assays such as transcriptomics profiling or imaging have also been established as routine profiling experiments thanks to the advancement of Next Generation Sequencing or automated microscopy technologies. In industrial pharmaceutical research, these technologies are typically established in conjunction with automated platforms in order to enable efficient handling of screening collections of thousands to millions of compounds. To exploit the ever-growing amount of data that are generated by these approaches, computational techniques are constantly evolving. In this regard, artificial intelligence technologies such as deep learning and machine learning methods play a key role in cheminformatics and bio-image analytics fields to address activity prediction, scaffold hopping, de novo molecule design, reaction/retrosynthesis predictions, or high content screening analysis. Herein we summarize the current state of analyzing large-scale compound data in industrial pharmaceutical research and describe the impact it has had on the drug discovery process over the last two decades, with a specific focus on deep-learning technologies.",2019 Nov 5;10:1303.,6.0
31749074,Pre- and Paralinguistic Vocal Production in ASD: Birth Through School Age,"Purpose of review:                    We review what is known about how pre-linguistic vocal differences in autism spectrum disorder (ASD) unfold across development and consider whether vocalization features can serve as useful diagnostic indicators.              Recent findings:                    Differences in the frequency and acoustic quality of several vocalization types (e.g., babbles and cries) during the first year of life are associated with later ASD diagnosis. Paralinguistic features (e.g., prosody) measured during early and middle childhood can accurately classify current ASD diagnosis using cross-validated machine learning approaches. Pre-linguistic vocalization differences in infants are promising behavioral markers of later ASD diagnosis. In older children, paralinguistic features hold promise as diagnostic indicators as well as clinical targets. Future research efforts should focus on (1) bridging the gap between basic research and practical implementations of early vocalization-based risk assessment tools, and (2) demonstrating the clinical impact of targeting atypical vocalization features during social skill interventions for older children.",2019 Nov 20;21(12):126.,2.0
31748800,Promises and Perils of Artificial Intelligence in Neurosurgery,"Artificial intelligence (AI)-facilitated clinical automation is expected to become increasingly prevalent in the near future. AI techniques may permit rapid and detailed analysis of the large quantities of clinical data generated in modern healthcare settings, at a level that is otherwise impossible by humans. Subsequently, AI may enhance clinical practice by pushing the limits of diagnostics, clinical decision making, and prognostication. Moreover, if combined with surgical robotics and other surgical adjuncts such as image guidance, AI may find its way into the operating room and permit more accurate interventions, with fewer errors. Despite the considerable hype surrounding the impending medical AI revolution, little has been written about potential downsides to increasing clinical automation. These may include both direct and indirect consequences. Directly, faulty, inadequately trained, or poorly understood algorithms may produce erroneous results, which may have wide-scale impact. Indirectly, increasing use of automation may exacerbate de-skilling of human physicians due to over-reliance, poor understanding, overconfidence, and lack of necessary vigilance of an automated clinical workflow. Many of these negative phenomena have already been witnessed in other industries that have already undergone, or are undergoing ""automation revolutions,"" namely commercial aviation and the automotive industry. This narrative review explores the potential benefits and consequences of the anticipated medical AI revolution from a neurosurgical perspective.",2020 Jul 1;87(1):33-44.,
31746287,An Overview of Computational Tools of Nucleic Acid Binding Site Prediction for Site-specific Proteins and Nucleases,"Understanding the interaction mechanism of proteins and nucleic acids is one of the most fundamental problems for genome editing with engineered nucleases. Due to some limitations of experimental investigations, computational methods have played an important role in obtaining the knowledge of protein-nucleic acid interaction. Over the past few years, dozens of computational tools have been used for identification of nucleic acid binding site for site-specific proteins and design of site-specific nucleases because of their significant advantages in genome editing. Here, we review existing widely-used computational tools for target prediction of site-specific proteins as well as off-target prediction of site-specific nucleases. This article provides a list of on-line prediction tools according to their features followed by the description of computational methods used by these tools, which range from various sequence mapping algorithms (like Bowtie, FetchGWI and BLAST) to different machine learning methods (such as Support Vector Machine, hidden Markov models, Random Forest, elastic network and deep neural networks). We also make suggestions on the further development in improving the accuracy of prediction methods. This survey will provide a reference guide for computational biologists working in the field of genome editing.",2020;27(5):370-384.,
31746088,Role of Breast MRI in the Evaluation and Detection of DCIS: Opportunities and Challenges,"Historically, breast magnetic resonance imaging (MRI) was not considered an effective modality in the evaluation of ductal carcinoma in situ (DCIS). Over the past decade this has changed, with studies demonstrating that MRI is the most sensitive imaging tool for detection of all grades of DCIS. It has been suggested that not only is breast MRI the most sensitive imaging tool for detection but it may also detect the most clinically relevant DCIS lesions. The role and outcomes of MRI in the preoperative setting for patients with DCIS remains controversial; however, several studies have shown benefit in the preoperative evaluation of extent of disease as well as predicting an underlying invasive component. The most common presentation of DCIS on MRI is nonmass enhancement (NME) in a linear or segmental distribution pattern. Maximizing breast MRI spatial resolution is therefore beneficial, given the frequent presentation of DCIS as NME on MRI. Emerging MRI techniques, such as diffusion-weighted imaging (DWI), have shown promising potential to discriminate DCIS from benign and invasive lesions. Future opportunities including advanced imaging visual techniques, radiomics/radiogenomics, and machine learning / artificial intelligence may also be applicable to the detection and treatment of DCIS. Level of Evidence: 3 Technical Efficacy Stage: 3 J. Magn. Reson. Imaging 2019. J. Magn. Reson. Imaging 2020;52:697-709.",2020 Sep;52(3):697-709.,2.0
31743905,Artificial Intelligence Techniques for Automated Diagnosis of Neurological Disorders,"Background:                    Authors have been advocating the research ideology that a computer-aided diagnosis (CAD) system trained using lots of patient data and physiological signals and images based on adroit integration of advanced signal processing and artificial intelligence (AI)/machine learning techniques in an automated fashion can assist neurologists, neurosurgeons, radiologists, and other medical providers to make better clinical decisions.              Summary:                    This paper presents a state-of-the-art review of research on automated diagnosis of 5 neurological disorders in the past 2 decades using AI techniques: epilepsy, Parkinson's disease, Alzheimer's disease, multiple sclerosis, and ischemic brain stroke using physiological signals and images. Recent research articles on different feature extraction methods, dimensionality reduction techniques, feature selection, and classification techniques are reviewed. Key Message: CAD systems using AI and advanced signal processing techniques can assist clinicians in analyzing and interpreting physiological signals and images more effectively.",2019;82(1-3):41-64.,3.0
31742424,CAD and AI for breast cancer-recent development and challenges,"Computer-aided diagnosis (CAD) has been a popular area of research and development in the past few decades. In CAD, machine learning methods and multidisciplinary knowledge and techniques are used to analyze the patient information and the results can be used to assist clinicians in their decision making process. CAD may analyze imaging information alone or in combination with other clinical data. It may provide the analyzed information directly to the clinician or correlate the analyzed results with the likelihood of certain diseases based on statistical modeling of the past cases in the population. CAD systems can be developed to provide decision support for many applications in the patient care processes, such as lesion detection, characterization, cancer staging, treatment planning and response assessment, recurrence and prognosis prediction. The new state-of-the-art machine learning technique, known as deep learning (DL), has revolutionized speech and text recognition as well as computer vision. The potential of major breakthrough by DL in medical image analysis and other CAD applications for patient care has brought about unprecedented excitement of applying CAD, or artificial intelligence (AI), to medicine in general and to radiology in particular. In this paper, we will provide an overview of the recent developments of CAD using DL in breast imaging and discuss some challenges and practical issues that may impact the advancement of artificial intelligence and its integration into clinical workflow.",2020 Apr;93(1108):20190580.,4.0
31737621,Raman Spectroscopy: Guiding Light for the Extracellular Matrix,"The extracellular matrix (ECM) consists of a complex mesh of proteins, glycoproteins, and glycosaminoglycans, and is essential for maintaining the integrity and function of biological tissues. Imaging and biomolecular characterization of the ECM is critical for understanding disease onset and for the development of novel, disease-modifying therapeutics. Recently, there has been a growing interest in the use of Raman spectroscopy to characterize the ECM. Raman spectroscopy is a label-free vibrational technique that offers unique insights into the structure and composition of tissues and cells at the molecular level. This technique can be applied across a broad range of ECM imaging applications, which encompass in vitro, ex vivo, and in vivo analysis. State-of-the-art confocal Raman microscopy imaging now enables label-free assessments of the ECM structure and composition in tissue sections with a remarkably high degree of biomolecular specificity. Further, novel fiber-optic instrumentation has opened up for clinical in vivo ECM diagnostic measurements across a range of tissue systems. A palette of advanced computational methods based on multivariate statistics, spectral unmixing, and machine learning can be applied to Raman data, allowing for the extraction of specific biochemical information of the ECM. Here, we review Raman spectroscopy techniques for ECM characterizations over a variety of exciting applications and tissue systems, including native tissue assessments (bone, cartilage, cardiovascular), regenerative medicine quality assessments, and diagnostics of disease states. We further discuss the challenges in the widespread adoption of Raman spectroscopy in biomedicine. The results of the latest discovery-driven Raman studies are summarized, illustrating the current and potential future applications of Raman spectroscopy in biomedicine.",2019 Nov 1;7:303.,6.0
31737349,Pre-hospital and emergency department pathways of care for exacerbations of chronic obstructive pulmonary disease (COPD),"Exacerbations are serious complications of chronic obstructive pulmonary disease (COPD) that often require acute care from pre-hospital and emergency department (ED) services. Despite being a frequent cause of emergency presentations, gaps remain in both literature and practice for emergency care pathways of COPD exacerbations. This review seeks to address these gaps and focuses on the literature of pre-hospital and ED systems of care and how these intersect with patients experiencing an exacerbation of COPD. The literature in this area is expanding rapidly; however, more research is required to further understand exacerbations and how they are addressed by emergency medical services worldwide. For the purpose of this review, the pre-hospital domain includes ambulance and other emergency transport services, and encompasses medical interventions delivered prior to arrival at an ED or hospital. The ED domain is defined as the area of a hospital or free-standing centre where patients arrive to receive emergent medical care prior to admission. In many studies there is a significant overlap between these two domains and frequent intersection and collaboration between services. In both of these domains, for the management of COPD exacerbations, several overarching themes have been identified in the literature. These include: the appropriate delivery of oxygen in the emergency setting; strategies to improve the provision of care in accordance with diagnostic and treatment guidelines; strategies to reduce the requirement for emergency presentations; and, technological advances including machine learning which are helping to improve emergency healthcare systems.",2019 Oct;11(Suppl 17):S2221-S2229.,
31734566,Machine learning for target discovery in drug development,"The discovery of macromolecular targets for bioactive agents is currently a bottleneck for the informed design of chemical probes and drug leads. Typically, activity profiling against genetically manipulated cell lines or chemical proteomics is pursued to shed light on their biology and deconvolute drug-target networks. By taking advantage of the ever-growing wealth of publicly available bioactivity data, learning algorithms now provide an attractive means to generate statistically motivated research hypotheses and thereby prioritize biochemical screens. Here, we highlight recent successes in machine intelligence for target identification and discuss challenges and opportunities for drug discovery.",2020 Jun;56:16-22.,4.0
31733673,Biomarkers of Infection and Sepsis,"The role of biomarkers for detection of sepsis has come a long way. Molecular biomarkers are taking front stage at present, but machine learning and other computational measures using bigdata sets are promising. Clinical research in sepsis is hampered by lack of specificity of the diagnosis; sepsis is a syndrome with no uniformly agreed definition. This lack of diagnostic precision means there is no gold standard for this diagnosis. The final conclusion is expert opinion, which is not bad but not perfect. Perhaps machine learning will displace expert opinion as the final and most accurate definition for sepsis.",2020 Jan;36(1):11-22.,4.0
31732118,"Digital transformation of academic medicine: Breaking barriers, borders, and boredom","Academic medicine is experiencing an exponential increase in knowledge, evidenced by approximately 2.5 million new articles published each year. As a result, staying apprised of practice-changing findings as a busy clinician is nearly impossible. The traditional methods of staying up to date through reading textbooks and journal articles or attending an annual conference are no longer enough. These old approaches do not distribute knowledge equally around the world or inform practitioners adequately of what they need to provide the best patient care. Luckily, digital technology, which contributed to our ability to generate this explosion in research, also holds the solution. We believe the improved filtration and curation of new knowledge will come through the combination of three elements: machine learning, crowd-sourcing, and new digital platforms. Machine learning can be harnessed to identify high-quality research while avoiding unconscious bias towards authors, institutions, or positions, and to create personalized reading lists that encompass essential articles while also addressing personal knowledge gaps. The crowd can also serve to curate the best research through an open-source platform that exposes each step of the research process, from developing questions through discussion of findings, functionally replacing editorial boards with crowd peer-review. Finally, embracing new digital platforms and multimedia delivery formats will move academic medicine into the 21st century, broadening its reach to diverse, international, and multigenerational learners. The digital age will continue to change life as we know it, but we have the power - and the responsibility - to control how it transforms academic medicine. LEVEL OF EVIDENCE: V (Expert).",2020 Feb;55(2):223-228.,
31728276,"Data-Driven Materials Science: Status, Challenges, and Perspectives","Data-driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning-typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high-throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data-driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data-driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",2019 Sep 1;6(21):1900808.,6.0
31727507,Discovering the Computational Relevance of Brain Network Organization,"Understanding neurocognitive computations will require not just localizing cognitive information distributed throughout the brain but also determining how that information got there. We review recent advances in linking empirical and simulated brain network organization with cognitive information processing. Building on these advances, we offer a new framework for understanding the role of connectivity in cognition: network coding (encoding/decoding) models. These models utilize connectivity to specify the transfer of information via neural activity flow processes, successfully predicting the formation of cognitive representations in empirical neural data. The success of these models supports the possibility that localized neural functions mechanistically emerge (are computed) from distributed activity flow processes that are specified primarily by connectivity patterns.",2020 Jan;24(1):25-38.,4.0
31725244,Brain Haemorrhage Detection Through SVM Classification of Electrical Impedance Tomography Measurements,"A brain haemorrhage constitutes a serious medical scenario with a need for rapid, accurate detection to facilitate treatment initiation. Machine learning (ML) techniques applied to such medical diagnostic problems can improve the rate and accuracy of bleed detection leading to improved patient outcomes. In this chapter we examine the potential role of support vector machine (SVM) type classifiers in detecting such haemorrhagic lesions (bleeds) using electrical impedance tomography (EIT) measurement frames as the source of training and test data. A two-layer computational model of the head is designed, with EIT frame generation simulated from electrodes placed on the surface of the head model. A wide variety of test scenarios are modelled, including variations in measurement noise, bleed size and location, electrode position, and anatomy. Initial results using a linear SVM classifier applied to test scenarios, with and without pre-processing of the EIT measurement frame, are summarised. The classifier returned detection accuracies >90% with signal-to-noise ratios of ≥60 dB; was independent of bleed location, capable of detecting bleeds as small as 10 ml; and was unaffected by slight variances of ±2 mm in electrode position. However, the performance was degraded with anatomical variations. Options for improvement of performance, including selection of a different kernel and pre-processing of the frames prior to implementing the classifier, are then examined. This analysis demonstrated that using the radial basis function as the kernel for the SVM classifier and principal component analysis (PCA) to select specific features leads to the most accurate and robust performance. The analysis and results indicate that the coupling of EIT with ML has potential for improvement in the detection of bleeds such as brain haemorrhages.",,
31725133,An FP's guide to AI-enabled clinical decision support,"To better understand the capabilities and challenges of artificial intelligence and machine learning, we look at the role they can play in screening for retinopathy and colon cancer.",2019 Nov;68(9):486;488;490;492.,
31720867,Machine Learning and Artificial Intelligence in Neurocritical Care: a Specialty-Wide Disruptive Transformation or a Strategy for Success,"Purpose of review:                    Neurocritical care combines the complexity of both medical and surgical disease states with the inherent limitations of assessing patients with neurologic injury. Artificial intelligence (AI) has garnered interest in the basic management of these complicated patients as data collection becomes increasingly automated.              Recent findings:                    In this opinion article, we highlight the potential AI has in aiding the clinician in several aspects of neurocritical care, particularly in monitoring and managing intracranial pressure, seizures, hemodynamics, and ventilation. The model-based method and data-driven method are currently the two major AI methods for analyzing critical care data. Both are able to analyze the vast quantities of patient data that are accumulated in the neurocritical care unit. AI has the potential to reduce healthcare costs, minimize delays in patient management, and reduce medical errors. However, these systems are an aid to, not a replacement for, the clinician's judgment.",2019 Nov 13;19(11):89.,1.0
31711929,Somatic mutations - Evolution within the individual,"With the rapid advancement of sequencing technologies over the last two decades, it is becoming feasible to detect rare variants from somatic tissue samples. Studying such somatic mutations can provide deep insights into various senescence-related diseases, including cancer, inflammation, and sporadic psychiatric disorders. While it is still a difficult task to identify true somatic mutations, relentless efforts to combine experimental and computational methods have made it possible to obtain reliable data. Furthermore, state-of-the-art machine learning approaches have drastically improved the efficiency and sensitivity of these methods. Meanwhile, we can regard somatic mutations as a counterpart of germline mutations, and it is possible to apply well-formulated mathematical frameworks developed for population genetics and molecular evolution to analyze this 'somatic evolution'. For example, retrospective cell lineage tracing is a promising technique to elucidate the mechanism of pre-diseases using single-cell RNA-sequencing (scRNA-seq) data.",2020 Apr 1;176:91-98.,
31711016,Novel text analytics approach to identify relevant literature for human health risk assessments: A pilot study with health effects of in utero exposures,"Systematic reviews involve mining literature databases to identify relevant studies. Identifying potentially relevant studies can be informed by computational tools comparing text similarity between candidate studies and selected key (i.e., seed) references. Challenge Using computational approaches to identify relevant studies for risk assessments is challenging, as these assessments examine multiple chemical effects across lifestages (e.g., human health risk assessments) or specific effects of multiple chemicals (e.g., cumulative risk). The broad scope of potentially relevant literature can make selection of seed references difficult. Approach We developed a generalized computational scoping strategy to identify human health relevant studies for multiple chemicals and multiple effects. We used semi-supervised machine learning to prioritize studies to review manually with training data derived from references cited in the hazard identification sections of several US EPA Integrated Risk Information System (IRIS) assessments. These generic training data or seed studies were clustered with the unclassified corpus to group studies based on text similarity. Clusters containing a high proportion of seed studies were prioritized for manual review. Chemical names were removed from seed studies prior to clustering resulting in a generic, chemical-independent method for identifying potentially human health relevant studies. We developed a case study that focused on identifying the array of chemicals that have been studied with respect to in utero exposure to test the recall of this novel literature searching strategy. We then evaluated the general strategy of using generic, chemical-independent training data with two previous IRIS assessments by comparing studies predicted relevant to those used in the assessments (i.e., total relevant). Outcome A keyword search designed to retrieve studies that examined the in utero effects of environmental chemicals identified over 54,000 candidate references. Clustering algorithms were applied using 1456 studies from multiple IRIS assessments with chemical names removed as training data or seeds (i.e., semi-supervised learning). Using a six-algorithm ensemble approach 2602 articles, or approximately 5% of candidate references, were ""voted"" relevant by four or more clustering algorithms and manual review confirmed nearly 50% of these studies were relevant. Further evaluations on two IRIS assessments, using a nine-algorithm ensemble approach and a set of generic, chemical-independent, externally-derived seed studies correctly identified 77-83% of hazard identification studies published in the assessments and eliminated the need to manually screen more than 75% of search results on average. Limitations The chemical-independent approach used to build the training literature set provides a broad and unbiased picture across a variety of endpoints and environmental exposures but does not systematically identify all available data. Variance between actual and predicted relevant studies will be greater because of the external and non-random origin of seed study selection. This approach depends on access to readily available generic training data that can be used to locate relevant references in an unclassified corpus. Impact A generic approach to identifying human health relevant studies could be an important first step in literature evaluation for risk assessments. This initial scoping approach could facilitate faster literature evaluation by focusing reviewer efforts, as well as potentially minimize reviewer bias in selection of key studies. Using externally-derived training data has applicability particularly for databases with very low search precision where identifying training data may be cost-prohibitive.",2020 Jan;134:105228.,1.0
31706869,Artificial Intelligence and Machine Learning in Cardiovascular Health Care,"Background:                    This review article provides an overview of artificial intelligence (AI) and machine learning (ML) as it relates to cardiovascular health care.              Methods:                    An overview of the terminology and algorithms used in ML as it relates to health care are provided by the author. Articles published up to August 1, 2019, in the field of AI and ML in cardiovascular medicine are also reviewed and placed in the context of the potential role these approaches will have in clinical practice in the future.              Results:                    AI is a broader term referring to the ability of machines to perform intelligent tasks, and ML is a subset of AI that refers to the ability of machines to learn independently and make accurate predictions. An expanding body of literature has been published using ML in cardiovascular health care. Moreover, ML has been applied in the settings of automated imaging interpretation, natural language processing and data extraction from electronic health records, and predictive analytics. Examples include automated interpretation of chest roentgenograms, electrocardiograms, echocardiograms, and angiography; identification of patients with early heart failure using clinical notes evaluated by ML; and predicting mortality or complications following percutaneous or surgical cardiovascular procedures.              Conclusions:                    Although there is an expanding body of literature on AI and ML in cardiovascular medicine, the future these fields will have in clinical practice remains to be paved. In particular, there is a promising role in providing automated imaging interpretation, automated data extraction and quality control, and clinical risk prediction, although these techniques require further refinement and evaluation.",2020 May;109(5):1323-1329.,4.0
31705495,Development of Neuroimaging-Based Biomarkers in Psychiatry,"This chapter presents an overview of accumulating neuroimaging data with emphasis on translational potential. The subject will be described in the context of three disease states, i.e., schizophrenia, bipolar disorder, and major depressive disorder, and for three clinical goals, i.e., disease risk assessment, subtyping, and treatment decision.",2019;1192:159-195.,1.0
31702942,Critical Issues in Dental and Medical Management of Obstructive Sleep Apnea,"This critical review focuses on obstructive sleep apnea (OSA) and its management from a dental medicine perspective. OSA is characterized by ≥10-s cessation of breathing (apnea) or reduction in airflow (hypopnea) ≥5 times per hour with a drop in oxygen and/or rise in carbon dioxide. It can be associated with sleepiness and fatigue, impaired mood and cognition, cardiometabolic complications, and risk for transportation and work accidents. Although sleep apnea is diagnosed by a sleep physician, its management is interdisciplinary. The dentist's role includes 1) screening patients for OSA risk factors (e.g., retrognathia, high arched palate, enlarged tonsils or tongue, enlarged tori, high Mallampati score, poor sleep, supine sleep position, obesity, hypertension, morning headache or orofacial pain, bruxism); 2) referring to an appropriate health professional as indicated; and 3) providing oral appliance therapy followed by regular dental and sleep medical follow-up. In addition to the device features and provider expertise, anatomic, behavioral, demographic, and neurophysiologic characteristics can influence oral appliance effectiveness in managing OSA. Therefore, OSA treatment should be tailored to each patient individually. This review highlights some of the putative action mechanisms related to oral appliance effectiveness and proposes future research directions.",2020 Jan;99(1):26-35.,1.0
31701320,Artificial Intelligence for Mental Health and Mental Illnesses: an Overview,"Purpose of review:                    Artificial intelligence (AI) technology holds both great promise to transform mental healthcare and potential pitfalls. This article provides an overview of AI and current applications in healthcare, a review of recent original research on AI specific to mental health, and a discussion of how AI can supplement clinical practice while considering its current limitations, areas needing additional research, and ethical implications regarding AI technology.              Recent findings:                    We reviewed 28 studies of AI and mental health that used electronic health records (EHRs), mood rating scales, brain imaging data, novel monitoring systems (e.g., smartphone, video), and social media platforms to predict, classify, or subgroup mental health illnesses including depression, schizophrenia or other psychiatric illnesses, and suicide ideation and attempts. Collectively, these studies revealed high accuracies and provided excellent examples of AI's potential in mental healthcare, but most should be considered early proof-of-concept works demonstrating the potential of using machine learning (ML) algorithms to address mental health questions, and which types of algorithms yield the best performance. As AI techniques continue to be refined and improved, it will be possible to help mental health practitioners re-define mental illnesses more objectively than currently done in the DSM-5, identify these illnesses at an earlier or prodromal stage when interventions may be more effective, and personalize treatments based on an individual's unique characteristics. However, caution is necessary in order to avoid over-interpreting preliminary results, and more work is required to bridge the gap between AI in mental health research and clinical care.",2019 Nov 7;21(11):116.,11.0
31696804,Recent Development of Computational Predicting Bioluminescent Proteins,"Bioluminescent Proteins (BLPs) are widely distributed in many living organisms that act as a key role of light emission in bioluminescence. Bioluminescence serves various functions in finding food and protecting the organisms from predators. With the routine biotechnological application of bioluminescence, it is recognized to be essential for many medical, commercial and other general technological advances. Therefore, the prediction and characterization of BLPs are significant and can help to explore more secrets about bioluminescence and promote the development of application of bioluminescence. Since the experimental methods are money and time-consuming for BLPs identification, bioinformatics tools have played important role in fast and accurate prediction of BLPs by combining their sequences information with machine learning methods. In this review, we summarized and compared the application of machine learning methods in the prediction of BLPs from different aspects. We wish that this review will provide insights and inspirations for researches on BLPs.",2019;25(40):4264-4273.,1.0
31696270,Initial experience with 3D CT cinematic rendering of acute pancreatitis and associated complications,"Inflammation of the pancreas can present with a wide range of imaging findings from mild enlargement of the gland and surrounding infiltrative fat stranding through extensive glandular necrosis. Complications of pancreatitis are varied and include infected fluid collections, pseudocysts, and vascular findings such as pseudoaneurysms and thromboses. Cross-sectional imaging with computed tomography (CT) is one of the mainstays of evaluating patients with pancreatitis. New methods that allow novel visualization volumetric CT data may improve diagnostic yield for the detection of findings that provide prognostic information in pancreatitis patients or can drive new avenues of research such as machine learning. Cinematic rendering (CR) is a photorealistic visualization method for volumetric imaging data that are being investigated for a variety of potential applications including the life-like display of complex anatomy and visual characterization of mass lesions. In this review, we describe the CR appearance of different types of pancreatitis and complications of pancreatitis. We also note possible future directions for research into the utility of CR for pancreatitis.",2020 May;45(5):1290-1298.,
31695786,Current status and future trends of clinical diagnoses via image-based deep learning,"With the recent developments in deep learning technologies, artificial intelligence (AI) has gradually been transformed from cutting-edge technology into practical applications. AI plays an important role in disease diagnosis and treatment, health management, drug research and development, and precision medicine. Interdisciplinary collaborations will be crucial to develop new AI algorithms for medical applications. In this paper, we review the basic workflow for building an AI model, identify publicly available databases of ocular fundus images, and summarize over 60 papers contributing to the field of AI development.",2019 Oct 12;9(25):7556-7565.,13.0
31694700,Quantifying the contribution of microbial immigration in engineered water systems,"Immigration is a process that can influence the assembly of microbial communities in natural and engineered environments. However, it remains challenging to quantitatively evaluate the contribution of this process to the microbial diversity and function in the receiving ecosystems. Currently used methods, i.e., counting shared microbial species, microbial source tracking, and neutral community model, rely on abundance profile to reveal the extent of overlapping between the upstream and downstream communities. Thus, they cannot suggest the quantitative contribution of immigrants to the downstream community function because activities of individual immigrants are not considered after entering the receiving environment. This limitation can be overcome by using an approach that couples a mass balance model with high-throughput DNA sequencing, i.e., ecogenomics-based mass balance. It calculates the net growth rate of individual microbial immigrants and partitions the entire community into active populations that contribute to the community function and inactive ones that carry minimal function. Linking activities of immigrants to their abundance further provides quantification of the contribution from an upstream environment to the downstream community. Considering only active populations can improve the accuracy of identifying key environmental parameters dictating process performance using methods such as machine learning.",2019 Nov 6;7(1):144.,4.0
31691863,Computer-aided diagnosis in rheumatic diseases using ultrasound: an overview,"Clinical evaluation of rheumatic and musculoskeletal diseases through images is a challenge for the beginner rheumatologist since image diagnosis is an expert task with a long learning curve. The aim of this work was to present a narrative review on the main ultrasound computer-aided diagnosis systems that may help clinicians thanks to the progress made in the application of artificial intelligence techniques. We performed a literature review searching for original articles in seven repositories, from 1970 to 2019, and identified 11 main methods currently used in ultrasound computer-aided diagnosis systems. Also, we found that rheumatoid arthritis, osteoarthritis, systemic lupus erythematosus, and idiopathic inflammatory myopathies are the four musculoskeletal and rheumatic diseases most studied that use these innovative systems, with an overall accuracy of > 75%.",2020 Apr;39(4):993-1005.,
31690036,Comprehensive Outline of Whole Exome Sequencing Data Analysis Tools Available in Clinical Oncology,"Whole exome sequencing (WES) enables the analysis of all protein coding sequences in the human genome. This technology enables the investigation of cancer-related genetic aberrations that are predominantly located in the exonic regions. WES delivers high-throughput results at a reasonable price. Here, we review analysis tools enabling utilization of WES data in clinical and research settings. Technically, WES initially allows the detection of single nucleotide variants (SNVs) and copy number variations (CNVs), and data obtained through these methods can be combined and further utilized. Variant calling algorithms for SNVs range from standalone tools to machine learning-based combined pipelines. Tools for CNV detection compare the number of reads aligned to a dedicated segment. Both SNVs and CNVs help to identify mutations resulting in pharmacologically druggable alterations. The identification of homologous recombination deficiency enables the use of PARP inhibitors. Determining microsatellite instability and tumor mutation burden helps to select patients eligible for immunotherapy. To pave the way for clinical applications, we have to recognize some limitations of WES, including its restricted ability to detect CNVs, low coverage compared to targeted sequencing, and the missing consensus regarding references and minimal application requirements. Recently, Galaxy became the leading platform in non-command line-based WES data processing. The maturation of next-generation sequencing is reinforced by Food and Drug Administration (FDA)-approved methods for cancer screening, detection, and follow-up. WES is on the verge of becoming an affordable and sufficiently evolved technology for everyday clinical use.",2019 Nov 4;11(11):1725.,5.0
31683734,Plant Disease Detection and Classification by Deep Learning,"Plant diseases affect the growth of their respective species, therefore their early identification is very important. Many Machine Learning (ML) models have been employed for the detection and classification of plant diseases but, after the advancements in a subset of ML, that is, Deep Learning (DL), this area of research appears to have great potential in terms of increased accuracy. Many developed/modified DL architectures are implemented along with several visualization techniques to detect and classify the symptoms of plant diseases. Moreover, several performance metrics are used for the evaluation of these architectures/techniques. This review provides a comprehensive explanation of DL models used to visualize various plant diseases. In addition, some research gaps are identified from which to obtain greater transparency for detecting diseases in plants, even before their symptoms appear clearly.",2019 Oct 31;8(11):468.,14.0
31682388,Portable Stroke Detection Devices for Patients with Stroke Symptoms: A Review of Diagnostic Accuracy and Cost-Effectiveness [Internet],"Stroke is a highly prevalent and potentially life-threatening medical emergency that requires prompt recognition and treatment in order to minimize morbidity and mortality. In Canada, there are an estimated 62,000 stroke events that occur each year. Moreover, stroke is the third leading cause of death in Canada, comprising nearly 14,000 deaths annually. Mortality rates have improved over time, possibly due to the establishment of integrated regional stroke systems, stroke unit care to prevent and manage complications, and decreases in stroke severity as a result of improved risk factor management. Despite this, over 740,000 Canadian adults over the age of 20 years are living with the effects of stroke (e.g., neurologic deficits, such as hemiparesis, aphasia, and sensory and cognitive deficits).                A stroke is the acute neurologic injury that occurs as a consequence of sudden loss of focal brain function due to cell death from poor or interrupted blood flow within the brain., There are two broad categories of stroke that have vastly different treatment approaches. Ischemic strokes are the most common type (80% of all stroke events) and are generally caused by a sudden blood vessel blockage. Hemorrhagic strokes are caused by a rupture of an artery in the brain. It is vital to differentiate between these two different types, as the treatment approaches are entirely different. Great efforts are made to minimize time delays in treatment, as delays are associated with worse outcomes, including death.                If an individual experiences a stroke in the community, the Emergency Medical Service (EMS) is usually called to the scene. Currently, EMS workers rely on patient history, physical examination (e.g., a tool that includes components of Face, Arm, Speech, and Time [FAST], such as National Institutes of Health Stroke Scale [NIHSS] or Los Angeles Motor Scale [LAMS]), and some diagnostic testing (e.g., blood glucose) to reach a pre-hospital diagnosis of stroke and to determine stroke severity. A pre-hospital assessment helps to determine what resources are required to appropriately treat the patient and thereby informs next steps, including whether to transport the patient to a stroke centre or the closest medical hospital. A more objective way to diagnose a patient at this stage may be using a portable stroke detection device. A portable stroke detection device is a health technology aimed to detect strokes in the pre-hospital environment by observing changes in blood flow to the brain using alternative ambulatory stroke detection methods (e.g., ultrasound, cranial electrodes., The use of portable stroke detection devices may increase the likelihood of pre-hospital diagnoses of stroke, which in turn could facilitate earlier initiation of stroke care (e.g., routing the patient to a stroke centre, earlier neurovascular imaging in-hospital, and definitive stroke therapies with thrombolysis/endovascular treatment if warranted)., However, there is a lack of clarity on the diagnostic accuracy and cost-effectiveness of portable stroke detection devices for patients with stroke symptoms.                A previous CADTH report (summary with critical appraisal) examined the diagnostic accuracy and cost-effectiveness of portable stroke detection devices for adults experiencing symptoms of stroke, including: combination of transcranial Doppler ultrasound, robotic headset blood flow monitor, and machine learning; bioimpedance spectroscopy visor (which uses volumetric impedance phase-shift spectroscopy); and microwave tomography system). The current report extends upon this previous report by evaluating the evidence regarding the diagnostic accuracy and cost-effectiveness of alternative ambulatory stroke detection methods for patients with symptoms of stroke (no age restriction) not covered in the previous report.",,
31679788,Elements of qualitative cognition: An information topology perspective,"Elementary quantitative and qualitative aspects of consciousness are investigated conjointly from the biology, neuroscience, physic and mathematic point of view, by the mean of a theory written with Bennequin that derives and extends information theory within algebraic topology. Information structures, that accounts for statistical dependencies within n-body interacting systems are interpreted a la Leibniz as a monadic-panpsychic framework where consciousness is information and physical, and arise from collective interactions. The electrodynamic intrinsic nature of consciousness, sustained by an analogical code, is illustrated by standard neuroscience and psychophysic results. It accounts for the diversity of the learning mechanisms, including adaptive and homeostatic processes on multiple scales, and details their expression within information theory. The axiomatization and logic of cognition are rooted in measure theory expressed within a topos intrinsic probabilistic constructive logic. Information topology provides a synthesis of the main models of consciousness (Neural Assemblies, Integrated Information, Global Neuronal Workspace, Free Energy Principle) within a formal Gestalt theory, an expression of information structures and patterns in correspondence with Galois cohomology and discrete symmetries. The methods provide new formalization of deep neural network with homologicaly imposed architecture applied to challenges in AI-machine learning.",2019 Dec;31:263-275.,
31677692,The Future Directions of Research in Cardiac Anesthesiology,"This article provides an overview of knowledge gaps that need to be addressed in cardiac anesthesia, including mitigating the inflammatory effects of cardiopulmonary bypass, defining myocardial infarction after cardiac surgery, improving perioperative neurologic outcomes, and the optimal management of patients undergoing valve replacement. In addition, emerging approaches to research conduct are discussed, including the use of new analytical techniques like machine learning, pragmatic trials, and adaptive designs.",2019 Dec;37(4):801-813.,
31677058,Non-invasive continuous blood pressure monitoring systems: current and proposed technology issues and challenges,"High blood pressure (BP) or hypertension is the single most crucial adjustable risk factor for cardiovascular diseases (CVDs) and monitoring the arterial blood pressure (ABP) is an efficient way to detect and control the prevalence of the cardiovascular health of patients. Therefore, monitoring the regulation of BP during patients' daily life plays a critical role in the ambulatory setting and the latest mobile health technology. In recent years, many studies have been conducted to explore the feasibility and performance of such techniques in the health care system. The ultimate aim of these studies is to find and develop an alternative to conventional BP monitoring by using cuff-less, easy-to-use, fast, and cost-effective devices for controlling and lowering the physical harm of CVDs to the human body. However, most of the current studies are at the prototype phase and face a range of issues and challenges to meet clinical standards. This review focuses on the description and analysis of the latest continuous and cuff-less methods along with their key challenges and barriers. Particularly, most advanced and standard technologies including pulse transit time (PTT), ultrasound, pulse arrival time (PAT), and machine learning are investigated. The accuracy, portability, and comfort of use of these technologies, and the ability to integrate to the wearable healthcare system are discussed. Finally, the future directions for further study are suggested.",2019 Nov 1.,4.0
31675466,"Simulation vs. Understanding: A Tension, in Quantum Chemistry and Beyond. Part A. Stage Setting","We begin our tripartite Essay with a triangle of understanding, theory and simulation. Sketching the intimate tie between explanation and teaching, we also point to the emotional impact of understanding. As we trace the development of theory in chemistry, Dirac's characterization of what is known and what is needed for theoretical chemistry comes up, as does the role of prediction, and Thom's phrase ""To predict is not to explain."" We give a typology of models, and then describe, no doubt inadequately, machine learning and neural networks. In the second part, we leave philosophy, beginning by describing Roald's being beaten by simulation. This leads us to artificial intelligence (AI), Searle's Chinese room, and Strevens' account of what a go-playing program knows. Back to our terrain-we ask ""Quantum Chemistry, † ca. 2020?"" Then move to examples of AI affecting social matters, ranging from trivial to scary. We argue that moral decisions are hardly to be left to a computer. At this point, we try to pull the reader up, giving the opposing view of an optimistic, limitless future a voice. But we don't do justice to that view-how could we? We return to questioning the ascetic dimension of scientists, their romance with black boxes. Onward: In the 3rd part of this Essay, we work our way up from pessimism. We trace (another triangle!) the special interests of experimentalists, who want the theory we love, and reliable numbers as well. We detail in our own science instances where theory gave us real joy. Two more examples-on magnetic coupling in inorganic diradicals, and the way to think about alkali metal halides, show us the way to integrate simulation with theory. Back and forth is how it should be-between painfully-obtained, intriguing numbers, begging for interpretation, in turn requiring new concepts, new models, new theoretically grounded tools of computation. Through such iterations understanding is formed. As our tripartite Essay ends, we outline a future of consilience, with a role both for fact-seekers, and searchers for understanding. Chemistry's streak of creation provides in that conjoined future a passage to art and to perceiving, as we argue we must, the sacred in science.",2020 Jul 27;59(31):12590-12610.,
31675462,"Simulation vs. Understanding: A Tension, in Quantum Chemistry and Beyond. Part B. The March of Simulation, for Better or Worse","In the second part of this Essay, we leave philosophy, and begin by describing Roald's being trashed by simulation. This leads us to a general sketch of artificial intelligence (AI), Searle's Chinese room, and Strevens' account of what a go-playing program knows. Back to our terrain-we ask ""Quantum Chemistry, † ca. 2020?"" Then we move to examples of Big Data, machine learning and neural networks in action, first in chemistry and then affecting social matters, trivial to scary. We argue that moral decisions are hardly to be left to a computer. And that posited causes, even if recognized as provisional, represent a much deeper level of understanding than correlations. At this point, we try to pull the reader up, giving voice to the opposing view of an optimistic, limitless future. But we don't do justice to that view-how could we, older mammals on the way to extinction that we are? We try. But then we return to fuss, questioning the ascetic dimension of scientists, their romance with black boxes. And argue for a science of many tongues.",2020 Aug 3;59(32):13156-13178.,
31673712,Advances in Data-Driven Responses to Preventing Spread of Antibiotic Resistance Across Health-Care Settings,"Among the most urgent and serious threats to public health are 7 antibiotic-resistant bacterial infections predominately acquired during health-care delivery. There is an emerging field of health-care epidemiology that is focused on preventing health care-associated infections with antibiotic-resistant bacteria and incorporates data from patient transfers or patient movements within and between facilities. This analytic field is being used to help public health professionals identify best opportunities for prevention. Different analytic approaches that draw on uses of big data are being explored to help target the use of limited public health resources, leverage expertise, and enact effective policy to maximize an impact on population-level health. Here, the following recent advances in data-driven responses to preventing spread of antibiotic resistance across health-care settings are summarized: leveraging big data for machine learning, integration or advances in tracking patient movement, and highlighting the value of coordinating response across institutions within a region.",2019 Jan 31;41(1):6-12.,
31668208,Artificial Intelligence in Radiation Oncology,"The integration of artificial intelligence in the radiation oncologist's workflow has multiple applications and significant potential. From the initial patient encounter, artificial intelligence may aid in pretreatment disease outcome and toxicity prediction. It may subsequently aid in treatment planning, and enhanced dose optimization. Artificial intelligence may also optimize the quality assurance process and support a higher level of safety, quality, and efficiency of care. This article describes components of the radiation consultation, planning, and treatment process and how the thoughtful integration of artificial intelligence may improve shared decision making, planning efficiency, planning quality, patient safety, and patient outcomes.",2019 Dec;33(6):1095-1104.,3.0
33693089,Application of Neural Networks to 12-Lead Electrocardiography - Current Status and Future Directions,"The 12-lead electrocardiogram (ECG) is a fast, non-invasive, powerful tool to diagnose or to evaluate the risk of various cardiac diseases. The vast majority of arrhythmias are diagnosed solely on 12-lead ECG. Initial detection of myocardial ischemia such as myocardial infarction (MI), acute coronary syndrome (ACS) and effort angina is also dependent upon 12-lead ECG. ECG reflects the electrophysiological state of the heart through body mass, and thus contains important information on the electricity-dependent function of the human heart. Indeed, 12-lead ECG data are complex. Therefore, the clinical interpretation of 12-lead ECG requires intense training, but still is prone to interobserver variability. Even with rich clinically relevant data, non-trained physicians cannot efficiently use this powerful tool. Furthermore, recent studies have shown that 12-lead ECG may contain information that is not recognized even by well-trained experts but which can be extracted by computer. Artificial intelligence (AI) based on neural networks (NN) has emerged as a strong tool to extract valuable information from ECG for clinical decision making. This article reviews the current status of the application of NN-based AI to the interpretation of 12-lead ECG and also discusses the current problems and future directions.",2019 Nov 2;1(11):481-486.,
31665258,Differential epigenetic factors in the prediction of cardiovascular risk in diabetic patients,"Hyperglycaemia can strongly alter the epigenetic signatures in many types of human vascular cells providing persistent perturbations of protein-protein interactions both in micro- and macro-domains. The establishment of these epigenetic changes may precede cardiovascular (CV) complications and help us to predict vascular lesions in diabetic patients. Importantly, these epigenetic marks may be transmitted across several generations (transgenerational effect) and increase the individual risk of disease. Aberrant DNA methylation and imbalance of histone modifications, mainly acetylation and methylation of H3, represent key determinants of vascular lesions and, thus, putative useful biomarkers for prevention and diagnosis of CV risk in diabetics. Moreover, a differential expression of some micro-RNAs (miRNAs), mainly miR-126, may be a useful prognostic biomarker for atherosclerosis development in asymptomatic subjects. Recently, also environmental-induced chemical perturbations in mRNA (epitranscriptome), mainly the N6-methyladenosine, have been associated with obesity and diabetes. Importantly, reversal of epigenetic changes by modulation of lifestyle and use of metformin, statins, fenofibrate, and apabetalone may offer useful therapeutic options to prevent or delay CV events in diabetics increasing the opportunity for personalized therapy. Network medicine is a promising molecular-bioinformatic approach to identify the signalling pathways underlying the pathogenesis of CV lesions in diabetic patients. Moreover, machine learning tools combined with tomography are advancing the individualized assessment of CV risk in these patients. We remark the need for combining epigenetics and advanced bioinformatic platforms to improve the prediction of vascular lesions in diabetics increasing the opportunity for CV precision medicine.",2020 Jul 1;6(4):239-247.,8.0
31664499,Review of high-content screening applications in toxicology,"High-content screening (HCS) technology combining automated microscopy and quantitative image analysis can address biological questions in academia and the pharmaceutical industry. Various HCS experimental applications have been utilized in the research field of in vitro toxicology. In this review, we describe several HCS application approaches used for studying the mechanism of compound toxicity, highlight some challenges faced in the toxicological community, and discuss the future directions of HCS in regards to new models, new reagents, data management, and informatics. Many specialized areas of toxicology including developmental toxicity, genotoxicity, developmental neurotoxicity/neurotoxicity, hepatotoxicity, cardiotoxicity, and nephrotoxicity will be examined. In addition, several newly developed cellular assay models including induced pluripotent stem cells (iPSCs), three-dimensional (3D) cell models, and tissues-on-a-chip will be discussed. New genome-editing technologies (e.g., CRISPR/Cas9), data analyzing tools for imaging, and coupling with high-content assays will be reviewed. Finally, the applications of machine learning to image processing will be explored. These new HCS approaches offer a huge step forward in dissecting biological processes, developing drugs, and making toxicology studies easier.",2019 Dec;93(12):3387-3396.,4.0
31661028,Measuring the impact of screening automation on meta-analyses of diagnostic test accuracy,"Background:                    The large and increasing number of new studies published each year is making literature identification in systematic reviews ever more time-consuming and costly. Technological assistance has been suggested as an alternative to the conventional, manual study identification to mitigate the cost, but previous literature has mainly evaluated methods in terms of recall (search sensitivity) and workload reduction. There is a need to also evaluate whether screening prioritization methods leads to the same results and conclusions as exhaustive manual screening. In this study, we examined the impact of one screening prioritization method based on active learning on sensitivity and specificity estimates in systematic reviews of diagnostic test accuracy.              Methods:                    We simulated the screening process in 48 Cochrane reviews of diagnostic test accuracy and re-run 400 meta-analyses based on a least 3 studies. We compared screening prioritization (with technological assistance) and screening in randomized order (standard practice without technology assistance). We examined if the screening could have been stopped before identifying all relevant studies while still producing reliable summary estimates. For all meta-analyses, we also examined the relationship between the number of relevant studies and the reliability of the final estimates.              Results:                    The main meta-analysis in each systematic review could have been performed after screening an average of 30% of the candidate articles (range 0.07 to 100%). No systematic review would have required screening more than 2308 studies, whereas manual screening would have required screening up to 43,363 studies. Despite an average 70% recall, the estimation error would have been 1.3% on average, compared to an average 2% estimation error expected when replicating summary estimate calculations.              Conclusion:                    Screening prioritization coupled with stopping criteria in diagnostic test accuracy reviews can reliably detect when the screening process has identified a sufficient number of studies to perform the main meta-analysis with an accuracy within pre-specified tolerance limits. However, many of the systematic reviews did not identify a sufficient number of studies that the meta-analyses were accurate within a 2% limit even with exhaustive manual screening, i.e., using current practice.",2019 Oct 28;8(1):243.,
31661003,Recent advances in nanotheranostics for triple negative breast cancer treatment,"Triple-negative breast cancer (TNBC) is the most complex and aggressive type of breast cancer encountered world widely in women. Absence of hormonal receptors on breast cancer cells necessitates the chemotherapy as the only treatment regime. High propensity to metastasize and relapse in addition to poor prognosis and survival motivated the oncologist, nano-medical scientist to develop novel and efficient nanotherapies to solve such a big TNBC challenge. Recently, the focus for enhanced availability, targeted cellular uptake with minimal toxicity is achieved by nano-carriers. These smart nano-carriers carrying all the necessary arsenals (drugs, tracking probe, and ligand) designed in such a way that specifically targets the TNBC cells at site. Articulating the targeted delivery system with multifunctional molecules for high specificity, tracking, diagnosis, and treatment emerged as theranostic approach. In this review, in addition to classical treatment modalities, recent advances in nanotheranostics for early and effective diagnostic and treatment is discussed. This review highlighted the recently FDA approved immunotherapy and all the ongoing clinical trials for TNBC, in addition to nanoparticle assisted immunotherapy. Futuristic but realistic advancements in artificial intelligence (AI) and machine learning not only improve early diagnosis but also assist clinicians for their workup in TNBC. The novel concept of Nanoparticles induced endothelial leakiness (NanoEL) as a way of tumor invasion is also discussed in addition to classical EPR effect. This review intends to provide basic insight and understanding of the novel nano-therapeutic modalities in TNBC diagnosis and treatment and to sensitize the readers for continue designing the novel nanomedicine. This is the first time that designing nanoparticles with stoichiometric definable number of antibodies per nanoparticle now represents the next level of precision by design in nanomedicine.",2019 Oct 28;38(1):430.,20.0
