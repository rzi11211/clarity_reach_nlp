pmid,citations,title,date,text
29990622,21.0,Scoring of tumor-infiltrating lymphocytes: From visual estimation to machine learning,2018 Oct;52(Pt 2):151-157.,"The extent of tumor-infiltrating lymphocytes (TILs), along with immunomodulatory ligands, tumor-mutational burden and other biomarkers, has been demonstrated to be a marker of response to immune-checkpoint therapy in several cancers. Pathologists have therefore started to devise standardized visual approaches to quantify TILs for therapy prediction. However, despite successful standardization efforts visual TIL estimation is slow, with limited precision and lacks the ability to evaluate more complex properties such as TIL distribution patterns. Therefore, computational image analysis approaches are needed to provide standardized and efficient TIL quantification. Here, we discuss different automated TIL scoring approaches ranging from classical image segmentation, where cell boundaries are identified and the resulting objects classified according to shape properties, to machine learning-based approaches that directly classify cells without segmentation but rely on large amounts of training data. In contrast to conventional machine learning (ML) approaches that are often criticized for their ""black-box"" characteristics, we also discuss explainable machine learning. Such approaches render ML results interpretable and explain the computational decision-making process through high-resolution heatmaps that highlight TILs and cancer cells and therefore allow for quantification and plausibility checks in biomedical research and diagnostics."
29989994,18.0,Deep Learning in Microscopy Image Analysis: A Survey,2018 Oct;29(10):4550-4568.,"Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning techniques have powered many aspects of medical investigation and clinical practice. Recently, deep learning is emerging as a leading machine learning tool in computer vision and has attracted considerable attention in biomedical image analysis. In this paper, we provide a snapshot of this fast-growing field, specifically for microscopy image analysis. We briefly introduce the popular deep neural networks and summarize current deep learning achievements in various tasks, such as detection, segmentation, and classification in microscopy image analysis. In particular, we explain the architectures and the principles of convolutional neural networks, fully convolutional networks, recurrent neural networks, stacked autoencoders, and deep belief networks, and interpret their formulations or modelings for specific tasks on various microscopy images. In addition, we discuss the open challenges and the potential trends of future research in microscopy image analysis using deep learning."
29989977,99.0,Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis,2018 Sep;22(5):1589-1604.,"The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHRs). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and deidentification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research."
29986160,30.0,Computational Principles of Supervised Learning in the Cerebellum,2018 Jul 8;41:233-253.,"Supervised learning plays a key role in the operation of many biological and artificial neural networks. Analysis of the computations underlying supervised learning is facilitated by the relatively simple and uniform architecture of the cerebellum, a brain area that supports numerous motor, sensory, and cognitive functions. We highlight recent discoveries indicating that the cerebellum implements supervised learning using the following organizational principles: ( a) extensive preprocessing of input representations (i.e., feature engineering), ( b) massively recurrent circuit architecture, ( c) linear input-output computations, ( d) sophisticated instructive signals that can be regulated and are predictive, ( e) adaptive mechanisms of plasticity with multiple timescales, and ( f) task-specific hardware specializations. The principles emerging from studies of the cerebellum have striking parallels with those in other brain areas and in artificial neural networks, as well as some notable differences, which can inform future research on supervised learning and inspire next-generation machine-based algorithms."
29982543,14.0,A Bibliometric Analysis of the Landscape of Cancer Rehabilitation Research (1992-2016),2018 Aug 1;110(8):815-824.,"Cancer rehabilitation research has accelerated as great attention has focused on improving survivorship care. Recent expert consensus has attempted to prioritize research needs and suggests greater focus on studying physical functioning of survivors. However, no analysis of the publication landscape has substantiated these proposed needs. This manuscript provides an analysis of PubMed indexed articles related to cancer rehabilitation published between 1992 and 2017. A total of 22 171 publications were analyzed using machine learning and text analysis to assess publication metrics, topic areas of emphasis, and their interrelationships through topic similarity networks. Publications have increased at a rate of 136 articles per year. Approximately 10% of publications were funded by the National Institutes of Health institutes and centers, with the National Cancer Institute being the most prominent funder. The greatest volume and rate of publication increase were in the topics of Cognitive and Behavioral Therapies and Psychological Interventions, followed by Depression and Exercise Therapy. Four research topic similarity networks were identified and provide insight on areas of robust publication and notable deficits. Findings suggest that publication emphasis has strongly supported cognitive, behavioral, and psychological therapies; however, studies of functional morbidity and physical rehabilitation research are lacking. Three areas of publication deficits are noted: research on populations outside of breast, prostate, and lung cancers; methods for integrating physical rehabilitation services with cancer care, specifically regarding functional screening and assessment; and physical rehabilitation interventions. These deficits align with the needs identified by expert consensus and support the supposition that future research should emphasize a focus on physical rehabilitation."
29982332,25.0,Trends in the development of miRNA bioinformatics tools,2019 Sep 27;20(5):1836-1852.,"MicroRNAs (miRNAs) are small noncoding RNAs that regulate gene expression via recognition of cognate sequences and interference of transcriptional, translational or epigenetic processes. Bioinformatics tools developed for miRNA study include those for miRNA prediction and discovery, structure, analysis and target prediction. We manually curated 95 review papers and ∼1000 miRNA bioinformatics tools published since 2003. We classified and ranked them based on citation number or PageRank score, and then performed network analysis and text mining (TM) to study the miRNA tools development trends. Five key trends were observed: (1) miRNA identification and target prediction have been hot spots in the past decade; (2) manual curation and TM are the main methods for collecting miRNA knowledge from literature; (3) most early tools are well maintained and widely used; (4) classic machine learning methods retain their utility; however, novel ones have begun to emerge; (5) disease-associated miRNA tools are emerging. Our analysis yields significant insight into the past development and future directions of miRNA tools."
29981923,5.0,In-silico approach for drug induced liver injury prediction: Recent advances,2018 Oct 1;295:288-295.,"Drug induced liver injury (DILI) is the prime cause of liver disfunction which may lead to mild non-specific symptoms to more severe signs like hepatitis, cholestasis, cirrhosis and jaundice. Not only the prescription medications, but the consumption of herbs and health supplements have also been reported to cause these adverse reactions resulting into high mortality rates and post marketing withdrawal of drugs. Due to the continuously increasing DILI incidences in recent years, robust prediction methods with high accuracy, specificity and sensitivity are of priority. Bioinformatics is the emerging field of science that has been used in the past few years to explore the mechanisms of DILI. The major emphasis of this review is the recent advances of in silico tools for the diagnostic and therapeutic interventions of DILI. These tools have been developed and widely used in the past few years for the prediction of pathways induced from both hepatotoxic as well as hepatoprotective Chinese drugs and for the identification of DILI specific biomarkers for prognostic purpose. In addition to this, advanced machine learning models have been developed for the classification of drugs into DILI causing and non-DILI causing. Moreover, development of 3 class models over 2 class offers better understanding of multi-class DILI risks and at the same time providing authentic prediction of toxicity during drug designing before clinical trials."
29980865,11.0,Future Direction for Using Artificial Intelligence to Predict and Manage Hypertension,2018 Jul 6;20(9):75.,"Purpose of review:                    Evidence that artificial intelligence (AI) is useful for predicting risk factors for hypertension and its management is emerging. However, we are far from harnessing the innovative AI tools to predict these risk factors for hypertension and applying them to personalized management. This review summarizes recent advances in the computer science and medical field, illustrating the innovative AI approach for potential prediction of early stages of hypertension. Additionally, we review ongoing research and future implications of AI in hypertension management and clinical trials, with an eye towards personalized medicine.              Recent findings:                    Although recent studies demonstrate that AI in hypertension research is feasible and possibly useful, AI-informed care has yet to transform blood pressure (BP) control. This is due, in part, to lack of data on AI's consistency, accuracy, and reliability in the BP sphere. However, many factors contribute to poorly controlled BP, including biological, environmental, and lifestyle issues. AI allows insight into extrapolating data analytics to inform prescribers and patients about specific factors that may impact their BP control. To date, AI has been mainly used to investigate risk factors for hypertension, but has not yet been utilized for hypertension management due to the limitations of study design and of physician's engagement in computer science literature. The future of AI with more robust architecture using multi-omics approaches and wearable technology will likely be an important tool allowing to incorporate biological, lifestyle, and environmental factors into decision-making of appropriate drug use for BP control."
29977864,18.0,Machine Learning and Radiogenomics: Lessons Learned and Future Directions,2018 Jun 21;8:228.,"Due to the rapid increase in the availability of patient data, there is significant interest in precision medicine that could facilitate the development of a personalized treatment plan for each patient on an individual basis. Radiation oncology is particularly suited for predictive machine learning (ML) models due to the enormous amount of diagnostic data used as input and therapeutic data generated as output. An emerging field in precision radiation oncology that can take advantage of ML approaches is radiogenomics, which is the study of the impact of genomic variations on the sensitivity of normal and tumor tissue to radiation. Currently, patients undergoing radiotherapy are treated using uniform dose constraints specific to the tumor and surrounding normal tissues. This is suboptimal in many ways. First, the dose that can be delivered to the target volume may be insufficient for control but is constrained by the surrounding normal tissue, as dose escalation can lead to significant morbidity and rare. Second, two patients with nearly identical dose distributions can have substantially different acute and late toxicities, resulting in lengthy treatment breaks and suboptimal control, or chronic morbidities leading to poor quality of life. Despite significant advances in radiogenomics, the magnitude of the genetic contribution to radiation response far exceeds our current understanding of individual risk variants. In the field of genomics, ML methods are being used to extract harder-to-detect knowledge, but these methods have yet to fully penetrate radiogenomics. Hence, the goal of this publication is to provide an overview of ML as it applies to radiogenomics. We begin with a brief history of radiogenomics and its relationship to precision medicine. We then introduce ML and compare it to statistical hypothesis testing to reflect on shared lessons and to avoid common pitfalls. Current ML approaches to genome-wide association studies are examined. The application of ML specifically to radiogenomics is next presented. We end with important lessons for the proper integration of ML into radiogenomics."
29977480,5.0,An Artificial Neural Network Integrated Pipeline for Biomarker Discovery Using Alzheimer's Disease as a Case Study,2018 Feb 21;16:77-87.,"The field of machine learning has allowed researchers to generate and analyse vast amounts of data using a wide variety of methodologies. Artificial Neural Networks (ANN) are some of the most commonly used statistical models and have been successful in biomarker discovery studies in multiple disease types. This review seeks to explore and evaluate an integrated ANN pipeline for biomarker discovery and validation in Alzheimer's disease, the most common form of dementia worldwide with no proven cause and no available cure. The proposed pipeline consists of analysing public data with a categorical and continuous stepwise algorithm and further examination through network inference to predict gene interactions. This methodology can reliably generate novel markers and further examine known ones and can be used to guide future research in Alzheimer's disease."
29974498,14.0,"Automation, machine learning, and artificial intelligence in echocardiography: A brave new world",2018 Sep;35(9):1402-1418.,"Automation, machine learning, and artificial intelligence (AI) are changing the landscape of echocardiography providing complimentary tools to physicians to enhance patient care. Multiple vendor software programs have incorporated automation to improve accuracy and efficiency of manual tracings. Automation with longitudinal strain and 3D echocardiography has shown great accuracy and reproducibility allowing the incorporation of these techniques into daily workflow. This will give further experience to nonexpert readers and allow the integration of these essential tools into more echocardiography laboratories. The potential for machine learning in cardiovascular imaging is still being discovered as algorithms are being created, with training on large data sets beyond what traditional statistical reasoning can handle. Deep learning when applied to large image repositories will recognize complex relationships and patterns integrating all properties of the image, which will unlock further connections about the natural history and prognosis of cardiac disease states. The purpose of this review article was to describe the role and current use of automation, machine learning, and AI in echocardiography and discuss potential limitations and challenges of in the future."
29972344,5.0,Oximetry use in obstructive sleep apnea,2018 Aug;12(8):665-681.,"Overnight oximetry has been proposed as an accessible, simple, and reliable technique for obstructive sleep apnea syndrome (OSAS) diagnosis. From visual inspection to advanced signal processing, several studies have demonstrated the usefulness of oximetry as a screening tool. However, there is still controversy regarding the general application of oximetry as a single screening methodology for OSAS. Areas covered: Currently, high-resolution portable devices combined with pattern recognition-based applications are able to achieve high performance in the detection of this disease. In this review, recent studies involving automated analysis of oximetry by means of advanced signal processing and machine learning algorithms are analyzed. Advantages and limitations are highlighted and novel research lines aimed at improving the screening ability of oximetry are proposed. Expert commentary: Oximetry is a cost-effective tool for OSAS screening in patients showing high pretest probability for the disease. Nevertheless, exhaustive analyses are still needed to further assess unattended oximetry monitoring as a single diagnostic test for sleep apnea, particularly in the pediatric population and in populations with significant comorbidities. In the following years, communication technologies and big data analyses will overcome current limitations of simplified sleep testing approaches, changing the detection and management of OSAS."
29970965,3.0,Cancer risk assessment in modern radiotherapy workflow with medical big data,2018 Jun 22;10:1665-1675.,"Modern radiotherapy (RT) is being enriched by big digital data and intensive technology. Multimodality image registration, intelligence-guided planning, real-time tracking, image-guided RT (IGRT), and automatic follow-up surveys are the products of the digital era. Enormous digital data are created in the process of treatment, including benefits and risks. Generally, decision making in RT tries to balance these two aspects, which is based on the archival and retrieving of data from various platforms. However, modern risk-based analysis shows that many errors that occur in radiation oncology are due to failures in workflow. These errors can lead to imbalance between benefits and risks. In addition, the exact mechanism and dose-response relationship for radiation-induced malignancy are not well understood. The cancer risk in modern RT workflow continues to be a problem. Therefore, in this review, we develop risk assessments based on our current knowledge of IGRT and provide strategies for cancer risk reduction. Artificial intelligence (AI) such as machine learning is also discussed because big data are transforming RT via AI."
29970286,,Non-invasive imaging techniques and assessment of carotid vasa vasorum neovascularization: Promises and pitfalls,2019 Feb;29(2):71-80.,"Carotid adventitia vasa vasorum neovascularization (VVn) is associated with the initial stages of arteriosclerosis and with the formation of unstable plaque. However, techniques to accurately quantify that neovascularization in a standard, fast, non-invasive, and efficient way are still lacking. The development of such techniques holds the promise of enabling wide, inexpensive, and safe screening programs that could stratify patients and help in personalized preventive cardiovascular medicine. In this paper, we review the recent scientific literature pertaining to imaging techniques that could set the stage for the development of standard methods for quantitative assessment of atherosclerotic plaque and carotid VVn. We present and discuss the alternative imaging techniques being used in clinical practice and we review the computational developments that are contributing to speed up image analysis and interpretation. We conclude that one of the greatest upcoming challenges will be the use of machine learning techniques to develop automated methods that assist in the interpretation of images to stratify patients according to their risk."
29959818,9.0,Materials Nanoarchitectonics for Mechanical Tools in Chemical and Biological Sensing,2018 Nov 16;13(22):3366-3377.,"In this Focus Review, nanoarchitectonic approaches for mechanical-action-based chemical and biological sensors are briefly discussed. In particular, recent examples of piezoelectric devices, such as quartz crystal microbalances (QCM and QCM-D) and a membrane-type surface stress sensor (MSS), are introduced. Sensors need well-designed nanostructured sensing materials for the sensitive and selective detection of specific targets. Nanoarchitectonic approaches for sensing materials, such as mesoporous materials, 2D materials, fullerene assemblies, supported lipid bilayers, and layer-by-layer assemblies, are highlighted. Based on these sensing approaches, examples of bioanalytical applications are presented for toxic gas detection, cell membrane interactions, label-free biomolecular assays, anticancer drug evaluation, complement activation-related multiprotein membrane attack complexes, and daily biodiagnosis, which are partially supported by data analysis, such as machine learning and principal component analysis."
31975919,5.0,Digital Technologies in Psychiatry: Present and Future,2018 Jul;16(3):251-258.,"The digital revolution has reached the world of mental health. Prominent examples include the rapidly growing use of mobile health apps, the integration of sophisticated machine learning or artificial intelligence for clinical decision support and automated therapy, and the incorporation of virtual reality-based treatments. These diverse technologies hold the promise of addressing several important problems in mental health care, including lack of measurement, uneven access to clinicians, delay in receiving care, fragmentation of care, and negative attitudes toward psychiatry. Here, the authors summarize the current and swiftly changing state of digital mental health. Specifically, they highlight the current unmet needs that emerging technologies may be able to address; summarize what digital health can offer for assessment, treatment, and care integration; and describe some of the challenges and some new directions for innovations in this field. The review concludes with guidance for clinicians to integrate digital technologies into their work and to provide responsible and useful advice to their patients."
29957849,13.0,Silicon Oxide (SiO x ): A Promising Material for Resistance Switching?,2018 Oct;30(43):e1801187.,"Interest in resistance switching is currently growing apace. The promise of novel high-density, low-power, high-speed nonvolatile memory devices is appealing enough, but beyond that there are exciting future possibilities for applications in hardware acceleration for machine learning and artificial intelligence, and for neuromorphic computing. A very wide range of material systems exhibit resistance switching, a number of which-primarily transition metal oxides-are currently being investigated as complementary metal-oxide-semiconductor (CMOS)-compatible technologies. Here, the case is made for silicon oxide, perhaps the most CMOS-compatible dielectric, yet one that has had comparatively little attention as a resistance-switching material. Herein, a taxonomy of switching mechanisms in silicon oxide is presented, and the current state of the art in modeling, understanding fundamental switching mechanisms, and exciting device applications is summarized. In conclusion, silicon oxide is an excellent choice for resistance-switching technologies, offering a number of compelling advantages over competing material systems."
29956866,5.0,"Utilizing state-of-the-art ""omics"" technology and bioinformatics to identify new biological mechanisms and biomarkers for coronary artery disease",2019 Feb;26(2):e12488.,"Identification of the four standard modifiable cardiovascular risk factors (SMuRFs)-diabetes mellitus, hyperlipidaemia, hypertension, and cigarette smoking-has allowed the development of risk scores. These have been used in conjunction with primary and secondary prevention strategies targeting SMuRFs to reduce the burden of CAD. Recent studies show that up to 25% of ACS patients do not have any SMuRFs. Thus, SMuRFs do not explain the entire burden of CAD. There appears to be variation at the individual level rendering some individuals relatively susceptible or resilient to developing atherosclerosis. Important disease pathways remain to be discovered, and there is renewed enthusiasm to discover novel biomarkers, biological mechanisms, and therapeutic targets for atherosclerosis. Two broad approaches are being taken: traditional approaches investigating known candidate pathways and unbiased omics approaches. We review recent progress in the field and discuss opportunities made possible by technological and data science advances. Developments in network analytics and machine learning algorithms used in conjunction with large-scale multi-omic platforms have the potential to uncover biological networks that may not have been identifiable using traditional approaches. These approaches are useful for both biomedical research and precision medicine strategies."
29956120,33.0,"Smartphones, Sensors, and Machine Learning to Advance Real-Time Prediction and Interventions for Suicide Prevention: a Review of Current Progress and Next Steps",2018 Jun 28;20(7):51.,"Purpose of review:                    As rates of suicide continue to rise, there is urgent need for innovative approaches to better understand, predict, and care for those at high risk of suicide. Numerous mobile and sensor technology solutions have already been proposed, are in development, or are already available today. This review seeks to assess their clinical evidence and help the reader understand the current state of the field.              Recent findings:                    Advances in smartphone sensing, machine learning methods, and mobile apps directed towards reducing suicide offer promising evidence; however, most of these innovative approaches are still nascent. Further replication and validation of preliminary results is needed. Whereas numerous promising mobile and sensor technology based solutions for real time understanding, predicting, and caring for those at highest risk of suicide are being studied today, their clinical utility remains largely unproven. However, given both the rapid pace and vast scale of current research efforts, we expect clinicians will soon see useful and impactful digital tools for this space within the next 2 to 5 years."
29956014,15.0,A Survey of Data Mining and Deep Learning in Bioinformatics,2018 Jun 28;42(8):139.,"The fields of medicine science and health informatics have made great progress recently and have led to in-depth analytics that is demanded by generation, collection and accumulation of massive data. Meanwhile, we are entering a new period where novel technologies are starting to analyze and explore knowledge from tremendous amount of data, bringing limitless potential for information growth. One fact that cannot be ignored is that the techniques of machine learning and deep learning applications play a more significant role in the success of bioinformatics exploration from biological data point of view, and a linkage is emphasized and established to bridge these two data analytics techniques and bioinformatics in both industry and academia. This survey concentrates on the review of recent researches using data mining and deep learning approaches for analyzing the specific domain knowledge of bioinformatics. The authors give a brief but pithy summarization of numerous data mining algorithms used for preprocessing, classification and clustering as well as various optimized neural network architectures in deep learning methods, and their advantages and disadvantages in the practical applications are also discussed and compared in terms of their industrial usage. It is believed that in this review paper, valuable insights are provided for those who are dedicated to start using data analytics methods in bioinformatics."
29953863,18.0,Phenotypic Image Analysis Software Tools for Exploring and Understanding Big Image Data from Cell-Based Assays,2018 Jun 27;6(6):636-653.,"Phenotypic image analysis is the task of recognizing variations in cell properties using microscopic image data. These variations, produced through a complex web of interactions between genes and the environment, may hold the key to uncover important biological phenomena or to understand the response to a drug candidate. Today, phenotypic analysis is rarely performed completely by hand. The abundance of high-dimensional image data produced by modern high-throughput microscopes necessitates computational solutions. Over the past decade, a number of software tools have been developed to address this need. They use statistical learning methods to infer relationships between a cell's phenotype and data from the image. In this review, we examine the strengths and weaknesses of non-commercial phenotypic image analysis software, cover recent developments in the field, identify challenges, and give a perspective on future possibilities."
29953247,7.0,The current limits in virtual screening and property prediction,2018 Jul 1;10(13):1623-1635.,"Beyond finding inhibitors that show high binding affinity to the respective target, there is the challenge of optimizing their properties with respect to metabolic and toxicological issues, as well as further off-target effects. To reduce the experimental effort of synthesizing and testing actual substances in corresponding assays, virtual screening has become an indispensable toolbox in preclinical development. The scope of application covers the prediction of molecular properties including solubility, metabolic liability and binding to antitargets, such as the hERG channel. Furthermore, prediction of binding sites and drugable targets are emerging aspects of virtual screening. Issues involved with the currently applied computational models including machine learning algorithms are outlined, such as limitations to the accuracy of prediction and overfitting."
29952835,6.0,MRI-based neuroimaging: atypical parkinsonisms and other movement disorders,2018 Aug;31(4):425-430.,"Purpose of review:                    MRI has become a well established technical tool for parkinsonism both in the diagnostic work-up to differentiate between causes and to serve as a neurobiological marker. This review summarizes current developments in the advanced MRI-based assessment of brain structure and function in atypical parkinsonian syndromes and explores their potential in a clinical and neuroscientific setting.              Recent findings:                    Computer-based unbiased quantitative MRI analyses were demonstrated to guide in the discrimination of parkinsonian syndromes at single-patient level, with major contributions when combined with machine-learning techniques/support vector machine classification. These techniques have shown their potential in tracking the disease progression, perhaps also as a read-out in clinical trials. The characterization of different brain compartments at various levels of structural and functional alterations can be provided by multiparametric MRI, including a growing variety of diffusion-weighted imaging approaches and potentially iron-sensitive and functional MRI.              Summary:                    In case that the recent advances in the MRI-based assessment of atypical parkinsonism will lead to standardized protocols for image acquisition and analysis after the confirmation in large-scale multicenter studies, these approaches may constitute a great achievement in the (operator-independent) detection, discrimination and characterization of degenerative parkinsonian disorders at an individual basis."
29944078,92.0,Current Applications and Future Impact of Machine Learning in Radiology,2018 Aug;288(2):318-328.,"Recent advances and future perspectives of machine learning techniques offer promising applications in medical imaging. Machine learning has the potential to improve different steps of the radiology workflow including order scheduling and triage, clinical decision support systems, detection and interpretation of findings, postprocessing and dose estimation, examination quality control, and radiology reporting. In this article, the authors review examples of current applications of machine learning and artificial intelligence techniques in diagnostic radiology. In addition, the future impact and natural extension of these techniques in radiology practice are discussed."
29935311,29.0,Molecular pathway activation - New type of biomarkers for tumor morphology and personalized selection of target drugs,2018 Dec;53:110-124.,"Anticancer target drugs (ATDs) specifically bind and inhibit molecular targets that play important roles in cancer development and progression, being deeply implicated in intracellular signaling pathways. To date, hundreds of different ATDs were approved for clinical use in the different countries. Compared to previous chemotherapy treatments, ATDs often demonstrate reduced side effects and increased efficiency, but also have higher costs. However, the efficiency of ATDs for the advanced stage tumors is still insufficient. Different ATDs have different mechanisms of action and are effective in different cohorts of patients. Personalized approaches are therefore needed to select the best ATD candidates for the individual patients. In this review, we focus on a new generation of biomarkers - molecular pathway activation - and on their applications for predicting individual tumor response to ATDs. The success in high throughput gene expression profiling and emergence of novel bioinformatic tools reinforced quick development of pathway related field of molecular biomedicine. The ability to quantitatively measure degree of a pathway activation using gene expression data has revolutionized this field and made the corresponding analysis quick, robust and inexpensive. This success was further enhanced by using machine learning algorithms for selection of the best biomarkers. We review here the current progress in translating these studies to clinical oncology and patient-oriented adjustment of cancer therapy."
29934920,85.0,Convolutional neural networks: an overview and application in radiology,2018 Aug;9(4):611-629.,"Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care. KEY POINTS: • Convolutional neural network is a class of deep learning methods which has become dominant in various computer vision tasks and is attracting interest across a variety of domains, including radiology. • Convolutional neural network is composed of multiple building blocks, such as convolution layers, pooling layers, and fully connected layers, and is designed to automatically and adaptively learn spatial hierarchies of features through a backpropagation algorithm. • Familiarity with the concepts and advantages, as well as limitations, of convolutional neural network is essential to leverage its potential to improve radiologist performance and, eventually, patient care."
29934910,,Predictive Systems Toxicology,2018;1800:535-557.,"In this review we address to what extent computational techniques can augment our ability to predict toxicity. The first section provides a brief history of empirical observations on toxicity dating back to the dawn of Sumerian civilization. Interestingly, the concept of dose emerged very early on, leading up to the modern emphasis on kinetic properties, which in turn encodes the insight that toxicity is not solely a property of a compound but instead depends on the interaction with the host organism. The next logical step is the current conception of evaluating drugs from a personalized medicine point of view. We review recent work on integrating what could be referred to as classical pharmacokinetic analysis with emerging systems biology approaches incorporating multiple omics data. These systems approaches employ advanced statistical analytical data processing complemented with machine learning techniques and use both pharmacokinetic and omics data. We find that such integrated approaches not only provide improved predictions of toxicity but also enable mechanistic interpretations of the molecular mechanisms underpinning toxicity and drug resistance. We conclude the chapter by discussing some of the main challenges, such as how to balance the inherent tension between the predicitive capacity of models, which in practice amounts to constraining the number of features in the models versus allowing for rich mechanistic interpretability, i.e., equipping models with numerous molecular features. This challenge also requires patient-specific predictions on toxicity, which in turn requires proper stratification of patients as regards how they respond, with or without adverse toxic effects. In summary, the transformation of the ancient concept of dose is currently successfully operationalized using rich integrative data encoded in patient-specific models."
29928237,6.0,Neuromarkers for Mental Disorders: Harnessing Population Neuroscience,2018 Jun 6;9:242.,"Despite abundant research into the neurobiology of mental disorders, to date neurobiological insights have had very little impact on psychiatric diagnosis or treatment. In this review, we contend that the search for neuroimaging biomarkers-neuromarkers-of mental disorders is a highly promising avenue toward improved psychiatric healthcare. However, many of the traditional tools used for psychiatric neuroimaging are inadequate for the identification of neuromarkers. Specifically, we highlight the need for larger samples and for multivariate analysis. Approaches such as machine learning are likely to be beneficial for interrogating high-dimensional neuroimaging data. We suggest that broad, population-based study designs will be important for developing neuromarkers of mental disorders, and will facilitate a move away from a phenomenological definition of mental disorder categories and toward psychiatric nosology based on biological evidence. We provide an outline of how the development of neuromarkers should occur, emphasizing the need for tests of external and construct validity, and for collaborative research efforts. Finally, we highlight some concerns regarding the development, and use of, neuromarkers in psychiatric healthcare."
29926817,6.0,How random is the random forest? Random forest algorithm on the service of structural imaging biomarkers for Alzheimer's disease: from Alzheimer's disease neuroimaging initiative (ADNI) database,2018 Jun;13(6):962-970.,"Neuroinformatics is a fascinating research field that applies computational models and analytical tools to high dimensional experimental neuroscience data for a better understanding of how the brain functions or dysfunctions in brain diseases. Neuroinformaticians work in the intersection of neuroscience and informatics supporting the integration of various sub-disciplines (behavioural neuroscience, genetics, cognitive psychology, etc.) working on brain research. Neuroinformaticians are the pathway of information exchange between informaticians and clinicians for a better understanding of the outcome of computational models and the clinical interpretation of the analysis. Machine learning is one of the most significant computational developments in the last decade giving tools to neuroinformaticians and finally to radiologists and clinicians for an automatic and early diagnosis-prognosis of a brain disease. Random forest (RF) algorithm has been successfully applied to high-dimensional neuroimaging data for feature reduction and also has been applied to classify the clinical label of a subject using single or multi-modal neuroimaging datasets. Our aim was to review the studies where RF was applied to correctly predict the Alzheimer's disease (AD), the conversion from mild cognitive impairment (MCI) and its robustness to overfitting, outliers and handling of non-linear data. Finally, we described our RF-based model that gave us the 1st position in an international challenge for automated prediction of MCI from MRI data."
29925268,4.0,Multimodal Neuroimaging: Basic Concepts and Classification of Neuropsychiatric Diseases,2019 Jan;50(1):20-33.,"Neuroimaging techniques are widely used in neuroscience to visualize neural activity, to improve our understanding of brain mechanisms, and to identify biomarkers-especially for psychiatric diseases; however, each neuroimaging technique has several limitations. These limitations led to the development of multimodal neuroimaging (MN), which combines data obtained from multiple neuroimaging techniques, such as electroencephalography, functional magnetic resonance imaging, and yields more detailed information about brain dynamics. There are several types of MN, including visual inspection, data integration, and data fusion. This literature review aimed to provide a brief summary and basic information about MN techniques (data fusion approaches in particular) and classification approaches. Data fusion approaches are generally categorized as asymmetric and symmetric. The present review focused exclusively on studies based on symmetric data fusion methods (data-driven methods), such as independent component analysis and principal component analysis. Machine learning techniques have recently been introduced for use in identifying diseases and biomarkers of disease. The machine learning technique most widely used by neuroscientists is classification-especially support vector machine classification. Several studies differentiated patients with psychiatric diseases and healthy controls with using combined datasets. The common conclusion among these studies is that the prediction of diseases increases when combining data via MN techniques; however, there remain a few challenges associated with MN, such as sample size. Perhaps in the future N-way fusion can be used to combine multiple neuroimaging techniques or nonimaging predictors (eg, cognitive ability) to overcome the limitations of MN."
29923622,3.0,Advances in the computational and molecular understanding of the prostate cancer cell nucleus,2018 Sep;119(9):7127-7142.,"Nuclear alterations are a hallmark of many types of cancers, including prostate cancer (PCa). Recent evidence shows that subvisual changes, ones that may not be visually perceptible to a pathologist, to the nucleus and its ultrastructural components can precede visual histopathological recognition of cancer. Alterations to nuclear features, such as nuclear size and shape, texture, and spatial architecture, reflect the complex molecular-level changes that occur during oncogenesis. Quantitative nuclear morphometry, a field that uses computational approaches to identify and quantify malignancy-induced nuclear changes, can enable a detailed and objective analysis of the PCa cell nucleus. Recent advances in machine learning-based approaches can now automatically mine data related to these changes to aid in the diagnosis, decision making, and prediction of PCa prognoses. In this review, we use PCa as a case study to connect the molecular-level mechanisms that underlie these nuclear changes to the machine learning computational approaches, bridging the gap between the clinical and computational understanding of PCa. First, we will discuss recent developments to our understanding of the molecular events that drive nuclear alterations in the context of PCa: the role of the nuclear matrix and lamina in size and shape changes, the role of 3-dimensional chromatin organization and epigenetic modifications in textural changes, and the role of the tumor microenvironment in altering nuclear spatial topology. We will then discuss the advances in the applications of machine learning algorithms to automatically segment nuclei in prostate histopathological images, extract nuclear features to aid in diagnostic decision making, and predict potential outcomes, such as biochemical recurrence and survival. Finally, we will discuss the challenges and opportunities associated with translation of the quantitative nuclear morphometry methodology into the clinical space. Ultimately, accurate identification and quantification of nuclear alterations can contribute to the field of nucleomics and has applications for computationally driven precision oncologic patient care."
29915783,3.0,Computational Chemical Synthesis Analysis and Pathway Design,2018 Jun 5;6:199.,"With the idea of retrosynthetic analysis, which was raised in the 1960s, chemical synthesis analysis and pathway design have been transformed from a complex problem to a regular process of structural simplification. This review aims to summarize the developments of computer-assisted synthetic analysis and design in recent years, and how machine-learning algorithms contributed to them. LHASA system started the pioneering work of designing semi-empirical reaction modes in computers, with its following rule-based and network-searching work not only expanding the databases, but also building new approaches to indicating reaction rules. Programs like ARChem Route Designer replaced hand-coded reaction modes with automatically-extracted rules, and programs like Chematica changed traditional designing into network searching. Afterward, with the help of machine learning, two-step models which combine reaction rules and statistical methods became the main stream. Recently, fully data-driven learning methods using deep neural networks which even do not require any prior knowledge, were applied into this field. Up to now, however, these methods still cannot replace experienced human organic chemists due to their relatively low accuracies. Future new algorithms with the aid of powerful computational hardware will make this topic promising and with good prospects."
29908150,3.0,Microscopy in Infectious Disease Research-Imaging Across Scales,2018 Aug 17;430(17):2612-2625.,"A comprehensive understanding of host-pathogen interactions requires quantitative assessment of molecular events across a wide range of spatiotemporal scales and organizational complexities. Due to recent technical developments, this is currently only achievable with microscopy. This article is providing a general perspective on the importance of microscopy in infectious disease research, with a focus on new imaging modalities that promise to have a major impact in biomedical research in the years to come. Every major technological breakthrough in light microscopy depends on, and is supported by, advancements in computing and information technologies. Bioimage acquisition and analysis based on machine learning will pave the way toward more robust, automated and objective implementation of new imaging modalities and in biomedical research in general. The combination of novel imaging technologies with machine learning and near-physiological model systems promises to accelerate discoveries and breakthroughs in our understanding of infectious diseases, from basic research all the way to clinical applications."
29907338,28.0,Artificial intelligence in radiation oncology: A specialty-wide disruptive transformation?,2018 Dec;129(3):421-426.,"Artificial intelligence (AI) is emerging as a technology with the power to transform established industries, and with applications from automated manufacturing to advertising and facial recognition to fully autonomous transportation. Advances in each of these domains have led some to call AI the ""fourth"" industrial revolution [1]. In healthcare, AI is emerging as both a productive and disruptive force across many disciplines. This is perhaps most evident in Diagnostic Radiology and Pathology, specialties largely built around the processing and complex interpretation of medical images, where the role of AI is increasingly seen as both a boon and a threat. In Radiation Oncology as well, AI seems poised to reshape the specialty in significant ways, though the impact of AI has been relatively limited at present, and may rightly seem more distant to many, given the predominantly interpersonal and complex interventional nature of the specialty. In this overview, we will explore the current state and anticipated future impact of AI on Radiation Oncology, in detail, focusing on key topics from multiple stakeholder perspectives, as well as the role our specialty may play in helping to shape the future of AI within the larger spectrum of medicine."
29904616,3.0,Artificial Intelligence in Medicine and Radiation Oncology,2018 Apr 13;10(4):e2475.,Artifical Intelligence (AI) was reviewed with a focus on its potential applicability to radiation oncology. The improvement of process efficiencies and the prevention of errors were found to be the most significant contributions of AI to radiation oncology. It was found that the prevention of errors is most effective when data transfer processes were automated and operational decisions were based on logical or learned evaluations by the system. It was concluded that AI could greatly improve the efficiency and accuracy of radiation oncology operations.
29900088,2.0,Improving the Standard for Deep Brain Stimulation Therapy: Target Structures and Feedback Signals for Adaptive Stimulation. Current Perspectives and Future Directions,2018 Apr 12;10(4):e2468.,"Deep brain stimulation (DBS) is an established therapeutic option for the treatment of various neurological disorders and has been used successfully in movement disorders for over 25 years. However, the standard stimulation schemes have not changed substantially. Two major points of interest for the further development of DBS are target-structures and novel adaptive stimulation techniques integrating feedback signals. We describe recent research results on target structures and on neural and behavioural feedback signals for adaptive deep brain stimulation (aDBS), as well as outline future directions."
29897410,40.0,iProt-Sub: a comprehensive package for accurately mapping and predicting protease-specific substrates and cleavage sites,2019 Mar 25;20(2):638-658.,"Regulation of proteolysis plays a critical role in a myriad of important cellular processes. The key to better understanding the mechanisms that control this process is to identify the specific substrates that each protease targets. To address this, we have developed iProt-Sub, a powerful bioinformatics tool for the accurate prediction of protease-specific substrates and their cleavage sites. Importantly, iProt-Sub represents a significantly advanced version of its successful predecessor, PROSPER. It provides optimized cleavage site prediction models with better prediction performance and coverage for more species-specific proteases (4 major protease families and 38 different proteases). iProt-Sub integrates heterogeneous sequence and structural features and uses a two-step feature selection procedure to further remove redundant and irrelevant features in an effort to improve the cleavage site prediction accuracy. Features used by iProt-Sub are encoded by 11 different sequence encoding schemes, including local amino acid sequence profile, secondary structure, solvent accessibility and native disorder, which will allow a more accurate representation of the protease specificity of approximately 38 proteases and training of the prediction models. Benchmarking experiments using cross-validation and independent tests showed that iProt-Sub is able to achieve a better performance than several existing generic tools. We anticipate that iProt-Sub will be a powerful tool for proteome-wide prediction of protease-specific substrates and their cleavage sites, and will facilitate hypothesis-driven functional interrogation of protease-specific substrate cleavage and proteolytic events."
29893792,17.0,Regulatory variants: from detection to predicting impact,2019 Sep 27;20(5):1639-1654.,"Variants within non-coding genomic regions can greatly affect disease. In recent years, increasing focus has been given to these variants, and how they can alter regulatory elements, such as enhancers, transcription factor binding sites and DNA methylation regions. Such variants can be considered regulatory variants. Concurrently, much effort has been put into establishing international consortia to undertake large projects aimed at discovering regulatory elements in different tissues, cell lines and organisms, and probing the effects of genetic variants on regulation by measuring gene expression. Here, we describe methods and techniques for discovering disease-associated non-coding variants using sequencing technologies. We then explain the computational procedures that can be used for annotating these variants using the information from the aforementioned projects, and prediction of their putative effects, including potential pathogenicity, based on rule-based and machine learning approaches. We provide the details of techniques to validate these predictions, by mapping chromatin-chromatin and chromatin-protein interactions, and introduce Clustered Regularly Interspaced Short Palindromic Repeats-Associated Protein 9 (CRISPR-Cas9) technology, which has already been used in this field and is likely to have a big impact on its future evolution. We also give examples of regulatory variants associated with multiple complex diseases. This review is aimed at bioinformaticians interested in the characterization of regulatory variants, molecular biologists and geneticists interested in understanding more about the nature and potential role of such variants from a functional point of views, and clinicians who may wish to learn about variants in non-coding genomic regions associated with a given disease and find out what to do next to uncover how they impact on the underlying mechanisms."
29887378,113.0,Next-Generation Machine Learning for Biological Networks,2018 Jun 14;173(7):1581-1592.,"Machine learning, a collection of data-analytical techniques aimed at building predictive models from multi-dimensional datasets, is becoming integral to modern biological research. By enabling one to generate models that learn from large datasets and make predictions on likely outcomes, machine learning can be used to study complex cellular systems such as biological networks. Here, we provide a primer on machine learning for life scientists, including an introduction to deep learning. We discuss opportunities and challenges at the intersection of machine learning and network biology, which could impact disease biology, drug discovery, microbiome research, and synthetic biology."
29885419,11.0,Secondary brain injury: Predicting and preventing insults,2019 Feb;145(Pt B):145-152.,"Mortality or severe disability affects the majority of patients after severe traumatic brain injury (TBI). Adherence to the brain trauma foundation guidelines has overall improved outcomes; however, traditional as well as novel interventions towards intracranial hypertension and secondary brain injury have come under scrutiny after series of negative randomized controlled trials. In fact, it would not be unfair to say there has been no single major breakthrough in the management of severe TBI in the last two decades. One plausible hypothesis for the aforementioned failures is that by the time treatment is initiated for neuroprotection, or physiologic optimization, irreversible brain injury has already set in. We, and others, have recently developed predictive models based on machine learning from continuous time series of intracranial pressure and partial brain tissue oxygenation. These models provide accurate predictions of physiologic crises events in a timely fashion, offering the opportunity for an earlier application of targeted interventions. In this article, we review the rationale for prediction, discuss available predictive models with examples, and offer suggestions for their future prospective testing in conjunction with preventive clinical algorithms. This article is part of the Special Issue entitled ""Novel Treatments for Traumatic Brain Injury""."
29885309,20.0,Micro RNA as a potential blood-based epigenetic biomarker for Alzheimer's disease,2018 Aug;58:5-14.,"As the prevalence of Alzheimer's disease (AD) increases, the search for a definitive, easy to access diagnostic biomarker has become increasingly important. Micro RNA (miRNA), involved in the epigenetic regulation of protein synthesis, is a biological mark which varies in association with a number of disease states, possibly including AD. Here we comprehensively review methods and findings from 26 studies comparing the measurement of miRNA in blood between AD cases and controls. Thirteen of these studies used receiver operator characteristic (ROC) analysis to determine the diagnostic accuracy of identified miRNA to predict AD, and three studies did this with a machine learning approach. Of 8098 individually measured miRNAs, 23 that were differentially expressed between AD cases and controls were found to be significant in two or more studies. Only six of these were consistent in their direction of expression between studies (miR-107, miR-125b, miR-146a, miR-181c, miR-29b, and miR-342), and they were all shown to be down regulated in individuals with AD compared to controls. Of these directionally concordant miRNAs, the strongest evidence was for miR-107 which has also been shown in previous studies to be involved in the dysregulation of proteins involved in aspects of AD pathology, as well as being consistently downregulated in studies of AD brains. We conclude that imperative to the discovery of reliable and replicable miRNA biomarkers of AD, standardised methods of measurements, appropriate statistical analysis, utilization of large datasets with machine learning approaches, and comprehensive reporting of findings is urgently needed."
29884988,11.0,Natural Language Processing and Its Implications for the Future of Medication Safety: A Narrative Review of Recent Advances and Challenges,2018 Aug;38(8):822-841.,"The safety of medication use has been a priority in the United States since the late 1930s. Recently, it has gained prominence due to the increasing amount of data suggesting that a large amount of patient harm is preventable and can be mitigated with effective risk strategies that have not been sufficiently adopted. Adverse events from medications are part of clinical practice, but the ability to identify a patient's risk and to minimize that risk must be a priority. The ability to identify adverse events has been a challenge due to limitations of available data sources, which are often free text. The use of natural language processing (NLP) may help to address these limitations. NLP is the artificial intelligence domain of computer science that uses computers to manipulate unstructured data (i.e., narrative text or speech data) in the context of a specific task. In this narrative review, we illustrate the fundamentals of NLP and discuss NLP's application to medication safety in four data sources: electronic health records, Internet-based data, published literature, and reporting systems. Given the magnitude of available data from these sources, a growing area is the use of computer algorithms to help automatically detect associations between medications and adverse effects. The main benefit of NLP is in the time savings associated with automation of various medication safety tasks such as the medication reconciliation process facilitated by computers, as well as the potential for near-real-time identification of adverse events for postmarketing surveillance such as those posted on social media that would otherwise go unanalyzed. NLP is limited by a lack of data sharing between health care organizations due to insufficient interoperability capabilities, inhibiting large-scale adverse event monitoring across populations. We anticipate that future work in this area will focus on the integration of data sources from different domains to improve the ability to identify potential adverse events more quickly and to improve clinical decision support with regard to a patient's estimated risk for specific adverse events at the time of medication prescription or review."
29882974,4.0,Neuroimaging Studies Illustrate the Commonalities Between Ageing and Brain Diseases,2018 Jul;40(7):e1700221.,"The lack of specificity in neuroimaging studies of neurological and psychiatric diseases suggests that these different diseases have more in common than is generally considered. Potentially, features that are secondary effects of different pathological processes may share common neurobiological underpinnings. Intriguingly, many of these mechanisms are also observed in studies of normal (i.e., non-pathological) brain ageing. Different brain diseases may be causing premature or accelerated ageing to the brain, an idea that is supported by a line of ""brain ageing"" research that combines neuroimaging data with machine learning analysis. In reviewing this field, I conclude that such observations could have important implications, suggesting that we should shift experimental paradigm: away from characterizing the average case-control brain differences resulting from a disease toward methods that place individuals in their age-appropriate context. This will also lead naturally to clinical applications, whereby neuroimaging can contribute to a personalized-medicine approach to improve brain health."
29880463,,Artificial Intelligence Approaches in Hematopoietic Cell Transplantation: A Review of the Current Status and Future Directions,2018 Aug 3;35(3):152-157.,"The evidence-based literature on healthcare is currently expanding exponentially. The opportunities provided by the advancement in artificial intelligence (AI) tools such as machine learning are appealing in tackling many of the current healthcare challenges. Thus, AI integration is expanding in most fields of healthcare, including the field of hematology. This study aims to review the current applications of AI in the field of hematopoietic cell transplantation (HCT). A literature search was done involving the following databases: Ovid MEDLINE, including In-Process and other non-indexed citations, and Google Scholar. The abstracts of the following professional societies were also screened: American Society of Hematology, American Society for Blood and Marrow Transplantation, and European Society for Blood and Marrow Transplantation. The literature review showed that the integration of AI in the field of HCT has grown remarkably in the last decade and offers promising avenues in diagnosis and prognosis in HCT populations targeting both pre- and post-transplant challenges. Studies of AI integration in HCT have many limitations that include poorly tested algorithms, lack of generalizability, and limited use of different AI tools. Machine learning techniques in HCT are an intense area of research that needs much development and extensive support from hematology and HCT societies and organizations globally as we believe that this will be the future practice paradigm."
29880128,87.0,Artificial Intelligence in Cardiology,2018 Jun 12;71(23):2668-2679.,"Artificial intelligence and machine learning are poised to influence nearly every aspect of the human condition, and cardiology is not an exception to this trend. This paper provides a guide for clinicians on relevant aspects of artificial intelligence and machine learning, reviews selected applications of these methods in cardiology to date, and identifies how cardiovascular medicine could incorporate artificial intelligence in the future. In particular, the paper first reviews predictive modeling concepts relevant to cardiology such as feature selection and frequent pitfalls such as improper dichotomization. Second, it discusses common algorithms used in supervised learning and reviews selected applications in cardiology and related disciplines. Third, it describes the advent of deep learning and related methods collectively called unsupervised learning, provides contextual examples both in general medicine and in cardiovascular medicine, and then explains how these methods could be applied to enable precision cardiology and improve patient outcomes."
29879881,10.0,Machine Learning-based Virtual Screening and Its Applications to Alzheimer's Drug Discovery: A Review,2018;24(28):3347-3358.,"Background:                    Virtual Screening (VS) has emerged as an important tool in the drug development process, as it conducts efficient in silico searches over millions of compounds, ultimately increasing yields of potential drug leads. As a subset of Artificial Intelligence (AI), Machine Learning (ML) is a powerful way of conducting VS for drug leads. ML for VS generally involves assembling a filtered training set of compounds, comprised of known actives and inactives. After training the model, it is validated and, if sufficiently accurate, used on previously unseen databases to screen for novel compounds with desired drug target binding activity.              Objective:                    The study aims to review ML-based methods used for VS and applications to Alzheimer's Disease (AD) drug discovery.              Methods:                    To update the current knowledge on ML for VS, we review thorough backgrounds, explanations, and VS applications of the following ML techniques: Naïve Bayes (NB), k-Nearest Neighbors (kNN), Support Vector Machines (SVM), Random Forests (RF), and Artificial Neural Networks (ANN).              Results:                    All techniques have found success in VS, but the future of VS is likely to lean more largely toward the use of neural networks - and more specifically, Convolutional Neural Networks (CNN), which are a subset of ANN that utilize convolution. We additionally conceptualize a work flow for conducting ML-based VS for potential therapeutics for AD, a complex neurodegenerative disease with no known cure and prevention. This both serves as an example of how to apply the concepts introduced earlier in the review and as a potential workflow for future implementation.              Conclusion:                    Different ML techniques are powerful tools for VS, and they have advantages and disadvantages albeit. ML-based VS can be applied to AD drug development."
29874824,19.0,Changing Trends in Computational Drug Repositioning,2018 Jun 5;11(2):57.,"Efforts to maximize the indications potential and revenue from drugs that are already marketed are largely motivated by what Sir James Black, a Nobel Prize-winning pharmacologist advocated-""The most fruitful basis for the discovery of a new drug is to start with an old drug"". However, rational design of drug mixtures poses formidable challenges because of the lack of or limited information about in vivo cell regulation, mechanisms of genetic pathway activation, and in vivo pathway interactions. Hence, most of the successfully repositioned drugs are the result of ""serendipity"", discovered during late phase clinical studies of unexpected but beneficial findings. The connections between drug candidates and their potential adverse drug reactions or new applications are often difficult to foresee because the underlying mechanism associating them is largely unknown, complex, or dispersed and buried in silos of information. Discovery of such multi-domain pharmacomodules-pharmacologically relevant sub-networks of biomolecules and/or pathways-from collection of databases by independent/simultaneous mining of multiple datasets is an active area of research. Here, while presenting some of the promising bioinformatics approaches and pipelines, we summarize and discuss the current and evolving landscape of computational drug repositioning."
29873832,10.0,Multimodal seizure detection: A review,2018 Jun;59 Suppl 1:42-47.,"A review is given on the combined use of multiple modalities in non electroencephalography (EEG)-based detection of motor seizures in children and adults. A literature search of papers was done on multimodal seizure detection with extraction of data on type of modalities, study design and algorithm, sensitivity, false detection rate, and seizure types. Evidence of superiority was sought for using multiple instead of single modalities. Seven papers were found from 2010 to 2017, mostly using contact sensors such as accelerometers (n = 5), electromyography (n = 2), heart rate (n = 2), electrodermal activity (n = 1), and oximetry (n = 1). Remote sensors included video, radar, movement, and sound. All studies but one were in-hospital, with video-EEG as a gold standard. Algorithms were based on physiology and supervised machine learning, but did not always include a separate test dataset. Sensitivity ranged from 4% to 100% and false detection rate from 0.25 to 20 per 8 hours. Tonic-clonic seizure detection performed best. False detections tended to be restricted to a minority (16%-30%) of patients. Use of multiple sensors increased sensitivity; false detections decreased in one study, but increased in another. These preliminary studies suggest that detection of tonic-clonic seizures might be feasible, but larger field studies are required under more rigorous design that precludes bias. Generic algorithms probably suffice for the majority of patients."
29872707,22.0,Network-based machine learning and graph theory algorithms for precision oncology,2017 Aug 8;1(1):25.,"Network-based analytics plays an increasingly important role in precision oncology. Growing evidence in recent studies suggests that cancer can be better understood through mutated or dysregulated pathways or networks rather than individual mutations and that the efficacy of repositioned drugs can be inferred from disease modules in molecular networks. This article reviews network-based machine learning and graph theory algorithms for integrative analysis of personal genomic data and biomedical knowledge bases to identify tumor-specific molecular mechanisms, candidate targets and repositioned drugs for personalized treatment. The review focuses on the algorithmic design and mathematical formulation of these methods to facilitate applications and implementations of network-based analysis in the practice of precision oncology. We review the methods applied in three scenarios to integrate genomic data and network models in different analysis pipelines, and we examine three categories of network-based approaches for repositioning drugs in drug-disease-gene networks. In addition, we perform a comprehensive subnetwork/pathway analysis of mutations in 31 cancer genome projects in the Cancer Genome Atlas and present a detailed case study on ovarian cancer. Finally, we discuss interesting observations, potential pitfalls and future directions in network-based precision oncology."
29871778,6.0,Internet of Health Things: Toward intelligent vital signs monitoring in hospital wards,2018 Jul;89:61-69.,"Background:                    Large amounts of patient data are routinely manually collected in hospitals by using standalone medical devices, including vital signs. Such data is sometimes stored in spreadsheets, not forming part of patients' electronic health records, and is therefore difficult for caregivers to combine and analyze. One possible solution to overcome these limitations is the interconnection of medical devices via the Internet using a distributed platform, namely the Internet of Things. This approach allows data from different sources to be combined in order to better diagnose patient health status and identify possible anticipatory actions.              Methods:                    This work introduces the concept of the Internet of Health Things (IoHT), focusing on surveying the different approaches that could be applied to gather and combine data on vital signs in hospitals. Common heuristic approaches are considered, such as weighted early warning scoring systems, and the possibility of employing intelligent algorithms is analyzed.              Results:                    As a result, this article proposes possible directions for combining patient data in hospital wards to improve efficiency, allow the optimization of resources, and minimize patient health deterioration.              Conclusion:                    It is concluded that a patient-centered approach is critical, and that the IoHT paradigm will continue to provide more optimal solutions for patient management in hospital wards."
29869300,17.0,A Review of the Evolution of Vision-Based Motion Analysis and the Integration of Advanced Computer Vision Methods Towards Developing a Markerless System,2018 Jun 5;4(1):24.,"Background:                    The study of human movement within sports biomechanics and rehabilitation settings has made considerable progress over recent decades. However, developing a motion analysis system that collects accurate kinematic data in a timely, unobtrusive and externally valid manner remains an open challenge.              Main body:                    This narrative review considers the evolution of methods for extracting kinematic information from images, observing how technology has progressed from laborious manual approaches to optoelectronic marker-based systems. The motion analysis systems which are currently most widely used in sports biomechanics and rehabilitation do not allow kinematic data to be collected automatically without the attachment of markers, controlled conditions and/or extensive processing times. These limitations can obstruct the routine use of motion capture in normal training or rehabilitation environments, and there is a clear desire for the development of automatic markerless systems. Such technology is emerging, often driven by the needs of the entertainment industry, and utilising many of the latest trends in computer vision and machine learning. However, the accuracy and practicality of these systems has yet to be fully scrutinised, meaning such markerless systems are not currently in widespread use within biomechanics.              Conclusions:                    This review aims to introduce the key state-of-the-art in markerless motion capture research from computer vision that is likely to have a future impact in biomechanics, while considering the challenges with accuracy and robustness that are yet to be addressed."
29867348,17.0,Critical Issues in BDNF Val66Met Genetic Studies of Neuropsychiatric Disorders,2018 May 15;11:156.,"Neurotrophins have been implicated in the pathophysiology of many neuropsychiatric diseases. Brain-derived neurotrophic factor (BDNF) is the most abundant and widely distributed neurotrophin in the brain. Its Val66Met polymorphism (refSNP Cluster Report: rs6265) is a common and functional single-nucleotide polymorphism (SNP) affecting the activity-dependent release of BDNF. BDNF Val66Met transgenic mice have been generated, which may provide further insight into the functional impact of this polymorphism in the brain. Considering the important role of BDNF in brain function, more than 1,100 genetic studies have investigated this polymorphism in the past 15 years. Although these studies have reported some encouraging positive findings initially, most of the findings cannot be replicated in following studies. These inconsistencies in BDNF Val66Met genetic studies may be attributed to many factors such as age, sex, environmental factors, ethnicity, genetic model used for analysis, and gene-gene interaction, which are discussed in this review. We also discuss the results of recent studies that have reported the novel functions of this polymorphism. Because many BDNF polymorphisms and non-genetic factors have been implicated in the complex traits of neuropsychiatric diseases, the conventional genetic association-based method is limited to address these complex interactions. Future studies should apply data mining and machine learning techniques to determine the genetic role of BDNF in neuropsychiatric diseases."
29861451,2.0,Bio-Signal Complexity Analysis in Epileptic Seizure Monitoring: A Topic Review,2018 May 26;18(6):1720.,"Complexity science has provided new perspectives and opportunities for understanding a variety of complex natural or social phenomena, including brain dysfunctions like epilepsy. By delving into the complexity in electrophysiological signals and neuroimaging, new insights have emerged. These discoveries have revealed that complexity is a fundamental aspect of physiological processes. The inherent nonlinearity and non-stationarity of physiological processes limits the methods based on simpler underlying assumptions to point out the pathway to a more comprehensive understanding of their behavior and relation with certain diseases. The perspective of complexity may benefit both the research and clinical practice through providing novel data analytics tools devoted for the understanding of and the intervention about epilepsies. This review aims to provide a sketchy overview of the methods derived from different disciplines lucubrating to the complexity of bio-signals in the field of epilepsy monitoring. Although the complexity of bio-signals is still not fully understood, bundles of new insights have been already obtained. Despite the promising results about epileptic seizure detection and prediction through offline analysis, we are still lacking robust, tried-and-true real-time applications. Multidisciplinary collaborations and more high-quality data accessible to the whole community are needed for reproducible research and the development of such applications."
29860027,11.0,Patient Similarity Networks for Precision Medicine,2018 Sep 14;430(18 Pt A):2924-2938.,"Clinical research and practice in the 21st century is poised to be transformed by analysis of computable electronic medical records and population-level genome-scale patient profiles. Genomic data capture genetic and environmental state, providing information on heterogeneity in disease and treatment outcome, but genomic-based clinical risk scores are limited. Achieving the goal of routine precision medicine that takes advantage of these rich genomics data will require computational methods that support heterogeneous data, have excellent predictive performance, and ideally, provide biologically interpretable results. Traditional machine-learning approaches excel at performance, but often have limited interpretability. Patient similarity networks are an emerging paradigm for precision medicine, in which patients are clustered or classified based on their similarities in various features, including genomic profiles. This strategy is analogous to standard medical diagnosis, has excellent performance, is interpretable, and can preserve patient privacy. We review new methods based on patient similarity networks, including Similarity Network Fusion for patient clustering and netDx for patient classification. While these methods are already useful, much work is required to improve their scalability for contemporary genetic cohorts, optimize parameters, and incorporate a wide range of genomics and clinical data. The coming 5 years will provide an opportunity to assess the utility of network-based algorithms for precision medicine."
29859766,1.0,"Computational morphogenesis - Embryogenesis, cancer research and digital pathology",2018 Jul;169-170:40-54.,"Overall survival benefits of cancer therapies have, in general, fallen short of what was expected from them. By examining the many parallels that exist between embryogenesis and carcinogenesis, we discuss how clinical and fundamental cancer research can benefit from computational morphogenesis (CM) insofar as carcinogenesis is causally related to altered mechanisms underlying embryogenesis and post-embryonic tissue homeostasis. We also discuss about the critical role played by digital pathology (DP) since it constitutes the main source of data for the model-fitting and validation of CM-generated virtual tissues. Conversely, we outline how CM can provide support to DP by generating annotated synthetic 2D and 3D data that can be fed into machine-learning methods dedicated to the automated diagnosis of DP slides."
29859198,6.0,Non-invasive biomarkers of fetal brain development reflecting prenatal stress: An integrative multi-scale multi-species perspective on data collection and analysis,2020 Oct;117:165-183.,"Prenatal stress (PS) impacts early postnatal behavioural and cognitive development. This process of 'fetal programming' is mediated by the effects of the prenatal experience on the developing hypothalamic-pituitary-adrenal (HPA) axis and autonomic nervous system (ANS). We derive a multi-scale multi-species approach to devising preclinical and clinical studies to identify early non-invasively available pre- and postnatal biomarkers of PS. The multiple scales include brain epigenome, metabolome, microbiome and the ANS activity gauged via an array of advanced non-invasively obtainable properties of fetal heart rate fluctuations. The proposed framework has the potential to reveal mechanistic links between maternal stress during pregnancy and changes across these physiological scales. Such biomarkers may hence be useful as early and non-invasive predictors of neurodevelopmental trajectories influenced by the PS as well as follow-up indicators of success of therapeutic interventions to correct such altered neurodevelopmental trajectories. PS studies must be conducted on multiple scales derived from concerted observations in multiple animal models and human cohorts performed in an interactive and iterative manner and deploying machine learning for data synthesis, identification and validation of the best non-invasive detection and follow-up biomarkers, a prerequisite for designing effective therapeutic interventions."
29858745,3.0,"Machine Learning to Predict, Detect, and Intervene Older Adults Vulnerable for Adverse Drug Events in the Emergency Department",2018 Sep;14(3):248-252.,"Adverse drug events (ADEs) are common and have serious consequences in older adults. ED visits are opportunities to identify and alter the course of such vulnerable patients. Current practice, however, is limited by inaccurate reporting of medication list, time-consuming medication reconciliation, and poor ADE assessment. This manuscript describes a novel approach to predict, detect, and intervene vulnerable older adults at risk of ADE using machine learning. Toxicologists' expertise in ADE is essential to creating the machine learning algorithm. Leveraging the existing electronic health records to better capture older adults at risk of ADE in the ED may improve their care."
29852943,8.0,PET/MRI Hybrid Systems,2018 Jul;48(4):332-347.,"Over the last decade, the combination of PET and MRI in one system has proven to be highly successful in basic preclinical research, as well as in clinical research. Nowadays, PET/MRI systems are well established in preclinical imaging and are progressing into clinical applications to provide further insights into specific diseases, therapeutic assessments, and biological pathways. Certain challenges in terms of hardware had to be resolved concurrently with the development of new techniques to be able to reach the full potential of both combined techniques. This review provides an overview of these challenges and describes the opportunities that simultaneous PET/MRI systems can exploit in comparison with stand-alone or other combined hybrid systems. New approaches were developed for simultaneous PET/MRI systems to correct for attenuation of 511 keV photons because MRI does not provide direct information on gamma photon attenuation properties. Furthermore, new algorithms to correct for motion were developed, because MRI can accurately detect motion with high temporal resolution. The additional information gained by the MRI can be employed to correct for partial volume effects as well. The development of new detector designs in combination with fast-decaying scintillator crystal materials enabled time-of-flight detection and incorporation in the reconstruction algorithms. Furthermore, this review lists the currently commercially available systems both for preclinical and clinical imaging and provides an overview of applications in both fields. In this regard, special emphasis has been placed on data analysis and the potential for both modalities to evolve with advanced image analysis tools, such as cluster analysis and machine learning."
29849612,1.0,Aquatic Toxic Analysis by Monitoring Fish Behavior Using Computer Vision: A Recent Progress,2018 Apr 3;2018:2591924.,"Video tracking based biological early warning system achieved a great progress with advanced computer vision and machine learning methods. Ability of video tracking of multiple biological organisms has been largely improved in recent years. Video based behavioral monitoring has become a common tool for acquiring quantified behavioral data for aquatic risk assessment. Investigation of behavioral responses under chemical and environmental stress has been boosted by rapidly developed machine learning and artificial intelligence. In this paper, we introduce the fundamental of video tracking and present the pioneer works in precise tracking of a group of individuals in 2D and 3D space. Technical and practical issues suffered in video tracking are explained. Subsequently, the toxic analysis based on fish behavioral data is summarized. Frequently used computational methods and machine learning are explained with their applications in aquatic toxicity detection and abnormal pattern analysis. Finally, advantages of recent developed deep learning approach in toxic prediction are presented."
29848472,33.0,Artificial Intelligence for Diabetes Management and Decision Support: Literature Review,2018 May 30;20(5):e10775.,"Background:                    Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis.              Objective:                    The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges.              Methods:                    A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review.              Results:                    We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results.              Conclusions:                    We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients' quality of life."
29847214,3.0,The Human Vaccines Project: Towards a comprehensive understanding of the human immune response to immunization,2018;14(9):2214-2216.,"Although the success of vaccination to date has been unprecedented, our inadequate understanding of the details of the human immune response to immunization has resulted in several recent vaccine failures and significant delays in the development of high-need vaccines for global infectious diseases and cancer. Because of the need to better understand the immense complexity of the human immune system, the Human Vaccines Project was launched in 2015 with the mission to decode the human immune response to accelerate development of vaccines and immunotherapies for major diseases. The Project currently has three programs: 1) The Human Immunome Program, with the goal of deciphering the complete repertoire of B and T cell receptors across the human population, termed the Human Immunome, 2) The Rules of Immunogenicity Program, with the goal of understanding the key principles of how a vaccine elicits a protective and durable response using a system immunology approach, and 3) The Universal Influenza Vaccine Initiative (UIVI), with the goal of conducting experimental clinical trials to understand the influence of influenza pre-exposures on subsequent influenza immunization and the mechanisms of protection. Given the dramatic advances in computational and systems biology, genomics, immune monitoring, bioinformatics and machine learning, there is now an unprecedented opportunity to unravel the intricacies of the human immune response to immunization, ushering in a new era in vaccine development."
29805337,10.0,Connecting Technological Innovation in Artificial Intelligence to Real-world Medical Practice through Rigorous Clinical Validation: What Peer-reviewed Medical Journals Could Do,2018 Apr 27;33(22):e152.,"Artificial intelligence (AI) is projected to substantially influence clinical practice in the foreseeable future. However, despite the excitement around the technologies, it is yet rare to see examples of robust clinical validation of the technologies and, as a result, very few are currently in clinical use. A thorough, systematic validation of AI technologies using adequately designed clinical research studies before their integration into clinical practice is critical to ensure patient benefit and safety while avoiding any inadvertent harms. We would like to suggest several specific points regarding the role that peer-reviewed medical journals can play, in terms of study design, registration, and reporting, to help achieve proper and meaningful clinical validation of AI technologies designed to make medical diagnosis and prediction, focusing on the evaluation of diagnostic accuracy efficacy. Peer-reviewed medical journals can encourage investigators who wish to validate the performance of AI systems for medical diagnosis and prediction to pay closer attention to the factors listed in this article by emphasizing their importance. Thereby, peer-reviewed medical journals can ultimately facilitate translating the technological innovations into real-world practice while securing patient safety and benefit."
29804538,3.0,The Application of Machine Learning Techniques in Clinical Drug Therapy,2019;15(2):111-119.,"Introduction:                    The development of a novel drug is an extremely complicated process that includes the target identification, design and manufacture, and proper therapy of the novel drug, as well as drug dose selection, drug efficacy evaluation, and adverse drug reaction control. Due to the limited resources, high costs, long duration, and low hit-to-lead ratio in the development of pharmacogenetics and computer technology, machine learning techniques have assisted novel drug development and have gradually received more attention by researchers.              Methods:                    According to current research, machine learning techniques are widely applied in the process of the discovery of new drugs and novel drug targets, the decision surrounding proper therapy and drug dose, and the prediction of drug efficacy and adverse drug reactions.              Results and conclusion:                    In this article, we discussed the history, workflow, and advantages and disadvantages of machine learning techniques in the processes mentioned above. Although the advantages of machine learning techniques are fairly obvious, the application of machine learning techniques is currently limited. With further research, the application of machine techniques in drug development could be much more widespread and could potentially be one of the major methods used in drug development."
29800865,19.0,"Cryptic binding sites on proteins: definition, detection, and druggability",2018 Jun;44:1-8.,"Many proteins in their unbound structures lack surface pockets appropriately sized for drug binding. Hence, a variety of experimental and computational tools have been developed for the identification of cryptic sites that are not evident in the unbound protein but form upon ligand binding, and can provide tractable drug target sites. The goal of this review is to discuss the definition, detection, and druggability of such sites, and their potential value for drug discovery. Novel methods based on molecular dynamics simulations are particularly promising and yield a large number of transient pockets, but it has been shown that only a minority of such sites are generally capable of binding ligands with substantial affinity. Based on recent studies, current methodology can be improved by combining molecular dynamics with fragment docking and machine learning approaches."
29800232,4.0,Predicting novel microRNA: a comprehensive comparison of machine learning approaches,2019 Sep 27;20(5):1607-1620.,"Motivation:                    The importance of microRNAs (miRNAs) is widely recognized in the community nowadays because these short segments of RNA can play several roles in almost all biological processes. The computational prediction of novel miRNAs involves training a classifier for identifying sequences having the highest chance of being precursors of miRNAs (pre-miRNAs). The big issue with this task is that well-known pre-miRNAs are usually few in comparison with the hundreds of thousands of candidate sequences in a genome, which results in high class imbalance. This imbalance has a strong influence on most standard classifiers, and if not properly addressed in the model and the experiments, not only performance reported can be completely unrealistic but also the classifier will not be able to work properly for pre-miRNA prediction. Besides, another important issue is that for most of the machine learning (ML) approaches already used (supervised methods), it is necessary to have both positive and negative examples. The selection of positive examples is straightforward (well-known pre-miRNAs). However, it is difficult to build a representative set of negative examples because they should be sequences with hairpin structure that do not contain a pre-miRNA.              Results:                    This review provides a comprehensive study and comparative assessment of methods from these two ML approaches for dealing with the prediction of novel pre-miRNAs: supervised and unsupervised training. We present and analyze the ML proposals that have appeared during the past 10 years in literature. They have been compared in several prediction tasks involving two model genomes and increasing imbalance levels. This work provides a review of existing ML approaches for pre-miRNA prediction and fair comparisons of the classifiers with same features and data sets, instead of just a revision of published software tools. The results and the discussion can help the community to select the most adequate bioinformatics approach according to the prediction task at hand. The comparative results obtained suggest that from low to mid-imbalance levels between classes, supervised methods can be the best. However, at very high imbalance levels, closer to real case scenarios, models including unsupervised and deep learning can provide better performance."
29792115,6.0,Using Machine Learning to Advance Personality Assessment and Theory,2019 May;23(2):190-203.,"Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality."
29792109,6.0,Computational functional genomics-based approaches in analgesic drug discovery and repurposing,2018 Jun 1;19(9):783-797.,"Persistent pain is a major healthcare problem affecting a fifth of adults worldwide with still limited treatment options. The search for new analgesics increasingly includes the novel research area of functional genomics, which combines data derived from various processes related to DNA sequence, gene expression or protein function and uses advanced methods of data mining and knowledge discovery with the goal of understanding the relationship between the genome and the phenotype. Its use in drug discovery and repurposing for analgesic indications has so far been performed using knowledge discovery in gene function and drug target-related databases; next-generation sequencing; and functional proteomics-based approaches. Here, we discuss recent efforts in functional genomics-based approaches to analgesic drug discovery and repurposing and highlight the potential of computational functional genomics in this field including a demonstration of the workflow using a novel R library 'dbtORA'."
29791959,18.0,"Sarcopenia: Beyond Muscle Atrophy and into the New Frontiers of Opportunistic Imaging, Precision Medicine, and Machine Learning",2018 Jul;22(3):307-322.,"As populations continue to age worldwide, the impact of sarcopenia on public health will continue to grow. The clinically relevant and increasingly common diagnosis of sarcopenia is at the confluence of three tectonic shifts in medicine: opportunistic imaging, precision medicine, and machine learning. This review focuses on the state-of-the-art imaging of sarcopenia and provides context for such imaging by discussing the epidemiology, pathophysiology, consequences, and future directions in the field of sarcopenia."
29790107,7.0,The Role of Pharmacogenomics in Bipolar Disorder: Moving Towards Precision Medicine,2018 Aug;22(4):409-420.,"Bipolar disorder (BD) is a common and disabling psychiatric condition with a severe socioeconomic impact. BD is treated with mood stabilizers, among which lithium represents the first-line treatment. Lithium alone or in combination is effective in 60% of chronically treated patients, but response remains heterogenous and a large number of patients require a change in therapy after several weeks or months. Many studies have so far tried to identify molecular and genetic markers that could help us to predict response to mood stabilizers or the risk for adverse drug reactions. Pharmacogenetic studies in BD have been for the most part focused on lithium, but the complexity and variability of the response phenotype, together with the unclear mechanism of action of lithium, limited the power of these studies to identify robust biomarkers. Recent pharmacogenomic studies on lithium response have provided promising findings, suggesting that the integration of genome-wide investigations with deep phenotyping, in silico analyses and machine learning could lead us closer to personalized treatments for BD. Nevertheless, to date none of the genes suggested by pharmacogenetic studies on mood stabilizers have been included in any of the genetic tests approved by the Food and Drug Administration (FDA) for drug efficacy. On the other hand, genetic information has been included in drug labels to test for the safety of carbamazepine and valproate. In this review, we will outline available studies investigating the pharmacogenetics and pharmacogenomics of lithium and other mood stabilizers, with a specific focus on the limitations of these studies and potential strategies to overcome them. We will also discuss FDA-approved pharmacogenetic tests for treatments commonly used in the management of BD."
29789997,2.0,Non-Gaussian Methods for Causal Structure Learning,2019 Apr;20(3):431-441.,"Causal structure learning is one of the most exciting new topics in the fields of machine learning and statistics. In many empirical sciences including prevention science, the causal mechanisms underlying various phenomena need to be studied. Nevertheless, in many cases, classical methods for causal structure learning are not capable of estimating the causal structure of variables. This is because it explicitly or implicitly assumes Gaussianity of data and typically utilizes only the covariance structure. In many applications, however, non-Gaussian data are often obtained, which means that more information may be contained in the data distribution than the covariance matrix is capable of containing. Thus, many new methods have recently been proposed for using the non-Gaussian structure of data and inferring the causal structure of variables. This paper introduces prevention scientists to such causal structure learning methods, particularly those based on the linear, non-Gaussian, acyclic model known as LiNGAM. These non-Gaussian data analysis tools can fully estimate the underlying causal structures of variables under assumptions even in the presence of unobserved common causes. This feature is in contrast to other approaches. A simulated example is also provided."
29789268,23.0,Making Individual Prognoses in Psychiatry Using Neuroimaging and Machine Learning,2018 Sep;3(9):798-808.,"Psychiatric prognosis is a difficult problem. Making a prognosis requires looking far into the future, as opposed to making a diagnosis, which is concerned with the current state. During the follow-up period, many factors will influence the course of the disease. Combined with the usually scarcer longitudinal data and the variability in the definition of outcomes/transition, this makes prognostic predictions a challenging endeavor. Employing neuroimaging data in this endeavor introduces the additional hurdle of high dimensionality. Machine learning techniques are especially suited to tackle this challenging problem. This review starts with a brief introduction to machine learning in the context of its application to clinical neuroimaging data. We highlight a few issues that are especially relevant for prediction of outcome and transition using neuroimaging. We then review the literature that discusses the application of machine learning for this purpose. Critical examination of the studies and their results with respect to the relevant issues revealed the following: 1) there is growing evidence for the prognostic capability of machine learning-based models using neuroimaging; and 2) reported accuracies may be too optimistic owing to small sample sizes and the lack of independent test samples. Finally, we discuss options to improve the reliability of (prognostic) prediction models. These include new methodologies and multimodal modeling. Paramount, however, is our conclusion that future work will need to provide properly (cross-)validated accuracy estimates of models trained on sufficiently large datasets. Nevertheless, with the technological advances enabling acquisition of large databases of patients and healthy subjects, machine learning represents a powerful tool in the search for psychiatric biomarkers."
29787940,33.0,Survey on deep learning for radiotherapy,2018 Jul 1;98:126-146.,"More than 50% of cancer patients are treated with radiotherapy, either exclusively or in combination with other methods. The planning and delivery of radiotherapy treatment is a complex process, but can now be greatly facilitated by artificial intelligence technology. Deep learning is the fastest-growing field in artificial intelligence and has been successfully used in recent years in many domains, including medicine. In this article, we first explain the concept of deep learning, addressing it in the broader context of machine learning. The most common network architectures are presented, with a more specific focus on convolutional neural networks. We then present a review of the published works on deep learning methods that can be applied to radiotherapy, which are classified into seven categories related to the patient workflow, and can provide some insights of potential future applications. We have attempted to make this paper accessible to both radiotherapy and deep learning communities, and hope that it will inspire new collaborations between these two communities to develop dedicated radiotherapy applications."
29786659,11.0,Deep Learning to Predict Falls in Older Adults Based on Daily-Life Trunk Accelerometry,2018 May 22;18(5):1654.,"Early detection of high fall risk is an essential component of fall prevention in older adults. Wearable sensors can provide valuable insight into daily-life activities; biomechanical features extracted from such inertial data have been shown to be of added value for the assessment of fall risk. Body-worn sensors such as accelerometers can provide valuable insight into fall risk. Currently, biomechanical features derived from accelerometer data are used for the assessment of fall risk. Here, we studied whether deep learning methods from machine learning are suited to automatically derive features from raw accelerometer data that assess fall risk. We used an existing dataset of 296 older adults. We compared the performance of three deep learning model architectures (convolutional neural network (CNN), long short-term memory (LSTM) and a combination of these two (ConvLSTM)) to each other and to a baseline model with biomechanical features on the same dataset. The results show that the deep learning models in a single-task learning mode are strong in recognition of identity of the subject, but that these models only slightly outperform the baseline method on fall risk assessment. When using multi-task learning, with gender and age as auxiliary tasks, deep learning models perform better. We also found that preprocessing of the data resulted in the best performance (AUC = 0.75). We conclude that deep learning models, and in particular multi-task learning, effectively assess fall risk on the basis of wearable sensor data."
29782369,7.0,Neuroimaging in epilepsy,2018 Aug;31(4):371-378.,"Purpose of review:                    Epilepsy neuroimaging is important for detecting the seizure onset zone, predicting and preventing deficits from surgery and illuminating mechanisms of epileptogenesis. An aspiration is to integrate imaging and genetic biomarkers to enable personalized epilepsy treatments.              Recent findings:                    The ability to detect lesions, particularly focal cortical dysplasia and hippocampal sclerosis, is increased using ultra high-field imaging and postprocessing techniques such as automated volumetry, T2 relaxometry, voxel-based morphometry and surface-based techniques. Statistical analysis of PET and single photon emission computer tomography (STATISCOM) are superior to qualitative analysis alone in identifying focal abnormalities in MRI-negative patients. These methods have also been used to study mechanisms of epileptogenesis and pharmacoresistance.Recent language fMRI studies aim to localize, and also lateralize language functions. Memory fMRI has been recommended to lateralize mnemonic function and predict outcome after surgery in temporal lobe epilepsy.              Summary:                    Combinations of structural, functional and post-processing methods have been used in multimodal and machine learning models to improve the identification of the seizure onset zone and increase understanding of mechanisms underlying structural and functional aberrations in epilepsy."
29781047,12.0,A Survey on Coronary Atherosclerotic Plaque Tissue Characterization in Intravascular Optical Coherence Tomography,2018 May 21;20(7):33.,"Purpose of review:                    Atherosclerotic plaque deposition within the coronary vessel wall leads to arterial stenosis and severe catastrophic events over time. Identification of these atherosclerotic plaque components is essential to pre-estimate the risk of cardiovascular disease (CVD) and stratify them as a high or low risk. The characterization and quantification of coronary plaque components are not only vital but also a challenging task which can be possible using high-resolution imaging techniques.              Recent finding:                    Atherosclerotic plaque components such as thin cap fibroatheroma (TCFA), fibrous cap, macrophage infiltration, large necrotic core, and thrombus are the microstructural plaque components that can be detected with only high-resolution imaging modalities such as intravascular ultrasound (IVUS) and optical coherence tomography (OCT). Light-based OCT provides better visualization of plaque tissue layers of coronary vessel walls as compared to IVUS. Three dominant paradigms have been identified to characterize atherosclerotic plaque components based on optical attenuation coefficients, machine learning algorithms, and deep learning techniques. This review (condensation of 126 papers after downloading 150 articles) presents a detailed comparison among various methodologies utilized for plaque tissue characterization, classification, and arterial measurements in OCT. Furthermore, this review presents the different ways to predict and stratify the risk associated with the CVD based on plaque characterization and measurements in OCT. Moreover, this review discovers three different paradigms for plaque characterization and their pros and cons. Among all of the techniques, a combination of machine learning and deep learning techniques is a best possible solution that provides improved OCT-based risk stratification."
29774657,6.0,"Cheminformatics in Drug Discovery, an Industrial Perspective",2018 Sep;37(9-10):e1800041.,"Cheminformatics has established itself as a core discipline within large scale drug discovery operations. It would be impossible to handle the amount of data generated today in a small molecule drug discovery project without persons skilled in cheminformatics. In addition, due to increased emphasis on ""Big Data"", machine learning and artificial intelligence, not only in the society in general, but also in drug discovery, it is expected that the cheminformatics field will be even more important in the future. Traditional areas like virtual screening, library design and high-throughput screening analysis are highlighted in this review. Applying machine learning in drug discovery is an area that has become very important. Applications of machine learning in early drug discovery has been extended from predicting ADME properties and target activity to tasks like de novo molecular design and prediction of chemical reactions."
29771496,6.0,What Can Pleiotropic Proteins in Innate Immunity Teach Us about Bioconjugation and Molecular Design?,2018 Jul 18;29(7):2127-2139.,"A common bioengineering strategy to add function to a given molecule is by conjugation of a new moiety onto that molecule. Adding multiple functions in this way becomes increasingly challenging and leads to composite molecules with larger molecular weights. In this review, we attempt to gain a new perspective by looking at this problem in reverse, by examining nature's strategies of multiplexing different functions into the same pleiotropic molecule using emerging analysis techniques such as machine learning. We concentrate on examples from the innate immune system, which employs a finite repertoire of molecules for a broad range of tasks. An improved understanding of how diverse functions are multiplexed into a single molecule can inspire new approaches for the deterministic design of multifunctional molecules."
29770293,3.0,Current monitoring and innovative predictive modeling to improve care in the pediatric cardiac intensive care unit,2018 Apr;7(2):120-128.,"The objectives of this review are (I) to describe the challenges associated with monitoring patients in the pediatric cardiac intensive care unit (PCICU) and (II) to discuss the use of innovative statistical and artificial intelligence (AI) software programs to attempt to predict significant clinical events. Patients cared for in the PCICU are clinically fragile and at risk for fatal decompensation. Current monitoring modalities are often ineffective, sometimes inaccurate, and fail to detect a deteriorating clinical status in a timely manner. Predictive models created by AI and machine learning may lead to earlier detection of patients at risk for clinical decompensation and thereby improve care for critically ill pediatric cardiac patients."
29769297,23.0,"Mechanistic models versus machine learning, a fight worth fighting for the biological community?",2018 May;14(5):20170660.,"Ninety per cent of the world's data have been generated in the last 5 years (Machine learning: the power and promise of computers that learn by example Report no. DES4702. Issued April 2017. Royal Society). A small fraction of these data is collected with the aim of validating specific hypotheses. These studies are led by the development of mechanistic models focused on the causality of input-output relationships. However, the vast majority is aimed at supporting statistical or correlation studies that bypass the need for causality and focus exclusively on prediction. Along these lines, there has been a vast increase in the use of machine learning models, in particular in the biomedical and clinical sciences, to try and keep pace with the rate of data generation. Recent successes now beg the question of whether mechanistic models are still relevant in this area. Said otherwise, why should we try to understand the mechanisms of disease progression when we can use machine learning tools to directly predict disease outcome?"
29766512,,Structured radiology reporting on an institutional level-benefit or new administrative burden?,2018 Dec;1434(1):274-281.,"Significant technical advances have been made in radiology since the first discovery of X-rays. Diagnostic techniques have become more and more complex, workflows have been digitized, and data production has increased exponentially. However, the radiology report as the main method for communicating examination results has largely remained unchanged. Growing evidence supports that more structured radiology reports offer various benefits over conventional narrative reports. Various efforts have been made to further develop and promote structured reporting. However, regardless of the potential benefits, structured reporting has still not seen widespread implementation into the clinical routine. With recent technical advances, especially new research topics such as big data and machine learning, structured reporting could prove essential for the future of radiology. New interoperable solutions are needed to facilitate the implementation of template-based structured reporting into the clinical routine."
29754806,13.0,Machine learning in cardiac CT: Basic concepts and contemporary data,May-Jun 2018;12(3):192-201.,"Propelled by the synergy of the groundbreaking advancements in the ability to analyze high-dimensional datasets and the increasing availability of imaging and clinical data, machine learning (ML) is poised to transform the practice of cardiovascular medicine. Owing to the growing body of literature validating both the diagnostic performance as well as the prognostic implications of anatomic and physiologic findings, coronary computed tomography angiography (CCTA) is now a well-established non-invasive modality for the assessment of cardiovascular disease. ML has been increasingly utilized to optimize performance as well as extract data from CCTA as well as non-contrast enhanced cardiac CT scans. The purpose of this review is to describe the contemporary state of ML based algorithms applied to cardiac CT, as well as to provide clinicians with an understanding of its benefits and associated limitations."
29752973,31.0,Big Data and Data Science in Critical Care,2018 Nov;154(5):1239-1248.,"The digitalization of the health-care system has resulted in a deluge of clinical big data and has prompted the rapid growth of data science in medicine. Data science, which is the field of study dedicated to the principled extraction of knowledge from complex data, is particularly relevant in the critical care setting. The availability of large amounts of data in the ICU, the need for better evidence-based care, and the complexity of critical illness makes the use of data science techniques and data-driven research particularly appealing to intensivists. Despite the increasing number of studies and publications in the field, thus far there have been few examples of data science projects that have resulted in successful implementations of data-driven systems in the ICU. However, given the expected growth in the field, intensivists should be familiar with the opportunities and challenges of big data and data science. The present article reviews the definitions, types of algorithms, applications, challenges, and future of big data and data science in critical care."
29750902,77.0,Machine learning in chemoinformatics and drug discovery,2018 Aug;23(8):1538-1546.,"Chemoinformatics is an established discipline focusing on extracting, processing and extrapolating meaningful data from chemical structures. With the rapid explosion of chemical 'big' data from HTS and combinatorial synthesis, machine learning has become an indispensable tool for drug designers to mine chemical information from large compound databases to design drugs with important biological properties. To process the chemical data, we first reviewed multiple processing layers in the chemoinformatics pipeline followed by the introduction of commonly used machine learning models in drug discovery and QSAR analysis. Here, we present basic principles and recent case studies to demonstrate the utility of machine learning techniques in chemoinformatics analyses; and we discuss limitations and future directions to guide further development in this evolving field."
29750730,29.0,The changing landscape of motor neuron disease imaging: the transition from descriptive studies to precision clinical tools,2018 Aug;31(4):431-438.,"Purpose of review:                    Neuroimaging in motor neuron disease (MND) has traditionally been seen as an academic tool with limited direct relevance to individualized patient care. This has changed radically in recent years as computational imaging has emerged as a viable clinical tool with true biomarker potential. This transition is not only fuelled by technological advances but also by important conceptual developments.              Recent findings:                    The natural history of MND is now evaluated by presymptomatic, postmortem and multi-timepoint longitudinal imaging studies. The anatomical spectrum of MND imaging has also been expanded from an overwhelmingly cerebral focus to innovative spinal and muscle applications. In contrast to the group-comparisons of previous studies, machine-learning and deep-learning approaches are increasingly utilized to model real-life diagnostic dilemmas and aid prognostic classification. The focus from evaluating focal structural changes has shifted to the appraisal of network integrity by connectivity-based approaches. The armamentarium of MND imaging has also been complemented by novel PET-ligands, spinal toolboxes and the availability of magnetoencephalography and high-field magnetic resonance (MR) imaging platforms.              Summary:                    In addition to the technological and conceptual advances, collaborative multicentre research efforts have also gained considerable momentum. This opinion-piece reviews emerging trends in MND imaging and their implications to clinical care and drug development."
29749590,6.0,"Imaging, Health Record, and Artificial Intelligence: Hype or Hope?",2018 May 10;20(6):48.,"Purpose of review:                    The review is focused on ""digital health"", which means advanced analytics based on multi-modal data. The ""Health Care Internet of Things"", which uses sensors, apps, and remote monitoring could provide continuous clinical information in the cloud that enables clinicians to access the information they need to care for patients everywhere. Greater standardization of acquisition protocols will be needed to maximize the potential gains from automation and machine learning.              Recent findings:                    Recent artificial intelligence applications on cardiac imaging will not be diagnosing patients and replacing doctors but will be augmenting their ability to find key relevant data they need to care for a patient and present it in a concise, easily digestible format. Risk stratification will transition from oversimplified population-based risk scores to machine learning-based metrics incorporating a large number of patient-specific clinical and imaging variables in real-time beyond the limits of human cognition. This will deliver highly accurate and individual personalized risk assessments and facilitate tailored management plans."
29746776,,From Cancer to Pain Target by Automated Selectivity Inversion of a Clinical Candidate,2018 Jun 14;61(11):4851-4859.,"Elimination of inadvertent binding is crucial for inhibitor design targeting conserved protein classes like kinases. Compounds in clinical trials provide a rich source for initiating drug design efforts by exploiting such secondary binding events. Considering both aspects, we shifted the selectivity of tozasertib, originally developed against AurA as cancer target, toward the pain target TrkA. First, selectivity-determining features in binding pockets were identified by fusing interaction grids of several key and off-target conformations. A focused library was subsequently created and prioritized using a multiobjective selection scheme that filters for selective and highly active compounds based on orthogonal methods grounded in computational chemistry and machine learning. Eighteen high-ranking compounds were synthesized and experimentally tested. The top-ranked compound has 10000-fold improved selectivity versus AurA, nanomolar cellular activity, and is highly selective in a kinase panel. This was achieved in a single round of automated in silico optimization, highlighting the power of recent advances in computer-aided drug design to automate design and selection processes."
29745270,4.0,QSAR modelling: a therapeutic patent review 2010-present,2018 Jun;28(6):467-476.,"Introduction:                    Quantitative Structure-Activity Relationship (QSAR) models are becoming one of the most interesting fields for developing therapeutics and therapeutics related patents. At present, QSAR methodologies comprise a series of possibilities, including joining forces with machine learning methods and increasing even more the swiftness they might bring to the prospective development of therapeutics in the Health Sciences scope.              Areas covered:                    After evaluating the period from 2010 to early 2018, the areas covered by the reviewed QSAR based therapeutics patents comprise three main fields (drug development, risk assessment and novel QSAR methodologies), and several areas, from cancer and cancer related symptomatology to neurodegenerative diseases, such as Parkinson's disease, or even monitoring several chemical particles carrier-mediums or interface frontiers.              Expert opinion:                    Among the several conclusions drawn from this reviewing, some pertain to the near future of investigative research on QSAR based inventions for therapeutic purposes, while others include the prospective of an even more grown interest on cytotoxicity assessment with in silico models and protocols. Further, the type of compounds described in these types of patents is likely to see an increase in neurodegenerative diseases therapeutics, as the novel methodologies meet the challenging global health needs as human life expectancy increases."
29741258,7.0,Image processing and machine learning in the morphological analysis of blood cells,2018 May;40 Suppl 1:46-53.,"Introduction:                    This review focuses on how image processing and machine learning can be useful for the morphological characterization and automatic recognition of cell images captured from peripheral blood smears.              Methods:                    The basics of the 3 core elements (segmentation, quantitative features, and classification) are outlined, and recent literature is discussed. Although red blood cells are a significant part of this context, this study focuses on malignant lymphoid cells and blast cells.              Results:                    There is no doubt that these technologies may help the cytologist to perform efficient, objective, and fast morphological analysis of blood cells. They may also help in the interpretation of some morphological features and may serve as learning and survey tools.              Conclusion:                    Although research is still needed, it is important to define screening strategies to exploit the potential of image-based automatic recognition systems integrated in the daily routine of laboratories along with other analysis methodologies."
29729378,7.0,Leveraging knowledge engineering and machine learning for microbial bio-manufacturing,Jul-Aug 2018;36(4):1308-1315.,"Genome scale modeling (GSM) predicts the performance of microbial workhorses and helps identify beneficial gene targets. GSM integrated with intracellular flux dynamics, omics, and thermodynamics have shown remarkable progress in both elucidating complex cellular phenomena and computational strain design (CSD). Nonetheless, these models still show high uncertainty due to a poor understanding of innate pathway regulations, metabolic burdens, and other factors (such as stress tolerance and metabolite channeling). Besides, the engineered hosts may have genetic mutations or non-genetic variations in bioreactor conditions and thus CSD rarely foresees fermentation rate and titer. Metabolic models play important role in design-build-test-learn cycles for strain improvement, and machine learning (ML) may provide a viable complementary approach for driving strain design and deciphering cellular processes. In order to develop quality ML models, knowledge engineering leverages and standardizes the wealth of information in literature (e.g., genomic/phenomic data, synthetic biology strategies, and bioprocess variables). Data driven frameworks can offer new constraints for mechanistic models to describe cellular regulations, to design pathways, to search gene targets, and to estimate fermentation titer/rate/yield under specified growth conditions (e.g., mixing, nutrients, and O2). This review highlights the scope of information collections, database constructions, and machine learning techniques (such as deep learning and transfer learning), which may facilitate ""Learn and Design"" for strain development."
29729293,11.0,Principles of Temporal Processing Across the Cortical Hierarchy,2018 Oct 1;389:161-174.,"The world is richly structured on multiple spatiotemporal scales. In order to represent spatial structure, many machine-learning models repeat a set of basic operations at each layer of a hierarchical architecture. These iterated spatial operations - including pooling, normalization and pattern completion - enable these systems to recognize and predict spatial structure, while robust to changes in the spatial scale, contrast and noisiness of the input signal. Because our brains also process temporal information that is rich and occurs across multiple time scales, might the brain employ an analogous set of operations for temporal information processing? Here we define a candidate set of temporal operations, and we review evidence that they are implemented in the mammalian cerebral cortex in a hierarchical manner. We conclude that multiple consecutive stages of cortical processing can be understood to perform temporal pooling, temporal normalization and temporal pattern completion."
29728226,19.0,Radiomics in radiooncology - Challenging the medical physicist,2018 Apr;48:27-36.,"Purpose:                    Noticing the fast growing translation of artificial intelligence (AI) technologies to medical image analysis this paper emphasizes the future role of the medical physicist in this evolving field. Specific challenges are addressed when implementing big data concepts with high-throughput image data processing like radiomics and machine learning in a radiooncology environment to support clinical decisions.              Methods:                    Based on the experience of our interdisciplinary radiomics working group, techniques for processing minable data, extracting radiomics features and associating this information with clinical, physical and biological data for the development of prediction models are described. A special emphasis was placed on the potential clinical significance of such an approach.              Results:                    Clinical studies demonstrate the role of radiomics analysis as an additional independent source of information with the potential to influence the radiooncology practice, i.e. to predict patient prognosis, treatment response and underlying genetic changes. Extending the radiomics approach to integrate imaging, clinical, genetic and dosimetric data ('panomics') challenges the medical physicist as member of the radiooncology team.              Conclusions:                    The new field of big data processing in radiooncology offers opportunities to support clinical decisions, to improve predicting treatment outcome and to stimulate fundamental research on radiation response both of tumor and normal tissue. The integration of physical data (e.g. treatment planning, dosimetric, image guidance data) demands an involvement of the medical physicist in the radiomics approach of radiooncology. To cope with this challenge national and international organizations for medical physics should organize more training opportunities in artificial intelligence technologies in radiooncology."
29725961,8.0,Hello World Deep Learning in Medical Imaging,2018 Jun;31(3):283-289.,"There is recent popularity in applying machine learning to medical imaging, notably deep learning, which has achieved state-of-the-art performance in image analysis and processing. The rapid adoption of deep learning may be attributed to the availability of machine learning frameworks and libraries to simplify their use. In this tutorial, we provide a high-level overview of how to build a deep neural network for medical image classification, and provide code that can help those new to the field begin their informatics projects."
29725101,66.0,Core microbiomes for sustainable agroecosystems,2018 May;4(5):247-257.,"In an era of ecosystem degradation and climate change, maximizing microbial functions in agroecosystems has become a prerequisite for the future of global agriculture. However, managing species-rich communities of plant-associated microbiomes remains a major challenge. Here, we propose interdisciplinary research strategies to optimize microbiome functions in agroecosystems. Informatics now allows us to identify members and characteristics of 'core microbiomes', which may be deployed to organize otherwise uncontrollable dynamics of resident microbiomes. Integration of microfluidics, robotics and machine learning provides novel ways to capitalize on core microbiomes for increasing resource-efficiency and stress-resistance of agroecosystems."
29724864,3.0,Empowering thyroid hormone research in human subjects using OMICs technologies,2018 Jul;238(1):R13-R29.,"OMICs subsume different physiological layers including the genome, transcriptome, proteome and metabolome. Recent advances in analytical techniques allow for the exhaustive determination of biomolecules in all OMICs levels from less invasive human specimens such as blood and urine. Investigating OMICs in deeply characterized population-based or experimental studies has led to seminal improvement of our understanding of genetic determinants of thyroid function, identified putative thyroid hormone target genes and thyroid hormone-induced shifts in the plasma protein and metabolite content. Consequently, plasma biomolecules have been suggested as surrogates of tissue-specific action of thyroid hormones. This review provides a brief introduction to OMICs in thyroid research with a particular focus on metabolomics studies in humans elucidating the important role of thyroid hormones for whole body metabolism in adults."
29723481,,The potential for machine learning algorithms to improve and reduce the cost of 3-dimensional printing for surgical planning,2018 May;15(5):349-356.,"Introduction:                    3D-printed anatomical models play an important role in medical and research settings. The recent successes of 3D anatomical models in healthcare have led many institutions to adopt the technology. However, there remain several issues that must be addressed before it can become more wide-spread. Of importance are the problems of cost and time of manufacturing. Machine learning (ML) could be utilized to solve these issues by streamlining the 3D modeling process through rapid medical image segmentation and improved patient selection and image acquisition. The current challenges, potential solutions, and future directions for ML and 3D anatomical modeling in healthcare are discussed.              Areas covered:                    This review covers research articles in the field of machine learning as related to 3D anatomical modeling. Topics discussed include automated image segmentation, cost reduction, and related time constraints.              Expert commentary:                    ML-based segmentation of medical images could potentially improve the process of 3D anatomical modeling. However, until more research is done to validate these technologies in clinical practice, their impact on patient outcomes will remain unknown. We have the necessary computational tools to tackle the problems discussed. The difficulty now lies in our ability to collect sufficient data."
29705713,2.0,The neural markers of MRI to differentiate depression and panic disorder,2019 Apr 20;91:72-78.,"Depression and panic disorder (PD) share the common pathophysiology from the perspectives of neurotransmitters. The relatively high comorbidity between depression and PD contributes to the substantial obstacles to differentiate from depression and PD, especially for the brain pathophysiology. There are significant differences in the diagnostic criteria between depression and PD. However, the paradox of similar pathophysiology and different diagnostic criteria in these two disorders were still the issues needing to be addressed. Therefore the clarification of potential difference in the field of neuroscience and pathophysiology between depression and PD can help the clinicians and scientists to understand more comprehensively about significant differences between depression and PD. The researchers should be curious about the underlying difference of pathophysiology beneath the significant distinction of clinical symptoms. In this review article, I tried to find some evidences for the differences between depression and PD, especially for neural markers revealed by magnetic resonance imaging (MRI). The distinctions of structural and functional alterations in depression and PD are reviewed. From the structural perspectives, PD seems to have less severe gray matter alterations in frontal and temporal lobes than depression. The study of white matter microintegrity reveals more widespread alterations in fronto-limbic circuit of depression patients than PD patients, such as the uncinate fasciculus and anterior thalamic radiation. PD might have a more restrictive pattern of structural alterations when compared to depression. For the functional perspectives, the core site of depression pathophysiology is the anterior subnetwork of resting-state network, such as anterior cingulate cortex, which is not significantly altered in PD. A possibly emerging pattern of fronto-limbic distinction between depression and PD has been revealed by these explorative reports. The future trend for machine learning and pattern recognition might confirm the differentiation pattern between depression and PD based on the explorative results."
29704807,47.0,Applications of low-cost sensing technologies for air quality monitoring and exposure assessment: How far have they gone?,2018 Jul;116:286-299.,"Over the past decade, a range of sensor technologies became available on the market, enabling a revolutionary shift in air pollution monitoring and assessment. With their cost of up to three orders of magnitude lower than standard/reference instruments, many avenues for applications have opened up. In particular, broader participation in air quality discussion and utilisation of information on air pollution by communities has become possible. However, many questions have been also asked about the actual benefits of these technologies. To address this issue, we conducted a comprehensive literature search including both the scientific and grey literature. We focused upon two questions: (1) Are these technologies fit for the various purposes envisaged? and (2) How far have these technologies and their applications progressed to provide answers and solutions? Regarding the former, we concluded that there is no clear answer to the question, due to a lack of: sensor/monitor manufacturers' quantitative specifications of performance, consensus regarding recommended end-use and associated minimal performance targets of these technologies, and the ability of the prospective users to formulate the requirements for their applications, or conditions of the intended use. Numerous studies have assessed and reported sensor/monitor performance under a range of specific conditions, and in many cases the performance was concluded to be satisfactory. The specific use cases for sensors/monitors included outdoor in a stationary mode, outdoor in a mobile mode, indoor environments and personal monitoring. Under certain conditions of application, project goals, and monitoring environments, some sensors/monitors were fit for a specific purpose. Based on analysis of 17 large projects, which reached applied outcome stage, and typically conducted by consortia of organizations, we observed that a sizable fraction of them (~ 30%) were commercial and/or crowd-funded. This fact by itself signals a paradigm change in air quality monitoring, which previously had been primarily implemented by government organizations. An additional paradigm-shift indicator is the growing use of machine learning or other advanced data processing approaches to improve sensor/monitor agreement with reference monitors. There is still some way to go in enhancing application of the technologies for source apportionment, which is of particular necessity and urgency in developing countries. Also, there has been somewhat less progress in wide-scale monitoring of personal exposures. However, it can be argued that with a significant future expansion of monitoring networks, including indoor environments, there may be less need for wearable or portable sensors/monitors to assess personal exposure. Traditional personal monitoring would still be valuable where spatial variability of pollutants of interest is at a finer resolution than the monitoring network can resolve."
29700073,8.0,Biomedical Informatics on the Cloud: A Treasure Hunt for Advancing Cardiovascular Medicine,2018 Apr 27;122(9):1290-1301.,"In the digital age of cardiovascular medicine, the rate of biomedical discovery can be greatly accelerated by the guidance and resources required to unearth potential collections of knowledge. A unified computational platform leverages metadata to not only provide direction but also empower researchers to mine a wealth of biomedical information and forge novel mechanistic insights. This review takes the opportunity to present an overview of the cloud-based computational environment, including the functional roles of metadata, the architecture schema of indexing and search, and the practical scenarios of machine learning-supported molecular signature extraction. By introducing several established resources and state-of-the-art workflows, we share with our readers a broadly defined informatics framework to phenotype cardiovascular health and disease."
29697740,10.0,Pattern recognition analysis on long noncoding RNAs: a tool for prediction in plants,2019 Mar 25;20(2):682-689.,"Motivation:                    Long noncoding RNAs (lncRNAs) correspond to a eukaryotic noncoding RNA class that gained great attention in the past years as a higher layer of regulation for gene expression in cells. There is, however, a lack of specific computational approaches to reliably predict lncRNA in plants, which contrast the variety of prediction tools available for mammalian lncRNAs. This distinction is not that obvious, given that biological features and mechanisms generating lncRNAs in the cell are likely different between animals and plants. Considering this, we present a machine learning analysis and a classifier approach called RNAplonc (https://github.com/TatianneNegri/RNAplonc/) to identify lncRNAs in plants.              Results:                    Our feature selection analysis considered 5468 features, and it used only 16 features to robustly identify lncRNA with the REPTree algorithm. That was the base to create the model and train it with lncRNA and mRNA data from five plant species (thale cress, cucumber, soybean, poplar and Asian rice). After an extensive comparison with other tools largely used in plants (CPC, CPC2, CPAT and PLncPRO), we found that RNAplonc produced more reliable lncRNA predictions from plant transcripts with 87.5% of the best result in eight tests in eight species from the GreeNC database and four independent studies in monocotyledonous (Brachypodium) and eudicotyledonous (Populus and Gossypium) species."
29697304,17.0,Deep learning in pharmacogenomics: from gene regulation to patient stratification,2018 May;19(7):629-650.,"This Perspective provides examples of current and future applications of deep learning in pharmacogenomics, including: identification of novel regulatory variants located in noncoding domains of the genome and their function as applied to pharmacoepigenomics; patient stratification from medical records; and the mechanistic prediction of drug response, targets and their interactions. Deep learning encapsulates a family of machine learning algorithms that has transformed many important subfields of artificial intelligence over the last decade, and has demonstrated breakthrough performance improvements on a wide range of tasks in biomedicine. We anticipate that in the future, deep learning will be widely used to predict personalized drug response and optimize medication selection and dosing, using knowledge extracted from large and complex molecular, epidemiological, clinical and demographic datasets."
