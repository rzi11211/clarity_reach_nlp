pmid,citations,title,date,text
30211115,28.0,The Network of Non-coding RNAs in Cancer Drug Resistance,2018 Aug 29;8:327.,"Non-coding RNAs (ncRNAs) have been implicated in most cellular functions. The disruption of their function through somatic mutations, genomic imprinting, transcriptional and post-transcriptional regulation, plays an ever-increasing role in cancer development. ncRNAs, including notorious microRNAs, have been thus proposed to function as tumor suppressors or oncogenes, often in a context-dependent fashion. In parallel, ncRNAs with altered expression in cancer have been reported to exert a key role in determining drug sensitivity or restoring drug responsiveness in resistant cells. Acquisition of resistance to anti-cancer drugs is a major hindrance to effective chemotherapy and is one of the most important causes of relapse and mortality in cancer patients. For these reasons, non-coding RNAs have become recent focuses as prognostic agents and modifiers of chemo-sensitivity. This review starts with a brief outline of the role of most studied non-coding RNAs in cancer and then highlights the modulation of cancer drug resistance via known ncRNAs based mechanisms. We identified from literature 388 ncRNA-drugs interactions and analyzed them using an unsupervised approach. Essentially, we performed a network analysis of the non-coding RNAs with direct relations with cancer drugs. Within such a machine-learning framework we detected the most representative ncRNAs-drug associations and groups. We finally discussed the higher integration of the drug-ncRNA clusters with the goal of disentangling effectors from downstream effects and further clarify the involvement of ncRNAs in the cellular mechanisms underlying resistance to cancer treatments."
30209997,9.0,A Brief Survey of Machine Learning Application in Cancerlectin Identification,2018;18(5):257-267.,"Proteins with at least one carbohydrate recognition domain are lectins that can identify and reversibly interact with glycan moiety of glycoconjugates or a soluble carbohydrate. It has been proved that lectins can play various vital roles in mediating signal transduction, cell-cell recognition and interaction, immune defense, and so on. Most organisms can synthesize and secret lectins. A portion of lectins closely related to diverse cancers, called cancerlectins, are involved in tumor initiation, growth and recrudescence. Cancerlectins have been investigated for their applications in the laboratory study, clinical diagnosis and therapy, and drug delivery and targeting of cancers. The identification of cancerlectin genes from a lot of lectins is helpful for dissecting cancers. Several cancerlectin prediction tools based on machine learning approaches have been established and have become an excellent complement to experimental methods. In this review, we comprehensively summarize and expound the indispensable materials for implementing cancerlectin prediction models. We hope that this review will contribute to understanding cancerlectins and provide valuable clues for the study of cancerlectins. Novel systems for cancerlectin gene identification are expected to be developed for clinical applications and gene therapy."
30204184,13.0,Towards operando computational modeling in heterogeneous catalysis,2018 Nov 12;47(22):8307-8348.,"An increased synergy between experimental and theoretical investigations in heterogeneous catalysis has become apparent during the last decade. Experimental work has extended from ultra-high vacuum and low temperature towards operando conditions. These developments have motivated the computational community to move from standard descriptive computational models, based on inspection of the potential energy surface at 0 K and low reactant concentrations (0 K/UHV model), to more realistic conditions. The transition from 0 K/UHV to operando models has been backed by significant developments in computer hardware and software over the past few decades. New methodological developments, designed to overcome part of the gap between 0 K/UHV and operando conditions, include (i) global optimization techniques, (ii) ab initio constrained thermodynamics, (iii) biased molecular dynamics, (iv) microkinetic models of reaction networks and (v) machine learning approaches. The importance of the transition is highlighted by discussing how the molecular level picture of catalytic sites and the associated reaction mechanisms changes when the chemical environment, pressure and temperature effects are correctly accounted for in molecular simulations. It is the purpose of this review to discuss each method on an equal footing, and to draw connections between methods, particularly where they may be applied in combination."
30201975,5.0,Paving the way for precision medicine v2.0 in intensive care by profiling necroinflammation in biofluids,2019 Jan;26(1):83-98.,"Current clinical diagnosis is typically based on a combination of approaches including clinical examination of the patient, clinical experience, physiologic and/or genetic parameters, high-tech diagnostic medical imaging, and an extended list of laboratory values mostly determined in biofluids such as blood and urine. One could consider this as precision medicine v1.0. However, recent advances in technology and better understanding of molecular mechanisms underlying disease will allow us to better characterize patients in the future. These improvements will enable us to distinguish patients who have similar clinical presentations but different cellular and molecular responses. Treatments will be able to be chosen more ""precisely"", resulting in more appropriate therapy, precision medicine v2.0. In this review, we will reflect on the potential added value of recent advances in technology and a better molecular understanding of necrosis and inflammation for improving diagnosis and treatment of critically ill patients. We give a brief overview on the mutual interplay between necrosis and inflammation, which are two crucial detrimental factors in organ and/or systemic dysfunction. One of the challenges for the future will thus be the cellular and molecular profiling of necroinflammation in biofluids. The huge amount of data generated by profiling biomolecules and single cells through, for example, different omic-approaches is needed for data mining methods to allow patient-clustering and identify novel biomarkers. The real-time monitoring of biomarkers will allow continuous (re)evaluation of treatment strategies using machine learning models. Ultimately, we may be able to offer precision therapies specifically designed to target the molecular set-up of an individual patient, as has begun to be done in cancer therapeutics."
30201875,2.0,Prediction Methods of Herbal Compounds in Chinese Medicinal Herbs,2018 Sep 10;23(9):2303.,"Chinese herbal medicine has recently gained worldwide attention. The curative mechanism of Chinese herbal medicine is compared with that of western medicine at the molecular level. The treatment mechanism of most Chinese herbal medicines is still not clear. How do we integrate Chinese herbal medicine compounds with modern medicine? Chinese herbal medicine drug-like prediction method is particularly important. A growing number of Chinese herbal source compounds are now widely used as drug-like compound candidates. An important way for pharmaceutical companies to develop drugs is to discover potentially active compounds from related herbs in Chinese herbs. The methods for predicting the drug-like properties of Chinese herbal compounds include the virtual screening method, pharmacophore model method and machine learning method. In this paper, we focus on the prediction methods for the medicinal properties of Chinese herbal medicines. We analyze the advantages and disadvantages of the above three methods, and then introduce the specific steps of the virtual screening method. Finally, we present the prospect of the joint application of various methods."
30201325,6.0,A survey on computer-assisted Parkinson's Disease diagnosis,2019 Apr;95:48-63.,"Background and objective:                    In this work, we present a systematic review concerning the recent enabling technologies as a tool to the diagnosis, treatment and better quality of life of patients diagnosed with Parkinson's Disease (PD), as well as an analysis of future trends on new approaches to this end.              Methods:                    In this review, we compile a number of works published at some well-established databases, such as Science Direct, IEEEXplore, PubMed, Plos One, Multidisciplinary Digital Publishing Institute (MDPI), Association for Computing Machinery (ACM), Springer and Hindawi Publishing Corporation. Each selected work has been carefully analyzed in order to identify its objective, methodology and results.              Results:                    The review showed the majority of works make use of signal-based data, which are often acquired by means of sensors. Also, we have observed the increasing number of works that employ virtual reality and e-health monitoring systems to increase the life quality of PD patients. Despite the different approaches found in the literature, almost all of them make use of some sort of machine learning mechanism to aid the automatic PD diagnosis.              Conclusions:                    The main focus of this survey is to consider computer-assisted diagnosis, and how effective they can be when handling the problem of PD identification. Also, the main contribution of this review is to consider very recent works only, mainly from 2015 and 2016."
30200333,20.0,Machine Learning for Drug-Target Interaction Prediction,2018 Aug 31;23(9):2208.,"Identifying drug-target interactions will greatly narrow down the scope of search of candidate medications, and thus can serve as the vital first step in drug discovery. Considering that in vitro experiments are extremely costly and time-consuming, high efficiency computational prediction methods could serve as promising strategies for drug-target interaction (DTI) prediction. In this review, our goal is to focus on machine learning approaches and provide a comprehensive overview. First, we summarize a brief list of databases frequently used in drug discovery. Next, we adopt a hierarchical classification scheme and introduce several representative methods of each category, especially the recent state-of-the-art methods. In addition, we compare the advantages and limitations of methods in each category. Lastly, we discuss the remaining challenges and future outlook of machine learning in DTI prediction. This article may provide a reference and tutorial insights on machine learning-based DTI prediction for future researchers."
30197419,4.0,"HLBS-PopOmics: an online knowledge base to accelerate dissemination and implementation of research advances in population genomics to reduce the burden of heart, lung, blood, and sleep disorders",2019 Mar;21(3):519-524.,"Recent dramatic advances in multiomics research coupled with exponentially increasing volume, complexity, and interdisciplinary nature of publications are making it challenging for scientists to stay up-to-date on the literature. Strategies to address this challenge include the creation of online databases and warehouses to support timely and targeted dissemination of research findings. Although most of the early examples have been in cancer genomics and pharmacogenomics, the approaches used can be adapted to support investigators in heart, lung, blood, and sleep (HLBS) disorders research. In this article, we describe the creation of an HLBS population genomics (HLBS-PopOmics) knowledge base as an online, continuously updated, searchable database to support the dissemination and implementation of studies and resources that are relevant to clinical and public health practice. In addition to targeted searches based on the HLBS disease categories, cross-cutting themes reflecting the ethical, legal, and social implications of genomics research; systematic evidence reviews; and clinical practice guidelines supporting screening, detection, evaluation, and treatment are also emphasized in HLBS-PopOmics. Future updates of the knowledge base will include additional emphasis on transcriptomics, proteomics, metabolomics, and other omics research; explore opportunities for leveraging data sets designed to support scientific discovery; and incorporate advanced machine learning bioinformatics capabilities."
30195423,5.0,A review of image analysis and machine learning techniques for automated cervical cancer screening from pap-smear images,2018 Oct;164:15-22.,"Background and objective:                    Early diagnosis and classification of a cancer type can help facilitate the subsequent clinical management of the patient. Cervical cancer ranks as the fourth most prevalent cancer affecting women worldwide and its early detection provides the opportunity to help save life. To that end, automated diagnosis and classification of cervical cancer from pap-smear images has become a necessity as it enables accurate, reliable and timely analysis of the condition's progress. This paper presents an overview of the state of the art as articulated in prominent recent publications focusing on automated detection of cervical cancer from pap-smear images.              Methods:                    The survey reviews publications on applications of image analysis and machine learning in automated diagnosis and classification of cervical cancer from pap-smear images spanning 15 years. The survey reviews 30 journal papers obtained electronically through four scientific databases (Google Scholar, Scopus, IEEE and Science Direct) searched using three sets of keywords: (1) segmentation, classification, cervical cancer; (2) medical imaging, machine learning, pap-smear; (3) automated system, classification, pap-smear.              Results:                    Most of the existing algorithms facilitate an accuracy of nearly 93.78% on an open pap-smear data set, segmented using CHAMP digital image software. K-nearest-neighbors and support vector machines algorithms have been reported to be excellent classifiers for cervical images with accuracies of over 99.27% and 98.5% respectively when applied to a 2-class classification problem (normal or abnormal).              Conclusion:                    The reviewed papers indicate that there are still weaknesses in the available techniques that result in low accuracy of classification in some classes of cells. Moreover, most of the existing algorithms work either on single or on multiple cervical smear images. This accuracy can be increased by varying various parameters such as the features to be extracted, improvement in noise removal, using hybrid segmentation and classification techniques such of multi-level classifiers. Combining K-nearest-neighbors algorithm with other algorithm(s) such as support vector machines, pixel level classifications and including statistical shape models can also improve performance. Further, most of the developed classifiers are tested on accurately segmented images using commercially available software such as CHAMP software. There is thus a deficit of evidence that these algorithms will work in clinical settings found in developing countries (where 85% of cervical cancer incidences occur) that lack sufficient trained cytologists and the funds to buy the commercial segmentation software."
30190674,8.0,A Comprehensive Review of Magnetoencephalography (MEG) Studies for Brain Functionality in Healthy Aging and Alzheimer's Disease (AD),2018 Aug 23;12:60.,"Neural oscillations were established with their association with neurophysiological activities and the altered rhythmic patterns are believed to be linked directly to the progression of cognitive decline. Magnetoencephalography (MEG) is a non-invasive technique to record such neuronal activity due to excellent temporal and fair amount of spatial resolution. Single channel, connectivity as well as brain network analysis using MEG data in resting state and task-based experiments were analyzed from existing literature. Single channel analysis studies reported a less complex, more regular and predictable oscillations in Alzheimer's disease (AD) primarily in the left parietal, temporal and occipital regions. Investigations on both functional connectivity (FC) and effective (EC) connectivity analysis demonstrated a loss of connectivity in AD compared to healthy control (HC) subjects found in higher frequency bands. It has been reported from multiplex network of MEG study in AD in the affected regions of hippocampus, posterior default mode network (DMN) and occipital areas, however, conclusions cannot be drawn due to limited availability of clinical literature. Potential utilization of high spatial resolution in MEG likely to provide information related to in-depth brain functioning and underlying factors responsible for changes in neuronal waves in AD. This review is a comprehensive report to investigate diagnostic biomarkers for AD may be identified by from MEG data. It is also important to note that MEG data can also be utilized for the same pursuit in combination with other imaging modalities."
30190664,20.0,Unraveling the bioactivity of anticancer peptides as deduced from machine learning,2018 Jul 25;17:734-752.,"Cancer imposes a global health burden as it represents one of the leading causes of morbidity and mortality while also giving rise to significant economic burden owing to the associated expenditures for its monitoring and treatment. In spite of advancements in cancer therapy, the low success rate and recurrence of tumor has necessitated the ongoing search for new therapeutic agents. Aside from drugs based on small molecules and protein-based biopharmaceuticals, there has been an intense effort geared towards the development of peptide-based therapeutics owing to its favorable and intrinsic properties of being relatively small, highly selective, potent, safe and low in production costs. In spite of these advantages, there are several inherent weaknesses that are in need of attention in the design and development of therapeutic peptides. An abundance of data on bioactive and therapeutic peptides have been accumulated over the years and the burgeoning area of artificial intelligence has set the stage for the lucrative utilization of machine learning to make sense of these large and high-dimensional data. This review summarizes the current state-of-the-art on the application of machine learning for studying the bioactivity of anticancer peptides along with future outlook of the field. Data and R codes used in the analysis herein are available on GitHub at https://github.com/Shoombuatong2527/anticancer-peptides-review."
30184176,15.0,Twenty years of bioinformatics research for protease-specific substrate and cleavage site prediction: a comprehensive revisit and benchmarking of existing methods,2019 Nov 27;20(6):2150-2166.,"The roles of proteolytic cleavage have been intensively investigated and discussed during the past two decades. This irreversible chemical process has been frequently reported to influence a number of crucial biological processes (BPs), such as cell cycle, protein regulation and inflammation. A number of advanced studies have been published aiming at deciphering the mechanisms of proteolytic cleavage. Given its significance and the large number of functionally enriched substrates targeted by specific proteases, many computational approaches have been established for accurate prediction of protease-specific substrates and their cleavage sites. Consequently, there is an urgent need to systematically assess the state-of-the-art computational approaches for protease-specific cleavage site prediction to further advance the existing methodologies and to improve the prediction performance. With this goal in mind, in this article, we carefully evaluated a total of 19 computational methods (including 8 scoring function-based methods and 11 machine learning-based methods) in terms of their underlying algorithm, calculated features, performance evaluation and software usability. Then, extensive independent tests were performed to assess the robustness and scalability of the reviewed methods using our carefully prepared independent test data sets with 3641 cleavage sites (specific to 10 proteases). The comparative experimental results demonstrate that PROSPERous is the most accurate generic method for predicting eight protease-specific cleavage sites, while GPS-CCD and LabCaS outperformed other predictors for calpain-specific cleavage sites. Based on our review, we then outlined some potential ways to improve the prediction performance and ease the computational burden by applying ensemble learning, deep learning, positive unlabeled learning and parallel and distributed computing techniques. We anticipate that our study will serve as a practical and useful guide for interested readers to further advance next-generation bioinformatics tools for protease-specific cleavage site prediction."
30184111,2.0,Molecular differential diagnosis of uterine leiomyomas and leiomyosarcomas,2019 Dec 24;101(6):1115-1123.,"Uterine leiomyomas (LM) and leiomyosarcomas (LMS) are considered biologically unrelated tumors due to their cytogenetic and molecular disparity. Yet, these tumors share morphological and molecular characteristics that cannot be differentiated through current clinical diagnostic tests, and thus cannot be definitively classified as benign or malignant until surgery. Newer approaches are needed for the identification of these tumors, as has been done for other tissues. The application of next generation sequencing enables the detection of new mutations that, when coupled to machine learning bioinformatic tools, advances our understanding of chromosomal instability. These approaches in the context of LM and LMS could allow the discovery of genetic variants and possible genomic markers. Additionally, the potential clinical utility of circulating cell-free tumor DNA could revolutionize the noninvasive detection and monitoring of these tumors. Here, we seek to provide a perspective on the molecular background of LM and LMS, recognizing their distinct molecular features that may lead to improved diagnosis and personalized treatments, which would have a measurable impact on women's reproductive health."
30182829,3.0,A Brief Review on Software Tools in Generating Chou's Pseudo-factor Representations for All Types of Biological Sequences,2018;25(9):822-829.,"Background:                    In the post-genome age, it is more urgent to understand the functions of genes and proteins. Since experimental methods are usually costly and time consuming, computational predictions are recognized as an alternative approach. In developing a predictive method for functional genomics and proteomics, one of the most important steps is to represent biological sequences with a fixed length numerical form, which can be further analyzed using machine learning algorithms. Chou's pseudo-amino acid compositions and the pseudo k-nucleotide compositions are algorithms for this purpose.              Conclusion:                    Since the appearance of these algorithms, several software tools have been developed as implementations. These software tools facilitate the application of these algorithms. As these software tools are developed with different technologies and for different application scenarios, we will briefly review the technical aspect of these software tools in this short review."
30182201,17.0,Machine learning: applications of artificial intelligence to imaging and diagnosis,2019 Feb;11(1):111-118.,"Machine learning (ML) is a form of artificial intelligence which is placed to transform the twenty-first century. Rapid, recent progress in its underlying architecture and algorithms and growth in the size of datasets have led to increasing computer competence across a range of fields. These include driving a vehicle, language translation, chatbots and beyond human performance at complex board games such as Go. Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimisation. We then summarise the applications of ML to medicine. In particular, we showcase recent diagnostic performances, and caveats, in the fields of dermatology, radiology, pathology and general microscopy."
30172554,2.0,Outcome Tracking in Facial Palsy,2018 Dec;51(6):1033-1050.,"Outcome tracking in facial palsy is multimodal, consisting of patient-reported outcome measures, clinician-graded scoring systems, objective assessment tools, and novel tools for layperson and spontaneity assessment. Patient-reported outcome measures are critical to understanding burden of disease in facial palsy and effects of interventions from the patient perspective. Clinician-graded scoring systems are inherently subjective and no 1 single system satisfies all needs. Objective assessment tools quantify facial movements but can be laborious. Recent advances in facial recognition technology have enabled automated facial measurements. Novel assessment tools analyze attributes such as spontaneous smile, emotional expressivity, disfigurement, and attractiveness as determined by laypersons."
30170101,19.0,A Deep Look Into the Future of Quantitative Imaging in Oncology: A Statement of Working Principles and Proposal for Change,2018 Nov 15;102(4):1074-1082.,"The adoption of enterprise digital imaging, along with the development of quantitative imaging methods and the re-emergence of statistical learning, has opened the opportunity for more personalized cancer treatments through transformative data science research. In the last 5 years, accumulating evidence has indicated that noninvasive advanced imaging analytics (i.e., radiomics) can reveal key components of tumor phenotype for multiple lesions at multiple time points over the course of treatment. Many groups using homegrown software have extracted engineered and deep quantitative features on 3-dimensional medical images for better spatial and longitudinal understanding of tumor biology and for the prediction of diverse outcomes. These developments could augment patient stratification and prognostication, buttressing emerging targeted therapeutic approaches. Unfortunately, the rapid growth in popularity of this immature scientific discipline has resulted in many early publications that miss key information or use underpowered patient data sets, without production of generalizable results. Quantitative imaging research is complex, and key principles should be followed to realize its full potential. The fields of quantitative imaging and radiomics in particular require a renewed focus on optimal study design and reporting practices, standardization, interpretability, data sharing, and clinical trials. Standardization of image acquisition, feature calculation, and statistical analysis (i.e., machine learning) are required for the field to move forward. A new data-sharing paradigm enacted among open and diverse participants (medical institutions, vendors and associations) should be embraced for faster development and comprehensive clinical validation of imaging biomarkers. In this review and critique of the field, we propose working principles and fundamental changes to the current scientific approach, with the goal of high-impact research and development of actionable prediction models that will yield more meaningful applications of precision cancer medicine."
30169341,2.0,What we can learn from Big Data about factors influencing perioperative outcome,2018 Dec;31(6):723-731.,"Purpose of review:                    This narrative review will discuss what value Big Data has to offer anesthesiology and aims to highlight recently published articles of large databases exploring factors influencing perioperative outcome. Additionally, the future perspectives of Big Data and its major pitfalls will be discussed.              Recent findings:                    The potential of Big Data has given an incentive to create nationwide and anesthesia-initiated registries like the MPOG and NACOR. These large databases have contributed in elucidating some of the rare perioperative complications, such as declined cognition after exposure to general anesthesia and epidural hematomas in parturients. Additionally, they are useful in finding patterns such as similar outcome in subtypes of beta-blockers and lower incidence of pneumonia in preoperative influenza vaccinations in the elderly.              Summary:                    Big Data is becoming increasingly popular with the collaborative collection of registries offering anesthesia a way to explore rare perioperative complications and outcome to encourage further hypotheses testing. Although Big Data has its flaws in security, lack of expertise and methodological concerns, the future potential of analytics combined with genomics, machine learning and real-time decision support looks promising."
30168572,2.0,Perspectives and applications of machine learning for evolutionary developmental biology,2018 Oct 8;14(5):289-306.,"Evolutionary Developmental Biology (Evo-Devo) is an ever-expanding field that aims to understand how development was modulated by the evolutionary process. In this sense, ""omic"" studies emerged as a powerful ally to unravel the molecular mechanisms underlying development. In this scenario, bioinformatics tools become necessary to analyze the growing amount of information. Among computational approaches, machine learning stands out as a promising field to generate knowledge and trace new research perspectives for bioinformatics. In this review, we aim to expose the current advances of machine learning applied to evolution and development. We draw clear perspectives and argue how evolution impacted machine learning techniques."
30167371,21.0,Structural neuroimaging as clinical predictor: A review of machine learning applications,2018 Aug 10;20:506-522.,"In this paper, we provide an extensive overview of machine learning techniques applied to structural magnetic resonance imaging (MRI) data to obtain clinical classifiers. We specifically address practical problems commonly encountered in the literature, with the aim of helping researchers improve the application of these techniques in future works. Additionally, we survey how these algorithms are applied to a wide range of diseases and disorders (e.g. Alzheimer's disease (AD), Parkinson's disease (PD), autism, multiple sclerosis, traumatic brain injury, etc.) in order to provide a comprehensive view of the state of the art in different fields."
30159406,24.0,Machine learning and image-based profiling in drug discovery,2018 Aug;10:43-52.,"The increase in imaging throughput, new analytical frameworks and high-performance computational resources open new avenues for data-rich phenotypic profiling of small molecules in drug discovery. Image-based profiling assays assessing single-cell phenotypes have been used to explore mechanisms of action, target efficacy and toxicity of small molecules. Technological advances to generate large data sets together with new machine learning approaches for the analysis of high-dimensional profiling data create opportunities to improve many steps in drug discovery. In this review, we will discuss how recent studies applied machine learning approaches in functional profiling workflows with a focus on chemical genetics. While their utility in image-based screening and profiling is predictably evident, examples of novel insights beyond the status quo based on the applications of machine learning approaches are just beginning to emerge. To enable discoveries, future studies also need to develop methodologies that lower the entry barriers to high-throughput profiling experiments by streamlining image-based profiling assays and providing applications for advanced learning technologies such as easy to deploy deep neural networks."
30158822,27.0,Natural Language Processing of Social Media as Screening for Suicide Risk,2018 Aug 27;10:1178222618792860.,"Suicide is among the 10 most common causes of death, as assessed by the World Health Organization. For every death by suicide, an estimated 138 people's lives are meaningfully affected, and almost any other statistic around suicide deaths is equally alarming. The pervasiveness of social media-and the near-ubiquity of mobile devices used to access social media networks-offers new types of data for understanding the behavior of those who (attempt to) take their own lives and suggests new possibilities for preventive intervention. We demonstrate the feasibility of using social media data to detect those at risk for suicide. Specifically, we use natural language processing and machine learning (specifically deep learning) techniques to detect quantifiable signals around suicide attempts, and describe designs for an automated system for estimating suicide risk, usable by those without specialized mental health training (eg, a primary care doctor). We also discuss the ethical use of such technology and examine privacy implications. Currently, this technology is only used for intervention for individuals who have ""opted in"" for the analysis and intervention, but the technology enables scalable screening for suicide risk, potentially identifying many people who are at risk preventively and prior to any engagement with a health care system. This raises a significant cultural question about the trade-off between privacy and prevention-we have potentially life-saving technology that is currently reaching only a fraction of the possible people at risk because of respect for their privacy. Is the current trade-off between privacy and prevention the right one?"
30157513,1.0,"Sensor, Signal, and Imaging Informatics in 2017",2018 Aug;27(1):110-113.,"Objective:                     To summarize significant contributions to sensor, signal, and imaging informatics literature published in 2017.              Methods:                     PubMed® and Web of Science® were searched to identify the scientific publications published in 2017 that addressed sensors, signals, and imaging in medical informatics. Fifteen papers were selected by consensus as candidate best papers. Each candidate article was reviewed by section editors and at least two other external reviewers. The final selection of the four best papers was conducted by the editorial board of the International Medical Informatics Association (IMIA) Yearbook.              Results:                     The selected papers of 2017 demonstrate the important scientific advances in management and analysis of sensor, signal, and imaging information.              Conclusion:                    The growth of signal and imaging data and the increasing power of machine learning techniques have engendered new opportunities for research in medical informatics. This synopsis highlights cutting-edge contributions to the science of Sensor, Signal, and Imaging Informatics."
30156155,9.0,Targeting Virus-host Protein Interactions: Feature Extraction and Machine Learning Approaches,2019;20(3):177-184.,"Background:                    Targeting critical viral-host Protein-Protein Interactions (PPIs) has enormous application prospects for therapeutics. Using experimental methods to evaluate all possible virus-host PPIs is labor-intensive and time-consuming. Recent growth in computational identification of virus-host PPIs provides new opportunities for gaining biological insights, including applications in disease control. We provide an overview of recent computational approaches for studying virus-host PPI interactions.              Methods:                    In this review, a variety of computational methods for virus-host PPIs prediction have been surveyed. These methods are categorized based on the features they utilize and different machine learning algorithms including classical and novel methods.              Results:                    We describe the pivotal and representative features extracted from relevant sources of biological data, mainly include sequence signatures, known domain interactions, protein motifs and protein structure information. We focus on state-of-the-art machine learning algorithms that are used to build binary prediction models for the classification of virus-host protein pairs and discuss their abilities, weakness and future directions.              Conclusion:                    The findings of this review confirm the importance of computational methods for finding the potential protein-protein interactions between virus and host. Although there has been significant progress in the prediction of virus-host PPIs in recent years, there is a lot of room for improvement in virus-host PPI prediction."
30155978,12.0,Current state and future prospects of artificial intelligence in ophthalmology: a review,2019 Jan;47(1):128-139.,"Artificial intelligence (AI) has emerged as a major frontier in computer science research. Although AI has broad application across many medical fields, it will have particular utility in ophthalmology and will dramatically change the diagnostic and treatment pathways for many eye conditions such as corneal ectasias, glaucoma, age-related macular degeneration and diabetic retinopathy. However, given that AI has primarily been driven as a computer science, its concepts and terminology are unfamiliar to many medical professionals. Important key terms such as machine learning and deep learning are often misunderstood and incorrectly used interchangeably. This article presents an overview of AI and new developments relevant to ophthalmology."
30153635,24.0,Applications of machine learning algorithms to predict therapeutic outcomes in depression: A meta-analysis and systematic review,2018 Dec 1;241:519-532.,"Background:                    No previous study has comprehensively reviewed the application of machine learning algorithms in mood disorders populations. Herein, we qualitatively and quantitatively evaluate previous studies of machine learning-devised models that predict therapeutic outcomes in mood disorders populations.              Methods:                    We searched Ovid MEDLINE/PubMed from inception to February 8, 2018 for relevant studies that included adults with bipolar or unipolar depression; assessed therapeutic outcomes with a pharmacological, neuromodulatory, or manual-based psychotherapeutic intervention for depression; applied a machine learning algorithm; and reported predictors of therapeutic response. A random-effects meta-analysis of proportions and meta-regression analyses were conducted.              Results:                    We identified 639 records: 75 full-text publications were assessed for eligibility; 26 studies (n=17,499) and 20 studies (n=6325) were included in qualitative and quantitative review, respectively. Classification algorithms were able to predict therapeutic outcomes with an overall accuracy of 0.82 (95% confidence interval [CI] of [0.77, 0.87]). Pooled estimates of classification accuracy were significantly greater (p < 0.01) in models informed by multiple data types (e.g., composite of phenomenological patient features and neuroimaging or peripheral gene expression data; pooled proportion [95% CI] = 0.93[0.86, 0.97]) when compared to models with lower-dimension data types (pooledproportion=0.68[0.62,0.74]to0.85[0.81,0.88]).              Limitations:                    Most studies were retrospective; differences in machine learning algorithms and their implementation (e.g., cross-validation, hyperparameter tuning); cannot infer importance of individual variables fed into learning algorithm.              Conclusions:                    Machine learning algorithms provide a powerful conceptual and analytic framework capable of integrating multiple data types and sources. An integrative approach may more effectively model neurobiological components as functional modules of pathophysiology embedded within the complex, social dynamics that influence the phenomenology of mental disorders."
30149790,9.0,Hyperspectral Sensors and Imaging Technologies in Phytopathology: State of the Art,2018 Aug 25;56:535-558.,"Plant disease detection represents a tremendous challenge for research and practical applications. Visual assessment by human raters is time-consuming, expensive, and error prone. Disease rating and plant protection need new and innovative techniques to address forthcoming challenges and trends in agricultural production that require more precision than ever before. Within this context, hyperspectral sensors and imaging techniques-intrinsically tied to efficient data analysis approaches-have shown an enormous potential to provide new insights into plant-pathogen interactions and for the detection of plant diseases. This article provides an overview of hyperspectral sensors and imaging technologies for assessing compatible and incompatible plant-pathogen interactions. Within the progress of digital technologies, the vision, which is increasingly discussed in the society and industry, includes smart and intuitive solutions for assessing plant features in plant phenotyping or for making decisions on plant protection measures in the context of precision agriculture."
30149250,5.0,"Calcium detection, its quantification, and grayscale morphology-based risk stratification using machine learning in multimodality big data coronary and carotid scans: A review",2018 Oct 1;101:184-198.,"Purpose of review:                    Atherosclerosis is the leading cause of cardiovascular disease (CVD) and stroke. Typically, atherosclerotic calcium is found during the mature stage of the atherosclerosis disease. It is therefore often a challenge to identify and quantify the calcium. This is due to the presence of multiple components of plaque buildup in the arterial walls. The American College of Cardiology/American Heart Association guidelines point to the importance of calcium in the coronary and carotid arteries and further recommend its quantification for the prevention of heart disease. It is therefore essential to stratify the CVD risk of the patient into low- and high-risk bins.              Recent finding:                    Calcium formation in the artery walls is multifocal in nature with sizes at the micrometer level. Thus, its detection requires high-resolution imaging. Clinical experience has shown that even though optical coherence tomography offers better resolution, intravascular ultrasound still remains an important imaging modality for coronary wall imaging. For a computer-based analysis system to be complete, it must be scientifically and clinically validated. This study presents a state-of-the-art review (condensation of 152 publications after examining 200 articles) covering the methods for calcium detection and its quantification for coronary and carotid arteries, the pros and cons of these methods, and the risk stratification strategies. The review also presents different kinds of statistical models and gold standard solutions for the evaluation of software systems useful for calcium detection and quantification. Finally, the review concludes with a possible vision for designing the next-generation system for better clinical outcomes."
30139641,14.0,Complex-Trait Prediction in the Era of Big Data,2018 Oct;34(10):746-754.,"Accurate prediction of complex traits requires using a large number of DNA variants. Advances in statistical and machine learning methodology enable the identification of complex patterns in high-dimensional settings. However, training these highly parameterized methods requires very large data sets. Until recently, such data sets were not available. But the situation is changing rapidly as very large biomedical data sets comprising individual genotype-phenotype data for hundreds of thousands of individuals become available in public and private domains. We argue that the convergence of advances in methodology and the advent of Big Genomic Data will enable unprecedented improvements in complex-trait prediction; we review theory and evidence supporting our claim and discuss challenges and opportunities that Big Data will bring to complex-trait prediction."
30137230,1.0,Machine learning meets genome assembly,2019 Nov 27;20(6):2116-2129.,"Motivation:                    With the recent advances in DNA sequencing technologies, the study of the genetic composition of living organisms has become more accessible for researchers. Several advances have been achieved because of it, especially in the health sciences. However, many challenges which emerge from the complexity of sequencing projects remain unsolved. Among them is the task of assembling DNA fragments from previously unsequenced organisms, which is classified as an NP-hard (nondeterministic polynomial time hard) problem, for which no efficient computational solution with reasonable execution time exists. However, several tools that produce approximate solutions have been used with results that have facilitated scientific discoveries, although there is ample room for improvement. As with other NP-hard problems, machine learning algorithms have been one of the approaches used in recent years in an attempt to find better solutions to the DNA fragment assembly problem, although still at a low scale.              Results:                    This paper presents a broad review of pioneering literature comprising artificial intelligence-based DNA assemblers-particularly the ones that use machine learning-to provide an overview of state-of-the-art approaches and to serve as a starting point for further study in this field."
30136381,29.0,Machine learning in major depression: From classification to treatment outcome prediction,2018 Nov;24(11):1037-1052.,"Aims:                    Major depression disorder (MDD) is the single greatest cause of disability and morbidity, and affects about 10% of the population worldwide. Currently, there are no clinically useful diagnostic biomarkers that are able to confirm a diagnosis of MDD from bipolar disorder (BD) in the early depressive episode. Therefore, exploring translational biomarkers of mood disorders based on machine learning is in pressing need, though it is challenging, but with great potential to improve our understanding of these disorders.              Discussions:                    In this study, we review popular machine-learning methods used for brain imaging classification and predictions, and provide an overview of studies, specifically for MDD, that have used magnetic resonance imaging data to either (a) classify MDDs from controls or other mood disorders or (b) investigate treatment outcome predictors for individual patients. Finally, challenges, future directions, and potential limitations related to MDD biomarker identification are also discussed, with a goal of offering a comprehensive overview that may help readers to better understand the applications of neuroimaging data mining in depression.              Conclusions:                    We hope such efforts may highlight the need for an urgently needed paradigm shift in treatment, to guide personalized optimal clinical care."
30126707,3.0,Pharmacovigilance: An Overview,2018 Dec;40(12):1991-2004.,"Purpose:                    Pharmacovigilance (PV) is a relatively new discipline in the pharmaceutical industry. Having undergone rapid growth over the past 2 decades, PV now touches many other disciplines in the research and development enterprise. With its growth has come a heightened awareness and interest in the medical community about the roles that PV plays. This article provides insights into the background and inner workings of PV.              Methods:                    This narrative review covers the core PV activities and other major areas of the pharmaceutical enterprise in which PV makes significant contributions.              Findings:                    Drug safety monitoring activities were organized by the US Food and Drug Administration and academic medical centers in the early 1950s in response to growing concern over the occurrence of aplastic anemia and other blood dyscrasias associated with the use of chloramphenicol. This experience was codified in the 1962 Kefauver-Harris Amendments to the Federal Food, Drug and Cosmetic Act as adverse event evaluation and reporting requirements. The ensuing decades have seen the development of core PV functions for pharmaceutical companies: case management, signal management, and benefit-risk management. A broader scope of PV has developed to include the following major activities: support of patient safety during the conduct of clinical trials through assuring proper use of informed consent and institutional review boards (ethics committees); selection of the first safe dose for use in humans, based on pharmacologic data obtained in animal studies; development of the safety profile for proper use of a new molecular entity and appropriate communication of that information to the range of relevant stakeholders; attendance to surveillance activities through a set of signal management processes; monitoring the manufactured product itself through collaborative activities with manufacturing professionals; management of benefit-risk to assure appropriate use in medical care after marketing; and maintenance of inspection readiness as a corporate cultural process.              Implications:                    The extent and pace of change promise to accelerate with the integration of biomedical informatics, analytics, artificial intelligence, and machine learning. This progress has implications for the development of the next generation of PV professionals who will need to be trained in entirely new skill sets to lead continued improvements in the safe use of pharmaceuticals."
33141723,1.0,"Robots for the people, by the people: Personalizing human-machine interaction",2018 Aug 22;3(21):eaat7451.,"Multimodal, interactive, and multitask machine learning can be applied to personalize human-robot and human-machine interactions for the broad diversity of individuals and their unique needs."
30124495,3.0,Novel imaging techniques in pulmonary hypertension,2018 Nov;33(6):587-593.,"Purpose of review:                    Pulmonary hypertension is a life-shortening condition, which may be idiopathic but is more frequently seen in association with other conditions. Current guidelines recommend cardiac catheterization to confirm the diagnosis of pulmonary hypertension. Evidence suggests an increasing role for noninvasive imaging modalities in the initial diagnostic and prognostic assessment and evaluation of treatment response.              Recent findings:                    In this review we examine the evidence for current noninvasive imaging methodologies: echocardiography computed tomography and MRI in the diagnostic and prognostic assessment of suspected pulmonary hypertension and explore the potential utility of modeling and machine-learning approaches.              Summary:                    Noninvasive imaging allows a comprehensive assessment of patients with suspected pulmonary hypertension. It plays a key part in the initial diagnostic and prognostic assessment and machine-learning approaches show promise in the diagnosis of pulmonary hypertension."
30124358,37.0,"Rise of Deep Learning for Genomic, Proteomic, and Metabolomic Data Integration in Precision Medicine",2018 Oct;22(10):630-636.,"Machine learning (ML) is being ubiquitously incorporated into everyday products such as Internet search, email spam filters, product recommendations, image classification, and speech recognition. New approaches for highly integrated manufacturing and automation such as the Industry 4.0 and the Internet of things are also converging with ML methodologies. Many approaches incorporate complex artificial neural network architectures and are collectively referred to as deep learning (DL) applications. These methods have been shown capable of representing and learning predictable relationships in many diverse forms of data and hold promise for transforming the future of omics research and applications in precision medicine. Omics and electronic health record data pose considerable challenges for DL. This is due to many factors such as low signal to noise, analytical variance, and complex data integration requirements. However, DL models have already been shown capable of both improving the ease of data encoding and predictive model performance over alternative approaches. It may not be surprising that concepts encountered in DL share similarities with those observed in biological message relay systems such as gene, protein, and metabolite networks. This expert review examines the challenges and opportunities for DL at a systems and biological scale for a precision medicine readership."
30124147,23.0,Survey of Machine Learning Techniques in Drug Discovery,2019;20(3):185-193.,"Background:                    Drug discovery, which is the process of discovering new candidate medications, is very important for pharmaceutical industries. At its current stage, discovering new drugs is still a very expensive and time-consuming process, requiring Phases I, II and III for clinical trials. Recently, machine learning techniques in Artificial Intelligence (AI), especially the deep learning techniques which allow a computational model to generate multiple layers, have been widely applied and achieved state-of-the-art performance in different fields, such as speech recognition, image classification, bioinformatics, etc. One very important application of these AI techniques is in the field of drug discovery.              Methods:                    We did a large-scale literature search on existing scientific websites (e.g, ScienceDirect, Arxiv) and startup companies to understand current status of machine learning techniques in drug discovery.              Results:                    Our experiments demonstrated that there are different patterns in machine learning fields and drug discovery fields. For example, keywords like prediction, brain, discovery, and treatment are usually in drug discovery fields. Also, the total number of papers published in drug discovery fields with machine learning techniques is increasing every year.              Conclusion:                    The main focus of this survey is to understand the current status of machine learning techniques in the drug discovery field within both academic and industrial settings, and discuss its potential future applications. Several interesting patterns for machine learning techniques in drug discovery fields are discussed in this survey."
30122222,8.0,Recent applications of machine learning in medicinal chemistry,2018 Sep 15;28(17):2807-2815.,"In recent decades, artificial intelligence and machine learning have played a significant role in increasing the efficiency of processes across a wide spectrum of industries. When it comes to the pharmaceutical and biotechnology sectors, numerous tools enabled by advancement of computer science have been developed and are now routinely utilized. However, there are many aspects of the drug discovery process, which can further benefit from refinement of computational methods and tools, as well as improvement of accessibility of these new technologies. In this review, examples of recent developments in machine learning application are described, which have the potential to impact different parts of the drug discovery and development flow scheme. Notably, new deep learning-based approaches across compound design and synthesis, prediction of binding, activity and ADMET properties, as well as applications of genetic algorithms are highlighted."
30116102,3.0,A Novel Approach for Identifying Relevant Genes for Breast Cancer Survivability on Specific Therapies,2018 Aug 10;14:1176934318790266.,"Analyzing the genetic activity of breast cancer survival for a specific type of therapy provides a better understanding of the body response to the treatment and helps select the best course of action and while leading to the design of drugs based on gene activity. In this work, we use supervised and nonsupervised machine learning methods to deal with a multiclass classification problem in which we label the samples based on the combination of the 5-year survivability and treatment; we focus on hormone therapy, radiotherapy, and surgery. The proposed nonsupervised hierarchical models are created to find the highest separability between combinations of the classes. The supervised model consists of a combination of feature selection techniques and efficient classifiers used to find a potential set of biomarker genes specific to response to therapy. The results show that different models achieve different performance scores with accuracies ranging from 80.9% to 100%. We have investigated the roles of many biomarkers through the literature and found that some of the discriminative genes in the computational model such as ZC3H11A, VAX2, MAF1, and ZFP91 are related to breast cancer and other types of cancer."
30110960,46.0,Machine Learning in Agriculture: A Review,2018 Aug 14;18(8):2674.,"Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action."
30105410,21.0,"Demystification of AI-driven medical image interpretation: past, present and future",2019 Mar;29(3):1616-1624.,"The recent explosion of 'big data' has ushered in a new era of artificial intelligence (AI) algorithms in every sphere of technological activity, including medicine, and in particular radiology. However, the recent success of AI in certain flagship applications has, to some extent, masked decades-long advances in computational technology development for medical image analysis. In this article, we provide an overview of the history of AI methods for radiological image analysis in order to provide a context for the latest developments. We review the functioning, strengths and limitations of more classical methods as well as of the more recent deep learning techniques. We discuss the unique characteristics of medical data and medical science that set medicine apart from other technological domains in order to highlight not only the potential of AI in radiology but also the very real and often overlooked constraints that may limit the applicability of certain AI methods. Finally, we provide a comprehensive perspective on the potential impact of AI on radiology and on how to evaluate it not only from a technical point of view but also from a clinical one, so that patients can ultimately benefit from it. KEY POINTS: • Artificial intelligence (AI) research in medical imaging has a long history • The functioning, strengths and limitations of more classical AI methods is reviewed, together with that of more recent deep learning methods. • A perspective is provided on the potential impact of AI on radiology and on its evaluation from both technical and clinical points of view."
30105183,5.0,Futuristic biosensors for cardiac health care: an artificial intelligence approach,2018 Aug;8(8):358.,"Biosensor-based devices are pioneering in the modern biomedical applications and will be the future of cardiac health care. The coupling of artificial intelligence (AI) for cardiac monitoring-based biosensors for the point of care (POC) diagnostics is prominently reviewed here. This review deciphers the most significant machine-learning algorithms for the futuristic biosensors along with the internet of things, computational techniques and microchip-based essential cardiac biomarkers for real-time health monitoring and improving patient compliance. The present review also discusses the recently developed cardiac biosensors along with technical strategies involved in their mechanism of working and their applications in healthcare. Additionally, it provides a key for the ontogeny of an effective and supportive hierarchical protocol for clinical decision-making about personalized medicine through combinatory information analysis, and integrated multidisciplinary AI approaches."
30104148,43.0,Deep Learning for Plant Stress Phenotyping: Trends and Future Perspectives,2018 Oct;23(10):883-898.,"Deep learning (DL), a subset of machine learning approaches, has emerged as a versatile tool to assimilate large amounts of heterogeneous data and provide reliable predictions of complex and uncertain phenomena. These tools are increasingly being used by the plant science community to make sense of the large datasets now regularly collected via high-throughput phenotyping and genotyping. We review recent work where DL principles have been utilized for digital image-based plant stress phenotyping. We provide a comparative assessment of DL tools against other existing techniques, with respect to decision accuracy, data size requirement, and applicability in various scenarios. Finally, we outline several avenues of research leveraging current and future DL tools in plant science."
30103448,10.0,Machine Learning Based Toxicity Prediction: From Chemical Structural Description to Transcriptome Analysis,2018 Aug 10;19(8):2358.,"Toxicity prediction is very important to public health. Among its many applications, toxicity prediction is essential to reduce the cost and labor of a drug's preclinical and clinical trials, because a lot of drug evaluations (cellular, animal, and clinical) can be spared due to the predicted toxicity. In the era of Big Data and artificial intelligence, toxicity prediction can benefit from machine learning, which has been widely used in many fields such as natural language processing, speech recognition, image recognition, computational chemistry, and bioinformatics, with excellent performance. In this article, we review machine learning methods that have been applied to toxicity prediction, including deep learning, random forests, k-nearest neighbors, and support vector machines. We also discuss the input parameter to the machine learning algorithm, especially its shift from chemical structural description only to that combined with human transcriptome data analysis, which can greatly enhance prediction accuracy."
30102808,25.0,eDoctor: machine learning and the future of medicine,2018 Dec;284(6):603-619.,"Machine learning (ML) is a burgeoning field of medicine with huge resources being applied to fuse computer science and statistics to medical problems. Proponents of ML extol its ability to deal with large, complex and disparate data, often found within medicine and feel that ML is the future for biomedical research, personalized medicine, computer-aided diagnosis to significantly advance global health care. However, the concepts of ML are unfamiliar to many medical professionals and there is untapped potential in the use of ML as a research tool. In this article, we provide an overview of the theory behind ML, explore the common ML algorithms used in medicine including their pitfalls and discuss the potential future of ML in medicine."
30101283,17.0,Bioinformatics applications on Apache Spark,2018 Aug 1;7(8):giy098.,"With the rapid development of next-generation sequencing technology, ever-increasing quantities of genomic data pose a tremendous challenge to data processing. Therefore, there is an urgent need for highly scalable and powerful computational systems. Among the state-of-the-art parallel computing platforms, Apache Spark is a fast, general-purpose, in-memory, iterative computing framework for large-scale data processing that ensures high fault tolerance and high scalability by introducing the resilient distributed dataset abstraction. In terms of performance, Spark can be up to 100 times faster in terms of memory access and 10 times faster in terms of disk access than Hadoop. Moreover, it provides advanced application programming interfaces in Java, Scala, Python, and R. It also supports some advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. We surveyed Spark-based applications used in next-generation sequencing and other biological domains, such as epigenetics, phylogeny, and drug discovery. The results of this survey are used to provide a comprehensive guideline allowing bioinformatics researchers to apply Spark in their own fields."
30101124,6.0,The Role of Machine Learning in Knowledge-Based Response-Adapted Radiotherapy,2018 Jul 27;8:266.,"With the continuous increase in radiotherapy patient-specific data from multimodality imaging and biotechnology molecular sources, knowledge-based response-adapted radiotherapy (KBR-ART) is emerging as a vital area for radiation oncology personalized treatment. In KBR-ART, planned dose distributions can be modified based on observed cues in patients' clinical, geometric, and physiological parameters. In this paper, we present current developments in the field of adaptive radiotherapy (ART), the progression toward KBR-ART, and examine several applications of static and dynamic machine learning approaches for realizing the KBR-ART framework potentials in maximizing tumor control and minimizing side effects with respect to individual radiotherapy patients. Specifically, three questions required for the realization of KBR-ART are addressed: (1) what knowledge is needed; (2) how to estimate RT outcomes accurately; and (3) how to adapt optimally. Different machine learning algorithms for KBR-ART application shall be discussed and contrasted. Representative examples of different KBR-ART stages are also visited."
30099485,11.0,Recent advances and prospects of computational methods for metabolite identification: a review with emphasis on machine learning approaches,2019 Nov 27;20(6):2028-2043.,"Metabolomics involves studies of a great number of metabolites, which are small molecules present in biological systems. They play a lot of important functions such as energy transport, signaling, building block of cells and inhibition/catalysis. Understanding biochemical characteristics of the metabolites is an essential and significant part of metabolomics to enlarge the knowledge of biological systems. It is also the key to the development of many applications and areas such as biotechnology, biomedicine or pharmaceuticals. However, the identification of the metabolites remains a challenging task in metabolomics with a huge number of potentially interesting but unknown metabolites. The standard method for identifying metabolites is based on the mass spectrometry (MS) preceded by a separation technique. Over many decades, many techniques with different approaches have been proposed for MS-based metabolite identification task, which can be divided into the following four groups: mass spectra database, in silico fragmentation, fragmentation tree and machine learning. In this review paper, we thoroughly survey currently available tools for metabolite identification with the focus on in silico fragmentation, and machine learning-based approaches. We also give an intensive discussion on advanced machine learning methods, which can lead to further improvement on this task."
30099083,7.0,Speech analysis for health: Current state-of-the-art and the increasing impact of deep learning,2018 Dec 1;151:41-54.,"Due to the complex and intricate nature associated with their production, the acoustic-prosodic properties of a speech signal are modulated with a range of health related effects. There is an active and growing area of machine learning research in this speech and health domain, focusing on developing paradigms to objectively extract and measure such effects. Concurrently, deep learning is transforming intelligent signal analysis, such that machines are now reaching near human capabilities in a range of recognition and analysis tasks. Herein, we review current state-of-the-art approaches with speech-based health detection, placing a particular focus on the impact of deep learning within this domain. Based on this overview, it is evident while that deep learning based solutions be become more present in the literature, it has not had the same overall dominating effect seen in other related fields. In this regard, we suggest some possible research directions aimed at fully leveraging the advantages that deep learning can offer speech-based health detection."
30097794,32.0,Machine learning and feature selection for drug response prediction in precision oncology applications,2019 Feb;11(1):31-39.,"In-depth modeling of the complex interplay among multiple omics data measured from cancer cell lines or patient tumors is providing new opportunities toward identification of tailored therapies for individual cancer patients. Supervised machine learning algorithms are increasingly being applied to the omics profiles as they enable integrative analyses among the high-dimensional data sets, as well as personalized predictions of therapy responses using multi-omics panels of response-predictive biomarkers identified through feature selection and cross-validation. However, technical variability and frequent missingness in input ""big data"" require the application of dedicated data preprocessing pipelines that often lead to some loss of information and compressed view of the biological signal. We describe here the state-of-the-art machine learning methods for anti-cancer drug response modeling and prediction and give our perspective on further opportunities to make better use of high-dimensional multi-omics profiles along with knowledge about cancer pathways targeted by anti-cancer compounds when predicting their phenotypic responses."
30097305,9.0,Priors in Animal and Artificial Intelligence: Where Does Learning Begin?,2018 Nov;22(11):963-965.,"A major goal for the next generation of artificial intelligence (AI) is to build machines that are able to reason and cope with novel tasks, environments, and situations in a manner that approaches the abilities of animals. Evidence from precocial species suggests that driving learning through suitable priors can help to successfully face this challenge."
30095052,,"Perturbation Theory Machine Learning Models: Theory, Regulatory Issues, and Applications to Organic Synthesis, Medicinal Chemistry, Protein Research, and Technology",2018;18(14):1203-1213.,"Machine Learning (ML) models are very useful to predict physicochemical properties of small organic molecules, proteins, proteomes, and complex systems. These methods may be useful to reduce the cost of research in terms of materials resources, time, and laboratory animal sacrifice. Recently different authors have reported Perturbation Theory (PT) methods combined with ML to obtain PTML (PT + ML) models. They have applied PTML models to the study of different biological systems and in technology as well. Here, we present one state-of- the-art review about the different applications of PTML models in Organic Synthesis, Medicinal Chemistry, Protein Research, and Technology. In this work, we also embrace an overview of regulatory issues for acceptance and validation of both: the Cheminformatics models, and the characterization of new Biomaterials. This is a main question in order to make scientific result self for humans and environment."
30092623,2.0,Prediction of Acquired Taxane Resistance Using a Personalized Pathway-Based Machine Learning Method,2019 Apr;51(2):672-684.,"Purpose:                    This study was conducted to develop and validate an individualized prediction model for automated detection of acquired taxane resistance (ATR).              Materials and methods:                    Penalized regression, combinedwith an individualized pathway score algorithm,was applied to construct a predictive model using publically available genomic cohorts of ATR and intrinsic taxane resistance (ITR). To develop a model with enhanced generalizability, we merged multiple ATR studies then updated the learning parameter via robust cross-study validation.              Results:                    For internal cross-study validation, the ATR model produced a perfect performance with an overall area under the receiver operating curve (AUROC) of 1.000 with an area under the precision-recall curve (AUPRC) of 1.000, a Brier score of 0.007, a sensitivity and a specificity of 100%. The model showed an excellent performance on two independent blind ATR cohorts (overall AUROC of 0.940, AUPRC of 0.940, a Brier score of 0.127). When we applied our algorithm to two large-scale pharmacogenomic resources for ITR, the Cancer Genome Project (CGP) and the Cancer Cell Line Encyclopedia (CCLE), an overall ITR cross-study AUROC was 0.70, which is a far better accuracy than an almost random level reported by previous studies. Furthermore, this model had a high transferability on blind ATR cohorts with an AUROC of 0.69, suggesting that general predictive features may be at work across both ITR and ATR.              Conclusion:                    We successfully constructed a multi-study-derived personalized prediction model for ATR with excellent accuracy, generalizability, and transferability."
30091413,3.0,Application of Machine Learning Approaches for the Design and Study of Anticancer Drugs,2019;20(5):488-500.,"Background:                    Globally the number of cancer patients and deaths are continuing to increase yearly, and cancer has, therefore, become one of the world's highest causes of morbidity and mortality. In recent years, the study of anticancer drugs has become one of the most popular medical topics.              Objective:                    In this review, in order to study the application of machine learning in predicting anticancer drugs activity, some machine learning approaches such as Linear Discriminant Analysis (LDA), Principal components analysis (PCA), Support Vector Machine (SVM), Random forest (RF), k-Nearest Neighbor (kNN), and Naïve Bayes (NB) were selected, and the examples of their applications in anticancer drugs design are listed.              Results:                    Machine learning contributes a lot to anticancer drugs design and helps researchers by saving time and is cost effective. However, it can only be an assisting tool for drug design.              Conclusion:                    This paper introduces the application of machine learning approaches in anticancer drug design. Many examples of success in identification and prediction in the area of anticancer drugs activity prediction are discussed, and the anticancer drugs research is still in active progress. Moreover, the merits of some web servers related to anticancer drugs are mentioned."
30090035,4.0,What's app? Electronic health technology in inflammatory bowel disease,2018 Jul;16(3):366-373.,"Electronic health (eHealth) data collection is increasingly used in many chronic illnesses, to track pattern of disease. eHealth systems have the potential to revolutionize care. Inflammatory bowel disease (IBD) is a paradigm for such an approach: this is a chronic disease that usually affects young and technologically literate patient population, who are motivated to be involved in their own care. A range of eHealth technologies are available for IBD. This review considers the strengths and weaknesses of 7 platforms that focus on patient-provider interaction. These have been developed in Denmark, United States, the Netherlands, and the United Kingdom, demonstrating an international interest in this form of technology and interaction. Not only do these technologies aim to improve care but they also have the potential to collect large amounts of information. Information includes demographics and patient reported outcomes (symptoms, quality of life), quality of care (steroid use, among other metrics) and outcomes such as hospitalization. These data could inform quality improvement programmes to improve their focus. eHealth technology is also open to machine learning to analyze large data sets, through which personalized algorithms may be developed."
30084867,13.0,"LncFinder: an integrated platform for long non-coding RNA identification utilizing sequence intrinsic composition, structural information and physicochemical property",2019 Nov 27;20(6):2009-2027.,"Discovering new long non-coding RNAs (lncRNAs) has been a fundamental step in lncRNA-related research. Nowadays, many machine learning-based tools have been developed for lncRNA identification. However, many methods predict lncRNAs using sequence-derived features alone, which tend to display unstable performances on different species. Moreover, the majority of tools cannot be re-trained or tailored by users and neither can the features be customized or integrated to meet researchers' requirements. In this study, features extracted from sequence-intrinsic composition, secondary structure and physicochemical property are comprehensively reviewed and evaluated. An integrated platform named LncFinder is also developed to enhance the performance and promote the research of lncRNA identification. LncFinder includes a novel lncRNA predictor using the heterologous features we designed. Experimental results show that our method outperforms several state-of-the-art tools on multiple species with more robust and satisfactory results. Researchers can additionally employ LncFinder to extract various classic features, build classifier with numerous machine learning algorithms and evaluate classifier performance effectively and efficiently. LncFinder can reveal the properties of lncRNA and mRNA from various perspectives and further inspire lncRNA-protein interaction prediction and lncRNA evolution analysis. It is anticipated that LncFinder can significantly facilitate lncRNA-related research, especially for the poorly explored species. LncFinder is released as R package (https://CRAN.R-project.org/package=LncFinder). A web server (http://bmbl.sdstate.edu/lncfinder/) is also developed to maximize its availability."
30084866,31.0,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",2019 Sep 27;20(5):1878-1912.,"The identification of interactions between drugs/compounds and their targets is crucial for the development of new drugs. In vitro screening experiments (i.e. bioassays) are frequently used for this purpose; however, experimental approaches are insufficient to explore novel drug-target interactions, mainly because of feasibility problems, as they are labour intensive, costly and time consuming. A computational field known as 'virtual screening' (VS) has emerged in the past decades to aid experimental drug discovery studies by statistically estimating unknown bio-interactions between compounds and biological targets. These methods use the physico-chemical and structural properties of compounds and/or target proteins along with the experimentally verified bio-interaction information to generate predictive models. Lately, sophisticated machine learning techniques are applied in VS to elevate the predictive performance. The objective of this study is to examine and discuss the recent applications of machine learning techniques in VS, including deep learning, which became highly popular after giving rise to epochal developments in the fields of computer vision and natural language processing. The past 3 years have witnessed an unprecedented amount of research studies considering the application of deep learning in biomedicine, including computational drug discovery. In this review, we first describe the main instruments of VS methods, including compound and protein features (i.e. representations and descriptors), frequently used libraries and toolkits for VS, bioactivity databases and gold-standard data sets for system training and benchmarking. We subsequently review recent VS studies with a strong emphasis on deep learning applications. Finally, we discuss the present state of the field, including the current challenges and suggest future directions. We believe that this survey will provide insight to the researchers working in the field of computational drug discovery in terms of comprehending and developing novel bio-prediction methods."
30084331,1.0,Advances in Computational Studies of Potential Drug Targets in Mycobacterium tuberculosis,2018;18(13):1062-1074.,"Tuberculosis continues to remain as one of the leading causes of death worldwide, in spite of significant progress being made in the last twenty years through increased compliance to treatment. The current review gives an overview of the recent efforts made in the endeavor to identify novel inhibitors and promising drug targets for Mycobacterium tuberculosis with structure and ligand-based approaches along with bioinformatics studies following complete sequencing of its genome. A large number of these studies target biomolecules in metabolic pathways that are vital for the survival of the microorganism. A discussion on efforts to study metalloproteins as relatively underexplored targets in the context of their druggability is also presented."
30082644,14.0,Evolution of In Silico Strategies for Protein-Protein Interaction Drug Discovery,2018 Aug 6;23(8):1963.,"The advent of advanced molecular modeling software, big data analytics, and high-speed processing units has led to the exponential evolution of modern drug discovery and better insights into complex biological processes and disease networks. This has progressively steered current research interests to understanding protein-protein interaction (PPI) systems that are related to a number of relevant diseases, such as cancer, neurological illnesses, metabolic disorders, etc. However, targeting PPIs are challenging due to their ""undruggable"" binding interfaces. In this review, we focus on the current obstacles that impede PPI drug discovery, and how recent discoveries and advances in in silico approaches can alleviate these barriers to expedite the search for potential leads, as shown in several exemplary studies. We will also discuss about currently available information on PPI compounds and systems, along with their usefulness in molecular modeling. Finally, we conclude by presenting the limits of in silico application in drug discovery and offer a perspective in the field of computer-aided PPI drug discovery."
30082307,2.0,Algorithmic accountability,2018 Sep 13;376(2128):20170362.,"There is enormous opportunity for positive social impact from the rise of algorithms and machine learning. But this requires a licence to operate from the public, based on trustworthiness. There are a range of concerns relating to how algorithms might be held to account in areas affecting the public sphere. This paper outlines a number of approaches including greater transparency, monitoring of outcomes and improved governance. It makes a case that public sector bodies that hold datasets should be more confident in negotiating terms with the private sector. It also argues that all regulators (not just data regulators) need to wake up to the challenges posed by changing technology. Other improvements include diversity of the workforce, ethics training, codes of conduct for data scientists, and new deliberative bodies. Even if these narrower issues are solved, the paper poses some wider concerns including data monopolies, the challenge to democracy, public participation and maintaining the public interest.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'."
30082306,4.0,How should we regulate artificial intelligence?,2018 Sep 13;376(2128):20170360.,"Using artificial intelligence (AI) technology to replace human decision-making will inevitably create new risks whose consequences are unforeseeable. This naturally leads to calls for regulation, but I argue that it is too early to attempt a general system of AI regulation. Instead, we should work incrementally within the existing legal and regulatory schemes which allocate responsibility, and therefore liability, to persons. Where AI clearly creates risks which current law and regulation cannot deal with adequately, then new regulation will be needed. But in most cases, the current system can work effectively if the producers of AI technology can provide sufficient transparency in explaining how AI decisions are made. Transparency ex post can often be achieved through retrospective analysis of the technology's operations, and will be sufficient if the main goal is to compensate victims of incorrect decisions. Ex ante transparency is more challenging, and can limit the use of some AI technologies such as neural networks. It should only be demanded by regulation where the AI presents risks to fundamental rights, or where society needs reassuring that the technology can safely be used. Masterly inactivity in regulation is likely to achieve a better long-term solution than a rush to regulate in ignorance.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'."
30082299,1.0,Algorithms: transparency and accountability,2018 Sep 13;376(2128):20170351.,"This opinion piece explores the issues of accountability and transparency in relation to the growing use of machine learning algorithms. Citing the recent work of the Royal Society and the British Academy, it looks at the legal protections for individuals afforded by the EU General Data Protection Regulation and asks whether the legal system will be able to adapt to rapid technological change. It concludes by calling for continuing debate that is itself accountable, transparent and public.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'."
30082298,6.0,Machine learning and genomics: precision medicine versus patient privacy,2018 Sep 13;376(2128):20170350.,"Machine learning can have a major societal impact in computational biology applications. In particular, it plays a central role in the development of precision medicine, whereby treatment is tailored to the clinical or genetic features of the patient. However, these advances require collecting and sharing among researchers large amounts of genomic data, which generates much concern about privacy. Researchers, study participants and governing bodies should be aware of the ways in which the privacy of participants might be compromised, as well as of the large body of research on technical solutions to these issues. We review how breaches in patient privacy can occur, present recent developments in computational data protection and discuss how they can be combined with legal and ethical perspectives to provide secure frameworks for genomic data sharing.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'."
30082061,5.0,Using Technology to Inform and Deliver Precise Personalized Care to Patients With End-Stage Kidney Disease,2018 Jul;38(4):418-425.,"Consistent with the increase of precision medicine, the care of patients with end-stage kidney disease (ESKD) requiring maintenance dialysis therapy should evolve to become more personalized. Precise and personalized care is nuanced and informed by a number of factors including an individual's needs and preferences, disease progression, and response to and tolerance of treatments. Technology can support the delivery of more precise and personalized care through multiple mechanisms, including more accurate and real-time assessments of key care elements, enhanced treatment monitoring, and remote monitoring of home dialysis therapies. Data from health care and non-health care sources and advanced analytical methods such as machine learning can be used to create novel insights, and large volumes of data can be integrated to support clinical decisions. Health care models continue to evolve and the opportunities and need for novel care approaches supported by technology and health informatics continue to expand as the delivery and organization of health care changes. Ultimately, precise personalized care for ESKD, including dialysis therapy, will become more feasible as the biological, social, and environmental determinants of health are more broadly understood and as advances in science, engineering, and information management create the means to provide truly precise care for ESKD."
30081607,27.0,Indirect Measurement of Ground Reaction Forces and Moments by Means of Wearable Inertial Sensors: A Systematic Review,2018 Aug 5;18(8):2564.,"In the last few years, estimating ground reaction forces by means of wearable sensors has come to be a challenging research topic paving the way to kinetic analysis and sport performance testing outside of labs. One possible approach involves estimating the ground reaction forces from kinematic data obtained by inertial measurement units (IMUs) worn by the subject. As estimating kinetic quantities from kinematic data is not an easy task, several models and protocols have been developed over the years. Non-wearable sensors, such as optoelectronic systems along with force platforms, remain the most accurate systems to record motion. In this review, we identified, selected and categorized the methodologies for estimating the ground reaction forces from IMUs as proposed across the years. Scopus, Google Scholar, IEEE Xplore, and PubMed databases were interrogated on the topic of Ground Reaction Forces estimation based on kinematic data obtained by IMUs. The identified papers were classified according to the methodology proposed: (i) methods based on direct modelling; (ii) methods based on machine learning. The methods based on direct modelling were further classified according to the task studied (walking, running, jumping, etc.). Finally, we comparatively examined the methods in order to identify the most reliable approaches for the implementation of a ground reaction force estimator based on IMU data."
30076935,67.0,Artificial intelligence in retina,2018 Nov;67:1-29.,"Major advances in diagnostic technologies are offering unprecedented insight into the condition of the retina and beyond ocular disease. Digital images providing millions of morphological datasets can fast and non-invasively be analyzed in a comprehensive manner using artificial intelligence (AI). Methods based on machine learning (ML) and particularly deep learning (DL) are able to identify, localize and quantify pathological features in almost every macular and retinal disease. Convolutional neural networks thereby mimic the path of the human brain for object recognition through learning of pathological features from training sets, supervised ML, or even extrapolation from patterns recognized independently, unsupervised ML. The methods of AI-based retinal analyses are diverse and differ widely in their applicability, interpretability and reliability in different datasets and diseases. Fully automated AI-based systems have recently been approved for screening of diabetic retinopathy (DR). The overall potential of ML/DL includes screening, diagnostic grading as well as guidance of therapy with automated detection of disease activity, recurrences, quantification of therapeutic effects and identification of relevant targets for novel therapeutic approaches. Prediction and prognostic conclusions further expand the potential benefit of AI in retina which will enable personalized health care as well as large scale management and will empower the ophthalmologist to provide high quality diagnosis/therapy and successfully deal with the complexity of 21st century ophthalmology."
30068955,7.0,Machine learning at the energy and intensity frontiers of particle physics,2018 Aug;560(7716):41-48.,"Our knowledge of the fundamental particles of nature and their interactions is summarized by the standard model of particle physics. Advancing our understanding in this field has required experiments that operate at ever higher energies and intensities, which produce extremely large and information-rich data samples. The use of machine-learning techniques is revolutionizing how we interpret these data samples, greatly increasing the discovery potential of present and future experiments. Here we summarize the challenges and opportunities that come with the use of machine learning at the frontiers of particle physics."
30068270,2.0,Recent Advances in Computational Methods for Identifying Anticancer Peptides,2019;20(5):481-487.,"Anticancer peptide (ACP) is a kind of small peptides that can kill cancer cells without damaging normal cells. In recent years, ACP has been pre-clinically used for cancer treatment. Therefore, accurate identification of ACPs will promote their clinical applications. In contrast to labor-intensive experimental techniques, a series of computational methods have been proposed for identifying ACPs. In this review, we briefly summarized the current progress in computational identification of ACPs. The challenges and future perspectives in developing reliable methods for identification of ACPs were also discussed. We anticipate that this review could provide novel insights into future researches on anticancer peptides."
30063164,6.0,Are there advances in pharmacotherapy for panic disorder? A systematic review of the past five years,2018 Aug;19(12):1357-1368.,"Introduction:                    Several effective medications are available for treating panic disorder (PD). However, outcomes are unsatisfactory in a number of patients, suggesting the usefulness of expanding the array of antipanic drugs and improving the quality of response to current recommended treatments.              Areas covered:                    The authors have performed an updated systematic review of pharmacological studies (phase III onwards) to examine whether advances have been made in the last five years. Only four studies were included. D-cycloserine no longer seemed promising as a cognitive-behavioral therapy (CBT) enhancer. Some preliminary findings concerning the optimization of recommended medications deserved consideration, including: the possibility that SSRIs are more effective than CBT alone in treating panic attacks, combined therapy is preferable when agoraphobia is present, and clonazepam is more potent than paroxetine in decreasing panic relapse.              Expert opinion:                    Given the lack of novel treatments, expanding a personalized approach to the existing medications seems to be the most feasible strategy to improve pharmacotherapy outcomes regarding PD. Recent technological progress, including wearable devices collecting real-time data, 'big data' platforms, and application of machine learning techniques might help make outcome prediction more reliable. Further research on previously promising novel treatments is also recommended."
31763498,4.0,How to find the right drug for each patient? Advances and challenges in pharmacogenomics,2018 Aug;10:53-62.,"Cancer is a highly heterogeneous disease with complex underlying biology. For these reasons, effective cancer treatment is still a challenge. Nowadays, it is clear that a cancer therapy that fits all the cases cannot be found, and as a result the design of therapies tailored to the patient's molecular characteristics is needed. Pharmacogenomics aims to study the relationship between an individual's genotype and drug response. Scientists use different biological models, ranging from cell lines to mouse models, as proxies for patients for preclinical and translational studies. The rapid development of ""-omics"" technologies is increasing the amount of features that can be measured in these models, expanding the possibilities of finding predictive biomarkers of drug response. Finding these relationships requires diverse computational approaches ranging from machine learning to dynamic modeling. Despite major advances, we are still far from being able to precisely predict drug efficacy in cancer models, let alone directly on patients. We believe that the new experimental techniques and computational approaches covered in this review will bring us closer to this goal."
31723874,2.0,Deep Learning in the Medical Domain: Predicting Cardiac Arrest Using Deep Learning,2018 Aug;33(3):117-120.,"With the wider adoption of electronic health records, the rapid response team initially believed that mortalities could be significantly reduced but due to low accuracy and false alarms, the healthcare system is currently fraught with many challenges. Rule-based methods (e.g., Modified Early Warning Score) and machine learning (e.g., random forest) were proposed as a solution but not effective. In this article, we introduce the DeepEWS (Deep learning based Early Warning Score), which is based on a novel deep learning algorithm. Relative to the standard of care and current solutions in the marketplace, there is high accuracy, and in the clinical setting even when we consider the number of alarms, the accuracy levels are superior."
30060039,42.0,Clinical applications of machine learning in cardiovascular disease and its relevance to cardiac imaging,2019 Jun 21;40(24):1975-1986.,"Artificial intelligence (AI) has transformed key aspects of human life. Machine learning (ML), which is a subset of AI wherein machines autonomously acquire information by extracting patterns from large databases, has been increasingly used within the medical community, and specifically within the domain of cardiovascular diseases. In this review, we present a brief overview of ML methodologies that are used for the construction of inferential and predictive data-driven models. We highlight several domains of ML application such as echocardiography, electrocardiography, and recently developed non-invasive imaging modalities such as coronary artery calcium scoring and coronary computed tomography angiography. We conclude by reviewing the limitations associated with contemporary application of ML algorithms within the cardiovascular disease field."
30058484,1.0,"Aqueous Drug Solubility: What Do We Measure, Calculate and QSPR Predict?",2019;19(5):362-372.,"Detailed critical analysis of publications devoted to QSPR of aqueous solubility is presented in the review with discussion of four types of aqueous solubility (three different thermodynamic solubilities with unknown solute structure, intrinsic solubility, solubility in physiological media at pH=7.4 and kinetic solubility), variety of molecular descriptors (from topological to quantum chemical), traditional statistical and machine learning methods as well as original QSPR models."
30054833,7.0,Artificial intelligence in drug design,2018 Oct;61(10):1191-1204.,"Thanks to the fast improvement of the computing power and the rapid development of the computational chemistry and biology, the computer-aided drug design techniques have been successfully applied in almost every stage of the drug discovery and development pipeline to speed up the process of research and reduce the cost and risk related to preclinical and clinical trials. Owing to the development of machine learning theory and the accumulation of pharmacological data, the artificial intelligence (AI) technology, as a powerful data mining tool, has cut a figure in various fields of the drug design, such as virtual screening, activity scoring, quantitative structure-activity relationship (QSAR) analysis, de novo drug design, and in silico evaluation of absorption, distribution, metabolism, excretion and toxicity (ADME/T) properties. Although it is still challenging to provide a physical explanation of the AI-based models, it indeed has been acting as a great power to help manipulating the drug discovery through the versatile frameworks. Recently, due to the strong generalization ability and powerful feature extraction capability, deep learning methods have been employed in predicting the molecular properties as well as generating the desired molecules, which will further promote the application of AI technologies in the field of drug design."
30053138,3.0,Sequencing era methods for identifying signatures of selection in the genome,2019 Nov 27;20(6):1997-2008.,"Insights into genetic loci which are under selection and their functional roles contribute to increased understanding of the patterns of phenotypic variation we observe today. The availability of whole-genome sequence data, for humans and other species, provides opportunities to investigate adaptation and evolution at unprecedented resolution. Many analytical methods have been developed to interrogate these large data sets and characterize signatures of selection in the genome. We review here recently developed methods and consider the impact of increased computing power and data availability on the detection of selection signatures. Consideration of demography, recombination and other confounding factors is important, and use of a range of methods in combination is a powerful route to resolving different forms of selection in genome sequence data. Overall, a substantial improvement in methods for application to whole-genome sequencing is evident, although further work is required to develop robust and computationally efficient approaches which may increase reproducibility across studies."
30051792,9.0,Applications of Machine Learning Methods in Drug Toxicity Prediction,2018;18(12):987-997.,"Toxicity evaluation is an important part of the preclinical safety assessment of new drugs, which is directly related to human health and the fate of drugs. It is of importance to study how to evaluate drug toxicity accurately and economically. The traditional in vitro and in vivo toxicity tests are laborious, time-consuming, highly expensive, and even involve animal welfare issues. Computational methods developed for drug toxicity prediction can compensate for the shortcomings of traditional methods and have been considered useful in the early stages of drug development. Numerous drug toxicity prediction models have been developed using a variety of computational methods. With the advance of the theory of machine learning and molecular representation, more and more drug toxicity prediction models are developed using a variety of machine learning methods, such as support vector machine, random forest, naive Bayesian, back propagation neural network. And significant advances have been made in many toxicity endpoints, such as carcinogenicity, mutagenicity, and hepatotoxicity. In this review, we aimed to provide a comprehensive overview of the machine learning based drug toxicity prediction studies conducted in recent years. In addition, we compared the performance of the models proposed in these studies in terms of accuracy, sensitivity, and specificity, providing a view of the current state-of-the-art in this field and highlighting the issues in the current studies."
30051410,7.0,"Big-Data Analysis, Cluster Analysis, and Machine-Learning Approaches",2018;1065:607-626.,"Medicine will experience many changes in the coming years because the so-called ""medicine of the future"" will be increasingly proactive, featuring four basic elements: predictive, personalized, preventive, and participatory. Drivers for these changes include the digitization of data in medicine and the availability of computational tools that deal with massive volumes of data. Thus, the need to apply machine-learning methods to medicine has increased dramatically in recent years while facing challenges related to an unprecedented large number of clinically relevant features and highly specific diagnostic tests. Advances regarding data-storage technology and the progress concerning genome studies have enabled collecting vast amounts of patient clinical details, thus permitting the extraction of valuable information. In consequence, big-data analytics is becoming a mandatory technology to be used in the clinical domain.Machine learning and big-data analytics can be used in the field of cardiology, for example, for the prediction of individual risk factors for cardiovascular disease, for clinical decision support, and for practicing precision medicine using genomic information. Several projects employ machine-learning techniques to address the problem of classification and prediction of heart failure (HF) subtypes and unbiased clustering analysis using dense phenomapping to identify phenotypically distinct HF categories. In this chapter, these ideas are further presented, and a computerized model allowing the distinction between two major HF phenotypes on the basis of ventricular-volume data analysis is discussed in detail."
30050769,1.0,"Computer Aided Nodule Analysis and Risk Yield (CANARY) characterization of adenocarcinoma: radiologic biopsy, risk stratification and future directions",2018 Jun;7(3):313-326.,"The majority of incidentally and screen-detected lung cancers are adenocarcinomas. Optimal management of these tumors is clinically challenging due to variability in tumor histopathology and behavior. Invasive adenocarcinoma (IA) is generally aggressive while adenocarcinoma in situ (AIS) and minimally invasive adenocarcinoma (MIA) may be extremely indolent. Computer Aided Nodule Analysis and Risk Yield (CANARY) is a quantitative computed tomography (CT) analysis tool that allows non-invasive assessment of tumor characteristics. This analysis may obviate the need for tissue biopsy and facilitate the risk stratification of adenocarcinoma of the lung. CANARY was developed by unsupervised machine learning techniques using CT data of histopathologically-characterized adenocarcinomas of the lung. This technique identified 9 distinct exemplars that constitute the spectrum of CT features found in adenocarcinoma of the lung. The distributions of these features in a nodule correlate with histopathology. Further automated clustering of CANARY nodules defined three distinct groups that have distinctly different post-resection disease free survival (DFS). CANARY has been validated within the NLST cohort and multiple other cohorts. Using semi-automated segmentation as input to CANARY, there is excellent repeatability and interoperator correlation of results. Confirmation and longitudinal tracking of indolent adenocarcinoma with CANARY may ultimately add decision support in nuanced cases where surgery may not be in the best interest of the patient due to competing comorbidity. Currently under investigation is CANARY's role in detecting differing driver mutations and tumor response to targeted chemotherapeutics. Combining the results from CANARY analysis with clinical information and other quantitative techniques such as analysis of the tumor-free surrounding lung may aid in building more powerful predictive models. The next step in CANARY investigation will be its prospective application, both in selecting low-risk stage 1 adenocarcinoma for active surveillance and investigation in selecting high-risk early stage adenocarcinoma for adjuvant therapy."
30050768,4.0,Lung cancer prediction using machine learning and advanced imaging techniques,2018 Jun;7(3):304-312.,"Machine learning based lung cancer prediction models have been proposed to assist clinicians in managing incidental or screen detected indeterminate pulmonary nodules. Such systems may be able to reduce variability in nodule classification, improve decision making and ultimately reduce the number of benign nodules that are needlessly followed or worked-up. In this article, we provide an overview of the main lung cancer prediction approaches proposed to date and highlight some of their relative strengths and weaknesses. We discuss some of the challenges in the development and validation of such techniques and outline the path to clinical adoption."
30050439,21.0,The Current State and Future of CRISPR-Cas9 gRNA Design Tools,2018 Jul 12;9:749.,"Recent years have seen the development of computational tools to assist researchers in performing CRISPR-Cas9 experiment optimally. More specifically, these tools aim to maximize on-target activity (guide efficiency) while also minimizing potential off-target effects (guide specificity) by analyzing the features of the target site. Nonetheless, currently available tools cannot robustly predict experimental success as prediction accuracy depends on the approximations of the underlying model and how closely the experimental setup matches the data the model was trained on. Here, we present an overview of the available computational tools, their current limitations and future considerations. We discuss new trends around personalized health by taking genomic variants into account when predicting target sites as well as discussing other governing factors that can improve prediction accuracy."
30049875,74.0,Inverse molecular design using machine learning: Generative models for matter engineering,2018 Jul 27;361(6400):360-365.,"The discovery of new materials can bring enormous societal and technological progress. In this context, exploring completely the large space of potential materials is computationally intractable. Here, we review methods for achieving inverse design, which aims to discover tailored materials from the starting point of a particular desired functionality. Recent advances from the rapidly growing field of artificial intelligence, mostly from the subfield of machine learning, have resulted in a fertile exchange of ideas, where approaches to inverse molecular design are being proposed and employed at a rapid pace. Among these, deep generative models have been applied to numerous classes of materials: rational design of prospective drugs, synthetic routes to organic compounds, and optimization of photovoltaics and redox flow batteries, as well as a variety of other solid-state materials."
30049358,12.0,The Emerging Role of Wearable Technologies in Detection of Arrhythmia,2018 Aug;34(8):1083-1087.,"Over the past decade, there has been an explosion of consumer devices for the purposes of health and fitness tracking. The wearable technology market, composed of devices that monitor physiological parameters, such as heart rate and sleep pattern, is anticipated to grow to 929 million connected devices in 2021. These devices encompass wristbands, glasses, in-ear monitors, or electronic shirts, with varying capacity to monitor heart rate, heart rhythm, blood pressure, physical activity, respiratory rate, blood glucose, and sleep patterns. For heart-rate monitoring, most wearable devices use photoplethysmography (PPG) technology, meaning they are inherently less accurate than conventional electrocardiography monitoring techniques (reference standard). However, a growing body of evidence suggests that these technologies can be harnessed to facilitate arrhythmia detection in the appropriate context. Studies evaluating PPG-based wearables in conjunction with machine-learning algorithms have shown promise in detection of such arrhythmias, as atrial fibrillation. Limitations of wearable technologies include their accuracy and accessibility and the clinical implications of wearable-detected arrhythmias. Despite this, wearable technologies represent an important frontier in health evaluation. Future wearables will benefit from improved reliability and accuracy, collect additional health and fitness parameters, support management of chronic disease, and provide real-time connectivity and feedback that may supplant conventional medical monitoring. Wearables have the potential to become truly disruptive in our health care sector, with large segments of the population soon to have readily available health data that the physician must interpret."
30049182,12.0,Diagnostic Accuracy of Different Machine Learning Algorithms for Breast Cancer Risk Calculation: a Meta-Analysis,2018 Jul 27;19(7):1747-1752.,"Objective: The aim of this study was to determine the diagnostic accuracy of different machine learning algorithms for breast cancer risk calculation. Methods: A meta-analysis was conducted of published research articles on diagnostic test accuracy of different machine learning algorithms for breast cancer risk calculation published between January 2000 and May 2018 in the online article databases of PubMed, ProQuest and EBSCO. Paired forest plots were employed for the analysis. Numerical values for sensitivity and specificity were obtained from false negative (FN), false positive (FP), true negative (TN) and true positive (TP) rates, presented alongside graphical representations with boxes marking the values and horizontal lines showing the confidence intervals (CIs). Summary receiver operating characteristic (SROC) curves were applied to assess the performance of diagnostic tests. Data were processed using Review Manager 5.3 (RevMan 5.3). Results: A total of 1,879 articles were reviewed, of which 11 were selected for systematic review and meta-analysis. Fve algorithms for machine learning able to predict breast cancer risk were identified: Super Vector Machine (SVM); Artificial Neural Networks (ANN); Decision Tree (DT); Naive Bayes (NB); and K-Nearest Neighbor (KNN). With the SVM, the Area Under Curve (AUC) from the SROC was > 90%, therefore classified into the excellent category. Conclusion: The meta-analysis confirmed that the SVM algorithm is able to calculate breast cancer risk with better accuracy value than other machine learning algorithms."
30048078,,Artificial Intelligence and Machine Learning to Accelerate Translational Research: Proceedings of a Workshop—in Brief,,"The big data revolution, accompanied by the development and deployment of wearable medical devices and mobile health applications, has enabled the biomedical community to apply artificial intelligence (AI) and machine learning algorithms to vast amounts of data. This shift has created new research opportunities in predictive analytics, precision medicine, virtual diagnosis, patient monitoring, and drug discovery and delivery, which has garnered the interests of government, academic, and industry researchers alike and is already putting new tools in the hands of practitioners.                This boom in digital health opportunities has also raised numerous questions concerning the future of biomedical research and healthcare practices. How reliable are deployed AI-driven diagnostic tools, and what is the impact of these tools on doctors and patients? How vulnerable are algorithms to bias and unfairness? How can research improve the process of detecting unfairness in machine learning algorithms? How are other fields simultaneously advancing AI applications? How will academia prepare scientists with the skills to meet the demands of the newly transformed industry? Informed answers to these and other questions require interdisciplinary discussion and collaboration. On February 13 and 14, 2018, the National Academies of Sciences, Engineering, and Medicine convened a workshop to explore these and other questions related to the emerging use of AI and machine learning technologies in translational research. This publication summarizes the presentations and discussions from the workshop."
30048614,25.0,"Representation, Pattern Information, and Brain Signatures: From Neurons to Neuroimaging",2018 Jul 25;99(2):257-273.,"Human neuroimaging research has transitioned from mapping local effects to developing predictive models of mental events that integrate information distributed across multiple brain systems. Here we review work demonstrating how multivariate predictive models have been utilized to provide quantitative, falsifiable predictions; establish mappings between brain and mind with larger effects than traditional approaches; and help explain how the brain represents mental constructs and processes. Although there is increasing progress toward the first two of these goals, models are only beginning to address the latter objective. By explicitly identifying gaps in knowledge, research programs can move deliberately and programmatically toward the goal of identifying brain representations underlying mental states and processes."
30046072,114.0,Machine learning for molecular and materials science,2018 Jul;559(7715):547-555.,"Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence."
30044996,2.0,Machine learning detects EEG microstate alterations in patients living with temporal lobe epilepsy,2018 Oct;61:8-13.,"Purpose:                    Quasi-stable electrical distribution in EEG called microstates could carry useful information on the dynamics of large scale brain networks. Using machine learning techniques we explored if abnormalities in microstates can identify patients with Temporal Lobe Epilepsy (TLE) in the absence of an interictal discharge (IED).              Method:                    4 Classes of microstates were computed from 2 min artefact free EEG epochs in 42 subjects (21 TLE and 21 controls). The percentage of time coverage, frequency of occurrence and duration for each of these microstates were computed and redundancy reduced using feature selection methods. Subsequently, Fishers Linear Discriminant Analysis (FLDA) and logistic regression were used for classification.              Result:                    FLDA distinguished TLE with 76.1% accuracy (85.0% sensitivity, 66.6% specificity) considering frequency of occurrence and percentage of time coverage of microstate C as features.              Conclusion:                    Microstate alterations are present in patients with TLE. This feature might be useful in the diagnosis of epilepsy even in the absence of an IED."
30044963,4.0,Machine learning approaches infer vitamin D signaling: Critical impact of vitamin D receptor binding within topologically associated domains,2019 Jan;185:103-109.,"The vitamin D-modulated transcriptome of highly responsive human cells, such as THP-1 monocytes, comprises more than 500 genes, half of which are primary targets. Recently, we proposed a chromatin model of vitamin D signaling demonstrating that nearly all vitamin D target genes are located within vitamin D-modulated topologically associated domains (TADs). This model is based on genome-wide binding patterns of the vitamin D receptor (VDR), the pioneer transcription factor PU.1, the chromatin organizer CTCF and histone markers of active promoter regions (H3K4me3) and active chromatin (H3K27ac). In addition, time-dependent data on accessible chromatin and mRNA expression are implemented. For the interrogation and in deep inspection of these high-dimensional datasets unsupervised and supervised machine learning algorithms were applied. Unsupervised methods, such as the vector quantization tool K-means and the dimensionality reduction algorithm self-organizing map, generated descriptions of how attributes, such as VDR binding and chromatin accessibility, affect each other as a function of time and/or co-localization within the same genomic region. Supervised algorithms, such as random forests, allowed the data to be classified into pre-existing categories like persistent (i.e. constant) and time-dependent (i.e. transient) VDR binding sites. The relative amounts of these VDR categories in TADs showed to be the main discriminator for sorting the latter into five classes carrying vitamin D target genes involved in distinct biological processes. In conclusion, via the application of machine learning methods we identified the spatio-temporal VDR binding pattern in TADs as the most critical attribute for specific regulation of vitamin D target genes and the segregation of vitamin D's physiologic function."
30041284,13.0,Understanding Neurogastroenterology From Neuroimaging Perspective: A Comprehensive Review of Functional and Structural Brain Imaging in Functional Gastrointestinal Disorders,2018 Oct 1;24(4):512-527.,"This review provides a comprehensive overview of brain imaging studies of the brain-gut interaction in functional gastrointestinal disorders (FGIDs). Functional neuroimaging studies during gut stimulation have shown enhanced brain responses in regions related to sensory processing of the homeostatic condition of the gut (homeostatic afferent) and responses to salience stimuli (salience network), as well as increased and decreased brain activity in the emotional response areas and reduced activation in areas associated with the top-down modulation of visceral afferent signals. Altered central regulation of the endocrine and autonomic nervous responses, the key mediators of the brain-gut axis, has been demonstrated. Studies using resting-state functional magnetic resonance imaging reported abnormal local and global connectivity in the areas related to pain processing and the default mode network (a physiological baseline of brain activity at rest associated with self-awareness and memory) in FGIDs. Structural imaging with brain morphometry and diffusion imaging demonstrated altered gray- and white-matter structures in areas that also showed changes in functional imaging studies, although this requires replication. Molecular imaging by magnetic resonance spectroscopy and positron emission tomography in FGIDs remains relatively sparse. Progress using analytical methods such as machine learning algorithms may shift neuroimaging studies from brain mapping to predicting clinical outcomes. Because several factors contribute to the pathophysiology of FGIDs and because its population is quite heterogeneous, a new model is needed in future studies to assess the importance of the factors and brain functions that are responsible for an optimal homeostatic state."
30034462,,Mining Big Neuron Morphological Data,2018 Jun 24;2018:8234734.,"The advent of automatic tracing and reconstruction technology has led to a surge in the number of neurons 3D reconstruction data and consequently the neuromorphology research. However, the lack of machine-driven annotation schema to automatically detect the types of the neurons based on their morphology still hinders the development of this branch of science. Neuromorphology is important because of the interplay between the shape and functionality of neurons and the far-reaching impact on the diagnostics and therapeutics in neurological disorders. This survey paper provides a comprehensive research in the field of automatic neurons classification and presents the existing challenges, methods, tools, and future directions for automatic neuromorphology analytics. We summarize the major automatic techniques applicable in the field and propose a systematic data processing pipeline for automatic neuron classification, covering data capturing, preprocessing, analyzing, classification, and retrieval. Various techniques and algorithms in machine learning are illustrated and compared to the same dataset to facilitate ongoing research in the field."
30017512,15.0,A review of statistical and machine learning methods for modeling cancer risk using structured clinical data,2018 Aug;90:1-14.,"Advancements are constantly being made in oncology, improving prevention and treatment of cancers. To help reduce the impact and deadliness of cancers, they must be detected early. Additionally, there is a risk of cancers recurring after potentially curative treatments are performed. Predictive models can be built using historical patient data to model the characteristics of patients that developed cancer or relapsed. These models can then be deployed into clinical settings to determine if new patients are at high risk for cancer development or recurrence. For large-scale predictive models to be built, structured data must be captured for a wide range of diverse patients. This paper explores current methods for building cancer risk models using structured clinical patient data. Trends in statistical and machine learning techniques are explored, and gaps are identified for future research. The field of cancer risk prediction is a high-impact one, and research must continue for these models to be embraced for clinical decision support of both practitioners and patients."
30013818,1.0,Development of a Behavioral Health Stigma Measure and Application of Machine Learning for Classification,2018 Jun 1;15(5-6):34-42.,"Objective: Given the growing public health importance of measuring the change in mental health stigma over time, the goal of this study was to demonstrate the potential for using machine learning as a tool to analyze patterns of social stigma as a complement to traditional research methods. Methods: A total of 1,904 participants were recruited through Sona Systems, Ltd (Tallinn, Estonia), an experiment management system for online research, to complete a self-reported survey. The collected data were used to develop a new measure of mental (behavioral) health stigma. To build a classification predictive model of stigma, a decision tree was used as the data mining tool, wherein a set of classification rules was generated and tested for its ability to examine the prevalence of stigma. Results: A three-factor stigma model was supported and confirmed. Results indicate that the measure is content-valid and internally consistent. Performance evaluation of the machine learning-based classification algorithm revealed a sufficient inter-rater reliability with a predictive accuracy of 92.4 percent. Conclusion: This study illustrates the potential for applying machine learning to derive a data-driven understanding of the extent to which stigma is prevalent in society. It establishes a framework for the development of an index to track stigma over time and to assist healthcare decision-makers with improving the health of populations and the experience of care for patients."
30013400,7.0,Automated data-adaptive analytics for electronic healthcare data to study causal treatment effects,2018 Jul 6;10:771-788.,"Background:                    Decision makers in health care increasingly rely on nonrandomized database analyses to assess the effectiveness, safety, and value of medical products. Health care data scientists use data-adaptive approaches that automatically optimize confounding control to study causal treatment effects. This article summarizes relevant experiences and extensions.              Methods:                    The literature was reviewed on the uses of high-dimensional propensity score (HDPS) and related approaches for health care database analyses, including methodological articles on their performance and improvement. Articles were grouped into applications, comparative performance studies, and statistical simulation experiments.              Results:                    The HDPS algorithm has been referenced frequently with a variety of clinical applications and data sources from around the world. The appeal of HDPS for database research rests in 1) its superior performance in situations of unobserved confounding through proxy adjustment, 2) its predictable efficiency in extracting confounding information from a given data source, 3) its ability to automate estimation of causal treatment effects to the extent achievable in a given data source, and 4) its independence of data source and coding system. Extensions of the HDPS approach have focused on improving variable selection when exposure is sparse, using free text information and time-varying confounding adjustment.              Conclusion:                    Semiautomated and optimized confounding adjustment in health care database analyses has proven successful across a wide range of settings. Machine-learning extensions further automate its use in estimating causal treatment effects across a range of data scenarios."
30013304,8.0,A Functional Spatial Analysis Platform for Discovery of Immunological Interactions Predictive of Low-Grade to High-Grade Transition of Pancreatic Intraductal Papillary Mucinous Neoplasms,2018 Jun 28;17:1176935118782880.,"Intraductal papillary mucinous neoplasms (IPMNs), critical precursors of the devastating tumor pancreatic ductal adenocarcinoma (PDAC), are poorly understood in the pancreatic cancer community. Researchers have shown that IPMN patients with high-grade dysplasia have a greater risk of subsequent development of PDAC in the remnant pancreas than do patients with low-grade dysplasia. In this study, we built a computational prediction model that encapsulates the spatial cellular interactions in IPMNs that play key roles in the transformation of low-grade IPMN cysts to high-grade cysts en route to PDAC. Using multiplex immunofluorescent images of IPMN cysts, we adopted algorithms from spatial statistics and functional data analysis to create metrics that summarize the spatial interactions in IPMNs. We showed that an ensemble of models learned using these spatial metrics can robustly predict, with high accuracy, (1) the dysplasia grade (low vs high grade) and (2) the risk of a low-grade cyst progressing to a high-grade cyst. We obtained high classification accuracies on both tasks, with areas under the curve of 0.81 (95% confidence interval: 0.71-0.9) for task 1 and 0.81 (95% confidence interval: 0.7-0.94) for task 2. To the best of our knowledge, this is the first application of an ensemble machine learning approach for discovering critical cellular spatial interactions in IPMNs using imaging data. We envision that our work can be used as a risk assessment tool for patients diagnosed with IPMNs and facilitate greater understanding and investigation of the cellular interactions that cause transition of IPMNs to PDAC."
30011882,20.0,Computational Methodologies in the Exploration of Marine Natural Product Leads,2018 Jul 13;16(7):236.,"Computational methodologies are assisting the exploration of marine natural products (MNPs) to make the discovery of new leads more efficient, to repurpose known MNPs, to target new metabolites on the basis of genome analysis, to reveal mechanisms of action, and to optimize leads. In silico efforts in drug discovery of NPs have mainly focused on two tasks: dereplication and prediction of bioactivities. The exploration of new chemical spaces and the application of predicted spectral data must be included in new approaches to select species, extracts, and growth conditions with maximum probabilities of medicinal chemistry novelty. In this review, the most relevant current computational dereplication methodologies are highlighted. Structure-based (SB) and ligand-based (LB) chemoinformatics approaches have become essential tools for the virtual screening of NPs either in small datasets of isolated compounds or in large-scale databases. The most common LB techniques include Quantitative Structure⁻Activity Relationships (QSAR), estimation of drug likeness, prediction of adsorption, distribution, metabolism, excretion, and toxicity (ADMET) properties, similarity searching, and pharmacophore identification. Analogously, molecular dynamics, docking and binding cavity analysis have been used in SB approaches. Their significance and achievements are the main focus of this review."
30008798,2.0,Information-Based Medicine in Glioma Patients: A Clinical Perspective,2018 Jun 13;2018:8572058.,"Glioma constitutes the most common type of primary brain tumor with a dismal survival, often measured in terms of months or years. The thin line between treatment effectiveness and patient harm underpins the importance of tailoring clinical management to the individual patient. Randomized trials have laid the foundation for many neuro-oncological guidelines. Despite this, their findings focus on group-level estimates. Given our current tools, we are limited in our ability to guide patients on what therapy is best for them as individuals, or even how long they should expect to survive. Machine learning, however, promises to provide the analytical support for personalizing treatment decisions, and deep learning allows clinicians to unlock insight from the vast amount of unstructured data that is collected on glioma patients. Although these novel techniques have achieved astonishing results across a variety of clinical applications, significant hurdles remain associated with the implementation of them in clinical practice. Future challenges include the assembly of well-curated cross-institutional datasets, improvement of the interpretability of machine learning models, and balancing novel evidence-based decision-making with the associated liability of automated inference. Although artificial intelligence already exceeds clinical expertise in a variety of applications, clinicians remain responsible for interpreting the implications of, and acting upon, each prediction."
29998104,14.0,Machine Learning in Orthopedics: A Literature Review,2018 Jun 27;6:75.,"In this paper we present the findings of a systematic literature review covering the articles published in the last two decades in which the authors described the application of a machine learning technique and method to an orthopedic problem or purpose. By searching both in the Scopus and Medline databases, we retrieved, screened and analyzed the content of 70 journal articles, and coded these resources following an iterative method within a Grounded Theory approach. We report the survey findings by outlining the articles' content in terms of the main machine learning techniques mentioned therein, the orthopedic application domains, the source data and the quality of their predictive performance."
29996520,2.0,A Meta-Prediction of Methylenetetrahydrofolate-Reductase Polymorphisms and Air Pollution Increased the Risk of Ischemic Heart Diseases Worldwide,2018 Jul 10;15(7):1453.,"Ischemic heart disease (IHD) is among the leading causes of death worldwide. Methylenetetrahydrofolate reductase (MTHFR) polymorphisms have been associated with IHD risk, but the findings presented with heterogeneity. The purpose of the present meta-analysis was to provide an updated evaluation by integrating machine-learning based analytics to examine the potential source of heterogeneity on the associations between MTHFR polymorphisms and the risk of various subtypes of IHD, as well as the possible impact of air pollution on MTHFR polymorphisms and IHD risks. A comprehensive search of various databases was conducted to locate 123 studies (29,697 cases and 31,028 controls) for MTHFR C677T, and 18 studies (7158 cases and 5482 controls) for MTHFR A1298C. Overall, MTHFR 677 polymorphisms were risks for IHD (TT: Risk ratio (RR) = 1.23, p < 0.0001; CT: RR = 1.04, p = 0.0028, and TT plus CT: RR = 1.09, p < 0.0001). In contrast, MTHFR 677 CC wildtype was protective against IHD (RR = 0.91, p < 0.00001) for overall populations. Three countries with elevated IHD risks from MTHFR C677T polymorphism with RR >2 included India, Turkey, and Tunisia. Meta-predictive analysis revealed that increased air pollution was associated with increased MTHFR 677 TT and CT polymorphisms in both the case and control group (p < 0.05), with the trend of increased IHD risk resulting from increased air pollution. These results associate the potential inflammatory pathway with air pollution and the folate pathway with MTHFR polymorphism. Future intervention studies can be designed to mitigate MTHFR enzyme deficiencies resulting from gene polymorphisms to prevent IHDs for at-risk populations."
29994590,9.0,A Review of Signal Processing Techniques for Electrocardiogram Signal Quality Assessment,2018;11:36-52.,"Electrocardiogram (ECG) signal quality assessment (SQA) plays a vital role in significantly improving the diagnostic accuracy and reliability of unsupervised ECG analysis systems. In practice, the ECG signal is often corrupted with different kinds of noises and artifacts. Therefore, numerous SQA methods were presented based on the ECG signal and/or noise features and the machine learning classifiers and/or heuristic decision rules. This paper presents an overview of current state-of-the-art SQA methods and highlights the practical limitations of the existing SQA methods. Based upon past and our studies, it is noticed that a lightweight ECG noise analysis framework is highly demanded for real-time detection, localization, and classification of single and combined ECG noises within the context of wearable ECG monitoring devices which are often resource constrained."
29994486,18.0,Natural Language Processing for EHR-Based Computational Phenotyping,Jan-Feb 2019;16(1):139-153.,"This article reviews recent advances in applying natural language processing (NLP) to Electronic Health Records (EHRs) for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction (DDI), and adverse drug event (ADE) detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often, better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives."
29992885,6.0,Toward Reproducible Results from Targeted Metabolomic Studies: Perspectives for Data Pre-processing and a Basis for Analytic Pipeline Development,2018;18(11):883-895.,"Contemporary metabolomics experiments generate a rich array of complex high-dimensional data. Consequently, there have been concurrent efforts to develop methodological standards and analytical workflows to streamline the generation of meaningful biochemical and clinical inferences from raw data generated using an analytical platform like mass spectrometry. While such considerations have been frequently addressed in untargeted metabolomics (i.e., the broad survey of all distinguishable metabolites within a sample of interest), this methodological scrutiny has seldom been applied to data generated using commercial, targeted metabolomics kits. We suggest that this may, in part, account for past and more recent incomplete replications of previously specified biomarker panels. Herein, we identify common impediments challenging the analysis of raw, targeted metabolomic abundance data from a commercial kit and review methods to remedy these issues. In doing so, we propose an analytical pipeline suitable for the pre-processing of data for downstream biomarker discovery. Operational and statistical considerations for integrating targeted data sets across experimental sites and analytical batches are discussed, as are best practices for developing predictive models relating pre-processed metabolomic data to associated phenotypic information."
