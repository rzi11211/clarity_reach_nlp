pmid,title,date,text,citations
33202280,Partners for life: building microbial consortia for the future,2020 Dec;66:292-300.,"New technologies have allowed researchers to better design, build, and analyze complex consortia. These developments are fueling a wider implementation of consortium-based bioprocessing by leveraging synthetic biology, delivering on the field's multitudinous promises of higher efficiencies, superior resiliency, augmented capabilities, and modular bioprocessing. Here we chronicle current progress by presenting a range of screening, computational, and biomolecular tools enabling robust population control, efficient division of labor, and programmatic spatial organization; furthermore, we detail corresponding advancements in areas including machine learning, biocontainment, and standardization. Additionally, we show applications in myriad sectors, including medicine, energy and waste sustainability, chemical production, agriculture, and biosensors. Concluding remarks outline areas of growth that will promote the utilization of complex community structures across the biotechnology spectrum.",
33200995,Comparison of Multivariable Logistic Regression and Other Machine Learning Algorithms for Prognostic Prediction Studies in Pregnancy Care: Systematic Review and Meta-Analysis,2020 Nov 17;8(11):e16503.,"Background:                    Predictions in pregnancy care are complex because of interactions among multiple factors. Hence, pregnancy outcomes are not easily predicted by a single predictor using only one algorithm or modeling method.              Objective:                    This study aims to review and compare the predictive performances between logistic regression (LR) and other machine learning algorithms for developing or validating a multivariable prognostic prediction model for pregnancy care to inform clinicians' decision making.              Methods:                    Research articles from MEDLINE, Scopus, Web of Science, and Google Scholar were reviewed following several guidelines for a prognostic prediction study, including a risk of bias (ROB) assessment. We report the results based on the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. Studies were primarily framed as PICOTS (population, index, comparator, outcomes, timing, and setting): Population: men or women in procreative management, pregnant women, and fetuses or newborns; Index: multivariable prognostic prediction models using non-LR algorithms for risk classification to inform clinicians' decision making; Comparator: the models applying an LR; Outcomes: pregnancy-related outcomes of procreation or pregnancy outcomes for pregnant women and fetuses or newborns; Timing: pre-, inter-, and peripregnancy periods (predictors), at the pregnancy, delivery, and either puerperal or neonatal period (outcome), and either short- or long-term prognoses (time interval); and Setting: primary care or hospital. The results were synthesized by reporting study characteristics and ROBs and by random effects modeling of the difference of the logit area under the receiver operating characteristic curve of each non-LR model compared with the LR model for the same pregnancy outcomes. We also reported between-study heterogeneity by using τ2 and I2.              Results:                    Of the 2093 records, we included 142 studies for the systematic review and 62 studies for a meta-analysis. Most prediction models used LR (92/142, 64.8%) and artificial neural networks (20/142, 14.1%) among non-LR algorithms. Only 16.9% (24/142) of studies had a low ROB. A total of 2 non-LR algorithms from low ROB studies significantly outperformed LR. The first algorithm was a random forest for preterm delivery (logit AUROC 2.51, 95% CI 1.49-3.53; I2=86%; τ2=0.77) and pre-eclampsia (logit AUROC 1.2, 95% CI 0.72-1.67; I2=75%; τ2=0.09). The second algorithm was gradient boosting for cesarean section (logit AUROC 2.26, 95% CI 1.39-3.13; I2=75%; τ2=0.43) and gestational diabetes (logit AUROC 1.03, 95% CI 0.69-1.37; I2=83%; τ2=0.07).              Conclusions:                    Prediction models with the best performances across studies were not necessarily those that used LR but also used random forest and gradient boosting that also performed well. We recommend a reanalysis of existing LR models for several pregnancy outcomes by comparing them with those algorithms that apply standard guidelines.              Trial registration:                    PROSPERO (International Prospective Register of Systematic Reviews) CRD42019136106; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=136106.",1.0
33199854,"If deep learning is the answer, what is the question?",2021 Jan;22(1):55-67.,"Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.",1.0
33198233,Machine Learning Methods in Drug Discovery,2020 Nov 12;25(22):5277.,"The advancements of information technology and related processing techniques have created a fertile base for progress in many scientific fields and industries. In the fields of drug discovery and development, machine learning techniques have been used for the development of novel drug candidates. The methods for designing drug targets and novel drug discovery now routinely combine machine learning and deep learning algorithms to enhance the efficiency, efficacy, and quality of developed outputs. The generation and incorporation of big data, through technologies such as high-throughput screening and high through-put computational analysis of databases used for both lead and target discovery, has increased the reliability of the machine learning and deep learning incorporated techniques. The use of these virtual screening and encompassing online information has also been highlighted in developing lead synthesis pathways. In this review, machine learning and deep learning algorithms utilized in drug discovery and associated techniques will be discussed. The applications that produce promising results and methods will be reviewed.",
33196915,Review of computational neuroaesthetics: bridging the gap between neuroaesthetics and computer science,2020 Nov 16;7(1):16.,"The mystery of aesthetics attracts scientists from various research fields. The topic of aesthetics, in combination with other disciplines such as neuroscience and computer science, has brought out the burgeoning fields of neuroaesthetics and computational aesthetics within less than two decades. Despite profound findings are carried out by experimental approaches in neuroaesthetics and by machine learning algorithms in computational neuroaesthetics, these two fields cannot be easily combined to benefit from each other and findings from each field are isolated. Computational neuroaesthetics, which inherits computational approaches from computational aesthetics and experimental approaches from neuroaesthetics, seems to be promising to bridge the gap between neuroaesthetics and computational aesthetics. Here, we review theoretical models and neuroimaging findings about brain activity in neuroaesthetics. Then machine learning algorithms and computational models in computational aesthetics are enumerated. Finally, we introduce studies in computational neuroaesthetics which combine computational models with neuroimaging data to analyze brain connectivity during aesthetic appreciation or give a prediction on aesthetic preference. This paper outlines the rich potential for computational neuroaesthetics to take advantages from both neuroaesthetics and computational aesthetics. We conclude by discussing some of the challenges and potential prospects in computational neuroaesthetics, and highlight issues for future consideration.",
33194711,A Review on Application of Deep Learning Algorithms in External Beam Radiotherapy Automated Treatment Planning,2020 Oct 23;10:580919.,"Treatment planning plays an important role in the process of radiotherapy (RT). The quality of the treatment plan directly and significantly affects patient treatment outcomes. In the past decades, technological advances in computer and software have promoted the development of RT treatment planning systems with sophisticated dose calculation and optimization algorithms. Treatment planners now have greater flexibility in designing highly complex RT treatment plans in order to mitigate the damage to healthy tissues better while maximizing radiation dose to tumor targets. Nevertheless, treatment planning is still largely a time-inefficient and labor-intensive process in current clinical practice. Artificial intelligence, including machine learning (ML) and deep learning (DL), has been recently used to automate RT treatment planning and has gained enormous attention in the RT community due to its great promises in improving treatment planning quality and efficiency. In this article, we reviewed the historical advancement, strengths, and weaknesses of various DL-based automated RT treatment planning techniques. We have also discussed the challenges, issues, and potential research directions of DL-based automated RT treatment planning techniques.",
33193667,A Review of Integrative Imputation for Multi-Omics Datasets,2020 Oct 15;11:570255.,"Multi-omics studies, which explore the interactions between multiple types of biological factors, have significant advantages over single-omics analysis for their ability to provide a more holistic view of biological processes, uncover the causal and functional mechanisms for complex diseases, and facilitate new discoveries in precision medicine. However, omics datasets often contain missing values, and in multi-omics study designs it is common for individuals to be represented for some omics layers but not all. Since most statistical analyses cannot be applied directly to the incomplete datasets, imputation is typically performed to infer the missing values. Integrative imputation techniques which make use of the correlations and shared information among multi-omics datasets are expected to outperform approaches that rely on single-omics information alone, resulting in more accurate results for the subsequent downstream analyses. In this review, we provide an overview of the currently available imputation methods for handling missing values in bioinformatics data with an emphasis on multi-omics imputation. In addition, we also provide a perspective on how deep learning methods might be developed for the integrative imputation of multi-omics datasets.",2.0
33193532,Modern Strategies to Assess and Breed Forest Tree Adaptation to Changing Climate,2020 Oct 21;11:583323.,"Studying the genetics of adaptation to new environments in ecologically and industrially important tree species is currently a major research line in the fields of plant science and genetic improvement for tolerance to abiotic stress. Specifically, exploring the genomic basis of local adaptation is imperative for assessing the conditions under which trees will successfully adapt in situ to global climate change. However, this knowledge has scarcely been used in conservation and forest tree improvement because woody perennials face major research limitations such as their outcrossing reproductive systems, long juvenile phase, and huge genome sizes. Therefore, in this review we discuss predictive genomic approaches that promise increasing adaptive selection accuracy and shortening generation intervals. They may also assist the detection of novel allelic variants from tree germplasm, and disclose the genomic potential of adaptation to different environments. For instance, natural populations of tree species invite using tools from the population genomics field to study the signatures of local adaptation. Conventional genetic markers and whole genome sequencing both help identifying genes and markers that diverge between local populations more than expected under neutrality, and that exhibit unique signatures of diversity indicative of ""selective sweeps."" Ultimately, these efforts inform the conservation and breeding status capable of pivoting forest health, ecosystem services, and sustainable production. Key long-term perspectives include understanding how trees' phylogeographic history may affect the adaptive relevant genetic variation available for adaptation to environmental change. Encouraging ""big data"" approaches (machine learning-ML) capable of comprehensively merging heterogeneous genomic and ecological datasets is becoming imperative, too.",3.0
33192663,Illuminating the Black Box: Interpreting Deep Neural Network Models for Psychiatric Research,2020 Oct 29;11:551299.,"Psychiatric research is often confronted with complex abstractions and dynamics that are not readily accessible or well-defined to our perception and measurements, making data-driven methods an appealing approach. Deep neural networks (DNNs) are capable of automatically learning abstractions in the data that can be entirely novel and have demonstrated superior performance over classical machine learning models across a range of tasks and, therefore, serve as a promising tool for making new discoveries in psychiatry. A key concern for the wider application of DNNs is their reputation as a ""black box"" approach-i.e., they are said to lack transparency or interpretability of how input data are transformed to model outputs. In fact, several existing and emerging tools are providing improvements in interpretability. However, most reviews of interpretability for DNNs focus on theoretical and/or engineering perspectives. This article reviews approaches to DNN interpretability issues that may be relevant to their application in psychiatric research and practice. It describes a framework for understanding these methods, reviews the conceptual basis of specific methods and their potential limitations, and discusses prospects for their implementation and future directions.",
33192435,Neurorobots as a Means Toward Neuroethology and Explainable AI,2020 Oct 19;14:570308.,"Understanding why deep neural networks and machine learning algorithms act as they do is a difficult endeavor. Neuroscientists are faced with similar problems. One way biologists address this issue is by closely observing behavior while recording neurons or manipulating brain circuits. This has been called neuroethology. In a similar way, neurorobotics can be used to explain how neural network activity leads to behavior. In real world settings, neurorobots have been shown to perform behaviors analogous to animals. Moreover, a neuroroboticist has total control over the network, and by analyzing different neural groups or studying the effect of network perturbations (e.g., simulated lesions), they may be able to explain how the robot's behavior arises from artificial brain activity. In this paper, we review neurorobot experiments by focusing on how the robot's behavior leads to a qualitative and quantitative explanation of neural activity, and vice versa, that is, how neural activity leads to behavior. We suggest that using neurorobots as a form of computational neuroethology can be a powerful methodology for understanding neuroscience, as well as for artificial intelligence and machine learning.",
33189866,Endothelial cell senescence: A machine learning-based meta-analysis of transcriptomic studies,2021 Jan;65:101213.,"Numerous systemic vascular dysfunction that leads to age-related diseases is highly associated with endothelial cell (EC) senescence; thus, identifying consensus features of EC senescence is crucial in understanding the mechanisms and identifying potential therapeutic targets. Here, by utilizing a total of 8 screened studies from different origins of ECs, we have successfully obtained common features in both gene and pathway level via sophisticated machine learning algorithms. A total of 400 differentially expressed genes (DEGs) were newly discovered with meta-analysis when compared to the usage of individual studies. The generated parsimonious model established 36 genes and 57 pathways features with non-zero coefficient, suggesting remarkable association of phosphoglycerate dehydrogenase and serine biosynthesis pathway with endothelial cellular senescence. For the cross-validation process to measure model performance of 36 deduced features, leave-one-study-out cross-validation (LOSOCV) was employed, resulting in an overall area under the receiver operating characteristic (AUROC) of 0.983 (95 % CI, 0.952, 1.000) showing excellent discriminative performance. Moreover, pathway-level analysis was performed by Pathifier algorithm, obtaining a total of 698 pathway deregulation scores from the 10,416 merged genes. In this process, high dimensional data was eventually narrowed down to 57 core pathways with AUROC value of 0.982 (95 % CI, 0.945, 1.000). The robust model with high performance underscores the merit of utilizing sophisticated meta-analysis in finding consensus features of endothelial cell senescence, which may lead to the development of therapeutic targets and advanced understanding of vascular dysfunction pathogenesis with further elucidation.",
33188785,The use of Motor and Cognitive Dual-Task quantitative assessment on subjects with mild cognitive impairment: A systematic review,2021 Jan;193:111393.,"Dementia and Alzheimer's Disease (AD) represent a health emergency. The identification of valid and noninvasive markers to identify people with Mild Cognitive Impairment (MCI) is profoundly advocated. This review outlines the use of quantitative Motor and Cognitive Dual-Task (MCDT) on MCI, by technologies aid. We describe the framework and the most valuable researches, displaying the adopted protocols, and the available technologies. PubMed Central, Web of Science, and Scopus were inspected between January 2010 and May 2020. 1939 articles were found in the initial quest. Exclusion criteria allowed the selection of the most relevant papers; 38 papers were included. The articles, regarding four technological solutions ""wearable sensors"", ""personal devices"", ""optokinetic systems"", and ""electronic walkways"", are organized into three categories: ""Quantitative MCDT"", ""MCDT Inspired by Neuropsychological Test"", and ""MCDT for MCI Stimulation"". MCDT might furnish clinical landmarks, supplying aid for disease stratication, risk prediction, and intervention optimization. Such protocols could foster the use of data mining and machine learning techniques. Notwithstanding, there is still a need to standardize and harmonize such protocols.",1.0
33186867,"Description, prediction and causation: Methodological challenges of studying child and adolescent development",2020 Dec;46:100867.,"Scientific research can be categorized into: a) descriptive research, with the main goal to summarize characteristics of a group (or person); b) predictive research, with the main goal to forecast future outcomes that can be used for screening, selection, or monitoring; and c) explanatory research, with the main goal to understand the underlying causal mechanism, which can then be used to develop interventions. Since each goal requires different research methods in terms of design, operationalization, model building and evaluation, it should form an important basis for decisions on how to set up and execute a study. To determine the extent to which developmental research is motivated by each goal and how this aligns with the research designs that are used, we evaluated 100 publications from the Consortium on Individual Development (CID). This analysis shows that the match between research goal and research design is not always optimal. We discuss alternative techniques, which are not yet part of the developmental scientist's standard toolbox, but that may help bridge some of the lurking gaps that developmental scientists encounter between their research design and their research goal. These include unsupervised and supervised machine learning, directed acyclical graphs, Mendelian randomization, and target trials.",2.0
33185950,Analytics with artificial intelligence to advance the treatment of acute respiratory distress syndrome,2020 Nov;13(4):301-312.,"Artificial intelligence (AI) has found its way into clinical studies in the era of big data. Acute respiratory distress syndrome (ARDS) or acute lung injury (ALI) is a clinical syndrome that encompasses a heterogeneous population. Management of such heterogeneous patient population is a big challenge for clinicians. With accumulating ALI datasets being publicly available, more knowledge could be discovered with sophisticated analytics. We reviewed literatures with big data analytics to understand the role of AI for improving the caring of patients with ALI/ARDS. Many studies have utilized the electronic medical records (EMR) data for the identification and prognostication of ARDS patients. As increasing number of ARDS clinical trials data is open to public, secondary analysis on these combined datasets provide a powerful way of finding solution to clinical questions with a new perspective. AI techniques such as Classification and Regression Tree (CART) and artificial neural networks (ANN) have also been successfully used in the investigation of ARDS problems. Individualized treatment of ARDS could be implemented with a support from AI as we are now able to classify ARDS into many subphenotypes by unsupervised machine learning algorithms. Interestingly, these subphenotypes show different responses to a certain intervention. However, current analytics involving ARDS have not fully incorporated information from omics such as transcriptome, proteomics, daily activities and environmental conditions. AI technology is assisting us to interpret complex data of ARDS patients and enable us to further improve the management of ARDS patients in future with individual treatment plans.",4.0
33184585,Cardiac Magnetic Resonance in Pulmonary Hypertension-an Update,2020;13(12):30.,"Purpose of review:                    This article reviews advances over the past 3 years in cardiac magnetic resonance (CMR) imaging in pulmonary hypertension (PH). We aim to bring the reader up-to-date with CMR applications in diagnosis, prognosis, 4D flow, strain analysis, T1 mapping, machine learning and ongoing research.              Recent findings:                    CMR volumetric and functional metrics are now established as valuable prognostic markers in PH. This imaging modality is increasingly used to assess treatment response and improves risk stratification when incorporated into PH risk scores. Emerging techniques such as myocardial T1 mapping may play a role in the follow-up of selected patients. Myocardial strain may be used as an early marker for right and left ventricular dysfunction and a predictor for mortality. Machine learning has offered a glimpse into future possibilities. Ongoing research of new PH therapies is increasingly using CMR as a clinical endpoint.              Summary:                    The last 3 years have seen several large studies establishing CMR as a valuable diagnostic and prognostic tool in patients with PH, with CMR increasingly considered as an endpoint in clinical trials of PH therapies. Machine learning approaches to improve automation and accuracy of CMR metrics and identify imaging features of PH is an area of active research interest with promising clinical utility.",
33182824,Evapotranspiration Estimation with Small UAVs in Precision Agriculture,2020 Nov 10;20(22):6427.,"Estimating evapotranspiration (ET) has been one of the most critical research areas in agriculture because of water scarcity, the growing population, and climate change. The accurate estimation and mapping of ET are necessary for crop water management. Traditionally, researchers use water balance, soil moisture, weighing lysimeters, or an energy balance approach, such as Bowen ratio or eddy covariance towers to estimate ET. However, these ET methods are point-specific or area-weighted measurements and cannot be extended to a large scale. With the advent of satellite technology, remote sensing images became able to provide spatially distributed measurements. However, the spatial resolution of multispectral satellite images is in the range of meters, tens of meters, or hundreds of meters, which is often not enough for crops with clumped canopy structures, such as trees and vines. Unmanned aerial vehicles (UAVs) can mitigate these spatial and temporal limitations. Lightweight cameras and sensors can be mounted on the UAVs and take high-resolution images. Unlike satellite imagery, the spatial resolution of the UAV images can be at the centimeter-level. UAVs can also fly on-demand, which provides high temporal imagery. In this study, the authors examined different UAV-based approaches of ET estimation at first. Models and algorithms, such as mapping evapotranspiration at high resolution with internalized calibration (METRIC), the two-source energy balance (TSEB) model, and machine learning (ML) are analyzed and discussed herein. Second, challenges and opportunities for UAVs in ET estimation are also discussed, such as uncooled thermal camera calibration, UAV image collection, and image processing. Then, the authors share views on ET estimation with UAVs for future research and draw conclusive remarks.",1.0
33182638,Wearable Sensors Incorporating Compensatory Reserve Measurement for Advancing Physiological Monitoring in Critically Injured Trauma Patients,2020 Nov 10;20(22):6413.,"Vital signs historically served as the primary method to triage patients and resources for trauma and emergency care, but have failed to provide clinically-meaningful predictive information about patient clinical status. In this review, a framework is presented that focuses on potential wearable sensor technologies that can harness necessary electronic physiological signal integration with a current state-of-the-art predictive machine-learning algorithm that provides early clinical assessment of hypovolemia status to impact patient outcome. The ability to study the physiology of hemorrhage using a human model of progressive central hypovolemia led to the development of a novel machine-learning algorithm known as the compensatory reserve measurement (CRM). Greater sensitivity, specificity, and diagnostic accuracy to detect hemorrhage and onset of decompensated shock has been demonstrated by the CRM when compared to all standard vital signs and hemodynamic variables. The development of CRM revealed that continuous measurements of changes in arterial waveform features represented the most integrated signal of physiological compensation for conditions of reduced systemic oxygen delivery. In this review, detailed analysis of sensor technologies that include photoplethysmography, tonometry, ultrasound-based blood pressure, and cardiogenic vibration are identified as potential candidates for harnessing arterial waveform analog features required for real-time calculation of CRM. The integration of wearable sensors with the CRM algorithm provides a potentially powerful medical monitoring advancement to save civilian and military lives in emergency medical settings.",
33180692,Can machine learning optimize the efficiency of the operating room in the era of COVID-19?,Nov-Dec 2020;63(6):E527-E529.,"The cancellation of large numbers of surgical procedures because of the coronavirus disease 2019 (COVID-19) pandemic has drastically extended wait lists and negatively affected patient care and experience. As many facilities resume clinical work owing to the currently low burden of disease in our community, we are faced with operative booking protocols and procedures that are not mathematically designed to optimize efficiency. Using a subset of artificial intelligence called ""machine learning,"" we have shown how the use of operating time can be optimized with a custom Python (a high-level programming language) script and an open source machine-learning algorithm, the ORTools software suite from the Google AI division of Alphabet Inc. This allowed the creation of customized models to optimize the efficiency of operating room booking times, which resulted in a reduction in nursing overtime of 21% - a theoretical cost savings of $469 000 over 3 years.",
33178623,"Diagnostic Methods, Clinical Guidelines, and Antibiotic Treatment for Group A Streptococcal Pharyngitis: A Narrative Review",2020 Oct 15;10:563627.,"The most common bacterial cause of pharyngitis is infection by Group A β-hemolytic streptococcus (GABHS), commonly known as strep throat. 5-15% of adults and 15-35% of children in the United States with pharyngitis have a GABHS infection. The symptoms of GABHS overlap with non-GABHS and viral causes of acute pharyngitis, complicating the problem of diagnosis. A careful physical examination and patient history is the starting point for diagnosing GABHS. After a physical examination and patient history is completed, five types of diagnostic methods can be used to ascertain the presence of a GABHS infection: clinical scoring systems, rapid antigen detection tests, throat culture, nucleic acid amplification tests, and machine learning and artificial intelligence. Clinical guidelines developed by professional associations can help medical professionals choose among available techniques to diagnose strep throat. However, guidelines for diagnosing GABHS created by the American and European professional associations vary significantly, and there is substantial evidence that most physicians do not follow any published guidelines. Treatment for GABHS using analgesics, antipyretics, and antibiotics seeks to provide symptom relief, shorten the duration of illness, prevent nonsuppurative and suppurative complications, and decrease the risk of contagion, while minimizing the unnecessary use of antibiotics. There is broad agreement that antibiotics with narrow spectrums of activity are appropriate for treating strep throat. But whether and when patients should be treated with antibiotics for GABHS remains a controversial question. There is no clearly superior management strategy for strep throat, as significant controversy exists regarding the best methods to diagnose GABHS and under what conditions antibiotics should be prescribed.",
33175480,Testing a Machine Learning Tool for Facilitating Living Systematic Reviews of Chronic Pain Treatments [Internet],,"Background:                    Living systematic reviews can more rapidly and efficiently incorporate new evidence into systematic reviews through ongoing updates. A challenge to conducting living systematic reviews is identifying new articles in a timely manner. Optimizing search strategies to identify new studies before they have undergone indexing in electronic databases and automation using machine learning classifiers may increase the efficiency of identifying relevant new studies.              Methods:                    This project had three stages: develop optimized search strategies (Stage 1), test machine learning classifier on optimized searches (Stage 2), and test machine learning classifier on monthly update searches (Stage 3). Ovid® MEDLINE® search strategies were developed for three previously conducted chronic pain reviews using standard methods, combining National Library of Medicine Medical Subject Headings (MeSH) terms and text words (“standard searches”). Text word-only search strategies (“optimized searches”) were also developed based on the inclusion criteria for each review. In Stage 2, a machine learning classifier was trained and refined using citations from each of the completed pain reviews (“training set”) and tested on a subset of more recent citations (“simulated update”), to develop models that could predict the relevant of citations for each topic. In Stage 3, the machine learning models were prospectively applied to “optimized” monthly update searches conducted for the three pain reviews.              Results:                    In Stage 1, the optimized searches were less precise than the standard searches (i.e., identified more citations that reviewers eventually excluded) but were highly sensitive. In Stage 2, a machine learning classifier using a support vector machine model achieved 96 to 100 percent recall for all topics, with precision of between 1 and 7 percent. Performance was similar using the training data and on the simulated updates. The machine learning classifier excluded 35 to 65 percent of studies classified as low relevance. In Stage 3, the machine classifier achieved 97 to 100 percent sensitivity and excluded (i.e., classified as very low probability) 45 to 76 percent of studies identified in prospective, actual update searches. The estimated savings in time using the machine classifier ranged from 2.0 to 13.2 hours.              Conclusions:                    Text word-only searches to facilitate the conduct of living systematic reviews are associated with high sensitivity but reduced precision compared with standard searches using MeSH indexing terms. A machine learning classifier had high recall for identifying studies identified using text word searches, but had low to moderate precision, resulting in a small to moderate estimated time savings when applied to update searches.",
33176311,Recent Advances in Medical Image Processing,2020 Nov 11;1-14.,"Background:                    Application and development of the artificial intelligence technology have generated a profound impact in the field of medical imaging. It helps medical personnel to make an early and more accurate diagnosis. Recently, the deep convolution neural network is emerging as a principal machine learning method in computer vision and has received significant attention in medical imaging. Key Message: In this paper, we will review recent advances in artificial intelligence, machine learning, and deep convolution neural network, focusing on their applications in medical image processing. To illustrate with a concrete example, we discuss in detail the architecture of a convolution neural network through visualization to help understand its internal working mechanism.              Summary:                    This review discusses several open questions, current trends, and critical challenges faced by medical image processing and artificial intelligence technology.",
33173516,Selective Review of Neuroimaging Findings in Youth at Clinical High Risk for Psychosis: On the Path to Biomarkers for Conversion,2020 Sep 23;11:567534.,"First episode psychosis (FEP), and subsequent diagnosis of schizophrenia or schizoaffective disorder, predominantly occurs during late adolescence, is accompanied by a significant decline in function and represents a traumatic experience for patients and families alike. Prior to first episode psychosis, most patients experience a prodromal period of 1-2 years, during which symptoms first appear and then progress. During that time period, subjects are referred to as being at Clinical High Risk (CHR), as a prodromal period can only be designated in hindsight in those who convert. The clinical high-risk period represents a critical window during which interventions may be targeted to slow or prevent conversion to psychosis. However, only one third of subjects at clinical high risk will convert to psychosis and receive a formal diagnosis of a primary psychotic disorder. Therefore, in order for targeted interventions to be developed and applied, predicting who among this population will convert is of critical importance. To date, a variety of neuroimaging modalities have identified numerous differences between CHR subjects and healthy controls. However, complicating attempts at predicting conversion are increasingly recognized co-morbidities, such as major depressive disorder, in a significant number of CHR subjects. The result of this is that phenotypes discovered between CHR subjects and healthy controls are likely non-specific to psychosis and generalized for major mental illness. In this paper, we selectively review evidence for neuroimaging phenotypes in CHR subjects who later converted to psychosis. We then evaluate the recent landscape of machine learning as it relates to neuroimaging phenotypes in predicting conversion to psychosis.",
33169338,Machine learning and statistical methods for predicting mortality in heart failure,2021 May;26(3):545-552.,"Heart failure is a debilitating clinical syndrome associated with increased morbidity, mortality, and frequent hospitalization, leading to increased healthcare budget utilization. Despite the exponential growth in the introduction of pharmacological agents and medical devices that improve survival, many heart failure patients, particularly those with a left ventricular ejection fraction less than 40%, still experience persistent clinical symptoms that lead to an overall decreased quality of life. Clinical risk prediction is one of the strategies that has been implemented for the selection of high-risk patients and for guiding therapy. However, most risk predictive models have not been well-integrated into the clinical setting. This is partly due to inherent limitations, such as creating risk predicting models using static clinical data that does not consider the dynamic nature of heart failure. Another limiting factor preventing clinicians from utilizing risk prediction models is the lack of insight into how predictive models are built. This review article focuses on describing how predictive models for risk-stratification of patients with heart failure are built.",1.0
33167561,Application of Transfer Learning in EEG Decoding Based on Brain-Computer Interfaces: A Review,2020 Nov 5;20(21):6321.,"The algorithms of electroencephalography (EEG) decoding are mainly based on machine learning in current research. One of the main assumptions of machine learning is that training and test data belong to the same feature space and are subject to the same probability distribution. However, this may be violated in EEG processing. Variations across sessions/subjects result in a deviation of the feature distribution of EEG signals in the same task, which reduces the accuracy of the decoding model for mental tasks. Recently, transfer learning (TL) has shown great potential in processing EEG signals across sessions/subjects. In this work, we reviewed 80 related published studies from 2010 to 2020 about TL application for EEG decoding. Herein, we report what kind of TL methods have been used (e.g., instance knowledge, feature representation knowledge, and model parameter knowledge), describe which types of EEG paradigms have been analyzed, and summarize the datasets that have been used to evaluate performance. Moreover, we discuss the state-of-the-art and future development of TL for EEG decoding. The results show that TL can significantly improve the performance of decoding models across subjects/sessions and can reduce the calibration time of brain-computer interface (BCI) systems. This review summarizes the current practical suggestions and performance outcomes in the hope that it will provide guidance and help for EEG research in the future.",2.0
33167558,Computational Diagnostic Techniques for Electrocardiogram Signal Analysis,2020 Nov 5;20(21):6318.,"Cardiovascular diseases (CVDs), including asymptomatic myocardial ischemia, angina, myocardial infarction, and ischemic heart failure, are the leading cause of death globally. Early detection and treatment of CVDs significantly contribute to the prevention or delay of cardiovascular death. Electrocardiogram (ECG) records the electrical impulses generated by heart muscles, which reflect regular or irregular beating activity. Computer-aided techniques provide fast and accurate tools to identify CVDs using a patient's ECG signal, which have achieved great success in recent years. Latest computational diagnostic techniques based on ECG signals for estimating CVDs conditions are summarized here. The procedure of ECG signals analysis is discussed in several subsections, including data preprocessing, feature engineering, classification, and application. In particular, the End-to-End models integrate feature extraction and classification into learning algorithms, which not only greatly simplifies the process of data analysis, but also shows excellent accuracy and robustness. Portable devices enable users to monitor their cardiovascular status at any time, bringing new scenarios as well as challenges to the application of ECG algorithms. Computational diagnostic techniques for ECG signal analysis show great potential for helping health care professionals, and their application in daily life benefits both patients and sub-healthy people.",
33165729,Suicide Risk Assessment Using Machine Learning and Social Networks: a Scoping Review,2020 Nov 9;44(12):205.,"According to the World Health Organization (WHO) report in 2016, around 800,000 of individuals have committed suicide. Moreover, suicide is the second cause of unnatural death in people between 15 and 29 years. This paper reviews state of the art on the literature concerning the use of machine learning methods for suicide detection on social networks. Consequently, the objectives, data collection techniques, development process and the validation metrics used for suicide detection on social networks are analyzed. The authors conducted a scoping review using the methodology proposed by Arksey and O'Malley et al. and the PRISMA protocol was adopted to select the relevant studies. This scoping review aims to identify the machine learning techniques used to predict suicide risk based on information posted on social networks. The databases used are PubMed, Science Direct, IEEE Xplore and Web of Science. In total, 50% of the included studies (8/16) report explicitly the use of data mining techniques for feature extraction, feature detection or entity identification. The most commonly reported method was the Linguistic Inquiry and Word Count (4/8, 50%), followed by Latent Dirichlet Analysis, Latent Semantic Analysis, and Word2vec (2/8, 25%). Non-negative Matrix Factorization and Principal Component Analysis were used only in one of the included studies (12.5%). In total, 3 out of 8 research papers (37.5%) combined more than one of those techniques. Supported Vector Machine was implemented in 10 out of the 16 included studies (62.5%). Finally, 75% of the analyzed studies implement machine learning-based models using Python.",1.0
33165308,Breakthrough healthcare technologies in the COVID-19 era: a unique opportunity for cardiovascular practitioners and patients,2021 Mar;63(1):62-74.,"Introduction:                    The Coronavirus disease 2019 (COVID-19) pandemic, caused by symptomatic severe acute respiratory syndrome-Coronavirus-2 (SARS-CoV-2) infection, has wreaked havoc globally, challenging the healthcare, economical, technological and social status quo of developing but also developed countries. For instance, the COVID-19 scare has reduced timely hospital admissions for ST-elevation myocardial infarction in Europe and the USA, causing unnecessary deaths and disabilities. While the emergency is still ongoing, enough efforts have been put to study and tackle this condition such that a comprehensive perspective and synthesis on the potential role of breakthrough healthcare technologies is possible. Indeed, current state-of-the-art information technologies can provide a unique opportunity to adapt and adjust to the current healthcare needs associated with COVID-19, either directly or indirectly, and in particular those of cardiovascular patients and practitioners.              Evidence acquisition:                    We searched several biomedical databases, websites and social media, including PubMed, Medscape, and Twitter, for smartcare approaches suitable for application in the COVID-19 pandemic.              Evidence synthesis:                    We retrieved details on several promising avenues for present and future healthcare technologies, capable of substantially reduce the mortality, morbidity, and resource use burden of COVID-19 as well as that of cardiovascular disease. In particular, we have found data supporting the importance of data sharing, model sharing, preprint archiving, social media, medical case sharing, distance learning and continuous medical education, smartphone apps, telemedicine, robotics, big data analysis, machine learning, and deep learning, with the ultimate goal of optimization of individual prevention, diagnosis, tracing, risk-stratification, treatment and rehabilitation.              Conclusions:                    We are confident that refinement and command of smartcare technologies will prove extremely beneficial in the short-term, but also dramatically reshape cardiovascular practice and healthcare delivery in the long-term future, for COVID-19 as well as other diseases.",1.0
33163152,Data mining in Raman imaging in a cellular biological system,2020 Oct 15;18:2920-2930.,"The distribution and dynamics of biomolecules in the cell is of critical interest in biological research. Raman imaging techniques have expanded our knowledge of cellular biological systems significantly. The technological developments that have led to the optimization of Raman instrumentation have helped to improve the speed of the measurement and the sensitivity. As well as instrumental developments, data mining plays a significant role in revealing the complicated chemical information contained within the spectral data. A number of data mining methods have been applied to extract the spectral information and translate them into biological information. Single-cell visualization, cell classification and biomolecular/drug quantification have all been achieved by the application of data mining to Raman imaging data. Herein we summarize the framework for Raman imaging data analysis, which involves preprocessing, pattern recognition and validation. There are multiple methods developed for each stage of analysis. The characteristics of these methods are described in relation to their application in Raman imaging of the cell. Furthermore, we summarize the software that can facilitate the implementation of these methods. Through its careful selection and application, data mining can act as an essential tool in the exploration of information-rich Raman spectral data.",
33162926,Machine Learning Applications in the Neuro ICU: A Solution to Big Data Mayhem?,2020 Oct 9;11:554633.,"The neurological ICU (neuro ICU) often suffers from significant limitations due to scarce resource availability for their neurocritical care patients. Neuro ICU patients require frequent neurological evaluations, continuous monitoring of various physiological parameters, frequent imaging, and routine lab testing. This amasses large amounts of data specific to each patient. Neuro ICU teams are often overburdened by the resulting complexity of data for each patient. Machine Learning algorithms (ML), are uniquely capable of interpreting high-dimensional datasets that are too difficult for humans to comprehend. Therefore, the application of ML in the neuro ICU could alleviate the burden of analyzing big datasets for each patient. This review serves to (1) briefly summarize ML and compare the different types of MLs, (2) review recent ML applications to improve neuro ICU management and (3) describe the future implications of ML to neuro ICU management.",
33162885,Classifying Intracortical Brain-Machine Interface Signal Disruptions Based on System Performance and Applicable Compensatory Strategies: A Review,2020 Oct 9;14:558987.,"Brain-machine interfaces (BMIs) record and translate neural activity into a control signal for assistive or other devices. Intracortical microelectrode arrays (MEAs) enable high degree-of-freedom BMI control for complex tasks by providing fine-resolution neural recording. However, chronically implanted MEAs are subject to a dynamic in vivo environment where transient or systematic disruptions can interfere with neural recording and degrade BMI performance. Typically, neural implant failure modes have been categorized as biological, material, or mechanical. While this categorization provides insight into a disruption's causal etiology, it is less helpful for understanding degree of impact on BMI function or possible strategies for compensation. Therefore, we propose a complementary classification framework for intracortical recording disruptions that is based on duration of impact on BMI performance and requirement for and responsiveness to interventions: (1) Transient disruptions interfere with recordings on the time scale of minutes to hours and can resolve spontaneously; (2) Reversible disruptions cause persistent interference in recordings but the root cause can be remedied by an appropriate intervention; (3) Irreversible compensable disruptions cause persistent or progressive decline in signal quality, but their effects on BMI performance can be mitigated algorithmically; and (4) Irreversible non-compensable disruptions cause permanent signal loss that is not amenable to remediation or compensation. This conceptualization of intracortical BMI disruption types is useful for highlighting specific areas for potential hardware improvements and also identifying opportunities for algorithmic interventions. We review recording disruptions that have been reported for MEAs and demonstrate how biological, material, and mechanical mechanisms of disruption can be further categorized according to their impact on signal characteristics. Then we discuss potential compensatory protocols for each of the proposed disruption classes. Specifically, transient disruptions may be minimized by using robust neural decoder features, data augmentation methods, adaptive machine learning models, and specialized signal referencing techniques. Statistical Process Control methods can identify reparable disruptions for rapid intervention. In-vivo diagnostics such as impedance spectroscopy can inform neural feature selection and decoding models to compensate for irreversible disruptions. Additional compensatory strategies for irreversible disruptions include information salvage techniques, data augmentation during decoder training, and adaptive decoding methods to down-weight damaged channels.",
33160516,Predictive modeling in reproductive medicine: Where will the future of artificial intelligence research take us?,2020 Nov;114(5):934-940.,"Artificial intelligence (AI) systems have been proposed for reproductive medicine since 1997. Although AI is the main driver of emergent technologies in reproduction, such as robotics, Big Data, and internet of things, it will continue to be the engine for technological innovation for the foreseeable future. What does the future of AI research look like?",
33160514,Evaluating predictive models in reproductive medicine,2020 Nov;114(5):921-926.,"Predictive modeling has become a distinct subdiscipline of reproductive medicine, and researchers and clinicians are just learning the skills and expertise to evaluate artificial intelligence (AI) studies. Diagnostic tests and model predictions are subject to evaluation. Their use offers potential for both harm and benefit in terms of diagnosis, treatment, and prognosis. The performance of AI models and their potential clinical utility hinge on the quality and size of the databases used, the types and distribution of data, and the particular AI method applied. Additionally, when images are involved, the method of capturing, preprocessing, and treatment and accurate labeling of images becomes an important component of AI modeling. Inconsistent image treatment or inaccurate labeling of images can lead to an inconsistent database, resulting in poor AI accuracy. We discuss the critical appraisal of AI models in reproductive medicine and convey the importance of transparency and standardization in reporting AI models so that the risk of bias and the potential clinical utility of AI can be assessed.",
33160513,Artificial intelligence in human in vitro fertilization and embryology,2020 Nov;114(5):914-920.,"Embryo evaluation and selection embody the aggregate manifestation of the entire in vitro fertilization (IVF) process. It aims to choose the ""best"" embryos from the larger cohort of fertilized oocytes, the majority of which will be determined to be not viable either as a result of abnormal development or due to chromosomal imbalances. Indeed, it is generally acknowledged that even after embryo selection based on morphology, time-lapse microscopic photography, or embryo biopsy with preimplantation genetic testing, implantation rates in the human are difficult to predict. Our pursuit of enhancing embryo evaluation and selection, as well as increasing live birth rates, will require the adoption of novel technologies. Recently, several artificial intelligence (AI)-based methods have emerged as objective, standardized, and efficient tools for evaluating human embryos. Moreover, AI-based methods can be implemented for other clinical aspects of IVF, such as assessing patient reproductive potential and individualizing gonadotropin stimulation protocols. As AI has the capability to analyze ""big"" data, the ultimate goal will be to apply AI tools to the analysis of all embryological, clinical, and genetic data in an effort to provide patient-tailored treatments. In this chapter, we present an overview of existing AI technologies in reproductive medicine and envision their potential future applications in the field.",1.0
33160512,Precision medicine and artificial intelligence: overview and relevance to reproductive medicine,2020 Nov;114(5):908-913.,"Traditionally, new treatments have been developed for the population at large. Recently, large-scale genomic sequencing analyses have revealed tremendous genetic diversity between individuals. In diseases driven by genetic events such as cancer, genomic sequencing can unravel all the mutations that drive individual tumors. The ability to capture the genetic makeup of individual patients has led to the concept of precision medicine, a modern, technology-driven form of personalized medicine. Precision medicine matches each individual to the best treatment in a way that is tailored to his or her genetic uniqueness. To further personalize medicine, precision medicine increasingly incorporates and integrates data beyond genomics, such as epigenomics and metabolomics, as well as imaging. Increasingly, the robust use and integration of these modalities in precision medicine require the use of artificial intelligence and machine learning. This modern view of precision medicine, adopted early in certain areas of medicine such as cancer, has started to impact the field of reproductive medicine. Here we review the concepts and history of precision medicine and artificial intelligence, highlight their growing impact on reproductive medicine, and outline some of the challenges and limitations that these new fields have encountered in medicine.",
33159918,"NIRS measures in pain and analgesia: Fundamentals, features, and function",2021 Jan;120:335-353.,"Current pain assessment techniques based only on clinical evaluation and self-reports are not objective and may lead to inadequate treatment. Having a functional biomarker will add to the clinical fidelity, diagnosis, and perhaps improve treatment efficacy in patients. While many approaches have been deployed in pain biomarker discovery, functional near-infrared spectroscopy (fNIRS) is a technology that allows for non-invasive measurement of cortical hemodynamics. The utility of fNIRS is especially attractive given its ability to detect specific changes in the somatosensory and high-order cortices as well as its ability to measure (1) brain function similar to functional magnetic resonance imaging, (2) graded responses to noxious and innocuous stimuli, (3) analgesia, and (4) nociception under anesthesia. In this review, we evaluate the utility of fNIRS in nociception/pain with particular focus on its sensitivity and specificity, methodological advantages and limitations, and the current and potential applications in various pain conditions. Everything considered, fNIRS technology could enhance our ability to evaluate evoked and persistent pain across different age groups and clinical populations.",
33159692,Current status and future perspective of artificial intelligence applications in endoscopic diagnosis and management of gastric cancer,2021 Jan;33(2):263-272.,"Image recognition using artificial intelligence (AI) has progressed significantly due to innovative technologies such as machine learning and deep learning. In the field of gastric cancer (GC) management, research on AI-based diagnosis such as anatomical classification of endoscopic images, diagnosis of Helicobacter pylori infection, and detection and qualitative diagnosis of GC is being conducted, and an accuracy equivalent to that of physicians has been reported. It is expected that AI will soon be introduced in the field of endoscopic diagnosis and management of gastric cancer as a supportive tool for physicians, thus improving the quality of medical care.",
33159244,Artificial cognition: How experimental psychology can help generate explainable artificial intelligence,2021 Apr;28(2):454-475.,"Artificial intelligence powered by deep neural networks has reached a level of complexity where it can be difficult or impossible to express how a model makes its decisions. This black-box problem is especially concerning when the model makes decisions with consequences for human well-being. In response, an emerging field called explainable artificial intelligence (XAI) aims to increase the interpretability, fairness, and transparency of machine learning. In this paper, we describe how cognitive psychologists can make contributions to XAI. The human mind is also a black box, and cognitive psychologists have over 150 years of experience modeling it through experimentation. We ought to translate the methods and rigor of cognitive psychology to the study of artificial black boxes in the service of explainability. We provide a review of XAI for psychologists, arguing that current methods possess a blind spot that can be complemented by the experimental cognitive tradition. We also provide a framework for research in XAI, highlight exemplary cases of experimentation within XAI inspired by psychological science, and provide a tutorial on experimenting with machines. We end by noting the advantages of an experimental approach and invite other psychologists to conduct research in this exciting new field.",
33157093,The opportunities and challenges of machine learning in the acute care setting for precision prevention of posttraumatic stress sequelae,2021 Feb;336:113526.,"Personalized medicine is among the most exciting innovations in recent clinical research, offering the opportunity for tailored screening and management at the individual level. Biomarker-enriched clinical trials have shown increased efficiency and informativeness in cancer research due to the selective exclusion of patients unlikely to benefit. In acute stress situations, clinically significant decisions are often made in time-sensitive manners and providers may be pressed to make decisions based on abbreviated clinical assessments. Up to 30% of trauma survivors admitted to the Emergency Department (ED) will develop long-lasting posttraumatic stress psychopathologies. The long-term impact of those survivors with posttraumatic stress sequelae are significant, impacting both long-term psychological and physiological recovery. An accurate prognostic model of who will develop posttraumatic stress symptoms does not exist yet. Additionally, no scalable and cost-effective method that can be easily integrated into routine care exists, even though especially the acute care setting provides a critical window of opportunity for prevention in the so-called golden hours when preventive measures are most effective. In this review, we aim to discuss emerging machine learning (ML) applications that are promising for precisely risk stratification and targeted treatments in the acute care setting. The aim of this narrative review is to present examples of digital health innovations and to discuss the potential of these new approaches for treatment selection and prevention of posttraumatic sequelae in the acute care setting. The application of artificial intelligence-based solutions have already had great success in other areas and are rapidly approaching the field of psychological care as well. New ways of algorithm-based risk predicting, and the use of digital phenotypes provide a high potential for predicting future risk of PTSD in acute care settings and to go new steps in precision psychiatry.",
33156423,Survival prediction of glioblastoma patients-are we there yet? A systematic review of prognostic modeling for glioblastoma and its clinical potential,2020 Nov 6.,"Glioblastoma is associated with a poor prognosis. Even though survival statistics are well-described at the population level, it remains challenging to predict the prognosis of an individual patient despite the increasing number of prognostic models. The aim of this study is to systematically review the literature on prognostic modeling in glioblastoma patients. A systematic literature search was performed to identify all relevant studies that developed a prognostic model for predicting overall survival in glioblastoma patients following the PRISMA guidelines. Participants, type of input, algorithm type, validation, and testing procedures were reviewed per prognostic model. Among 595 citations, 27 studies were included for qualitative review. The included studies developed and evaluated a total of 59 models, of which only seven were externally validated in a different patient cohort. The predictive performance among these studies varied widely according to the AUC (0.58-0.98), accuracy (0.69-0.98), and C-index (0.66-0.70). Three studies deployed their model as an online prediction tool, all of which were based on a statistical algorithm. The increasing performance of survival prediction models will aid personalized clinical decision-making in glioblastoma patients. The scientific realm is gravitating towards the use of machine learning models developed on high-dimensional data, often with promising results. However, none of these models has been implemented into clinical care. To facilitate the clinical implementation of high-performing survival prediction models, future efforts should focus on harmonizing data acquisition methods, improving model interpretability, and externally validating these models in multicentered, prospective fashion.",
33156361,"Evolving robotic surgery training and improving patient safety, with the integration of novel technologies",2020 Nov 6.,"Introduction:                    Robot-assisted surgery is becoming increasingly adopted by multiple surgical specialties. There is evidence of inherent risks of utilising new technologies that are unfamiliar early in the learning curve. The development of standardised and validated training programmes is crucial to deliver safe introduction. In this review, we aim to evaluate the current evidence and opportunities to integrate novel technologies into modern digitalised robotic training curricula.              Methods:                    A systematic literature review of the current evidence for novel technologies in surgical training was conducted online and relevant publications and information were identified. Evaluation was made on how these technologies could further enable digitalisation of training.              Results:                    Overall, the quality of available studies was found to be low with current available evidence consisting largely of expert opinion, consensus statements and small qualitative studies. The review identified that there are several novel technologies already being utilised in robotic surgery training. There is also a trend towards standardised validated robotic training curricula. Currently, the majority of the validated curricula do not incorporate novel technologies and training is delivered with more traditional methods that includes centralisation of training services with wet laboratories that have access to cadavers and dedicated training robots.              Conclusions:                    Improvements to training standards and understanding performance data have good potential to significantly lower complications in patients. Digitalisation automates data collection and brings data together for analysis. Machine learning has potential to develop automated performance feedback for trainees. Digitalised training aims to build on the current gold standards and to further improve the 'continuum of training' by integrating PBP training, 3D-printed models, telementoring, telemetry and machine learning.",
33154949,Artificial Intelligence (AI)-Based Systems Biology Approaches in Multi-Omics Data Analysis of Cancer,2020 Oct 14;10:588221.,"Cancer is the manifestation of abnormalities of different physiological processes involving genes, DNAs, RNAs, proteins, and other biomolecules whose profiles are reflected in different omics data types. As these bio-entities are very much correlated, integrative analysis of different types of omics data, multi-omics data, is required to understanding the disease from the tumorigenesis to the disease progression. Artificial intelligence (AI), specifically machine learning algorithms, has the ability to make decisive interpretation of ""big""-sized complex data and, hence, appears as the most effective tool for the analysis and understanding of multi-omics data for patient-specific observations. In this review, we have discussed about the recent outcomes of employing AI in multi-omics data analysis of different types of cancer. Based on the research trends and significance in patient treatment, we have primarily focused on the AI-based analysis for determining cancer subtypes, disease prognosis, and therapeutic targets. We have also discussed about AI analysis of some non-canonical types of omics data as they have the capability of playing the determiner role in cancer patient care. Additionally, we have briefly discussed about the data repositories because of their pivotal role in multi-omics data storing, processing, and analysis.",1.0
33153682,"Informatics Approaches for Recognition, Management, and Prevention of Occupational Respiratory Disease",2020 Dec;41(4):605-621.,"Computer and information systems can improve occupational respiratory disease prevention and surveillance by providing efficient resources for patients, workers, clinicians, and public health practitioners. Advances include interlinking electronic health records, autocoding surveillance data, clinical decision support systems, and social media applications for acquiring and disseminating information. Obstacles to advances include inflexible hierarchical coding schemes, inadequate occupational health electronic health record systems, and inadequate public focus on occupational respiratory disease. Potentially transformative approaches include machine learning, natural language processing, and improved ontologies.",
33151420,Denouements of machine learning and multimodal diagnostic classification of Alzheimer's disease,2020 Nov 5;3(1):26.,"Alzheimer's disease (AD) is the most common type of dementia. The exact cause and treatment of the disease are still unknown. Different neuroimaging modalities, such as magnetic resonance imaging (MRI), positron emission tomography, and single-photon emission computed tomography, have played a significant role in the study of AD. However, the effective diagnosis of AD, as well as mild cognitive impairment (MCI), has recently drawn large attention. Various technological advancements, such as robots, global positioning system technology, sensors, and machine learning (ML) algorithms, have helped improve the diagnostic process of AD. This study aimed to determine the influence of implementing different ML classifiers in MRI and analyze the use of support vector machines with various multimodal scans for classifying patients with AD/MCI and healthy controls. Conclusions have been drawn in terms of employing different classifier techniques and presenting the optimal multimodal paradigm for the classification of AD.",1.0
33148398,Artificial intelligence in medicine: A matter of joy or concern?,2021 Jan;50(1):101962.,"Artificial Intelligence (AI), a concept which dates back to the 1950s, is increasingly being developed by many medical specialties, especially those based on imaging or surgery. While the cognitive component of AI is far superior to that of human intelligence, it lacks consciousness, feelings, intuition and adaptation to unexpected situations. Furthermore, fundamental questions arise with regard to data security, the impact on healthcare professions, and the distribution of roles between physicians and AI especially concerning consent to medical care and liability in the event of a therapeutic accident.",
33147836,Review of Laser Raman Spectroscopy for Surgical Breast Cancer Detection: Stochastic Backpropagation Neural Networks,2020 Nov 2;20(21):6260.,"Laser Raman spectroscopy (LRS) is a highly specific biomolecular technique which has been shown to have the ability to distinguish malignant and normal breast tissue. This paper discusses significant advancements in the use of LRS in surgical breast cancer diagnosis, with an emphasis on statistical and machine learning strategies employed for precise, transparent and real-time analysis of Raman spectra. When combined with a variety of ""machine learning"" techniques LRS has been increasingly employed in oncogenic diagnostics. This paper proposes that the majority of these algorithms fail to provide the two most critical pieces of information required by the practicing surgeon: a probability that the classification of a tissue is correct, and, more importantly, the expected error in that probability. Stochastic backpropagation artificial neural networks inherently provide both pieces of information for each and every tissue site examined by LRS. If the networks are trained using both human experts and an unsupervised classification algorithm as gold standards, rapid progress can be made understanding what additional contextual data is needed to improve network classification performance. Our patients expect us to not simply have an opinion about their tumor, but to know how certain we are that we are correct. Stochastic networks can provide that information.",
33145991,Which features of postural sway are effective in distinguishing Parkinson's disease from controls? A systematic review,2021 Jan;11(1):e01929.,"Background:                    Postural sway may be useful as an objective measure of Parkinson's disease (PD). Existing studies have analyzed many different features of sway using different experimental paradigms. We aimed to determine what features have been used to measure sway and then to assess which feature(s) best differentiate PD patients from controls. We also aimed to determine whether any refinements might improve discriminative power and so assist in standardizing experimental conditions and analysis of data.              Methods:                    In this systematic review of the literature, effect size (ES) was calculated for every feature reported by each article and then collapsed across articles where appropriate. The influence of clinical medication status, visual state, and sampling rate on ES was also assessed.              Results:                    Four hundred and forty-three papers were retrieved. 25 contained enough information for further analysis. The most commonly used features were not the most effective (e.g., PathLength, used 14 times, had ES of 0.47, while TotalEnergy, used only once, had ES of 1.78). Increased sampling rate was associated with increased ES (PathLength ES increased to 1.12 at 100 Hz from 0.40 at 10 Hz). Measurement during ""OFF"" clinical status was associated with increased ES (PathLength ES was 0.83 OFF compared to 0.21 ON).              Conclusions:                    This review identified promising features for analysis of postural sway in PD, recommending a sampling rate of 100 Hz and studying patients when OFF to maximize ES. ES complements statistical significance as it is clinically relevant and is easily compared across experiments. We suggest that machine learning is a promising tool for the future analysis of postural sway in PD.",
33145087,Radiogenomics of lung cancer,2020 Sep;12(9):5104-5109.,"Machine learning (ML) and artificial intelligence (AI) are aiding in improving sensitivity and specificity of diagnostic imaging. The rapid adoption of these advanced ML algorithms is transforming imaging analysis; taking us from noninvasive detection of pathology to noninvasive precise diagnosis of the pathology by identifying whether detected abnormality is a secondary to infection, inflammation and/or neoplasm. This is led to the emergence of ""Radiobiogenomics""; referring to the concept of identifying biologic (genomic, proteomic) alterations in the detected lesion. Radiobiogenomic involves image segmentation, feature extraction, and ML model to predict underlying tumor genotype and clinical outcomes. Lung cancer is the most common cause of cancer related death worldwide. There are several histologic subtypes of lung cancer, e.g., small cell lung cancer (SCLC), non-small cell lung cancer (NSCLC) (adenocarcinoma, squamous cell carcinoma). These variable histologic subtypes not only appear different at microscopic level, but these also differ at genetic and transcription level. This intrinsic heterogeneity reveals itself as different morphologic appearances on diagnostic imaging, such as CT, PET/CT and MRI. Traditional evaluation of imaging findings of lung cancer is limited to morphologic characteristics, such as lesion size, margins, density. Radiomics takes image analysis a step further by looking at imaging phenotype with higher order statistics in efforts to quantify intralesional heterogeneity. This heterogeneity, in turn, can be potentially used to extract intralesional genomic and proteomic data. This review aims to highlight novel concepts in ML and AI and their potential applications in identifying radiobiogenomics of lung cancer.",
33143137,A Review of the Important Role of CYP2D6 in Pharmacogenomics,2020 Oct 30;11(11):1295.,"Cytochrome P450 2D6 (CYP2D6) is a critical pharmacogene involved in the metabolism of ~20% of commonly used drugs across a broad spectrum of medical disciplines including psychiatry, pain management, oncology and cardiology. Nevertheless, CYP2D6 is highly polymorphic with single-nucleotide polymorphisms, small insertions/deletions and larger structural variants including multiplications, deletions, tandem arrangements, and hybridisations with non-functional CYP2D7 pseudogenes. The frequency of these variants differs across populations, and they significantly influence the drug-metabolising enzymatic function of CYP2D6. Importantly, altered CYP2D6 function has been associated with both adverse drug reactions and reduced drug efficacy, and there is growing recognition of the clinical and economic burdens associated with suboptimal drug utilisation. To date, pharmacogenomic clinical guidelines for at least 48 CYP2D6-substrate drugs have been developed by prominent pharmacogenomics societies, which contain therapeutic recommendations based on CYP2D6-predicted categories of metaboliser phenotype. Novel algorithms to interpret CYP2D6 function from sequencing data that consider structural variants, and machine learning approaches to characterise the functional impact of novel variants, are being developed. However, CYP2D6 genotyping is yet to be implemented broadly into clinical practice, and so further effort and initiatives are required to overcome the implementation challenges and deliver the potential benefits to the bedside.",3.0
33142863,Federated Learning in Smart City Sensing: Challenges and Opportunities,2020 Oct 31;20(21):6230.,"Smart Cities sensing is an emerging paradigm to facilitate the transition into smart city services. The advent of the Internet of Things (IoT) and the widespread use of mobile devices with computing and sensing capabilities has motivated applications that require data acquisition at a societal scale. These valuable data can be leveraged to train advanced Artificial Intelligence (AI) models that serve various smart services that benefit society in all aspects. Despite their effectiveness, legacy data acquisition models backed with centralized Machine Learning models entail security and privacy concerns, and lead to less participation in large-scale sensing and data provision for smart city services. To overcome these challenges, Federated Learning is a novel concept that can serve as a solution to the privacy and security issues encountered within the process of data collection. This survey article presents an overview of smart city sensing and its current challenges followed by the potential of Federated Learning in addressing those challenges. A comprehensive discussion of the state-of-the-art methods for Federated Learning is provided along with an in-depth discussion on the applicability of Federated Learning in smart city sensing; clear insights on open issues, challenges, and opportunities in this field are provided as guidance for the researchers studying this subject matter.",1.0
33141088,Classification of Depression Through Resting-State Electroencephalogram as a Novel Practice in Psychiatry: Review,2020 Nov 3;22(11):e19548.,"Background:                    Machine learning applications in health care have increased considerably in the recent past, and this review focuses on an important application in psychiatry related to the detection of depression. Since the advent of computational psychiatry, research based on functional magnetic resonance imaging has yielded remarkable results, but these tools tend to be too expensive for everyday clinical use.              Objective:                    This review focuses on an affordable data-driven approach based on electroencephalographic recordings. Web-based applications via public or private cloud-based platforms would be a logical next step. We aim to compare several different approaches to the detection of depression from electroencephalographic recordings using various features and machine learning models.              Methods:                    To detect depression, we reviewed published detection studies based on resting-state electroencephalogram with final machine learning, and to predict therapy outcomes, we reviewed a set of interventional studies using some form of stimulation in their methodology.              Results:                    We reviewed 14 detection studies and 12 interventional studies published between 2008 and 2019. As direct comparison was not possible due to the large diversity of theoretical approaches and methods used, we compared them based on the steps in analysis and accuracies yielded. In addition, we compared possible drawbacks in terms of sample size, feature extraction, feature selection, classification, internal and external validation, and possible unwarranted optimism and reproducibility. In addition, we suggested desirable practices to avoid misinterpretation of results and optimism.              Conclusions:                    This review shows the need for larger data sets and more systematic procedures to improve the use of the solution for clinical diagnostics. Therefore, regulation of the pipeline and standard requirements for methodology used should become mandatory to increase the reliability and accuracy of the complete methodology for it to be translated to modern psychiatry.",
33140591,Artificial Intelligence in Health Care: Current Applications and Issues,2020 Nov 2;35(42):e379.,"In recent years, artificial intelligence (AI) technologies have greatly advanced and become a reality in many areas of our daily lives. In the health care field, numerous efforts are being made to implement the AI technology for practical medical treatments. With the rapid developments in machine learning algorithms and improvements in hardware performances, the AI technology is expected to play an important role in effectively analyzing and utilizing extensive amounts of health and medical data. However, the AI technology has various unique characteristics that are different from the existing health care technologies. Subsequently, there are a number of areas that need to be supplemented within the current health care system for the AI to be utilized more effectively and frequently in health care. In addition, the number of medical practitioners and public that accept AI in the health care is still low; moreover, there are various concerns regarding the safety and reliability of AI technology implementations. Therefore, this paper aims to introduce the current research and application status of AI technology in health care and discuss the issues that need to be resolved.",
33139966,Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review,2021 Apr 1;167:114155.,"The COVID-19 pandemic caused by the novel coronavirus SARS-CoV-2 occurred unexpectedly in China in December 2019. Tens of millions of confirmed cases and more than hundreds of thousands of confirmed deaths are reported worldwide according to the World Health Organisation. News about the virus is spreading all over social media websites. Consequently, these social media outlets are experiencing and presenting different views, opinions and emotions during various outbreak-related incidents. For computer scientists and researchers, big data are valuable assets for understanding people's sentiments regarding current events, especially those related to the pandemic. Therefore, analysing these sentiments will yield remarkable findings. To the best of our knowledge, previous related studies have focused on one kind of infectious disease. No previous study has examined multiple diseases via sentiment analysis. Accordingly, this research aimed to review and analyse articles about the occurrence of different types of infectious diseases, such as epidemics, pandemics, viruses or outbreaks, during the last 10 years, understand the application of sentiment analysis and obtain the most important literature findings. Articles on related topics were systematically searched in five major databases, namely, ScienceDirect, PubMed, Web of Science, IEEE Xplore and Scopus, from 1 January 2010 to 30 June 2020. These indices were considered sufficiently extensive and reliable to cover our scope of the literature. Articles were selected based on our inclusion and exclusion criteria for the systematic review, with a total of n = 28 articles selected. All these articles were formed into a coherent taxonomy to describe the corresponding current standpoints in the literature in accordance with four main categories: lexicon-based models, machine learning-based models, hybrid-based models and individuals. The obtained articles were categorised into motivations related to disease mitigation, data analysis and challenges faced by researchers with respect to data, social media platforms and community. Other aspects, such as the protocol being followed by the systematic review and demographic statistics of the literature distribution, were included in the review. Interesting patterns were observed in the literature, and the identified articles were grouped accordingly. This study emphasised the current standpoint and opportunities for research in this area and promoted additional efforts towards the understanding of this research field.",4.0
33139614,Machine Learning's Application in Deep Brain Stimulation for Parkinson's Disease: A Review,2020 Nov 1;10(11):809.,"Deep brain stimulation (DBS) is a surgical treatment for advanced Parkinson's disease (PD) that has undergone technological evolution that parallels an expansion in clinical phenotyping, neurophysiology, and neuroimaging of the disease state. Machine learning (ML) has been successfully used in a wide range of healthcare problems, including DBS. As computational power increases and more data become available, the application of ML in DBS is expected to grow. We review the literature of ML in DBS and discuss future opportunities for such applications. Specifically, we perform a comprehensive review of the literature from PubMed, the Institute for Scientific Information's Web of Science, Cochrane Database of Systematic Reviews, and Institute of Electrical and Electronics Engineers' (IEEE) Xplore Digital Library for ML applications in DBS. These studies are broadly placed in the following categories: (1) DBS candidate selection; (2) programming optimization; (3) surgical targeting; and (4) insights into DBS mechanisms. For each category, we provide and contextualize the current body of research and discuss potential future directions for the application of ML in DBS.",
33139605,Role of Artificial Intelligence in Fighting Antimicrobial Resistance in Pediatrics,2020 Nov 1;9(11):767.,"Artificial intelligence (AI) is a field of science and engineering concerned with the computational understanding of what is commonly called intelligent behavior. AI is extremely useful in many human activities including medicine. The aim of our narrative review is to show the potential role of AI in fighting antimicrobial resistance in pediatric patients. We searched for PubMed articles published from April 2010 to April 2020 containing the keywords ""artificial intelligence"", ""machine learning"", ""antimicrobial resistance"", ""antimicrobial stewardship"", ""pediatric"", and ""children"", and we described the different strategies for the application of AI in these fields. Literature analysis showed that the applications of AI in health care are potentially endless, contributing to a reduction in the development time of new antimicrobial agents, greater diagnostic and therapeutic appropriateness, and, simultaneously, a reduction in costs. Most of the proposed AI solutions for medicine are not intended to replace the doctor's opinion or expertise, but to provide a useful tool for easing their work. Considering pediatric infectious diseases, AI could play a primary role in fighting antibiotic resistance. In the pediatric field, a greater willingness to invest in this field could help antimicrobial stewardship reach levels of effectiveness that were unthinkable a few years ago.",3.0
33136182,CT-based radiomics for differentiating renal tumours: a systematic review,2020 Nov 2.,"Purpose:                    Differentiating renal tumours into grades and tumour subtype from medical imaging is important for patient management; however, there is an element of subjectivity when performed qualitatively. Quantitative analysis such as radiomics may provide a more objective approach. The purpose of this article is to systematically review the literature on computed tomography (CT) radiomics for grading and differentiating renal tumour subtypes. An educational perspective will also be provided.              Methods:                    The Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist was followed. PubMed, Scopus and Web of Science were searched for relevant articles. The quality of each study was assessed using the Radiomic Quality Score (RQS).              Results:                    13 studies were found. The main outcomes were prediction of pathological grade and differentiating between renal tumour types, measured as area under the curve (AUC) for either the receiver operator curve or precision recall curve. Features extracted to predict pathological grade or tumour subtype included shape, intensity, texture and wavelet (a type of higher order feature). Four studies differentiated between low-grade and high-grade clear cell renal cell cancer (RCC) with good performance (AUC = 0.82-0.978). One other study differentiated low- and high-grade chromophobe with AUC = 0.84. Finally, eight studies used radiomics to differentiate between tumour types such as clear cell RCC, fat-poor angiomyolipoma, papillary RCC, chromophobe RCC and renal oncocytoma with high levels of performance (AUC 0.82-0.96).              Conclusion:                    Renal tumours can be pathologically classified using CT-based radiomics with good performance. The main radiomic feature used for tumour differentiation was texture. Fuhrman was the most common pathologic grading system used in the reviewed studies. Renal tumour grading studies should be extended beyond clear cell RCC and chromophobe RCC. Further research with larger prospective studies, performed in the clinical setting, across multiple institutions would help with clinical translation to the radiologist's workstation.",2.0
33135368,Engineering Tobacco Mosaic Virus and Its Virus-Like-Particles for Synthesis of Biotemplated Nanomaterials,2021 Apr;16(4):e2000311.,"Biomolecules are increasingly attractive templates for the synthesis of functional nanomaterials. Chief among them is the plant tobacco mosaic virus (TMV) due to its high aspect ratio, narrow size distribution, diverse biochemical functionalities presented on the surface, and compatibility with a number of chemical conjugations. These properties are also easily manipulated by genetic modification to enable the synthesis of a range of metallic and non-metallic nanomaterials for diverse applications. This article reviews the characteristics of TMV and related viruses, and their virus-like particle (VLP) derivatives, and how these may be manipulated to extend their use and function. A focus of recent efforts has been on greater understanding and control of the self-assembly processes that drive biotemplate formation. How these features have been exploited in engineering applications such as, sensing, catalysis, and energy storage are briefly outlined. While control of VLP surface features is well-established, fewer tools exist to control VLP self-assembly, which limits efforts to control template uniformity and synthesis of certain templated nanomaterials. However, emerging advances in synthetic biology, machine learning, and other fields promise to accelerate efforts to control template uniformity and nanomaterial synthesis enabling more widescale industrial use of VLP-based biotemplates.",1.0
33134890,Integrating Machine Learning with Human Knowledge,2020 Oct 9;23(11):101656.,"Machine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.",2.0
33133423,Deep metabolome: Applications of deep learning in metabolomics,2020 Oct 1;18:2818-2825.,"In the past few years, deep learning has been successfully applied to various omics data. However, the applications of deep learning in metabolomics are still relatively low compared to others omics. Currently, data pre-processing using convolutional neural network architecture appears to benefit the most from deep learning. Compound/structure identification and quantification using artificial neural network/deep learning performed relatively better than traditional machine learning techniques, whereas only marginally better results are observed in biological interpretations. Before deep learning can be effectively applied to metabolomics, several challenges should be addressed, including metabolome-specific deep learning architectures, dimensionality problems, and model evaluation regimes.",1.0
33131715,Parsing the Functional Impact of Noncoding Genetic Variants in the Brain Epigenome,2021 Jan 1;89(1):65-75.,"The heritability of common psychiatric disorders has motivated global efforts to identify risk-associated genetic variants and elucidate molecular pathways connecting DNA sequence to disease-associated brain dysfunction. The overrepresentation of risk variants among gene regulatory loci instead of protein-coding loci, however, poses a unique challenge in discerning which among the many thousands of variants identified contribute functionally to disease etiology. Defined broadly, psychiatric epigenomics seeks to understand the effects of disease-associated genetic variation on functional readouts of chromatin in an effort to prioritize variants in terms of their impact on gene expression in the brain. Here, we provide an overview of epigenomic mapping in the human brain and highlight findings of particular relevance to psychiatric genetics. Computational methods, including convolutional neuronal networks, and other machine learning approaches hold great promise for elucidating the functional impact of both common and rare genetic variants, thereby refining the epigenomic architecture of psychiatric disorders and enabling integrative analyses of regulatory noncoding variants in the context of large population-level genome and phenome databases.",
33130528,Machine learning for suicidology: A practical review of exploratory and hypothesis-driven approaches,2020 Dec;82:101940.,"Machine learning is being used to discover models to predict the progression from suicidal ideation to action in clinical populations. While quantifiable improvements in prediction accuracy have been achieved over theory-driven efforts, models discovered through machine learning continue to fall short of clinical relevance. Thus, the value of machine learning for reaching this objective is hotly contested. We agree that machine learning, treated as a ""black box"" approach antithetical to theory-building, will not discover clinically relevant models of suicide. However, such models may be developed through deliberate synthesis of data- and theory-driven approaches. By providing an accessible overview of essential concepts and common methods, we highlight how generalizable models and scientific insight may be obtained by incorporating prior knowledge and expectations to machine learning research, drawing examples from suicidology. We then discuss challenges investigators will face when using machine learning to discover models of low prevalence outcomes, such as suicide.",
33129349,"THE FUTURE OF MEDICINE, healthcare innovation through precision medicine: policy case study of Qatar",2020 Nov 1;16(1):12.,"In 2016, the World Innovation Summit for Health (WISH) published its Forum Report on precision medicine ""PRECISION MEDICINE - A GLOBAL ACTION PLAN FOR IMPACT"". Healthcare is undergoing a transformation, and it is imperative to leverage new technologies to generate new data and support the advent of precision medicine (PM). Recent scientific breakthroughs and technological advancements have improved our disease knowledge and altered diagnosis and treatment approaches resulting in a more precise, predictive, preventative and personalized health care that is customized for the individual patient. Consequently, the big data revolution has provided an opportunity to apply artificial intelligence and machine learning algorithms to mine such a vast data set. Additionally, personalized medicine promises to revolutionize healthcare, with its key goal of providing the right treatment to the right patient at the right time and dose, and thus the potential of improving quality of life and helping to bring down healthcare costs.This policy briefing will look in detail at the issues surrounding continued development, sustained investment, risk factors, testing and approval of innovations for better strategy and faster process. The paper will serve as a policy bridge that is required to enhance a conscious decision among the powers-that-be in Qatar in order to find a way to harmonize multiple strands of activity and responsibility in the health arena. The end goal will be for Qatar to enhance public awareness and engagement and to integrate effectively the incredible advances in research into healthcare systems, for the benefit of all patients.The PM policy briefing provides concrete recommendations on moving forward with PM initiatives in Qatar and internationally. Equally important, integration of PM within a primary care setting, building a coalition of community champions through awareness and advocacy, finally, communicating PM value, patient engagement/empowerment and education/continued professional development programs of the healthcare workforce.Key recommendations for implementation of precision medicine inside and outside Qatar: 1. Create Community Awareness and PM Education Programs 2. Engage and Empower Patients 3. Communicate PM Value 4. Develop appropriate Infrastructure and Information Management Systems 5. Integrate PM into standard Healthcare System and Ensure Access to Care PM is no longer futuristic. It is here. Implementing PM in routine clinical care does require some investment and infrastructure development. Invariably, cost and lack of expertise are cited as barriers to PM implementation. Equally consequential, are the curriculum and professional development of medical care experts.Policymakers need to lead and coordinate effort among stakeholders and consider cultural and faith perspectives to ensure success. It is essential that policymakers integrate PM approaches into national strategies to improve health and health care for all, and to drive towards the future of medicine precision health.",1.0
33127296,Transforming vaccine development,2020 Aug;50:101413.,"The urgency to develop vaccines against Covid-19 is putting pressure on the long and expensive development timelines that are normally required for development of lifesaving vaccines. There is a unique opportunity to take advantage of new technologies, the smart and flexible design of clinical trials, and evolving regulatory science to speed up vaccine development against Covid-19 and transform vaccine development altogether.",1.0
33126123,Towards a comprehensive pipeline to identify and functionally annotate long noncoding RNA (lncRNA),2020 Dec;127:104028.,"Long noncoding RNAs (lncRNAs) are implicated in various genetic diseases and cancer, attributed to their critical role in gene regulation. They are a divergent group of RNAs and are easily differentiated from other types with unique characteristics, functions, and mechanisms of action. In this review, we provide a list of some of the prominent data repositories containing lncRNAs, their interactome, and predicted and validated disease associations. Next, we discuss various wet-lab experiments formulated to obtain the data for these repositories. We also provide a critical review of in silico methods available for the identification purpose and suggest techniques to further improve their performance. The bulk of the methods currently focus on distinguishing lncRNA transcripts from the coding ones. Functional annotation of these transcripts still remains a grey area and more efforts are needed in that space. Finally, we provide details of current progress, discuss impediments, and illustrate a roadmap for developing a generalized computational pipeline for comprehensive annotation of lncRNAs, which is essential to accelerate research in this area.",2.0
33120976,Automatic Gene Function Prediction in the 2020's,2020 Oct 27;11(11):1264.,"The current rate at which new DNA and protein sequences are being generated is too fast to experimentally discover the functions of those sequences, emphasizing the need for accurate Automatic Function Prediction (AFP) methods. AFP has been an active and growing research field for decades and has made considerable progress in that time. However, it is certainly not solved. In this paper, we describe challenges that the AFP field still has to overcome in the future to increase its applicability. The challenges we consider are how to: (1) include condition-specific functional annotation, (2) predict functions for non-model species, (3) include new informative data sources, (4) deal with the biases of Gene Ontology (GO) annotations, and (5) maximally exploit the GO to obtain performance gains. We also provide recommendations for addressing those challenges, by adapting (1) the way we represent proteins and genes, (2) the way we represent gene functions, and (3) the algorithms that perform the prediction from gene to function. Together, we show that AFP is still a vibrant research area that can benefit from continuing advances in machine learning with which AFP in the 2020s can again take a large step forward reinforcing the power of computational biology.",
33120974,A Systematic Review of Machine Learning Techniques in Hematopoietic Stem Cell Transplantation (HSCT),2020 Oct 27;20(21):6100.,"Machine learning techniques are widely used nowadays in the healthcare domain for the diagnosis, prognosis, and treatment of diseases. These techniques have applications in the field of hematopoietic cell transplantation (HCT), which is a potentially curative therapy for hematological malignancies. Herein, a systematic review of the application of machine learning (ML) techniques in the HCT setting was conducted. We examined the type of data streams included, specific ML techniques used, and type of clinical outcomes measured. A systematic review of English articles using PubMed, Scopus, Web of Science, and IEEE Xplore databases was performed. Search terms included ""hematopoietic cell transplantation (HCT),"" ""autologous HCT,"" ""allogeneic HCT,"" ""machine learning,"" and ""artificial intelligence."" Only full-text studies reported between January 2015 and July 2020 were included. Data were extracted by two authors using predefined data fields. Following PRISMA guidelines, a total of 242 studies were identified, of which 27 studies met the inclusion criteria. These studies were sub-categorized into three broad topics and the type of ML techniques used included ensemble learning (63%), regression (44%), Bayesian learning (30%), and support vector machine (30%). The majority of studies examined models to predict HCT outcomes (e.g., survival, relapse, graft-versus-host disease). Clinical and genetic data were the most commonly used predictors in the modeling process. Overall, this review provided a systematic review of ML techniques applied in the context of HCT. The evidence is not sufficiently robust to determine the optimal ML technique to use in the HCT setting and/or what minimal data variables are required.",1.0
33120345,VIRS based detection in combination with machine learning for mapping soil pollution,2021 Jan 1;268(Pt A):115845.,"Widespread soil contamination threatens living standards and weakens global efforts towards the Sustainable Development Goals (SDGs). Detailed soil mapping is needed to guide effective countermeasures and sustainable remediation operations. Here, we review visible and infrared reflectance spectroscopy (VIRS) based detection methods in combination with machine learning. To date, proximal, airborne and spaceborne carrier devices have been employed for soil contamination detection, allowing large areas to be covered at low cost and with minimal secondary environmental impact. In this way, soil contaminants can be monitored remotely, either directly or through correlation with soil components (e.g. Fe-oxides, soil organic matter, clay minerals). Observed vegetation reflectance spectra has also been proven an effective indicator for mapping soil pollution. Calibration models based on machine learning are used to interpret spectral data and predict soil contamination levels. The algorithms used for this include partial least squares regression, neural networks, and random forest. The processes underlying each of these approaches are outlined in this review. Finally, current challenges and future research directions are explored and discussed.",
33120319,Findings from machine learning in clinical medical imaging applications - Lessons for translation to the forensic setting,2020 Nov;316:110538.,"Machine learning (ML) techniques are increasingly being used in clinical medical imaging to automate distinct processing tasks. In post-mortem forensic radiology, the use of these algorithms presents significant challenges due to variability in organ position, structural changes from decomposition, inconsistent body placement in the scanner, and the presence of foreign bodies. Existing ML approaches in clinical imaging can likely be transferred to the forensic setting with careful consideration to account for the increased variability and temporal factors that affect the data used to train these algorithms. Additional steps are required to deal with these issues, by incorporating the possible variability into the training data through data augmentation, or by using atlases as a pre-processing step to account for death-related factors. A key application of ML would be then to highlight anatomical and gross pathological features of interest, or present information to help optimally determine the cause of death. In this review, we highlight results and limitations of applications in clinical medical imaging that use ML to determine key implications for their application in the forensic setting.",
33120200,Knowledge gaps in immune response and immunotherapy involving nanomaterials: Databases and artificial intelligence for material design,2021 Jan;266:120469.,"Exploring the interactions between the immune system and nanomaterials (NMs) is critical for designing effective and safe NMs, but large knowledge gaps remain to be filled prior to clinical applications (e.g., immunotherapy). The lack of databases on interactions between the immune system and NMs affects the discovery of new NMs for immunotherapy. Complement activation and inhibition by NMs have been widely studied, but the general rules remain unclear. Biomimetic nanocoating to promote the clearance of NMs by the immune system is an alternative strategy for the immune response mediation of the biological corona. Immune response predictions based on NM properties can facilitate the design of NMs for immunotherapy, and artificial intelligences deserve much attention in the field. This review addresses the knowledge gaps regarding immune response and immunotherapy in relation to NMs, effective immunotherapy and material design without adverse immune responses.",2.0
33120126,LMI-DForest: A deep forest model towards the prediction of lncRNA-miRNA interactions,2020 Dec;89:107406.,"The interactions between miRNAs and long non-coding RNAs (lncRNAs) are subject to intensive recent studies due to its critical role in gene regulations. Computational prediction of lncRNA-miRNA interactions has become a popular alternative strategy to the experimental methods for identification of underlying interactions. It is desirable to develop the machine learning-based models for prediction of lncRNA-miRNA based on the experimentally validated interactions between lncRNAs and miRNAs. The accuracy and robustness of existing models based on machine learning techniques are subject to further improvement. Considering that the attributes of lncRNA and miRNA contribute key importance in the interaction between these two RNAs, a deep learning model, named LMI-DForest, is proposed here by combining the deep forest and autoencoder strategies. Systematic comparison on the experiment validated datasets for lncRNA-miRNA interaction datasets demonstrates that the proposed method consistently shows superior performance over the other machine learning models in the lncRNA-miRNA interaction prediction.",1.0
33118431,Application of Artificial Intelligence in Dentistry,2021 Mar;100(3):232-244.,"Artificial intelligence (AI) is a technology that utilizes machines to mimic intelligent human behavior. To appreciate human-technology interaction in the clinical setting, augmented intelligence has been proposed as a cognitive extension of AI in health care, emphasizing its assistive and supplementary role to medical professionals. While truly autonomous medical robotic systems are still beyond reach, the virtual component of AI, known as software-type algorithms, is the main component used in dentistry. Because of their powerful capabilities in data analysis, these virtual algorithms are expected to improve the accuracy and efficacy of dental diagnosis, provide visualized anatomic guidance for treatment, simulate and evaluate prospective results, and project the occurrence and prognosis of oral diseases. Potential obstacles in contemporary algorithms that prevent routine implementation of AI include the lack of data curation, sharing, and readability; the inability to illustrate the inner decision-making process; the insufficient power of classical computing; and the neglect of ethical principles in the design of AI frameworks. It is necessary to maintain a proactive attitude toward AI to ensure its affirmative development and promote human-technology rapport to revolutionize dental practice. The present review outlines the progress and potential dental applications of AI in medical-aided diagnosis, treatment, and disease prediction and discusses their data limitations, interpretability, computing power, and ethical considerations, as well as their impact on dentists, with the objective of creating a backdrop for future research in this rapidly expanding arena.",2.0
33118327,Biophysical analysis of SARS-CoV-2 transmission and theranostic development via N protein computational characterization,2021 Mar;37(2):e3096.,"Recently, SARS-CoV-2 has been identified as the causative factor of viral infection called COVID-19 that belongs to the zoonotic beta coronavirus family known to cause respiratory disorders or viral pneumonia, followed by an extensive attack on organs that express angiotensin-converting enzyme II (ACE2). Human transmission of this virus occurs via respiratory droplets from symptomatic and asymptomatic patients, which are released into the environment after sneezing or coughing. These droplets are capable of staying in the air as aerosols or surfaces and can be transmitted to persons through inhalation or contact with contaminated surfaces. Thus, there is an urgent need for advanced theranostic solutions to control the spread of COVID-19 infection. The development of such fit-for-purpose technologies hinges on a proper understanding of the transmission, incubation, and structural characteristics of the virus in the external environment and within the host. Hence, this article describes the development of an intrinsic model to describe the incubation characteristics of the virus under varying environmental factors. It also discusses on the evaluation of SARS-CoV-2 structural nucleocapsid protein properties via computational approaches to generate high-affinity binding probes for effective diagnosis and targeted treatment applications by specific targeting of viruses. In addition, this article provides useful insights on the transmission behavior of the virus and creates new opportunities for theranostics development.",
33117669,Application of Radiomics for the Prediction of Radiation-Induced Toxicity in the IMRT Era: Current State-of-the-Art,2020 Oct 6;10:1708.,"Normal tissue complication probability (NTCP) models that were formulated in the Quantitative Analyses of Normal Tissue Effects in the Clinic (QUANTEC) are one of the pillars in support of everyday's clinical radiation oncology. Because of steady therapeutic refinements and the availability of cutting-edge technical solutions, the ceiling of organs-at-risk-sparing has been reached for photon-based intensity modulated radiotherapy (IMRT). The possibility to capture heterogeneity of patients and tissues in the prediction of toxicity is still an unmet need in modern radiation therapy. Potentially, a major step towards a wider therapeutic index could be obtained from refined assessment of radiation-induced morbidity at an individual level. The rising integration of quantitative imaging and machine learning applications into radiation oncology workflow offers an unprecedented opportunity to further explore the biologic interplay underlying the normal tissue response to radiation. Based on these premises, in this review we focused on the current-state-of-the-art on the use of radiomics for the prediction of toxicity in the field of head and neck, lung, breast and prostate radiotherapy.",1.0
33117259,Role of Artificial Intelligence in TeleStroke: An Overview,2020 Oct 7;11:559322.,"Teleneurology has provided access to neurological expertise and state-of-the-art stroke care where previously they have been inaccessible. The use of Artificial Intelligence with machine learning to assist telestroke care can be revolutionary. This includes more rapid and more reliable diagnosis through imaging analysis as well as prediction of hospital course and 3-month prognosis. Intelligent Electronic Medical Records can search free text and provide decision assistance by analyzing patient charts. Speech recognition has advanced enough to be reliable and highly convenient. Smart contextually aware communication and alert programs can enhance efficiency of patient flow and improve outcomes. Automated data collection and analysis can make quality improvement and research projects quicker and much less burdensome. Despite current challenges, these synergistic technologies hold immense promise in enhancing the clinician experience, helping to reduce physician burnout while improving patient health outcomes at a lower cost. This brief overview discusses the multifaceted potential of AI use in telestroke.",
33117114,A Survey on Deep Learning for Neuroimaging-Based Brain Disorder Analysis,2020 Oct 8;14:779.,"Deep learning has recently been used for the analysis of neuroimages, such as structural magnetic resonance imaging (MRI), functional MRI, and positron emission tomography (PET), and it has achieved significant performance improvements over traditional machine learning in computer-aided diagnosis of brain disorders. This paper reviews the applications of deep learning methods for neuroimaging-based brain disorder analysis. We first provide a comprehensive overview of deep learning techniques and popular network architectures by introducing various types of deep neural networks and recent developments. We then review deep learning methods for computer-aided analysis of four typical brain disorders, including Alzheimer's disease, Parkinson's disease, Autism spectrum disorder, and Schizophrenia, where the first two diseases are neurodegenerative disorders and the last two are neurodevelopmental and psychiatric disorders, respectively. More importantly, we discuss the limitations of existing studies and present possible future directions.",1.0
33115272,Computationally Driven Discovery in Coagulation,2021 Jan;41(1):79-86.,"Bleeding frequency and severity within clinical categories of hemophilia A are highly variable and the origin of this variation is unknown. Solving this mystery in coagulation requires the generation and analysis of large data sets comprised of experimental outputs or patient samples, both of which are subject to limited availability. In this review, we describe how a computationally driven approach bypasses such limitations by generating large synthetic patient data sets. These data sets were created with a mechanistic mathematical model, by varying the model inputs, clotting factor, and inhibitor concentrations, within normal physiological ranges. Specific mathematical metrics were chosen from the model output, used as a surrogate measure for bleeding severity, and statistically analyzed for further exploration and hypothesis generation. We highlight results from our recent study that employed this computationally driven approach to identify FV (factor V) as a key modifier of thrombin generation in mild to moderate hemophilia A, which was confirmed with complementary experimental assays. The mathematical model was used further to propose a potential mechanism for these observations whereby thrombin generation is rescued in FVIII-deficient plasma due to reduced substrate competition between FV and FVIII for FXa (activated factor X).",
33114254,An Optimal Time for Treatment-Predicting Circadian Time by Machine Learning and Mathematical Modelling,2020 Oct 23;12(11):3103.,"Tailoring medical interventions to a particular patient and pathology has been termed personalized medicine. The outcome of cancer treatments is improved when the intervention is timed in accordance with the patient's internal time. Yet, one challenge of personalized medicine is how to consider the biological time of the patient. Prerequisite for this so-called chronotherapy is an accurate characterization of the internal circadian time of the patient. As an alternative to time-consuming measurements in a sleep-laboratory, recent studies in chronobiology predict circadian time by applying machine learning approaches and mathematical modelling to easier accessible observables such as gene expression. Embedding these results into the mathematical dynamics between clock and cancer in mammals, we review the precision of predictions and the potential usage with respect to cancer treatment and discuss whether the patient's internal time and circadian observables, may provide an additional indication for individualized treatment timing. Besides the health improvement, timing treatment may imply financial advantages, by ameliorating side effects of treatments, thus reducing costs. Summarizing the advances of recent years, this review brings together the current clinical standard for measuring biological time, the general assessment of circadian rhythmicity, the usage of rhythmic variables to predict biological time and models of circadian rhythmicity.",1.0
33110340,A comprehensive survey of AI-enabled phishing attacks detection techniques,2020 Oct 23;1-16.,"In recent times, a phishing attack has become one of the most prominent attacks faced by internet users, governments, and service-providing organizations. In a phishing attack, the attacker(s) collects the client's sensitive data (i.e., user account login details, credit/debit card numbers, etc.) by using spoofed emails or fake websites. Phishing websites are common entry points of online social engineering attacks, including numerous frauds on the websites. In such types of attacks, the attacker(s) create website pages by copying the behavior of legitimate websites and sends URL(s) to the targeted victims through spam messages, texts, or social networking. To provide a thorough understanding of phishing attack(s), this paper provides a literature review of Artificial Intelligence (AI) techniques: Machine Learning, Deep Learning, Hybrid Learning, and Scenario-based techniques for phishing attack detection. This paper also presents the comparison of different studies detecting the phishing attack for each AI technique and examines the qualities and shortcomings of these methodologies. Furthermore, this paper provides a comprehensive set of current challenges of phishing attacks and future research direction in this domain.",1.0
33106154,Literature mining for context-specific molecular relations using multimodal representations (COMMODAR),2020 Oct 26;21(Suppl 5):250.,"Biological contextual information helps understand various phenomena occurring in the biological systems consisting of complex molecular relations. The construction of context-specific relational resources vastly relies on laborious manual extraction from unstructured literature. In this paper, we propose COMMODAR, a machine learning-based literature mining framework for context-specific molecular relations using multimodal representations. The main idea of COMMODAR is the feature augmentation by the cooperation of multimodal representations for relation extraction. We leveraged biomedical domain knowledge as well as canonical linguistic information for more comprehensive representations of textual sources. The models based on multiple modalities outperformed those solely based on the linguistic modality. We applied COMMODAR to the 14 million PubMed abstracts and extracted 9214 context-specific molecular relations. All corpora, extracted data, evaluation results, and the implementation code are downloadable at https://github.com/jae-hyun-lee/commodar . CCS CONCEPTS: • Computing methodologies~Information extraction • Computing methodologies~Neural networks • Applied computing~Biological networks.",
33101385,Predicting Thermal Adaptation by Looking Into Populations' Genomic Past,2020 Sep 25;11:564515.,"Molecular evolution offers an insightful theory to interpret the genomic consequences of thermal adaptation to previous events of climate change beyond range shifts. However, disentangling often mixed footprints of selective and demographic processes from those due to lineage sorting, recombination rate variation, and genomic constrains is not trivial. Therefore, here we condense current and historical population genomic tools to study thermal adaptation and outline key developments (genomic prediction, machine learning) that might assist their utilization for improving forecasts of populations' responses to thermal variation. We start by summarizing how recent thermal-driven selective and demographic responses can be inferred by coalescent methods and in turn how quantitative genetic theory offers suitable multi-trait predictions over a few generations via the breeder's equation. We later assume that enough generations have passed as to display genomic signatures of divergent selection to thermal variation and describe how these footprints can be reconstructed using genome-wide association and selection scans or, alternatively, may be used for forward prediction over multiple generations under an infinitesimal genomic prediction model. Finally, we move deeper in time to comprehend the genomic consequences of thermal shifts at an evolutionary time scale by relying on phylogeographic approaches that allow for reticulate evolution and ecological parapatric speciation, and end by envisioning the potential of modern machine learning techniques to better inform long-term predictions. We conclude that foreseeing future thermal adaptive responses requires bridging the multiple spatial scales of historical and predictive environmental change research under modern cohesive approaches such as genomic prediction and machine learning frameworks.",3.0
33098140,The application of artificial intelligence for the diagnosis and treatment of liver diseases,2020 Oct 23.,"Modern medical care produces large volumes of multi-modal patient data, which many clinicians struggle to process and synthesize into actionable knowledge. In recent years, artificial intelligence (AI) has emerged as an effective tool in this regard. The field of hepatology is no exception, with a growing number of studies published that apply artificial intelligence techniques to the diagnosis and treatment of liver diseases. These have included Machine Learning algorithms (such as regression models, Bayesian networks, and support vector machines) to predict disease progression, the presence of complications, and mortality; Deep Learning algorithms to enable rapid, automated interpretation of radiologic and pathologic images; and Natural Language Processing to extract clinically meaningful concepts from vast quantities of unstructured data in Electronic Health Records. This review article will provide a comprehensive overview of hepatology-focused AI research, discuss some of the barriers to clinical implementation and adoption, and suggest future directions for the field.",
33098123,Current status of artificial intelligence analysis for endoscopic ultrasonography,2021 Jan;33(2):298-305.,"Endoscopic ultrasonography (EUS) is an essential diagnostic tool for various types of pancreatic diseases such as pancreatic tumors and chronic pancreatitis; however, EUS imaging has low specificity for the diagnosis of pancreatic diseases. Artificial intelligence (AI) is a mathematical prediction technique that automates learning and recognizes patterns in data. This review describes the details and principles of AI and deep learning algorithms. The term AI does not have any definite definition; almost all AI systems fall under narrow AI, which can handle single or limited tasks. Deep learning is based on neural networks, which is a machine learning technique that is widely used in the medical field. Deep learning involves three phases: data collection and annotation, building the deep learning architecture, and training and ability validation. For medical image diagnosis, image classification, object detection, and semantic segmentation are performed. In EUS, AI is used for detecting anatomical features, differential pancreatic tumors, and cysts. For this, conventional machine learning architectures are used, and deep learning architecture has been used in only two reports. Although the diagnostic abilities in these reports were about 85-95%, these were exploratory research and very few reports have included substantial evidence. AI is increasingly being used for medical image diagnosis due to its high performance and will soon become an essential technique for medical diagnosis.",4.0
33096595,Digital Twin Coaching for Physical Activities: A Survey,2020 Oct 21;20(20):5936.,"Digital Twin technology has been rising in popularity thanks to the popularity of machine learning in the last decade. As the life expectancy of people around the world is increasing, so is the focus on physical activity to remain healthy especially in the current times where people are staying sedentary while in quarantine. This article aims to provide a survey on the field of Digital Twin technology focusing on machine learning and coaching techniques as they have not been explored yet. We also define what Digital Twin Coaching is and categorize the work done so far in terms of the objective of the physical activity. We also list common Digital Twin Coaching characteristics found in the articles reviewed in terms of concepts such as interactivity, privacy and security and also detail future perspectives in multimodal interaction and standardization, to name a few, that can guide researchers if they choose to work in this field. Finally, we provide a diagram for the Digital Twin Ecosystem showing the interaction between relevant entities and the information flow as well as an idealization of an ideal Digital Twin Ecosystem for team sports' athlete tracking.",
33094899,Machine Learning in the Prediction of Medical Inpatient Length of Stay,2020 Oct 23.,"Background:                    Length of stay (LOS) estimates are important for patients, doctors and hospital administrators. However, making accurate estimates of LOS can be difficult for medical patients.              Aims:                    This review was conducted with the aim of identifying and assessing previous studies on the application of machine learning to the prediction of total hospital inpatient LOS for medical patients.              Methods:                    A review of machine learning in the prediction of total hospital LOS for medical inpatients was conducted using the databases PubMed, EMBASE and Web of Science.              Results:                    Of the 673 publications returned by the initial search, 21 articles met inclusion criteria. Of these articles the most commonly represented medical specialty was cardiology. Studies were also identified that had specifically evaluated machine learning LOS prediction in patients with diabetes and tuberculosis. The performance of the machine learning models in the identified studies varied significantly depending on factors including differing input datasets and different LOS thresholds and outcome metrics. Common methodological shortcomings included a lack of reporting of patient demographics and lack of reporting of clinical details of included patients.              Conclusions:                    The variable performance reported by the studies identified in this review supports the need for further research of the utility of machine learning in the prediction of total inpatient LOS in medical patients. Future studies should follow and report a more standardised methodology to better assess performance and to allow replication and validation. In particular, prospective validation studies and studies assessing the clinical impact of such machine learning models would be beneficial. This article is protected by copyright. All rights reserved.",
33094613,Cannabinoid Receptor Subtype 2 (CB2R) in a Multitarget Approach: Perspective of an Innovative Strategy in Cancer and Neurodegeneration,2020 Dec 10;63(23):14448-14469.,"The cannabinoid receptor subtype 2 (CB2R) represents an interesting and new therapeutic target for its involvement in the first steps of neurodegeneration as well as in cancer onset and progression. Several studies, focused on different types of tumors, report a promising anticancer activity induced by CB2R agonists due to their ability to reduce inflammation and cell proliferation. Moreover, in neuroinflammation, the stimulation of CB2R, overexpressed in microglial cells, exerts beneficial effects in neurodegenerative disorders. With the aim to overcome current treatment limitations, new drugs can be developed by specifically modulating, together with CB2R, other targets involved in such multifactorial disorders. Building on successful case studies of already developed multitarget strategies involving CB2R, in this Perspective we aim at prompting the scientific community to consider new promising target associations involving HDACs (histone deacetylases) and σ receptors by employing modern approaches based on molecular hybridization, computational polypharmacology, and machine learning algorithms.",
33094391,Trend analysis of global usage of digital soil mapping models in the prediction of potentially toxic elements in soil/sediments: a bibliometric review,2021 May;43(5):1715-1739.,"The rising and continuous pollution of the soil from anthropogenic activities is of great concern. Owing to this concern, the advent of digital soil mapping (DSM) has been a tool that soil scientists use in this era to predict the potentially toxic element (PTE) content in the soil. The purpose of this paper was to conduct a review of articles, summarize and analyse the spatial prediction of potentially toxic elements, determine and compare the models' usage as well as their performance over time. Through Scopus, the Web of Science and Google Scholar, we collected papers between the year 2001 and the first quarter of 2019, which were tailored towards the spatial PTE prediction using DSM approaches. The results indicated that soil pollution emanates from diverse sources. However, it provided reasons why the authors investigate a piece of land or area, highlighting the uncertainties in mapping, number of publications per journal and continental efforts to research as well as published on trending issues regarding DSM. This paper reveals the complementary role machine learning algorithms and the geostatistical models play in DSM. Nevertheless, geostatistical approaches remain the most preferred model compared to machine learning algorithms.",
33094213,Artificial intelligence for brain diseases: A systematic review,2020 Oct 13;4(4):041503.,"Artificial intelligence (AI) is a major branch of computer science that is fruitfully used for analyzing complex medical data and extracting meaningful relationships in datasets, for several clinical aims. Specifically, in the brain care domain, several innovative approaches have achieved remarkable results and open new perspectives in terms of diagnosis, planning, and outcome prediction. In this work, we present an overview of different artificial intelligent techniques used in the brain care domain, along with a review of important clinical applications. A systematic and careful literature search in major databases such as Pubmed, Scopus, and Web of Science was carried out using ""artificial intelligence"" and ""brain"" as main keywords. Further references were integrated by cross-referencing from key articles. 155 studies out of 2696 were identified, which actually made use of AI algorithms for different purposes (diagnosis, surgical treatment, intra-operative assistance, and postoperative assessment). Artificial neural networks have risen to prominent positions among the most widely used analytical tools. Classic machine learning approaches such as support vector machine and random forest are still widely used. Task-specific algorithms are designed for solving specific problems. Brain images are one of the most used data types. AI has the possibility to improve clinicians' decision-making ability in neuroscience applications. However, major issues still need to be addressed for a better practical use of AI in the brain. To this aim, it is important to both gather comprehensive data and build explainable AI algorithms.",
33093807,Evolution of Sequence-based Bioinformatics Tools for Protein-protein Interaction Prediction,2020 Sep;21(6):454-463.,"Protein-protein interactions (PPIs) are the physical connections between two or more proteins via electrostatic forces or hydrophobic effects. Identification of the PPIs is pivotal, which contributes to many biological processes including protein function, disease incidence, and therapy design. The experimental identification of PPIs via high-throughput technology is time-consuming and expensive. Bioinformatics approaches are expected to solve such restrictions. In this review, our main goal is to provide an inclusive view of the existing sequence-based computational prediction of PPIs. Initially, we briefly introduce the currently available PPI databases and then review the state-of-the-art bioinformatics approaches, working principles, and their performances. Finally, we discuss the caveats and future perspective of the next generation algorithms for the prediction of PPIs.",2.0
33092893,Distributional Reinforcement Learning in the Brain,2020 Dec;43(12):980-997.,"Learning about rewards and punishments is critical for survival. Classical studies have demonstrated an impressive correspondence between the firing of dopamine neurons in the mammalian midbrain and the reward prediction errors of reinforcement learning algorithms, which express the difference between actual reward and predicted mean reward. However, it may be advantageous to learn not only the mean but also the complete distribution of potential rewards. Recent advances in machine learning have revealed a biologically plausible set of algorithms for reconstructing this reward distribution from experience. Here, we review the mathematical foundations of these algorithms as well as initial evidence for their neurobiological implementation. We conclude by highlighting outstanding questions regarding the circuit computation and behavioral readout of these distributional codes.",1.0
33088903,Towards accurate models for predicting smartphone applications' QoE with data from a living lab study,2020;5(1):10.,"Progressively, smartphones have become the pocket Swiss army knife for everyone. They support their users needs to accomplish tasks in numerous contexts. However, the applications executing those tasks are regularly not performing as they should, and the user-perceived experience is altered. In this paper, we present our approach to model and predict the Quality of Experience (QoE) of mobile applications used over WiFi or cellular network. We aimed to create predictive QoE models and to derive recommendations for mobile application developers to create QoE aware applications. Previous works on smartphone applications' QoE prediction only focus on qualitative or quantitative data. We collected both qualitative and quantitative data ""in the wild"" through our living lab. We ran a 4-week-long study with 38 Android phone users. We focused on frequently used and highly interactive applications. The participants rated their mobile applications' expectation and QoE and in various contexts resulting in a total of 6086 ratings. Simultaneously, our smartphone logger (mQoL-Log) collected background information such as network information, user physical activity, battery statistics, and more. We apply various data aggregation approaches and features selection processes to train multiple predictive QoE models. We obtain better model performances using ratings acquired within 14.85 minutes after the application usage. Additionally, we boost our models' performance with the users expectation as a new feature. We create an on-device prediction model with on-smartphone only features. We compare its performance metrics against the previous model. The on-device model performs below the full features models. Surprisingly, among the following top three features: the intended task to accomplish with the app, application's name (e.g., WhatsApp, Spotify), and network Quality of Service (QoS), the user physical activity is the most important feature (e.g., if walking). Finally, we share our recommendations with the application developers, and we discuss the implications of QoE and expectations in mobile application design.",
33088156,Application of artificial intelligence in the diagnosis and treatment of hepatocellular carcinoma: A review,2020 Oct 7;26(37):5617-5628.,"Although artificial intelligence (AI) was initially developed many years ago, it has experienced spectacular advances over the last 10 years for application in the field of medicine, and is now used for diagnostic, therapeutic and prognostic purposes in almost all fields. Its application in the area of hepatology is especially relevant for the study of hepatocellular carcinoma (HCC), as this is a very common tumor, with particular radiological characteristics that allow its diagnosis without the need for a histological study. However, the interpretation and analysis of the resulting images is not always easy, in addition to which the images vary during the course of the disease, and prognosis and treatment response can be conditioned by multiple factors. The vast amount of data available lend themselves to study and analysis by AI in its various branches, such as deep-learning (DL) and machine learning (ML), which play a fundamental role in decision-making as well as overcoming the constraints involved in human evaluation. ML is a form of AI based on automated learning from a set of previously provided data and training in algorithms to organize and recognize patterns. DL is a more extensive form of learning that attempts to simulate the working of the human brain, using a lot more data and more complex algorithms. This review specifies the type of AI used by the various authors. However, well-designed prospective studies are needed in order to avoid as far as possible any bias that may later affect the interpretability of the images and thereby limit the acceptance and application of these models in clinical practice. In addition, professionals now need to understand the true usefulness of these techniques, as well as their associated strengths and limitations.",
33086465,"Source tracking of antibiotic resistance genes in the environment - Challenges, progress, and prospects",2020 Oct 15;185:116127.,"Antibiotic resistance has become a global public health concern, rendering common infections untreatable. Given the widespread occurrence, increasing attention is being turned toward environmental pathways that potentially contribute to antibiotic resistance gene (ARG) dissemination outside the clinical realm. Studies during the past decade have clearly proved the increased ARG pollution trend along with gradient of anthropogenic interference, mainly through marker-ARG detection by PCR-based approaches. However, accurate source-tracking has been always confounded by various factors in previous studies, such as autochthonous ARG level, spatiotemporal variability and environmental resistome complexity, as well as inherent method limitation. The rapidly developed metagenomics profiles ARG occurrence within the sample-wide genomic context, opening a new avenue for source tracking of environmental ARG pollution. Coupling with machine-learning classification, it has been demonstrated the potential of metagenomic ARG profiles in unambiguously assigning source contribution. Through identifying indicator ARG and recovering ARG-host genomes, metagenomics-based analysis will further increase the resolution and accuracy of source tracking. In this review, challenges and progresses in source-tracking studies on environmental ARG pollution will be discussed, with specific focus on recent metagenomics-guide approaches. We propose an integrative metagenomics-based framework, in which coordinated efforts on experimental design and metagenomic analysis will assist in realizing the ultimate goal of robust source-tracking in environmental ARG pollution.",1.0
33085787,The 26th Annual Prostate Cancer Foundation Scientific Retreat Report,2021 Jan;81(1):3-19.,"Background:                    The 26th Annual Prostate Cancer Foundation (PCF) Scientific Retreat was held from October 24-26, 2019 in Carlsbad, CA.              Methods:                    The Annual PCF Scientific Retreat is a global scientific research conference that focuses on the most promising and interesting new research in the prostate cancer field, and topics arising from other fields that have the potential to impact and advance prostate cancer research and clinical care.              Results:                    The primary topic areas addressed at the 2019 PCF Retreat included: (i) new insights into prostate cancer biology and treatment; (ii) new drugs and drug targets in prostate cancer; (iii) advances in prostate cancer genomics; (iv) lessons from the multi-arm, multistage randomized phase 3 STAMPEDE trial; (v) advances in immunotherapy for prostate cancer; (vi) factors contributing to prostate cancer racial disparities; (vii) treatment-associated small-cell/neuroendocrine prostate cancer (t-SCNC); (viii) artificial intelligence and machine learning in cancer research and development; (ix) population science research on prostate cancer; and (x) prostate cancer research in the Department of Veterans Affairs.              Conclusions:                    This article reviews the presentations from the 2019 PCF Scientific Retreat. We hope that this knowledge will accelerate research leading to new understandings of prostate cancer biology and improve treatments for patients with prostate cancer.",1.0
33083571,Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines,2020 Oct 16;3:136.,"Advancements in deep learning techniques carry the potential to make significant contributions to healthcare, particularly in fields that utilize medical imaging for diagnosis, prognosis, and treatment decisions. The current state-of-the-art deep learning models for radiology applications consider only pixel-value information without data informing clinical context. Yet in practice, pertinent and accurate non-imaging data based on the clinical history and laboratory data enable physicians to interpret imaging findings in the appropriate clinical context, leading to a higher diagnostic accuracy, informative clinical decision making, and improved patient outcomes. To achieve a similar goal using deep learning, medical imaging pixel-based models must also achieve the capability to process contextual data from electronic health records (EHR) in addition to pixel data. In this paper, we describe different data fusion techniques that can be applied to combine medical imaging with EHR, and systematically review medical data fusion literature published between 2012 and 2020. We conducted a systematic search on PubMed and Scopus for original research articles leveraging deep learning for fusion of multimodality data. In total, we screened 985 studies and extracted data from 17 papers. By means of this systematic review, we present current knowledge, summarize important results and provide implementation guidelines to serve as a reference for researchers interested in the application of multimodal fusion in medical imaging.",2.0
33080488,Effector Biology of Biotrophic Plant Fungal Pathogens: Current Advances and Future Prospects,2020 Dec;241:126567.,"The interaction of fungal pathogens with their host requires a novel invading mechanism and the presence of various virulence-associated components responsible for promoting the infection. The small secretory proteins, explicitly known as effector proteins, are one of the prime mechanisms of host manipulation utilized by the pathogen to disarm the host. Several effector proteins are known to translocate from fungus to the plant cell for host manipulation. Many fungal effectors have been identified using genomic, transcriptomic, and bioinformatics approaches. Most of the effector proteins are devoid of any conserved signatures, and their prediction based on sequence homology is very challenging, therefore by combining the sequence consensus based upon machine learning features, multiple tools have also been developed for predicting apoplastic and cytoplasmic effectors. Various post-genomics approaches like transcriptomics of virulent isolates have also been utilized for identifying active consortia of effectors. Significant progress has been made in understanding biotrophic effectors; however, most of it is underway due to their complex interaction with host and complicated recognition and signaling networks. This review discusses advances, and challenges in effector identification and highlighted various features of the potential effector proteins and approaches for understanding their genetics and strategies for regulation.",1.0
33074628,"Artificial Intelligence for Classification of Lung Nodules: A Review of Clinical Utility, Diagnostic Accuracy, Cost-Effectiveness, and Guidelines [Internet]",,"A lung nodule is a small (< 30 millimetres), well defined lesion completely surrounded by pulmonary parenchyma (i.e., functional tissue of the lung).– Lung nodules are classified as solid or subsolid, and subsolid nodules are subdivided into pure ground-glass nodules (no solid component) and part-solid nodules (both ground glass and solid components)., A lesion that measures over 30 millimetres is considered a lung mass. An important distinction for the patient and treatment plan is whether the presenting lung nodule(s) are benign or malignant. For lung nodules, this appropriate classification is crucial to prevent any unnecessary procedures as well as for appropriate treatment planning (e.g., biopsy, surgical resection). It has been found that the majority of lung nodules identified on computed tomography (CT) scans are benign with a prevalence of malignancy as low as one percent for Canadians with lung nodules.,            To discern whether a lung nodule is benign or malignant, the initial evaluation usually involves a radiologist using clinical and radiographic features (often from a CT scan) to determine the likelihood of malignancy; this likelihood assists in determining further management (e.g., CT surveillance, biopsy). However, discerning malignancy from clinical and radiographic features can be challenging and novel methods are being considered, including artificial intelligence (AI).                AI is a branch of computer science concerned with the development of systems that can perform tasks that would usually require human intelligence, such as problem-solving, reasoning, and recognition.– AI is an umbrella term that includes a number of subfields and approaches., AI algorithms for reading CT scans often include a machine learning system (e.g., support vector machine [SVM], artificial neural networks [deep learning, including convolutional neural network or CNN]). Machine learning involves training an algorithm to perform tasks by learning from patterns in data rather than performing a task that it is explicitly programmed to do., In order to train the machine learning program, data are divided into learning sets (i.e., human indicates if an outcome of interest is present or absent) and validation sets (i.e., system used what it learns to indicate if the outcome of interest is present or absent).,            CADTH has previously reviewed the evidence for the use of AI for nodule classification in screening, incidental identification, or known or suspected malignancies for lung cancer via a Rapid Response Summary of Abstracts. The aim of the current report is to summarize and critically appraise the evidence initially identified in the Summary of Abstracts, based on additional screening and review of the full text of these publications.",
33073257,Statistical Hypothesis Testing versus Machine Learning Binary Classification: Distinctions and Guidelines,2020 Oct 9;1(7):100115.,"Making binary decisions is a common data analytical task in scientific research and industrial applications. In data sciences, there are two related but distinct strategies: hypothesis testing and binary classification. In practice, how to choose between these two strategies can be unclear and rather confusing. Here, we summarize key distinctions between these two strategies in three aspects and list five practical guidelines for data analysts to choose the appropriate strategy for specific analysis needs. We demonstrate the use of those guidelines in a cancer driver gene prediction example.",
33073256,"High Tech, High Risk: Tech Ethics Lessons for the COVID-19 Pandemic Response",2020 Oct 9;1(7):100102.,"The COVID-19 pandemic has, in a matter of a few short months, drastically reshaped society around the world. Because of the growing perception of machine learning as a technology capable of addressing large problems at scale, machine learning applications have been seen as desirable interventions in mitigating the risks of the pandemic disease. However, machine learning, like many tools of technocratic governance, is deeply implicated in the social production and distribution of risk and the role of machine learning in the production of risk must be considered as engineers and other technologists develop tools for the current crisis. This paper describes the coupling of machine learning and the social production of risk, generally, and in pandemic responses specifically. It goes on to describe the role of risk management in the effort to institutionalize ethics in the technology industry and how such efforts can benefit from a deeper understanding of the social production of risk through machine learning.",1.0
33072513,Systems biology approaches integrated with artificial intelligence for optimized metabolic engineering,2020 Dec;11:e00149.,"Metabolic engineering aims to maximize the production of bio-economically important substances (compounds, enzymes, or other proteins) through the optimization of the genetics, cellular processes and growth conditions of microorganisms. This requires detailed understanding of underlying metabolic pathways involved in the production of the targeted substances, and how the cellular processes or growth conditions are regulated by the engineering. To achieve this goal, a large system of experimental techniques, compound libraries, computational methods and data resources, including multi-omics data, are used. The recent advent of multi-omics systems biology approaches significantly impacted the field by opening new avenues to perform dynamic and large-scale analyses that deepen our knowledge on the manipulations. However, with the enormous transcriptomics, proteomics and metabolomics available, it is a daunting task to integrate the data for a more holistic understanding. Novel data mining and analytics approaches, including Artificial Intelligence (AI), can provide breakthroughs where traditional low-throughput experiment-alone methods cannot easily achieve. Here, we review the latest attempts of combining systems biology and AI in metabolic engineering research, and highlight how this alliance can help overcome the current challenges facing industrial biotechnology, especially for food-related substances and compounds using microorganisms.",
33071738,Heart Rate Variability in the Perinatal Period: A Critical and Conceptual Review,2020 Sep 25;14:561186.,"Neonatal intensive care units (NICUs) greatly expand the use of technology. There is a need to accurately diagnose discomfort, pain, and complications, such as sepsis, mainly before they occur. While specific treatments are possible, they are often time-consuming, invasive, or painful, with detrimental effects for the development of the infant. In the last 40 years, heart rate variability (HRV) has emerged as a non-invasive measurement to monitor newborns and infants, but it still is underused. Hence, the present paper aims to review the utility of HRV in neonatology and the instruments available to assess it, showing how HRV could be an innovative tool in the years to come. When continuously monitored, HRV could help assess the baby's overall wellbeing and neurological development to detect stress-/pain-related behaviors or pathological conditions, such as respiratory distress syndrome and hyperbilirubinemia, to address when to perform procedures to reduce the baby's stress/pain and interventions, such as therapeutic hypothermia, and to avoid severe complications, such as sepsis and necrotizing enterocolitis, thus reducing mortality. Based on literature and previous experiences, the first step to efficiently introduce HRV in the NICUs could consist in a monitoring system that uses photoplethysmography, which is low-cost and non-invasive, and displays one or a few metrics with good clinical utility. However, to fully harness HRV clinical potential and to greatly improve neonatal care, the monitoring systems will have to rely on modern bioinformatics (machine learning and artificial intelligence algorithms), which could easily integrate infant's HRV metrics, vital signs, and especially past history, thus elaborating models capable to efficiently monitor and predict the infant's clinical conditions. For this reason, hospitals and institutions will have to establish tight collaborations between the obstetric, neonatal, and pediatric departments: this way, healthcare would truly improve in every stage of the perinatal period (from conception to the first years of life), since information about patients' health would flow freely among different professionals, and high-quality research could be performed integrating the data recorded in those departments.",2.0
33071613,Recent Development of Machine Learning Methods in Microbial Phosphorylation Sites,2020 Apr;21(3):194-203.,"A variety of protein post-translational modifications has been identified that control many cellular functions. Phosphorylation studies in mycobacterial organisms have shown critical importance in diverse biological processes, such as intercellular communication and cell division. Recent technical advances in high-precision mass spectrometry have determined a large number of microbial phosphorylated proteins and phosphorylation sites throughout the proteome analysis. Identification of phosphorylated proteins with specific modified residues through experimentation is often labor-intensive, costly and time-consuming. All these limitations could be overcome through the application of machine learning (ML) approaches. However, only a limited number of computational phosphorylation site prediction tools have been developed so far. This work aims to present a complete survey of the existing ML-predictors for microbial phosphorylation. We cover a variety of important aspects for developing a successful predictor, including operating ML algorithms, feature selection methods, window size, and software utility. Initially, we review the currently available phosphorylation site databases of the microbiome, the state-of-the-art ML approaches, working principles, and their performances. Lastly, we discuss the limitations and future directions of the computational ML methods for the prediction of phosphorylation.",
