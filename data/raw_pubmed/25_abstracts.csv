pmid,citations,title,date,text
29344895,2.0,Bioinformatics Approaches to Predict Drug Responses from Genomic Sequencing,2018;1711:277-296.,"Fulfilling the promises of precision medicine will depend on our ability to create patient-specific treatment regimens. Therefore, being able to translate genomic sequencing into predicting how a patient will respond to a given drug is critical. In this chapter, we review common bioinformatics approaches that aim to use sequencing data to predict sample-specific drug susceptibility. First, we explain the importance of customized drug regimens to the future of medical care. Second, we discuss the different public databases and community efforts that can be leveraged to develop new methods for identifying new predictive biomarkers. Third, we cover the basic methods that are currently used to identify markers or signatures of drug response, without any prior knowledge of the drug's mechanism of action. We further discuss how one can integrate knowledge about drug targets, mechanisms, and predictive markers to better estimate drug response in a diverse set of samples. We begin this section with a primer on popular methods to identify targets and mechanism of action for new small molecules. This discussion also includes a set of computational methods that incorporate other drug features, which do not relate to drug-induced genetic changes or sequencing data such as drug structures, side-effects, and efficacy profiles. Those additional drug properties can aid in gaining higher accuracy for the identification of drug target and mechanism of action. We then progress to discuss using these targets in combination with disease-specific expression patterns, known pathways, and genetic interaction networks to aid drug choice. Finally, we conclude this chapter with a general overview of machine learning methods that can integrate multiple pieces of sequencing data along with prior drug or biological knowledge to drastically improve response prediction."
29331490,65.0,Supervised Machine Learning for Population Genetics: A New Paradigm,2018 Apr;34(4):301-312.,"As population genomic datasets grow in size, researchers are faced with the daunting task of making sense of a flood of information. To keep pace with this explosion of data, computational methodologies for population genetic inference are rapidly being developed to best utilize genomic sequence data. In this review we discuss a new paradigm that has emerged in computational population genomics: that of supervised machine learning (ML). We review the fundamentals of ML, discuss recent applications of supervised ML to population genetics that outperform competing methods, and describe promising future directions in this area. Ultimately, we argue that supervised ML is an important and underutilized tool that has considerable potential for the world of evolutionary genomics."
29327813,17.0,Computational Tools for the Identification and Interpretation of Sequence Motifs in Immunopeptidomes,2018 Jun;18(12):e1700252.,"Recent advances in proteomics and mass-spectrometry have widely expanded the detectable peptide repertoire presented by major histocompatibility complex (MHC) molecules on the cell surface, collectively known as the immunopeptidome. Finely characterizing the immunopeptidome brings about important basic insights into the mechanisms of antigen presentation, but can also reveal promising targets for vaccine development and cancer immunotherapy. This report describes a number of practical and efficient approaches to analyze immunopeptidomics data, discussing the identification of meaningful sequence motifs in various scenarios and considering current limitations. Guidelines are provided for the filtering of false hits and contaminants, and to address the problem of motif deconvolution in cell lines expressing multiple MHC alleles, both for the MHC class I and class II systems. Finally, it is demonstrated how machine learning can be readily employed by non-expert users to generate accurate prediction models directly from mass-spectrometry eluted ligand data sets."
29324649,20.0,Machine Learning Methods for Analysis of Metabolic Data and Metabolic Pathway Modeling,2018 Jan 11;8(1):4.,"Machine learning uses experimental data to optimize clustering or classification of samples or features, or to develop, augment or verify models that can be used to predict behavior or properties of systems. It is expected that machine learning will help provide actionable knowledge from a variety of big data including metabolomics data, as well as results of metabolism models. A variety of machine learning methods has been applied in bioinformatics and metabolism analyses including self-organizing maps, support vector machines, the kernel machine, Bayesian networks or fuzzy logic. To a lesser extent, machine learning has also been utilized to take advantage of the increasing availability of genomics and metabolomics data for the optimization of metabolic network models and their analysis. In this context, machine learning has aided the development of metabolic networks, the calculation of parameters for stoichiometric and kinetic models, as well as the analysis of major features in the model for the optimal application of bioreactors. Examples of this very interesting, albeit highly complex, application of machine learning for metabolism modeling will be the primary focus of this review presenting several different types of applications for model optimization, parameter determination or system analysis using models, as well as the utilization of several different types of machine learning technologies."
29324298,13.0,Calibration of raw accelerometer data to measure physical activity: A systematic review,2018 Mar;61:98-110.,"Most of calibration studies based on accelerometry were developed using count-based analyses. In contrast, calibration studies based on raw acceleration signals are relatively recent and their evidences are incipient. The aim of the current study was to systematically review the literature in order to summarize methodological characteristics and results from raw data calibration studies. The review was conducted up to May 2017 using four databases: PubMed, Scopus, SPORTDiscus and Web of Science. Methodological quality of the included studies was evaluated using the Landis and Koch's guidelines. Initially, 1669 titles were identified and, after assessing titles, abstracts and full-articles, 20 studies were included. All studies were conducted in high-income countries, most of them with relatively small samples and specific population groups. Physical activity protocols were different among studies and the indirect calorimetry was the criterion measure mostly used. High mean values of sensitivity, specificity and accuracy from the intensity thresholds of cut-point-based studies were observed (93.7%, 91.9% and 95.8%, respectively). The most frequent statistical approach applied was machine learning-based modelling, in which the mean coefficient of determination was 0.70 to predict physical activity energy expenditure. Regarding the recognition of physical activity types, the mean values of accuracy for sedentary, household and locomotive activities were 82.9%, 55.4% and 89.7%, respectively. In conclusion, considering the construct of physical activity that each approach assesses, linear regression, machine-learning and cut-point-based approaches presented promising validity parameters."
29321268,19.0,Computational techniques for ECG analysis and interpretation in light of their contribution to medical advances,2018 Jan;15(138):20170821.,"Widely developed for clinical screening, electrocardiogram (ECG) recordings capture the cardiac electrical activity from the body surface. ECG analysis can therefore be a crucial first step to help diagnose, understand and predict cardiovascular disorders responsible for 30% of deaths worldwide. Computational techniques, and more specifically machine learning techniques and computational modelling are powerful tools for classification, clustering and simulation, and they have recently been applied to address the analysis of medical data, especially ECG data. This review describes the computational methods in use for ECG analysis, with a focus on machine learning and 3D computer simulations, as well as their accuracy, clinical implications and contributions to medical advances. The first section focuses on heartbeat classification and the techniques developed to extract and classify abnormal from regular beats. The second section focuses on patient diagnosis from whole recordings, applied to different diseases. The third section presents real-time diagnosis and applications to wearable devices. The fourth section highlights the recent field of personalized ECG computer simulations and their interpretation. Finally, the discussion section outlines the challenges of ECG analysis and provides a critical assessment of the methods presented. The computational methods reported in this review are a strong asset for medical discoveries and their translation to the clinical world may lead to promising advances."
29320410,5.0,Advances in Non-Destructive Early Assessment of Fruit Ripeness towards Defining Optimal Time of Harvest and Yield Prediction-A Review,2018 Jan 10;7(1):3.,"Global food security for the increasing world population not only requires increased sustainable production of food but a significant reduction in pre- and post-harvest waste. The timing of when a fruit is harvested is critical for reducing waste along the supply chain and increasing fruit quality for consumers. The early in-field assessment of fruit ripeness and prediction of the harvest date and yield by non-destructive technologies have the potential to revolutionize farming practices and enable the consumer to eat the tastiest and freshest fruit possible. A variety of non-destructive techniques have been applied to estimate the ripeness or maturity but not all of them are applicable for in situ (field or glasshouse) assessment. This review focuses on the non-destructive methods which are promising for, or have already been applied to, the pre-harvest in-field measurements including colorimetry, visible imaging, spectroscopy and spectroscopic imaging. Machine learning and regression models used in assessing ripeness are also discussed."
29312932,6.0,Fetal Cardiac Doppler Signal Processing Techniques: Challenges and Future Research Directions,2017 Dec 22;5:82.,"The fetal Doppler Ultrasound (DUS) is commonly used for monitoring fetal heart rate and can also be used for identifying the event timings of fetal cardiac valve motions. In early-stage fetuses, the detected Doppler signal suffers from noise and signal loss due to the fetal movements and changing fetal location during the measurement procedure. The fetal cardiac intervals, which can be estimated by measuring the fetal cardiac event timings, are the most important markers of fetal development and well-being. To advance DUS-based fetal monitoring methods, several powerful and well-advanced signal processing and machine learning methods have recently been developed. This review provides an overview of the existing techniques used in fetal cardiac activity monitoring and a comprehensive survey on fetal cardiac Doppler signal processing frameworks. The review is structured with a focus on their shortcomings and advantages, which helps in understanding fetal Doppler cardiogram signal processing methods and the related Doppler signal analysis procedures by providing valuable clinical information. Finally, a set of recommendations are suggested for future research directions and the use of fetal cardiac Doppler signal analysis, processing, and modeling to address the underlying challenges."
29312134,1.0,Multivariate Analysis and Machine Learning in Cerebral Palsy Research,2017 Dec 21;8:715.,"Cerebral palsy (CP), a common pediatric movement disorder, causes the most severe physical disability in children. Early diagnosis in high-risk infants is critical for early intervention and possible early recovery. In recent years, multivariate analytic and machine learning (ML) approaches have been increasingly used in CP research. This paper aims to identify such multivariate studies and provide an overview of this relatively young field. Studies reviewed in this paper have demonstrated that multivariate analytic methods are useful in identification of risk factors, detection of CP, movement assessment for CP prediction, and outcome assessment, and ML approaches have made it possible to automatically identify movement impairments in high-risk infants. In addition, outcome predictors for surgical treatments have been identified by multivariate outcome studies. To make the multivariate and ML approaches useful in clinical settings, further research with large samples is needed to verify and improve these multivariate methods in risk factor identification, CP detection, movement assessment, and outcome evaluation or prediction. As multivariate analysis, ML and data processing technologies advance in the era of Big Data of this century, it is expected that multivariate analysis and ML will play a bigger role in improving the diagnosis and treatment of CP to reduce mortality and morbidity rates, and enhance patient care for children with CP."
29305324,39.0,Real-world outcomes in patients with neovascular age-related macular degeneration treated with intravitreal vascular endothelial growth factor inhibitors,2018 Jul;65:127-146.,"Clinical trials identified intravitreal vascular endothelial growth factor inhibitors (anti-VEGF agents) have the potential to stabilise or even improve visual acuity outcomes in neovascular age-related macular degeneration (AMD), a sight-threatening disease. Real-world evidence allows us to assess whether results from randomised controlled trials can be applied to the general population. We describe the development of global registries, in particular the Fight Retinal Blindness! registry that originated in Australia, the United Kingdom AMD Electronic Medical Records User Group and the IRIS registry in the USA. Real-world observations relating to efficacy, safety and resource utilisation of intravitreal anti-VEGF therapy for neovascular AMD are then summarised. Novel observations that would have been challenging to identify in a clinical trial setting are then highlighted, including the risk of late disease reactivation, outcomes in second versus first treated eyes, and the increased risk of posterior capsular rupture during cataract surgery in patients who have received intravitreal anti-VEGF therapy. We conclude by exploring future directions in the field. This includes the development of a global consensus on real-world outcome measures to allow greater comparison of results. Real-world neovascular AMD outcome registries can be linked with other databases to determine systemic safety or genetic predictors of treatment efficacy. Machine learning offers opportunities to extract useful insights from ""Big Data"" often collected in these registries. Real-world registries could be used by drug regulatory authorities and industry as an alternative to more costly and time-consuming phase 4 clinical trials, potentially allowing medication costs to be based on outcomes achieved."
29290259,86.0,Radiomics and radiogenomics in lung cancer: A review for the clinician,2018 Jan;115:34-41.,"Lung cancer is responsible for a large proportion of cancer-related deaths across the globe, with delayed detection being perhaps the most significant factor for its high mortality rate. Though the National Lung Screening Trial argues for screening of certain at-risk populations, the practical implementation of these screening efforts has not yet been successful and remains in high demand. Radiomics refers to the computerized extraction of data from radiologic images, and provides unique potential for making lung cancer screening more rapid and accurate using machine learning algorithms. The quantitative features analyzed express subvisual characteristics of images which correlate with pathogenesis of diseases. These features are broadly classified into four categories: intensity, structure, texture/gradient, and wavelet, based on the types of image attributes they capture. Many studies have been done to show correlation between these features and the malignant potential of a nodule on a chest CT. In cancer patients, these nodules also have features that can be correlated with prognosis and mutation status. The major limitations of radiomics are the lack of standardization of acquisition parameters, inconsistent radiomic methods, and lack of reproducibility. Researchers are working on overcoming these limitations, which would make radiomics more acceptable in the medical community."
29288495,36.0,PanCancer insights from The Cancer Genome Atlas: the pathologist's perspective,2018 Apr;244(5):512-524.,"The Cancer Genome Atlas (TCGA) represents one of several international consortia dedicated to performing comprehensive genomic and epigenomic analyses of selected tumour types to advance our understanding of disease and provide an open-access resource for worldwide cancer research. Thirty-three tumour types (selected by histology or tissue of origin, to include both common and rare diseases), comprising >11 000 specimens, were subjected to DNA sequencing, copy number and methylation analysis, and transcriptomic, proteomic and histological evaluation. Each cancer type was analysed individually to identify tissue-specific alterations, and make correlations across different molecular platforms. The final dataset was then normalized and combined for the PanCancer Initiative, which seeks to identify commonalities across different cancer types or cells of origin/lineage, or within anatomically or morphologically related groups. An important resource generated along with the rich molecular studies is an extensive digital pathology slide archive, composed of frozen section tissue directly related to the tissues analysed as part of TCGA, and representative formalin-fixed paraffin-embedded, haematoxylin and eosin (H&E)-stained diagnostic slides. These H&E image resources have primarily been used to verify diagnoses and histological subtypes with some limited extraction of standard pathological variables such as mitotic activity, grade, and lymphocytic infiltrates. Largely overlooked is the richness of these scanned images for more sophisticated feature extraction approaches coupled with machine learning, and ultimately correlation with molecular features and clinical endpoints. Here, we document initial attempts to exploit TCGA imaging archives, and describe some of the tools, and the rapidly evolving image analysis/feature extraction landscape. Our hope is to inform, and ultimately inspire and challenge, the pathology and cancer research communities to exploit these imaging resources so that the full potential of this integral platform of TCGA can be used to complement and enhance the insightful integrated analyses from the genomic and epigenomic platforms. Copyright © 2017 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd."
29278737,3.0,Network science in clinical trials: A patient-centered approach,2018 Oct;52(Pt 2):135-150.,"There has been a paradigm shift in translational oncology with the advent of novel molecular diagnostic tools in the clinic. However, several challenges are associated with the integration of these sophisticated tools into clinical oncology and daily practice. High-throughput profiling at the DNA, RNA and protein levels (omics) generate a massive amount of data. The analysis and interpretation of these is non-trivial but will allow a more thorough understanding of cancer. Linear modelling of the data as it is often used today is likely to limit our understanding of cancer as a complex disease, and at times under-performs to capture a phenotype of interest. Network science and systems biology-based approaches, using machine learning and network science principles, that integrate multiple data sources, can uncover complex changes in a biological system. This approach will integrate a large number of potential biomarkers in preclinical studies to better inform therapeutic decisions and ultimately make substantial progress towards precision medicine. It will however require development of a new generation of clinical trials. Beyond discussing the challenges of high-throughput technologies, this review will develop a framework on how to implement a network science approach in new clinical trial designs in order to advance cancer care."
29275361,62.0,Applications of Support Vector Machine (SVM) Learning in Cancer Genomics,Jan-Feb 2018;15(1):41-51.,"Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications."
29274047,2.0,Quantitative Analysis of Uncertainty in Medical Reporting: Creating a Standardized and Objective Methodology,2018 Apr;31(2):145-149.,"Uncertainty in text-based medical reports has long been recognized as problematic, frequently resulting in misunderstanding and miscommunication. One strategy for addressing the negative clinical ramifications of report uncertainty would be the creation of a standardized methodology for characterizing and quantifying uncertainty language, which could provide both the report author and reader with context related to the perceived level of diagnostic confidence and accuracy. A number of computerized strategies could be employed in the creation of this analysis including string search, natural language processing and understanding, histogram analysis, topic modeling, and machine learning. The derived uncertainty data offers the potential to objectively analyze report uncertainty in real time and correlate with outcomes analysis for the purpose of context and user-specific decision support at the point of care, where intervention would have the greatest clinical impact."
29261408,34.0,"Big Data in Public Health: Terminology, Machine Learning, and Privacy",2018 Apr 1;39:95-112.,"The digital world is generating data at a staggering and still increasing rate. While these ""big data"" have unlocked novel opportunities to understand public health, they hold still greater potential for research and practice. This review explores several key issues that have arisen around big data. First, we propose a taxonomy of sources of big data to clarify terminology and identify threads common across some subtypes of big data. Next, we consider common public health research and practice uses for big data, including surveillance, hypothesis-generating research, and causal inference, while exploring the role that machine learning may play in each use. We then consider the ethical implications of the big data revolution with particular emphasis on maintaining appropriate care for privacy in a world in which technology is rapidly changing social norms regarding the need for (and even the meaning of) privacy. Finally, we make suggestions regarding structuring teams and training to succeed in working with big data in research and practice."
29261360,4.0,Precision Medicine: Functional Advancements,2018 Jan 29;69:1-18.,"Precision medicine was conceptualized on the strength of genomic sequence analysis. High-throughput functional metrics have enhanced sequence interpretation and clinical precision. These technologies include metabolomics, magnetic resonance imaging, and I rhythm (cardiac monitoring), among others. These technologies are discussed and placed in clinical context for the medical specialties of internal medicine, pediatrics, obstetrics, and gynecology. Publications in these fields support the concept of a higher level of precision in identifying disease risk. Precise disease risk identification has the potential to enable intervention with greater specificity, resulting in disease prevention-an important goal of precision medicine."
29251699,14.0,Artificial intelligence in diagnosis of obstructive lung disease: current status and future potential,2018 Mar;24(2):117-123.,"Purpose of review:                    The application of artificial intelligence in the diagnosis of obstructive lung diseases is an exciting phenomenon. Artificial intelligence algorithms work by finding patterns in data obtained from diagnostic tests, which can be used to predict clinical outcomes or to detect obstructive phenotypes. The purpose of this review is to describe the latest trends and to discuss the future potential of artificial intelligence in the diagnosis of obstructive lung diseases.              Recent findings:                    Machine learning has been successfully used in automated interpretation of pulmonary function tests for differential diagnosis of obstructive lung diseases. Deep learning models such as convolutional neural network are state-of-the art for obstructive pattern recognition in computed tomography. Machine learning has also been applied in other diagnostic approaches such as forced oscillation test, breath analysis, lung sound analysis and telemedicine with promising results in small-scale studies.              Summary:                    Overall, the application of artificial intelligence has produced encouraging results in the diagnosis of obstructive lung diseases. However, large-scale studies are still required to validate current findings and to boost its adoption by the medical community."
29249829,3.0,A polygenic score for schizophrenia predicts glycemic control,2017 Dec 18;7(12):1295.,"Schizophrenia is substantially comorbid with type 2 diabetes (T2D), but the molecular basis of this effect is incompletely understood. Here, we show that a cortical schizophrenia expression score predicts glycemic control from pancreatic islet cell expression. We used machine learning to identify a cortical expression signature in 212 schizophrenia patients and controls, which explained ~25% of the illness-associated variance. The algorithm was predicted in expression data from 51 subjects (9 with T2D), explained up to 26.3% of the variance in the glycemic control indicator HbA1c and could significantly differentiate T2D patients from controls. The cross-tissue prediction was driven by processes previously linked to diabetes. Genes contributing to this prediction were involved in the electron transport chain as well as kidney development and support oxidative stress as a molecular process underlying the comorbidity between both conditions. Together, the present results suggest a molecular commonality between schizophrenia and glycemic markers of type 2 diabetes."
29249344,1.0,Computational methods for corpus callosum segmentation on MRI: A systematic literature review,2018 Feb;154:25-35.,"Background and objective:                    The corpus callosum (CC) is the largest white matter structure in the brain and has a significant role in central nervous system diseases. Its volume correlates with the severity and/or extent of neurodegenerative disease. Even though the CC's role has been extensively studied over the last decades, and different algorithms and methods have been published regarding CC segmentation and parcellation, no reviews or surveys covering such developments have been reported so far. To bridge this gap, this paper presents a systematic literature review of computational methods focusing on CC segmentation and parcellation acquired on magnetic resonance imaging.              Methods:                    IEEExplore, PubMed, EBSCO Host, and Scopus database were searched with the following search terms: ((Segmentation OR Parcellation) AND (Corpus Callosum) AND (DTI OR MRI OR Diffusion Tensor Imag* OR Diffusion Tractography OR Magnetic Resonance Imag*)), resulting in 802 publications. Two reviewers independently evaluated all articles and 36 studies were selected through the systematic literature review process.              Results:                    This work reviewed four main segmentation methods groups: model-based, region-based, thresholding, and machine learning; 32 different validity metrics were reported. Even though model-based techniques are the most recurrently used for the segmentation task (13 articles), machine learning approaches achieved better outcomes of 95% when analyzing mean values for segmentation and classification metrics results. Moreover, CC segmentation is better established in T1-weighted images, having more methods implemented and also being tested in larger datasets, compared with diffusion tensor images.              Conclusions:                    The analyzed computational methods used to perform CC segmentation on magnetic resonance imaging have not yet overcome all presented challenges owing to metrics variability and lack of traceable materials."
29234465,82.0,Ten quick tips for machine learning in computational biology,2017 Dec 8;10:35.,"Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences."
29222764,6.0,"Optimization of Multi-Omic Genome-Scale Models: Methodologies, Hands-on Tutorial, and Perspectives",2018;1716:389-408.,"Genome-scale metabolic models are valuable tools for assessing the metabolic potential of living organisms. Being downstream of gene expression, metabolism is increasingly being used as an indicator of the phenotypic outcome for drugs and therapies. We here present a review of the principal methods used for constraint-based modelling in systems biology, and explore how the integration of multi-omic data can be used to improve phenotypic predictions of genome-scale metabolic models. We believe that the large-scale comparison of the metabolic response of an organism to different environmental conditions will be an important challenge for genome-scale models. Therefore, within the context of multi-omic methods, we describe a tutorial for multi-objective optimization using the metabolic and transcriptomics adaptation estimator (METRADE), implemented in MATLAB. METRADE uses microarray and codon usage data to model bacterial metabolic response to environmental conditions (e.g., antibiotics, temperatures, heat shock). Finally, we discuss key considerations for the integration of multi-omic networks into metabolic models, towards automatically extracting knowledge from such models."
29220074,5.0,Prediction of Protein-Protein Interactions,2017 Dec 8;60:8.2.1-8.2.14.,"The authors provide an overview of physical protein-protein interaction prediction, covering the main strategies for predicting interactions, approaches for assessing predictions, and online resources for accessing predictions. This unit focuses on the main advancements in each of these areas over the last decade. The methods and resources that are presented here are not an exhaustive set, but characterize the current state of the field-highlighting key challenges and achievements. © 2017 by John Wiley & Sons, Inc."
29216413,18.0,Quantum Machine Learning in Chemical Compound Space,2018 Apr 9;57(16):4164-4169.,"Rather than numerically solving the computationally demanding equations of quantum or statistical mechanics, machine learning methods can infer approximate solutions, interpolating previously acquired property data sets of molecules and materials. The case is made for quantum machine learning: An inductive molecular modeling approach which can be applied to quantum chemistry problems."
29215763,18.0,Breast cancer: The translation of big genomic data to cancer precision medicine,2018 Mar;109(3):497-506.,"Cancer is a complex genetic disease that develops from the accumulation of genomic alterations in which germline variations predispose individuals to cancer and somatic alterations initiate and trigger the progression of cancer. For the past 2 decades, genomic research has advanced remarkably, evolving from single-gene to whole-genome screening by using genome-wide association study and next-generation sequencing that contributes to big genomic data. International collaborative efforts have contributed to curating these data to identify clinically significant alterations that could be used in clinical settings. Focusing on breast cancer, the present review summarizes the identification of genomic alterations with high-throughput screening as well as the use of genomic information in clinical trials that match cancer patients to therapies, which further leads to cancer precision medicine. Furthermore, cancer screening and monitoring were enhanced greatly by the use of liquid biopsies. With the growing data complexity and size, there is much anticipation in exploiting deep machine learning and artificial intelligence to curate integrative ""-omics"" data to refine the current medical practice to be applied in the near future."
29194464,16.0,"Computer-aided biomarker discovery for precision medicine: data resources, models and applications",2019 May 21;20(3):952-975.,"Biomarkers are a class of measurable and evaluable indicators with the potential to predict disease initiation and progression. In contrast to disease-associated factors, biomarkers hold the promise to capture the changeable signatures of biological states. With methodological advances, computer-aided biomarker discovery has now become a burgeoning paradigm in the field of biomedical science. In recent years, the 'big data' term has accumulated for the systematical investigation of complex biological phenomena and promoted the flourishing of computational methods for systems-level biomarker screening. Compared with routine wet-lab experiments, bioinformatics approaches are more efficient to decode disease pathogenesis under a holistic framework, which is propitious to identify biomarkers ranging from single molecules to molecular networks for disease diagnosis, prognosis and therapy. In this review, the concept and characteristics of typical biomarker types, e.g. single molecular biomarkers, module/network biomarkers, cross-level biomarkers, etc., are explicated on the guidance of systems biology. Then, publicly available data resources together with some well-constructed biomarker databases and knowledge bases are introduced. Biomarker identification models using mathematical, network and machine learning theories are sequentially discussed. Based on network substructural and functional evidences, a novel bioinformatics model is particularly highlighted for microRNA biomarker discovery. This article aims to give deep insights into the advantages and challenges of current computational approaches for biomarker detection, and to light up the future wisdom toward precision medicine and nation-wide healthcare."
29194052,13.0,Machine learning in heart failure: ready for prime time,2018 Mar;33(2):190-195.,"Purpose of review:                    The aim of this review is to present an up-to-date overview of the application of machine learning methods in heart failure including diagnosis, classification, readmissions and medication adherence.              Recent findings:                    Recent studies have shown that the application of machine learning techniques may have the potential to improve heart failure outcomes and management, including cost savings by improving existing diagnostic and treatment support systems. Recently developed deep learning methods are expected to yield even better performance than traditional machine learning techniques in performing complex tasks by learning the intricate patterns hidden in big medical data.              Summary:                    The review summarizes the recent developments in the application of machine and deep learning methods in heart failure management."
29192299,17.0,Machine learning to detect signatures of disease in liquid biopsies - a user's guide,2018 Jan 30;18(3):395-405.,"New technologies that measure sparse molecular biomarkers from easily accessible bodily fluids (e.g. blood, urine, and saliva) are revolutionizing disease diagnostics and precision medicine. Microchip devices can measure more disease biomarkers with better sensitivity and specificity each year, but clinical interpretation of these biomarkers remains a challenge. Single biomarkers in 'liquid biopsy' often cannot accurately predict the state of a disease due to heterogeneity in phenotype and disease expression across individuals. To address this challenge, investigators are combining multiplexed measurements of different biomarkers that together define robust signatures for specific disease states. Machine learning is a useful tool to automatically discover and detect these signatures, especially as new technologies output increasing quantities of molecular data. In this paper, we review the state of the field of machine learning applied to molecular diagnostics and provide practical guidance to use this tool effectively and to avoid common pitfalls."
29185073,3.0,Computational systems biology approaches for Parkinson's disease,2018 Jul;373(1):91-109.,"Parkinson's disease (PD) is a prime example of a complex and heterogeneous disorder, characterized by multifaceted and varied motor- and non-motor symptoms and different possible interplays of genetic and environmental risk factors. While investigations of individual PD-causing mutations and risk factors in isolation are providing important insights to improve our understanding of the molecular mechanisms behind PD, there is a growing consensus that a more complete understanding of these mechanisms will require an integrative modeling of multifactorial disease-associated perturbations in molecular networks. Identifying and interpreting the combinatorial effects of multiple PD-associated molecular changes may pave the way towards an earlier and reliable diagnosis and more effective therapeutic interventions. This review provides an overview of computational systems biology approaches developed in recent years to study multifactorial molecular alterations in complex disorders, with a focus on PD research applications. Strengths and weaknesses of different cellular pathway and network analyses, and multivariate machine learning techniques for investigating PD-related omics data are discussed, and strategies proposed to exploit the synergies of multiple biological knowledge and data sources. A final outlook provides an overview of specific challenges and possible next steps for translating systems biology findings in PD to new omics-based diagnostic tools and targeted, drug-based therapeutic approaches."
29184897,3.0,"Deep learning for cardiac computer-aided diagnosis: benefits, issues & solutions",2017 Oct 19;3:45.,"Cardiovascular diseases are one of the top causes of deaths worldwide. In developing nations and rural areas, difficulties with diagnosis and treatment are made worse due to the deficiency of healthcare facilities. A viable solution to this issue is telemedicine, which involves delivering health care and sharing medical knowledge at a distance. Additionally, mHealth, the utilization of mobile devices for medical care, has also proven to be a feasible choice. The integration of telemedicine, mHealth and computer-aided diagnosis systems with the fields of machine and deep learning has enabled the creation of effective services that are adaptable to a multitude of scenarios. The objective of this review is to provide an overview of heart disease diagnosis and management, especially within the context of rural healthcare, as well as discuss the benefits, issues and solutions of implementing deep learning algorithms to improve the efficacy of relevant medical applications."
29184889,8.0,The conceptualization of a Just-In-Time Adaptive Intervention (JITAI) for the reduction of sedentary behavior in older adults,2017 Sep 12;3:37.,"Low physical activity and high sedentary behavior in older adults can be addressed with interventions that are delivered through modern technology. Just-In-Time Adaptive Interventions (JITAIs) are an emerging technology-driven behavior-change intervention type and capitalize on data that is collected via mobile sensing technology (e.g., smartphones) to trigger appropriate support in real-life. In this paper we integrated behavior change and aging theory and research as well as knowledge around older adult's technology use to conceptualize a JITAI targeting the reduction of sedentary behavior in older adults. The JITAIs ultimate goal is to encourage older adults to take regular activity breaks from prolonged sitting. As a proximal outcome, we suggest the number of daily activity breaks from sitting. Support provided to interrupt sitting time can be based on tailoring variables: (I) the current accumulated sitting time; (II) the location of the individual; (III) the time of the day; (IV) the frequency of daily support prompts; and (V) the response to previous support prompts. Data on these variables can be collected using sensors that are commonly inbuilt into smartphones (e.g., accelerometer, GPS). Support prompts might be best delivered via traditional text messages as older adults are usually familiar and comfortable with this function. The content of the prompts should encourage breaks from prolonged sitting by highlighting immediate benefits of sitting time interruptions. Additionally, light physical activities that could be done during the breaks should also be presented (e.g., walking into the kitchen to prepare a cup of tea). Although the conceptualized JITAI can be developed and implemented to test its efficacy, more work is required to identify ways to collect, aggregate, organize and immediately use dense data on the proposed and other potentially important tailoring variables. Machine learning and other computational modelling techniques commonly used by computer scientists and engineers appear promising. With this, to develop powerful JITAIs and to actualize the full potential of modern sensing technologies transdisciplinary approaches are required."
29183655,17.0,Predicting Violent Behavior: What Can Neuroscience Add?,2018 Feb;22(2):111-123.,"The ability to accurately predict violence and other forms of serious antisocial behavior would provide important societal benefits, and there is substantial enthusiasm for the potential predictive accuracy of neuroimaging techniques. Here, we review the current status of violence prediction using actuarial and clinical methods, and assess the current state of neuroprediction. We then outline several questions that need to be addressed by future studies of neuroprediction if neuroimaging and other neuroscientific markers are to be successfully translated into public policy."
32923746,9.0,Designing and interpreting 'multi-omic' experiments that may change our understanding of biology,2017 Dec;6:37-45.,"Most biological mechanisms involve more than one type of biomolecule, and hence operate not solely at the level of either genome, transcriptome, proteome, metabolome or ionome. Datasets resulting from single-omic analysis are rapidly increasing in throughput and quality, rendering multi-omic studies feasible. These should offer a comprehensive, structured and interactive overview of a biological mechanism. However, combining single-omic datasets in a meaningful manner has so far proved challenging, and the discovery of new biological information lags behind expectation. One reason is that experiments conducted in different laboratories can typically not to be combined without restriction. Second, the interpretation of multi-omic datasets represents a significant challenge by nature, as the biological datasets are heterogeneous not only for technical, but also for biological, chemical, and physical reasons. Here, multi-layer network theory and methods of artificial intelligence might contribute to solve these problems. For the efficient application of machine learning however, biological datasets need to become more systematic, more precise - and much larger. We conclude our review with basic guidelines for the successful set-up of a multi-omic experiment."
29179711,16.0,A systematic review of data mining and machine learning for air pollution epidemiology,2017 Nov 28;17(1):907.,"Background:                    Data measuring airborne pollutants, public health and environmental factors are increasingly being stored and merged. These big datasets offer great potential, but also challenge traditional epidemiological methods. This has motivated the exploration of alternative methods to make predictions, find patterns and extract information. To this end, data mining and machine learning algorithms are increasingly being applied to air pollution epidemiology.              Methods:                    We conducted a systematic literature review on the application of data mining and machine learning methods in air pollution epidemiology. We carried out our search process in PubMed, the MEDLINE database and Google Scholar. Research articles applying data mining and machine learning methods to air pollution epidemiology were queried and reviewed.              Results:                    Our search queries resulted in 400 research articles. Our fine-grained analysis employed our inclusion/exclusion criteria to reduce the results to 47 articles, which we separate into three primary areas of interest: 1) source apportionment; 2) forecasting/prediction of air pollution/quality or exposure; and 3) generating hypotheses. Early applications had a preference for artificial neural networks. In more recent work, decision trees, support vector machines, k-means clustering and the APRIORI algorithm have been widely applied. Our survey shows that the majority of the research has been conducted in Europe, China and the USA, and that data mining is becoming an increasingly common tool in environmental health. For potential new directions, we have identified that deep learning and geo-spacial pattern mining are two burgeoning areas of data mining that have good potential for future applications in air pollution epidemiology.              Conclusions:                    We carried out a systematic review identifying the current trends, challenges and new directions to explore in the application of data mining methods to air pollution epidemiology. This work shows that data mining is increasingly being applied in air pollution epidemiology. The potential to support air pollution epidemiology continues to grow with advancements in data mining related to temporal and geo-spacial mining, and deep learning. This is further supported by new sensors and storage mediums that enable larger, better quality data. This suggests that many more fruitful applications can be expected in the future."
29175265,30.0,Digital image analysis in breast pathology-from image processing techniques to artificial intelligence,2018 Apr;194:19-35.,"Breast cancer is the most common malignant disease in women worldwide. In recent decades, earlier diagnosis and better adjuvant therapy have substantially improved patient outcome. Diagnosis by histopathology has proven to be instrumental to guide breast cancer treatment, but new challenges have emerged as our increasing understanding of cancer over the years has revealed its complex nature. As patient demand for personalized breast cancer therapy grows, we face an urgent need for more precise biomarker assessment and more accurate histopathologic breast cancer diagnosis to make better therapy decisions. The digitization of pathology data has opened the door to faster, more reproducible, and more precise diagnoses through computerized image analysis. Software to assist diagnostic breast pathology through image processing techniques have been around for years. But recent breakthroughs in artificial intelligence (AI) promise to fundamentally change the way we detect and treat breast cancer in the near future. Machine learning, a subfield of AI that applies statistical methods to learn from data, has seen an explosion of interest in recent years because of its ability to recognize patterns in data with less need for human instruction. One technique in particular, known as deep learning, has produced groundbreaking results in many important problems including image classification and speech recognition. In this review, we will cover the use of AI and deep learning in diagnostic breast pathology, and other recent developments in digital image analysis."
29174666,18.0,"Combining ecological momentary assessment with objective, ambulatory measures of behavior and physiology in substance-use research",2018 Aug;83:5-17.,"Whereas substance-use researchers have long combined self-report with objective measures of behavior and physiology inside the laboratory, developments in mobile/wearable electronic technology are increasingly allowing for the collection of both subjective and objective information in participants' daily lives. For self-report, ecological momentary assessment (EMA), as implemented on contemporary smartphones or personal digital assistants, can provide researchers with near-real-time information on participants' behavior and mood in their natural environments. Data from portable/wearable electronic sensors measuring participants' internal and external environments can be combined with EMA (e.g., by timestamps recorded on questionnaires) to provide objective information useful in determining the momentary context of behavior and mood and/or validating participants' self-reports. Here, we review three objective ambulatory monitoring techniques that have been combined with EMA, with a focus on detecting drug use and/or measuring the behavioral or physiological correlates of mental events (i.e., emotions, cognitions): (1) collection and processing of biological samples in the field to measure drug use or participants' physiological activity (e.g., hypothalamic-pituitary-adrenal axis activity); (2) global positioning system (GPS) location information to link environmental characteristics (disorder/disadvantage, retail drug outlets) to drug use and affect; (3) ambulatory electronic physiological monitoring (e.g., electrocardiography) to detect drug use and mental events, as advances in machine learning algorithms make it possible to distinguish target changes from confounds (e.g., physical activity). Finally, we consider several other mobile/wearable technologies that hold promise to be combined with EMA, as well as potential challenges faced by researchers working with multiple mobile/wearable technologies simultaneously in the field."
29173233,2.0,Fact or fiction: reducing the proportion and impact of false positives,2018 May;48(7):1084-1091.,"False positive findings in science are inevitable, but are they particularly common in psychology and psychiatry? The evidence that we review suggests that while not restricted to our field, the problem is acute. We describe the concept of researcher 'degrees-of-freedom' to explain how many false-positive findings arise, and how the various strategies of registration, pre-specification, and reporting standards that are being adopted both reduce and make these visible. We review possible benefits and harms of proposed statistical solutions, from tougher requirements for significance, to Bayesian and machine learning approaches to analysis. Finally we consider the organisation and methods for replication and systematic review in psychology and psychiatry."
29173045,4.0,Multiplicity issues in exploratory subgroup analysis,2018;28(1):63-81.,"The general topic of subgroup identification has attracted much attention in the clinical trial literature due to its important role in the development of tailored therapies and personalized medicine. Subgroup search methods are commonly used in late-phase clinical trials to identify subsets of the trial population with certain desirable characteristics. Post-hoc or exploratory subgroup exploration has been criticized for being extremely unreliable. Principled approaches to exploratory subgroup analysis based on recent advances in machine learning and data mining have been developed to address this criticism. These approaches emphasize fundamental statistical principles, including the importance of performing multiplicity adjustments to account for selection bias inherent in subgroup search. This article provides a detailed review of multiplicity issues arising in exploratory subgroup analysis. Multiplicity corrections in the context of principled subgroup search will be illustrated using the family of SIDES (subgroup identification based on differential effect search) methods. A case study based on a Phase III oncology trial will be presented to discuss the details of subgroup search algorithms with resampling-based multiplicity adjustment procedures."
29155639,41.0,When Machines Think: Radiology's Next Frontier,2017 Dec;285(3):713-718.,"Artificial intelligence (AI), machine learning, and deep learning are terms now seen frequently, all of which refer to computer algorithms that change as they are exposed to more data. Many of these algorithms are surprisingly good at recognizing objects in images. The combination of large amounts of machine-consumable digital data, increased and cheaper computing power, and increasingly sophisticated statistical models combine to enable machines to find patterns in data in ways that are not only cost-effective but also potentially beyond humans' abilities. Building an AI algorithm can be surprisingly easy. Understanding the associated data structures and statistics, on the other hand, is often difficult and obscure. Converting the algorithm into a sophisticated product that works consistently in broad, general clinical use is complex and incompletely understood. To show how these AI products reduce costs and improve outcomes will require clinical translation and industrial-grade integration into routine workflow. Radiology has the chance to leverage AI to become a center of intelligently aggregated, quantitative, diagnostic information. Centaur radiologists, formed as a synergy of human plus computer, will provide interpretations using data extracted from images by humans and image-analysis computer algorithms, as well as the electronic health record, genomics, and other disparate sources. These interpretations will form the foundation of precision health care, or care customized to an individual patient. © RSNA, 2017."
29153163,9.0,A 100-Year Review: Methods and impact of genetic selection in dairy cattle-From daughter-dam comparisons to deep learning algorithms,2017 Dec;100(12):10234-10250.,"In the early 1900s, breed society herdbooks had been established and milk-recording programs were in their infancy. Farmers wanted to improve the productivity of their cattle, but the foundations of population genetics, quantitative genetics, and animal breeding had not been laid. Early animal breeders struggled to identify genetically superior families using performance records that were influenced by local environmental conditions and herd-specific management practices. Daughter-dam comparisons were used for more than 30 yr and, although genetic progress was minimal, the attention given to performance recording, genetic theory, and statistical methods paid off in future years. Contemporary (herdmate) comparison methods allowed more accurate accounting for environmental factors and genetic progress began to accelerate when these methods were coupled with artificial insemination and progeny testing. Advances in computing facilitated the implementation of mixed linear models that used pedigree and performance data optimally and enabled accurate selection decisions. Sequencing of the bovine genome led to a revolution in dairy cattle breeding, and the pace of scientific discovery and genetic progress accelerated rapidly. Pedigree-based models have given way to whole-genome prediction, and Bayesian regression models and machine learning algorithms have joined mixed linear models in the toolbox of modern animal breeders. Future developments will likely include elucidation of the mechanisms of genetic inheritance and epigenetic modification in key biological pathways, and genomic data will be used with data from on-farm sensors to facilitate precision management on modern dairy farms."
29150140,1.0,Visual pathways from the perspective of cost functions and multi-task deep neural networks,2018 Jan;98:249-261.,"Vision research has been shaped by the seminal insight that we can understand the higher-tier visual cortex from the perspective of multiple functional pathways with different goals. In this paper, we try to give a computational account of the functional organization of this system by reasoning from the perspective of multi-task deep neural networks. Machine learning has shown that tasks become easier to solve when they are decomposed into subtasks with their own cost function. We hypothesize that the visual system optimizes multiple cost functions of unrelated tasks and this causes the emergence of a ventral pathway dedicated to vision for perception, and a dorsal pathway dedicated to vision for action. To evaluate the functional organization in multi-task deep neural networks, we propose a method that measures the contribution of a unit towards each task, applying it to two networks that have been trained on either two related or two unrelated tasks, using an identical stimulus set. Results show that the network trained on the unrelated tasks shows a decreasing degree of feature representation sharing towards higher-tier layers while the network trained on related tasks uniformly shows high degree of sharing. We conjecture that the method we propose can be used to analyze the anatomical and functional organization of the visual system and beyond. We predict that the degree to which tasks are related is a good descriptor of the degree to which they share downstream cortical-units."
29149080,13.0,Novel Tactile Sensor Technology and Smart Tactile Sensing Systems: A Review,2017 Nov 17;17(11):2653.,"During the last decades, smart tactile sensing systems based on different sensing techniques have been developed due to their high potential in industry and biomedical engineering. However, smart tactile sensing technologies and systems are still in their infancy, as many technological and system issues remain unresolved and require strong interdisciplinary efforts to address them. This paper provides an overview of smart tactile sensing systems, with a focus on signal processing technologies used to interpret the measured information from tactile sensors and/or sensors for other sensory modalities. The tactile sensing transduction and principles, fabrication and structures are also discussed with their merits and demerits. Finally, the challenges that tactile sensing technology needs to overcome are highlighted."
29147562,3.0,Cognitive computing and eScience in health and life science research: artificial intelligence and obesity intervention programs,2017 Nov 1;5(1):13.,"Objective:                    To present research models based on artificial intelligence and discuss the concept of cognitive computing and eScience as disruptive factors in health and life science research methodologies.              Methods:                    The paper identifies big data as a catalyst to innovation and the development of artificial intelligence, presents a framework for computer-supported human problem solving and describes a transformation of research support models. This framework includes traditional computer support; federated cognition using machine learning and cognitive agents to augment human intelligence; and a semi-autonomous/autonomous cognitive model, based on deep machine learning, which supports eScience.              Results:                    The paper provides a forward view of the impact of artificial intelligence on our human-computer support and research methods in health and life science research.              Conclusions:                    By augmenting or amplifying human task performance with artificial intelligence, cognitive computing and eScience research models are discussed as novel and innovative systems for developing more effective adaptive obesity intervention programs."
29147555,21.0,"What can machine learning do for antimicrobial peptides, and what can antimicrobial peptides do for machine learning?",2017 Dec 6;7(6):20160153.,"Antimicrobial peptides (AMPs) are a diverse class of well-studied membrane-permeating peptides with important functions in innate host defense. In this short review, we provide a historical overview of AMPs, summarize previous applications of machine learning to AMPs, and discuss the results of our studies in the context of the latest AMP literature. Much work has been recently done in leveraging computational tools to design new AMP candidates with high therapeutic efficacies for drug-resistant infections. We show that machine learning on AMPs can be used to identify essential physico-chemical determinants of AMP functionality, and identify and design peptide sequences to generate membrane curvature. In a broader scope, we discuss the implications of our findings for the discovery of membrane-active peptides in general, and uncovering membrane activity in new and existing peptide taxonomies."
29136580,11.0,Advancing the large-scale CCS database for metabolomics and lipidomics at the machine-learning era,2018 Feb;42:34-41.,"Metabolomics and lipidomics aim to comprehensively measure the dynamic changes of all metabolites and lipids that are present in biological systems. The use of ion mobility-mass spectrometry (IM-MS) for metabolomics and lipidomics has facilitated the separation and the identification of metabolites and lipids in complex biological samples. The collision cross-section (CCS) value derived from IM-MS is a valuable physiochemical property for the unambiguous identification of metabolites and lipids. However, CCS values obtained from experimental measurement and computational modeling are limited available, which significantly restricts the application of IM-MS. In this review, we will discuss the recently developed machine-learning based prediction approach, which could efficiently generate precise CCS databases in a large scale. We will also highlight the applications of CCS databases to support metabolomics and lipidomics."
29134342,,An introduction and overview of machine learning in neurosurgical care,2018 Jan;160(1):29-38.,"Background:                    Machine learning (ML) is a branch of artificial intelligence that allows computers to learn from large complex datasets without being explicitly programmed. Although ML is already widely manifest in our daily lives in various forms, the considerable potential of ML has yet to find its way into mainstream medical research and day-to-day clinical care. The complex diagnostic and therapeutic modalities used in neurosurgery provide a vast amount of data that is ideally suited for ML models. This systematic review explores ML's potential to assist and improve neurosurgical care.              Method:                    A systematic literature search was performed in the PubMed and Embase databases to identify all potentially relevant studies up to January 1, 2017. All studies were included that evaluated ML models assisting neurosurgical treatment.              Results:                    Of the 6,402 citations identified, 221 studies were selected after subsequent title/abstract and full-text screening. In these studies, ML was used to assist surgical treatment of patients with epilepsy, brain tumors, spinal lesions, neurovascular pathology, Parkinson's disease, traumatic brain injury, and hydrocephalus. Across multiple paradigms, ML was found to be a valuable tool for presurgical planning, intraoperative guidance, neurophysiological monitoring, and neurosurgical outcome prediction.              Conclusions:                    ML has started to find applications aimed at improving neurosurgical care by increasing the efficiency and precision of perioperative decision-making. A thorough validation of specific ML models is essential before implementation in clinical neurosurgical care. To bridge the gap between research and clinical care, practical and ethical issues should be considered parallel to the development of these techniques."
29131760,132.0,Deep Learning: A Primer for Radiologists,Nov-Dec 2017;37(7):2113-2131.,"Deep learning is a class of machine learning methods that are gaining success and attracting interest in many domains, including computer vision, speech recognition, natural language processing, and playing games. Deep learning methods produce a mapping from raw inputs to desired outputs (eg, image classes). Unlike traditional machine learning methods, which require hand-engineered feature extraction from inputs, deep learning methods learn these features directly from data. With the advent of large datasets and increased computing power, these methods can produce models with exceptional performance. These models are multilayer artificial neural networks, loosely inspired by biologic neural systems. Weighted connections between nodes (neurons) in the network are iteratively adjusted based on example pairs of inputs and target outputs by back-propagating a corrective error signal through the network. For computer vision tasks, convolutional neural networks (CNNs) have proven to be effective. Recently, several clinical applications of CNNs have been proposed and studied in radiology for classification, detection, and segmentation tasks. This article reviews the key concepts of deep learning for clinical radiologists, discusses technical requirements, describes emerging applications in clinical radiology, and outlines limitations and future directions in this field. Radiologists should become familiar with the principles and potential applications of deep learning in medical imaging. ©RSNA, 2017."
29127902,5.0,A survey of machine learning applications in HIV clinical research and care,2017 Dec 1;91:366-371.,"A wealth of genetic, demographic, clinical and biomarker data is collected from routine clinical care of HIV patients and exists in the form of medical records available among the medical care and research communities. Machine learning (ML) methods have the ability to identify and discover patterns in complex datasets and predict future outcomes of HIV treatment. We survey published studies that make use of ML techniques in HIV clinical research and care. An advanced search relevant to the use of ML in HIV research was conducted in the PubMed biomedical database. The survey outcomes of interest include data sources, ML techniques, ML tasks and ML application paradigms. A growing trend in application of ML in HIV research was observed. The application paradigm has diversified to include practical clinical application, but statistical analysis remains the most dominant application. There is an increase in the use of genomic sources of data and high performance non-parametric ML methods with a focus on combating resistance to antiretroviral therapy (ART). There is need for improvement in collection of health records data and increased training in ML so as to translate ML research into clinical application in HIV management."
29126825,62.0,Artificial Intelligence in Medical Practice: The Question to the Answer?,2018 Feb;131(2):129-133.,"Computer science advances and ultra-fast computing speeds find artificial intelligence (AI) broadly benefitting modern society-forecasting weather, recognizing faces, detecting fraud, and deciphering genomics. AI's future role in medical practice remains an unanswered question. Machines (computers) learn to detect patterns not decipherable using biostatistics by processing massive datasets (big data) through layered mathematical models (algorithms). Correcting algorithm mistakes (training) adds to AI predictive model confidence. AI is being successfully applied for image analysis in radiology, pathology, and dermatology, with diagnostic speed exceeding, and accuracy paralleling, medical experts. While diagnostic confidence never reaches 100%, combining machines plus physicians reliably enhances system performance. Cognitive programs are impacting medical practice by applying natural language processing to read the rapidly expanding scientific literature and collate years of diverse electronic medical records. In this and other ways, AI may optimize the care trajectory of chronic disease patients, suggest precision therapies for complex illnesses, reduce medical errors, and improve subject enrollment into clinical trials."
29114182,6.0,Application of Deep Learning in Automated Analysis of Molecular Images in Cancer: A Survey,2017 Oct 15;2017:9512370.,"Molecular imaging enables the visualization and quantitative analysis of the alterations of biological procedures at molecular and/or cellular level, which is of great significance for early detection of cancer. In recent years, deep leaning has been widely used in medical imaging analysis, as it overcomes the limitations of visual assessment and traditional machine learning techniques by extracting hierarchical features with powerful representation capability. Research on cancer molecular images using deep learning techniques is also increasing dynamically. Hence, in this paper, we review the applications of deep learning in molecular imaging in terms of tumor lesion segmentation, tumor classification, and survival prediction. We also outline some future directions in which researchers may develop more powerful deep learning models for better performance in the applications in cancer molecular imaging."
29098674,2.0,Development of New Diagnostic Techniques - Machine Learning,2017;1010:203-215.,"Traditional diagnoses on addiction reply on the patients' self-reports, which are easy to be dampened by false memory or malingering. Machine learning (ML) is a data-driven procedure that learns algorithms from training data and makes predictions. It is quickly developed and is more and more utilized into clinical applications including diagnoses of addiction. This chapter reviewed the basic concepts and processes of ML. Some studies utilizing ML to classify addicts and non-addicts, separate different types of addiction, and evaluate the effects of treatment are also reviewed. Both advantages and shortcomings of ML in diagnoses of addiction are discussed."
29089139,3.0,Towards In Silico Prediction of the Immune-Checkpoint Blockade Response,2017 Dec;38(12):1041-1051.,"Cancer immunotherapy with immune-checkpoint blockade (ICB) is considered a promising strategy for cancer treatment. Identifying predictive biomarkers and developing efficient computational models to predict the ICB response are important issues for successful immunotherapy. Here, we present a concise and intuitive survey of the computational issues for ICB response prediction, providing a summary of the available predictive biomarkers and building of one-stop machine-learning models that integrate biomarkers calculable from high-throughput sequencing (HTS) data. Several points for discussion are highlighted to inspire further research for improving ICB treatment. Continuing efforts are required to improve ICB response prediction and to identify novel predictive biomarkers by taking advantage of the rapid development of computational models and HTS techniques for effective and personalized cancer immunotherapy."
29074332,8.0,Improving individual predictions: Machine learning approaches for detecting and attacking heterogeneity in schizophrenia (and other psychiatric diseases),2019 Dec;214:34-42.,"Psychiatric diseases are very heterogeneous both in clinical manifestation and etiology. With the recent rise of using machine learning techniques to attempt to diagnose and prognose these disorders, the issue of heterogeneity becomes increasingly important. With the growing interest in personalized medicine, it becomes even more important to not only classify someone as a patient with a certain disorder, its treatment needs a more precise definition of the underlying neurobiology, since different biological origins of the same disease may require (very) different treatments. We review the possible contributions that machine learning techniques could make to explore the heterogeneous nature of psychiatric disorders with a focus on schizophrenia. First we will review how heterogeneity shows up and how machine learning, or multivariate pattern recognition methods in general, can be used to discover it. Secondly, we will discuss the possible uses of these techniques to attack heterogeneity, leading to improved predictions and understanding of the neurobiological background of the disorder."
29074032,89.0,Predicting Age Using Neuroimaging: Innovative Brain Ageing Biomarkers,2017 Dec;40(12):681-690.,"The brain changes as we age and these changes are associated with functional deterioration and neurodegenerative disease. It is vital that we better understand individual differences in the brain ageing process; hence, techniques for making individualised predictions of brain ageing have been developed. We present evidence supporting the use of neuroimaging-based 'brain age' as a biomarker of an individual's brain health. Increasingly, research is showing how brain disease or poor physical health negatively impacts brain age. Importantly, recent evidence shows that having an 'older'-appearing brain relates to advanced physiological and cognitive ageing and the risk of mortality. We discuss controversies surrounding brain age and highlight emerging trends such as the use of multimodality neuroimaging and the employment of 'deep learning' methods."
29063566,,Clinical Research Informatics: Contributions from 2016,2017 Aug;26(1):209-213.,"Objectives: To summarize key contributions to current research in the field of Clinical Research Informatics (CRI) and to select the best papers published in 2016. Methods: A bibliographic search using a combination of MeSH and free terms on CRI was performed using PubMed, followed by a double-blind review in order to select a list of candidate best papers to be then peer-reviewed by external reviewers. A consensus meeting between the two section editors and the editorial team was organized to finally conclude on the selection of best papers. Results: Among the 452 papers published in 2016 in the various areas of CRI and returned by the query, the full review process selected four best papers. The authors of the first paper utilized a comprehensive representation of the patient medical record and semi-automatically labeled training sets to create phenotype models via a machine learning process. The second selected paper describes an open source tool chain securely connecting ResearchKit compatible applications (Apps) to the widely-used clinical research infrastructure Informatics for Integrating Biology and the Bedside (i2b2). The third selected paper describes the FAIR Guiding Principles for scientific data management and stewardship. The fourth selected paper focuses on the evaluation of the risk of privacy breaches in releasing genomics datasets. Conclusions: A major trend in the 2016 publications is the variety of research on ""real-world data"" - healthcare-generated data, person health data, and patient-reported outcomes -highlighting the opportunities provided by new machine learning techniques as well as new potential risks of privacy breaches."
29063560,2.0,Secondary Use of Recorded or Self-expressed Personal Data: Consumer Health Informatics and Education in the Era of Social Media and Health Apps,2017 Aug;26(1):172-177.,"Objective: To summarize the state of the art during the year 2016 in the areas related to consumer health informatics and education with a special emphasis in secondary use of patient data. Methods: We conducted a systematic review of articles published in 2016, using PubMed with a predefined set of queries. We identified over 320 potential articles for review. Papers were considered according to their relevance for the topic of the section. Using consensus, we selected the 15 most representative papers, which were submitted to external reviewers for full review and scoring. Based on the scoring and quality criteria, five papers were finally selected as best papers Results: The five best papers can be grouped in two major areas: 1) methods and tools to identify and collect formal requirements for secondary use of data, and 2) innovative topics highlighting the interest of carrying on ""secondary"" studies on patient data, more specifically on the data self-expressed by patients through social media tools. Regarding the formal requirements about informed consent, the selected papers report a comparison of legal aspects in European countries to find a common and unified grammar around the concept of ""data donation"". Regarding innovative approaches to value patient data, the selected papers report machine learning algorithms to extract knowledge from patient experience and satisfaction with health care delivery, drug and medication use, treatment compliance and barriers during cancer disease, or acceptation of public health actions such as vaccination. Conclusions: Secondary use of patient data (apart from personal health care record data) can be expressed according to many ways. Requirements to allow this secondary use have to be harmonized between countries, and social media platforms can be efficiently used to explore and create knowledge on patient experience with health problems or activities. Machine learning algorithms can explore those massive amounts of data to support health care professionals, and institutions provide more accurate knowledge about use and usage, behaviour, sentiment, or satisfaction about health care delivery."
29063553,3.0,Contributions from the 2016 Literature on Clinical Decision Support,2017 Aug;26(1):133-138.,"Objectives: To summarize recent research and select the best papers published in 2016 in the field of computerized clinical decision support for the Decision Support section of the IMIA yearbook. Methods: A literature review was performed by searching two bibliographic databases for papers related to clinical decision support systems (CDSSs). The aim was to identify a list of candidate best papers from the retrieved papers that were then peer-reviewed by external reviewers. A consensus meeting of the IMIA editorial team finally selected the best papers on the basis of all reviews and section editor evaluation. Results: Among the 1,145 retrieved papers, the entire review process resulted in the selection of four best papers. The first paper describes machine learning models used to predict breast cancer multidisciplinary team decisions and compares them with two predictors based on guideline knowledge. The second paper introduces a linked-data approach for publication, discovery, and interoperability of CDSSs. The third paper assessed the variation in high-priority drug-drug interaction (DDI) alerts across 14 Electronic Health Record systems, operating in different institutions in the US. The fourth paper proposes a generic framework for modeling multiple concurrent guidelines and detecting their recommendation interactions using semantic web technologies. Conclusions: The process of identifying and selecting best papers in the domain of CDSSs demonstrated that the research in this field is very active concerning diverse dimensions, such as the types of CDSSs, e.g. guideline-based, machine-learning-based, knowledge-fusion-based, etc., and addresses challenging areas, such as the concurrent application of multiple guidelines for comorbid patients, the resolution of interoperability issues, and the evaluation of CDSSs. Nevertheless, this process also showed that CDSSs are not yet fully part of the digitalized healthcare ecosystem. Many challenges remain to be faced with regard to the evidence of their output, the dissemination of their technologies, as well as their adoption for better and safer healthcare delivery."
29063093,2.0,NMR window of molecular complexity showing homeostasis in superorganisms,2017 Nov 6;142(22):4161-4172.,"NMR offers tremendous advantages in the analyses of molecular complexity, such as crude bio-fluids, bio-extracts, and intact cells and tissues. Here we introduce recent applications of NMR approaches, as well as next generation sequencing (NGS), for the evaluation of human and environmental health (i.e., maintenance of a homeostatic state) based on metabolic and microbial profiling and data science. We describe useful databases and web tools that are used to support these studies by facilitating the characterization of metabolites from complex NMR spectra. Because the NMR spectra of metabolic mixtures can produce numerical matrix data (e.g., chemical shift versus intensity) with high reproducibility and inter-institution convertibility, advanced data science approaches, such as multivariate analysis and machine learning, are desirable; therefore, we also introduce informatics techniques derived from heterogeneously measured data, such as environmental microbiota, for the extraction of submerged information using data science approaches. We summarize recent studies of microbiomes that are based on these techniques and show that, particularly in human studies, NMR-based metabolic characterization of non-invasive samples, such as feces, can provide a large quantity of beneficial information regarding human health and disease."
29062966,1.0,Bacterial cell-free expression technology to in vitro systems engineering and optimization,2017 Aug 7;2(2):97-104.,"Cell-free expression system is a technology for the synthesis of proteins in vitro. The system is a platform for several bioengineering projects, e.g. cell-free metabolic engineering, evolutionary design of experiments, and synthetic minimal cell construction. Bacterial cell-free protein synthesis system (CFPS) is a robust tool for synthetic biology. The bacteria lysate, the DNA, and the energy module, which are the three optimized sub-systems for in vitro protein synthesis, compose the integrated system. Currently, an optimized E. coli cell-free expression system can produce up to ∼2.3 mg/mL of a fluorescent reporter protein. Herein, I will describe the features of ATP-regeneration systems for in vitro protein synthesis, and I will present a machine-learning experiment for optimizing the protein yield of E. coli cell-free protein synthesis systems. Moreover, I will introduce experiments on the synthesis of a minimal cell using liposomes as dynamic containers, and E. coli cell-free expression system as biochemical platform for metabolism and gene expression. CFPS can be further integrated with other technologies for novel applications in environmental, medical and material science."
29058219,,LEMRG: Decision Rule Generation Algorithm for Mining MicroRNA Expression Data,2017;1028:105-137.,"Recently, research on mining microRNA (or miRNA) expression data has received a lot of attention, mainly because of its role in gene regulation. However, such type of data - usually saved in the form of microarrays - are very specific, because they contain only a small number of cases (often less than 100) compared with large number of attributes (equal to several hundreds or even tens of thousand). The small number of cases available during the learning process can cause instability of the newly created classifiers. Secondly, the huge number of attributes imposes the necessity of selecting only a few dominant attributes strongly correlated with the decision. Thus, an application of fundamental machine learning approaches of mining microarray data and its further classification is problematic or even could just fail.Thus, the main goal of our research is to develop the generalized algorithm of mining microarray data (including miRNA data sets), mainly to improve stability and, consequently, accuracy of classification for the newly created learning classifiers. The main concept of the novel approach is based on iteratively inducing many subsequent decision rule sets - called decision rule generations - instead of inducing only a single decision rule set, as it is done routinely. The decision rules have been chosen as the baseline classifiers of the newly developed LEMRG (Learning from Examples Module based on Rule Generations) algorithm mainly because the decision rule-based knowledge representation is easier for humans to comprehend, rather than other learning models. In our research we used a miRNA expression level learning data set describing 11 types of human cancers, while the testing data set contained poorly differentiated cases of only four types of cancers. As expected, our new classifiers - saved in the form of so-called cumulative decision rule sets - had better stability and accuracy of classification than single decision rule sets induced in the traditional manner. Furthermore, the LEMRG was compared with other machine learning models. It was proven that only 3 out of all 16 tested classifiers enabled so effective classification as our newly developed approach. Thus, using our cumulative set of decision rules, all cases of cancer from two selected concepts - colon and ovary - were correctly classified. Furthermore, we showed the role of these selected miRNAs as the potential biomarkers for diagnosis of tumors.A preliminary result of our research on decision rule generations was initially presented at the first International Conference of Digital Medicine and Medical 3D Printing (17-19.06.2016, Nanjing, China)."
29056906,40.0,Random Forest Algorithm for the Classification of Neuroimaging Data in Alzheimer's Disease: A Systematic Review,2017 Oct 6;9:329.,"Objective: Machine learning classification has been the most important computational development in the last years to satisfy the primary need of clinicians for automatic early diagnosis and prognosis. Nowadays, Random Forest (RF) algorithm has been successfully applied for reducing high dimensional and multi-source data in many scientific realms. Our aim was to explore the state of the art of the application of RF on single and multi-modal neuroimaging data for the prediction of Alzheimer's disease. Methods: A systematic review following PRISMA guidelines was conducted on this field of study. In particular, we constructed an advanced query using boolean operators as follows: (""random forest"" OR ""random forests"") AND neuroimaging AND (""alzheimer's disease"" OR alzheimer's OR alzheimer) AND (prediction OR classification). The query was then searched in four well-known scientific databases: Pubmed, Scopus, Google Scholar and Web of Science. Results: Twelve articles-published between the 2007 and 2017-have been included in this systematic review after a quantitative and qualitative selection. The lesson learnt from these works suggest that when RF was applied on multi-modal data for prediction of Alzheimer's disease (AD) conversion from the Mild Cognitive Impairment (MCI), it produces one of the best accuracies to date. Moreover, the RF has important advantages in terms of robustness to overfitting, ability to handle highly non-linear data, stability in the presence of outliers and opportunity for efficient parallel processing mainly when applied on multi-modality neuroimaging data, such as, MRI morphometric, diffusion tensor imaging, and PET images. Conclusions: We discussed the strengths of RF, considering also possible limitations and by encouraging further studies on the comparisons of this algorithm with other commonly used classification approaches, particularly in the early prediction of the progression from MCI to AD."
29056896,25.0,Classical Statistics and Statistical Learning in Imaging Neuroscience,2017 Oct 6;11:543.,"Brain-imaging research has predominantly generated insight by means of classical statistics, including regression-type analyses and null-hypothesis testing using t-test and ANOVA. Throughout recent years, statistical learning methods enjoy increasing popularity especially for applications in rich and complex data, including cross-validated out-of-sample prediction using pattern classification and sparsity-inducing regression. This concept paper discusses the implications of inferential justifications and algorithmic methodologies in common data analysis scenarios in neuroimaging. It is retraced how classical statistics and statistical learning originated from different historical contexts, build on different theoretical foundations, make different assumptions, and evaluate different outcome metrics to permit differently nuanced conclusions. The present considerations should help reduce current confusion between model-driven classical hypothesis testing and data-driven learning algorithms for investigating the brain with imaging techniques."
29055936,9.0,Machine learning in laboratory medicine: waiting for the flood?,2018 Mar 28;56(4):516-524.,"This review focuses on machine learning and on how methods and models combining data analytics and artificial intelligence have been applied to laboratory medicine so far. Although still in its infancy, the potential for applying machine learning to laboratory data for both diagnostic and prognostic purposes deserves more attention by the readership of this journal, as well as by physician-scientists who will want to take advantage of this new computer-based support in pathology and laboratory medicine."
29049075,2.0,Anesthesia Information Management Systems,2018 Jul;127(1):90-94.,"Anesthesia information management systems (AIMS) have evolved from simple, automated intraoperative record keepers in a select few institutions to widely adopted, sophisticated hardware and software solutions that are integrated into a hospital's electronic health record system and used to manage and document a patient's entire perioperative experience. AIMS implementations have resulted in numerous billing, research, and clinical benefits, yet there remain challenges and areas of potential improvement to AIMS utilization. This article provides an overview of the history of AIMS, the components and features of AIMS, and the benefits and challenges associated with implementing and using AIMS. As AIMS continue to proliferate and data are increasingly shared across multi-institutional collaborations, visual analytics and advanced analytics techniques such as machine learning may be applied to AIMS data to reap even more benefits."
29048559,24.0,Plant phenomics: an overview of image acquisition technologies and image data analysis algorithms,2017 Nov 1;6(11):1-18.,"The study of phenomes or phenomics has been a central part of biology. The field of automatic phenotype acquisition technologies based on images has seen an important advance in the last years. As with other high-throughput technologies, it addresses a common set of problems, including data acquisition and analysis. In this review, we give an overview of the main systems developed to acquire images. We give an in-depth analysis of image processing with its major issues and the algorithms that are being used or emerging as useful to obtain data out of images in an automatic fashion."
29047033,7.0,Characterization of Pulmonary Nodules Based on Features of Margin Sharpness and Texture,2018 Aug;31(4):451-463.,"Lung cancer is the leading cause of cancer-related deaths in the world, and one of its manifestations occurs with the appearance of pulmonary nodules. The classification of pulmonary nodules may be a complex task to specialists due to temporal, subjective, and qualitative aspects. Therefore, it is important to integrate computational tools to the early pulmonary nodule classification process, since they have the potential to characterize objectively and quantitatively the lesions. In this context, the goal of this work is to perform the classification of pulmonary nodules based on image features of texture and margin sharpness. Computed tomography scans were obtained from a publicly available image database. Texture attributes were extracted from a co-occurrence matrix obtained from the nodule volume. Margin sharpness attributes were extracted from perpendicular lines drawn over the borders on all nodule slices. Feature selection was performed by different algorithms. Classification was performed by several machine learning classifiers and assessed by the area under the receiver operating characteristic curve, sensitivity, specificity, and accuracy. Highest classification performance was obtained by a random forest algorithm with all 48 extracted features. However, a decision tree using only two selected features obtained statistically equivalent performance on sensitivity and specificity."
29047032,6.0,Rethinking Skin Lesion Segmentation in a Convolutional Classifier,2018 Aug;31(4):435-440.,"Melanoma is a fatal form of skin cancer when left undiagnosed. Computer-aided diagnosis systems powered by convolutional neural networks (CNNs) can improve diagnostic accuracy and save lives. CNNs have been successfully used in both skin lesion segmentation and classification. For reasons heretofore unclear, previous works have found image segmentation to be, conflictingly, both detrimental and beneficial to skin lesion classification. We investigate the effect of expanding the segmentation border to include pixels surrounding the target lesion. Ostensibly, segmenting a target skin lesion will remove inessential information, non-lesion skin, and artifacts to aid in classification. Our results indicate that segmentation border enlargement produces, to a certain degree, better results across all metrics of interest when using a convolutional based classifier built using the transfer learning paradigm. Consequently, preprocessing methods which produce borders larger than the actual lesion can potentially improve classifier performance, more than both perfect segmentation, using dermatologist created ground truth masks, and no segmentation altogether."
29045685,123.0,MicroRNAs and complex diseases: from experimental results to computational models,2019 Mar 22;20(2):515-539.,"Plenty of microRNAs (miRNAs) were discovered at a rapid pace in plants, green algae, viruses and animals. As one of the most important components in the cell, miRNAs play a growing important role in various essential and important biological processes. For the recent few decades, amounts of experimental methods and computational models have been designed and implemented to identify novel miRNA-disease associations. In this review, the functions of miRNAs, miRNA-target interactions, miRNA-disease associations and some important publicly available miRNA-related databases were discussed in detail. Specially, considering the important fact that an increasing number of miRNA-disease associations have been experimentally confirmed, we selected five important miRNA-related human diseases and five crucial disease-related miRNAs and provided corresponding introductions. Identifying disease-related miRNAs has become an important goal of biomedical research, which will accelerate the understanding of disease pathogenesis at the molecular level and molecular tools design for disease diagnosis, treatment and prevention. Computational models have become an important means for novel miRNA-disease association identification, which could select the most promising miRNA-disease pairs for experimental validation and significantly reduce the time and cost of the biological experiments. Here, we reviewed 20 state-of-the-art computational models of predicting miRNA-disease associations from different perspectives. Finally, we summarized four important factors for the difficulties of predicting potential disease-related miRNAs, the framework of constructing powerful computational models to predict potential miRNA-disease associations including five feasible and important research schemas, and future directions for further development of computational models."
29044470,5.0,Analysis of Gene-Gene Interactions,2017 Oct 18;95:1.14.1-1.14.10.,"The goal of this unit is to introduce epistasis, or gene-gene interactions, as a significant contributor to the genetic architecture of complex traits, including disease susceptibility. This unit begins with an historical overview of the concept of epistasis and the challenges inherent in the identification of potential gene-gene interactions. Then, it reviews statistical and machine learning methods for discovering epistasis in the context of genetic studies of quantitative and categorical traits. This unit concludes with a discussion of meta-analysis, replication, and other topics of active research. © 2017 by John Wiley & Sons, Inc."
29042216,23.0,Mapping human brain lesions and their functional consequences,2018 Jan 15;165:180-189.,"Neuroscience has a long history of inferring brain function by examining the relationship between brain injury and subsequent behavioral impairments. The primary advantage of this method over correlative methods is that it can tell us if a certain brain region is necessary for a given cognitive function. In addition, lesion-based analyses provide unique insights into clinical deficits. In the last decade, statistical voxel-based lesion behavior mapping (VLBM) emerged as a powerful method for understanding the architecture of the human brain. This review illustrates how VLBM improves our knowledge of functional brain architecture, as well as how it is inherently limited by its mass-univariate approach. A wide array of recently developed methods appear to supplement traditional VLBM. This paper provides an overview of these new methods, including the use of specialized imaging modalities, the combination of structural imaging with normative connectome data, as well as multivariate analyses of structural imaging data. We see these new methods as complementing rather than replacing traditional VLBM, providing synergistic tools to answer related questions. Finally, we discuss the potential for these methods to become established in cognitive neuroscience and in clinical applications."
29037014,30.0,Deep into the Brain: Artificial Intelligence in Stroke Imaging,2017 Sep;19(3):277-285.,"Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives."
29035422,6.0,The Use of Telemedicine and Mobile Technology to Promote Population Health and Population Management for Psychiatric Disorders,2017 Oct 16;19(11):88.,"Purpose of review:                    This article discusses recent applications in telemedicine to promote the goals of population health and population management for people suffering psychiatric disorders.              Recent findings:                    The use of telemedicine to promote collaborative care, self-monitoring and chronic disease management, and population screening has demonstrated broad applicability and effectiveness. Collaborative care using videoconferencing to facilitate mental health specialty consults has demonstrated effectiveness in the treatment of depression, PTSD, and also ADHD in pediatric populations. Mobile health is currently being harnessed to monitor patient symptom trajectories with the goal of using machine learning algorithms to predict illness relapse. Patient portals serve as a bridge between patients and providers. They provide an electronically secure shared space for providers and patients to collaborate and optimize care. To date, research has supported the effectiveness of telemedicine in promoting population health. Future endeavors should focus on developing the most effective clinical protocols for using these technologies to ensure long-term use and maximum effectiveness in reducing population burden of mental health."
29031836,3.0,Optofluidic time-stretch quantitative phase microscopy,2018 Mar 1;136:116-125.,"Innovations in optical microscopy have opened new windows onto scientific research, industrial quality control, and medical practice over the last few decades. One of such innovations is optofluidic time-stretch quantitative phase microscopy - an emerging method for high-throughput quantitative phase imaging that builds on the interference between temporally stretched signal and reference pulses by using dispersive properties of light in both spatial and temporal domains in an interferometric configuration on a microfluidic platform. It achieves the continuous acquisition of both intensity and phase images with a high throughput of more than 10,000 particles or cells per second by overcoming speed limitations that exist in conventional quantitative phase imaging methods. Applications enabled by such capabilities are versatile and include characterization of cancer cells and microalgal cultures. In this paper, we review the principles and applications of optofluidic time-stretch quantitative phase microscopy and discuss its future perspective."
29031831,14.0,Large-scale retrieval for medical image analytics: A comprehensive review,2018 Jan;43:66-84.,"Over the past decades, medical image analytics was greatly facilitated by the explosion of digital imaging techniques, where huge amounts of medical images were produced with ever-increasing quality and diversity. However, conventional methods for analyzing medical images have achieved limited success, as they are not capable to tackle the huge amount of image data. In this paper, we review state-of-the-art approaches for large-scale medical image analysis, which are mainly based on recent advances in computer vision, machine learning and information retrieval. Specifically, we first present the general pipeline of large-scale retrieval, summarize the challenges/opportunities of medical image analytics on a large-scale. Then, we provide a comprehensive review of algorithms and techniques relevant to major processes in the pipeline, including feature representation, feature indexing, searching, etc. On the basis of existing work, we introduce the evaluation protocols and multiple applications of large-scale medical image retrieval, with a variety of exploratory and diagnostic scenarios. Finally, we discuss future directions of large-scale retrieval, which can further improve the performance of medical image analysis."
29029029,2.0,A manifesto for cardiovascular imaging: addressing the human factor,2017 Dec 1;18(12):1311-1321.,"Our use of modern cardiovascular imaging tools has not kept pace with their technological development. Diagnostic errors are common but seldom investigated systematically. Rather than more impressive pictures, our main goal should be more precise tests of function which we select because their appropriate use has therapeutic implications which in turn have a beneficial impact on morbidity or mortality. We should practise analytical thinking, use checklists to avoid diagnostic pitfalls, and apply strategies that will reduce biases and avoid overdiagnosis. We should develop normative databases, so that we can apply diagnostic algorithms that take account of variations with age and risk factors and that allow us to calculate pre-test probability and report the post-test probability of disease. We should report the imprecision of a test, or its confidence limits, so that reference change values can be considered in daily clinical practice. We should develop decision support tools to improve the quality and interpretation of diagnostic imaging, so that we choose the single best test irrespective of modality. New imaging tools should be evaluated rigorously, so that their diagnostic performance is established before they are widely disseminated; this should be a shared responsibility of manufacturers with clinicians, leading to cost-effective implementation. Trials should evaluate diagnostic strategies against independent reference criteria. We should exploit advances in machine learning to analyse digital data sets and identify those features that best predict prognosis or responses to treatment. Addressing these human factors will reap benefit for patients, while technological advances continue unpredictably."
29027093,1.0,Long-Term Prognostic Value of Coronary Computed Tomography Angiography,2017 Oct 12;19(12):90.,"Coronary CT angiography (CTA) is a highly accurate test for the diagnosis of coronary artery disease (CAD), with its use guided by numerous contemporary appropriate use criteria and clinical guidelines. Unique among non-invasive tests for CAD, coronary CTA provides direct visualization of coronary atherosclerosis for the assessment of angiographic stenosis, as well as validated measures of plaque vulnerability. Long-term studies now clearly demonstrate that the absence of CAD on coronary CTA identifies a patient that is at very low risk for future cardiovascular events. Conversely, the presence, location, and severity of CAD as measured on coronary CTA provide powerful prognostic information that is superior to traditional risk factors and other clinical variables. Observational studies and data obtained from clinical trials suggest that the anatomic information derived from coronary CTA significantly increases the utilization of statins and aspirin. Furthermore, these changes are associated with reductions in the risk for mortality, revascularizations, and incident myocardial infarctions among subjects with coronary atherosclerosis. As a result, current societal consensus statements have attempted to standardize coronary CTA reporting, to include incorporation of vulnerable plaque features and recommendations on the use of preventive therapies, such as statins, so to more consistently link important prognostic findings on coronary CTA to appropriate preventive and therapeutic interventions. Automated measures of total coronary plaque volume, machine learning, and CT-derived fractional flow reserve may further refine the prognostic accuracy of coronary CTA. Herein, we summarize recently published literature that reports the long-term (≥ 5 years of follow-up) prognostic usefulness of coronary CTA."
29025029,,CT coronary imaging-a fast evolving world,2018 Sep 1;111(9):595-604.,"Computed tomography (CT) has become an important modality in the evaluation of coronary artery disease (CAD). The tremendous technological advances in CT in the last two decades has made it possible to obtain high quality images of coronary arteries with high spatial and temporal resolutions. Multiple trials have confirmed the accuracy of CT compared to invasive catheter angiography. CT is also able to evaluate beyond the lumen in characterizing and quantifying atherosclerotic plaques, including evaluation of high risk features. Although CTA has low specificity in identification of lesion-specific ischemia, functional techniques are now possible such as CT myocardial perfusion and CT-fractional flow reserve (FFR) which evaluate the hemodynamic significance of stenosis and help with revascularization strategies. Multi-energy CT provides additional information beyond what is possible with a conventional CT and is useful in variety of clinical applications, including myocardial perfusion imaging, lesion characterization and low contrast studies. Large trials have confirmed the ability of CT to predict major adverse cardiovascular events and recent trials have even demonstrated improved clinical outcomes by using CT for the evaluation of CAD. CT is also useful in structural heart disease and 3 D printing is now increasingly used for surgical/interventional planning. Machine learning is evolving rapidly and is likely to impact diagnosis and management."
29020316,39.0,Machine Learning for Healthcare: On the Verge of a Major Shift in Healthcare Epidemiology,2018 Jan 6;66(1):149-153.,"The increasing availability of electronic health data presents a major opportunity in healthcare for both discovery and practical applications to improve healthcare. However, for healthcare epidemiologists to best use these data, computational techniques that can handle large complex datasets are required. Machine learning (ML), the study of tools and methods for identifying patterns in data, can help. The appropriate application of ML to these data promises to transform patient risk stratification broadly in the field of medicine and especially in infectious diseases. This, in turn, could lead to targeted interventions that reduce the spread of healthcare-associated pathogens. In this review, we begin with an introduction to the basics of ML. We then move on to discuss how ML can transform healthcare epidemiology, providing examples of successful applications. Finally, we present special considerations for those healthcare epidemiologists who want to use and apply ML."
29018336,23.0,Encoding and Decoding Models in Cognitive Electrophysiology,2017 Sep 26;11:61.,"Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of ""Encoding"" models, in which stimulus features are used to model brain activity, and ""Decoding"" models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aim is to provide a practical understanding of predictive modeling of human brain data and to propose best-practices in conducting these analyses."
28992412,3.0,Protein Science by DNA Sequencing: How Advances in Molecular Biology Are Accelerating Biochemistry,2018 Jan 9;57(1):38-46.,"A fundamental goal of protein biochemistry is to determine the sequence-function relationship, but the vastness of sequence space makes comprehensive evaluation of this landscape difficult. However, advances in DNA synthesis and sequencing now allow researchers to assess the functional impact of every single mutation in many proteins, but challenges remain in library construction and the development of general assays applicable to a diverse range of protein functions. This Perspective briefly outlines the technical innovations in DNA manipulation that allow massively parallel protein biochemistry and then summarizes the methods currently available for library construction and the functional assays of protein variants. Areas in need of future innovation are highlighted with a particular focus on assay development and the use of computational analysis with machine learning to effectively traverse the sequence-function landscape. Finally, applications in the fundamentals of protein biochemistry, disease prediction, and protein engineering are presented."
28990839,3.0,A review of intelligent systems for heart sound signal analysis,2017 Oct;41(7):553-563.,"Intelligent computer-aided diagnosis (CAD) systems can enhance the diagnostic capabilities of physicians and reduce the time required for accurate diagnosis. CAD systems could provide physicians with a suggestion about the diagnostic of heart diseases. The objective of this paper is to review the recent published preprocessing, feature extraction and classification techniques and their state of the art of phonocardiogram (PCG) signal analysis. Published literature reviewed in this paper shows the potential of machine learning techniques as a design tool in PCG CAD systems and reveals that the CAD systems for PCG signal analysis are still an open problem. Related studies are compared to their datasets, feature extraction techniques and the classifiers they used. Current achievements and limitations in developing CAD systems for PCG signal analysis using machine learning techniques are presented and discussed. In the light of this review, a number of future research directions for PCG signal analysis are provided."
28990168,1.0,Automated analysis of seizure semiology and brain electrical activity in presurgery evaluation of epilepsy: A focused survey,2017 Nov;58(11):1817-1831.,"Epilepsy being one of the most prevalent neurological disorders, affecting approximately 50 million people worldwide, and with almost 30-40% of patients experiencing partial epilepsy being nonresponsive to medication, epilepsy surgery is widely accepted as an effective therapeutic option. Presurgical evaluation has advanced significantly using noninvasive techniques based on video monitoring, neuroimaging, and electrophysiological and neuropsychological tests; however, certain clinical settings call for invasive intracranial recordings such as stereoelectroencephalography (SEEG), aiming to accurately map the eloquent brain networks involved during a seizure. Most of the current presurgical evaluation procedures focus on semiautomatic techniques, where surgery diagnosis relies immensely on neurologists' experience and their time-consuming subjective interpretation of semiology or the manifestations of epilepsy and their correlation with the brain's electrical activity. Because surgery misdiagnosis reaches a rate of 30%, and more than one-third of all epilepsies are poorly understood, there is an evident keen interest in improving diagnostic precision using computer-based methodologies that in the past few years have shown near-human performance. Among them, deep learning has excelled in many biological and medical applications, but has advanced insufficiently in epilepsy evaluation and automated understanding of neural bases of semiology. In this paper, we systematically review the automatic applications in epilepsy for human motion analysis, brain electrical activity, and the anatomoelectroclinical correlation to attribute anatomical localization of the epileptogenic network to distinctive epilepsy patterns. Notably, recent advances in deep learning techniques will be investigated in the contexts of epilepsy to address the challenges exhibited by traditional machine learning techniques. Finally, we discuss and propose future research on epilepsy surgery assessment that can jointly learn across visually observed semiologic patterns and recorded brain electrical activity."
28986230,30.0,Machine Learning and Neurosurgical Outcome Prediction: A Systematic Review,2018 Jan;109:476-486.e1.,"Objective:                    Accurate measurement of surgical outcomes is highly desirable to optimize surgical decision-making. An important element of surgical decision making is identification of the patient cohort that will benefit from surgery before the intervention. Machine learning (ML) enables computers to learn from previous data to make accurate predictions on new data. In this systematic review, we evaluate the potential of ML for neurosurgical outcome prediction.              Methods:                    A systematic search in the PubMed and Embase databases was performed to identify all potential relevant studies up to January 1, 2017.              Results:                    Thirty studies were identified that evaluated ML algorithms used as prediction models for survival, recurrence, symptom improvement, and adverse events in patients undergoing surgery for epilepsy, brain tumor, spinal lesions, neurovascular disease, movement disorders, traumatic brain injury, and hydrocephalus. Depending on the specific prediction task evaluated and the type of input features included, ML models predicted outcomes after neurosurgery with a median accuracy and area under the receiver operating curve of 94.5% and 0.83, respectively. Compared with logistic regression, ML models performed significantly better and showed a median absolute improvement in accuracy and area under the receiver operating curve of 15% and 0.06, respectively. Some studies also demonstrated a better performance in ML models compared with established prognostic indices and clinical experts.              Conclusions:                    In the research setting, ML has been studied extensively, demonstrating an excellent performance in outcome prediction for a wide range of neurosurgical conditions. However, future studies should investigate how ML can be implemented as a practical tool supporting neurosurgical care."
28985937,2.0,Applications of Resting State Functional MR Imaging to Traumatic Brain Injury,2017 Nov;27(4):685-696.,"Traumatic brain injury (TBI) is an important public health issue. TBI includes a broad spectrum of injury severities and abnormalities. Functional MR imaging (fMR imaging), both resting state (rs) and task, has been used often in research to study the effects of TBI. Although rs-fMR imaging is not currently applicable in clinical diagnosis of TBI, computer-aided tools are making this a possibility for the future. Specifically, graph theory is being used to study the change in networks after TBI. Machine learning methods allow researchers to build models capable of predicting injury severity and recovery trajectories."
28985932,1.0,Machine Learning Applications to Resting-State Functional MR Imaging Analysis,2017 Nov;27(4):609-620.,"Machine learning is one of the most exciting and rapidly expanding fields within computer science. Academic and commercial research entities are investing in machine learning methods, especially in personalized medicine via patient-level classification. There is great promise that machine learning methods combined with resting state functional MR imaging will aid in diagnosis of disease and guide potential treatment for conditions thought to be impossible to identify based on imaging alone, such as psychiatric disorders. We discuss machine learning methods and explore recent advances."
28982791,63.0,"Radiomics in Brain Tumor: Image Assessment, Quantitative Feature Descriptors, and Machine-Learning Approaches",2018 Feb;39(2):208-216.,"Radiomics describes a broad set of computational methods that extract quantitative features from radiographic images. The resulting features can be used to inform imaging diagnosis, prognosis, and therapy response in oncology. However, major challenges remain for methodologic developments to optimize feature extraction and provide rapid information flow in clinical settings. Equally important, to be clinically useful, predictive radiomic properties must be clearly linked to meaningful biologic characteristics and qualitative imaging properties familiar to radiologists. Here we use a cross-disciplinary approach to highlight studies in radiomics. We review brain tumor radiologic studies (eg, imaging interpretation) through computational models (eg, computer vision and machine learning) that provide novel clinical insights. We outline current quantitative image feature extraction and prediction strategies with different levels of available clinical classes for supporting clinical decision-making. We further discuss machine-learning challenges and data opportunities to advance radiomic studies."
28981352,13.0,Imaging Genetic Heterogeneity in Glioblastoma and Other Glial Tumors: Review of Current Methods and Future Directions,2018 Jan;210(1):30-38.,"Objective:                    The purpose of this review is to summarize advances in the molecular analysis of gliomas, the role genetics plays in MRI features, and how machine-learning approaches can be used to survey the tumoral environment.              Conclusion:                    The genetic profile of gliomas influences the course of treatment and clinical outcomes. Though biopsy is the reference standard for determining tumor genetics, it can suffer diagnostic delays due to surgical planning and pathologic assessment. Radiogenomics may allow rapid, low-risk characterization of genetic heterogeneity."
28980044,,"Analysis, Recognition, and Classification of Biological Membrane Images",2017;227:119-140.,"Biological membrane images contain a variety of objects and patterns, which convey information about the underlying biological structures and mechanisms. The field of image analysis includes methods of computation which convert features and objects identified in images into quantitative information about biological structures represented in these images. Microscopy images are complex, noisy, and full of artifacts and consequently require multiple image processing steps for the extraction of meaningful quantitative information. This review is focused on methods of analysis of images of cells and biological membranes such as detection, segmentation, classification and machine learning, registration, tracking, and visualization. These methods could make possible, for example, to automatically identify defects in the cell membrane which affect physiological processes. Detailed analysis of membrane images could facilitate understanding of the underlying physiological structures or help in the interpretation of biological experiments."
28974388,2.0,(Machine-)Learning to analyze in vivo microscopy: Support vector machines,2017 Nov;1865(11 Pt B):1719-1727.,"The development of new microscopy techniques for super-resolved, long-term monitoring of cellular and subcellular dynamics in living organisms is revealing new fundamental aspects of tissue development and repair. However, new microscopy approaches present several challenges. In addition to unprecedented requirements for data storage, the analysis of high resolution, time-lapse images is too complex to be done manually. Machine learning techniques are ideally suited for the (semi-)automated analysis of multidimensional image data. In particular, support vector machines (SVMs), have emerged as an efficient method to analyze microscopy images obtained from animals. Here, we discuss the use of SVMs to analyze in vivo microscopy data. We introduce the mathematical framework behind SVMs, and we describe the metrics used by SVMs and other machine learning approaches to classify image data. We discuss the influence of different SVM parameters in the context of an algorithm for cell segmentation and tracking. Finally, we describe how the application of SVMs has been critical to study protein localization in yeast screens, for lineage tracing in C. elegans, or to determine the developmental stage of Drosophila embryos to investigate gene expression dynamics. We propose that SVMs will become central tools in the analysis of the complex image data that novel microscopy modalities have made possible. This article is part of a Special Issue entitled: Biophysics in Canada, edited by Lewis Kay, John Baenziger, Albert Berghuis and Peter Tieleman."
28971622,2.0,"Perspectives, potentials and trends of ex vivo and in vivo optical molecular pathology",2018 Jan;11(1).,"It is pivotal for medical applications, such as noninvasive histopathologic characterization of tissue, to realize label-free and molecule-specific representation of morphologic and biochemical composition in real-time with subcellular spatial resolution. This unmet clinical need requires new approaches for rapid and reliable real-time assessment of pathologies to complement established diagnostic tools. Photonic imaging combined with digitalization offers the potential to provide the clinician the requested information both under in vivo and ex vivo conditions. This report summarizes photonic approaches and their use in combination with image processing, machine learning and augmented virtual reality that might solve current challenges in modern medicine. Details are given for pathology, intraoperative diagnosis in head and neck cancer and endoscopic diagnosis in gastroenterology."
28971278,8.0,A Systematic Review of Wearable Systems for Cancer Detection: Current State and Challenges,2017 Oct 2;41(11):180.,"Rapid growth of sensor and computing platforms have introduced the wearable systems. In recent years, wearable systems have led to new applications across all medical fields. The aim of this review is to present current state-of-the-art approach in the field of wearable system based cancer detection and identify key challenges that resist it from clinical adoption. A total of 472 records were screened and 11 were finally included in this study. Two types of records were studied in this context that includes 45% research articles and 55% manufactured products. The review was performed per PRISMA guidelines where considerations was given to records that were published or reported between 2009 and 2017. The identified records included 4 cancer detecting wearable systems such as breast cancer (36.3%), skin cancer (36.3%), prostate cancer (18.1%), and multi-type cancer (9%). Most works involved sensor based smart systems comprising of microcontroller, Bluetooth module, and smart phone. Few demonstrated Ultra-Wide Band (i.e. UWB) antenna based wearable systems. Skin cancer detecting wearable systems were most comprehensible ones. The current works are gradually progressing with seamless integration of sensory units along with smart networking. However, they lack in cloud computing and long-range communication paradigms. Artificial intelligence and machine learning are key ports that need to be attached with current wearable systems. Further, clinical inertia, lack of awareness, and high cost are altogether pulling back the actual growth of such system. It is well comprehended that upon sincere orientation of all identified challenges, wearable systems would emerge as vital alternative to futuristic cancer detection."
32309594,,Artificial Intelligence versus Doctors' Intelligence: A Glance on Machine Learning Benefaction in Electrocardiography,2017 Sep 30;5(3):e76.,"Computational machine learning, especially self-enhancing algorithms, prove remarkable effectiveness in applications, including cardiovascular medicine. This review summarizes and cross-compares the current machine learning algorithms applied to electrocardiogram interpretation. In practice, continuous real-time monitoring of electrocardiograms is still difficult to realize. Furthermore, automated ECG interpretation by implementing specific artificial intelligence algorithms is even more challenging. By collecting large datasets from one individual, computational approaches can assure an efficient personalized treatment strategy, such as a correct prediction on patient-specific disease progression, therapeutic success rate and limitations of certain interventions, thus reducing the hospitalization costs and physicians' workload. Clearly such aims can be achieved by a perfect symbiosis of a multidisciplinary team involving clinicians, researchers and computer scientists. Summarizing, continuous cross-examination between machine intelligence and human intelligence is a combination of precision, rationale and high-throughput scientific engine integrated into a challenging framework of big data science."
28956772,20.0,Machine Learning Approaches in Cardiovascular Imaging,2017 Oct;10(10):e005614.,"Cardiovascular imaging technologies continue to increase in their capacity to capture and store large quantities of data. Modern computational methods, developed in the field of machine learning, offer new approaches to leveraging the growing volume of imaging data available for analyses. Machine learning methods can now address data-related problems ranging from simple analytic queries of existing measurement data to the more complex challenges involved in analyzing raw images. To date, machine learning has been used in 2 broad and highly interconnected areas: automation of tasks that might otherwise be performed by a human and generation of clinically important new knowledge. Most cardiovascular imaging studies have focused on task-oriented problems, but more studies involving algorithms aimed at generating new clinical insights are emerging. Continued expansion in the size and dimensionality of cardiovascular imaging databases is driving strong interest in applying powerful deep learning methods, in particular, to analyze these data. Overall, the most effective approaches will require an investment in the resources needed to appropriately prepare such large data sets for analyses. Notwithstanding current technical and logistical challenges, machine learning and especially deep learning methods have much to offer and will substantially impact the future practice and science of cardiovascular imaging."
28954825,20.0,Deep learning guided stroke management: a review of clinical applications,2018 Apr;10(4):358-362.,"Stroke is a leading cause of long-term disability, and outcome is directly related to timely intervention. Not all patients benefit from rapid intervention, however. Thus a significant amount of attention has been paid to using neuroimaging to assess potential benefit by identifying areas of ischemia that have not yet experienced cellular death. The perfusion-diffusion mismatch, is used as a simple metric for potential benefit with timely intervention, yet penumbral patterns provide an inaccurate predictor of clinical outcome. Machine learning research in the form of deep learning (artificial intelligence) techniques using deep neural networks (DNNs) excel at working with complex inputs. The key areas where deep learning may be imminently applied to stroke management are image segmentation, automated featurization (radiomics), and multimodal prognostication. The application of convolutional neural networks, the family of DNN architectures designed to work with images, to stroke imaging data is a perfect match between a mature deep learning technique and a data type that is naturally suited to benefit from deep learning's strengths. These powerful tools have opened up exciting opportunities for data-driven stroke management for acute intervention and for guiding prognosis. Deep learning techniques are useful for the speed and power of results they can deliver and will become an increasingly standard tool in the modern stroke specialist's arsenal for delivering personalized medicine to patients with ischemic stroke."
28949520,14.0,"Interpretation of Quantitative Structure-Activity Relationship Models: Past, Present, and Future",2017 Nov 27;57(11):2618-2639.,"This paper is an overview of the most significant and impactful interpretation approaches of quantitative structure-activity relationship (QSAR) models, their development, and application. The evolution of the interpretation paradigm from ""model → descriptors → (structure)"" to ""model → structure"" is indicated. The latter makes all models interpretable regardless of machine learning methods or descriptors used for modeling. This opens wide prospects for application of corresponding interpretation approaches to retrieve structure-property relationships captured by any models. Issues of separate approaches are discussed as well as general issues and prospects of QSAR model interpretation."
28932174,,Internet-based computer technology on radiotherapy,Nov-Dec 2017;22(6):455-462.,"Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well."
28930544,2.0,Neural signatures of attention: insights from decoding population activity patterns,2018 Jan 1;23:221-246.,"Understanding brain function and the computations that individual neurons and neuronal ensembles carry out during cognitive functions is one of the biggest challenges in neuroscientific research. To this end, invasive electrophysiological studies have provided important insights by recording the activity of single neurons in behaving animals. To average out noise, responses are typically averaged across repetitions and across neurons that are usually recorded on different days. However, the brain makes decisions on short time scales based on limited exposure to sensory stimulation by interpreting responses of populations of neurons on a moment to moment basis. Recent studies have employed machine-learning algorithms in attention and other cognitive tasks to decode the information content of distributed activity patterns across neuronal ensembles on a single trial basis. Here, we review results from studies that have used pattern-classification decoding approaches to explore the population representation of cognitive functions. These studies have offered significant insights into population coding mechanisms. Moreover, we discuss how such advances can aid the development of cognitive brain-computer interfaces."
28918672,6.0,"Protein complexes, big data, machine learning and integrative proteomics: lessons learned over a decade of systematic analysis of protein interaction networks",2017 Oct;14(10):845-855.,"Elucidation of the networks of physical (functional) interactions present in cells and tissues is fundamental for understanding the molecular organization of biological systems, the mechanistic basis of essential and disease-related processes, and for functional annotation of previously uncharacterized proteins (via guilt-by-association or -correlation). After a decade in the field, we felt it timely to document our own experiences in the systematic analysis of protein interaction networks. Areas covered: Researchers worldwide have contributed innovative experimental and computational approaches that have driven the rapidly evolving field of 'functional proteomics'. These include mass spectrometry-based methods to characterize macromolecular complexes on a global-scale and sophisticated data analysis tools - most notably machine learning - that allow for the generation of high-quality protein association maps. Expert commentary: Here, we recount some key lessons learned, with an emphasis on successful workflows, and challenges, arising from our own and other groups' ongoing efforts to generate, interpret and report proteome-scale interaction networks in increasingly diverse biological contexts."
28914640,9.0,Machine learning: novel bioinformatics approaches for combating antimicrobial resistance,2017 Dec;30(6):511-517.,"Purpose of review:                    Antimicrobial resistance (AMR) is a threat to global health and new approaches to combating AMR are needed. Use of machine learning in addressing AMR is in its infancy but has made promising steps. We reviewed the current literature on the use of machine learning for studying bacterial AMR.              Recent findings:                    The advent of large-scale data sets provided by next-generation sequencing and electronic health records make applying machine learning to the study and treatment of AMR possible. To date, it has been used for antimicrobial susceptibility genotype/phenotype prediction, development of AMR clinical decision rules, novel antimicrobial agent discovery and antimicrobial therapy optimization.              Summary:                    Application of machine learning to studying AMR is feasible but remains limited. Implementation of machine learning in clinical settings faces barriers to uptake with concerns regarding model interpretability and data quality.Future applications of machine learning to AMR are likely to be laboratory-based, such as antimicrobial susceptibility phenotype prediction."
28905541,3.0,The Role of New Imaging Methods in Managing Age-Related Macular Degeneration,Nov-Dec 2017;6(6):498-507.,"The use of imaging for age-related macular degeneration (AMD) depends on how it benefits clinical management and on reimbursement. The latter should relate to the former. This review assesses how different forms of AMD can be imaged and what information this provides. For nonneovascular AMD high-resolution optical coherence tomography (OCT), autofluorescence, and near infrared imaging can identify the type of drusen, such as reticular pseudodrusen, which influences prognosis, and the amount of atrophy, for which phase 3 trials are underway. Clarifying the correct diagnosis for late-onset Stargardt and macular telangiectasia, if treatment becomes available, will be especially important. Choroidal thickness can be measured and changes with anti‒vascular endothelial growth factor treatment, but how this influences management is less clear. The finding of a thick choroid may alter the diagnosis to pachychoroid neovasculopathy, which may have a different treatment response. Peripheral retinal changes are commonly found on ultrawide-field imaging but their importance is not yet determined. The mainstay of imaging is OCT, which can detect neovascular AMD by detecting thickening and be used for follow-up, as the presence or absence of thickening is the main determinant of treatment. Higher resolution systems and now OCT angiography are able to distinguish neovascular type, especially type 2 choroidal neovascularization but also polypoidal choroidal vasculopathy and retinal angiomatous proliferation. Fundus fluorescein and indocyanine green angiographies still have a role, although that partly depends on whether photodynamic therapy is being considered. Automated image analysis and machine learning will be increasingly important in supporting clinician decisions."
