pmid,title,date,text,citations
32962078,Review: Application and Prospective Discussion of Machine Learning for the Management of Dairy Farms,2020 Sep 18;10(9):1690.,"Dairy farmers use herd management systems, behavioral sensors, feeding lists, breeding schedules, and health records to document herd characteristics. Consequently, large amounts of dairy data are becoming available. However, a lack of data integration makes it difficult for farmers to analyze the data on their dairy farm, which indicates that these data are currently not being used to their full potential. Hence, multiple issues in dairy farming such as low longevity, poor performance, and health issues remain. We aimed to evaluate whether machine learning (ML) methods can solve some of these existing issues in dairy farming. This review summarizes peer-reviewed ML papers published in the dairy sector between 2015 and 2020. Ultimately, 97 papers from the subdomains of management, physiology, reproduction, behavior analysis, and feeding were considered in this review. The results confirm that ML algorithms have become common tools in most areas of dairy research, particularly to predict data. Despite the quantity of research available, most tested algorithms have not performed sufficiently for a reliable implementation in practice. This may be due to poor training data. The availability of data resources from multiple farms covering longer periods would be useful to improve prediction accuracies. In conclusion, ML is a promising tool in dairy research, which could be used to develop and improve decision support for farmers. As the cow is a multifactorial system, ML algorithms could analyze integrated data sources that describe and ultimately allow managing cows according to all relevant influencing factors. However, both the integration of multiple data sources and the obtainability of public data currently remain challenging.",
32961909,Data-Driven Molecular Dynamics: A Multifaceted Challenge,2020 Sep 18;13(9):253.,"The big data concept is currently revolutionizing several fields of science including drug discovery and development. While opening up new perspectives for better drug design and related strategies, big data analysis strongly challenges our current ability to manage and exploit an extraordinarily large and possibly diverse amount of information. The recent renewal of machine learning (ML)-based algorithms is key in providing the proper framework for addressing this issue. In this respect, the impact on the exploitation of molecular dynamics (MD) simulations, which have recently reached mainstream status in computational drug discovery, can be remarkable. Here, we review the recent progress in the use of ML methods coupled to biomolecular simulations with potentially relevant implications for drug design. Specifically, we show how different ML-based strategies can be applied to the outcome of MD simulations for gaining knowledge and enhancing sampling. Finally, we discuss how intrinsic limitations of MD in accurately modeling biomolecular systems can be alleviated by including information coming from experimental data.",1.0
32961749,Comprehensive Survey and Comparative Assessment of RNA-Binding Residue Predictions with Analysis by RNA Type,2020 Sep 19;21(18):6879.,"With close to 30 sequence-based predictors of RNA-binding residues (RBRs), this comparative survey aims to help with understanding and selection of the appropriate tools. We discuss past reviews on this topic, survey a comprehensive collection of predictors, and comparatively assess six representative methods. We provide a novel and well-designed benchmark dataset and we are the first to report and compare protein-level and datasets-level results, and to contextualize performance to specific types of RNAs. The methods considered here are well-cited and rely on machine learning algorithms on occasion combined with homology-based prediction. Empirical tests reveal that they provide relatively accurate predictions. Virtually all methods perform well for the proteins that interact with rRNAs, some generate accurate predictions for mRNAs, snRNA, SRP and IRES, while proteins that bind tRNAs are predicted poorly. Moreover, except for DRNApred, they confuse DNA and RNA-binding residues. None of the six methods consistently outperforms the others when tested on individual proteins. This variable and complementary protein-level performance suggests that users should not rely on applying just the single best dataset-level predictor. We recommend that future work should focus on the development of approaches that facilitate protein-level selection of accurate predictors and the consensus-based prediction of RBRs.",
32961306,Dementia medical screening using mobile applications: A systematic review with a new mapping model,2020 Nov;111:103573.,"Early detection is the key to successfully tackling dementia, a neurocognitive condition common among the elderly. Therefore, screening using technological platforms such as mobile applications (apps) may provide an important opportunity to speed up the diagnosis process and improve accessibility. Due to the lack of research into dementia diagnosis and screening tools based on mobile apps, this systematic review aims to identify the available mobile-based dementia and mild cognitive impairment (MCI) apps using specific inclusion and exclusion criteria. More importantly, we critically analyse these tools in terms of their comprehensiveness, validity, performance, and the use of artificial intelligence (AI) techniques. The research findings suggest diagnosticians in a clinical setting use dementia screening apps such as ALZ and CognitiveExams since they cover most of the domains for the diagnosis of neurocognitive disorders. Further, apps such as Cognity and ACE-Mobile have great potential as they use machine learning (ML) and AI techniques, thus improving the accuracy of the outcome and the efficiency of the screening process. Lastly, there was overlapping among the dementia screening apps in terms of activities and questions they contain therefore mapping these apps to the designated cognitive domains is a challenging task, which has been done in this research.",1.0
32960410,Image-based state-of-the-art techniques for the identification and classification of brain diseases: a review,2020 Nov;58(11):2603-2620.,"Detection and classification methods have a vital and important role in identifying brain diseases. Timely detection and classification of brain diseases enable an accurate identification and effective management of brain impairment. Brain disorders are commonly most spreadable diseases and the diagnosing process is time-consuming and highly expensive. There is an utmost need to develop effective and advantageous methods for brain diseases detection and characterization. Magnetic resonance imaging (MRI), computed tomography (CT), and other various brain imaging scans are used to identify different brain diseases and disorders. Brain imaging scans are the efficient tool to understand the anatomical changes in brain in fast and accurate manner. These different brain imaging scans used with segmentation techniques and along with machine learning and deep learning techniques give maximum accuracy and efficiency. This paper focuses on different conventional approaches, machine learning and deep learning techniques used for the detection, and classification of brain diseases and abnormalities. This paper also summarizes the research gap and problems in the existing techniques used for detection and classification of brain disorders. Comparison and evaluation of different machine learning and deep learning techniques in terms of efficiency and accuracy are also highlighted in this paper. Furthermore, different brain diseases like leukoariaosis, Alzheimer's, Parkinson's, and Wilson's disorder are studied in the scope of machine learning and deep learning techniques.",1.0
32959233,Popular Computational Tools Used for miRNA Prediction and Their Future Development Prospects,2020 Dec;12(4):395-413.,"MicroRNAs (miRNAs) are 19-24 nucleotide (nt)-long noncoding, single-stranded RNA molecules that play significant roles in regulating the gene expression, growth, and development of plants and animals. From the year that miRNAs were first discovered until the beginning of the twenty-first century, researchers used experimental methods such as cloning and sequencing to identify new miRNAs and their roles in the posttranscriptional regulation of protein synthesis. Later, in the early 2000s, informatics approaches to the discovery of new miRNAs began to be implemented. With increasing knowledge about miRNA, more efficient algorithms have been developed for computational miRNA prediction. The miRNA research community, hoping for greater coverage and faster results, has shifted from cumbersome and expensive traditional experimental approaches to computational approaches. These computational methods started with homology-based comparisons of known miRNAs with orthologs in the genomes of other species; this method could identify a known miRNA in new species. Second-generation sequencing and next-generation sequencing of mRNA at different developmental stages and in specific tissues, in combination with a better search and alignment algorithm, have accelerated the process of predicting novel miRNAs in a particular species. Using the accumulated annotated miRNA sequence information, researchers have been able to design ab initio algorithms for miRNA prediction independent of genome sequence knowledge. Here, the methods recently used for miRNA computational prediction are summarized and classified into the following four categories: homology-based, target-based, scoring-based, and machine-learning-based approaches. Finally, the future developmental directions of miRNA prediction methods are discussed.",
32957598,AI Approaches Towards Prechtl's Assessment of General Movements: A Systematic Literature Review,2020 Sep 17;20(18):5321.,"General movements (GMs) are spontaneous movements of infants up to five months post-term involving the whole body varying in sequence, speed, and amplitude. The assessment of GMs has shown its importance for identifying infants at risk for neuromotor deficits, especially for the detection of cerebral palsy. As the assessment is based on videos of the infant that are rated by trained professionals, the method is time-consuming and expensive. Therefore, approaches based on Artificial Intelligence have gained significantly increased attention in the last years. In this article, we systematically analyze and discuss the main design features of all existing technological approaches seeking to transfer the Prechtl's assessment of general movements from an individual visual perception to computer-based analysis. After identifying their shared shortcomings, we explain the methodological reasons for their limited practical performance and classification rates. As a conclusion of our literature study, we conceptually propose a methodological solution to the defined problem based on the groundbreaking innovation in the area of Deep Learning.",2.0
32955675,Technological advancements and opportunities in Neuromarketing: a systematic review,2020 Sep 21;7(1):10.,"Neuromarketing has become an academic and commercial area of interest, as the advancements in neural recording techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response of consumers to the marketing stimuli. This article presents the very first systematic review of the technological advancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a total of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic or empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both product and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band signals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha asymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional magnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to its low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, skin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical studies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component analysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and classification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) have performed with the highest average accuracy among other machine learning algorithms used in these literatures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarketing for making novel contributions.",
32954550,"High Throughput Methods in the Synthesis, Characterization, and Optimization of Porous Materials",2020 Nov;32(44):e2002780.,"Porous materials are widely employed in a large range of applications, in particular, for storage, separation, and catalysis of fine chemicals. Synthesis, characterization, and pre- and post-synthetic computer simulations are mostly carried out in a piecemeal and ad hoc manner. Whilst high throughput approaches have been used for more than 30 years in the porous material fields, routine integration of experimental and computational processes is only now becoming more established. Herein, important developments are highlighted and emerging challenges for the community identified, including the need to work toward more integrated workflows.",
32952403,"Artificial Intelligence, Machine Learning, and Cardiovascular Disease",2020 Sep 9;14:1179546820927404.,"Artificial intelligence (AI)-based applications have found widespread applications in many fields of science, technology, and medicine. The use of enhanced computing power of machines in clinical medicine and diagnostics has been under exploration since the 1960s. More recently, with the advent of advances in computing, algorithms enabling machine learning, especially deep learning networks that mimic the human brain in function, there has been renewed interest to use them in clinical medicine. In cardiovascular medicine, AI-based systems have found new applications in cardiovascular imaging, cardiovascular risk prediction, and newer drug targets. This article aims to describe different AI applications including machine learning and deep learning and their applications in cardiovascular medicine. AI-based applications have enhanced our understanding of different phenotypes of heart failure and congenital heart disease. These applications have led to newer treatment strategies for different types of cardiovascular diseases, newer approach to cardiovascular drug therapy and postmarketing survey of prescription drugs. However, there are several challenges in the clinical use of AI-based applications and interpretation of the results including data privacy, poorly selected/outdated data, selection bias, and unintentional continuance of historical biases/stereotypes in the data which can lead to erroneous conclusions. Still, AI is a transformative technology and has immense potential in health care.",
32950874,Use and performance of machine learning models for type 2 diabetes prediction in community settings: A systematic review and meta-analysis,2020 Nov;143:104268.,"Objective:                    We aimed to identify machine learning (ML) models for type 2 diabetes (T2DM) prediction in community settings and determine their predictive performance.              Method:                    Systematic review of ML predictive modelling studies in 13 databases since 2009 was conducted. Primary outcomes included metrics of discrimination, calibration, and classification. Secondary outcomes included important variables, level of validation, and intended use of models. Meta-analysis of c-indices, subgroup analyses, meta-regression, publication bias assessments and sensitivity analyses were conducted.              Results:                    Twenty-three studies (40 prediction models) were included. Studies with high-, moderate-, and low- risk of bias were 3, 14, and 6 respectively. All studies conducted internal validation whereas none conducted external validation of their models. Twenty studies provided classification metrics to varying extents whereas only 7 studies performed model calibration. Eighteen studies reported information on both the variables used for model development and the feature importance. Twelve studies highlighted potential applicability of their models for T2DM screening. Meta-analysis produced a good pooled c-index (0.812). Sources of heterogeneity were identified through subgroup analyses and meta-regression. Issues pertaining to methodological quality and reporting were observed.              Conclusions:                    We found evidence of good performance of ML models for T2DM prediction in the community. Improvements to methodology, reporting and validation are needed before they can be used at scale.",1.0
32948476,Developing an AI project,2020 Dec;51(4):550-559.,"Artificial intelligence applications can very powerful in areas of speech recognition, image processing and identification, medical diagnosis and clustering to name a few. There is a perception that developing your own artificial intelligence (AI) application can be a daunting task, requiring in-depth knowledge and programming skills. This is not entirely true since many desktop and laptop systems have computing power that can accommodate machine- and deeplearning development, the available options for code development and a broad support base. A generic guide in developing a platform for AI project development is presented.",
32946413,Artificial Intelligence for COVID-19: Rapid Review,2020 Oct 27;22(10):e21476.,"Background:                    COVID-19 was first discovered in December 2019 and has since evolved into a pandemic.              Objective:                    To address this global health crisis, artificial intelligence (AI) has been deployed at various levels of the health care system. However, AI has both potential benefits and limitations. We therefore conducted a review of AI applications for COVID-19.              Methods:                    We performed an extensive search of the PubMed and EMBASE databases for COVID-19-related English-language studies published between December 1, 2019, and March 31, 2020. We supplemented the database search with reference list checks. A thematic analysis and narrative review of AI applications for COVID-19 was conducted.              Results:                    In total, 11 papers were included for review. AI was applied to COVID-19 in four areas: diagnosis, public health, clinical decision making, and therapeutics. We identified several limitations including insufficient data, omission of multimodal methods of AI-based assessment, delay in realization of benefits, poor internal/external validation, inability to be used by laypersons, inability to be used in resource-poor settings, presence of ethical pitfalls, and presence of legal barriers. AI could potentially be explored in four other areas: surveillance, combination with big data, operation of other core clinical services, and management of patients with COVID-19.              Conclusions:                    In view of the continuing increase in the number of cases, and given that multiple waves of infections may occur, there is a need for effective methods to help control the COVID-19 pandemic. Despite its shortcomings, AI holds the potential to greatly augment existing human efforts, which may otherwise be overwhelmed by high patient numbers.",4.0
32946272,Artificial Intelligence and Deep Learning in Neuroradiology: Exploring the New Frontier,2021 Feb;72(1):35-44.,"There have been many recently published studies exploring machine learning (ML) and deep learning applications within neuroradiology. The improvement in performance of these techniques has resulted in an ever-increasing number of commercially available tools for the neuroradiologist. In this narrative review, recent publications exploring ML in neuroradiology are assessed with a focus on several key clinical domains. In particular, major advances are reviewed in the context of: (1) intracranial hemorrhage detection, (2) stroke imaging, (3) intracranial aneurysm screening, (4) multiple sclerosis imaging, (5) neuro-oncology, (6) head and tumor imaging, and (7) spine imaging.",1.0
32944070,Machine intelligence for nerve conduit design and production,2020 Sep 9;14:25.,"Nerve guidance conduits (NGCs) have emerged from recent advances within tissue engineering as a promising alternative to autografts for peripheral nerve repair. NGCs are tubular structures with engineered biomaterials, which guide axonal regeneration from the injured proximal nerve to the distal stump. NGC design can synergistically combine multiple properties to enhance proliferation of stem and neuronal cells, improve nerve migration, attenuate inflammation and reduce scar tissue formation. The aim of most laboratories fabricating NGCs is the development of an automated process that incorporates patient-specific features and complex tissue blueprints (e.g. neurovascular conduit) that serve as the basis for more complicated muscular and skin grafts. One of the major limitations for tissue engineering is lack of guidance for generating tissue blueprints and the absence of streamlined manufacturing processes. With the rapid expansion of machine intelligence, high dimensional image analysis, and computational scaffold design, optimized tissue templates for 3D bioprinting (3DBP) are feasible. In this review, we examine the translational challenges to peripheral nerve regeneration and where machine intelligence can innovate bottlenecks in neural tissue engineering.",
32942236,Application of machine learning methods for the prediction of organic solid waste treatment and recycling processes: A review,2021 Jan;319:124114.,"Conventional treatment and recycling methods of organic solid waste contain inherent flaws, such as low efficiency, low accuracy, high cost, and potential environmental risks. In the past decade, machine learning has gradually attracted increasing attention in solving the complex problems of organic solid waste treatment. Although significant research has been carried out, there is a lack of a systematic review of the research findings in this field. This study sorts the research studies published between 2003 and 2020, summarizes the specific application fields, characteristics, and suitability of different machine learning models, and discusses the relevant application limitations and future prospects. It can be concluded that studies mostly focused on municipal solid waste management, followed by anaerobic digestion, thermal treatment, composting, and landfill. The most widely used model is the artificial neural network, which has been successfully applied to various complicated non-linear organic solid waste related problems.",
32941995,Computational methods and next-generation sequencing approaches to analyze epigenetics data: Profiling of methods and applications,2021 Mar;187:92-103.,"Epigenetics is mainly comprised of features that regulate genomic interactions thereby playing a crucial role in a vast array of biological processes. Epigenetic mechanisms such as DNA methylation and histone modifications influence gene expression by modulating the packaging of DNA in the nucleus. A plethora of studies have emphasized the importance of analyzing epigenetics data through genome-wide studies and high-throughput approaches, thereby providing key insights towards epigenetics-based diseases such as cancer. Recent advancements have been made towards translating epigenetics research into a high throughput approach such as genome-scale profiling. Amongst all, bioinformatics plays a pivotal role in achieving epigenetics-related computational studies. Despite significant advancements towards epigenomic profiling, it is challenging to understand how various epigenetic modifications such as chromatin modifications and DNA methylation regulate gene expression. Next-generation sequencing (NGS) provides accurate and parallel sequencing thereby allowing researchers to comprehend epigenomic profiling. In this review, we summarize different computational methods such as machine learning and other bioinformatics tools, publicly available databases and resources to identify key modifications associated with epigenetic machinery. Additionally, the review also focuses on understanding recent methodologies related to epigenome profiling using NGS methods ranging from library preparation, different sequencing platforms and analytical techniques to evaluate various epigenetic modifications such as DNA methylation and histone modifications. We also provide detailed information on bioinformatics tools and computational strategies responsible for analyzing large scale data in epigenetics.",2.0
32941248,Novel classifications for systemic sclerosis: challenging historical subsets to unlock new doors,2020 Nov;32(6):463-471.,"Purpose of review:                    Systemic sclerosis (SSc) is a severe rheumatic disease characterized by a considerable heterogeneity in clinical presentations and pathophysiological mechanisms. This variability has a substantial impact on morbidity and mortality and limits the generalizability of clinical trial results. This review aims to highlight recent studies that have proposed new innovative approaches to decipher this heterogeneity, in particular, by attempting to optimize disease classification.              Recent findings:                    The historical dichotomy limited/diffuse subsets based on cutaneous involvement has been challenged by studies highlighting an underestimated heterogeneity between these two subtypes and showing that presence of organ damage and autoantibody profiles markedly influenced survival beyond skin extension. Advanced computational methods using unsupervised machine learning analyses of clinical variables and/or high-throughput omics technologies, clinical variables trajectories modelling overtime or radiomics have provided significant insights on key pathogenic processes that could help defining new subgroups beyond the diffuse/limited subsets.              Summary:                    We can anticipate that a future classification of SSc patients will integrate innovative approaches encompassing clinical phenotypes, variables trajectories, serological features and innovative omics molecular signatures. It nevertheless seems crucial to also pursue the implementation and standardization of readily available and easy to use tools that can be used in clinical practice.",1.0
32937683,Understanding mass spectrometry images: complexity to clarity with machine learning,2021 Apr;112(4):e23400.,"The application of artificial intelligence and machine learning to hyperspectral mass spectrometry imaging (MSI) data has received considerable attention over recent years. Various methodologies have shown great promise in their ability to handle the complexity and size of MSI data sets. Advances in this area have been particularly appealing for MSI of biological samples, which typically produce highly complicated data with often subtle relationships between features. There are many different machine learning approaches that have been applied to MSI data over the past two decades. In this review, we focus on a subset of non-linear machine learning techniques that have mostly only been applied in the past 5 years. Specifically, we review the use of the self-organizing map (SOM), SOM with relational perspective mapping (SOM-RPM), t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP). While not their only functionality, we have grouped these techniques based on their ability to produce what we refer to as similarity maps. Similarity maps are color representations of hyperspectral data, in which spectral similarity between pixels-that is, their distance in high-dimensional space-is represented by relative color similarity. In discussing these techniques, we describe, briefly, their associated algorithms and functionalities, and also outline applications in MSI research with a strong focus on biological sample types. The aim of this review is therefore to introduce this relatively recent paradigm for visualizing and exploring hyperspectral MSI, while also providing a comparison between each technique discussed.",
32936770,Identification of Risk Factors and Symptoms of COVID-19: Analysis of Biomedical Literature and Social Media Data,2020 Oct 2;22(10):e20509.,"Background:                    In December 2019, the COVID-19 outbreak started in China and rapidly spread around the world. Lack of a vaccine or optimized intervention raised the importance of characterizing risk factors and symptoms for the early identification and successful treatment of patients with COVID-19.              Objective:                    This study aims to investigate and analyze biomedical literature and public social media data to understand the association of risk factors and symptoms with the various outcomes observed in patients with COVID-19.              Methods:                    Through semantic analysis, we collected 45 retrospective cohort studies, which evaluated 303 clinical and demographic variables across 13 different outcomes of patients with COVID-19, and 84,140 Twitter posts from 1036 COVID-19-positive users. Machine learning tools to extract biomedical information were introduced to identify mentions of uncommon or novel symptoms in tweets. We then examined and compared two data sets to expand our landscape of risk factors and symptoms related to COVID-19.              Results:                    From the biomedical literature, approximately 90% of clinical and demographic variables showed inconsistent associations with COVID-19 outcomes. Consensus analysis identified 72 risk factors that were specifically associated with individual outcomes. From the social media data, 51 symptoms were characterized and analyzed. By comparing social media data with biomedical literature, we identified 25 novel symptoms that were specifically mentioned in tweets but have been not previously well characterized. Furthermore, there were certain combinations of symptoms that were frequently mentioned together in social media.              Conclusions:                    Identified outcome-specific risk factors, symptoms, and combinations of symptoms may serve as surrogate indicators to identify patients with COVID-19 and predict their clinical outcomes in order to provide appropriate treatments.",3.0
32936673,The RNS System: brain-responsive neurostimulation for the treatment of epilepsy,2021 Feb;18(2):129-138.,"Introduction: Epilepsy affects more than 1% of the US population, and over 30% of adults with epilepsy do not respond to antiseizure medications without life-impacting medication-related side effects. Resection of the seizure focus is not an option for many patients because it would cause unacceptable neurological or cognitive harm. For these patients, neuromodulation has emerged as a nondestructive, effective, and safe alternative. The NeuroPace® RNS® System, the only brain-responsive neurostimulation device, records neural activity from leads placed at one or two seizure foci. When the neurostimulator detects epileptiform activity, as defined for each patient by his or her physician, brief pulses of electrical stimulation are delivered to normalize the activity.Areas covered: This review describes the RNS System, the results of multi-year clinical trials, and the research discoveries enabled by the chronic ambulatory brain data collected by the RNS System.Expert commentary: Brain-responsive neurostimulation could potentially be used to treat any episodic neurological disorder that's accompanied by a neurophysiological biomarker of severity. Combining advanced machine learning approaches with the chronic ambulatory brain data collected by the RNS System could eventually enable automatic fine-tuning of detection and stimulation for each patient, creating a general-purpose neurotechnological platform for precision medicine.",1.0
32936088,Artificial Intelligence for the Prediction of Helicobacter Pylori Infection in Endoscopic Images: Systematic Review and Meta-Analysis Of Diagnostic Test Accuracy,2020 Sep 16;22(9):e21983.,"Background:                    Helicobacter pylori plays a central role in the development of gastric cancer, and prediction of H pylori infection by visual inspection of the gastric mucosa is an important function of endoscopy. However, there are currently no established methods of optical diagnosis of H pylori infection using endoscopic images. Definitive diagnosis requires endoscopic biopsy. Artificial intelligence (AI) has been increasingly adopted in clinical practice, especially for image recognition and classification.              Objective:                    This study aimed to evaluate the diagnostic test accuracy of AI for the prediction of H pylori infection using endoscopic images.              Methods:                    Two independent evaluators searched core databases. The inclusion criteria included studies with endoscopic images of H pylori infection and with application of AI for the prediction of H pylori infection presenting diagnostic performance. Systematic review and diagnostic test accuracy meta-analysis were performed.              Results:                    Ultimately, 8 studies were identified. Pooled sensitivity, specificity, diagnostic odds ratio, and area under the curve of AI for the prediction of H pylori infection were 0.87 (95% CI 0.72-0.94), 0.86 (95% CI 0.77-0.92), 40 (95% CI 15-112), and 0.92 (95% CI 0.90-0.94), respectively, in the 1719 patients (385 patients with H pylori infection vs 1334 controls). Meta-regression showed methodological quality and included the number of patients in each study for the purpose of heterogeneity. There was no evidence of publication bias. The accuracy of the AI algorithm reached 82% for discrimination between noninfected images and posteradication images.              Conclusions:                    An AI algorithm is a reliable tool for endoscopic diagnosis of H pylori infection. The limitations of lacking external validation performance and being conducted only in Asia should be overcome.              Trial registration:                    PROSPERO CRD42020175957; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=175957.",1.0
32935376,Artificial intelligence in endoscopy: Present and future perspectives,2021 Jan;33(2):218-230.,"Artificial intelligence (AI) has been attracting considerable attention as an important scientific topic in the field of medicine. Deep-leaning (DL) technologies have been applied more dominantly than other traditional machine-learning methods. They have demonstrated excellent capability to retract visual features of objectives, even unnoticeable ones for humans, and analyze huge amounts of information within short periods. The amount of research applying DL-based models to real-time computer-aided diagnosis (CAD) systems has been increasing steadily in the GI endoscopy field. An array of published data has already demonstrated the advantages of DL-based CAD models in the detection and characterization of various neoplastic lesions, regardless of the level of the GI tract. Although the diagnostic performances and study designs vary widely, owing to a lack of academic standards to assess the capability of AI for GI endoscopic diagnosis fairly, the superiority of CAD models has been demonstrated for almost all applications studied so far. Most of the challenges associated with AI in the endoscopy field are general problems for AI models used in the real world outside of medical fields. Solutions have been explored seriously and some solutions have been tested in the endoscopy field. Given that AI has become the basic technology to make machines react to the environment, AI would be a major technological paradigm shift, for not only diagnosis but also treatment. In the near future, autonomous endoscopic diagnosis might no longer be just a dream, as we are witnessing with the advent of autonomously driven electric vehicles.",
32933862,Big Data Solutions for Controversies in Breast Cancer Treatment,2020 Aug 12;S1526-8209(20)30208-1.,"The digital world of data is expanding with an annual growth rate of 40%, and health care is among the fastest growing sector of the digital world with an annual growth rate of 48%. Rapid growth in technology has augmented data generation; for example, electronic health records produce huge amounts of patient-level data, whereas national registries capture information on numerous factors affecting health care delivery and patient outcomes. This big data can be utilized to improve health care outcomes. This review discusses relevant applications in breast cancer treatment.",
32931932,Deep learning of brain magnetic resonance images: A brief review,2020 Sep 12;S1046-2023(20)30202-4.,"Magnetic resonance imaging (MRI) is one of the most popular techniques in brain science and is important for understanding brain function and neuropsychiatric disorders. However, the processing and analysis of MRI is not a trivial task with lots of challenges. Recently, deep learning has shown superior performance over traditional machine learning approaches in image analysis. In this survey, we give a brief review of the recent popular deep learning approaches and their applications in brain MRI analysis. Furthermore, popular brain MRI databases and deep learning tools are also introduced. The strength and weaknesses of different approaches are addressed, and challenges as well as future directions are also discussed.",
32931022,"Clinical trial design: Past, present, and future in the context of big data and precision medicine",2020 Nov 15;126(22):4838-4846.,"Clinical trials are fundamental for advances in cancer treatment. The traditional framework of phase 1 to 3 trials is designed for incremental advances between regimens. However, our ability to understand and treat cancer has evolved with the increase in drugs targeting an expanding array of therapeutic targets, the development of progressively comprehensive data sets, and emerging computational analytics, all of which are reshaping our treatment strategies. A more robust linkage between drugs and underlying cancer biology is blurring historical lines that define trials on the basis of cancer type. The complexity of the molecular basis of cancer, coupled with manifold variations in clinical status, is driving the individually tailored use of combinations of precision targeted drugs. This approach is spawning a new era of clinical trial types. Although most care is delivered in a community setting, large centers support real-time multi-omic analytics and their integrated interpretation by using machine learning in the context of real-world data sets. Coupling the analytic capabilities of large centers to the tailored delivery of therapy in the community is forging a paradigm that is optimizing service for patients. Understanding the importance of these evolving trends across the health care spectrum will affect our treatment of cancer in the future and is the focus of this review.",1.0
32930431,Rational Design of Semiconductor-Based Chemiresistors and their Libraries for Next-Generation Artificial Olfaction,2020 Dec;32(51):e2002075.,"Artificial olfaction based on gas sensor arrays aims to substitute for, support, and surpass human olfaction. Like mammalian olfaction, a larger number of sensors and more signal processing are crucial for strengthening artificial olfaction. Due to rapid progress in computing capabilities and machine-learning algorithms, on-demand high-performance artificial olfaction that can eclipse human olfaction becomes inevitable once diverse and versatile gas sensing materials are provided. Here, rational strategies to design a myriad of different semiconductor-based chemiresistors and to grow gas sensing libraries enough to identify a wide range of odors and gases are reviewed, discussed, and suggested. Key approaches include the use of p-type oxide semiconductors, multinary perovskite and spinel oxides, carbon-based materials, metal chalcogenides, their heterostructures, as well as heterocomposites as distinctive sensing materials, the utilization of bilayer sensor design, the design of robust sensing materials, and the high-throughput screening of sensing materials. In addition, the state-of-the-art and key issues in the implementation of electronic noses are discussed. Finally, a perspective on chemiresistive sensing materials for next-generation artificial olfaction is provided.",2.0
32928475,Constructing bi-plots for random forest: Tutorial,2020 Sep 22;1131:146-155.,"Current technological developments have allowed for a significant increase and availability of data. Consequently, this has opened enormous opportunities for the machine learning and data science field, translating into the development of new algorithms in a wide range of applications in medical, biomedical, daily-life, and national security areas. Ensemble techniques are among the pillars of the machine learning field, and they can be defined as approaches in which multiple, complex, independent/uncorrelated, predictive models are subsequently combined by either averaging or voting to yield a higher model performance. Random forest (RF), a popular ensemble method, has been successfully applied in various domains due to its ability to build predictive models with high certainty and little necessity of model optimization. RF provides both a predictive model and an estimation of the variable importance. However, the estimation of the variable importance is based on thousands of trees, and therefore, it does not specify which variable is important for which sample group. The present study demonstrates an approach based on the pseudo-sample principle that allows for construction of bi-plots (i.e. spin plots) associated with RF models. The pseudo-sample principle for RF. is explained and demonstrated by using two simulated datasets, and three different types of real data, which include political sciences, food chemistry and the human microbiome data. The pseudo-sample bi-plots, associated with RF and its unsupervised version, allow for a versatile visualization of multivariate models, and the variable importance and the relation among them.",
32925312,Machine learning in the optimization of robotics in the operative field,2020 Nov;30(6):808-816.,"Purpose of review:                    The increasing use of robotics in urologic surgery facilitates collection of 'big data'. Machine learning enables computers to infer patterns from large datasets. This review aims to highlight recent findings and applications of machine learning in robotic-assisted urologic surgery.              Recent findings:                    Machine learning has been used in surgical performance assessment and skill training, surgical candidate selection, and autonomous surgery. Autonomous segmentation and classification of surgical data have been explored, which serves as the stepping-stone for providing real-time surgical assessment and ultimately, improve surgical safety and quality. Predictive machine learning models have been created to guide appropriate surgical candidate selection, whereas intraoperative machine learning algorithms have been designed to provide 3-D augmented reality and real-time surgical margin checks. Reinforcement-learning strategies have been utilized in autonomous robotic surgery, and the combination of expert demonstrations and trial-and-error learning by the robot itself is a promising approach towards autonomy.              Summary:                    Robot-assisted urologic surgery coupled with machine learning is a burgeoning area of study that demonstrates exciting potential. However, further validation and clinical trials are required to ensure the safety and efficacy of incorporating machine learning into surgical practice.",
32924178,"The future is now? Clinical and translational aspects of ""Omics"" technologies",2021 Feb;99(2):168-176.,"Big data has become a central part of medical research, as well as modern life generally. ""Omics"" technologies include genomics, proteomics, microbiomics and increasingly other omics. These have been driven by rapid advances in laboratory techniques and equipment. Crucially, improved information handling capabilities have allowed concepts such as artificial intelligence and machine learning to enter the research world. The COVID-19 pandemic has shown how quickly information can be generated and analyzed using such approaches, but also showed its limitations. This review will look at how ""omics"" has begun to be translated into clinical practice. While there appears almost limitless potential in using big data for ""precision"" or ""personalized"" medicine, the reality is that this remains largely aspirational. Oncology is the only field of medicine that is widely adopting such technologies, and even in this field uptake is irregular. There are practical and ethical reasons for this lack of translation of increasingly affordable techniques into the clinic. Undoubtedly, there will be increasing use of large data sets from traditional (e.g. tumor samples, patient genomics) and nontraditional (e.g. smartphone) sources. It is perhaps the greatest challenge of the health-care sector over the coming decade to integrate these resources in an effective, practical and ethical way.",2.0
32922515,Using artificial intelligence for improving stroke diagnosis in emergency departments: a practical framework,2020 Aug 25;13:1756286420938962.,"Stroke is the fifth leading cause of death in the United States and a major cause of severe disability worldwide. Yet, recognizing the signs of stroke in an acute setting is still challenging and leads to loss of opportunity to intervene, given the narrow therapeutic window. A decision support system using artificial intelligence (AI) and clinical data from electronic health records combined with patients' presenting symptoms can be designed to support emergency department providers in stroke diagnosis and subsequently reduce the treatment delay. In this article, we present a practical framework to develop a decision support system using AI by reflecting on the various stages, which could eventually improve patient care and outcome. We also discuss the technical, operational, and ethical challenges of the process.",1.0
32921526,"Past, present and future EEG in the clinical workup of dementias",2020 Dec 30;306:111182.,"Electroencephalography (EEG), as non-invasive, global measure of neuronal activity, is a prime candidate functional marker of synapse dysfunction and loss in dementias. Nevertheless, EEG currently has no established role in the clinical workup of individual patients. This opinion paper presents our critical view on why EEG has so far failed to keep its promise, and where we believe EEG will be clinically useful for patients threatened with cognitive decline in the future. Individual EEGs are an integral outcome of many causally intermixing upstream factors contributing to dementia. Therefore, EEG cannot become a clinically useful ""simple"" stand-alone biomarker of some pathognomic accumulations of specific brain proteins, but rather offer unique opportunities for more comprehensive and richly faceted insights into the functional status of brain systems. EEG may thus remain an essential window into the brain when it comes to the at-risk and presymptomatic phases of dementias, where it can be uniquely informative about concepts such as burdens of plasticity and repair, cognitive reserve, and sleep. Jointly with rapid gains in usability, portability, machine learning, closed loop systems, and understanding of the role of EEG-based sleep stages for memory and brain repair, EEG may come to keep its initial promise after all.",
32921304,Review of medical image recognition technologies to detect melanomas using neural networks,2020 Sep 14;21(Suppl 11):270.,"Background:                    Melanoma is one of the most aggressive types of cancer that has become a world-class problem. According to the World Health Organization estimates, 132,000 cases of the disease and 66,000 deaths from malignant melanoma and other forms of skin cancer are reported annually worldwide ( https://apps.who.int/gho/data/?theme=main ) and those numbers continue to grow. In our opinion, due to the increasing incidence of the disease, it is necessary to find new, easy to use and sensitive methods for the early diagnosis of melanoma in a large number of people around the world. Over the last decade, neural networks show highly sensitive, specific, and accurate results.              Objective:                    This study presents a review of PubMed papers including requests «melanoma neural network» and «melanoma neural network dermatoscopy». We review recent researches and discuss their opportunities acceptable in clinical practice.              Methods:                    We searched the PubMed database for systematic reviews and original research papers on the requests «melanoma neural network» and «melanoma neural network dermatoscopy» published in English. Only papers that reported results, progress and outcomes are included in this review.              Results:                    We found 11 papers that match our requests that observed convolutional and deep-learning neural networks combined with fuzzy clustering or World Cup Optimization algorithms in analyzing dermatoscopic images. All of them require an ABCD (asymmetry, border, color, and differential structures) algorithm and its derivates (in combination with ABCD algorithm or separately). Also, they require a large dataset of dermatoscopic images and optimized estimation parameters to provide high specificity, accuracy and sensitivity.              Conclusions:                    According to the analyzed papers, neural networks show higher specificity, accuracy and sensitivity than dermatologists. Neural networks are able to evaluate features that might be unavailable to the naked human eye. Despite that, we need more datasets to confirm those statements. Nowadays machine learning becomes a helpful tool in early diagnosing skin diseases, especially melanoma.",2.0
32920050,MPTherm-Pred: Analysis and Prediction of Thermal Stability Changes upon Mutations in Transmembrane Proteins,2020 Sep 10;S0022-2836(20)30538-6.,"The stability of membrane proteins differs from globular proteins due to the presence of nonpolar membrane-spanning regions. Using a dataset of 929 membrane protein mutations whose effects on thermal stability (ΔTm) were experimentally determined, we found that the average ΔTm due to 190 stabilizing and 232 destabilizing mutations occurring in membrane-spanning regions are 2.43 ± 3.1 and - 5.48 ± 5.5 °C, respectively. The ΔTm values for mutations occurring in solvent-exposed regions are 2.56 ± 2.82 and - 6.8 ± 7.2 °C. We have systematically analyzed the factors influencing the stability of mutants and observed that changes in hydrophobicity, number of contacts between Cα atoms and frequency of aliphatic residues are important determinants of the stability change induced by mutations occurring in membrane-spanning regions. We have developed structure- and sequence-based machine learning predictors of ΔTm due to mutations specifically for membrane proteins. They showed a correlation and mean absolute error (MAE) of 0.72 and 2.85 °C, respectively, between experimental and predicted ΔTm for mutations in membrane-spanning regions on 10-fold group-wise cross-validation. The average correlation and MAE for mutations in aqueous regions are 0.73 and 3.7 °C, respectively. These MAE values are about 50% lower than standard deviations from the mean ΔTm values. The reliability of the method was affirmed on a test set of mutations occurring in evolutionary independent protein sequences. The developed MPTherm-pred server for predicting thermal stability changes upon mutations in membrane proteins is available at https://web.iitm.ac.in/bioinfo2/mpthermpred/. Our results provide insights into factors influencing the stability of membrane proteins and can aid in designing mutants that are more resistant to thermal stress.",1.0
32920005,Overview of artificial intelligence-based applications in radiotherapy: Recommendations for implementation and quality assurance,2020 Dec;153:55-66.,"Artificial Intelligence (AI) is currently being introduced into different domains, including medicine. Specifically in radiation oncology, machine learning models allow automation and optimization of the workflow. A lack of knowledge and interpretation of these AI models can hold back wide-spread and full deployment into clinical practice. To facilitate the integration of AI models in the radiotherapy workflow, generally applicable recommendations on implementation and quality assurance (QA) of AI models are presented. For commonly used applications in radiotherapy such as auto-segmentation, automated treatment planning and synthetic computed tomography (sCT) the basic concepts are discussed in depth. Emphasis is put on the commissioning, implementation and case-specific and routine QA of AI models needed for a methodical introduction in clinical practice.",3.0
32918159,Comparison of performances of conventional and deep learning-based methods in segmentation of lung vessels and registration of chest radiographs,2021 Mar;14(1):6-15.,"Conventional machine learning-based methods have been effective in assisting physicians in making accurate decisions and utilized in computer-aided diagnosis for more than 30 years. Recently, deep learning-based methods, and convolutional neural networks in particular, have rapidly become preferred options in medical image analysis because of their state-of-the-art performance. However, the performances of conventional and deep learning-based methods cannot be compared reliably because of their evaluations on different datasets. Hence, we developed both conventional and deep learning-based methods for lung vessel segmentation and chest radiograph registration, and subsequently compared their performances on the same datasets. The results strongly indicated the superiority of deep learning-based methods over their conventional counterparts.",
32917886,Designing and understanding light-harvesting devices with machine learning,2020 Sep 11;11(1):4587.,"Understanding the fundamental processes of light-harvesting is crucial to the development of clean energy materials and devices. Biological organisms have evolved complex metabolic mechanisms to efficiently convert sunlight into chemical energy. Unraveling the secrets of this conversion has inspired the design of clean energy technologies, including solar cells and photocatalytic water splitting. Describing the emergence of macroscopic properties from microscopic processes poses the challenge to bridge length and time scales of several orders of magnitude. Machine learning experiences increased popularity as a tool to bridge the gap between multi-level theoretical models and Edisonian trial-and-error approaches. Machine learning offers opportunities to gain detailed scientific insights into the underlying principles governing light-harvesting phenomena and can accelerate the fabrication of light-harvesting devices.",1.0
32917408,Circuit-Based Biomarkers for Mood and Anxiety Disorders,2020 Nov;43(11):902-915.,"Mood and anxiety disorders are complex heterogeneous syndromes that manifest in dysfunctions across multiple brain regions, cell types, and circuits. Biomarkers using brain-wide activity patterns in humans have proven useful in distinguishing between disorder subtypes and identifying effective treatments. In order to improve biomarker identification, it is crucial to understand the basic circuitry underpinning brain-wide activity patterns. Leveraging a large repertoire of techniques, animal studies have examined roles of specific cell types and circuits in driving maladaptive behavior. Recent advances in multiregion recording techniques, data-driven analysis approaches, and machine-learning-based behavioral analysis tools can further push the boundary of animal studies and bridge the gap with human studies, to assess how brain-wide activity patterns encode and drive emotional behavior. Together, these efforts will allow identifying more precise biomarkers to enhance diagnosis and treatment.",1.0
32916943,Research Progress of Automated Visual Surface Defect Detection for Industrial Metal Planar Materials,2020 Sep 9;20(18):5136.,"The computer-vision-based surface defect detection of metal planar materials is a research hotspot in the field of metallurgical industry. The high standard of planar surface quality in the metal manufacturing industry requires that the performance of an automated visual inspection system and its algorithms are constantly improved. This paper attempts to present a comprehensive survey on both two-dimensional and three-dimensional surface defect detection technologies based on reviewing over 160 publications for some typical metal planar material products of steel, aluminum, copper plates and strips. According to the algorithm properties as well as the image features, the existing two-dimensional methodologies are categorized into four groups: statistical, spectral, model, and machine learning-based methods. On the basis of three-dimensional data acquisition, the three-dimensional technologies are divided into stereoscopic vision, photometric stereo, laser scanner, and structured light measurement methods. These classical algorithms and emerging methods are introduced, analyzed, and compared in this review. Finally, the remaining challenges and future research trends of visual defect detection are discussed and forecasted at an abstract level.",
32916652,"To operate, or not to operate? Narrative review of the role of survival predictors in patient selection for operative management of patients with metastatic spine disease",2020 Sep 11;1-15.,"Accurate prediction of patient survival is an essential component of the preoperative evaluation of patients with spinal metastases. Over the past quarter of a century, a number of predictors have been developed, although none have been accurate enough to be instituted as a staple of clinical practice. However, recently more comprehensive survival calculators have been published that make use of larger data sets and machine learning to predict postoperative survival among patients with spine metastases. Given the glut of calculators that have been published, the authors sought to perform a narrative review of the current literature, highlighting existing calculators along with the strengths and weaknesses of each. In doing so, they identify two ""generations"" of scoring systems-a first generation based on a priori factor weighting and a second generation comprising predictive tools that are developed using advanced statistical modeling and are focused on clinical deployment. In spite of recent advances, the authors found that most predictors have only a moderate ability to explain variation in patient survival. Second-generation models have a greater prognostic accuracy relative to first-generation scoring systems, but most still require external validation. Given this, it seems that there are two outstanding goals for these survival predictors, foremost being external validation of current calculators in multicenter prospective cohorts, as the majority have been developed from, and internally validated within, the same single-institution data sets. Lastly, current predictors should be modified to incorporate advances in targeted systemic therapy and radiotherapy, which have been heretofore largely ignored.",
32916179,Prediction modeling-part 2: using machine learning strategies to improve transplantation outcomes,2021 Apr;99(4):817-823.,"Kidney transplant recipients and transplant physicians face important clinical questions where machine learning methods may help improve the decision-making process. This mini-review explores potential applications of machine learning methods to key stages of a kidney transplant recipient's journey, from initial waitlisting and donor selection, to personalization of immunosuppression and prediction of post-transplantation events. Both unsupervised and supervised machine learning methods are presented, including k-means clustering, principal components analysis, k-nearest neighbors, and random forests. The various challenges of these approaches are also discussed.",
32915503,Advances in optical gastrointestinal endoscopy: a technical review,2020 Sep 11.,"Optical endoscopy is the primary diagnostic and therapeutic tool for management of gastrointestinal (GI) malignancies. Most GI neoplasms arise from precancerous lesions; thus, technical innovations to improve detection and diagnosis of precancerous lesions and early cancers play a pivotal role in improving outcomes. Over the last few decades, the field of GI endoscopy has witnessed enormous and focused efforts to develop and translate accurate, user-friendly, and minimally invasive optical imaging modalities. From a technical point of view, a wide range of novel optical techniques is now available to probe different aspects of light-tissue interaction at macroscopic and microscopic scales, complementing white light endoscopy. Most of these new modalities have been successfully validated and translated to routine clinical practice. Herein, we provide a technical review of the current status of existing and promising new optical endoscopic imaging technologies for GI cancer screening and surveillance. We summarize the underlying principles of light-tissue interaction, the imaging performance at different scales, and highlight what is known about clinical applicability and effectiveness. Furthermore, we discuss recent discovery and translation of novel molecular probes that have shown promise to augment endoscopists' ability to diagnose GI lesions with high specificity. We also review and discuss the role and potential clinical integration of artificial intelligence-based algorithms to provide decision support in real time. Finally, we provide perspectives on future technology development and its potential to transform endoscopic GI cancer detection and diagnosis.",
32915223,Artificial Intelligence and global health: opportunities and challenges,2019 Nov 27;3(6):741-746.,"Artificial Intelligence (AI) offers unprecedented opportunities and challenges for humanity. If AI can be positioned and leveraged correctly, it can rapidly accelerate progress on achieving the United Nations' Sustainable Development Goals (SDGs), including SDG #3: 'Ensure healthy lives and promote wellbeing for all at all ages'. Achieving this goal could have a transformative impact on global health. An ethical, transparent and responsible approach to AI development will result in AI translating data into contextually relevant knowledge, conclusions, and impactful actions.",
32912543,Machine Learning for Brain Stroke: A Review,2020 Oct;29(10):105162.,"Machine Learning (ML) delivers an accurate and quick prediction outcome and it has become a powerful tool in health settings, offering personalized clinical care for stroke patients. An application of ML and Deep Learning in health care is growing however, some research areas do not catch enough attention for scientific investigation though there is real need of research. Therefore, the aim of this work is to classify state-of-arts on ML techniques for brain stroke into 4 categories based on their functionalities or similarity, and then review studies of each category systematically. A total of 39 studies were identified from the results of ScienceDirect web scientific database on ML for brain stroke from the year 2007 to 2019. Support Vector Machine (SVM) is obtained as optimal models in 10 studies for stroke problems. Besides, maximum studies are found in stroke diagnosis although number for stroke treatment is least thus, it identifies a research gap for further investigation. Similarly, CT images are a frequently used dataset in stroke. Finally SVM and Random Forests are efficient techniques used under each category. The present study showcases the contribution of various ML approaches applied to brain stroke.",1.0
32912474,Proposed Requirements for Cardiovascular Imaging-Related Machine Learning Evaluation (PRIME): A Checklist: Reviewed by the American College of Cardiology Healthcare Innovation Council,2020 Sep;13(9):2017-2035.,"Machine learning (ML) has been increasingly used within cardiology, particularly in the domain of cardiovascular imaging. Due to the inherent complexity and flexibility of ML algorithms, inconsistencies in the model performance and interpretation may occur. Several review articles have been recently published that introduce the fundamental principles and clinical application of ML for cardiologists. This paper builds on these introductory principles and outlines a more comprehensive list of crucial responsibilities that need to be completed when developing ML models. This paper aims to serve as a scientific foundation to aid investigators, data scientists, authors, editors, and reviewers involved in machine learning research with the intent of uniform reporting of ML investigations. An independent multidisciplinary panel of ML experts, clinicians, and statisticians worked together to review the theoretical rationale underlying 7 sets of requirements that may reduce algorithmic errors and biases. Finally, the paper summarizes a list of reporting items as an itemized checklist that highlights steps for ensuring correct application of ML models and the consistent reporting of model specifications and results. It is expected that the rapid pace of research and development and the increased availability of real-world evidence may require periodic updates to the checklist.",4.0
32911665,"Comparison of Conventional Statistical Methods with Machine Learning in Medicine: Diagnosis, Drug Development, and Treatment",2020 Sep 8;56(9):455.,"Futurists have anticipated that novel autonomous technologies, embedded with machine learning (ML), will substantially influence healthcare. ML is focused on making predictions as accurate as possible, while traditional statistical models are aimed at inferring relationships between variables. The benefits of ML comprise flexibility and scalability compared with conventional statistical approaches, which makes it deployable for several tasks, such as diagnosis and classification, and survival predictions. However, much of ML-based analysis remains scattered, lacking a cohesive structure. There is a need to evaluate and compare the performance of well-developed conventional statistical methods and ML on patient outcomes, such as survival, response to treatment, and patient-reported outcomes (PROs). In this article, we compare the usefulness and limitations of traditional statistical methods and ML, when applied to the medical field. Traditional statistical methods seem to be more useful when the number of cases largely exceeds the number of variables under study and a priori knowledge on the topic under study is substantial such as in public health. ML could be more suited in highly innovative fields with a huge bulk of data, such as omics, radiodiagnostics, drug development, and personalized treatment. Integration of the two approaches should be preferred over a unidirectional choice of either approach.",1.0
32908491,Machine Learning in Neuroimaging: A New Approach to Understand Acupuncture for Neuroplasticity,2020 Aug 24;2020:8871712.,"The effects of acupuncture facilitating neural plasticity for treating diseases have been identified by clinical and experimental studies. In the last two decades, the application of neuroimaging techniques in acupuncture research provided visualized evidence for acupuncture promoting neuroplasticity. Recently, the integration of machine learning (ML) and neuroimaging techniques becomes a focus in neuroscience and brings a new and promising approach to understand the facilitation of acupuncture on neuroplasticity at the individual level. This review is aimed at providing an overview of this rapidly growing field by introducing the commonly used ML algorithms in neuroimaging studies briefly and analyzing the characteristics of the acupuncture studies based on ML and neuroimaging, so as to provide references for future research.",
32908264,Illuminating the dark spaces of healthcare with ambient intelligence,2020 Sep;585(7824):193-202.,"Advances in machine learning and contactless sensors have given rise to ambient intelligence-physical spaces that are sensitive and responsive to the presence of humans. Here we review how this technology could improve our understanding of the metaphorically dark, unobserved spaces of healthcare. In hospital spaces, early applications could soon enable more efficient clinical workflows and improved patient safety in intensive care units and operating rooms. In daily living spaces, ambient intelligence could prolong the independence of older individuals and improve the management of individuals with a chronic disease by understanding everyday behaviour. Similar to other technologies, transformation into clinical applications at scale must overcome challenges such as rigorous clinical validation, appropriate data privacy and model transparency. Thoughtful use of this technology would enable us to understand the complex interplay between the physical environment and health-critical human behaviours.",4.0
32906819,3D Deep Learning on Medical Images: A Review,2020 Sep 7;20(18):5097.,"The rapid advancements in machine learning, graphics processing technologies and the availability of medical imaging data have led to a rapid increase in the use of deep learning models in the medical domain. This was exacerbated by the rapid advancements in convolutional neural network (CNN) based architectures, which were adopted by the medical imaging community to assist clinicians in disease diagnosis. Since the grand success of AlexNet in 2012, CNNs have been increasingly used in medical image analysis to improve the efficiency of human clinicians. In recent years, three-dimensional (3D) CNNs have been employed for the analysis of medical images. In this paper, we trace the history of how the 3D CNN was developed from its machine learning roots, we provide a brief mathematical description of 3D CNN and provide the preprocessing steps required for medical images before feeding them to 3D CNNs. We review the significant research in the field of 3D medical imaging analysis using 3D CNNs (and its variants) in different medical areas such as classification, segmentation, detection and localization. We conclude by discussing the challenges associated with the use of 3D CNNs in the medical imaging domain (and the use of deep learning models in general) and possible future trends in the field.",4.0
32906056,A review of Cloud computing technologies for comprehensive microRNA analyses,2020 Oct;88:107365.,"Cloud computing revolutionized many fields that require ample computational power. Cloud platforms may also provide huge support for microRNA analysis mainly through disclosing scalable resources of different types. In Clouds, these resources are available as services, which simplifies their allocation and releasing. This feature is especially useful during the analysis of large volumes of data, like the one produced by next generation sequencing experiments, which require not only extended storage space but also a distributed computing environment. In this paper, we show which of the Cloud properties and service models can be especially beneficial for microRNA analysis. We also explain the most useful services of the Cloud (including storage space, computational power, web application hosting, machine learning models, and Big Data frameworks) that can be used for microRNA analysis. At the same time, we review several solutions for microRNA and show that the utilization of the Cloud in this field is still weak, but can increase in the future when the awareness of their applicability grows.",2.0
32905873,"Clinical applications of machine learning in the diagnosis, classification, and prediction of heart failure",2020 Nov;229:1-17.,"Machine learning and artificial intelligence are generating significant attention in the scientific community and media. Such algorithms have great potential in medicine for personalizing and improving patient care, including in the diagnosis and management of heart failure. Many physicians are familiar with these terms and the excitement surrounding them, but many are unfamiliar with the basics of these algorithms and how they are applied to medicine. Within heart failure research, current applications of machine learning include creating new approaches to diagnosis, classifying patients into novel phenotypic groups, and improving prediction capabilities. In this paper, we provide an overview of machine learning targeted for the practicing clinician and evaluate current applications of machine learning in the diagnosis, classification, and prediction of heart failure.",1.0
32905155,Artificial intelligence and deep learning in ophthalmology - present and future (Review),2020 Oct;20(4):3469-3473.,"Since its introduction in 1959, artificial intelligence technology has evolved rapidly and helped benefit research, industries and medicine. Deep learning, as a process of artificial intelligence (AI) is used in ophthalmology for data analysis, segmentation, automated diagnosis and possible outcome predictions. The association of deep learning and optical coherence tomography (OCT) technologies has proven reliable for the detection of retinal diseases and improving the diagnostic performance of the eye's posterior segment diseases. This review explored the possibility of implementing and using AI in establishing the diagnosis of retinal disorders. The benefits and limitations of AI in the field of retinal disease medical management were investigated by analyzing the most recent literature data. Furthermore, the future trends of AI involvement in ophthalmology were analyzed, as AI will be part of the decision-making regarding the scientific investigation, diagnosis and therapeutic management.",1.0
32904333,Conceptualising Artificial Intelligence as a Digital Healthcare Innovation: An Introductory Review,2020 Aug 20;13:223-230.,"Artificial intelligence (AI) is widely recognised as a transformative innovation and is already proving capable of outperforming human clinicians in the diagnosis of specific medical conditions, especially in image analysis within dermatology and radiology. These abilities are enhanced by the capacity of AI systems to learn from patient records, genomic information and real-time patient data. Uses of AI range from integrating with robotics to creating training material for clinicians. Whilst AI research is mounting, less attention has been paid to the practical implications on healthcare services and potential barriers to implementation. AI is recognised as a ""Software as a Medical Device (SaMD)"" and is increasingly becoming a topic of interest for regulators. Unless the introduction of AI is carefully considered and gradual, there are risks of automation bias, overdependence and long-term staffing problems. This is in addition to already well-documented generic risks associated with AI, such as data privacy, algorithmic biases and corrigibility. AI is able to potentiate innovations which preceded it, using Internet of Things, digitisation of patient records and genetic data as data sources. These synergies are important in both realising the potential of AI and utilising the potential of the data. As machine learning systems begin to cross-examine an array of databases, we must ensure that clinicians retain autonomy over the diagnostic process and understand the algorithmic processes generating diagnoses. This review uses established management literature to explore artificial intelligence as a digital healthcare innovation and highlight potential risks and opportunities.",1.0
32903956,Computer-Aided Diagnosis Systems in Diagnosing Malignant Thyroid Nodules on Ultrasonography: A Systematic Review and Meta-Analysis,2020 Jul;9(4):186-193.,"Background:                    Computer-aided diagnosis (CAD) systems are being applied to the ultrasonographic diagnosis of malignant thyroid nodules, but it remains controversial whether the systems add any accuracy for radiologists.              Objective:                    To determine the accuracy of CAD systems in diagnosing malignant thyroid nodules.              Methods:                    PubMed, EMBASE, and the Cochrane Library were searched for studies on the diagnostic performance of CAD systems. The diagnostic performance was assessed by pooled sensitivity and specificity, and their accuracy was compared with that of radiologists. The present systematic review was registered in PROSPERO (CRD42019134460).              Results:                    Nineteen studies with 4,781 thyroid nodules were included. Both the classic machine learning- and the deep learning-based CAD system had good performance in diagnosing malignant thyroid nodules (classic machine learning: sensitivity 0.86 [95% CI 0.79-0.92], specificity 0.85 [95% CI 0.77-0.91], diagnostic odds ratio (DOR) 37.41 [95% CI 24.91-56.20]; deep learning: sensitivity 0.89 [95% CI 0.81-0.93], specificity 0.84 [95% CI 0.75-0.90], DOR 40.87 [95% CI 18.13-92.13]). The diagnostic performance of the deep learning-based CAD system was comparable to that of the radiologists (sensitivity 0.87 [95% CI 0.78-0.93] vs. 0.87 [95% CI 0.85-0.89], specificity 0.85 [95% CI 0.76-0.91] vs. 0.87 [95% CI 0.81-0.91], DOR 40.12 [95% CI 15.58-103.33] vs. DOR 44.88 [95% CI 30.71-65.57]).              Conclusions:                    The CAD systems demonstrated good performance in diagnosing malignant thyroid nodules. However, experienced radiologists may still have an advantage over CAD systems during real-time diagnosis.",2.0
32903110,Using Bayesian networks to clarify interpretation of exposure-response regression coefficients: blood lead-mortality association as an example,2020 Aug;50(7):539-550.,"We examine how Bayesian network (BN) learning and analysis methods can help to meet several methodological challenges that arise in interpreting significant regression coefficients in exposure-response regression modeling. As a motivating example, we consider the challenge of interpreting positive regression coefficients for blood lead level (BLL) as a predictor of mortality risk for nonsmoking men. We first note that practices such as dichotomizing or categorizing continuous confounders (e.g. income), omitting potentially important socioeconomic confounders (e.g. education), and assuming specific parametric regression model forms leave unclear to what extent a positive regression coefficient reflects these modeling choices, rather than a direct dependence of mortality risk on exposure. Therefore, significant exposure-response coefficients in parametric regression models do not necessarily reveal the extent to which reducing exposure-related variables (e.g. BLL) alone, while leaving fixed other correlates of exposure and mortality risks (e.g. education, income, etc.) would reduce adverse outcome risks (e.g. mortality risks). We then consider how BN structure-learning and inference algorithms and nonparametric estimation methods (partial dependence plots) can be used to clarify dependencies between variables, variable selection, confounding, and quantification of joint effects of multiple factors on risk, including possible high-order interactions and nonlinearities. We conclude that these details must be carefully modeled to determine whether a data set provides evidence that exposure itself directly affects risks; and that BN and nonparametric effect estimation and uncertainty quantification methods can complement regression modeling and help to improve the scientific basis for risk management decisions and policy-making by addressing these issues.",
32896771,Increasing metabolic pathway flux by using machine learning models,2020 Dec;66:179-185.,"Machine learning is transforming many industries through self-improving models that are fueled by big data and high computing power. The field of metabolic engineering, which uses cellular biochemical network to manufacture useful small molecules, has also witnessed the first wave of machine learning applications in the past five years, covering reaction route design, enzyme selection, pathway engineering and process optimization. This review focuses on pathway engineering, and uses a few recent studies to illustrate (1) how machine learning models can be useful in overcoming an evident rate-limiting step, and (2) how the models may be used to exhaustively search - or guide optimization algorithms to search - a large design space when the cellular regulation of the reaction network is more convoluted.",
32891632,Sequence-enabled community-based microbial source tracking in surface waters using machine learning classification: A review,2020 Oct;177:106050.,"The development of Microbial Source Tracking (MST) technologies was borne out of necessity. This was largely due to the: 1) inadequacies of the fecal indicator bacterial paradigm, 2) fact that many fecal bacteria can survive and often grow in the environment, 3) inability of traditional microbiological methods to attribute source, 4) lack of correspondence between numbers of fecal indicator bacteria in waterways and many human pathogens, and 5) source allocation requirements and load determinations needed for total maximum daily loads. The MST tools have changed over time, evolving from culture-dependent to culture-independent molecular analyses. More recently, MST tools based on microbial community analyses, mainly DNA sequencing-based approaches, have been developed in an attempt to overcome some of these issues. These approaches generate large data sets and require the use of sophisticated machine learning algorithms to allocate potential host sources to contaminated waterways. In this review we discuss the origins and needs for community-based MST methods, as well as elaborate on the Bayesian algorithm-based program SourceTracker, which is increasingly being used for the determination of sources of fecal contamination of waterways.",1.0
32888635,Predicting High-Risk Patients and High-Risk Outcomes in Heart Failure,2020 Oct;16(4):387-407.,"Identifying patients with heart failure at high risk for poor outcomes is important for patient care, resource allocation, and process improvement. Although numerous risk models exist to predict mortality, hospitalization, and patient-reported health status, they are infrequently used for several reasons, including modest performance, lack of evidence to support routine clinical use, and barriers to implementation. Artificial intelligence has the potential to enhance the performance of risk prediction models, but has its own limitations and remains unproved.",1.0
32888634,Identification of Patients with Heart Failure in Large Datasets,2020 Oct;16(4):379-386.,"Large registries, administrative data, and the electronic health record (EHR) offer opportunities to identify patients with heart failure, which can be used for research purposes, process improvement, and optimal care delivery. Identification of cases is challenging because of the heterogeneous nature of the disease, which encompasses various phenotypes that may respond differently to treatment. The increasing availability of both structured and unstructured data in the EHR has expanded opportunities for cohort construction. This article reviews the current literature on approaches to identification of heart failure, and looks toward the future of machine learning, big data, and phenomapping.",
32888472,Molecular profiling of neuroendocrine tumours to predict response and toxicity to peptide receptor radionuclide therapy,2020 Sep;21(9):e431-e443.,"Peptide receptor radionuclide therapy (PRRT) is a type of radiotherapy that targets peptide receptors and is typically used for neuroendocrine tumours (NETs). Some of the key challenges in its use are the prediction of efficacy and toxicity, patient selection, and response optimisation. In this Review, we assess current knowledge on the molecular profile of NETs and the strategies and tools used to predict, monitor, and assess the toxicity of PRRT. The few mutations in tumour genes that can be evaluated (eg, ATM and DAXX) are limited to pancreatic NETs and are most likely not informative. Assays that are transcriptomic or based on genes are effective in the prediction of radiotherapy response in other cancers. A blood-based assay for eight genes (the PRRT prediction quotient [PPQ]) has an overall accuracy of 95% for predicting responses to PRRT in NETs. No molecular markers exist that can predict the toxicity of PRRT. Candidate molecular targets include seven single nucleotide polymorphisms (SNPs) that are susceptible to radiation. Transcriptomic evaluations of blood and a combination of gene expression and specific SNPs, assessed by machine learning with algorithms that are tumour-specific, might yield molecular tools to enhance the efficacy and safety of PRRT.",7.0
32884217,Current understanding of the metabolism of micronutrients in chronic alcoholic liver disease,2020 Aug 21;26(31):4567-4578.,"Alcoholic liver disease (ALD) remains an important health problem worldwide. Perturbation of micronutrients has been broadly reported to be a common characteristic in patients with ALD, given the fact that micronutrients often act as composition or coenzymes of many biochemical enzymes responsible for the inflammatory response, oxidative stress, and cell proliferation. Mapping the metabolic pattern and the function of these micronutrients is a prerequisite before targeted intervention can be delivered in clinical practice. Recent years have registered a significant improvement in our understanding of the role of micronutrients on the pathogenesis and progression of ALD. However, how and to what extent these micronutrients are involved in the pathophysiology of ALD remains largely unknown. In the current study, we provide a review of recent studies that investigated the imbalance of micronutrients in patients with ALD with a focus on zinc, iron, copper, magnesium, selenium, vitamin D and vitamin E, and determine how disturbances in micronutrients relates to the pathophysiology of ALD. Overall, zinc, selenium, vitamin D, and vitamin E uniformly exhibited a deficiency, and iron demonstrated an elevated trend. While for copper, both an elevation and deficiency were observed from existing literature. More importantly, we also highlight several challenges in terms of low sample size, study design discrepancies, sample heterogeneity across studies, and the use of machine learning approaches.",
32876837,A systematic review of radiomics in osteosarcoma: utilizing radiomics quality score as a tool promoting clinical translation,2021 Mar;31(3):1526-1535.,"Objectives:                    To assess the methodological quality and risk of bias in radiomics studies investigating diagnosis, therapy response, and survival of patients with osteosarcoma.              Methods:                    In this systematic review, literatures on radiomics in osteosarcoma were included and assessed for methodological quality through the radiomics quality score (RQS). The risk of bias and concern of application was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool. A meta-analysis of studies focusing on predicting osteosarcoma response to neoadjuvant chemotherapy was performed.              Results:                    Twelve radiomics studies exploring osteosarcoma were identified, and five were included in meta-analysis. The RQS reached an average of 20.4% (6.92 of 36) with good inter-rater agreement (ICC 0.95, 95% CI 0.85-0.99). Four studies validated results with an internal dataset, none of which used external dataset; one study was prospectively designed, and another one shared part of the dataset. The risk of bias and concern of application were mainly related to index test aspect. The meta-analysis showed a diagnostic odds ratio of 43.68 (95%CI 13.5-141.31) for predicting response to neoadjuvant chemotherapy with high heterogeneity and low methodological quality.              Conclusions:                    The overall scientific quality of included studies is insufficient; however, radiomics remains a promising technology for predicting treatment response, which might guide therapeutic decision-making and related to prognosis. Improvements in study design, validation, and open science needs to be made to demonstrate the generalizability of findings and to achieve clinical applications. Widespread application of RQS, pre-trained RQS scoring procedure, and modification of RQS in response to clinical needs are necessary.              Key points:                    • Limited radiomics studies were established in osteosarcoma with mean RQS of 20.4%, commonly due to unvalidated results, retrospective study design, and absence of open science. • Meta-analysis of radiomics studies predicting osteosarcoma response to neoadjuvant chemotherapy showed high diagnostic odds ratio 43.68, while high heterogeneity and low methodological quality were the main concerns. • A previously trained data extraction instrument allowed reaching moderate inter-rater agreement in RQS applications, while RQS still needs improvement to become a wide adaptive tool in reviews of radiomics studies, in routine self-check before manuscript submitting and in study design.",
32873989,Evaluation-oriented exploration of photo energy conversion systems: from fundamental optoelectronics and material screening to the combination with data science,2020 Aug 28;1-15.,"Light is a form of energy that can be converted to electric and chemical energies. Thus, organic photovoltaics (OPVs), perovskite solar cells (PSCs), photocatalysts, and photodetectors have evolved as scientific and commercial enterprises. However, the complex photochemical reactions and multicomponent materials involved in these systems have hampered rapid progress in their fundamental understanding and material design. This review showcases the evaluation-oriented exploration of photo energy conversion materials by using electrodeless time-resolved microwave conductivity (TRMC) and materials informatics (MI). TRMC with its unique options (excitation sources, environmental control, frequency modulation, etc.) provides not only accelerated experimental screening of OPV and PSC materials but also a versatile route toward shedding light on their charge carrier dynamics. Furthermore, MI powered by machine learning is shown to allow extremely high-throughput exploration in the large molecular space, which is compatible with experimental screening and combinatorial synthesis.",
32873936,Beyond dichotomies in reinforcement learning,2020 Oct;21(10):576-586.,"Reinforcement learning (RL) is a framework of particular importance to psychology, neuroscience and machine learning. Interactions between these fields, as promoted through the common hub of RL, has facilitated paradigm shifts that relate multiple levels of analysis in a singular framework (for example, relating dopamine function to a computationally defined RL signal). Recently, more sophisticated RL algorithms have been proposed to better account for human learning, and in particular its oft-documented reliance on two separable systems: a model-based (MB) system and a model-free (MF) system. However, along with many benefits, this dichotomous lens can distort questions, and may contribute to an unnecessarily narrow perspective on learning and decision-making. Here, we outline some of the consequences that come from overconfidently mapping algorithms, such as MB versus MF RL, with putative cognitive processes. We argue that the field is well positioned to move beyond simplistic dichotomies, and we propose a means of refocusing research questions towards the rich and complex components that comprise learning and decision-making.",
32873407,Cancer Nanomedicines in an Evolving Oncology Landscape,2020 Oct;41(10):730-742.,"Nanomedicine represents an important class of cancer therapy. Clinical translation of cancer nanomedicine has significantly reduced the toxicity and adverse consequences of standard-of-care chemotherapy. Recent advances in new cancer treatment modalities (e.g., gene and immune therapies) are profoundly changing the oncology landscape, bringing with them new requirements and challenges for next-generation cancer nanomedicines. We present an overview of cancer nanomedicines in four emerging oncology-associated fields: (i) gene therapy, (ii) immunotherapy, (iii) extracellular vesicle (EV) therapy, and (iv) machine learning-assisted therapy. We discuss the incorporation of nanomedicine into these emerging disciplines, present prominent examples, and evaluate their advantages and challenges. Finally, we discuss future opportunities for next-generation cancer nanomedicines.",4.0
32872562,Proteomics and Metabolomics Approaches towards a Functional Insight onto AUTISM Spectrum Disorders: Phenotype Stratification and Biomarker Discovery,2020 Aug 30;21(17):6274.,"Autism spectrum disorders (ASDs) are neurodevelopmental disorders characterized by behavioral alterations and currently affect about 1% of children. Significant genetic factors and mechanisms underline the causation of ASD. Indeed, many affected individuals are diagnosed with chromosomal abnormalities, submicroscopic deletions or duplications, single-gene disorders or variants. However, a range of metabolic abnormalities has been highlighted in many patients, by identifying biofluid metabolome and proteome profiles potentially usable as ASD biomarkers. Indeed, next-generation sequencing and other omics platforms, including proteomics and metabolomics, have uncovered early age disease biomarkers which may lead to novel diagnostic tools and treatment targets that may vary from patient to patient depending on the specific genomic and other omics findings. The progressive identification of new proteins and metabolites acting as biomarker candidates, combined with patient genetic and clinical data and environmental factors, including microbiota, would bring us towards advanced clinical decision support systems (CDSSs) assisted by machine learning models for advanced ASD-personalized medicine. Herein, we will discuss novel computational solutions to evaluate new proteome and metabolome ASD biomarker candidates, in terms of their recurrence in the reviewed literature and laboratory medicine feasibility. Moreover, the way to exploit CDSS, performed by artificial intelligence, is presented as an effective tool to integrate omics data to electronic health/medical records (EHR/EMR), hopefully acting as added value in the near future for the clinical management of ASD.",5.0
32871426,Coronavirus Disease 2019 (COVID-19) diagnostic technologies: A country-based retrospective analysis of screening and containment procedures during the first wave of the pandemic,2020 Nov;67:219-225.,"Since first report of a novel coronavirus in December of 2019, the Coronavirus Disease 2019 (COVID-19) pandemic has crippled healthcare systems around the world. While many initial screening protocols centered around laboratory detection of the virus, early testing assays were thought to be poorly sensitive in comparison to chest computed tomography, especially in asymptomatic disease. Coupled with shortages of reverse transcription polymerase chain reaction (RT-PCR) testing kits in many parts of the world, these regions instead turned to the use of advanced imaging as a first-line screening modality. However, in contrast to previous Severe Acute Respiratory Syndrome and Middle East Respiratory Syndrome coronavirus epidemics, chest X-ray has not demonstrated optimal sensitivity to be of much utility in first-line screening protocols. Though current national and international guidelines recommend for the use of RT-PCR as the primary screening tool for suspected cases of COVID-19, institutional and regional protocols must consider local availability of resources when issuing universal recommendations. Successful containment and social mitigation strategies worldwide have been thus far predicated on unified governmental responses, though the underlying ideologies of these practices may not be widely applicable in many Western nations. As the strain on the radiology workforce continues to mount, early results indicate a promising role for the use of machine-learning algorithms as risk stratification schema in the months to come.",9.0
32871186,Connecting the dots: Advances in modern metabolomics and its application in yeast system,2020 Nov 15;44:107616.,"History of metabolism originates with yeast making bread. In fact, study based on ""Yeast"" was so crucial in the development of the field of biochemistry that the word ""enzyme"" is derived from the Greek word meaning leavened (yeast). Yeast has always been a point of interest as a eukaryotic model system to demonstrate the metabolites and their function. In recent times their metabolites are widely studied to predict their role in various pathways. Many traditional and analytical techniques have been employed, but its study through metabolomics is of recent interest in research. The present review focuses on details about yeast metabolomics based on preliminary research on various analytical techniques along with computational approaches. The review also aimed to highlight machine learning and various inceptions of yeast metabolomics.",
32866909,Machine learning techniques for detecting electrode misplacement and interchanges when recording ECGs: A systematic review and meta-analysis,Sep-Oct 2020;62:116-123.,"Introduction:                    Electrode misplacement and interchange errors are known problems when recording the 12‑lead electrocardiogram (ECG). Automatic detection of these errors could play an important role for improving clinical decision making and outcomes in cardiac care. The objectives of this systematic review and meta-analysis is to 1) study the impact of electrode misplacement on ECG signals and ECG interpretation, 2) to determine the most challenging electrode misplacements to detect using machine learning (ML), 3) to analyse the ML performance of algorithms that detect electrode misplacement or interchange according to sensitivity and specificity and 4) to identify the most commonly used ML technique for detecting electrode misplacement/interchange. This review analysed the current literature regarding electrode misplacement/interchange recognition accuracy using machine learning techniques.              Method:                    A search of three online databases including IEEE, PubMed and ScienceDirect identified 228 articles, while 3 articles were included from additional sources from co-authors. According to the eligibility criteria, 14 articles were selected. The selected articles were considered for qualitative analysis and meta-analysis.              Results:                    The articles showed the effect of lead interchange on ECG morphology and as a consequence on patient diagnoses. Statistical analysis of the included articles found that machine learning performance is high in detecting electrode misplacement/interchange except left arm/left leg interchange.              Conclusion:                    This review emphasises the importance of detecting electrode misplacement detection in ECG diagnosis and the effects on decision making. Machine learning shows promise in detecting lead misplacement/interchange and highlights an opportunity for developing and operationalising deep learning algorithms such as convolutional neural network (CNN) to detect electrode misplacement/interchange.",
32866134,Machine learning (ML) for the diagnosis of autism spectrum disorder (ASD) using brain imaging,2020 Aug 31;/j/revneuro.ahead-of-print/revneuro-2020-0043/revneuro-2020-0043.xml.,"Autism spectrum disorder (ASD) is a neurodevelopmental incurable disorder with a long diagnostic period encountered in the early years of life. If diagnosed early, the negative effects of this disease can be reduced by starting special education early. Machine learning (ML), an increasingly ubiquitous technology, can be applied for the early diagnosis of ASD. The aim of this study is to examine and provide a comprehensive state-of-the-art review of ML research for the diagnosis of ASD based on (a) structural magnetic resonance image (MRI), (b) functional MRI and (c) hybrid imaging techniques over the past decade. The accuracy of the studies with a large number of participants is in general lower than those with fewer participants leading to the conclusion that further large-scale studies are needed. An examination of the age of the participants shows that the accuracy of the automated diagnosis of ASD is higher at a younger age range. ML technology is expected to contribute significantly to the early and rapid diagnosis of ASD in the coming years and become available to clinicians in the near future. This review is aimed to facilitate that.",1.0
32865001,Computational Radiology in Breast Cancer Screening and Diagnosis Using Artificial Intelligence,2021 Feb;72(1):98-108.,"Breast cancer screening has been shown to significantly reduce mortality in women. The increased utilization of screening examinations has led to growing demands for rapid and accurate diagnostic reporting. In modern breast imaging centers, full-field digital mammography (FFDM) has replaced traditional analog mammography, and this has opened new opportunities for developing computational frameworks to automate detection and diagnosis. Artificial intelligence (AI), and its subdomain of deep learning, is showing promising results and improvements on diagnostic accuracy, compared to previous computer-based methods, known as computer-aided detection and diagnosis.In this commentary, we review the current status of computational radiology, with a focus on deep neural networks used in breast cancer screening and diagnosis. Recent studies are developing a new generation of computer-aided detection and diagnosis systems, as well as leveraging AI-driven tools to efficiently interpret digital mammograms, and breast tomosynthesis imaging. The use of AI in computational radiology necessitates transparency and rigorous testing. However, the overall impact of AI to radiology workflows will potentially yield more efficient and standardized processes as well as improve the level of care to patients with high diagnostic accuracy.",
32864782,AI-Enhanced Diagnosis of Challenging Lesions in Breast MRI: A Methodology and Application Primer,2020 Aug 30.,"Computer-aided diagnosis (CAD) systems have become an important tool in the assessment of breast tumors with magnetic resonance imaging (MRI). CAD systems can be used for the detection and diagnosis of breast tumors as a ""second opinion"" review complementing the radiologist's review. CAD systems have many common parts, such as image preprocessing, tumor feature extraction, and data classification that are mostly based on machine-learning (ML) techniques. In this review article, we describe applications of ML-based CAD systems in MRI covering the detection of diagnostically challenging lesions of the breast such as nonmass enhancing (NME) lesions, and furthermore discuss how multiparametric MRI and radiomics can be applied to the study of NME, including prediction of response to neoadjuvant chemotherapy (NAC). Since ML has been widely used in the medical imaging community, we provide an overview about the state-of-the-art and novel techniques applied as classifiers to CAD systems. The differences in the CAD systems in MRI of the breast for several standard and novel applications for NME are explained in detail to provide important examples, illustrating: 1) CAD for detection and diagnosis, 2) CAD in multiparametric imaging, 3) CAD in NAC, and 4) breast cancer radiomics. We aim to provide a comparison between these CAD applications and to illustrate a global view on intelligent CAD systems based on machine and deep learning in MRI of the breast. LEVEL OF EVIDENCE: 2 TECHNICAL EFFICACY STAGE: 2.",
32864600,The myth of generalisability in clinical research and machine learning in health care,2020 Sep;2(9):e489-e492.,An emphasis on overly broad notions of generalisability as it pertains to applications of machine learning in health care can overlook situations in which machine learning might provide clinical utility. We believe that this narrow focus on generalisability should be replaced with wider considerations for the ultimate goal of building machine learning systems that are useful at the bedside.,7.0
32862251,Deep Learning in Radiation Oncology Treatment Planning for Prostate Cancer: A Systematic Review,2020 Aug 30;44(10):179.,"Radiation oncology for prostate cancer is important as it can decrease the morbidity and mortality associated with this disease. Planning for this modality of treatment is both fundamental, time-consuming and prone to human-errors, leading to potentially avoidable delays in start of treatment. A fundamental step in radiotherapy planning is contouring of radiation targets, where medical specialists contouring, i.e., segment, the boundaries of the structures to be irradiated. Automating this step can potentially lead to faster treatment planning without a decrease in quality, while increasing time available to physicians and also more consistent treatment results. This can be framed as an image segmentation task, which has been studied for many decades in the fields of Computer Vision and Machine Learning. With the advent of Deep Learning, there have been many proposals for different network architectures achieving high performance levels. In this review, we searched the literature for those methods and describe them briefly, grouping those based on Computed Tomography (CT) or Magnetic Resonance Imaging (MRI). This is a booming field, evidenced by the date of the publications found. However, most publications use data from a very limited number of patients, which presents an obstacle to deep learning models training. Although the performance of the models has achieved very satisfactory results, there is still room for improvement, and there is arguably a long way before these models can be used safely and effectively in clinical practice.",
32859549,Brain Age Prediction Reveals Aberrant Brain White Matter in Schizophrenia and Bipolar Disorder: A Multisample Diffusion Tensor Imaging Study,2020 Dec;5(12):1095-1103.,"Background:                    Schizophrenia (SZ) and bipolar disorder (BD) share substantial neurodevelopmental components affecting brain maturation and architecture. This necessitates a dynamic lifespan perspective in which brain aberrations are inferred from deviations from expected lifespan trajectories. We applied machine learning to diffusion tensor imaging (DTI) indices of white matter structure and organization to estimate and compare brain age between patients with SZ, patients with BD, and healthy control (HC) subjects across 10 cohorts.              Methods:                    We trained 6 cross-validated models using different combinations of DTI data from 927 HC subjects (18-94 years of age) and applied the models to the test sets including 648 patients with SZ (18-66 years of age), 185 patients with BD (18-64 years of age), and 990 HC subjects (17-68 years of age), estimating the brain age for each participant. Group differences were assessed using linear models, accounting for age, sex, and scanner. A meta-analytic framework was applied to assess the heterogeneity and generalizability of the results.              Results:                    Tenfold cross-validation revealed high accuracy for all models. Compared with HC subjects, the model including all feature sets significantly overestimated the age of patients with SZ (Cohen's d = -0.29) and patients with BD (Cohen's d = 0.18), with similar effects for the other models. The meta-analysis converged on the same findings. Fractional anisotropy-based models showed larger group differences than the models based on other DTI-derived metrics.              Conclusions:                    Brain age prediction based on DTI provides informative and robust proxies for brain white matter integrity. Our results further suggest that white matter aberrations in SZ and BD primarily consist of anatomically distributed deviations from expected lifespan trajectories that generalize across cohorts and scanners.",2.0
32854412,Wearable Activity Trackers in the Management of Rheumatic Diseases: Where Are We in 2020?,2020 Aug 25;20(17):4797.,"In healthcare, physical activity can be monitored in two ways: self-monitoring by the patient himself or external monitoring by health professionals. Regarding self-monitoring, wearable activity trackers allow automated passive data collection that educate and motivate patients. Wearing an activity tracker can improve walking time by around 1500 steps per day. However, there are concerns about measurement accuracy (e.g., lack of a common validation protocol or measurement discrepancies between different devices). For external monitoring, many innovative electronic tools are currently used in rheumatology to help support physician time management, to reduce the burden on clinic time, and to prioritize patients who may need further attention. In inflammatory arthritis, such as rheumatoid arthritis, regular monitoring of patients to detect disease flares improves outcomes. In a pilot study applying machine learning to activity tracker steps, we showed that physical activity was strongly linked to disease flares and that patterns of physical activity could be used to predict flares with great accuracy, with a sensitivity and specificity above 95%. Thus, automatic monitoring of steps may lead to improved disease control through potential early identification of disease flares. However, activity trackers have some limitations when applied to rheumatic patients, such as tracker adherence, lack of clarity on long-term effectiveness, or the potential multiplicity of trackers.",
32852654,AI (Artificial Intelligence) and Hypertension Research,2020 Aug 27;22(9):70.,"Purpose of review:                    This review a highlights that to use artificial intelligence (AI) tools effectively for hypertension research, a new foundation to further understand the biology of hypertension needs to occur by leveraging genome and RNA sequencing technology and derived tools on a broad scale in hypertension.              Recent findings:                    For the last few years, progress in research and management of essential hypertension has been stagnating while at the same time, the sequencing of the human genome has been generating many new research tools and opportunities to investigate the biology of hypertension. Cancer research has applied modern tools derived from DNA and RNA sequencing on a large scale, enabling the improved understanding of cancer biology and leading to many clinical applications. Compared with cancer, studies in hypertension, using whole genome, exome, or RNA sequencing tools, total less than 2% of the number cancer studies. While true, sequencing the genome of cancer tissue has provided cancer research an advantage, DNA and RNA sequencing derived tools can also be used in hypertension to generate new understanding how complex protein network, in non-cancer tissue, adapts and learns to be effective when for example, somatic mutations or environmental inputs change the gene expression profiles at different network nodes. The amount of data and differences in clinical condition classification at the individual sample level might be of such magnitude to overwhelm and stretch comprehension. Here is the opportunity to use AI tools for the analysis of data streams derived from DNA and RNA sequencing tools combined with clinical data to generate new hypotheses leading to the discovery of mechanisms and potential target molecules from which drugs or treatments can be developed and tested. Basic and clinical research taking advantage of new gene sequencing-based tools, to uncover mechanisms how complex protein networks regulate blood pressure in health and disease, will be critical to lift hypertension research and management from its stagnation. The use of AI analytic tools will help leverage such insights. However, applying AI tools to vast amounts of data that certainly exist in hypertension, without taking advantage of new gene sequencing-based research tools, will generate questionable results and will miss many new potential molecular targets and possibly treatments. Without such approaches, the vision of precision medicine for hypertension will be hard to accomplish and most likely not occur in the near future.",
32849924,"Biobanks in the era of big data: objectives, challenges, perspectives, and innovations for predictive, preventive, and personalised medicine",2020 Jun 18;11(3):333-341.,"Biobanking is entering the new era-era of big data. New technologies, techniques, and knowledge opened the potential of the whole domain of biobanking. Biobanks collect, analyse, store, and share the samples and associated data. Both samples and especially associated data are growing enormously, and new innovative approaches are required to handle samples and to utilize the potential of biobanking data. The data reached the quantity and quality of big data, and the scientists are facing the questions how to use them more efficiently, both retrospectively and prospectively with the aim to discover new preventive methods, optimize treatment, and follow up and to optimize healthcare processes. Biobanking in the era of big data contribute to the development of predictive, preventive, and personalised medicine, for every patient providing the right treatment at the right time. Biobanking in the era of big data contributes to the paradigm shift towards personalising of healthcare.",2.0
32848206,Digital pathology and computational image analysis in nephropathology,2020 Nov;16(11):669-685.,"The emergence of digital pathology - an image-based environment for the acquisition, management and interpretation of pathology information supported by computational techniques for data extraction and analysis - is changing the pathology ecosystem. In particular, by virtue of our new-found ability to generate and curate digital libraries, the field of machine vision can now be effectively applied to histopathological subject matter by individuals who do not have deep expertise in machine vision techniques. Although these novel approaches have already advanced the detection, classification, and prognostication of diseases in the fields of radiology and oncology, renal pathology is just entering the digital era, with the establishment of consortia and digital pathology repositories for the collection, analysis and integration of pathology data with other domains. The development of machine-learning approaches for the extraction of information from image data, allows for tissue interrogation in a way that was not previously possible. The application of these novel tools are placing pathology centre stage in the process of defining new, integrated, biologically and clinically homogeneous disease categories, to identify patients at risk of progression, and shifting current paradigms for the treatment and prevention of kidney diseases.",1.0
32841628,"Computational medicine, present and the future: obstetrics and gynecology perspective",2021 Jan;224(1):16-34.,"Medicine is, in its essence, decision making under uncertainty; the decisions are made about tests to be performed and treatments to be administered. Traditionally, the uncertainty in decision making was handled using expertise collected by individual providers and, more recently, systematic appraisal of research in the form of evidence-based medicine. The traditional approach has been used successfully in medicine for a very long time. However, it has substantial limitations because of the complexity of the system of the human body and healthcare. The complex systems are a network of highly coupled components intensely interacting with each other. These interactions give those systems redundancy and thus robustness to failure and, at the same time, equifinality, that is, many different causative pathways leading to the same outcome. The equifinality of the complex systems of the human body and healthcare system demand the individualization of medical care, medicine, and medical decision making. Computational models excel in modeling complex systems and, consequently, enabling individualization of medical decision making and medicine. Computational models are theory- or knowledge-based models, data-driven models, or models that combine both approaches. Data are essential, although to a different degree, for computational models to successfully represent complex systems. The individualized decision making, made possible by the computational modeling of complex systems, has the potential to revolutionize the entire spectrum of medicine from individual patient care to policymaking. This approach allows applying tests and treatments to individuals who receive a net benefit from them, for whom benefits outweigh the risk, rather than treating all individuals in a population because, on average, the population benefits. Thus, the computational modeling-enabled individualization of medical decision making has the potential to both improve health outcomes and decrease the costs of healthcare.",
32837980,Data-driven modeling of COVID-19-Lessons learned,2020 Oct;40:100921.,"Understanding the outbreak dynamics of COVID-19 through the lens of mathematical models is an elusive but significant goal. Within only half a year, the COVID-19 pandemic has resulted in more than 19 million reported cases across 188 countries with more than 700,000 deaths worldwide. Unlike any other disease in history, COVID-19 has generated an unprecedented volume of data, well documented, continuously updated, and broadly available to the general public. Yet, the precise role of mathematical modeling in providing quantitative insight into the COVID-19 pandemic remains a topic of ongoing debate. Here we discuss the lessons learned from six month of modeling COVID-19. We highlight the early success of classical models for infectious diseases and show why these models fail to predict the current outbreak dynamics of COVID-19. We illustrate how data-driven modeling can integrate classical epidemiology modeling and machine learning to infer critical disease parameters-in real time-from reported case data to make informed predictions and guide political decision making. We critically discuss questions that these models can and cannot answer and showcase controversial decisions around the early outbreak dynamics, outbreak control, and exit strategies. We anticipate that this summary will stimulate discussion within the modeling community and help provide guidelines for robust mathematical models to understand and manage the COVID-19 pandemic. EML webinar speakers, videos, and overviews are updated at https://imechanica.org/node/24098.",4.0
32837454,Simulation-optimization methods for designing and assessing resilient supply chain networks under uncertainty scenarios: A review,2021 Jan;106:102166.,"The design of supply chain networks (SCNs) aims at determining the number, location, and capacity of production facilities, as well as the allocation of markets (customers) and suppliers to one or more of these facilities. This paper reviews the existing literature on the use of simulation-optimization methods in the design of resilient SCNs. From this review, we classify some of the many works in the topic according to factors such as their methodology, the approach they use to deal with uncertainty and risk, etc. The paper also identifies several research opportunities, such as the inclusion of multiple criteria (e.g., monetary, environmental, and social dimensions) during the design-optimization process and the convenience of considering hybrid approaches combining metaheuristic algorithms, simulation, and machine learning methods to account for uncertainty and dynamic conditions, respectively.",
32836901,Time-series forecasting of Bitcoin prices using high-dimensional features: a machine learning approach,2020 Jul 4;1-15.,"Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65% accuracy for next-day forecast and scoring from 62 to 64% accuracy for seventh-ninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44%, while it varies from 2.88 to 4.10% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.",1.0
32834612,Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: A review,2020 Oct;139:110059.,"Background and objective:                    During the recent global urgency, scientists, clinicians, and healthcare experts around the globe keep on searching for a new technology to support in tackling the Covid-19 pandemic. The evidence of Machine Learning (ML) and Artificial Intelligence (AI) application on the previous epidemic encourage researchers by giving a new angle to fight against the novel Coronavirus outbreak. This paper aims to comprehensively review the role of AI and ML as one significant method in the arena of screening, predicting, forecasting, contact tracing, and drug development for SARS-CoV-2 and its related epidemic.              Method:                    A selective assessment of information on the research article was executed on the databases related to the application of ML and AI technology on Covid-19. Rapid and critical analysis of the three crucial parameters, i.e., abstract, methodology, and the conclusion was done to relate to the model's possibilities for tackling the SARS-CoV-2 epidemic.              Result:                    This paper addresses on recent studies that apply ML and AI technology towards augmenting the researchers on multiple angles. It also addresses a few errors and challenges while using such algorithms in real-world problems. The paper also discusses suggestions conveying researchers on model design, medical experts, and policymakers in the current situation while tackling the Covid-19 pandemic and ahead.              Conclusion:                    The ongoing development in AI and ML has significantly improved treatment, medication, screening, prediction, forecasting, contact tracing, and drug/vaccine development process for the Covid-19 pandemic and reduce the human intervention in medical practice. However, most of the models are not deployed enough to show their real-world operation, but they are still up to the mark to tackle the SARS-CoV-2 epidemic.",36.0
32833571,How Will Machine Learning Inform the Clinical Care of Atrial Fibrillation?,2020 Jun 19;127(1):155-169.,"Machine learning applications in cardiology have rapidly evolved in the past decade. With the availability of machine learning tools coupled with vast data sources, the management of atrial fibrillation (AF), a common chronic disease with significant associated morbidity and socioeconomic impact, is undergoing a knowledge and practice transformation in the increasingly complex healthcare environment. Among other advances, deep-learning machine learning methods, including convolutional neural networks, have enabled the development of AF screening pathways using the ubiquitous 12-lead ECG to detect asymptomatic paroxysmal AF in at-risk populations (such as those with cryptogenic stroke), the refinement of AF and stroke prediction schemes through comprehensive digital phenotyping using structured and unstructured data abstraction from the electronic health record or wearable monitoring technologies, and the optimization of treatment strategies, ranging from stroke prophylaxis to monitoring of antiarrhythmic drug (AAD) therapy. Although the clinical and population-wide impact of these tools continues to be elucidated, such transformative progress does not come without challenges, such as the concerns about adopting black box technologies, assessing input data quality for training such models, and the risk of perpetuating rather than alleviating health disparities. This review critically appraises the advances of machine learning related to the care of AF thus far, their potential future directions, and its potential limitations and challenges.",
32830044,Challenges and Opportunities in Machine-Augmented Plant Stress Phenotyping,2021 Jan;26(1):53-69.,"Plant stress phenotyping is essential to select stress-resistant varieties and develop better stress-management strategies. Standardization of visual assessments and deployment of imaging techniques have improved the accuracy and reliability of stress assessment in comparison with unaided visual measurement. The growing capabilities of machine learning (ML) methods in conjunction with image-based phenotyping can extract new insights from curated, annotated, and high-dimensional datasets across varied crops and stresses. We propose an overarching strategy for utilizing ML techniques that methodically enables the application of plant stress phenotyping at multiple scales across different types of stresses, program goals, and environments.",
32827173,Low-Field MRI of Stroke: Challenges and Opportunities,2020 Aug 22;e27324.,"Stroke is a leading cause of death and disability worldwide. The reasons for increased stroke burden in developing countries are inadequately controlled risk factors resulting from poor public awareness and inadequate infrastructure. Computed tomography and MRI are common neuroimaging modalities used to assess stroke with diffusion-weighted MRI, in particular, being the recommended choice for acute stroke imaging. However, access to these imaging modalities is primarily restricted to major cities and high-income groups. In the case of stroke, the time-window of treatment to limit the damage is of a few hours and needs a point-of-care diagnosis. A low-cost MR system typically achieved at the ultra-low- and very-low-field would meet the need for a geographically accessible and portable solution. We review studies focused on accessible stroke imaging and recent developments in MR methodologies, including hardware, to image at low fields. We hypothesize that in the absence of a formal, rapid stroke triaging system, the value of timely on-site delivery of the scanner to the stroke patient can be significant. To this end, we discuss multiple recent hardware and methods developments in the low-field regime. Our review suggests a compelling need to explore further the trade-offs between high signal, contrast, and accessibility at low fields in low-income communities. LEVEL OF EVIDENCE: 4 TECHNICAL EFFICACY STAGE: 6.",
32825928,The future of pathology is digital,2020 Sep;216(9):153040.,"Information, archives, and intelligent artificial systems are part of everyday life in modern medicine. They already support medical staff by mapping their workflows with shared availability of cases' referral information, as needed for example, by the pathologist, and this support will be increased in the future even more. In radiology, established standards define information models, data transmission mechanisms, and workflows. Other disciplines, such as pathology, cardiology, and radiation therapy, now define further demands in addition to these established standards. Pathology may have the highest technical demands on the systems, with very complex workflows, and the digitization of slides generating enormous amounts of data up to Gigabytes per biopsy. This requires enormous amounts of data to be generated per biopsy, up to the gigabyte range. Digital pathology allows a change from classical histopathological diagnosis with microscopes and glass slides to virtual microscopy on the computer, with multiple tools using artificial intelligence and machine learning to support pathologists in their future work.",1.0
32825520,"The Role of EEG in the Diagnosis, Prognosis and Clinical Correlations of Dementia with Lewy Bodies-A Systematic Review",2020 Aug 20;10(9):616.,"Despite improvements in diagnostic criteria for dementia with Lewy bodies (DLB), the ability to discriminate DLB from Alzheimer's disease (AD) and other dementias remains suboptimal. Electroencephalography (EEG) is currently a supportive biomarker in the diagnosis of DLB. We performed a systematic review to better clarify the diagnostic and prognostic role of EEG in DLB and define the clinical correlates of various EEG features described in DLB. MEDLINE, EMBASE, and PsycINFO were searched using search strategies for relevant articles up to 6 August 2020. We included 43 studies comparing EEG in DLB with other diagnoses, 42 of them included a comparison of DLB with AD, 10 studies compared DLB with Parkinson's disease dementia, and 6 studies compared DLB with other dementias. The studies were visual EEG assessment (6), quantitative EEG (35) and event-related potential studies (2). The most consistent observation was the slowing of the dominant EEG rhythm (<8 Hz) assessed visually or through quantitative EEG, which was observed in ~90% of patients with DLB and only ~10% of patients with AD. Other findings based on qualitative rating, spectral power analyses, connectivity, microstate and machine learning algorithms were largely heterogenous due to differences in study design, EEG acquisition, preprocessing and analysis. EEG protocols should be standardized to allow replication and validation of promising EEG features as potential biomarkers in DLB.",1.0
32824342,Machine Learning Models to Predict Childhood and Adolescent Obesity: A Review,2020 Aug 16;12(8):2466.,"The prevalence of childhood and adolescence overweight an obesity is raising at an alarming rate in many countries. This poses a serious threat to the current and near-future health systems, given the association of these conditions with different comorbidities (cardiovascular diseases, type II diabetes, and metabolic syndrome) and even death. In order to design appropriate strategies for its prevention, as well as understand its origins, the development of predictive models for childhood/adolescent overweight/obesity and related outcomes is of extreme value. Obesity has a complex etiology, and in the case of childhood and adolescence obesity, this etiology includes also specific factors like (pre)-gestational ones; weaning; and the huge anthropometric, metabolic, and hormonal changes that during this period the body suffers. In this way, Machine Learning models are becoming extremely useful tools in this area, given their excellent predictive power; ability to model complex, nonlinear relationships between variables; and capacity to deal with high-dimensional data typical in this area. This is especially important given the recent appearance of large repositories of Electronic Health Records (EHR) that allow the development of models using datasets with many instances and predictor variables, from which Deep Learning variants can generate extremely accurate predictions. In the current work, the area of Machine Learning models to predict childhood and adolescent obesity and related outcomes is comprehensively and critically reviewed, including the latest ones using Deep Learning with EHR. These models are compared with the traditional statistical ones that used mainly logistic regression. The main features and applications appearing from these models are described, and the future opportunities are discussed.",1.0
32823322,From Patient Engagement to Precision Oncology: Leveraging Informatics to Advance Cancer Care,2020 Aug;29(1):235-242.,"Objectives:                    Conduct a survey of the literature for advancements in cancer informatics over the last three years in three specific areas where there has been unprecedented growth: 1) digital health; 2) machine learning; and 3) precision oncology. We also highlight the ethical implications and future opportunities within each area.              Methods:                    A search was conducted over a three-year period in two electronic databases (PubMed, Google Scholar) to identify peer-reviewed articles and conference proceedings. Search terms included variations of the following: neoplasms[MeSH], informatics[MeSH], cancer, oncology, clinical cancer informatics, medical cancer informatics. The search returned too many articles for practical review (23,994 from PubMed and 23,100 from Google Scholar). Thus, we conducted searches of key PubMed-indexed informatics journals and proceedings. We further limited our search to manuscripts that demonstrated a clear focus on clinical or translational cancer informatics. Manuscripts were then selected based on their methodological rigor, scientific impact, innovation, and contribution towards cancer informatics as a field or on their impact on cancer care and research.              Results:                    Key developments and opportunities in cancer informatics research in the areas of digital health, machine learning, and precision oncology were summarized.              Conclusion:                    While there are numerous innovations in the field of cancer informatics to advance prevention and clinical care, considerable challenges remain related to data sharing and privacy, digital accessibility, and algorithm biases and interpretation. The implementation and application of these findings in cancer care necessitates further consideration and research.",1.0
32823318,Medical Information Extraction in the Age of Deep Learning,2020 Aug;29(1):208-220.,"Objectives:                    We survey recent developments in medical Information Extraction (IE) as reported in the literature from the past three years. Our focus is on the fundamental methodological paradigm shift from standard Machine Learning (ML) techniques to Deep Neural Networks (DNNs). We describe applications of this new paradigm concentrating on two basic IE tasks, named entity recognition and relation extraction, for two selected semantic classes-diseases and drugs (or medications)-and relations between them.              Methods:                    For the time period from 2017 to early 2020, we searched for relevant publications from three major scientific communities: medicine and medical informatics, natural language processing, as well as neural networks and artificial intelligence.              Results:                    In the past decade, the field of Natural Language Processing (NLP) has undergone a profound methodological shift from symbolic to distributed representations based on the paradigm of Deep Learning (DL). Meanwhile, this trend is, although with some delay, also reflected in the medical NLP community. In the reporting period, overwhelming experimental evidence has been gathered, as illustrated in this survey for medical IE, that DL-based approaches outperform non-DL ones by often large margins. Still, small-sized and access-limited corpora create intrinsic problems for data-greedy DL as do special linguistic phenomena of medical sublanguages that have to be overcome by adaptive learning strategies.              Conclusions:                    The paradigm shift from (feature-engineered) ML to DNNs changes the fundamental methodological rules of the game for medical NLP. This change is by no means restricted to medical IE but should also deeply influence other areas of medical informatics, either NLP- or non-NLP-based.",2.0
32823316,Review of Clinical Research Informatics,2020 Aug;29(1):193-202.,"Objectives:                    Clinical Research Informatics (CRI) declares its scope in its name, but its content, both in terms of the clinical research it supports-and sometimes initiates-and the methods it has developed over time, reach much further than the name suggests. The goal of this review is to celebrate the extraordinary diversity of activity and of results, not as a prize-giving pageant, but in recognition of the field, the community that both serves and is sustained by it, and of its interdisciplinarity and its international dimension.              Methods:                    Beyond personal awareness of a range of work commensurate with the author's own research, it is clear that, even with a thorough literature search, a comprehensive review is impossible. Moreover, the field has grown and subdivided to an extent that makes it very hard for one individual to be familiar with every branch or with more than a few branches in any depth. A literature survey was conducted that focused on informatics-related terms in the general biomedical and healthcare literature, and specific concerns (""artificial intelligence"", ""data models"", ""analytics"", etc.) in the biomedical informatics (BMI) literature. In addition to a selection from the results from these searches, suggestive references within them were also considered.              Results:                    The substantive sections of the paper-Artificial Intelligence, Machine Learning, and ""Big Data"" Analytics; Common Data Models, Data Quality, and Standards; Phenotyping and Cohort Discovery; Privacy: Deidentification, Distributed Computation, Blockchain; Causal Inference and Real-World Evidence-provide broad coverage of these active research areas, with, no doubt, a bias towards this reviewer's interests and preferences, landing on a number of papers that stood out in one way or another, or, alternatively, exemplified a particular line of work.              Conclusions:                    CRI is thriving, not only in the familiar major centers of research, but more widely, throughout the world. This is not to pretend that the distribution is uniform, but to highlight the potential for this domain to play a prominent role in supporting progress in medicine, healthcare, and wellbeing everywhere. We conclude with the observation that CRI and its practitioners would make apt stewards of the new medical knowledge that their methods will bring forward.",
32823315,Contributions from the 2019 Literature on Bioinformatics and Translational Informatics,2020 Aug;29(1):188-192.,"Objectives:                    Summarize recent research and select the best papers published in 2019 in the field of Bioinformatics and Translational Informatics (BTI) for the corresponding section of the International Medical Informatics Association Yearbook.              Methods:                    A literature review was performed for retrieving from PubMed papers indexed with keywords and free terms related to BTI. Independent review allowed the section editors to select a list of 15 candidate best papers which were subsequently peer-reviewed. A final consensus meeting gathering the whole Yearbook editorial committee was organized to finally decide on the selection of the best papers.              Results:                    Among the 931 retrieved papers covering the various subareas of BTI, the review process selected four best papers. The first paper presents a logical modeling of cancer pathways. Using their tools, the authors are able to identify two known behaviours of tumors. The second paper describes a deep-learning approach to predicting resistance to antibiotics in Mycobacterium tuberculosis. The authors of the third paper introduce a Genomic Global Positioning System (GPS) enabling comparison of genomic data with other individuals or genomics databases while preserving privacy. The fourth paper presents a multi-omics and temporal sequence-based approach to provide a better understanding of the sequence of events leading to Alzheimer's Disease.              Conclusions:                    Thanks to the normalization of open data and open science practices, research in BTI continues to develop and mature. Noteworthy achievements are sophisticated applications of leading edge machine-learning methods dedicated to personalized medicine.",
32823311,Design and Use of Semantic Resources: Findings from the Section on Knowledge Representation and Management of the 2020 International Medical Informatics Association Yearbook,2020 Aug;29(1):163-168.,"Objective:                    To select, present, and summarize the best papers in the field of Knowledge Representation and Management (KRM) published in 2019.              Methods:                    A comprehensive and standardized review of the biomedical informatics literature was performed to select the most interesting papers of KRM published in 2019, based on PubMed and ISI Web Of Knowledge queries.              Results:                    Four best papers were selected among 1,189 publications retrieved, following the usual International Medical Informatics Association Yearbook reviewing process. In 2019, research areas covered by pre-selected papers were represented by the design of semantic resources (methods, visualization, curation) and the application of semantic representations for the integration/enrichment of biomedical data. Besides new ontologies and sound methodological guidance to rethink knowledge bases design, we observed large scale applications, promising results for phenotypes characterization, semantic-aware machine learning solutions for biomedical data analysis, and semantic provenance information representations for scientific reproducibility evaluation.              Conclusion:                    In the KRM selection for 2019, research on knowledge representation demonstrated significant contributions both in the design and in the application of semantic resources. Semantic representations serve a great variety of applications across many medical domains, with actionable results.",
32823307,"Notable Papers and Trends from 2019 in Sensors, Signals, and Imaging Informatics",2020 Aug;29(1):139-144.,"Objective:                    To highlight noteworthy papers that are representative of 2019 developments in the fields of sensors, signals, and imaging informatics.              Method:                    A broad literature search was conducted in January 2020 using PubMed. Separate predefined queries were created for sensors/signals and imaging informatics using a combination of Medical Subject Heading (MeSH) terms and keywords. Section editors reviewed the titles and abstracts of both sets of results. Papers were assessed on a three-point Likert scale by two co-editors, rated from 3 (do not include) to 1 (should be included). Papers with an average score of 2 or less were then read by all three section editors, and the group nominated top papers based on consensus. These candidate best papers were then rated by at least six external reviewers.              Results:                    The query related to signals and sensors returned a set of 255 papers from 140 unique journals. The imaging informatics query returned a set of 3,262 papers from 870 unique journals. Based on titles and abstracts, the section co-editors jointly filtered the list down to 50 papers from which 15 candidate best papers were nominated after discussion. A composite rating after review determined four papers which were then approved by consensus of the International Medical Informatics Association (IMIA) Yearbook editorial board. These best papers represent different international groups and journals.              Conclusions:                    The four best papers represent state-of-the-art approaches for processing, combining, and analyzing heterogeneous sensor and imaging data. These papers demonstrate the use of advanced machine learning techniques to improve comparisons between images acquired at different time points, fuse information from multiple sensors, and translate images from one modality to another.",
32822545,What's New for Clinical Whole-body MRI (WB-MRI) in the 21st Century,2020 Nov 1;93(1115):20200562.,"Whole-body MRI (WB-MRI) has evolved since its first introduction in the 1970s as an imaging technique to detect and survey disease across multiple sites and organ systems in the body. The development of diffusion-weighted MRI (DWI) has added a new dimension to the implementation of WB-MRI on modern scanners, offering excellent lesion-to-background contrast, while achieving acceptable spatial resolution to detect focal lesions 5 to 10 mm in size. MRI hardware and software advances have reduced acquisition times, with studies taking 40-50 min to complete.The rising awareness of medical radiation exposure coupled with the advantages of MRI has resulted in increased utilization of WB-MRI in oncology, paediatrics, rheumatological and musculoskeletal conditions and more recently in population screening. There is recognition that WB-MRI can be used to track disease evolution and monitor response heterogeneity in patients with cancer. There are also opportunities to combine WB-MRI with molecular imaging on PET-MRI systems to harness the strengths of hybrid imaging. The advent of artificial intelligence and machine learning will shorten image acquisition times and image analyses, making the technique more competitive against other imaging technologies.",3.0
32821856,Digitizing clinical trials,2020 Jul 31;3:101.,"Clinical trials are a fundamental tool used to evaluate the efficacy and safety of new drugs and medical devices and other health system interventions. The traditional clinical trials system acts as a quality funnel for the development and implementation of new drugs, devices and health system interventions. The concept of a ""digital clinical trial"" involves leveraging digital technology to improve participant access, engagement, trial-related measurements, and/or interventions, enable concealed randomized intervention allocation, and has the potential to transform clinical trials and to lower their cost. In April 2019, the US National Institutes of Health (NIH) and the National Science Foundation (NSF) held a workshop bringing together experts in clinical trials, digital technology, and digital analytics to discuss strategies to implement the use of digital technologies in clinical trials while considering potential challenges. This position paper builds on this workshop to describe the current state of the art for digital clinical trials including (1) defining and outlining the composition and elements of digital trials; (2) describing recruitment and retention using digital technology; (3) outlining data collection elements including mobile health, wearable technologies, application programming interfaces (APIs), digital transmission of data, and consideration of regulatory oversight and guidance for data security, privacy, and remotely provided informed consent; (4) elucidating digital analytics and data science approaches leveraging artificial intelligence and machine learning algorithms; and (5) setting future priorities and strategies that should be addressed to successfully harness digital methods and the myriad benefits of such technologies for clinical research.",3.0
32821854,Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities,2020 Jul 30;3:99.,"The machine learning community has become alert to the ways that predictive algorithms can inadvertently introduce unfairness in decision-making. Herein, we discuss how concepts of algorithmic fairness might apply in healthcare, where predictive algorithms are being increasingly used to support decision-making. Central to our discussion is the distinction between algorithmic fairness and algorithmic bias. Fairness concerns apply specifically when algorithms are used to support polar decisions (i.e., where one pole of prediction leads to decisions that are generally more desired than the other), such as when predictions are used to allocate scarce health care resources to a group of patients that could all benefit. We review different fairness criteria and demonstrate their mutual incompatibility. Even when models are used to balance benefits-harms to make optimal decisions for individuals (i.e., for non-polar decisions)-and fairness concerns are not germane-model, data or sampling issues can lead to biased predictions that support decisions that are differentially harmful/beneficial across groups. We review these potential sources of bias, and also discuss ways to diagnose and remedy algorithmic bias. We note that remedies for algorithmic fairness may be more problematic, since we lack agreed upon definitions of fairness. Finally, we propose a provisional framework for the evaluation of clinical prediction models offered for further elaboration and refinement. Given the proliferation of prediction models used to guide clinical decisions, developing consensus for how these concerns can be addressed should be prioritized.",
32821348,The role of artificial intelligence in colon polyps detection,Summer 2020;13(3):191-199.,"Over the past few decades, artificial intelligence (AI) has evolved dramatically and is believed to have a significant impact on all aspects of technology and daily life. The use of AI in the healthcare system has been rapidly growing, owing to the large amount of data. Various methods of AI including machine learning, deep learning and convolutional neural network (CNN) have been used in diagnostic imaging, which have helped physicians in the accurate diagnosis of diseases and determination of appropriate treatment for them. Using and collecting a huge number of digital images and medical records has led to the creation of big data over a time period. Currently, considerations regarding the diagnosis of various presentations in all endoscopic procedures and imaging findings are solely handled by endoscopists. Moreover, AI has shown to be highly effective in the field of gastroenterology in terms of diagnosis, prognosis, and image processing. Herein, this review aimed to discuss different aspects of AI use for early detection and treatment of gastroenterology diseases.",
