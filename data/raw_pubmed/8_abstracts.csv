pmid,title,date,text,citations
32578067,Machine Learning Within Studies of Early-Life Environmental Exposures and Child Health: Review of the Current Literature and Discussion of Next Steps,2020 Sep;7(3):170-184.,"Purpose of review:                    The goal of this article is to review the use of machine learning (ML) within studies of environmental exposures and children's health, identify common themes across studies, and provide recommendations to advance their use in research and practice.              Recent findings:                    We identified 42 articles reporting upon the use of ML within studies of environmental exposures and children's health between 2017 and 2019. The common themes among the articles were analysis of mixture data, exposure prediction, disease prediction and forecasting, analysis of complex data, and causal inference. With the increasing complexity of environmental health data, we anticipate greater use of ML to address the challenges that cannot be handled by traditional analytics. In order for these methods to beneficially impact public health, the ML techniques we use need to be appropriate for our study questions, rigorously evaluated and reported in a way that can be critically assessed by the scientific community.",
32576276,"""Right-to-Try"" experimental drugs: an overview",2020 Jun 23;18(1):253.,"The ""Right-to-Try"" experimental drugs act passed by Donald Trump in 2018 provides an opportunity of early access to experimental drugs for the treatment of life-threatening diseases and a potential boon to many young and under-capitalized biotechnology or pharmaceutical companies. The pros and cons of experimental drugs, including a number of ""cutting edge"" scientific, clinical, and a number of synergistic approaches such as artificial intelligence, machine learning, big data, data refineries, electronic health records, data driven clinical decisions and risk mitigation are reviewed.",
32575560,"Current Applications, Opportunities, and Limitations of AI for 3D Imaging in Dental Research and Practice",2020 Jun 19;17(12):4424.,"The increasing use of three-dimensional (3D) imaging techniques in dental medicine has boosted the development and use of artificial intelligence (AI) systems for various clinical problems. Cone beam computed tomography (CBCT) and intraoral/facial scans are potential sources of image data to develop 3D image-based AI systems for automated diagnosis, treatment planning, and prediction of treatment outcome. This review focuses on current developments and performance of AI for 3D imaging in dentomaxillofacial radiology (DMFR) as well as intraoral and facial scanning. In DMFR, machine learning-based algorithms proposed in the literature focus on three main applications, including automated diagnosis of dental and maxillofacial diseases, localization of anatomical landmarks for orthodontic and orthognathic treatment planning, and general improvement of image quality. Automatic recognition of teeth and diagnosis of facial deformations using AI systems based on intraoral and facial scanning will very likely be a field of increased interest in the future. The review is aimed at providing dental practitioners and interested colleagues in healthcare with a comprehensive understanding of the current trend of AI developments in the field of 3D imaging in dental medicine.",1.0
32570374,Emerging Concepts and Applied Machine Learning Research in Patients with Drug-Induced Repolarization Disorders,2020 Jun 16;270:198-202.,"The paper presents a review of current research to develop predictive models for automated detection of drug-induced repolarization disorders and shows a feasibility study for developing machine learning tools trained on massive multimodal datasets of narrative, textual and electrocardiographic records. The goal is to reduce drug-induced long QT and associated complications (Torsades-de-Pointes, sudden cardiac death), by identifying prescription patterns with pro-arrhythmic propensity using a validated electronic application for the detection of adverse drug events with data mining and natural language processing; and to compute individual-based predictive scores in order to further identify clinical conditions, concomitant diseases, or other variables that correlate with higher risk of pro-arrhythmic situations.",
32568489,An Examination of Emerging Bioethical Issues in Biomedical Research: Proceedings of a Workshop,,"On February 26, 2020, the Board on Health Sciences Policy of the National Academies of Sciences, Engineering, and Medicine hosted a 1-day public workshop in Washington, DC, to examine current and emerging bioethical issues that might arise in the context of biomedical research and to consider research topics in bioethics that could benefit from further attention. The scope of bioethical issues in research is broad, but this workshop focused on issues related to the development and use of digital technologies, artificial intelligence, and machine learning in research and clinical practice; issues emerging as nontraditional approaches to health research become more widespread; the role of bioethics in addressing racial and structural inequalities in health; and enhancing the capacity and diversity of the bioethics workforce. This publication summarizes the presentations and discussions from the workshop.",
32567390,In silico models for genotoxicity and drug regulation,2020 Aug;16(8):651-662.,"Introduction:                    Whereas in the past, (Q)SAR methods have been largely used to support the design of new drugs, in the last few decades, there has been a new interest in its applications for the assessment of drug safety. In particular, the ICH M7 guideline has introduced the concept that (Q)SAR predictions for the Ames mutagenicity of drug impurities can be used for regulatory purposes.              Areas covered:                    This review introduces the ICH M7 conceptual framework and illustrates the most updated evaluations of the in silico approaches for the prediction of genotoxicity. The strengths and weaknesses of the state-of-the-art are presented and future perspectives are discussed.              Expert opinion:                    Given the growing recognition of (Q)SAR approaches, more investment will be devoted to its improvement. The major areas of research should be the expansion and curation of the experimental training sets, with particular attention to the portions of chemical space which are poorly represented. New modeling methodologies (e.g. machine-learning methods) may support this effort, particularly for treating proprietary data without disclosure. Research on new integrative approaches for regulatory decisions will also be important.",1.0
32567255,Drawing Guidelines for Receiver Operating Characteristic Curve in Preparation of Manuscripts,2020 Jun 22;35(24):e171.,"The appropriate plot effectively conveys the author's conclusions to the readers. The Journal of Korean Medical Science provides a series of special articles to show you how to make consistent and excellent plots easier. In the second article, drawing receiver operating characteristic (ROC) curve is introduced. A ROC curve is a graphic plot that illustrates the diagnostic ability as its discrimination threshold is varied. It is widely used as logistic regression analysis as machine learning becomes widespread. It has great visual effect in comparing various diagnostic tools.",
32566759,Machine learning approaches to drug response prediction: challenges and recent progress,2020 Jun 15;4:19.,"Cancer is a leading cause of death worldwide. Identifying the best treatment using computational models to personalize drug response prediction holds great promise to improve patient's chances of successful recovery. Unfortunately, the computational task of predicting drug response is very challenging, partially due to the limitations of the available data and partially due to algorithmic shortcomings. The recent advances in deep learning may open a new chapter in the search for computational drug response prediction models and ultimately result in more accurate tools for therapy response. This review provides an overview of the computational challenges and advances in drug response prediction, and focuses on comparing the machine learning techniques to be of utmost practical use for clinicians and machine learning non-experts. The incorporation of new data modalities such as single-cell profiling, along with techniques that rapidly find effective drug combinations will likely be instrumental in improving cancer care.",5.0
32563708,The compatibility of theoretical frameworks with machine learning analyses in psychological research,2020 Dec;36:83-88.,"Supervised machine learning has been increasingly used in psychology and psychiatry research. Machine learning offers an important advantage over traditional statistical analyses: statistical model training in example data to enhance predictions in external test data. Additional advantages include advanced, improved statistical algorithms, and empirical methods to select a smaller set of predictor variables. Yet machine learning researchers often use large numbers of predictor variables, without using theory to guide variable selection. Such approach leads to Type I error, spurious findings, and decreased generalizability. We discuss the importance of theory to the psychology field. We offer suggestions for using theory to drive variable selection and data analyses using machine learning in psychological research, including an example from the cyberpsychology field.",
32562154,Artificial Intelligence-Enabled ECG: a Modern Lens on an Old Technology,2020 Jun 19;22(8):57.,"Purpose of review:                    To (i) review the concept of artificial intelligence (AI); (ii) summarize recent developments in artificial intelligence-enabled electrocardiogram (AI-ECG); (iii) address notable inherent limitations and challenges of AI-ECG; and (iv) discuss the future direction of the field.              Recent findings:                    Advancements in machine learning and computing methods have led to application of AI-ECG and potential new applications to patient care. Further study is needed to verify previous findings in diverse populations as well as begin to confront the limitations needed for clinical implementation. Nearly one century after the Nobel Prize was awarded to Willem Einthoven for demonstrating that an electrocardiogram (ECG) could record the electrical signature of the heart, the ECG remains one of the most important diagnostic tests in modern medicine. We now stand at the edge of true ECG innovation. Simultaneous advancements in computing power, wireless technology, digitized data availability, and machine learning have led to the birth of AI-ECG algorithms with novel capabilities and real potential for clinical application. AI has the potential to improve diagnostic accuracy and efficiency by providing fully automated, unbiased, and unambiguous ECG analysis along with promising new findings that may unlock new value in the ECG. These breakthroughs may cause a paradigm shift in clinical workflow as well as patient monitoring and management.",1.0
32562100,Machine Learning-Based Segmentation of Left Ventricular Myocardial Fibrosis from Magnetic Resonance Imaging,2020 Jun 19;22(8):65.,"Purpose of review:                    Myocardial fibrosis (MF) arises due to myocardial infarction and numerous cardiac diseases. MF may lead to several heart disorders, such as heart failure, arrhythmias, and ischemia. Cardiac magnetic resonance (CMR) imaging techniques, such as late gadolinium enhancement (LGE) CMR, enable non-invasive assessment of MF in the left ventricle (LV). Manual assessment of MF on CMR is a tedious and time-consuming task that is subject to high observer variability. Automated segmentation and quantification of MF is important for risk stratification and treatment planning in patients with heart disorders. This article aims to review the machine learning (ML)-based methodologies developed for MF quantification in the LV using CMR images.              Recent findings:                    With the availability of relatively large labeled datasets supervised learning methods based on both conventional ML and state-of-the-art deep learning (DL) methods have been successfully applied for automated segmentation of MF. The incorporation of ML algorithms into imaging techniques such as 3D LGE CMR permits fast characterization of MF on CMR imaging and may enhance the diagnosis and prognosis of patients with heart disorders. Concurrently, the studies using cine CMR images have revealed that accurate segmentation of MF on non-contrast CMR imaging might be possible. The application of ML/DL tools in CMR image interpretation is likely to result in accurate and efficient quantification of MF.",
32561444,Recent advances of HCI in decision-making tasks for optimized clinical workflows and precision medicine,2020 Aug;108:103479.,"The ever-increasing amount of biomedical data is enabling new large-scale studies, even though ad hoc computational solutions are required. The most recent Machine Learning (ML) and Artificial Intelligence (AI) techniques have been achieving outstanding performance and an important impact in clinical research, aiming at precision medicine, as well as improving healthcare workflows. However, the inherent heterogeneity and uncertainty in the healthcare information sources pose new compelling challenges for clinicians in their decision-making tasks. Only the proper combination of AI and human intelligence capabilities, by explicitly taking into account effective and safe interaction paradigms, can permit the delivery of care that outperforms what either can do separately. Therefore, Human-Computer Interaction (HCI) plays a crucial role in the design of software oriented to decision-making in medicine. In this work, we systematically review and discuss several research fields strictly linked to HCI and clinical decision-making, by subdividing the articles into six themes, namely: Interfaces, Visualization, Electronic Health Records, Devices, Usability, and Clinical Decision Support Systems. However, these articles typically present overlaps among the themes, revealing that HCI inter-connects multiple topics. With the goal of focusing on HCI and design aspects, the articles under consideration were grouped into four clusters. The advances in AI can effectively support the physicians' cognitive processes, which certainly play a central role in decision-making tasks because the human mental behavior cannot be completely emulated and captured; the human mind might solve a complex problem even without a statistically significant amount of data by relying upon domain knowledge. For this reason, technology must focus on interactive solutions for supporting the physicians effectively in their daily activities, by exploiting their unique knowledge and evidence-based reasoning, as well as improving the various aspects highlighted in this review.",2.0
32561299,Image-based high-content screening in drug discovery,2020 Aug;25(8):1348-1361.,"While target-based drug discovery strategies rely on the precise knowledge of the identity and function of the drug targets, phenotypic drug discovery (PDD) approaches allow the identification of novel drugs based on knowledge of a distinct phenotype. Image-based high-content screening (HCS) is a potent PDD strategy that characterizes small-molecule effects through the quantification of features that depict cellular changes among or within cell populations, thereby generating valuable data sets for subsequent data analysis. However, these data can be complex, making image analysis from large HCS campaigns challenging. Technological advances in image acquisition, processing, and analysis as well as machine-learning (ML) approaches for the analysis of multidimensional data sets have rendered HCS as a viable technology for small-molecule drug discovery. Here, we discuss HCS concepts, current workflows as well as opportunities and challenges of image-based phenotypic screening and data analysis.",
32560091,"COVID-19 Diagnostics, Tools, and Prevention",2020 Jun 16;10(6):409.,"The Coronavirus Disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), outbreak from Wuhan City, Hubei province, China in 2019 has become an ongoing global health emergency. The emerging virus, SARS-CoV-2, causes coughing, fever, muscle ache, and shortness of breath or dyspnea in symptomatic patients. The pathogenic particles that are generated by coughing and sneezing remain suspended in the air or attach to a surface to facilitate transmission in an aerosol form. This review focuses on the recent trends in pandemic biology, diagnostics methods, prevention tools, and policies for COVID-19 management. To meet the growing demand for medical supplies during the COVID-19 era, a variety of personal protective equipment (PPE) and ventilators have been developed using do-it-yourself (DIY) manufacturing. COVID-19 diagnosis and the prediction of virus transmission are analyzed by machine learning algorithms, simulations, and digital monitoring. Until the discovery of a clinically approved vaccine for COVID-19, pandemics remain a public concern. Therefore, technological developments, biomedical research, and policy development are needed to decipher the coronavirus mechanism and epidemiological characteristics, prevent transmission, and develop therapeutic drugs.",13.0
32560074,The Future of Protein Secondary Structure Prediction Was Invented by Oleg Ptitsyn,2020 Jun 16;10(6):910.,"When Oleg Ptitsyn and his group published the first secondary structure prediction for a protein sequence, they started a research field that is still active today. Oleg Ptitsyn combined fundamental rules of physics with human understanding of protein structures. Most followers in this field, however, use machine learning methods and aim at the highest (average) percentage correctly predicted residues in a set of proteins that were not used to train the prediction method. We show that one single method is unlikely to predict the secondary structure of all protein sequences, with the exception, perhaps, of future deep learning methods based on very large neural networks, and we suggest that some concepts pioneered by Oleg Ptitsyn and his group in the 70s of the previous century likely are today's best way forward in the protein secondary structure prediction field.",
32559068,Machine Learning Force Fields and Coarse-Grained Variables in Molecular Dynamics: Application to Materials and Biological Systems,2020 Aug 11;16(8):4757-4775.,"Machine learning encompasses tools and algorithms that are now becoming popular in almost all scientific and technological fields. This is true for molecular dynamics as well, where machine learning offers promises of extracting valuable information from the enormous amounts of data generated by simulation of complex systems. We provide here a review of our current understanding of goals, benefits, and limitations of machine learning techniques for computational studies on atomistic systems, focusing on the construction of empirical force fields from ab initio databases and the determination of reaction coordinates for free energy computation and enhanced sampling.",2.0
32558902,Automated video monitoring of insect pollinators in the field,2020 Jul 2;4(1):87-97.,"Ecosystems are at increasing risk from the global pollination crisis. Gaining better knowledge about pollinators and their interactions with plants is an urgent need. However, conventional methods of manually recording pollinator activity in the field can be time- and cost-consuming in terms of labour. Field-deployable video recording systems have become more common in ecological studies as they enable the capture of plant-insect interactions in fine detail. Standard video recording can be effective, although there are issues with hardware reliability under field-conditions (e.g. weatherproofing), and reviewing raw video manually is a time-consuming task. Automated video monitoring systems based on motion detection partly overcome these issues by only recording when activity occurs hence reducing the time needed to review footage during post-processing. Another advantage of these systems is that the hardware has relatively low power requirements. A few systems have been tested in the field which permit the collection of large datasets. Compared with other systems, automated monitoring allows vast increases in sampling at broad spatiotemporal scales. Some tools such as post-recording computer vision software and data-import scripts exist, further reducing users' time spent processing and analysing the data. Integrated computer vision and automated species recognition using machine learning models have great potential to further the study of pollinators in the field. Together, it is predicted that future advances in technology-based field monitoring methods will contribute significantly to understanding the causes underpinning pollinator declines and, hence, developing effective solutions for dealing with this global challenge.",1.0
32552005,The application of machine learning techniques to innovative antibacterial discovery and development,2020 Oct;15(10):1165-1180.,"Introduction:                    After the initial wave of antibiotic discovery, few novel classes of antibiotics have emerged, with the latest dating back to the 1980's. Furthermore, the pace of antibiotic drug discovery is unable to keep up with the increasing prevalence of antibiotic drug resistance. However, the increasing amount of available data promotes the use of machine learning techniques (MLT) in drug discovery projects (e.g. construction of regression/classification models and ranking/virtual screening of compounds).              Areas covered:                    In this review, the authors cover some of the applications of MLT in medicinal chemistry, focusing on the development of new antibiotics, the prediction of resistance and its mechanisms. The aim of this review is to illustrate the main advantages and disadvantages and the major trends from studies over the past 5 years.              Expert opinion:                    The application of MLT to antibacterial drug discovery can aid the selection of new and potent lead compounds, with desirable pharmacokinetic and toxic profiles for further optimization. The increasing volume of available data along with the constant improvement in computational power and algorithms has meant that we are experiencing a transition in the way we face modern issues such as drug resistance, where our decisions are data-driven and experiments can be focused by data-suggested hypotheses.",2.0
32547805,Quantified Self-Using Consumer Wearable Device: Predicting Physical and Mental Health,2020 Apr;26(2):83-92.,"Objectives:                    Recently, wearable device technology has gained more popularity in supporting a healthy lifestyle. Hence, researchers have begun to put significant efforts into studying the direct and indirect benefits of wearable devices for health and wellbeing. This paper summarizes recent studies on the use of consumer wearable devices to improve physical activity, mental health, and health consciousness.              Methods:                    A thorough literature search was performed from several reputable databases, such as PubMed, Scopus, ScienceDirect, arXiv, and bioRxiv mainly using ""wearable device research"" as a keyword, no earlier than 2018. As a result, 25 of the most recent and relevant papers included in this review cover several topics, such as previous literature reviews (9 papers), wearable device accuracy (3 papers), self-reported data collection tools (3 papers), and wearable device intervention (10 papers).              Results:                    All the chosen studies are discussed based on the wearable device used, complementary data, study design, and data processing method. All these previous studies indicate that wearable devices are used either to validate their benefits for general wellbeing or for more serious medical contexts, such as cardiovascular disorders and post-stroke treatment.              Conclusions:                    Despite their huge potential for adoption in clinical settings, wearable device accuracy and validity remain the key challenge to be met. Some lessons learned and future projections, such as combining traditional study design with statistical and machine learning methods, are highlighted in this paper to provide a useful overview for other researchers carrying out similar research.",
32547596,Gene Regulatory Network Inference: Connecting Plant Biology and Mathematical Modeling,2020 May 25;11:457.,"Plant responses to environmental and intrinsic signals are tightly controlled by multiple transcription factors (TFs). These TFs and their regulatory connections form gene regulatory networks (GRNs), which provide a blueprint of the transcriptional regulations underlying plant development and environmental responses. This review provides examples of experimental methodologies commonly used to identify regulatory interactions and generate GRNs. Additionally, this review describes network inference techniques that leverage gene expression data to predict regulatory interactions. These computational and experimental methodologies yield complex networks that can identify new regulatory interactions, driving novel hypotheses. Biological properties that contribute to the complexity of GRNs are also described in this review. These include network topology, network size, transient binding of TFs to DNA, and competition between multiple upstream regulators. Finally, this review highlights the potential of machine learning approaches to leverage gene expression data to predict phenotypic outputs.",1.0
32545768,Machine Learning Applications for Mass Spectrometry-Based Metabolomics,2020 Jun 13;10(6):243.,"The metabolome of an organism depends on environmental factors and intracellular regulation and provides information about the physiological conditions. Metabolomics helps to understand disease progression in clinical settings or estimate metabolite overproduction for metabolic engineering. The most popular analytical metabolomics platform is mass spectrometry (MS). However, MS metabolome data analysis is complicated, since metabolites interact nonlinearly, and the data structures themselves are complex. Machine learning methods have become immensely popular for statistical analysis due to the inherent nonlinear data representation and the ability to process large and heterogeneous data rapidly. In this review, we address recent developments in using machine learning for processing MS spectra and show how machine learning generates new biological insights. In particular, supervised machine learning has great potential in metabolomics research because of the ability to supply quantitative predictions. We review here commonly used tools, such as random forest, support vector machines, artificial neural networks, and genetic algorithms. During processing steps, the supervised machine learning methods help peak picking, normalization, and missing data imputation. For knowledge-driven analysis, machine learning contributes to biomarker detection, classification and regression, biochemical pathway identification, and carbon flux determination. Of important relevance is the combination of different omics data to identify the contributions of the various regulatory levels. Our overview of the recent publications also highlights that data quality determines analysis quality, but also adds to the challenge of choosing the right model for the data. Machine learning methods applied to MS-based metabolomics ease data analysis and can support clinical decisions, guide metabolic engineering, and stimulate fundamental biological discoveries.",9.0
32542455,Machine Learning for 3D Kinematic Analysis of Movements in Neurorehabilitation,2020 Jun 15;20(8):29.,"Purpose of review:                    Recent advances in the machine learning field, especially in deep learning, provide the opportunity for automated, detailed, and unbiased analysis of motor behavior. Although there has not yet been wide use of these techniques in the motor rehabilitation field, they have great potential. In this review, I describe how the current state of machine learning can be applied to 3D kinematic analysis, and how this will have an impact on neurorehabilitation.              Recent findings:                    Applications of deep learning methods, in the form of convolutional neural networks, have been revolutionary for image analysis such as face recognition and object detection in images, exceeding human level performance. Recent studies have shown applicability of these deep learning approaches to human posture and movement classification. It is to be expected that portable stereo-camera systems will bring 3D pose estimation into the clinical setting and allow the assessment of movement quality in response to interventions. Advances in machine learning can help automate the process of obtaining 3D kinematics of human movements and to identify/classify patterns of movement.",1.0
32541594,Explainable AI (xAI) for Anatomic Pathology,2020 Jul;27(4):241-250.,"Pathologists are adopting whole slide images (WSIs) for diagnosis, thanks to recent FDA approval of WSI systems as class II medical devices. In response to new market forces and recent technology advances outside of pathology, a new field of computational pathology has emerged that applies artificial intelligence (AI) and machine learning algorithms to WSIs. Computational pathology has great potential for augmenting pathologists' accuracy and efficiency, but there are important concerns regarding trust of AI due to the opaque, black-box nature of most AI algorithms. In addition, there is a lack of consensus on how pathologists should incorporate computational pathology systems into their workflow. To address these concerns, building computational pathology systems with explainable AI (xAI) mechanisms is a powerful and transparent alternative to black-box AI models. xAI can reveal underlying causes for its decisions; this is intended to promote safety and reliability of AI for critical tasks such as pathology diagnosis. This article outlines xAI enabled applications in anatomic pathology workflow that improves efficiency and accuracy of the practice. In addition, we describe HistoMapr-Breast, an initial xAI enabled software application for breast core biopsies. HistoMapr-Breast automatically previews breast core WSIs and recognizes the regions of interest to rapidly present the key diagnostic areas in an interactive and explainable manner. We anticipate xAI will ultimately serve pathologists as an interactive computational guide for computer-assisted primary diagnosis.",2.0
32537842,Role of Artificial Intelligence and Machine Learning in Nanosafety,2020 Sep;16(36):e2001883.,"Robotics and automation provide potentially paradigm shifting improvements in the way materials are synthesized and characterized, generating large, complex data sets that are ideal for modeling and analysis by modern machine learning (ML) methods. Nanomaterials have not yet fully captured the benefits of automation, so lag behind in the application of ML methods of data analysis. Here, some key developments in, and roadblocks to the application of ML methods are reviewed to model and predict potentially adverse biological and environmental effects of nanomaterials. This work focuses on the diverse ways a range of ML algorithms are applied to understand and predict nanomaterials properties, provides examples of the application of traditional ML and deep learning methods to nanosafety, and provides context and future perspectives on developments that are likely to occur, or need to occur in the near future that allow artificial intelligence to make a deeper contribution to nanosafety.",3.0
32536431,Artificial Intelligence and Echocardiography: A Primer for Cardiac Sonographers,2020 Sep;33(9):1061-1066.,"Artificial intelligence (AI) is emerging as a key component in diagnostic medical imaging, including echocardiography. AI with deep learning has already been used with automated view labeling, measurements, and interpretation. As the development and use of AI in echocardiography increase, potential concerns may be raised by cardiac sonographers and the profession. This report, from a sonographer's perspective, focuses on defining AI, the basics of the technology, identifying some current applications of AI, and how the use of AI may improve patient care in the future.",3.0
32532762,Artificial intelligence for anterior segment diseases: Emerging applications in ophthalmology,2021 Feb;105(2):158-168.,"With the advancement of computational power, refinement of learning algorithms and architectures, and availability of big data, artificial intelligence (AI) technology, particularly with machine learning and deep learning, is paving the way for 'intelligent' healthcare systems. AI-related research in ophthalmology previously focused on the screening and diagnosis of posterior segment diseases, particularly diabetic retinopathy, age-related macular degeneration and glaucoma. There is now emerging evidence demonstrating the application of AI to the diagnosis and management of a variety of anterior segment conditions. In this review, we provide an overview of AI applications to the anterior segment addressing keratoconus, infectious keratitis, refractive surgery, corneal transplant, adult and paediatric cataracts, angle-closure glaucoma and iris tumour, and highlight important clinical considerations for adoption of AI technologies, potential integration with telemedicine and future directions.",4.0
32532187,A Review of Drug Side Effect Identification Methods,2020;26(26):3096-3104.,"Drug side effects have become an important indicator for evaluating the safety of drugs. There are two main factors in the frequent occurrence of drug safety problems; on the one hand, the clinical understanding of drug side effects is insufficient, leading to frequent adverse drug reactions, while on the other hand, due to the long-term period and complexity of clinical trials, side effects of approved drugs on the market cannot be reported in a timely manner. Therefore, many researchers have focused on developing methods to identify drug side effects. In this review, we summarize the methods of identifying drug side effects and common databases in this field. We classified methods of identifying side effects into four categories: biological experimental, machine learning, text mining and network methods. We point out the key points of each kind of method. In addition, we also explain the advantages and disadvantages of each method. Finally, we propose future research directions.",
32529700,Global Leadership Initiative on Malnutrition (GLIM): Guidance on Validation of the Operational Criteria for the Diagnosis of Protein-Energy Malnutrition in Adults,2020 Aug;44(6):992-1003.,"Background:                    The Global Leadership Initiative on Malnutrition (GLIM) created a consensus-based framework consisting of phenotypic and etiologic criteria to record the occurrence of malnutrition in adults. This is a minimum set of practicable indicators for use in characterizing a patient/client as malnourished, considering the global variations in screening and nutrition assessment, and to be used across different healthcare settings. As with other consensus-based frameworks for diagnosing disease states, these operational criteria require validation and reliability testing, as they are currently based solely on expert opinion.              Methods:                    Several forms of validation and reliability are reviewed in the context of GLIM, providing guidance on how to conduct retrospective and prospective studies for criterion and construct validity.              Results:                    There are some aspects of GLIM that require refinement; research using large databases can be employed to reach this goal. Machine learning is also introduced as a potential method to support identification of the best cut points and combinations of indicators for use with the different forms of malnutrition, which the GLIM criteria were created to denote. It is noted as well that validation and reliability testing need to occur in a variety of sectors and populations and with diverse persons using GLIM criteria.              Conclusion:                    The guidance presented supports the conduct and publication of quality validation and reliability studies for GLIM.",1.0
32528977,Mini Review: Deep Learning for Atrial Segmentation From Late Gadolinium-Enhanced MRIs,2020 May 27;7:86.,"Segmentation and 3D reconstruction of the human atria is of crucial importance for precise diagnosis and treatment of atrial fibrillation, the most common cardiac arrhythmia. However, the current manual segmentation of the atria from medical images is a time-consuming, labor-intensive, and error-prone process. The recent emergence of artificial intelligence, particularly deep learning, provides an alternative solution to the traditional methods that fail to accurately segment atrial structures from clinical images. This has been illustrated during the recent 2018 Atrial Segmentation Challenge for which most of the challengers developed deep learning approaches for atrial segmentation, reaching high accuracy (>90% Dice score). However, as significant discrepancies exist between the approaches developed, many important questions remain unanswered, such as which deep learning architectures and methods to ensure reliability while achieving the best performance. In this paper, we conduct an in-depth review of the current state-of-the-art of deep learning approaches for atrial segmentation from late gadolinium-enhanced MRIs, and provide critical insights for overcoming the main hindrances faced in this task.",
32528777,Artificial Intelligence in Modern Medicine - The Evolving Necessity of the Present and Role in Transforming the Future of Medical Care,2020 May 9;12(5):e8041.,"The dexterity of computer systems to resemble and mimic human intelligence is artificial intelligence. Artificial intelligence has reformed the diagnostic and therapeutic precision and competence in various fields of medicine. Artificial intelligence appears to play a bright role in medical diagnosis. Computer systems using artificial intelligence help in the assessment of medical images and enormous data. This research aims to identify how artificial intelligence-based technology is reforming the art of medicine. Artificial intelligence empowers providers in improving efficiency and overall healthcare. Newer machine learning techniques lead the automatic diagnostic systems. Areas of medicine such as medical imaging, automated clinical decision-making support have made significant advances with respect to artificial intelligence technology. With improved diagnosis and prognosis, artificial intelligence possesses the capability to revolutionize various fields of medicine. Artificial intelligence has its own limitations and cannot replace a bedside clinician. In the evolving modern medical digital world, physicians need to support artificial intelligence rather than fear it replacing trained physicians for improved healthcare.",2.0
32525020,A survey on single and multi omics data mining methods in cancer data classification,2020 Jul;107:103466.,"Data analytics is routinely used to support biomedical research in all areas, with particular focus on the most relevant clinical conditions, such as cancer. Bioinformatics approaches, in particular, have been used to characterize the molecular aspects of diseases. In recent years, numerous studies have been performed on cancer based upon single and multi-omics data. For example, Single-omics-based studies have employed a diverse set of data, such as gene expression, DNA methylation, or miRNA, to name only a few instances. Despite that, a significant part of literature reports studies on gene expression with microarray datasets. Single-omics data have high numbers of attributes and very low sample counts. This characteristic makes them paradigmatic of an under-sampled, small-n large-p machine learning problem. An important goal of single-omics data analysis is to find the most relevant genes, in terms of their potential use in clinics and research, in the batch of available data. This problem has been addressed in gene selection as one of the pre-processing steps in data mining. An analysis that use only one type of data (single-omics) often miss the complexity of the landscape of molecular phenomena underlying the disease. As a result, they provide limited and sometimes poorly reliable information about the disease mechanisms. Therefore, in recent years, researchers have been eager to build models that are more complex, obtaining more reliable results using multi-omics data. However, to achieve this, the most important challenge is data integration. In this paper, we provide a comprehensive overview of the challenges in single and multi-omics data analysis of cancer data, focusing on gene selection and data integration methods.",
32522530,"Radiomics in neuro-oncology: Basics, workflow, and applications",2021 Apr;188:112-121.,"Over the last years, the amount, variety, and complexity of neuroimaging data acquired in patients with brain tumors for routine clinical purposes and the resulting number of imaging parameters have substantially increased. Consequently, a timely and cost-effective evaluation of imaging data is hardly feasible without the support of methods from the field of artificial intelligence (AI). AI can facilitate and shorten various time-consuming steps in the image processing workflow, e.g., tumor segmentation, thereby optimizing productivity. Besides, the automated and computer-based analysis of imaging data may help to increase data comparability as it is independent of the experience level of the evaluating clinician. Importantly, AI offers the potential to extract new features from the routinely acquired neuroimages of brain tumor patients. In combination with patient data such as survival, molecular markers, or genomics, mathematical models can be generated that allow, for example, the prediction of treatment response or prognosis, as well as the noninvasive assessment of molecular markers. The subdiscipline of AI dealing with the computation, identification, and extraction of image features, as well as the generation of prognostic or predictive mathematical models, is termed radiomics. This review article summarizes the basics, the current workflow, and methods used in radiomics with a focus on feature-based radiomics in neuro-oncology and provides selected examples of its clinical application.",4.0
32520786,Precision transplant pathology,2020 Aug;25(4):412-419.,"Purpose of review:                    Transplant pathology contributes substantially to personalized treatment of organ allograft recipients. Rapidly advancing next-generation human leukocyte antigen (HLA) sequencing and pathology are enhancing the abilities to improve donor/recipient matching and allograft monitoring.              Recent findings:                    The present review summarizes the workflow of a prototypical patient through a pathology practice, highlighting histocompatibility assessment and pathologic review of tissues as areas that are evolving to incorporate next-generation technologies while emphasizing critical needs of the field.              Summary:                    Successful organ transplantation starts with the most precise pratical donor-recipient histocompatibility matching. Next-generation sequencing provides the highest resolution donor-recipient matching and enables eplet mismatch scores and more precise monitoring of donor-specific antibodies (DSAs) that may arise after transplant. Multiplex labeling combined with hand-crafted machine learning is transforming traditional histopathology. The combination of traditional blood/body fluid laboratory tests, eplet and DSA analysis, traditional and next-generation histopathology, and -omics-based platforms enables risk stratification and identification of early subclinical molecular-based changes that precede a decline in allograft function. Needs include software integration of data derived from diverse platforms that can render the most accurate assessment of allograft health and needs for immunosuppression adjustments.",
32520404,Comprehensive data integration-Toward a more personalized assessment of diastolic function,2020 Nov;37(11):1926-1935.,"Background and aim:                    The main challenge of assessing diastolic function is the balance between clinical utility, in the sense of usability and time-efficiency, and overall applicability, in the sense of precision for the patient under investigation. In this review, we aim to explore the challenges of integrating data in the assessment of diastolic function and discuss the perspectives of a more comprehensive data integration approach.              Methods:                    Review of traditional and novel approaches regarding data integration in the assessment of diastolic function.              Results:                    Comprehensive data integration can lead to improved understanding of disease phenotypes and better relation of these phenotypes to underlying pathophysiological processes-which may help affirm diagnostic reasoning, guide treatment options, and reduce limitations related to previously unaddressed confounders. The optimal assessment of diastolic function should ideally integrate all relevant clinical information with all available structural and functional whole cardiac cycle echocardiographic data-envisioning a personalized approach to patient care, a high-reaching future goal in medicine.              Conclusion:                    Complete data integration seems to be a long-lasting goal, the way forward in diastology, and machine learning seems to be one of the tools suited for the challenge. With perpetual evidence that traditional approaches to complex problems may not the optimal solution, there is room for a steady and cautious, and inherently very exciting paradigm shift toward novel diagnostic tools and workflows to reach a more personalized, comprehensive, and integrated assessment of cardiac function.",
32518043,Towards a Digital Bioprocess Replica: Computational Approaches in Biopharmaceutical Development and Manufacturing,2020 Oct;38(10):1141-1153.,"Quantitative unit operation models for the optimization and refinement of modern late-stage biopharmaceutical drug manufacturing processes have recently attracted increasing attention. The supplementary benefits of these models include increased process robustness and control in combination with a more stringent design of the bioprocess due to a reduced number of exploratory experiments. In addition to unit operations, further efforts also focus on digital bioprocess replicas, which are straightforward combinations of unit operation and process models from inoculum to the fill and finish phase. In this review, we shed more light on digital bioprocess replicas in addition to standard unit operation models and discuss their strengths and weaknesses. We comment on the current usage of these approaches for late stage processes and outline the associated benefits, challenges and limitations.",
32517801,What scans we will read: imaging instrumentation trends in clinical oncology,2020 Jun 9;20(1):38.,"Oncological diseases account for a significant portion of the burden on public healthcare systems with associated costs driven primarily by complex and long-lasting therapies. Through the visualization of patient-specific morphology and functional-molecular pathways, cancerous tissue can be detected and characterized non-invasively, so as to provide referring oncologists with essential information to support therapy management decisions. Following the onset of stand-alone anatomical and functional imaging, we witness a push towards integrating molecular image information through various methods, including anato-metabolic imaging (e.g., PET/CT), advanced MRI, optical or ultrasound imaging.This perspective paper highlights a number of key technological and methodological advances in imaging instrumentation related to anatomical, functional, molecular medicine and hybrid imaging, that is understood as the hardware-based combination of complementary anatomical and molecular imaging. These include novel detector technologies for ionizing radiation used in CT and nuclear medicine imaging, and novel system developments in MRI and optical as well as opto-acoustic imaging. We will also highlight new data processing methods for improved non-invasive tissue characterization. Following a general introduction to the role of imaging in oncology patient management we introduce imaging methods with well-defined clinical applications and potential for clinical translation. For each modality, we report first on the status quo and, then point to perceived technological and methodological advances in a subsequent status go section. Considering the breadth and dynamics of these developments, this perspective ends with a critical reflection on where the authors, with the majority of them being imaging experts with a background in physics and engineering, believe imaging methods will be in a few years from now.Overall, methodological and technological medical imaging advances are geared towards increased image contrast, the derivation of reproducible quantitative parameters, an increase in volume sensitivity and a reduction in overall examination time. To ensure full translation to the clinic, this progress in technologies and instrumentation is complemented by advances in relevant acquisition and image-processing protocols and improved data analysis. To this end, we should accept diagnostic images as ""data"", and - through the wider adoption of advanced analysis, including machine learning approaches and a ""big data"" concept - move to the next stage of non-invasive tumour phenotyping. The scans we will be reading in 10 years from now will likely be composed of highly diverse multi-dimensional data from multiple sources, which mandate the use of advanced and interactive visualization and analysis platforms powered by Artificial Intelligence (AI) for real-time data handling by cross-specialty clinical experts with a domain knowledge that will need to go beyond that of plain imaging.",1.0
32517778,The use of machine learning in rare diseases: a scoping review,2020 Jun 9;15(1):145.,"Background:                    Emerging machine learning technologies are beginning to transform medicine and healthcare and could also improve the diagnosis and treatment of rare diseases. Currently, there are no systematic reviews that investigate, from a general perspective, how machine learning is used in a rare disease context. This scoping review aims to address this gap and explores the use of machine learning in rare diseases, investigating, for example, in which rare diseases machine learning is applied, which types of algorithms and input data are used or which medical applications (e.g., diagnosis, prognosis or treatment) are studied.              Methods:                    Using a complex search string including generic search terms and 381 individual disease names, studies from the past 10 years (2010-2019) that applied machine learning in a rare disease context were identified on PubMed. To systematically map the research activity, eligible studies were categorized along different dimensions (e.g., rare disease group, type of algorithm, input data), and the number of studies within these categories was analyzed.              Results:                    Two hundred eleven studies from 32 countries investigating 74 different rare diseases were identified. Diseases with a higher prevalence appeared more often in the studies than diseases with a lower prevalence. Moreover, some rare disease groups were investigated more frequently than to be expected (e.g., rare neurologic diseases and rare systemic or rheumatologic diseases), others less frequently (e.g., rare inborn errors of metabolism and rare skin diseases). Ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%) were the algorithms most commonly applied in the studies. Only a small proportion of studies evaluated their algorithms on an external data set (11.8%) or against a human expert (2.4%). As input data, images (32.2%), demographic data (27.0%) and ""omics"" data (26.5%) were used most frequently. Most studies used machine learning for diagnosis (40.8%) or prognosis (38.4%) whereas studies aiming to improve treatment were relatively scarce (4.7%). Patient numbers in the studies were small, typically ranging from 20 to 99 (35.5%).              Conclusion:                    Our review provides an overview of the use of machine learning in rare diseases. Mapping the current research activity, it can guide future work and help to facilitate the successful application of machine learning in rare diseases.",
32517304,MRI Segmentation and Classification of Human Brain Using Deep Learning for Diagnosis of Alzheimer's Disease: A Survey,2020 Jun 7;20(11):3243.,"Many neurological diseases and delineating pathological regions have been analyzed, and the anatomical structure of the brain researched with the aid of magnetic resonance imaging (MRI). It is important to identify patients with Alzheimer's disease (AD) early so that preventative measures can be taken. A detailed analysis of the tissue structures from segmented MRI leads to a more accurate classification of specific brain disorders. Several segmentation methods to diagnose AD have been proposed with varying complexity. Segmentation of the brain structure and classification of AD using deep learning approaches has gained attention as it can provide effective results over a large set of data. Hence, deep learning methods are now preferred over state-of-the-art machine learning methods. We aim to provide an outline of current deep learning-based segmentation approaches for the quantitative analysis of brain MRI for the diagnosis of AD. Here, we report how convolutional neural network architectures are used to analyze the anatomical brain structure and diagnose AD, discuss how brain MRI segmentation improves AD classification, describe the state-of-the-art approaches, and summarize their results using publicly available datasets. Finally, we provide insight into current issues and discuss possible future research directions in building a computer-aided diagnostic system for AD.",3.0
32515148,Radiomics in liver diseases: Current progress and future opportunities,2020 Sep;40(9):2050-2063.,"Liver diseases, a wide spectrum of pathologies from inflammation to neoplasm, have become an increasingly significant health problem worldwide. Noninvasive imaging plays a critical role in the clinical workflow of liver diseases, but conventional imaging assessment may provide limited information. Accurate detection, characterization and monitoring remain challenging. With progress in quantitative imaging analysis techniques, radiomics emerged as an efficient tool that shows promise to aid in personalized diagnosis and treatment decision-making. Radiomics could reflect the heterogeneity of liver lesions via extracting high-throughput and high-dimensional features from multi-modality imaging. Machine learning algorithms are then used to construct clinical target-oriented imaging biomarkers to assist disease management. Here, we review the methodological process in liver disease radiomics studies in a stepwise fashion from data acquisition and curation, region of interest segmentation, liver-specific feature extraction, to task-oriented modelling. Furthermore, the applications of radiomics in liver diseases are outlined in aspects of diagnosis and staging, evaluation of liver tumour biological behaviours, and prognosis according to different disease type. Finally, we discuss the current limitations of radiomics in liver disease studies and explore its future opportunities.",1.0
32514649,Convolutional neural networks for brain tumour segmentation,2020 Jun 8;11(1):77.,"The introduction of quantitative image analysis has given rise to fields such as radiomics which have been used to predict clinical sequelae. One growing area of interest for analysis is brain tumours, in particular glioblastoma multiforme (GBM). Tumour segmentation is an important step in the pipeline in the analysis of this pathology. Manual segmentation is often inconsistent as it varies between observers. Automated segmentation has been proposed to combat this issue. Methodologies such as convolutional neural networks (CNNs) which are machine learning pipelines modelled on the biological process of neurons (called nodes) and synapses (connections) have been of interest in the literature. We investigate the role of CNNs to segment brain tumours by firstly taking an educational look at CNNs and perform a literature search to determine an example pipeline for segmentation. We then investigate the future use of CNNs by exploring a novel field-radiomics. This examines quantitative features of brain tumours such as shape, texture, and signal intensity to predict clinical outcomes such as survival and response to therapy.",2.0
32514496,Evaluation of liver tumour response by imaging,2020 Apr 28;2(3):100100.,"The goal of assessing tumour response on imaging is to identify patients who are likely to benefit - or not - from anticancer treatment, especially in relation to survival. The World Health Organization was the first to develop assessment criteria. This early score, which assessed tumour burden by standardising lesion size measurements, laid the groundwork for many of the criteria that followed. This was then improved by the Response Evaluation Criteria in Solid Tumours (RECIST) which was quickly adopted by the oncology community. At the same time, many interventional oncology treatments were developed to target specific features of liver tumours that result in significant changes in tumours but have little effect on tumour size. New criteria focusing on the viable part of tumours were therefore designed to provide more appropriate feedback to guide patient management. Targeted therapy has resulted in a breakthrough that challenges conventional response criteria due to the non-linear relationship between response and tumour size, requiring the development of methods that emphasize the appearance of tumours. More recently, research into functional and quantitative imaging has created new opportunities in liver imaging. These results have suggested that certain parameters could serve as early predictors of response or could predict later tumour response at baseline. These approaches have now been extended by machine learning and deep learning. This clinical review focuses on the progress made in the evaluation of liver tumours on imaging, discussing the rationale for this approach, addressing challenges and controversies in the field, and suggesting possible future developments.",5.0
32514380,"Artificial Intelligence, Data Sensors and Interconnectivity: Future Opportunities for Heart Failure",2020 May 12;6:e11.,"A higher proportion of patients with heart failure have benefitted from a wide and expanding variety of sensor-enabled implantable devices than any other patient group. These patients can now also take advantage of the ever-increasing availability and affordability of consumer electronics. Wearable, on- and near-body sensor technologies, much like implantable devices, generate massive amounts of data. The connectivity of all these devices has created opportunities for pooling data from multiple sensors - so-called interconnectivity - and for artificial intelligence to provide new diagnostic, triage, risk-stratification and disease management insights for the delivery of better, more personalised and cost-effective healthcare. Artificial intelligence is also bringing important and previously inaccessible insights from our conventional cardiac investigations. The aim of this article is to review the convergence of artificial intelligence, sensor technologies and interconnectivity and the way in which this combination is set to change the care of patients with heart failure.",1.0
32513061,Artificial Intelligence Applications in Otology: A State of the Art Review,2020 Dec;163(6):1123-1133.,"Objective:                    Recent advances in artificial intelligence (AI) are driving innovative new health care solutions. We aim to review the state of the art of AI in otology and provide a discussion of work underway, current limitations, and future directions.              Data sources:                    Two comprehensive databases, MEDLINE and EMBASE, were mined using a directed search strategy to identify all articles that applied AI to otology.              Review methods:                    An initial abstract and title screening was completed. Exclusion criteria included nonavailable abstract and full text, language, and nonrelevance. References of included studies and relevant review articles were cross-checked to identify additional studies.              Conclusion:                    The database search identified 1374 articles. Abstract and title screening resulted in full-text retrieval of 96 articles. A total of N = 38 articles were retained. Applications of AI technologies involved the optimization of hearing aid technology (n = 5; 13% of all articles), speech enhancement technologies (n = 4; 11%), diagnosis and management of vestibular disorders (n = 11; 29%), prediction of sensorineural hearing loss outcomes (n = 9; 24%), interpretation of automatic brainstem responses (n = 5; 13%), and imaging modalities and image-processing techniques (n = 4; 10%). Publication counts of the included articles from each decade demonstrated a marked increase in interest in AI in recent years.              Implications for practice:                    This review highlights several applications of AI that otologists and otolaryngologists alike should be aware of given the possibility of implementation in mainstream clinical practice. Although there remain significant ethical and regulatory challenges, AI powered systems offer great potential to shape how healthcare systems of the future operate and clinicians are key stakeholders in this process.",
32512174,New methodologies in ageing research,2020 Sep;62:101094.,"Ageing is arguably the most complex phenotype that occurs in humans. To understand and treat ageing as well as associated diseases, highly specialised technologies are emerging that reveal critical insight into the underlying mechanisms and provide new hope for previously untreated diseases. Herein, we describe the latest developments in cutting edge technologies applied across the field of ageing research. We cover emerging model organisms, high-throughput methodologies and machine-driven approaches. In all, this review will give you a glimpse of what will be pushing the field onwards and upwards.",1.0
32511198,Applying Artificial Intelligence to Mitigate Effects of Patient Motion or Other Complicating Factors on Image Quality,2020 Aug;29(4):175-180.,"Artificial intelligence, particularly deep learning, offers several possibilities to improve the quality or speed of image acquisition in magnetic resonance imaging (MRI). In this article, we briefly review basic machine learning concepts and discuss commonly used neural network architectures for image-to-image translation. Recent examples in the literature describing application of machine learning techniques to clinical MR image acquisition or postprocessing are discussed. Machine learning can contribute to better image quality by improving spatial resolution, reducing image noise, and removing undesired motion or other artifacts. As patients occasionally are unable to tolerate lengthy acquisition times or gadolinium agents, machine learning can potentially assist MRI workflow and patient comfort by facilitating faster acquisitions or reducing exogenous contrast dosage. Although artificial intelligence approaches often have limitations, such as problems with generalizability or explainability, there is potential for these techniques to improve diagnostic utility, throughput, and patient experience in clinical MRI practice.",2.0
32510252,Artificial intelligence: improving the efficiency of cardiovascular imaging,2020 Jun;17(6):565-577.,"Introduction:                    Artificial intelligence (AI) describes the use of computational techniques to mimic human intelligence. In healthcare, this typically involves large medical datasets being used to predict a diagnosis, identify new disease genotypes or phenotypes, or guide treatment strategies. Noninvasive imaging remains a cornerstone for the diagnosis, risk stratification, and management of patients with cardiovascular disease. AI can facilitate every stage of the imaging process, from acquisition and reconstruction, to segmentation, measurement, interpretation, and subsequent clinical pathways.              Areas covered:                    In this paper, we review state-of-the-art AI techniques and their current applications in cardiac imaging, and discuss the future role of AI as a precision medicine tool.              Expert opinion:                    Cardiovascular medicine is primed for scalable AI applications which can interpret vast amounts of clinical and imaging data in greater depth than ever before. AI-augmented medical systems have the potential to improve workflow and provide reproducible and objective quantitative results which can inform clinical decisions. In the foreseeable future, AI may work in the background of cardiac image analysis software and routine clinical reporting, automatically collecting data and enabling real-time diagnosis and risk stratification.",2.0
32510054,On the Interpretability of Artificial Intelligence in Radiology: Challenges and Opportunities,2020 May 27;2(3):e190043.,"As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI ""interpretable"" have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists' opinions on the topic and suggests trends and challenges that need to be addressed to effectively streamline interpretability methods in clinical practice. Supplemental material is available for this article.  RSNA, 2020 See also the commentary by Gastounioti and Kontos in this issue.",12.0
32509998,"Aging - Oxidative stress, antioxidants and computational modeling",2020 May 31;6(5):e04107.,"Aging is a degenerative, biological, time-dependent, universally conserved process thus designed as one of the highest known risk factors for morbidity and mortality. Every individual has its own aging mechanisms as both environmental conditions (75%) and genetics (25%) account for aging. Several theories have been proposed until now but not even a single theory solves this mystery. There are still some queries un-answered to the scientific community regarding mechanisms behind aging. However, oxidative stress theory (OST) is considered one of the famous theories that sees mitochondria as one of the leading organelles which largely contribute to the aging process. Many reactive oxygen species (ROS) are produced endogenously and exogenously that are associated with aging. But the mitochondrial ROS contribute largely to the aging process as mitochondrial dysfunction due to oxidative stress is considered one of the contributors toward aging. Although ROS is known to damage cell machinery, new evidence suggests their role in signal transduction to regulate biological and physiological processes. Moreover, besides mitochondria, other important cell organelles such as peroxisome and endoplasmic reticulum also produce ROS that contribute to aging. However, nature has provided humans with free radical scavengers called antioxidants that protect from harmful effects of ROS. Future predictions regarding aging, biochemical mechanisms involved, biomarkers internal and external factors can be easily done with machine learning algorithms and other computational models. This review explains important aspects of aging, the contribution of ROS producing organelles in aging, importance of antioxidants fighting against ROS, different computational models developed to understand the complexities of the aging.",6.0
32506273,E-Health in Hypertension Management: an Insight into the Current and Future Role of Blood Pressure Telemonitoring,2020 Jun 6;22(6):42.,"Purpose of review:                    Out-of-office blood pressure (BP) monitoring techniques, including home and ambulatory BP monitoring, are currently recommended by hypertension guidelines worldwide to confirm the diagnosis of hypertension and to monitor the appropriateness of treatment. However, such techniques are not always effectively implemented or timely available in the routine clinical practice. In recent years, the widespread availability of e-health solutions has stimulated the development of blood pressure telemonitoring (BPT) systems, which allow remote BP tracking and tighter and more efficient monitoring of patients' health status.              Recent findings:                    There is currently strong evidence that BPT may be of benefit for hypertension screening and diagnosis and for improving hypertension management. The advantage is more significant when BPT is coupled with multimodal interventions involving a physician, a nurse or pharmacist, and including education on lifestyle and risk factors and drug management. Several randomized controlled studies documented enhanced hypertension management and improved BP control of hypertensive patients through BPT. Potential additional effects of BPT are represented by improved compliance to treatment, intensification, and optimization of drug use, improved quality of life, reduction in risk of developing cardiovascular complications, and cost-saving. Applications based on m-health and making use of wearables or smartwatches integrated with machine learning models are particularly promising for the future development of efficient BPT solutions, and they will provide remarkable support decision tools for doctors. BPT and telehealth will soon disrupt hypertension management. However, which approach will be the most effective and whether it will be sustainable in the long-term still need to be elucidated.",2.0
32504192,"The basics of data, big data, and machine learning in clinical practice",2021 Jan;40(1):11-23.,"Health informatics and biomedical computing have introduced the use of computer methods to analyze clinical information and provide tools to assist clinicians during the diagnosis and treatment of diverse clinical conditions. With the amount of information that can be obtained in the healthcare setting, new methods to acquire, organize, and analyze the data are being developed each day, including new applications in the world of big data and machine learning. In this review, first we present the most basic concepts in data science, including the structural hierarchy of information and how it is managed. A section is dedicated to discussing topics relevant to the acquisition of data, importantly the availability and use of online resources such as survey software and cloud computing services. Along with digital datasets, these tools make it possible to create more diverse models and facilitate collaboration. After, we describe concepts and techniques in machine learning used to process and analyze health data, especially those most widely applied in rheumatology. Overall, the objective of this review is to aid in the comprehension of how data science is used in health, with a special emphasis on the relevance to the field of rheumatology. It provides clinicians with basic tools on how to approach and understand new trends in health informatics analysis currently being used in rheumatology practice. If clinicians understand the potential use and limitations of health informatics, this will facilitate interdisciplinary conversations and continued projects relating to data, big data, and machine learning.",
32503819,Coronavirus Disease (COVID-19): A Machine Learning Bibliometric Analysis,2020 Jun;34(3 Suppl):1613-1617.,"Background/aim:                    To evaluate the research trends in coronavirus disease (COVID-19).              Materials and methods:                    A bibliometric analysis was performed using a machine learning bibliometric methodology. Information regarding publication outputs, countries, institutions, journals, keywords, funding and citation counts was retrieved from Scopus database.              Results:                    A total of 1883 eligible papers were returned. An exponential increase in the COVID-19 publications occurred in the last months. As expected, China produced the majority of articles, followed by the United States of America, the United Kingdom and Italy. There is greater collaboration between highly contributing authors and institutions. The ""BMJ"" published the highest number of papers (n=129) and ""The Lancet"" had the most citations (n=1439). The most ubiquitous topic was COVID-19 clinical features.              Conclusion:                    This bibliometric analysis presents the most influential references related to COVID-19 during this time and could be useful to improve understanding and management of COVID-19.",13.0
32500471,Ethical Considerations of Using Machine Learning for Decision Support in Occupational Health: An Example Involving Periodic Workers' Health Assessments,2020 Sep;30(3):343-353.,"Purpose Computer algorithms and Machine Learning (ML) will be integrated into clinical decision support within occupational health care. This will change the interaction between health care professionals and their clients, with unknown consequences. The aim of this study was to explore ethical considerations and potential consequences of using ML based decision support tools (DSTs) in the context of occupational health. Methods We conducted an ethical deliberation. This was supported by a narrative literature review of publications about ML and DSTs in occupational health and by an assessment of the potential impact of ML-DSTs according to frameworks from medical ethics and philosophy of technology. We introduce a hypothetical clinical scenario from a workers' health assessment to reflect on biomedical ethical principles: respect for autonomy, beneficence, non-maleficence and justice. Results Respect for autonomy is affected by uncertainty about what future consequences the worker is consenting to as a result of the fluctuating nature of ML-DSTs and validity evidence used to inform the worker. A beneficent advisory process is influenced because the three elements of evidence based practice are affected through use of a ML-DST. The principle of non-maleficence is challenged by the balance between group-level benefits and individual harm, the vulnerability of the worker in the occupational context, and the possibility of function creep. Justice might be empowered when the ML-DST is valid, but profiling and discrimination are potential risks. Conclusions Implications of ethical considerations have been described for the socially responsible design of ML-DSTs. Three recommendations were provided to minimize undesirable adverse effects of the development and implementation of ML-DSTs.",
32500035,Next Generation Sequencing and Machine Learning Technologies Are Painting the Epigenetic Portrait of Glioblastoma,2020 May 15;10:798.,"Even with a rare occurrence of only 1.35% of cancer cases in the United States of America, brain tumors are considered as one of the most lethal malignancies. The most aggressive and invasive type of brain tumor, glioblastoma, accounts for 60-70% of all gliomas and presents with life expectancy of only 12-18 months. Despite trimodal treatment and advances in diagnostic and therapeutic methods, there are no significant changes in patient outcome. Our understanding of glioblastoma was significantly improved with the introduction of next generation sequencing technologies. This led to the identification of different genetic and molecular subtypes, which greatly improve glioblastoma diagnosis. Still, because of the poor life expectancy, novel diagnostic, and treatment methods are broadly explored. Epigenetic modifications like methylation and changes in histone acetylation are such examples. Recently, in addition to genetic and molecular characteristics, epigenetic profiling of glioblastomas is also used for sample classification. Further advancement of next generation sequencing technologies is expected to identify in detail the epigenetic signature of glioblastoma that can open up new therapeutic opportunities for glioblastoma patients. This should be complemented with the use of computational power i.e., machine and deep learning algorithms for objective diagnostics and design of individualized therapies. Using a combination of phenotypic, genotypic, and epigenetic parameters in glioblastoma diagnostics will bring us closer to precision medicine where therapies will be tailored to suit the genetic profile and epigenetic signature of the tumor, which will grant longer life expectancy and better quality of life. Still, a number of obstacles including potential bias, availability of data for minorities in heterogeneous populations, data protection, and validation and independent testing of the learning algorithms have to be overcome on the way.",1.0
32499001,Automated machine learning: Review of the state-of-the-art and opportunities for healthcare,2020 Apr;104:101822.,"Objective:                    This work aims to provide a review of the existing literature in the field of automated machine learning (AutoML) to help healthcare professionals better utilize machine learning models ""off-the-shelf"" with limited data science expertise. We also identify the potential opportunities and barriers to using AutoML in healthcare, as well as existing applications of AutoML in healthcare.              Methods:                    Published papers, accompanied with code, describing work in the field of AutoML from both a computer science perspective or a biomedical informatics perspective were reviewed. We also provide a short summary of a series of AutoML challenges hosted by ChaLearn.              Results:                    A review of 101 papers in the field of AutoML revealed that these automated techniques can match or improve upon expert human performance in certain machine learning tasks, often in a shorter amount of time. The main limitation of AutoML at this point is the ability to get these systems to work efficiently on a large scale, i.e. beyond small- and medium-size retrospective datasets.              Discussion:                    The utilization of machine learning techniques has the demonstrated potential to improve health outcomes, cut healthcare costs, and advance clinical research. However, most hospitals are not currently deploying machine learning solutions. One reason for this is that health care professionals often lack the machine learning expertise that is necessary to build a successful model, deploy it in production, and integrate it with the clinical workflow. In order to make machine learning techniques easier to apply and to reduce the demand for human experts, automated machine learning (AutoML) has emerged as a growing field that seeks to automatically select, compose, and parametrize machine learning models, so as to achieve optimal performance on a given task and/or dataset.              Conclusion:                    While there have already been some use cases of AutoML in the healthcare field, more work needs to be done in order for there to be widespread adoption of AutoML in healthcare.",9.0
32498995,Computerized decision support and machine learning applications for the prevention and treatment of childhood obesity: A systematic review of the literature,2020 Apr;104:101844.,"Background:                    Digital health interventions based on tools for Computerized Decision Support (CDS) and Machine Learning (ML), which take advantage of new information, sensing and communication technologies, can play a key role in childhood obesity prevention and treatment.              Objectives:                    We present a systematic literature review of CDS and ML applications for the prevention and treatment of childhood obesity. The main characteristics and outcomes of studies using CDS and ML are demonstrated, to advance our understanding towards the development of smart and effective interventions for childhood obesity care.              Methods:                    A search in the bibliographic databases of PubMed and Scopus was performed to identify childhood obesity studies incorporating either CDS interventions, or advanced data analytics through ML algorithms. Ongoing, case, and qualitative studies, along with those not providing specific quantitative outcomes were excluded. The studies incorporating CDS were synthesized according to the intervention's main technology (e.g., mobile app), design type (e.g., randomized controlled trial), number of enrolled participants, target age of children, participants' follow-up duration, primary outcome (e.g., Body Mass Index (BMI)), and main CDS feature(s) and their outcomes (e.g., alerts for caregivers when BMI is high). The studies incorporating ML were synthesized according to the number of subjects included and their age, the ML algorithm(s) used (e.g., logistic regression), as well as their main outcome (e.g., prediction of obesity).              Results:                    The literature search identified 8 studies incorporating CDS interventions and 9 studies utilizing ML algorithms, which met our eligibility criteria. All studies reported statistically significant interventional or ML model outcomes (e.g., in terms of accuracy). More than half of the interventional studies (n = 5, 63 %) were designed as randomized controlled trials. Half of the interventional studies (n = 4, 50 %) utilized Electronic Health Records (EHRs) and alerts for BMI as means of CDS. From the 9 studies using ML, the highest percentage targeted at the prognosis of obesity (n = 4, 44 %). In the studies incorporating more than one ML algorithms and reporting accuracy, it was shown that decision trees and artificial neural networks can accurately predict childhood obesity.              Conclusions:                    This review has found that CDS tools can be useful for the self-management or remote medical management of childhood obesity, whereas ML algorithms such as decision trees and artificial neural networks can be helpful for prediction purposes. Further rigorous studies in the area of CDS and ML for childhood obesity care are needed, considering the low number of studies identified in this review, their methodological limitations, and the scarcity of interventional studies incorporating ML algorithms in CDS tools.",3.0
32495652,Machine Learning in Cardiology-Ensuring Clinical Impact Lives Up to the Hype,2020 Sep;25(5):379-390.,"Despite substantial advances in the study, treatment, and prevention of cardiovascular disease, numerous challenges relating to optimally screening, diagnosing, and managing patients remain. Simultaneous improvements in computing power, data storage, and data analytics have led to the development of new techniques to address these challenges. One powerful tool to this end is machine learning (ML), which aims to algorithmically identify and represent structure within data. Machine learning's ability to efficiently analyze large and highly complex data sets make it a desirable investigative approach in modern biomedical research. Despite this potential and enormous public and private sector investment, few prospective studies have demonstrated improved clinical outcomes from this technology. This is particularly true in cardiology, despite its emphasis on objective, data-driven results. This threatens to stifle ML's growth and use in mainstream medicine. We outline the current state of ML in cardiology and outline methods through which impactful and sustainable ML research can occur. Following these steps can ensure ML reaches its potential as a transformative technology in medicine.",1.0
32487891,Machine-learning algorithms for predicting results in liver transplantation: the problem of donor-recipient matching,2020 Aug;25(4):406-411.,"Purpose of review:                    Classifiers based on artificial intelligence can be useful to solve decision problems related to the inclusion or removal of possible liver transplant candidates, and assisting in the heterogeneous field of donor-recipient (D-R) matching.              Recent findings:                    Artificial intelligence models can show a great advantage by being able to handle a multitude of variables, be objective and help in cases of similar probabilities. In the field of liver transplantation, the most commonly used classifiers have been artificial neural networks (ANNs) and random forest classifiers. ANNs are excellent tools for finding patterns which are far too complex for a clinician and are capable of generating near-perfect predictions on the data on which they are fit, yielding excellent prediction capabilities reaching 95% for 3 months graft survival. On the other hand, RF can overcome ANNs in some of their limitations, mainly because of the lack of information on the variables they provide. Random forest algorithms may allow for improved confidence with the use of marginal organs and better outcome after transplantation.              Summary:                    ANNs and random forest can handle a multitude of structured and unstructured parameters, and establish non explicit relationships among risk factors of clinical relevance.",
32487887,Artificial intelligence in transplantation (machine-learning classifiers and transplant oncology),2020 Aug;25(4):426-434.,"Purpose of review:                    To highlight recent efforts in the development and implementation of machine learning in transplant oncology - a field that uses liver transplantation for the treatment of hepatobiliary malignancies - and particularly in hepatocellular carcinoma, the most commonly treated diagnosis in transplant oncology.              Recent findings:                    The development of machine learning has occurred within three domains related to hepatocellular carcinoma: identification of key clinicopathological variables, genomics, and image processing.              Summary:                    Machine-learning classifiers can be effectively applied for more accurate clinical prediction and handling of data, such as genetics and imaging in transplant oncology. This has allowed for the identification of factors that most significantly influence recurrence and survival in disease, such as hepatocellular carcinoma, and thus help in prognosticating patients who may benefit from a liver transplant. Although progress has been made in using these methods to analyse clinicopathological information, genomic profiles, and image processed data (both histopathological and radiomic), future progress relies on integrating data across these domains.",
32486411,Advances in Smart Environment Monitoring Systems Using IoT and Sensors,2020 May 31;20(11):3113.,"Air quality, water pollution, and radiation pollution are major factors that pose genuine challenges in the environment. Suitable monitoring is necessary so that the world can achieve sustainable growth, by maintaining a healthy society. In recent years, the environment monitoring has turned into a smart environment monitoring (SEM) system, with the advances in the internet of things (IoT) and the development of modern sensors. Under this scenario, the present manuscript aims to accomplish a critical review of noteworthy contributions and research studies on SEM, that involve monitoring of air quality, water quality, radiation pollution, and agriculture systems. The review is divided on the basis of the purposes where SEM methods are applied, and then each purpose is further analyzed in terms of the sensors used, machine learning techniques involved, and classification methods used. The detailed analysis follows the extensive review which has suggested major recommendations and impacts of SEM research on the basis of discussion results and research trends analyzed. The authors have critically studied how the advances in sensor technology, IoT and machine learning methods make environment monitoring a truly smart monitoring system. Finally, the framework of robust methods of machine learning; denoising methods and development of suitable standards for wireless sensor networks (WSNs), has been suggested.",2.0
32484920,Noninvasive detection of focal seizures in ambulatory patients,2020 Nov;61 Suppl 1(Suppl 1):S47-S54.,"Reliably detecting focal seizures without secondary generalization during daily life activities, chronically, using convenient portable or wearable devices, would offer patients with active epilepsy a number of potential benefits, such as providing more reliable seizure count to optimize treatment and seizure forecasting, and triggering alarms to promote safeguarding interventions. However, no generic solution is currently available to reach these objectives. A number of biosignals are sensitive to specific forms of focal seizures, in particular heart rate and its variability for seizures affecting the neurovegetative system, and accelerometry for those responsible for prominent motor activity. However, most studies demonstrate high rates of false detection or poor sensitivity, with only a minority of patients benefiting from acceptable levels of accuracy. To tackle this challenging issue, several lines of technological progress are envisioned, including multimodal biosensing with cross-modal analytics, a combination of embedded and distributed self-aware machine learning, and ultra-low-power design to enable appropriate autonomy of such sophisticated portable solutions.",
32482370,Enhancing sepsis management through machine learning techniques: A review,2020 May 29;S0210-5691(20)30102-9.,"Sepsis is a major public health problem and a leading cause of death in the world, where delay in the beginning of treatment, along with clinical guidelines non-adherence have been proved to be associated with higher mortality. Machine Learning is increasingly being adopted in developing innovative Clinical Decision Support Systems in many areas of medicine, showing a great potential for automatic prediction of diverse patient conditions, as well as assistance in clinical decision making. In this context, this work conducts a narrative review to provide an overview of how specific Machine Learning techniques can be used to improve sepsis management, discussing the main tasks addressed, the most popular methods and techniques, as well as the obtained results, in terms of both intelligent system accuracy and clinical outcomes improvement.",
32481542,Radiomics Applications in Renal Tumor Assessment: A Comprehensive Review of the Literature,2020 May 28;12(6):1387.,"Radiomics texture analysis offers objective image information that could otherwise not be obtained by radiologists' subjective radiological interpretation. We investigated radiomics applications in renal tumor assessment and provide a comprehensive review. A detailed search of original articles was performed using the PubMed-MEDLINE database until 20 March 2020 to identify English literature relevant to radiomics applications in renal tumor assessment. In total, 42 articles were included in the analysis and divided into four main categories: renal mass differentiation, nuclear grade prediction, gene expression-based molecular signatures, and patient outcome prediction. The main area of research involves accurately differentiating benign and malignant renal masses, specifically between renal cell carcinoma (RCC) subtypes and from angiomyolipoma without visible fat and oncocytoma. Nuclear grade prediction may enhance proper patient selection for risk-stratified treatment. Radiomics-predicted gene mutations may serve as surrogate biomarkers for high-risk disease, while predicting patients' responses to targeted therapies and their outcomes will help develop personalized treatment algorithms. Studies generally reported the superiority of radiomics over expert radiological interpretation. Radiomics provides an alternative to subjective image interpretation for improving renal tumor diagnostic accuracy. Further incorporation of clinical and imaging data into radiomics algorithms will augment tumor prediction accuracy and enhance individualized medicine.",3.0
32479253,Artificial intelligence (AI) in urology-Current use and future directions: An iTRUE study,2020 Nov;46(Supp. 1):S27-S39.,"Objective:                    Artificial intelligence (AI) is used in various urological conditions such as urolithiasis, pediatric urology, urogynecology, benign prostate hyperplasia (BPH), renal transplant, and uro-oncology. The various models of AI and its application in urology subspecialties are reviewed and discussed.              Material and methods:                    Search strategy was adapted to identify and review the literature pertaining to the application of AI in urology using the keywords ""urology,"" ""artificial intelligence,"" ""machine learning,"" ""deep learning,"" ""artificial neural networks,"" ""computer vision,"" and ""natural language processing"" were included and categorized. Review articles, editorial comments, and non-urologic studies were excluded.              Results:                    The article reviewed 47 articles that reported characteristics and implementation of AI in urological cancer. In all cases with benign conditions, artificial intelligence was used to predict outcomes of the surgical procedure. In urolithiasis, it was used to predict stone composition, whereas in pediatric urology and BPH, it was applied to predict the severity of condition. In cases with malignant conditions, it was applied to predict the treatment response, survival, prognosis, and recurrence on the basis of the genomic and biomarker studies. These results were also found to be statistically better than routine approaches. Application of radiomics in classification and nuclear grading of renal masses, cystoscopic diagnosis of bladder cancers, predicting Gleason score, and magnetic resonance imaging with computer-assisted diagnosis for prostate cancers are few applications of AI that have been studied extensively.              Conclusions:                    In the near future, we will see a shift in the clinical paradigm as AI applications will find their place in the guidelines and revolutionize the decision-making process.",5.0
32479180,Reporting and Implementing Interventions Involving Machine Learning and Artificial Intelligence,2020 Jun 2;172(11 Suppl):S137-S144.,"Increasingly, interventions aimed at improving care are likely to use such technologies as machine learning and artificial intelligence. However, health care has been relatively late to adopt them. This article provides clinical examples in which machine learning and artificial intelligence are already in use in health care and appear to deliver benefit. Three key bottlenecks toward increasing the pace of diffusion and adoption are methodological issues in evaluation of artificial intelligence-based interventions, reporting standards to enable assessment of model performance, and issues that need to be addressed for an institution to adopt these interventions. Methodological best practices will include external validation, ideally at a different site; use of proactive learning algorithms to correct for site-specific biases and increase robustness as algorithms are deployed across multiple sites; addressing subgroup performance; and communicating to providers the uncertainty of predictions. Regarding reporting, especially important issues are the extent to which implementing standardized approaches for introducing clinical decision support has been followed, describing the data sources, reporting on data assumptions, and addressing biases. Although most health care organizations in the United States have adopted electronic health records, they may be ill prepared to adopt machine learning and artificial intelligence. Several steps can enable this: preparing data, developing tools to get suggestions to clinicians in useful ways, and getting clinicians engaged in the process. Open challenges and the role of regulation in this area are briefly discussed. Although these techniques have enormous potential to improve care and personalize recommendations for individuals, the hype regarding them is tremendous. Organizations will need to approach this domain carefully with knowledgeable partners to obtain the hoped-for benefits and avoid failures.",3.0
32475607,Artificial intelligence and machine learning in nephropathology,2020 Jul;98(1):65-75.,"Artificial intelligence (AI) for the purpose of this review is an umbrella term for technologies emulating a nephropathologist's ability to extract information on diagnosis, prognosis, and therapy responsiveness from native or transplant kidney biopsies. Although AI can be used to analyze a wide variety of biopsy-related data, this review focuses on whole slide images traditionally used in nephropathology. AI applications in nephropathology have recently become available through several advancing technologies, including (i) widespread introduction of glass slide scanners, (ii) data servers in pathology departments worldwide, and (iii) through greatly improved computer hardware to enable AI training. In this review, we explain how AI can enhance the reproducibility of nephropathology results for certain parameters in the context of precision medicine using advanced architectures, such as convolutional neural networks, that are currently the state of the art in machine learning software for this task. Because AI applications in nephropathology are still in their infancy, we show the power and potential of AI applications mostly in the example of oncopathology. Moreover, we discuss the technological obstacles as well as the current stakeholder and regulatory concerns about developing AI applications in nephropathology from the perspective of nephropathologists and the wider nephrology community. We expect the gradual introduction of these technologies into routine diagnostics and research for selective tasks, suggesting that this technology will enhance the performance of nephropathologists rather than making them redundant.",3.0
32472247,Chest Pain Evaluation in the Emergency Department: Risk Scores and High-Sensitivity Cardiac Troponin,2020 May 29;22(7):49.,"Purpose of review:                    As many as 10 million patients present annually to the emergency department in the USA with symptoms concerning for acute myocardial infarction. The use of risk scores for patients with chest pain or equivalent without ST-segment elevation on the electrocardiogram. The adaptation in the USA of high sensitivity troponin assays requires rethinking of how to best optimize troponin testing within a risk score.              Recent findings:                    Patients are risk stratified using a combination of validated risk scores, biomarkers, and both noninvasive and invasive testing. The advent of high-sensitivity troponins has served to augment existing risk scores in the identification of low-risk patients for early discharge, as well as led to the introduction of new rapid rule-out protocols by which acute myocardial infarction can be excluded by biomarker evaluation more quickly. The emergence of machine learning algorithms may further enhance provider's ability to quickly diagnose or exclude myocardial infarction in the emergency department. The addition of high sensitive troponin assays to established emergency department risk scores is providing new opportunities to improve the timeliness and accuracy of the evaluation of patients presenting with a possible myocardial infarction. Utilizing the time between troponin measures as a variable combined with clinical risk factors with new algorithms may further serve to improve diagnostic accuracy.",
32472189,Artificial Intelligence in Intracoronary Imaging,2020 May 29;22(7):46.,"Purpose of review:                    This paper investigates present uses and future potential of artificial intelligence (AI) applied to intracoronary imaging technologies.              Recent findings:                    Advances in data analytics and digitized medical imaging have enabled clinical application of AI to improve patient outcomes and reduce costs through better diagnosis and enhanced workflow. Applications of AI to IVUS and IVOCT have produced improvements in image segmentation, plaque analysis, and stent evaluation. Machine learning algorithms are able to predict future coronary events through the use of imaging results, clinical evaluations, laboratory tests, and demographics. The application of AI to intracoronary imaging holds significant promise for improved understanding and treatment of coronary heart disease. Even in these early stages, AI has demonstrated the ability to improve the prediction of cardiac events. Large curated data sets and databases are needed to speed the development of AI and enable testing and comparison among algorithms.",
32468528,Artificial Neural Networks in Computer-Aided Drug Design: An Overview of Recent Advances,2020;1194:115-125.,"Computer-aided drug design (CADD) is the framework in which the huge amount of data accumulated by high-throughput experimental methods used in drug design is quantitatively studied. Its objectives include pattern recognition, biomarker identification and/or classification, etc. In order to achieve these objectives, machine learning algorithms and especially artificial neural networks (ANNs) have been used over ADMET factor testing and QSAR modeling evaluation. This paper provides an overview of the current trends in CADD-applied ANNs, since their use was re-boosted over a decade ago.",
32468523,Neuroeducation and Computer Programming: A Review,2020;1194:59-66.,"Over the past 5 years, a significant number of studies focused on computer programming and code writing (software development, code comprehension, program debugging, code optimization, developer training), using the capabilities of brain imaging techniques and of biomarkers. With the use of the aforementioned techniques, researchers have explored the role of programming experience and knowledge, the relation between coding and writing, and the possibilities of improving program debugging with machine learning techniques. In this paper, a review of existing literature and discussion of research issues that should be examined in the future are explored. Research may link the neuroscientific field with training issues in programming, so as to contribute to the learning process.",
32466283,A Survey of Marker-Less Tracking and Registration Techniques for Health & Environmental Applications to Augmented Reality and Ubiquitous Geospatial Information Systems,2020 May 25;20(10):2997.,"Most existing augmented reality (AR) applications are suitable for cases in which only a small number of real world entities are involved, such as superimposing a character on a single surface. In this case, we only need to calculate pose of the camera relative to that surface. However, when an AR health or environmental application involves a one-to-one relationship between an entity in the real-world and the corresponding object in the computer model (geo-referenced object), we need to estimate the pose of the camera in reference to a common coordinate system for better geo-referenced object registration in the real-world. New innovations in developing cheap sensors, computer vision techniques, machine learning, and computing power have helped to develop applications with more precise matching between a real world and a virtual content. AR Tracking techniques can be divided into two subcategories: marker-based and marker-less approaches. This paper provides a comprehensive overview of marker-less registration and tracking techniques and reviews their most important categories in the context of ubiquitous Geospatial Information Systems (GIS) and AR focusing to health and environmental applications. Basic ideas, advantages, and disadvantages, as well as challenges, are discussed for each subcategory of tracking and registration techniques. We need precise enough virtual models of the environment for both calibrations of tracking and visualization. Ubiquitous GISs can play an important role in developing AR in terms of providing seamless and precise spatial data for outdoor (e.g., environmental applications) and indoor (e.g., health applications) environments.",
32463370,Challenges of Clustering Multimodal Clinical Data: Review of Applications in Asthma Subtyping,2020 May 28;8(5):e16452.,"Background:                    In the current era of personalized medicine, there is increasing interest in understanding the heterogeneity in disease populations. Cluster analysis is a method commonly used to identify subtypes in heterogeneous disease populations. The clinical data used in such applications are typically multimodal, which can make the application of traditional cluster analysis methods challenging.              Objective:                    This study aimed to review the research literature on the application of clustering multimodal clinical data to identify asthma subtypes. We assessed common problems and shortcomings in the application of cluster analysis methods in determining asthma subtypes, such that they can be brought to the attention of the research community and avoided in future studies.              Methods:                    We searched PubMed and Scopus bibliographic databases with terms related to cluster analysis and asthma to identify studies that applied dissimilarity-based cluster analysis methods. We recorded the analytic methods used in each study at each step of the cluster analysis process.              Results:                    Our literature search identified 63 studies that applied cluster analysis to multimodal clinical data to identify asthma subtypes. The features fed into the cluster algorithms were of a mixed type in 47 (75%) studies and continuous in 12 (19%), and the feature type was unclear in the remaining 4 (6%) studies. A total of 23 (37%) studies used hierarchical clustering with Ward linkage, and 22 (35%) studies used k-means clustering. Of these 45 studies, 39 had mixed-type features, but only 5 specified dissimilarity measures that could handle mixed-type features. A further 9 (14%) studies used a preclustering step to create small clusters to feed on a hierarchical method. The original sample sizes in these 9 studies ranged from 84 to 349. The remaining studies used hierarchical clustering with other linkages (n=3), medoid-based methods (n=3), spectral clustering (n=1), and multiple kernel k-means clustering (n=1), and in 1 study, the methods were unclear. Of 63 studies, 54 (86%) explained the methods used to determine the number of clusters, 24 (38%) studies tested the quality of their cluster solution, and 11 (17%) studies tested the stability of their solution. Reporting of the cluster analysis was generally poor in terms of the methods employed and their justification.              Conclusions:                    This review highlights common issues in the application of cluster analysis to multimodal clinical data to identify asthma subtypes. Some of these issues were related to the multimodal nature of the data, but many were more general issues in the application of cluster analysis. Although cluster analysis may be a useful tool for investigating disease subtypes, we recommend that future studies carefully consider the implications of clustering multimodal data, the cluster analysis process itself, and the reporting of methods to facilitate replication and interpretation of findings.",1.0
32462093,Current challenges and possible future developments in personalized psychiatry with an emphasis on psychotic disorders,2020 May 20;6(5):e03990.,"A personalized medicine approach seems to be particularly applicable to psychiatry. Indeed, considering mental illness as deregulation, unique to each patient, of molecular pathways, governing the development and functioning of the brain, seems to be the most justified way to understand and treat disorders of this medical category. In order to extract correct information about the implicated molecular pathways, data can be drawn from sampling phenotypic and genetic biomarkers and then analyzed by a machine learning algorithm. This review describes current difficulties in the field of personalized psychiatry and gives several examples of possibly actionable biomarkers of psychotic and other psychiatric disorders, including several examples of genetic studies relevant to personalized psychiatry. Most of these biomarkers are not yet ready to be introduced in clinical practice. In a next step, a perspective on the path personalized psychiatry may take in the future is given, paying particular attention to machine learning algorithms that can be used with the goal of handling multidimensional datasets.",
32457844,Extracellular Vesicles in Renal Cell Carcinoma: Multifaceted Roles and Potential Applications Identified by Experimental and Computational Methods,2020 May 7;10:724.,"Renal cell carcinoma (RCC) is the most common type of kidney cancer. Increasingly evidences indicate that extracellular vesicles (EVs) orchestrate multiple processes in tumorigenesis, metastasis, immune evasion, and drug response of RCC. EVs are lipid membrane-bound vesicles in nanometer size and secreted by almost all cell types into the extracellular milieu. A myriad of bioactive molecules such as RNA, DNA, protein, and lipid are able to be delivered via EVs for the intercellular communication. Hence, the abundant content of EVs is appealing reservoir for biomarker identification through computational analysis and experimental validation. EVs with excellent biocompatibility and biodistribution are natural platforms that can be engineered to offer achievable drug delivery strategies for RCC therapies. Moreover, the multifaceted roles of EVs in RCC progression also provide substantial targets and facilitate EVs-based drug discovery, which will be accelerated by using artificial intelligence approaches. In this review, we summarized the vital roles of EVs in occurrence, metastasis, immune evasion, and drug resistance of RCC. Furthermore, we also recapitulated and prospected the EVs-based potential applications in RCC, including biomarker identification, drug vehicle development as well as drug target discovery.",2.0
32457178,Monitoring Big Data During Mechanical Ventilation in the ICU,2020 Jun;65(6):894-910.,"The electronic health record allows the assimilation of large amounts of clinical and laboratory data. Big data describes the analysis of large data sets using computational modeling to reveal patterns, trends, and associations. How can big data be used to predict ventilator discontinuation or impending compromise, and how can it be incorporated into the clinical workflow? This article will serve 2 purposes. First, a general overview is provided for the layperson and introduces key concepts, definitions, best practices, and things to watch out for when reading a paper that incorporates machine learning. Second, recent publications at the intersection of big data, machine learning, and mechanical ventilation are presented.",
32452701,An up-to-date overview of computational polypharmacology in modern drug discovery,2020 Sep;15(9):1025-1044.,"Introduction:                    In recent years, computational polypharmacology has gained significant attention to study the promiscuous nature of drugs. Despite tremendous challenges, community-wide efforts have led to a variety of novel approaches for predicting drug polypharmacology. In particular, some rapid advances using machine learning and artificial intelligence have been reported with great success.              Areas covered:                    In this article, the authors provide a comprehensive update on the current state-of-the-art polypharmacology approaches and their applications, focusing on those reports published after our 2017 review article. The authors particularly discuss some novel, groundbreaking concepts, and methods that have been developed recently and applied to drug polypharmacology studies.              Expert opinion:                    Polypharmacology is evolving and novel concepts are being introduced to counter the current challenges in the field. However, major hurdles remain including incompleteness of high-quality experimental data, lack of in vitro and in vivo assays to characterize multi-targeting agents, shortage of robust computational methods, and challenges to identify the best target combinations and design effective multi-targeting agents. Fortunately, numerous national/international efforts including multi-omics and artificial intelligence initiatives as well as most recent collaborations on addressing the COVID-19 pandemic have shown significant promise to propel the field of polypharmacology forward.",1.0
32451639,A review of epileptic seizure detection using machine learning classifiers,2020 May 25;7(1):5.,"Epilepsy is a serious chronic neurological disorder, can be detected by analyzing the brain signals produced by brain neurons. Neurons are connected to each other in a complex way to communicate with human organs and generate signals. The monitoring of these brain signals is commonly done using Electroencephalogram (EEG) and Electrocorticography (ECoG) media. These signals are complex, noisy, non-linear, non-stationary and produce a high volume of data. Hence, the detection of seizures and discovery of the brain-related knowledge is a challenging task. Machine learning classifiers are able to classify EEG data and detect seizures along with revealing relevant sensible patterns without compromising performance. As such, various researchers have developed number of approaches to seizure detection using machine learning classifiers and statistical features. The main challenges are selecting appropriate classifiers and features. The aim of this paper is to present an overview of the wide varieties of these techniques over the last few years based on the taxonomy of statistical features and machine learning classifiers-'black-box' and 'non-black-box'. The presented state-of-the-art methods and ideas will give a detailed understanding about seizure detection and classification, and research directions in the future.",8.0
32449232,"The use of artificial intelligence, machine learning and deep learning in oncologic histopathology",2020 Oct;49(9):849-856.,"Background:                    Recently, there has been a momentous drive to apply advanced artificial intelligence (AI) technologies to diagnostic medicine. The introduction of AI has provided vast new opportunities to improve health care and has introduced a new wave of heightened precision in oncologic pathology. The impact of AI on oncologic pathology has now become apparent, and its use with respect to oral oncology is still in the nascent stage.              Discussion:                    A foundational overview of AI classification systems used in medicine and a review of common terminology used in machine learning and computational pathology will be presented. This paper provides a focused review on the recent advances in AI and deep learning in oncologic histopathology and oral oncology. In addition, specific emphasis on recent studies that have applied these technologies to oral cancer prognostication will also be discussed.              Conclusion:                    Machine and deep learning methods designed to enhance prognostication of oral cancer have been proposed with much of the work focused on prediction models on patient survival and locoregional recurrences in patients with oral squamous cell carcinomas (OSCC). Few studies have explored machine learning methods on OSCC digital histopathologic images. It is evident that further research at the whole slide image level is needed and future collaborations with computer scientists may progress the field of oral oncology.",
32446113,The emerging roles of artificial intelligence in cancer drug development and precision therapy,2020 Aug;128:110255.,"Artificial intelligence (AI) has strong logical reasoning ability and independent learning ability, which can simulate the thinking process of the human brain. AI technologies such as machine learning can profoundly optimize the existing mode of anticancer drug research. But at present AI also has its relative limitation. In this paper, the development of artificial intelligence technology such as deep learning and machine learning in anticancer drug research is reviewed. At the same time, we look forward to the future of AI.",4.0
32439080,Risk Stratification Strategies for Colorectal Cancer Screening: From Logistic Regression to Artificial Intelligence,2020 Jul;30(3):423-440.,"Risk stratification is a system by which clinically meaningful separation of risk is achieved in a group of otherwise similar persons. Although parametric logistic regression dominates risk prediction, use of nonparametric and semiparametric methods, including artificial neural networks, is increasing. These statistical-learning and machine-learning methods, along with simple rules, are collectively referred to as ""artificial intelligence"" (AI). AI requires knowledge of study validity, understanding of model metrics, and determination of whether and to what extent the model can and should be applied to the patient or population under consideration. Further investigation is needed, especially in model validation and impact assessment.",
32437828,Predicting treatment effects in unipolar depression: A meta-review,2020 Aug;212:107557.,"There is increasing interest in clinical prediction models in psychiatry, which focus on developing multivariate algorithms to guide personalized diagnostic or management decisions. The main target of these models is the prediction of treatment response to different antidepressant therapies. This is because the ability to predict response based on patients' personal data may allow clinicians to make improved treatment decisions, and to provide more efficacious or more tolerable medications to the right patient. We searched the literature for systematic reviews about treatment prediction in the context of existing treatment modalities for adult unipolar depression, until July 2019. Treatment effect is defined broadly to include efficacy, safety, tolerability and acceptability outcomes. We first focused on the identification of individual predictor variables that might predict treatment response, and second, we considered multivariate clinical prediction models. Our meta-review included a total of 10 systematic reviews; seven (from 2014 to 2018) focusing on individual predictor variables and three focusing on clinical prediction models. These identified a number of sociodemographic, phenomenological, clinical, neuroimaging, remote monitoring, genetic and serum marker variables as possible predictor variables for treatment response, alongside statistical and machine-learning approaches to clinical prediction model development. Effect sizes for individual predictor variables were generally small and clinical prediction models had generally not been validated in external populations. There is a need for rigorous model validation in large external data-sets to prove the clinical utility of models. We also discuss potential future avenues in the field of personalized psychiatry, particularly the combination of multiple sources of data and the emerging field of artificial intelligence and digital mental health to identify new individual predictor variables.",4.0
32437744,Artificial intelligence and neuropsychological measures: The case of Alzheimer's disease,2020 Jul;114:211-228.,"One of the current challenges in the field of Alzheimer's disease (AD) is to identify patients with mild cognitive impairment (MCI) that will convert to AD. Artificial intelligence, in particular machine learning (ML), has established as one of more powerful approach to extract reliable predictors and to automatically classify different AD phenotypes. It is time to accelerate the translation of this knowledge in clinical practice, mainly by using low-cost features originating from the neuropsychological assessment. We performed a meta-analysis to assess the contribution of ML and neuropsychological measures for the automated classification of MCI patients and the prediction of their conversion to AD. The pooled sensitivity and specificity of patients' classifications was obtained by means of a quantitative bivariate random-effect meta-analytic approach. Although a high heterogeneity was observed, the results of meta-analysis show that ML applied to neuropsychological measures can lead to a successful automatic classification, being more specific as screening rather than prognosis tool. Relevant categories of neuropsychological tests can be extracted by ML that maximize the classification accuracy.",
32435959,Viewpoint on Time Series and Interrupted Time Series Optimum Modeling for Predicting Arthritic Disease Outcomes,2020 May 20;22(7):27.,"Purpose of review:                    The propose of this viewpoint is to improve or facilitate the clinical decision-making in the management/treatment strategies of arthritis patients through knowing, understanding, and having access to an interactive process allowing assessment of the patient disease outcome in the future.              Recent findings:                    In recent years, the time series (TS) concept has become the center of attention as a predictive model for making forecast of unseen data values. TS and one of its technologies, the interrupted TS (ITS) analysis (TS with one or more interventions), predict the next period(s) value(s) of a given patient based on their past and current information. Traditional TS/ITS methods involve segmented regression-based technologies (linear and nonlinear), while stochastic (linear modeling) and artificial intelligence approaches, including machine learning (complex nonlinear relationships between variables), are also used; however, each have limitations. We will briefly describe TS/ITS, provide examples of their application in arthritic diseases; describe their methods, challenges, and limitations; and propose a combined (stochastic and artificial intelligence) procedure in post-intervention that will optimize ITS modeling. This combined method will increase the accuracy of ITS modeling by profiting from the advantages of both stochastic and nonlinear models to capture all ITS deterministic and stochastic components. In addition, this combined method will allow ITS outcomes to be predicted as continuous variables without having to consider the time lag produced between the pre- and post-intervention periods, thus minimizing the prediction error not only for the given data but also for all possible future patterns in ITS. The use of reliable prediction methodologies for arthritis patients will permit treatment of not only the disease, but also the patient with the disease, ensuring the best outcome prediction for the patient.",1.0
32434436,Oral microbiome-systemic link studies: perspectives on current limitations and future artificial intelligence-based approaches,2020 May;46(3):288-299.,"In the past decade, there has been a tremendous increase in studies on the link between oral microbiome and systemic diseases. However, variations in study design and confounding variables across studies often lead to inconsistent observations. In this narrative review, we have discussed the potential influence of study design and confounding variables on the current sequencing-based oral microbiome-systemic disease link studies. The current limitations of oral microbiome-systemic link studies on type 2 diabetes mellitus, rheumatoid arthritis, pregnancy, atherosclerosis, and pancreatic cancer are discussed in this review, followed by our perspective on how artificial intelligence (AI), particularly machine learning and deep learning approaches, can be employed for predicting systemic disease and host metadata from the oral microbiome. The application of AI for predicting systemic disease as well as host metadata requires the establishment of a global database repository with microbiome sequences and annotated host metadata. However, this task requires collective efforts from researchers working in the field of oral microbiome to establish more comprehensive datasets with appropriate host metadata. Development of AI-based models by incorporating consistent host metadata will allow prediction of systemic diseases with higher accuracies, bringing considerable clinical benefits.",
32434014,Evolution of Minimally Invasive Lumbar Spine Surgery,2020 Aug;140:622-626.,"Spine surgery has evolved over centuries from first being practiced with Hippocratic boards and ladders to now being able to treat spinal pathologies with minimal tissue invasion. With the advent of new imaging and surgical technologies, spine surgeries can now be performed minimally invasively with smaller incisions, less blood loss, quicker return to daily activities, and increased visualization. Modern minimally invasive procedures include percutaneous pedicle screw fixation techniques and minimally invasive lateral approach for lumbar interbody fusion (i.e., minimally invasive transforaminal lumbar interbody fusion, extreme lateral interbody fusion, oblique lateral interbody fusion) and midline lumbar fusion with cortical bone trajectory screws. Just as evolutions in surgical techniques have helped revolutionize the field of spine surgery, imaging technologies have also contributed significantly. The advent of computer image guidance has allowed spine surgeons to advance their ability to refine surgical techniques, increase the accuracy of spinal hardware placement, and reduce radiation exposure to the operating room staff. As the field of spine surgery looks to the future, many novel technologies are on the horizon, including robotic spine surgery, artificial intelligence, and machine learning to help improve preoperative planning, improve surgical execution, and optimize patient selection to ensure improved postoperative outcomes and patient satisfaction. As more spine surgeons begin incorporating these novel minimally invasive techniques into practice, the field of minimally invasive spine surgery will continue to innovate and evolve over the coming years.",
32434009,Deep Learning for Dermatologists: Part I Fundamental Concepts,2020 May 17;S0190-9622(20)30921-X.,"Artificial intelligence (AI) is generating substantial interest in the field of medicine. One form of artificial intelligence, deep learning, has led to rapid advances in automated image analysis. In 2017, an algorithm demonstrated the ability to diagnose certain skin cancers from clinical photographs with the accuracy of an expert dermatologist. Subsequently, deep learning has been applied to a range of dermatology applications. Though experts will never be replaced by AI, it will certainly impact the specialty of dermatology. In this first article of a two-part series, the basic concepts of deep learning will be reviewed with the goal of laying the groundwork for effective communication between clinicians and technical colleagues. In part two of the series, the clinical applications of deep learning in dermatology will be reviewed considering limitations and opportunities.",1.0
32429394,Application and Algorithm of Ground-Penetrating Radar for Plant Root Detection: A Review,2020 May 16;20(10):2836.,"Attention to the natural environment is equivalent to observing the space in which we live. Plant roots, which are important organs of plants, require our close attention. The method of detecting root system without damaging plants has gradually become mainstream. At the same time, machine learning has been achieving good results in recent years; it has helped develop many tools to help us detect the underground environment of plants. Therefore, this article will introduce some existing content related to root detection technology and machine detection algorithms for root detection, proving that machine learning root detection technology has good recognition capabilities.",
32429287,Metabolomics and Multi-Omics Integration: A Survey of Computational Methods and Resources,2020 May 15;10(5):202.,"As researchers are increasingly able to collect data on a large scale from multiple clinical and omics modalities, multi-omics integration is becoming a critical component of metabolomics research. This introduces a need for increased understanding by the metabolomics researcher of computational and statistical analysis methods relevant to multi-omics studies. In this review, we discuss common types of analyses performed in multi-omics studies and the computational and statistical methods that can be used for each type of analysis. We pinpoint the caveats and considerations for analysis methods, including required parameters, sample size and data distribution requirements, sources of a priori knowledge, and techniques for the evaluation of model accuracy. Finally, for the types of analyses discussed, we provide examples of the applications of corresponding methods to clinical and basic research. We intend that our review may be used as a guide for metabolomics researchers to choose effective techniques for multi-omics analyses relevant to their field of study.",6.0
32428608,Deep learning for dermatologists: Part II. Current applications,2020 May 16;S0190-9622(20)30918-X.,"Because of a convergence of the availability of large data sets, graphics-specific computer hardware, and important theoretical advancements, artificial intelligence has recently contributed to dramatic progress in medicine. One type of artificial intelligence known as deep learning has been particularly impactful for medical image analysis. Deep learning applications have shown promising results in dermatology and other specialties, including radiology, cardiology, and ophthalmology. The modern clinician will benefit from an understanding of the basic features of deep learning to effectively use new applications and to better gauge their utility and limitations. In this second article of a 2-part series, we review the existing and emerging clinical applications of deep learning in dermatology and discuss future opportunities and limitations. Part 1 of this series offered an introduction to the basic concepts of deep learning to facilitate effective communication between clinicians and technical experts.",3.0
32425581,Challenges and Future Prospects of Precision Medicine in Psychiatry,2020 Apr 23;13:127-140.,"Precision medicine is increasingly recognized as a promising approach to improve disease treatment, taking into consideration the individual clinical and biological characteristics shared by specific subgroups of patients. In specific fields such as oncology and hematology, precision medicine has already started to be implemented in the clinical setting and molecular testing is routinely used to select treatments with higher efficacy and reduced adverse effects. The application of precision medicine in psychiatry is still in its early phases. However, there are already examples of predictive models based on clinical data or combinations of clinical, neuroimaging and biological data. While the power of single clinical predictors would remain inadequate if analyzed only with traditional statistical approaches, these predictors are now increasingly used to impute machine learning models that can have adequate accuracy even in the presence of relatively small sample size. These models have started to be applied to disentangle relevant clinical questions that could lead to a more effective management of psychiatric disorders, such as prediction of response to the mood stabilizer lithium, resistance to antidepressants in major depressive disorder or stratification of the risk and outcome prediction in schizophrenia. In this narrative review, we summarized the most important findings in precision medicine in psychiatry based on studies that constructed machine learning models using clinical, neuroimaging and/or biological data. Limitations and barriers to the implementation of precision psychiatry in the clinical setting, as well as possible solutions and future perspectives, will be presented.",3.0
32424281,Integrated multi-omics approaches to improve classification of chronic kidney disease,2020 Nov;16(11):657-668.,"Chronic kidney diseases (CKDs) are currently classified according to their clinical features, associated comorbidities and pattern of injury on biopsy. Even within a given classification, considerable variation exists in disease presentation, progression and response to therapy, highlighting heterogeneity in the underlying biological mechanisms. As a result, patients and clinicians experience uncertainty when considering optimal treatment approaches and risk projection. Technological advances now enable large-scale datasets, including DNA and RNA sequence data, proteomics and metabolomics data, to be captured from individuals and groups of patients along the genotype-phenotype continuum of CKD. The ability to combine these high-dimensional datasets, in which the number of variables exceeds the number of clinical outcome observations, using computational approaches such as machine learning, provides an opportunity to re-classify patients into molecularly defined subgroups that better reflect underlying disease mechanisms. Patients with CKD are uniquely poised to benefit from these integrative, multi-omics approaches since the kidney biopsy, blood and urine samples used to generate these different types of molecular data are frequently obtained during routine clinical care. The ultimate goal of developing an integrated molecular classification is to improve diagnostic classification, risk stratification and assignment of molecular, disease-specific therapies to improve the care of patients with CKD.",5.0
32424119,AI for social good: unlocking the opportunity for positive impact,2020 May 18;11(1):2468.,"Advances in machine learning (ML) and artificial intelligence (AI) present an opportunity to build better tools and solutions to help address some of the world's most pressing challenges, and deliver positive social impact in accordance with the priorities outlined in the United Nations' 17 Sustainable Development Goals (SDGs). The AI for Social Good (AI4SG) movement aims to establish interdisciplinary partnerships centred around AI applications towards SDGs. We provide a set of guidelines for establishing successful long-term collaborations between AI researchers and application-domain experts, relate them to existing AI4SG projects and identify key opportunities for future AI applications targeted towards social good.",
32420327,A Survey on Recent Advances in Wearable Fall Detection Systems,2020 Jan 13;2020:2167160.,"With advances in medicine and healthcare systems, the average life expectancy of human beings has increased to more than 80 yrs. As a result, the demographic old-age dependency ratio (people aged 65 or above relative to those aged 15-64) is expected to increase, by 2060, from 28% to 50% in the European Union and from 33% to 45% in Asia (Ageing Report European Economy, 2015). Therefore, the percentage of people who need additional care is also expected to increase. For instance, per studies conducted by the National Program for Health Care of the Elderly (NPHCE), elderly population in India will increase to 12% of the national population by 2025 with 8%-10% requiring utmost care. Geriatric healthcare has gained a lot of prominence in recent years, with specific focus on fall detection systems (FDSs) because of their impact on public lives. According to a World Health Organization report, the frequency of falls increases with increase in age and frailty. Older people living in nursing homes fall more often than those living in the community and 40% of them experience recurrent falls (World Health Organization, 2007). Machine learning (ML) has found its application in geriatric healthcare systems, especially in FDSs. In this paper, we examine the requirements of a typical FDS. Then we present a survey of the recent work in the area of fall detection systems, with focus on the application of machine learning. We also analyze the challenges in FDS systems based on the literature survey.",1.0
32418724,Emergence of New Disease: How Can Artificial Intelligence Help?,2020 Jul;26(7):627-629.,"Emergence of new disease remains a critical parameter in human health and society. Advances in artificial intelligence (AI) allow for rapid processing and analysis of massive and complex data. In this forum article, the recent applications across disease prediction and drug development in relation to the COVID-19 pandemic are reviewed.",4.0
32418341,"Artificial intelligence-based clinical decision support in modern medical physics: Selection, acceptance, commissioning, and quality assurance",2020 Jun;47(5):e228-e235.,"Background:                    Recent advances in machine and deep learning based on an increased availability of clinical data have fueled renewed interest in computerized clinical decision support systems (CDSSs). CDSSs have shown great potential to improve healthcare, increase patient safety and reduce costs. However, the use of CDSSs is not without pitfalls, as an inadequate or faulty CDSS can potentially deteriorate the quality of healthcare and put patients at risk. In addition, the adoption of a CDSS might fail because its intended users ignore the output of the CDSS due to lack of trust, relevancy or actionability.              Aim:                    In this article, we provide guidance based on literature for the different aspects involved in the adoption of a CDSS with a special focus on machine and deep learning based systems: selection, acceptance testing, commissioning, implementation and quality assurance.              Results:                    A rigorous selection process will help identify the CDSS that best fits the preferences and requirements of the local site. Acceptance testing will make sure that the selected CDSS fulfills the defined specifications and satisfies the safety requirements. The commissioning process will prepare the CDSS for safe clinical use at the local site. An effective implementation phase should result in an orderly roll out of the CDSS to the well-trained end-users whose expectations have been managed. And finally, quality assurance will make sure that the performance of the CDSS is maintained and that any issues are promptly identified and solved.              Conclusion:                    We conclude that a systematic approach to the adoption of a CDSS will help avoid pitfalls, improve patient safety and increase the chances of success.",1.0
32418340,Computer-aided diagnosis in the era of deep learning,2020 Jun;47(5):e218-e227.,"Computer-aided diagnosis (CAD) has been a major field of research for the past few decades. CAD uses machine learning methods to analyze imaging and/or nonimaging patient data and makes assessment of the patient's condition, which can then be used to assist clinicians in their decision-making process. The recent success of the deep learning technology in machine learning spurs new research and development efforts to improve CAD performance and to develop CAD for many other complex clinical tasks. In this paper, we discuss the potential and challenges in developing CAD tools using deep learning technology or artificial intelligence (AI) in general, the pitfalls and lessons learned from CAD in screening mammography and considerations needed for future implementation of CAD or AI in clinical use. It is hoped that the past experiences and the deep learning technology will lead to successful advancement and lasting growth in this new era of CAD, thereby enabling CAD to deliver intelligent aids to improve health care.",3.0
32418338,Machine learning for radiation outcome modeling and prediction,2020 Jun;47(5):e178-e184.,"Aims:                    This review paper intends to summarize the application of machine learning to radiotherapy outcome modeling based on structured and un-structured radiation oncology datasets.              Materials and methods:                    The most appropriate machine learning approaches for structured datasets in terms of accuracy and interpretability are identified. For un-structured datasets, deep learning algorithms are explored and a critical view of the use of these approaches in radiation oncology is also provided.              Conclusions:                    We discuss the challenges in radiotherapy outcome prediction, and suggest to improve radiation outcome modeling by developing appropriate machine learning approaches where both accuracy and interpretability are taken into account.",1.0
32418337,Machine learning techniques for biomedical image segmentation: An overview of technical aspects and introduction to state-of-art applications,2020 Jun;47(5):e148-e167.,"In recent years, significant progress has been made in developing more accurate and efficient machine learning algorithms for segmentation of medical and natural images. In this review article, we highlight the imperative role of machine learning algorithms in enabling efficient and accurate segmentation in the field of medical imaging. We specifically focus on several key studies pertaining to the application of machine learning methods to biomedical image segmentation. We review classical machine learning algorithms such as Markov random fields, k-means clustering, random forest, etc. Although such classical learning models are often less accurate compared to the deep-learning techniques, they are often more sample efficient and have a less complex structure. We also review different deep-learning architectures, such as the artificial neural networks (ANNs), the convolutional neural networks (CNNs), and the recurrent neural networks (RNNs), and present the segmentation results attained by those learning models that were published in the past 3 yr. We highlight the successes and limitations of each machine learning paradigm. In addition, we discuss several challenges related to the training of different machine learning models, and we present some heuristics to address those challenges.",3.0
32418336,Machine and deep learning methods for radiomics,2020 Jun;47(5):e185-e202.,"Radiomics is an emerging area in quantitative image analysis that aims to relate large-scale extracted imaging information to clinical and biological endpoints. The development of quantitative imaging methods along with machine learning has enabled the opportunity to move data science research towards translation for more personalized cancer treatments. Accumulating evidence has indeed demonstrated that noninvasive advanced imaging analytics, that is, radiomics, can reveal key components of tumor phenotype for multiple three-dimensional lesions at multiple time points over and beyond the course of treatment. These developments in the use of CT, PET, US, and MR imaging could augment patient stratification and prognostication buttressing emerging targeted therapeutic approaches. In recent years, deep learning architectures have demonstrated their tremendous potential for image segmentation, reconstruction, recognition, and classification. Many powerful open-source and commercial platforms are currently available to embark in new research areas of radiomics. Quantitative imaging research, however, is complex and key statistical principles should be followed to realize its full potential. The field of radiomics, in particular, requires a renewed focus on optimal study design/reporting practices and standardization of image acquisition, feature calculation, and rigorous statistical analysis for the field to move forward. In this article, the role of machine and deep learning as a major computational vehicle for advanced model building of radiomics-based signatures or classifiers, and diverse clinical applications, working principles, research opportunities, and available computational platforms for radiomics will be reviewed with examples drawn primarily from oncology. We also address issues related to common applications in medical physics, such as standardization, feature extraction, model building, and validation.",10.0
32418335,Genomics models in radiotherapy: From mechanistic to machine learning,2020 Jun;47(5):e203-e217.,"Machine learning (ML) provides a broad framework for addressing high-dimensional prediction problems in classification and regression. While ML is often applied for imaging problems in medical physics, there are many efforts to apply these principles to biological data toward questions of radiation biology. Here, we provide a review of radiogenomics modeling frameworks and efforts toward genomically guided radiotherapy. We first discuss medical oncology efforts to develop precision biomarkers. We next discuss similar efforts to create clinical assays for normal tissue or tumor radiosensitivity. We then discuss modeling frameworks for radiosensitivity and the evolution of ML to create predictive models for radiogenomics.",4.0
32417653,Synthetic macromolecules as therapeutics that overcome resistance in cancer and microbial infection,2020 Sep;252:120078.,"Synthetic macromolecular antimicrobials have shown efficacy in the treatment of multidrug resistant (MDR) pathogens. These synthetic macromolecules, inspired by Nature's antimicrobial peptides (AMPs), mitigate resistance by disrupting microbial cell membrane or targeting multiple intracellular proteins or genes. Unlike AMPs, these polymers are less prone to degradation by proteases and are easier to synthesize on a large scale. Recently, various studies have revealed that cancer cell membrane, like that of microbes, is negatively charged, and AMPs can be used as anticancer agents. Nevertheless, efforts in developing polymers as anticancer agents has remained limited. This review highlights the recent advancement in the development of synthetic biodegradable antimicrobial polymers (e.g. polycarbonates, polyesters and polypeptides) and anticancer macromolecules including peptides and polymers. Additionally, strategies to improve their in vivo bioavailability and selectivity towards bacteria and cancer cells are examined. Lastly, future perspectives, including use of artificial intelligence or machine learning, in the development of antimicrobial and anticancer macromolecules are discussed.",3.0
