pmid,citations,title,date,text
31970529,3.0,"Quantifying the Collision Dose in Rugby League: A Systematic Review, Meta-analysis, and Critical Analysis",2020 Jan 22;6(1):6.,"Background:                    Collisions (i.e. tackles, ball carries, and collisions) in the rugby league have the potential to increase injury risk, delay recovery, and influence individual and team performance. Understanding the collision demands of the rugby league may enable practitioners to optimise player health, recovery, and performance.              Objective:                    The aim of this review was to (1) characterise the dose of collisions experienced within senior male rugby league match-play and training, (2) systematically and critically evaluate the methods used to describe the relative and absolute frequency and intensity of collisions, and (3) provide recommendations on collision monitoring.              Methods:                    A systematic search of electronic databases (PubMed, SPORTDiscus, Scopus, and Web of Science) using keywords was undertaken. A meta-analysis provided a pooled mean of collision frequency or intensity metrics on comparable data sets from at least two studies.              Results:                    Forty-three articles addressing the absolute (n) or relative collision frequency (n min-1) or intensity of senior male rugby league collisions were included. Meta-analysis of video-based studies identified that forwards completed approximately twice the number of tackles per game than backs (n = 24.6 vs 12.8), whilst ball carry frequency remained similar between backs and forwards (n = 11.4 vs 11.2). Variable findings were observed at the subgroup level with a limited number of studies suggesting wide-running forwards, outside backs, and hit-up forwards complete similar ball carries whilst tackling frequency differed. For microtechnology, at the team level, players complete an average of 32.7 collisions per match. Limited data suggested hit-up and wide-running forwards complete the most collisions per match, when compared to adjustables and outside backs. Relative to playing time, forwards (n min-1 = 0.44) complete a far greater frequency of collision than backs (n min-1 = 0.16), with data suggesting hit-up forwards undertake more than adjustables, and outside backs. Studies investigating g force intensity zones utilised five unique intensity schemes with zones ranging from 2-3 g to 13-16 g. Given the disparity between device setups and zone classification systems between studies, further analyses were inappropriate. It is recommended that practitioners independently validate microtechnology against video to establish criterion validity.              Conclusions:                    Video- and microtechnology-based methods have been utilised to quantify collisions in the rugby league with differential collision profiles observed between forward and back positional groups, and their distinct subgroups. The ball carry demands of forwards and backs were similar, whilst tackle demands were greater for forwards than backs. Microtechnology has been used inconsistently to quantify collision frequency and intensity. Despite widespread popularity, a number of the microtechnology devices have yet to be appropriately validated. Limitations exist in using microtechnology to quantify collision intensity, including the lack of consistency and limited validation. Future directions include application of machine learning approaches to differentiate types of collisions in microtechnology datasets."
31968694,1.0,Synergistic Approach of Ultrafast Spectroscopy and Molecular Simulations in the Characterization of Intramolecular Charge Transfer in Push-Pull Molecules,2020 Jan 20;25(2):430.,"The comprehensive characterization of Intramolecular Charge Transfer (ICT) stemming in push-pull molecules with a delocalized Ï€-system of electrons is noteworthy for a bespoke design of organic materials, spanning widespread applications from photovoltaics to nanomedicine imaging devices. Photo-induced ICT is characterized by structural reorganizations, which allows the molecule to adapt to the new electronic density distribution. Herein, we discuss recent photophysical advances combined with recent progresses in the computational chemistry of photoactive molecular ensembles. We focus the discussion on femtosecond Transient Absorption Spectroscopy (TAS) enabling us to follow the transition from a Locally Excited (LE) state to the ICT and to understand how the environment polarity influences radiative and non-radiative decay mechanisms. In many cases, the charge transfer transition is accompanied by structural rearrangements, such as the twisting or molecule planarization. The possibility of an accurate prediction of the charge-transfer occurring in complex molecules and molecular materials represents an enormous advantage in guiding new molecular and materials design. We briefly report on recent advances in ultrafast multidimensional spectroscopy, in particular, Two-Dimensional Electronic Spectroscopy (2DES), in unraveling the ICT nature of push-pull molecular systems. A theoretical description at the atomistic level of photo-induced molecular transitions can predict with reasonable accuracy the properties of photoactive molecules. In this framework, the review includes a discussion on the advances from simulation and modeling, which have provided, over the years, significant information on photoexcitation, emission, charge-transport, and decay pathways. Density Functional Theory (DFT) coupled with the Time-Dependent (TD) framework can describe electronic properties and dynamics for a limited system size. More recently, Machine Learning (ML) or deep learning approaches, as well as free-energy simulations containing excited state potentials, can speed up the calculations with transferable accuracy to more complex molecules with extended system size. A perspective on combining ultrafast spectroscopy with molecular simulations is foreseen for optimizing the design of photoactive compounds with tunable properties."
31965266,24.0,Machine learning for the prediction of sepsis: a systematic review and meta-analysis of diagnostic test accuracy,2020 Mar;46(3):383-400.,"Purpose:                    Early clinical recognition of sepsis can be challenging. With the advancement of machine learning, promising real-time models to predict sepsis have emerged. We assessed their performance by carrying out a systematic review and meta-analysis.              Methods:                    A systematic search was performed in PubMed, Embase.com and Scopus. Studies targeting sepsis, severe sepsis or septic shock in any hospital setting were eligible for inclusion. The index test was any supervised machine learning model for real-time prediction of these conditions. Quality of evidence was assessed using the Grading of Recommendations Assessment, Development and Evaluation (GRADE) methodology, with a tailored Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) checklist to evaluate risk of bias. Models with a reported area under the curve of the receiver operating characteristic (AUROC) metric were meta-analyzed to identify strongest contributors to model performance.              Results:                    After screening, a total of 28 papers were eligible for synthesis, from which 130 models were extracted. The majority of papers were developed in the intensive care unit (ICU, n = 15; 54%), followed by hospital wards (n = 7; 25%), the emergency department (ED, n = 4; 14%) and all of these settings (n = 2; 7%). For the prediction of sepsis, diagnostic test accuracy assessed by the AUROC ranged from 0.68-0.99 in the ICU, to 0.96-0.98 in-hospital and 0.87 to 0.97 in the ED. Varying sepsis definitions limit pooling of the performance across studies. Only three papers clinically implemented models with mixed results. In the multivariate analysis, temperature, lab values, and model type contributed most to model performance.              Conclusion:                    This systematic review and meta-analysis show that on retrospective data, individual machine learning models can accurately predict sepsis onset ahead of time. Although they present alternatives to traditional scoring systems, between-study heterogeneity limits the assessment of pooled results. Systematic reporting and clinical implementation studies are needed to bridge the gap between bytes and bedside."
31963480,2.0,Can Artificial Intelligence Improve the Management of Pneumonia,2020 Jan 17;9(1):248.,"The use of artificial intelligence (AI) to support clinical medical decisions is a rather promising concept. There are two important factors that have driven these advances: the availability of data from electronic health records (EHR) and progress made in computational performance. These two concepts are interrelated with respect to complex mathematical functions such as machine learning (ML) or neural networks (NN). Indeed, some published articles have already demonstrated the potential of these approaches in medicine. When considering the diagnosis and management of pneumonia, the use of AI and chest X-ray (CXR) images primarily have been indicative of early diagnosis, prompt antimicrobial therapy, and ultimately, better prognosis. Coupled with this is the growing research involving empirical therapy and mortality prediction, too. Maximizing the power of NN, the majority of studies have reported high accuracy rates in their predictions. As AI can handle large amounts of data and execute mathematical functions such as machine learning and neural networks, AI can be revolutionary in supporting the clinical decision-making processes. In this review, we describe and discuss the most relevant studies of AI in pneumonia."
31962244,2.0,Incorporating biological structure into machine learning models in biomedicine,2020 Jun;63:126-134.,"In biomedical applications of machine learning, relevant information often has a rich structure that is not easily encoded as real-valued predictors. Examples of such data include DNA or RNA sequences, gene sets or pathways, gene interaction or coexpression networks, ontologies, and phylogenetic trees. We highlight recent examples of machine learning models that use structure to constrain model architecture or incorporate structured data into model training. For machine learning in biomedicine, where sample size is limited and model interpretability is crucial, incorporating prior knowledge in the form of structured data can be particularly useful. The area of research would benefit from performant open source implementations and independent benchmarking efforts."
31960771,2.0,Carotid artery ultrasound image analysis: A review of the literature,2020 May;234(5):417-443.,"Stroke is one of the prominent causes of death in the recent days. The existence of susceptible plaque in the carotid artery can be used in ascertaining the possibilities of cardiovascular diseases and long-term disabilities. The imaging modality used for early screening of the disease is B-mode ultrasound image of the person in the artery area. The objective of this article is to give a widespread review of the imaging modes and methods used for studying the carotid artery for identifying stroke, atherosclerosis and related cardiovascular diseases. We encompass the review in methods used for artery wall tracking, intima-media, and lumen segmentation which will help in finding the extent of the disease. Due to the characteristics of the imaging modality used, the images have speckle noise which worsens the image quality. Adaptive homomorphic filtering with wavelet and contourlet transforms, Levy Shrink, gamma distribution were used for image denoising. Learning-based neural network approaches for denoising give better edge preservation. Domain knowledge-based segmentation approaches have proved to provide more accurate intima-media thickness measurements. There is a requirement of useful fully automatic segmentation approaches, 3D, 4D systems, and plaque motion analysis. Taking into consideration the image priors like geometry, imaging physics, intensity and temporal data, image analysis has to be performed. Encouragingly more research has focused on content-specific segmentation and classification techniques. With the evaluation of machine learning algorithms, classifying the image as with or without a fat deposit has gained better accuracy and sensitivity. Machine learning-based approaches like self-organizing map, k-nearest neighborhood and support vector machine achieve promising accuracy and sensitivity in classification. The literature reveals that there is more scope in identifying a patient-specific model in a fully automatic manner."
31960635,1.0,"Future Directions in Coronary CT Angiography: CT-Fractional Flow Reserve, Plaque Vulnerability, and Quantitative Plaque Assessment",2020 Mar;50(3):185-202.,"Coronary computed tomography angiography (CCTA) is a well-validated and noninvasive imaging modality for the assessment of coronary artery disease (CAD) in patients with stable ischemic heart disease and acute coronary syndromes (ACSs). CCTA not only delineates the anatomy of the heart and coronary arteries in detail, but also allows for intra- and extraluminal imaging of coronary arteries. Emerging technologies have promoted new CCTA applications, resulting in a comprehensive assessment of coronary plaques and their clinical significance. The application of computational fluid dynamics to CCTA resulted in a robust tool for noninvasive assessment of coronary blood flow hemodynamics and determination of hemodynamically significant stenosis. Detailed evaluation of plaque morphology and identification of high-risk plaque features by CCTA have been confirmed as predictors of future outcomes, identifying patients at risk for ACSs. With quantitative coronary plaque assessment, the progression of the CAD or the response to therapy could be monitored by CCTA. The aim of this article is to review the future directions of emerging applications in CCTA, such as computed tomography (CT)-fractional flow reserve, imaging of vulnerable plaque features, and quantitative plaque imaging. We will also briefly discuss novel methods appearing in the coronary imaging scenario, such as machine learning, radiomics, and spectral CT."
31960407,6.0,What is AI? Applications of artificial intelligence to dermatology,2020 Sep;183(3):423-430.,"In the past, the skills required to make an accurate dermatological diagnosis have required exposure to thousands of patients over many years. However, in recent years, artificial intelligence (AI) has made enormous advances, particularly in the area of image classification. This has led computer scientists to apply these techniques to develop algorithms that are able to recognize skin lesions, particularly melanoma. Since 2017, there have been numerous studies assessing the accuracy of algorithms, with some reporting that the accuracy matches or surpasses that of a dermatologist. While the principles underlying these methods are relatively straightforward, it can be challenging for the practising dermatologist to make sense of a plethora of unfamiliar terms in this domain. Here we explain the concepts of AI, machine learning, neural networks and deep learning, and explore the principles of how these tasks are accomplished. We critically evaluate the studies that have assessed the efficacy of these methods and discuss limitations and potential ethical issues. The burden of skin cancer is growing within the Western world, with major implications for both population skin health and the provision of dermatology services. AI has the potential to assist in the diagnosis of skin lesions and may have particular value at the interface between primary and secondary care. The emerging technology represents an exciting opportunity for dermatologists, who are the individuals best informed to explore the utility of this powerful novel diagnostic tool, and facilitate its safe and ethical implementation within healthcare systems."
31960282,3.0,Artificial Intelligence and Polyp Detection,2020 Jan 21;10.1007/s11938-020-00274-2.,"Purpose of review:                    This review highlights the history, recent advances, and ongoing challenges of artificial intelligence (AI) technology in colonic polyp detection.              Recent findings:                    Hand-crafted AI algorithms have recently given way to convolutional neural networks with the ability to detect polyps in real-time. The first randomized controlled trial comparing an AI system to standard colonoscopy found a 9% increase in adenoma detection rate, but the improvement was restricted to polyps smaller than 10 mm and the results need validation. As this field rapidly evolves, important issues to consider include standardization of outcomes, dataset availability, real-world applications, and regulatory approval. AI has shown great potential for improving colonic polyp detection while requiring minimal training for endoscopists. The question of when AI will enter endoscopic practice depends on whether the technology can be integrated into existing hardware and an assessment of its added value for patient care."
31958430,1.0,"Engineering Stability, Viscosity, and Immunogenicity of Antibodies by Computational Design",2020 May;109(5):1631-1651.,"In recent years, computational methods have garnered much attention in protein engineering. A large number of computational methods have been developed to analyze the sequences and structures of proteins and have been used to predict the various properties. Antibodies are one of the emergent protein therapeutics, and thus, methods to control their physicochemical properties are highly desirable. However, despite the tremendous efforts of past decades, computational methods to predict the physicochemical properties of antibodies are still in their infancy. Experimental validations are certainly required for real-world applications, and the results should be interpreted with caution. Among the various properties of antibodies, we focus in this review on stability, viscosity, and immunogenicity, and we present the current status of computational methods to engineer such properties."
31957003,5.0,Will Artificial Intelligence for Drug Discovery Impact Clinical Pharmacology?,2020 Apr;107(4):780-785.,"As the field of artificial intelligence and machine learning (AI/ML) for drug discovery is rapidly advancing, we address the question ""What is the impact of recent AI/ML trends in the area of Clinical Pharmacology?"" We address difficulties and AI/ML developments for target identification, their use in generative chemistry for small molecule drug discovery, and the potential role of AI/ML in clinical trial outcome evaluation. We briefly discuss current trends in the use of AI/ML in health care and the impact of AI/ML context of the daily practice of clinical pharmacologists."
31956754,2.0,"Machine Learning in Catalysis, From Proposal to Practicing",2019 Dec 24;5(1):83-88.,"Recently, machine learning (ML) methods have gained popularity and have performed as powerfully predictive tools in various areas of academic and industrious activities. In comparison, their application in catalysis has been underdeveloped. Relying on the rapid development of different algorithms and their implementation, it is the right timing to harvest the potential of ML in catalysis across academy and industry spectra. Herein, we discuss the current applications in the field of homogeneous and heterogeneous catalysis by using various ML approaches. To the best of our knowledge, modern statistical learning techniques will be a strong tool for computational optimization and discovery. This in turn will accurately extract the underlying mechanism in the model that converts readily available data and precatalysts into their promising and useful ones."
31955441,4.0,Invited Review: DNA methylation-based classification of paediatric brain tumours,2020 Feb;46(1):28-47.,"DNA methylation-based machine learning algorithms represent powerful diagnostic tools that are currently emerging for several fields of tumour classification. For various reasons, paediatric brain tumours have been the main driving forces behind this rapid development and brain tumour classification tools are likely further advanced than in any other field of cancer diagnostics. In this review, we will discuss the main characteristics that were important for this rapid advance, namely the high clinical need for improvement of paediatric brain tumour diagnostics, the robustness of methylated DNA and the consequential possibility to generate high-quality molecular data from archival formalin-fixed paraffin-embedded pathology specimens, the implementation of a single array platform by most laboratories allowing data exchange and data pooling to an unprecedented extent, as well as the high suitability of the data format for machine learning. We will further discuss the four most central output qualities of DNA methylation profiling in a diagnostic setting (tumour classification, tumour sub-classification, copy number analysis and guidance for additional molecular testing) individually for the most frequent types of paediatric brain tumours. Lastly, we will discuss DNA methylation profiling as a tool for the detection of new paediatric brain tumour classes and will give an overview of the rapidly growing family of new tumours identified with the aid of this technique."
31954954,1.0,Computational approaches for detection of cardiac rhythm abnormalities: Are we there yet?,Mar-Apr 2020;59:28-34.,"The analysis of an electrocardiogram (ECG) is able to provide vital information on the electrical activity of the heart and is crucial for the accurate diagnosis of cardiac arrhythmias. Due to the nature of some arrhythmias, this might be a time-consuming and difficult to accomplish process. The advent of novel machine learning technologies in this field has a potential to revolutionise the use of the ECG. In this review, we outline key advances in ECG analysis for atrial, ventricular and complex multiform arrhythmias, as well as discuss the current limitations of the technology and the barriers that must be overcome before clinical integration is feasible."
31954511,2.0,From Summary Statistics to Gene Trees: Methods for Inferring Positive Selection,2020 Apr;36(4):243-258.,"Methods to detect signals of natural selection from genomic data have traditionally emphasized the use of simple summary statistics. Here, we review a new generation of methods that consider combinations of conventional summary statistics and/or richer features derived from inferred gene trees and ancestral recombination graphs (ARGs). We also review recent advances in methods for population genetic simulation and ARG reconstruction. Finally, we describe opportunities for future work on a variety of related topics, including the genetics of speciation, estimation of selection coefficients, and inference of selection on polygenic traits. Together, these emerging methods offer promising new directions in the study of natural selection."
31952684,1.0,Prediction and targeting of GPCR oligomer interfaces,2020;169:105-149.,"GPCR oligomerization has emerged as a hot topic in the GPCR field in the last years. Receptors that are part of these oligomers can influence each other's function, although it is not yet entirely understood how these interactions work. The existence of such a highly complex network of interactions between GPCRs generates the possibility of alternative targets for new therapeutic approaches. However, challenges still exist in the characterization of these complexes, especially at the interface level. Different experimental approaches, such as FRET or BRET, are usually combined to study GPCR oligomer interactions. Computational methods have been applied as a useful tool for retrieving information from GPCR sequences and the few X-ray-resolved oligomeric structures that are accessible, as well as for predicting new and trustworthy GPCR oligomeric interfaces. Machine-learning (ML) approaches have recently helped with some hindrances of other methods. By joining and evaluating multiple structure-, sequence- and co-evolution-based features on the same algorithm, it is possible to dilute the issues of particular structures and residues that arise from the experimental methodology into all-encompassing algorithms capable of accurately predict GPCR-GPCR interfaces. All these methods used as a single or a combined approach provide useful information about GPCR oligomerization and its role in GPCR function and dynamics. Altogether, we present experimental, computational and machine-learning methods used to study oligomers interfaces, as well as strategies that have been used to target these dynamic complexes."
31951873,4.0,Artificial intelligence in multiparametric prostate cancer imaging with focus on deep-learning methods,2020 Jun;189:105316.,"Prostate cancer represents today the most typical example of a pathology whose diagnosis requires multiparametric imaging, a strategy where multiple imaging techniques are combined to reach an acceptable diagnostic performance. However, the reviewing, weighing and coupling of multiple images not only places additional burden on the radiologist, it also complicates the reviewing process. Prostate cancer imaging has therefore been an important target for the development of computer-aided diagnostic (CAD) tools. In this survey, we discuss the advances in CAD for prostate cancer over the last decades with special attention to the deep-learning techniques that have been designed in the last few years. Moreover, we elaborate and compare the methods employed to deliver the CAD output to the operator for further medical decision making."
31951162,1.0,A Review of Recent Developments and Progress in Computational Drug Repositioning,2020;26(26):3059-3068.,"Computational drug repositioning is an efficient approach towards discovering new indications for existing drugs. In recent years, with the accumulation of online health-related information and the extensive use of biomedical databases, computational drug repositioning approaches have achieved significant progress in drug discovery. In this review, we summarize recent advancements in drug repositioning. Firstly, we explicitly demonstrated the available data source information which is conducive to identifying novel indications. Furthermore, we provide a summary of the commonly used computing approaches. For each method, we briefly described techniques, case studies, and evaluation criteria. Finally, we discuss the limitations of the existing computing approaches."
31950592,6.0,Radiomics and Machine Learning in Oral Healthcare,2020 May;14(3):e1900040.,"The increasing storage of information, data, and forms of knowledge has led to the development of new technologies that can help to accomplish complex tasks in different areas, such as in dentistry. In this context, the role of computational methods, such as radiomics and Artificial Intelligence (AI) applications, has been progressing remarkably for dentomaxillofacial radiology (DMFR). These tools bring new perspectives for diagnosis, classification, and prediction of oral diseases, treatment planning, and for the evaluation and prediction of outcomes, minimizing the possibilities of human errors. A comprehensive review of the state-of-the-art of using radiomics and machine learning (ML) for imaging in oral healthcare is presented in this paper. Although the number of published studies is still relatively low, the preliminary results are very promising and in a near future, an augmented dentomaxillofacial radiology (ADMFR) will combine the use of radiomics-based and AI-based analyses with the radiologist's evaluation. In addition to the opportunities and possibilities, some challenges and limitations have also been discussed for further investigations."
31945959,,Outlier Detection in Health Record Free-Text using Deep Learning,2019 Jul;2019:550-555.,"In recent years, machine learning approaches have been successfully applied to analysis of patient symptom data in the context of disease diagnosis, at least where such data is well codified. However, much of the data present in Electronic Health Records (EHR) is unlikely to prove suitable for classic machine learning approaches. In particular, the use of free (or unstructured) text for clinical notes presents significant analytical opportunities, but also unique difficulties. Furthermore, the wide dispersal of health data relating to individuals necessitates the development of decentralized solutions. We provide, in this paper, an overview of our approach to develop a neural network framework for patient classification in the environment of EHRs where data may be heterogeneous, incomplete (containing missing values), and noisy. In this paper we describe our system which provides prediction of outlier cases which are likely to relate to frequent attender patients, which acheives an Area-Under-the-Curve score of up to 0.92."
31939856,7.0,"Artificial Intelligence in Anesthesiology: Current Techniques, Clinical Applications, and Limitations",2020 Feb;132(2):379-394.,"Artificial intelligence has been advancing in fields including anesthesiology. This scoping review of the intersection of artificial intelligence and anesthesia research identified and summarized six themes of applications of artificial intelligence in anesthesiology: (1) depth of anesthesia monitoring, (2) control of anesthesia, (3) event and risk prediction, (4) ultrasound guidance, (5) pain management, and (6) operating room logistics. Based on papers identified in the review, several topics within artificial intelligence were described and summarized: (1) machine learning (including supervised, unsupervised, and reinforcement learning), (2) techniques in artificial intelligence (e.g., classical machine learning, neural networks and deep learning, Bayesian methods), and (3) major applied fields in artificial intelligence.The implications of artificial intelligence for the practicing anesthesiologist are discussed as are its limitations and the role of clinicians in further developing artificial intelligence for use in clinical care. Artificial intelligence has the potential to impact the practice of anesthesiology in aspects ranging from perioperative support to critical care delivery to outpatient pain management."
31938923,,Of Machines and Men: Intelligent Diagnosis and the Shape of Things to Come,2020 Jan 14;22(1):9.,"Artificial Intelligence (AI), although well established in many areas of everyday life, has only recently been trialed in the diagnosis and management of common clinical conditions. This editorial review highlights progress to date and suggests further improvements in and trials of AI in the management of conditions such as hypertension."
31936321,7.0,Machine-Learning-Assisted De Novo Design of Organic Molecules and Polymers: Opportunities and Challenges,2020 Jan 8;12(1):163.,"Organic molecules and polymers have a broad range of applications in biomedical, chemical, and materials science fields. Traditional design approaches for organic molecules and polymers are mainly experimentally-driven, guided by experience, intuition, and conceptual insights. Though they have been successfully applied to discover many important materials, these methods are facing significant challenges due to the tremendous demand of new materials and vast design space of organic molecules and polymers. Accelerated and inverse materials design is an ideal solution to these challenges. With advancements in high-throughput computation, artificial intelligence (especially machining learning, ML), and the growth of materials databases, ML-assisted materials design is emerging as a promising tool to flourish breakthroughs in many areas of materials science and engineering. To date, using ML-assisted approaches, the quantitative structure property/activity relation for material property prediction can be established more accurately and efficiently. In addition, materials design can be revolutionized and accelerated much faster than ever, through ML-enabled molecular generation and inverse molecular design. In this perspective, we review the recent progresses in ML-guided design of organic molecules and polymers, highlight several successful examples, and examine future opportunities in biomedical, chemical, and materials science fields. We further discuss the relevant challenges to solve in order to fully realize the potential of ML-assisted materials design for organic molecules and polymers. In particular, this study summarizes publicly available materials databases, feature representations for organic molecules, open-source tools for feature generation, methods for molecular generation, and ML models for prediction of material properties, which serve as a tutorial for researchers who have little experience with ML before and want to apply ML for various applications. Last but not least, it draws insights into the current limitations of ML-guided design of organic molecules and polymers. We anticipate that ML-assisted materials design for organic molecules and polymers will be the driving force in the near future, to meet the tremendous demand of new materials with tailored properties in different fields."
31936210,5.0,Practices and Trends of Machine Learning Application in Nanotoxicology,2020 Jan 8;10(1):116.,"Machine Learning (ML) techniques have been applied in the field of nanotoxicology with very encouraging results. Adverse effects of nanoforms are affected by multiple features described by theoretical descriptors, nano-specific measured properties, and experimental conditions. ML has been proven very helpful in this field in order to gain an insight into features effecting toxicity, predicting possible adverse effects as part of proactive risk analysis, and informing safe design. At this juncture, it is important to document and categorize the work that has been carried out. This study investigates and bookmarks ML methodologies used to predict nano (eco)-toxicological outcomes in nanotoxicology during the last decade. It provides a review of the sequenced steps involved in implementing an ML model, from data pre-processing, to model implementation, model validation, and applicability domain. The review gathers and presents the step-wise information on techniques and procedures of existing models that can be used readily to assemble new nanotoxicological in silico studies and accelerates the regulation of in silico tools in nanotoxicology. ML applications in nanotoxicology comprise an active and diverse collection of ongoing efforts, although it is still in their early steps toward a scientific accord, subsequent guidelines, and regulation adoption. This study is an important bookend to a decade of ML applications to nanotoxicology and serves as a useful guide to further in silico applications."
31935669,6.0,Artificial intelligence in digital breast pathology: Techniques and applications,2020 Feb;49:267-273.,"Breast cancer is the most common cancer and second leading cause of cancer-related death worldwide. The mainstay of breast cancer workup is histopathological diagnosis - which guides therapy and prognosis. However, emerging knowledge about the complex nature of cancer and the availability of tailored therapies have exposed opportunities for improvements in diagnostic precision. In parallel, advances in artificial intelligence (AI) along with the growing digitization of pathology slides for the primary diagnosis are a promising approach to meet the demand for more accurate detection, classification and prediction of behaviour of breast tumours. In this article, we cover the current and prospective uses of AI in digital pathology for breast cancer, review the basics of digital pathology and AI, and outline outstanding challenges in the field."
31934891,1.0,Environmental mixtures and children's health: identifying appropriate statistical approaches,2020 Apr;32(2):315-320.,"Purpose of review:                    Biomonitoring studies have shown that children are constantly exposed to complex patterns of chemical and nonchemical exposures. Here, we briefly summarize the rationale for studying multiple exposures, also called mixture, in relation to child health and key statistical approaches that can be used. We discuss advantages over traditional methods, limitations and appropriateness of the context.              Recent findings:                    New approaches allow pediatric researchers to answer increasingly complex questions related to environmental mixtures. We present methods to identify the most relevant exposures among a high-multitude of variables, via shrinkage and variable selection techniques, and identify the overall mixture effect, via Weighted Quantile Sum and Bayesian Kernel Machine regressions. We then describe novel extensions that handle high-dimensional exposure data and allow identification of critical exposure windows.              Summary:                    Recent advances in statistics and machine learning enable researchers to identify important mixture components, estimate joint mixture effects and pinpoint critical windows of exposure. Despite many advantages over single chemical approaches, measurement error and biases may be amplified in mixtures research, requiring careful study planning and design. Future research requires increased collaboration between epidemiologists, statisticians and data scientists, and further integration with causal inference methods."
31934647,11.0,Photoplethysmography based atrial fibrillation detection: a review,2020 Jan 10;3:3.,"Atrial fibrillation (AF) is a cardiac rhythm disorder associated with increased morbidity and mortality. It is the leading risk factor for cardioembolic stroke and its early detection is crucial in both primary and secondary stroke prevention. Continuous monitoring of cardiac rhythm is today possible thanks to consumer-grade wearable devices, enabling transformative diagnostic and patient management tools. Such monitoring is possible using low-cost easy-to-implement optical sensors that today equip the majority of wearables. These sensors record blood volume variations-a technology known as photoplethysmography (PPG)-from which the heart rate and other physiological parameters can be extracted to inform about user activity, fitness, sleep, and health. Recently, new wearable devices were introduced as being capable of AF detection, evidenced by large prospective trials in some cases. Such devices would allow for early screening of AF and initiation of therapy to prevent stroke. This review is a summary of a body of work on AF detection using PPG. A thorough account of the signal processing, machine learning, and deep learning approaches used in these studies is presented, followed by a discussion of their limitations and challenges towards clinical applications."
31932230,4.0,Toward Addiction Prediction: An Overview of Cross-Validated Predictive Modeling Findings and Considerations for Future Neuroimaging Research,2020 Aug;5(8):748-758.,"Substance use is a leading cause of disability and death worldwide. Despite the existence of evidence-based treatments, clinical outcomes are highly variable across individuals, and relapse rates following treatment remain high. Within this context, methods to identify individuals at particular risk for unsuccessful treatment (i.e., limited within-treatment abstinence), or for relapse following treatment, are needed to improve outcomes. Cumulatively, the literature generally supports the hypothesis that individual differences in brain function and structure are linked to differences in treatment outcomes, although anatomical loci and directions of associations have differed across studies. However, this work has almost entirely used methods that may overfit the data, leading to inflated effect size estimates and reduced likelihood of reproducibility in novel clinical samples. In contrast, cross-validated predictive modeling (i.e., machine learning) approaches are designed to overcome limitations of traditional approaches by focusing on individual differences and generalization to novel subjects (i.e., cross-validation), thereby increasing the likelihood of replication and potential translation to novel clinical settings. Here, we review recent studies using these approaches to generate brain-behavior models of treatment outcomes in addictions and provide recommendations for further work using these methods."
31927437,8.0,"Computational approaches in cancer multidrug resistance research: Identification of potential biomarkers, drug targets and drug-target interactions",2020 Jan;48:100662.,"Like physics in the 19th century, biology and molecular biology in particular, has been fertilized and enhanced like few other scientific fields, by the incorporation of mathematical methods. In the last decades, a whole new scientific field, bioinformatics, has developed with an output of over 30,000 papers a year (Pubmed search using the keyword ""bioinformatics""). Huge databases of mass throughput data have been established, with ArrayExpress alone containing more than 2.7 million assays (October 2019). Computational methods have become indispensable tools in molecular biology, particularly in one of the most challenging areas of cancer research, multidrug resistance (MDR). However, confronted with a plethora of different algorithms, approaches, and methods, the average researcher faces key questions: Which methods do exist? Which methods can be used to tackle the aims of a given study? Or, more generally, how do I use computational biology/bioinformatics to bolster my research? The current review is aimed at providing guidance to existing methods with relevance to MDR research. In particular, we provide an overview on: a) the identification of potential biomarkers using expression data; b) the prediction of treatment response by machine learning methods; c) the employment of network approaches to identify gene/protein regulatory networks and potential key players; d) the identification of drug-target interactions; e) the use of bipartite networks to identify multidrug targets; f) the identification of cellular subpopulations with the MDR phenotype; and, finally, g) the use of molecular modeling methods to guide and enhance drug discovery. This review shall serve as a guide through some of the basic concepts useful in MDR research. It shall give the reader some ideas about the possibilities in MDR research by using computational tools, and, finally, it shall provide a short overview of relevant literature."
31926317,,Key indicators of phase transition for clinical trials through machine learning,2020 Feb;25(2):414-421.,"A significant number of drugs fail during the clinical testing stage. To understand the attrition of drugs through the regulatory process, here we review and advance machine-learning (ML) and natural language-processing algorithms to investigate the importance of factors in clinical trials that are linked with failure in Phases II and III. We find that clinical trial phase transitions can be predicted with an average accuracy of 80%. Identifying these trials provides information to sponsors facing difficult decisions about whether these higher risk trials should be modified or halted. We also find common protocol characteristics across therapeutic areas that are linked to phase success, including the number of endpoints and the complexity of the eligibility criteria."
31924424,2.0,Contrast-Enhanced Ultrasound Quantification: From Kinetic Modeling to Machine Learning,2020 Mar;46(3):518-543.,"Ultrasound contrast agents (UCAs) have opened up immense diagnostic possibilities by combined use of indicator dilution principles and dynamic contrast-enhanced ultrasound (DCE-US) imaging. UCAs are microbubbles encapsulated in a biocompatible shell. With a rheology comparable to that of red blood cells, UCAs provide an intravascular indicator for functional imaging of the (micro)vasculature by quantitative DCE-US. Several models of the UCA intravascular kinetics have been proposed to provide functional quantitative maps, aiding diagnosis of different pathological conditions. This article is a comprehensive review of the available methods for quantitative DCE-US imaging based on temporal, spatial and spatiotemporal analysis of the UCA kinetics. The recent introduction of novel UCAs that are targeted to specific vascular receptors has advanced DCE-US to a molecular imaging modality. In parallel, new kinetic models of increased complexity have been developed. The extraction of multiple quantitative maps, reflecting complementary variables of the underlying physiological processes, requires an integrative approach to their interpretation. A probabilistic framework based on emerging machine-learning methods represents nowadays the ultimate approach, improving the diagnostic accuracy of DCE-US imaging by optimal combination of the extracted complementary information. The current value and future perspective of all these advances are critically discussed."
31923803,,Power dynamics in intergroup relations,2020 Jun;33:250-255.,"Power and intergroup relations are complex, multilevel, and dynamic. Using Power Basis Theory, we explain our criteria for deciding whether theory or research addresses intergroup power dynamics: it must (a) address power and not authority or other topics, (b) involve attempted or real change regarding groups and power, or the prevention of change, (c) involve protracted interactions among multiple actors through more than one channel, (d) involve more than one level of social organization (e.g. person, group, superordinate group). We organize our 10-year review by these criteria. Research meeting all our criteria is rare. We explain relevant new theory and new research tools, including multi-level modelling, multi-player games, agent-based models, big data, and machine-learning, that can help fill the gap."
31922268,28.0,Machine intelligence in peptide therapeutics: A next-generation tool for rapid disease screening,2020 Jul;40(4):1276-1314.,"Discovery and development of biopeptides are time-consuming, laborious, and dependent on various factors. Data-driven computational methods, especially machine learning (ML) approach, can rapidly and efficiently predict the utility of therapeutic peptides. ML methods offer an array of tools that can accelerate and enhance decision making and discovery for well-defined queries with ample and sophisticated data quality. Various ML approaches, such as support vector machines, random forest, extremely randomized tree, and more recently deep learning methods, are useful in peptide-based drug discovery. These approaches leverage the peptide data sets, created via high-throughput sequencing and computational methods, and enable the prediction of functional peptides with increased levels of accuracy. The use of ML approaches in the development of peptide-based therapeutics is relatively recent; however, these techniques are already revolutionizing protein research by unraveling their novel therapeutic peptide functions. In this review, we discuss several ML-based state-of-the-art peptide-prediction tools and compare these methods in terms of their algorithms, feature encodings, prediction scores, evaluation methodologies, and software utilities. We also assessed the prediction performance of these methods using well-constructed independent data sets. In addition, we discuss the common pitfalls and challenges of using ML approaches for peptide therapeutics. Overall, we show that using ML models in peptide research can streamline the development of targeted peptide therapies."
31922162,10.0,Recent advances in fast-scan cyclic voltammetry,2020 Feb 17;145(4):1087-1102.,"Fast-scan cyclic voltammetry (FSCV) at carbon-fiber microelectrodes (CFMEs) is a versatile electrochemical technique to probe neurochemical dynamics in vivo. Progress in FSCV methodology continues to address analytical challenges arising from biological needs to measure low concentrations of neurotransmitters at specific sites. This review summarizes recent advances in FSCV method development in three areas: (1) waveform optimization, (2) electrode development, and (3) data analysis. First, FSCV waveform parameters such as holding potential, switching potential, and scan rate have been optimized to monitor new neurochemicals. The new waveform shapes introduce better selectivity toward specific molecules such as serotonin, histamine, hydrogen peroxide, octopamine, adenosine, guanosine, and neuropeptides. Second, CFMEs have been modified with nanomaterials such as carbon nanotubes or replaced with conducting polymers to enhance sensitivity, selectivity, and antifouling properties. Different geometries can be obtained by 3D-printing, manufacturing arrays, or fabricating carbon nanopipettes. Third, data analysis is important to sort through the thousands of CVs obtained. Recent developments in data analysis include preprocessing by digital filtering, principal components analysis for distinguishing analytes, and developing automated algorithms to detect peaks. Future challenges include multisite measurements, machine learning, and integration with other techniques. Advances in FSCV will accelerate research in neurochemistry to answer new biological questions about dynamics of signaling in the brain."
31921555,2.0,Progress and Challenges Toward the Rational Design of Oxygen Electrocatalysts Based on a Descriptor Approach,2019 Nov 27;7(1):1901614.,"Oxygen redox catalysis, including the oxygen reduction reaction (ORR) and oxygen evolution reaction (OER), is crucial in determining the electrochemical performance of energy conversion and storage devices such as fuel cells, metal-air batteries,and electrolyzers. The rational design of electrochemical catalysts replaces the traditional trial-and-error methods and thus promotes the R&D process. Identifying descriptors that link structure and activity as well as selectivity of catalysts is the key for rational design. In the past few decades, two types of descriptors including bulk- and surface-based have been developed to probe the structure-property relationships. Correlating the current descriptors to one another will promote the understanding of the underlying physics and chemistry, triggering further development of more universal descriptors for the future design of electrocatalysts. Herein, the current benchmark activity descriptors for oxygen electrocatalysis as well as their applications are reviewed. Particular attention is paid to circumventing the scaling relationship of oxygen-containing intermediates. For hybrid materials, multiple descriptors will show stronger predictive power by considering more factors such as interface reconstruction, confinement effect, multisite adsorption, etc. Machine learning and high-throughput simulations can thus be crucial in assisting the discovery of new multiple descriptors and reaction mechanisms."
31917651,5.0,Are We Meeting the Promise of Endotypes and Precision Medicine in Asthma?,2020 Jul 1;100(3):983-1017.,"While the term asthma has long been known to describe heterogeneous groupings of patients, only recently have data evolved which enable a molecular understanding of the clinical differences. The evolution of transcriptomics (and other 'omics platforms) and improved statistical analyses in combination with large clinical cohorts opened the door for molecular characterization of pathobiologic processes associated with a range of asthma patients. When linked with data from animal models and clinical trials of targeted biologic therapies, emerging distinctions arose between patients with and without elevations in type 2 immune and inflammatory pathways, leading to the confirmation of a broad categorization of type 2-Hi asthma. Differences in the ratios, sources, and location of type 2 cytokines and their relation to additional immune pathway activation appear to distinguish several different (sub)molecular phenotypes, and perhaps endotypes of type 2-Hi asthma, which respond differently to broad and targeted anti-inflammatory therapies. Asthma in the absence of type 2 inflammation is much less well defined, without clear biomarkers, but is generally linked with poor responses to corticosteroids. Integration of ""big data"" from large cohorts, over time, using machine learning approaches, combined with validation and iterative learning in animal (and human) model systems is needed to identify the biomarkers and tightly defined molecular phenotypes/endotypes required to fulfill the promise of precision medicine."
31912547,1.0,"NMR signal processing, prediction, and structure verification with machine learning techniques",2020 Jun;58(6):512-519.,"Machine learning (ML) methods have been present in the field of NMR since decades, but it has experienced a tremendous growth in the last few years, especially thanks to the emergence of deep learning (DL) techniques taking advantage of the increased amounts of data and available computer power. These algorithms are successfully employed for classification, regression, clustering, or dimensionality reduction tasks of large data sets and have been intensively applied in different areas of NMR including metabonomics, clinical diagnosis, or relaxometry. In this article, we concentrate on the various applications of ML/DL in the areas of NMR signal processing and analysis of small molecules, including automatic structure verification and prediction of NMR observables in solution."
31910421,2.0,Machine Learning in Fetal Cardiology: What to Expect,2020;47(5):363-372.,"In fetal cardiology, imaging (especially echocardiography) has demonstrated to help in the diagnosis and monitoring of fetuses with a compromised cardiovascular system potentially associated with several fetal conditions. Different ultrasound approaches are currently used to evaluate fetal cardiac structure and function, including conventional 2-D imaging and M-mode and tissue Doppler imaging among others. However, assessment of the fetal heart is still challenging mainly due to involuntary movements of the fetus, the small size of the heart, and the lack of expertise in fetal echocardiography of some sonographers. Therefore, the use of new technologies to improve the primary acquired images, to help extract measurements, or to aid in the diagnosis of cardiac abnormalities is of great importance for optimal assessment of the fetal heart. Machine leaning (ML) is a computer science discipline focused on teaching a computer to perform tasks with specific goals without explicitly programming the rules on how to perform this task. In this review we provide a brief overview on the potential of ML techniques to improve the evaluation of fetal cardiac function by optimizing image acquisition and quantification/segmentation, as well as aid in improving the prenatal diagnoses of fetal cardiac remodeling and abnormalities."
31907954,3.0,Applying Machine Learning in Liver Disease and Transplantation: A Comprehensive Review,2020 Mar;71(3):1093-1105.,"Machine learning (ML) utilizes artificial intelligence to generate predictive models efficiently and more effectively than conventional methods through detection of hidden patterns within large data sets. With this in mind, there are several areas within hepatology where these methods can be applied. In this review, we examine the literature pertaining to machine learning in hepatology and liver transplant medicine. We provide an overview of the strengths and limitations of ML tools and their potential applications to both clinical and molecular data in hepatology. ML has been applied to various types of data in liver disease research, including clinical, demographic, molecular, radiological, and pathological data. We anticipate that use of ML tools to generate predictive algorithms will change the face of clinical practice in hepatology and transplantation. This review will provide readers with the opportunity to learn about the ML tools available and potential applications to questions of interest in hepatology."
31907710,1.0,Machine learning applications in imaging analysis for patients with pituitary tumors: a review of the current literature and future directions,2020 Jun;23(3):273-293.,"Purpose:                    To provide an overview of fundamental concepts in machine learning (ML), review the literature on ML applications in imaging analysis of pituitary tumors for the last 10 years, and highlight the future directions on potential applications of ML for pituitary tumor patients.              Method:                    We presented an overview of the fundamental concepts in ML, its various stages used in healthcare, and highlighted the key components typically present in an imaging-based tumor analysis pipeline. A search was conducted across four databases (PubMed, Ovid, Embase, and Google Scholar) to gather research articles from the past 10 years (2009-2019) involving imaging related to pituitary tumor and ML. We grouped the studies by imaging modalities and analyzed the ML tasks in terms of the data inputs, reference standards, methodologies, and limitations.              Results:                    Of the 16 studies included in our analysis, 10 appeared in 2018-2019. Most of the studies utilized retrospective data and followed a semi-automatic ML pipeline. The studies included use of magnetic resonance imaging (MRI), facial photographs, surgical microscopic video, spectrometry, and spectroscopy imaging. The objectives of the studies covered 14 distinct applications and majority of the studies addressed a binary classification problem. Only five of the 11 MRI-based studies had an external validation or a holdout set to test the performance of a final trained model.              Conclusion:                    Through our concise evaluation and comparison of the studies using the concepts presented, we highlight future directions so that potential ML applications using different imaging modalities can be developed to benefit the clinical care of pituitary tumor patients."
31905969,11.0,"Epigenetics Analysis and Integrated Analysis of Multiomics Data, Including Epigenetic Data, Using Artificial Intelligence in the Era of Precision Medicine",2019 Dec 30;10(1):62.,"To clarify the mechanisms of diseases, such as cancer, studies analyzing genetic mutations have been actively conducted for a long time, and a large number of achievements have already been reported. Indeed, genomic medicine is considered the core discipline of precision medicine, and currently, the clinical application of cutting-edge genomic medicine aimed at improving the prevention, diagnosis and treatment of a wide range of diseases is promoted. However, although the Human Genome Project was completed in 2003 and large-scale genetic analyses have since been accomplished worldwide with the development of next-generation sequencing (NGS), explaining the mechanism of disease onset only using genetic variation has been recognized as difficult. Meanwhile, the importance of epigenetics, which describes inheritance by mechanisms other than the genomic DNA sequence, has recently attracted attention, and, in particular, many studies have reported the involvement of epigenetic deregulation in human cancer. So far, given that genetic and epigenetic studies tend to be accomplished independently, physiological relationships between genetics and epigenetics in diseases remain almost unknown. Since this situation may be a disadvantage to developing precision medicine, the integrated understanding of genetic variation and epigenetic deregulation appears to be now critical. Importantly, the current progress of artificial intelligence (AI) technologies, such as machine learning and deep learning, is remarkable and enables multimodal analyses of big omics data. In this regard, it is important to develop a platform that can conduct multimodal analysis of medical big data using AI as this may accelerate the realization of precision medicine. In this review, we discuss the importance of genome-wide epigenetic and multiomics analyses using AI in the era of precision medicine."
31904426,7.0,Machine and deep learning approaches for cancer drug repurposing,2021 Jan;68:132-142.,"Knowledge of the underpinnings of cancer initiation, progression and metastasis has increased exponentially in recent years. Advanced ""omics"" coupled with machine learning and artificial intelligence (deep learning) methods have helped elucidate targets and pathways critical to those processes that may be amenable to pharmacologic modulation. However, the current anti-cancer therapeutic armamentarium continues to lag behind. As the cost of developing a new drug remains prohibitively expensive, repurposing of existing approved and investigational drugs is sought after given known safety profiles and reduction in the cost barrier. Notably, successes in oncologic drug repurposing have been infrequent. Computational in-silico strategies have been developed to aid in modeling biological processes to find new disease-relevant targets and discovering novel drug-target and drug-phenotype associations. Machine and deep learning methods have especially enabled leaps in those successes. This review will discuss these methods as they pertain to cancer biology as well as immunomodulation for drug repurposing opportunities in oncologic diseases."
31902580,4.0,Discovery and Validation of Prediction Algorithms for Psychosis in Youths at Clinical High Risk,2020 Aug;5(8):738-747.,"In the past 2 to 3 decades, clinicians have used the clinical high risk for psychosis (CHR-P) paradigm to better understand factors that contribute to the onset of psychotic disorders. While this paradigm is useful to identify individuals at risk, the CHR-P criteria are not sufficient to predict outcomes from the CHR-P population. Because approximately 25% of the CHR-P population will ultimately convert to psychosis, more precise methods of prediction are needed to account for heterogeneity in both risk factors and outcomes in the CHR-P population. To this end, several groups in recent years have used data-driven approaches to refine predictive algorithms to predict both conversion to psychosis and functional outcomes. These models have generally used either clinical and behavioral data, including demographics and measures of symptom severity, neurocognitive functioning, and social functioning, or neuroimaging data, including structural and functional measures, to predict conversion to psychosis in CHR-P samples. This review focuses on the empirical models that have been derived within each of these lines of research and evaluates the performance and methodology of these models. This review also serves to inform best practices for data-driven approaches and directions moving forward to improve our prediction of psychotic disorders and associated outcomes. Because sample size is still the most critical consideration in the current models, we urge that algorithms to predict conversion be conducted using multisite data in order to obtain the power necessary to conclusively determine predictive accuracy without overfitting."
31902468,3.0,Computational prediction of cytochrome P450 inhibition and induction,2020 Feb;35(1):30-44.,"Cytochrome P450 (CYP) enzymes play an important role in the phase I metabolism of many xenobiotics. Most drug-drug interactions (DDIs) associated with CYP are caused by either CYP inhibition or induction. The early detection of potential DDIs is highly desirable in the pharmaceutical industry because DDIs can cause serious adverse events, which can lead to poor patient health and drug development failures. Recently, many computational studies predicting CYP inhibition and induction have been reported. The current computational modeling approaches for CYP metabolism are classified as ligand- and structure-based; various techniques, such as quantitative structure-activity relationships, machine learning, docking, and molecular dynamic simulation, are involved in both the approaches. Recently, combining these two approaches have resulted in improvements in the prediction accuracy of DDIs. In this review, we present important, recent developments in the computational prediction of the inhibition of four clinically crucial CYP isoforms (CYP1A2, 2C9, 2D6, and 3A4) and three nuclear receptors (aryl hydrocarbon receptor, constitutive androstane receptor, and pregnane X receptor) involved in the induction of CYP1A2, 2B6, and 3A4, respectively."
31898014,6.0,AI-based computer-aided diagnosis (AI-CAD): the latest review to read first,2020 Mar;13(1):6-19.,"The third artificial intelligence (AI) boom is coming, and there is an inkling that the speed of its evolution is quickly increasing. In games like chess, shogi, and go, AI has already defeated human champions, and the fact that it is able to achieve autonomous driving is also being realized. Under these circumstances, AI has evolved and diversified at a remarkable pace in medical diagnosis, especially in diagnostic imaging. Therefore, this commentary focuses on AI in medical diagnostic imaging and explains the recent development trends and practical applications of computer-aided detection/diagnosis using artificial intelligence, especially deep learning technology, as well as some topics surrounding it."
31897624,2.0,Artificial Intelligence in Plastic Surgery: Applications and Challenges,2021 Apr;45(2):784-790.,"New developments in artificial intelligence (AI) offer opportunities to enhance plastic surgery practice, research, and education. In this article, we review relevant AI tools and applications, including machine learning, reinforcement learning, and natural language processing. Our own Markov decision process for keloid treatment illustrates how these models are developed and can be used to enhance decision-making in clinical practice. Finally, we discuss challenges of implementing AI and knowledge gaps that must be addressed to successfully apply AI in plastic surgery. Level of Evidence V This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266 ."
31895021,,A review of knowledge discovery process in control and mitigation of avian influenza,2019 Jun;20(1):61-71.,"In the last several decades, avian influenza virus has caused numerous outbreaks around the world. These outbreaks pose a significant threat to the poultry industry and also to public health. When an avian influenza (AI) outbreak occurs, it is critical to make informed decisions about the potential risks, impact, and control measures. To this end, many modeling approaches have been proposed to acquire knowledge from different sources of data and perspectives to enhance decision making. Although some of these approaches have shown to be effective, they do not follow the process of knowledge discovery in databases (KDD). KDD is an iterative process, consisting of five steps, that aims at extracting unknown and useful information from the data. The present review attempts to survey AI modeling methods in the context of KDD process. We first divide the modeling techniques used in AI into two main categories: data-intensive modeling and small-data modeling. We then investigate the existing gaps in the literature and suggest several potential directions and techniques for future studies. Overall, this review provides insights into the control of AI in terms of the risk of introduction and spread of the virus."
31895018,2.0,A review of traditional and machine learning methods applied to animal breeding,2019 Jun;20(1):31-46.,"The current livestock management landscape is transitioning to a high-throughput digital era where large amounts of information captured by systems of electro-optical, acoustical, mechanical, and biosensors is stored and analyzed on a daily and hourly basis, and actionable decisions are made based on quantitative and qualitative analytic results. While traditional animal breeding prediction methods have been used with great success until recently, the deluge of information starts to create a computational and storage bottleneck that could lead to negative long-term impacts on herd management strategies if not handled properly. A plethora of machine learning approaches, successfully used in various industrial and scientific applications, made their way in the mainstream approaches for livestock breeding techniques, and current results show that such methods have the potential to match or surpass the traditional approaches, while most of the time they are more scalable from a computational and storage perspective. This article provides a succinct view on what traditional and novel prediction methods are currently used in the livestock breeding field, how successful they are, and how the future of the field looks in the new digital agriculture era."
31893575,1.0,FOLFOX treatment response prediction in metastatic or recurrent colorectal cancer patients via machine learning algorithms,2020 Feb;9(4):1419-1429.,"Early identification of metastatic or recurrent colorectal cancer (CRC) patients who will be sensitive to FOLFOX (5-FU, leucovorin and oxaliplatin) therapy is very important. We performed microarray meta-analysis to identify differentially expressed genes (DEGs) between FOLFOX responders and nonresponders in metastatic or recurrent CRC patients, and found that the expression levels of WASHC4, HELZ, ERN1, RPS6KB1, and APPBP2 were downregulated, while the expression levels of IRF7, EML3, LYPLA2, DRAP1, RNH1, PKP3, TSPAN17, LSS, MLKL, PPP1R7, GCDH, C19ORF24, and CCDC124 were upregulated in FOLFOX responders compared with nonresponders. Subsequent functional annotation showed that DEGs were significantly enriched in autophagy, ErbB signaling pathway, mitophagy, endocytosis, FoxO signaling pathway, apoptosis, and antifolate resistance pathways. Based on those candidate genes, several machine learning algorithms were applied to the training set, then performances of models were assessed via the cross validation method. Candidate models with the best tuning parameters were applied to the test set and the final model showed satisfactory performance. In addition, we also reported that MLKL and CCDC124 gene expression were independent prognostic factors for metastatic CRC patients undergoing FOLFOX therapy."
31890956,,A bibliometric study on intelligent techniques of bankruptcy prediction for corporate firms,2019 Dec 18;5(12):e02997.,"Bibliometric analysis is an effective method to carry out quantitative study of academic output to address the research trends on a given area of investigation through analysing existing documents. This paper aims to explore the application of intelligent techniques in bankruptcy predictions so as to assess its progress and describe the research trend through bibliometric analysis over the last five decades. The results indicate that, although there is a significant increase in publication number since the 2008 financial crisis, the collaboration among authors is weak, especially at the international dimension. Also, the findings provide a comprehensive view of interdisciplinary research on bankruptcy modelling in finance, business management and computer science fields. The authors sought to contribute to the theoretical development of bankruptcy prediction modeling by bringing new knowledge and key insights. Artificial intelligent techniques are now serving as important alternatives to statistical methods and demonstrate very promising results. This paper has both theoretical and practical implications. First, it provides insights for scholars into the theoretical evolution and intellectual structure for conducting future research in this field. Second, it sheds light on identifying under-explored machine learning techniques applied in bankruptcy prediction which can be crucial in management and decision-making for corporate firm managers and policy makers."
31890467,,Calibration and validation of accelerometry to measure physical activity in adult clinical groups: A systematic review,2019 Nov 6;16:101001.,"A growing body of research calibrating and validating accelerometers to classify physical activity intensities has led to a range of cut-points. However, the applicability of current calibration protocols to clinical populations remains to be addressed. The aim of this review was to evaluate the accuracy of the methods for calibrating and validating of accelerometers to estimate physical activity intensity thresholds for clinical populations. Six databases were searched between March and July to 2017 using text words and subject headings. Studies developing moderate-to-vigorous intensity physical activity cut-points for adult clinical populations were included. The risk of bias was assessed using the health measurement instruments and a specific checklist for calibration studies. A total of 543,741 titles were found and 323 articles were selected for full-text assessment, with 11 meeting the inclusion criteria. Twenty-three different methods for calibration were identified using different models of ActiGraph and Actical accelerometers. Disease-specific cut-points ranged from 591 to 2717 countsÂ·min-1 and were identified for two main groups of clinical conditions: neuromusculoskeletal disorders and metabolic diseases. The heterogeneity in the available clinical protocols hinders the applicability and comparison of the developed cut-points. As such, a mixed protocol containing a controlled laboratory exercise test and activities of daily-life is suggested. It is recommended that this be combined with a statistical approach that allows for adjustments according to disease severity or the use of machine learning models. Finally, this review highlights the generalisation of cut-points developed on healthy populations to clinical populations is inappropriate."
31890287,2.0,Wide-field imaging of sickle retinopathy,2019 Dec 12;5(Suppl 1):27.,"Background:                    Wide-field imaging is a newer retinal imaging technology, capturing up to 200 degrees of the retina in a single photograph. Individuals with sickle cell retinopathy commonly exhibit peripheral retinal ischemia. Patients with proliferative sickle cell retinopathy develop pathologic retinal neovascularization of the peripheral retina which may progress into sight-threatening sequelae of vitreous hemorrhage and/or retinal detachment. The purpose of this review is to provide an overview of current and future applications of wide-field retinal imaging for sickle cell retinopathy, and recommend indications for best use.              Main body:                    There are several advantages to wide-field imaging in the clinical management of sickle cell disease patients. Retrospective and prospective studies support the success of wide-field imaging in detecting more sickle cell induced retinal microvascular abnormalities than traditional non-wide-field imaging. Clinicians can easily capture a greater extent of the retinal periphery in a patient's clinical baseline imaging to follow the changes at an earlier point and determine the rate of progression over time. Wide-field imaging minimizes patient and photographer burden, necessitating less photos and technical skill to capture the peripheral retina. Minimizing the number of necessary images can be especially helpful for pediatric patients with sickle cell retinopathy. Wide-field imaging has already been successful in identifying new biomarkers and risk factors for the development of proliferative sickle cell retinopathy. While these advantages should be considered, clinicians need to perform a careful risk-benefit analysis before ordering this test. Although wide-field fluorescein angiography successfully detects additional pathologic abnormalities compared to traditional imaging, a recent research study suggests that peripheral changes differentially detected by wide-field imaging may not change clinical management for most sickle cell patients.              Conclusions:                    While wide-field imaging may not carry a clinically significant direct benefit to all patients, it shows future promise in expanding our knowledge of sickle cell retinopathy. Clinicians may monitor peripheral retinal pathology such as retinal ischemia and retinal neovascularization over progressive time points, and use sequential wide-field retinal images to monitor response to treatment. Future applications for wide-field imaging may include providing data to facilitate machine learning, and potential use in tele-ophthalmology screening for proliferative sickle retinopathy."
31890142,9.0,Computational approaches for effective CRISPR guide RNA design and evaluation,2019 Nov 29;18:35-44.,"The Clustered Regularly Interspaced Short Palindromic Repeat (CRISPR)/ CRISPR-associated (Cas) system has emerged as the main technology for gene editing. Successful editing by CRISPR requires an appropriate Cas protein and guide RNA. However, low cleavage efficiency and off-target effects hamper the development and application of CRISPR/Cas systems. To predict cleavage efficiency and specificity, numerous computational approaches have been developed for scoring guide RNAs. Most scores are empirical or trained by experimental datasets, and scores are implemented using various computational methods. Herein, we discuss these approaches, focusing mainly on the features or computational methods they utilise. Furthermore, we summarise these tools and give some suggestions for their usage. We also recommend three versatile web-based tools with user-friendly interfaces and preferable functions. The review provides a comprehensive and up-to-date overview of computational approaches for guide RNA design that could help users to select the optimal tools for their research."
31888592,2.0,DeepFHR: intelligent prediction of fetal Acidemia using fetal heart rate signals based on convolutional neural network,2019 Dec 30;19(1):286.,"Background:                    Fetal heart rate (FHR) monitoring is a screening tool used by obstetricians to evaluate the fetal state. Because of the complexity and non-linearity, a visual interpretation of FHR signals using common guidelines usually results in significant subjective inter-observer and intra-observer variability.              Objective:                    Therefore, computer aided diagnosis (CAD) systems based on advanced artificial intelligence (AI) technology have recently been developed to assist obstetricians in making objective medical decisions.              Methods:                    In this work, we present an 8-layer deep convolutional neural network (CNN) framework to automatically predict fetal acidemia. After signal preprocessing, the input 2-dimensional (2D) images are obtained using the continuous wavelet transform (CWT), which provides a better way to observe and capture the hidden characteristic information of the FHR signals in both the time and frequency domains. Unlike the conventional machine learning (ML) approaches, this work does not require the execution of complex feature engineering, i.e., feature extraction and selection. In fact, 2D CNN model can self-learn useful features from the input data with the prerequisite of not losing informative features, representing the tremendous advantage of deep learning (DL) over ML.              Results:                    Based on the test open-access database (CTU-UHB), after comprehensive experimentation, we achieved better classification performance using the optimal CNN configuration compared to other state-of-the-art methods: the averaged ten-fold cross-validation of the accuracy, sensitivity, specificity, quality index defined as the geometric mean of the sensitivity and specificity, and the area under the curve yielded results of 98.34, 98.22, 94.87, 96.53 and 97.82%, respectively CONCLUSIONS: Once the proposed CNN model is successfully trained, the corresponding CAD system can be served as an effective tool to predict fetal asphyxia objectively and accurately."
31887283,4.0,Machine Learning Characterization of COPD Subtypes: Insights From the COPDGene Study,2020 May;157(5):1147-1157.,"COPD is a heterogeneous syndrome. Many COPD subtypes have been proposed, but there is not yet consensus on how many COPD subtypes there are and how they should be defined. The COPD Genetic Epidemiology Study (COPDGene), which has generated 10-year longitudinal chest imaging, spirometry, and molecular data, is a rich resource for relating COPD phenotypes to underlying genetic and molecular mechanisms. In this article, we place COPDGene clustering studies in context with other highly cited COPD clustering studies, and summarize the main COPD subtype findings from COPDGene. First, most manifestations of COPD occur along a continuum, which explains why continuous aspects of COPD or disease axes may be more accurate and reproducible than subtypes identified through clustering methods. Second, continuous COPD-related measures can be used to create subgroups through the use of predictive models to define cut-points, and we review COPDGene research on blood eosinophil count thresholds as a specific example. Third, COPD phenotypes identified or prioritized through machine learning methods have led to novel biological discoveries, including novel emphysema genetic risk variants and systemic inflammatory subtypes of COPD. Fourth, trajectory-based COPD subtyping captures differences in the longitudinal evolution of COPD, addressing a major limitation of clustering analyses that are confounded by disease severity. Ongoing longitudinal characterization of subjects in COPDGene will provide useful insights about the relationship between lung imaging parameters, molecular markers, and COPD progression that will enable the identification of subtypes based on underlying disease processes and distinct patterns of disease progression, with the potential to improve the clinical relevance and reproducibility of COPD subtypes."
31886649,5.0,"Quantitative Structure-Selectivity Relationships in Enantioselective Catalysis: Past, Present, and Future",2020 Feb 12;120(3):1620-1689.,"The dawn of the 21st century has brought with it a surge of research related to computer-guided approaches to catalyst design. In the past two decades, chemoinformatics, the application of informatics to solve problems in chemistry, has increasingly influenced prediction of activity and mechanistic investigations of organic reactions. The advent of advanced statistical and machine learning methods, as well as dramatic increases in computational speed and memory, has contributed to this emerging field of study. This review summarizes strategies to employ quantitative structure-selectivity relationships (QSSR) in asymmetric catalytic reactions. The coverage is structured by initially introducing the basic features of these methods. Subsequent topics are discussed according to increasing complexity of molecular representations. As the most applied subfield of QSSR in enantioselective catalysis, the application of local parametrization approaches and linear free energy relationships (LFERs) along with multivariate modeling techniques is described first. This section is followed by a description of global parametrization methods, the first of which is continuous chirality measures (CCM) because it is a single parameter derived from the global structure of a molecule. Chirality codes, global, multivariate descriptors, are then introduced followed by molecular interaction fields (MIFs), a global descriptor class that typically has the highest dimensionality. To highlight the current reach of QSSR in enantioselective transformations, a comprehensive collection of examples is presented. When combined with traditional experimental approaches, chemoinformatics holds great promise to predict new catalyst structures, rationalize mechanistic behavior, and profoundly change the way chemists discover and optimize reactions."
31886259,2.0,Application of Computational Biology and Artificial Intelligence Technologies in Cancer Precision Drug Discovery,2019 Nov 11;2019:8427042.,"Artificial intelligence (AI) proves to have enormous potential in many areas of healthcare including research and chemical discoveries. Using large amounts of aggregated data, the AI can discover and learn further transforming these data into ""usable"" knowledge. Being well aware of this, the world's leading pharmaceutical companies have already begun to use artificial intelligence to improve their research regarding new drugs. The goal is to exploit modern computational biology and machine learning systems to predict the molecular behaviour and the likelihood of getting a useful drug, thus saving time and money on unnecessary tests. Clinical studies, electronic medical records, high-resolution medical images, and genomic profiles can be used as resources to aid drug development. Pharmaceutical and medical researchers have extensive data sets that can be analyzed by strong AI systems. This review focused on how computational biology and artificial intelligence technologies can be implemented by integrating the knowledge of cancer drugs, drug resistance, next-generation sequencing, genetic variants, and structural biology in the cancer precision drug discovery."
31885533,1.0,Dragonfly Algorithm and Its Applications in Applied Science Survey,2019 Dec 6;2019:9293617.,"One of the most recently developed heuristic optimization algorithms is dragonfly by Mirjalili. Dragonfly algorithm has shown its ability to optimizing different real-world problems. It has three variants. In this work, an overview of the algorithm and its variants is presented. Moreover, the hybridization versions of the algorithm are discussed. Furthermore, the results of the applications that utilized the dragonfly algorithm in applied science are offered in the following area: machine learning, image processing, wireless, and networking. It is then compared with some other metaheuristic algorithms. In addition, the algorithm is tested on the CEC-C06 2019 benchmark functions. The results prove that the algorithm has great exploration ability and its convergence rate is better than the other algorithms in the literature, such as PSO and GA. In general, in this survey, the strong and weak points of the algorithm are discussed. Furthermore, some future works that will help in improving the algorithm's weak points are recommended. This study is conducted with the hope of offering beneficial information about dragonfly algorithm to the researchers who want to study the algorithm."
31884734,3.0,Medical Big Data Is Not Yet Available: Why We Need Realism Rather than Exaggeration,2019 Dec;34(4):349-354.,"Most people are now familiar with the concepts of big data, deep learning, machine learning, and artificial intelligence (AI) and have a vague expectation that AI using medical big data can be used to improve the quality of medical care. However, the expectation that big data could change the field of medicine is inconsistent with the current reality. The clinical meaningfulness of the results of research using medical big data needs to be examined. Medical staff needs to be clear about the purpose of AI that utilizes medical big data and to focus on the quality of this data, rather than the quantity. Further, medical professionals should understand the necessary precautions for using medical big data, as well as its advantages. No doubt that someday, medical big data will play an essential role in healthcare; however, at present, it seems too early to actively use it in clinical practice. The field continues to work toward developing medical big data and making it appropriate for healthcare. Researchers should continue to engage in empirical research to ensure that appropriate processes are in place to empirically evaluate the results of its use in healthcare."
31884065,4.0,Machine Learning Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology: A Narrative Review,2020 Mar;51(1):182-193.,"Histopathology is a method used for breast cancer diagnosis. Machine learning (ML) methods have achieved success for supervised learning tasks in the medical domain. In this article, we investigate the impact of ML for the diagnosis of breast cancer using histopathology images of conventional photomicroscopy. Cancer diagnosis is the identification of images as cancer or noncancer, and this involves image preprocessing, feature extraction, classification, and performance analysis. In this article, different approaches to perform these necessary steps are reviewed. We find that most ML research for breast cancer diagnosis has been focused on deep learning. Based on inferences from the recent research activities, we discuss how ML methods can benefit conventional microscopy-based breast cancer diagnosis. Finally, we discuss the research gaps of ML approaches for the implementation in a real pathology environment and propose future research guidelines."
31883846,5.0,Artificial intelligence approaches using natural language processing to advance EHR-based clinical research,2020 Feb;145(2):463-469.,"The wide adoption of electronic health record systems in health care generates big real-world data that open new venues to conduct clinical research. As a large amount of valuable clinical information is locked in clinical narratives, natural language processing techniques as an artificial intelligence approach have been leveraged to extract information from clinical narratives in electronic health records. This capability of natural language processing potentially enables automated chart review for identifying patients with distinctive clinical characteristics in clinical care and reduces methodological heterogeneity in defining phenotype, obscuring biological heterogeneity in research concerning allergy, asthma, and immunology. This brief review discusses the current literature on the secondary use of electronic health record data for clinical research concerning allergy, asthma, and immunology and highlights the potential, challenges, and implications of natural language processing techniques."
31883552,5.0,De novo Molecular Design with Generative Long Short-term Memory,2019 Dec 18;73(12):1006-1011.,"Drug discovery benefits from computational models aiding the identification of new chemical matter with bespoke properties. The field of de novo drug design has been particularly revitalized by adaptation of generative machine learning models from the field of natural language processing. These deep neural network models are trained on recognizing molecular structures and generate new molecular entities without relying on pre-determined sets of molecular building blocks and chemical transformations for virtual molecule construction. Implicit representation of chemical knowledge provides an alternative to formulating the molecular design task in terms of the established, explicit chemical vocabulary. Here, we review de novo molecular design approaches from the field of 'artificial intelligence', focusing on instances of deep generative models, and highlight the prospective application of long short-term memory models to hit and lead finding in medicinal chemistry."
31881663,5.0,Applications and Trends of Machine Learning in Genomics and Phenomics for Next-Generation Breeding,2019 Dec 25;9(1):34.,"Crops are the major source of food supply and raw materials for the processing industry. A balance between crop production and food consumption is continually threatened by plant diseases and adverse environmental conditions. This leads to serious losses every year and results in food shortages, particularly in developing countries. Presently, cutting-edge technologies for genome sequencing and phenotyping of crops combined with progress in computational sciences are leading a revolution in plant breeding, boosting the identification of the genetic basis of traits at a precision never reached before. In this frame, machine learning (ML) plays a pivotal role in data-mining and analysis, providing relevant information for decision-making towards achieving breeding targets. To this end, we summarize the recent progress in next-generation sequencing and the role of phenotyping technologies in genomics-assisted breeding toward the exploitation of the natural variation and the identification of target genes. We also explore the application of ML in managing big data and predictive models, reporting a case study using microRNAs (miRNAs) to identify genes related to stress conditions."
31881449,4.0,Machine learning for protein folding and dynamics,2020 Feb;60:77-84.,"Many aspects of the study of protein folding and dynamics have been affected by the recent advances in machine learning. Methods for the prediction of protein structures from their sequences are now heavily based on machine learning tools. The way simulations are performed to explore the energy landscape of protein systems is also changing as force-fields are started to be designed by means of machine learning methods. These methods are also used to extract the essential information from large simulation datasets and to enhance the sampling of rare events such as folding/unfolding transitions. While significant challenges still need to be tackled, we expect these methods to play an important role on the study of protein folding and dynamics in the near future. We discuss here the recent advances on all these fronts and the questions that need to be addressed for machine learning approaches to become mainstream in protein simulation."
31880240,,Computational Models for Self-Interacting Proteins Prediction,2020;27(5):392-399.,"Self-Interacting Proteins (SIPs), whose two or more copies can interact with each other, have significant roles in cellular functions and evolution of Protein Interaction Networks (PINs). Knowing whether a protein can act on itself is important to understand its functions. Previous studies on SIPs have focused on their structures and functions, while their whole properties are less emphasized. Not surprisingly, identifying SIPs is one of the most important works in biomedical research, which will help to understanding the function and mechanism of proteins. It is worth noting that high throughput methods can be used for SIPs prediction, but can be costly, time consuming and challenging. Therefore, it is urgent to design computational models for the identification of SIPs. In this review, the concept and function of SIPs were introduced in detail. We further introduced SIPs data and some excellent computational models that have been designed for SIPs prediction. Specially, the most existing approaches were developed based on machine learning through carrying out different extract feature methods. Finally, we discussed several difficult problems in developing computational models for SIPs prediction."
31878333,3.0,Advances in Structure Modeling Methods for Cryo-Electron Microscopy Maps,2019 Dec 24;25(1):82.,"Cryo-electron microscopy (cryo-EM) has now become a widely used technique for structure determination of macromolecular complexes. For modeling molecular structures from density maps of different resolutions, many algorithms have been developed. These algorithms can be categorized into rigid fitting, flexible fitting, and de novo modeling methods. It is also observed that machine learning (ML) techniques have been increasingly applied following the rapid progress of the ML field. Here, we review these different categories of macromolecule structure modeling methods and discuss their advances over time."
31878065,3.0,Tackling Faults in the Industry 4.0 Era-A Survey of Machine-Learning Solutions and Key Aspects,2019 Dec 23;20(1):109.,"The recent advancements in the fields of artificial intelligence (AI) and machine learning (ML) have affected several research fields, leading to improvements that could not have been possible with conventional optimization techniques. Among the sectors where AI/ML enables a plethora of opportunities, industrial manufacturing can expect significant gains from the increased process automation. At the same time, the introduction of the Industrial Internet of Things (IIoT), providing improved wireless connectivity for real-time manufacturing data collection and processing, has resulted in the culmination of the fourth industrial revolution, also known as Industry 4.0. In this survey, we focus on the vital processes of fault detection, prediction and prevention in Industry 4.0 and present recent developments in ML-based solutions. We start by examining various proposed cloud/fog/edge architectures, highlighting their importance for acquiring manufacturing data in order to train the ML algorithms. In addition, as faults might also occur from sources beyond machine degradation, the potential of ML in safeguarding cyber-security is thoroughly discussed. Moreover, a major concern in the Industry 4.0 ecosystem is the role of human operators and workers. Towards this end, a detailed overview of ML-based human-machine interaction techniques is provided, allowing humans to be in-the-loop of the manufacturing processes in a symbiotic manner with minimal errors. Finally, open issues in these relevant fields are given, stimulating further research."
31876546,5.0,Computational analysis of flow cytometry data in hematological malignancies: future clinical practice?,2020 Mar;32(2):162-169.,"Purpose of review:                    This review outlines the advancements that have been made in computational analysis for clinical flow cytometry data in hematological malignancies.              Recent findings:                    In recent years, computational analysis methods have been applied to clinical flow cytometry data of hematological malignancies with promising results. Most studies combined dimension reduction (principle component analysis) or clustering methods (FlowSOM, generalized mixture models) with machine learning classifiers (support vector machines, random forest). For diagnosis and classification of hematological malignancies, many studies have reported results concordant with manual expert analysis, including B-cell chronic lymphoid leukemia detection and acute leukemia classification. Other studies, e.g. concerning diagnosis of myelodysplastic syndromes and classification of lymphoma, have shown to be able to increase diagnostic accuracy. With respect to treatment response monitoring, studies have focused on, for example, computational minimal residual disease detection in multiple myeloma and posttreatment classification of healthy or diseased in acute myeloid leukemia. The results of these studies are encouraging, although accurate relapse prediction remains challenging. To facilitate clinical implementation, collaboration and (prospective) validation in multicenter setting are necessary.              Summary:                    Computational analysis methods for clinical flow cytometry data hold the potential to increase ease of use, objectivity and accuracy in the clinical work-up of hematological malignancies."
33489002,2.0,Machine learning applications in drug development,2019 Dec 26;18:241-252.,"Due to the huge amount of biological and medical data available today, along with well-established machine learning algorithms, the design of largely automated drug development pipelines can now be envisioned. These pipelines may guide, or speed up, drug discovery; provide a better understanding of diseases and associated biological phenomena; help planning preclinical wet-lab experiments, and even future clinical trials. This automation of the drug development process might be key to the current issue of low productivity rate that pharmaceutical companies currently face. In this survey, we will particularly focus on two classes of methods: sequential learning and recommender systems, which are active biomedical fields of research."
31874386,12.0,Recent advances in glycoinformatic platforms for glycomics and glycoproteomics,2020 Jun;62:56-69.,"Protein glycosylation is the most complex and prevalent post-translation modification in terms of the number of proteins modified and the diversity generated. To understand the functional roles of glycoproteins it is important to gain an insight into the repertoire of oligosaccharides present. The comparison and relative quantitation of glycoforms combined with site-specific identification and occupancy are necessary steps in this direction. Computational platforms have continued to mature assisting researchers with the interpretation of such glycomics and glycoproteomics data sets, but frequently support dedicated workflows and users rely on the manual interpretation of data to gain insights into the glycoproteome. The growth of site-specific knowledge has also led to the implementation of machine-learning algorithms to predict glycosylation which is now being integrated into glycoproteomics pipelines. This short review describes commercial and open-access databases and software with an emphasis on those that are actively maintained and designed to support current analytical workflows."
31867668,3.0,Machine learning and its applications in plant molecular studies,2020 Jan 22;19(1):40-48.,"The advent of high-throughput genomic technologies has resulted in the accumulation of massive amounts of genomic information. However, biologists are challenged with how to effectively analyze these data. Machine learning can provide tools for better and more efficient data analysis. Unfortunately, because many plant biologists are unfamiliar with machine learning, its application in plant molecular studies has been restricted to a few species and a limited set of algorithms. Thus, in this study, we provide the basic steps for developing machine learning frameworks and present a comprehensive overview of machine learning algorithms and various evaluation metrics. Furthermore, we introduce sources of important curated plant genomic data and R packages to enable plant biologists to easily and quickly apply appropriate machine learning algorithms in their research. Finally, we discuss current applications of machine learning algorithms for identifying various genes related to resistance to biotic and abiotic stress. Broad application of machine learning and the accumulation of plant sequencing data will advance plant molecular studies."
31863465,2.0,Beyond the Randomized Clinical Trial: Innovative Data Science to Close the Pediatric Evidence Gap,2020 Apr;107(4):786-795.,"Despite the application of advanced statistical and pharmacometric approaches to pediatric trial data, a large pediatric evidence gap still remains. Here, we discuss how to collect more data from children by using real-world data from electronic health records, mobile applications, wearables, and social media. The large datasets collected with these approaches enable and may demand the use of artificial intelligence and machine learning to allow the data to be analyzed for decision making. Applications of this approach are presented, which include the prediction of future clinical complications, medical image analysis, identification of new pediatric end points and biomarkers, the prediction of treatment nonresponders, and the prediction of placebo-responders for trial enrichment. Finally, we discuss how to bring machine learning from science to pediatric clinical practice. We conclude that advantage should be taken of the current opportunities offered by innovations in data science and machine learning to close the pediatric evidence gap."
31861734,2.0,"Convolutional-Neural Network-Based Image Crowd Counting: Review, Categorization, Analysis, and Performance Evaluation",2019 Dec 19;20(1):43.,"Traditional handcrafted crowd-counting techniques in an image are currently transformed via machine-learning and artificial-intelligence techniques into intelligent crowd-counting techniques. This paradigm shift offers many advanced features in terms of adaptive monitoring and the control of dynamic crowd gatherings. Adaptive monitoring, identification/recognition, and the management of diverse crowd gatherings can improve many crowd-management-related tasks in terms of efficiency, capacity, reliability, and safety. Despite many challenges, such as occlusion, clutter, and irregular object distribution and nonuniform object scale, convolutional neural networks are a promising technology for intelligent image crowd counting and analysis. In this article, we review, categorize, analyze (limitations and distinctive features), and provide a detailed performance evaluation of the latest convolutional-neural-network-based crowd-counting techniques. We also highlight the potential applications of convolutional-neural-network-based crowd-counting techniques. Finally, we conclude this article by presenting our key observations, providing strong foundation for future research directions while designing convolutional-neural-network-based crowd-counting techniques. Further, the article discusses new advancements toward understanding crowd counting in smart cities using the Internet of Things (IoT)."
31861476,3.0,Development Trends and Perspectives of Future Sensors and MEMS/NEMS,2019 Dec 18;11(1):7.,"With the fast development of the fifth-generation cellular network technology (5G), the future sensors and microelectromechanical systems (MEMS)/nanoelectromechanical systems (NEMS) are presenting a more and more critical role to provide information in our daily life. This review paper introduces the development trends and perspectives of the future sensors and MEMS/NEMS. Starting from the issues of the MEMS fabrication, we introduced typical MEMS sensors for their applications in the Internet of Things (IoTs), such as MEMS physical sensor, MEMS acoustic sensor, and MEMS gas sensor. Toward the trends in intelligence and less power consumption, MEMS components including MEMS/NEMS switch, piezoelectric micromachined ultrasonic transducer (PMUT), and MEMS energy harvesting were investigated to assist the future sensors, such as event-based or almost zero-power. Furthermore, MEMS rigid substrate toward NEMS flexible-based for flexibility and interface was discussed as another important development trend for next-generation wearable or multi-functional sensors. Around the issues about the big data and human-machine realization for human beings' manipulation, artificial intelligence (AI) and virtual reality (VR) technologies were finally realized using sensor nodes and its wave identification as future trends for various scenarios."
31861438,1.0,Knowledge Generation with Rule Induction in Cancer Omics,2019 Dec 18;21(1):18.,"The explosion of omics data availability in cancer research has boosted the knowledge of the molecular basis of cancer, although the strategies for its definitive resolution are still not well established. The complexity of cancer biology, given by the high heterogeneity of cancer cells, leads to the development of pharmacoresistance for many patients, hampering the efficacy of therapeutic approaches. Machine learning techniques have been implemented to extract knowledge from cancer omics data in order to address fundamental issues in cancer research, as well as the classification of clinically relevant sub-groups of patients and for the identification of biomarkers for disease risk and prognosis. Rule induction algorithms are a group of pattern discovery approaches that represents discovered relationships in the form of human readable associative rules. The application of such techniques to the modern plethora of collected cancer omics data can effectively boost our understanding of cancer-related mechanisms. In fact, the capability of these methods to extract a huge amount of human readable knowledge will eventually help to uncover unknown relationships between molecular attributes and the malignant phenotype. In this review, we describe applications and strategies for the usage of rule induction approaches in cancer omics data analysis. In particular, we explore the canonical applications and the future challenges and opportunities posed by multi-omics integration problems."
31853838,1.0,Advancing neuro-oncology of glial tumors from big data and multidisciplinary studies,2020 Jan;146(1):1-7.,"Introduction:                    Multidisciplinary studies for glial tumors has produced an enormous amount of information including imaging, histology, and a large cohort of molecular data (i.e. genomics, epigenomics, metabolomics, proteomics, etc.). The big data resources are made possible through open access that offers great potential for new biomarker or therapeutic intervention via deep-learning and/or machine learning for integrated multi-omics analysis. An equally important effort to define the hallmarks of glial tumors will also advance precision neuro-oncology and inform patient-specific therapeutics. This review summarizes past studies regarding tumor classification, hallmarks of cancer, and hypothetical mechanisms. Leveraging on advanced big data approaches and ongoing cross-disciplinary endeavors, this review also discusses how to integrate multiple layers of big data toward the goal of precision medicine.              Results:                    In addition to basic research of cancer biology, the results from integrated multi-omics analysis will highlight biological processes and potential candidates as biomarkers or therapeutic targets. Ultimately, these collective resources built upon an armamentarium of accessible data can re-form clinical and molecular data to stratify patient-tailored therapy.              Conclusion:                    We envision that a comprehensive understanding of the link between molecular signatures, tumor locations, and patients' history will identify a molecular taxonomy of glial tumors to advance the improvements in early diagnosis, prevention, and treatment."
31850970,1.0,Molecular prediction of metastasis in cutaneous squamous cell carcinoma,2020 Mar;32(2):129-136.,"Purpose of review:                    Cutaneous squamous cell carcinoma (cSCC) is a highly prevalent malignancy frequently occurring on body surfaces chronically exposed to ultraviolet radiation. While a large majority of tumors remain localized to the skin and immediate subcutaneous tissue and are cured with surgical excision, a small subset of patients with cSCC will develop metastatic disease. Risk stratification for cSCC is performed using clinical staging systems, but given a high mutational burden and advances in targeted and immunotherapy, there is growing interest in molecular predictors of high-risk disease.              Recent findings:                    Recent literature on the risk for metastasis in cSCC includes notable findings in genes involved in cell-cycle regulation, tumor suppression, tissue invasion and microenvironment, interactions with the host-immune system, and epigenetic regulation.              Summary:                    cSCC is a highly mutated tumor with complex carcinogenesis. Regulators of tumor growth and local invasion are numerous and increasingly well-understood but drivers of metastasis are less established. Areas of importance include central system regulators (NOTCH, miRNAs), proteins involved in tissue invasion (podoplanin, E-cadherin), and targets of existing and emerging therapeutics (PD-1, epidermal growth factor receptor). Given the complexity of cSCC carcinogenesis, the use of machine learning algorithms and computational genomics may provide ultimate insight and prospective studies are needed to verify clinical relevance."
31850639,,Periodontics in the USA: An introduction,2020 Feb;82(1):9-11.,"The United States continues to be an incubator for new concepts and approaches to the diagnosis, treatment, and prevention of periodontal diseases. This volume of Periodontology 2000 presents some of these newer areas of research and paradigms that have emerged in the United States from both long-established and new investigators. These areas include: (1) more comprehensive approaches to assessing the total periodontal microbiome, including bacteria, viruses, and fungi, and their interactions with both the local and systemic inflammatory and immune responses, as well as with other oral and systemic conditions and diseases; (2) new developments for a more comprehensive characterization of the patient genome, transcriptome, and proteome profiles and the role of these profiles in periodontal disease pathogenesis; (3) new developments in nonsurgical approaches to periodontal diseases, including broad-based lines of attack using natural antimicrobials and host-modulation therapies and more focused approaches that target specific interactions in the host response; and (4) new big data analysis, machine learning, and imaging approaches, both for understanding the pathogenesis of periodontal diseases and for developing improved risk-assessment tools and better treatment outcomes."
31850202,3.0,Study Progress of Radiomics With Machine Learning for Precision Medicine in Bladder Cancer Management,2019 Nov 28;9:1296.,"Bladder cancer is a fatal cancer that happens in the genitourinary tract with quite high morbidity and mortality annually. The high level of recurrence rate ranging from 50 to 80% makes bladder cancer one of the most challenging and costly diseases to manage. Faced with various problems in existing methods, a recently emerging concept for the measurement of imaging biomarkers and extraction of quantitative features called ""radiomics"" shows great potential in the application of detection, grading, and follow-up management of bladder cancer. Furthermore, machine-learning (ML) algorithms on the basis of ""big data"" are fueling the powers of radiomics for bladder cancer monitoring in the era of precision medicine. Currently, the usefulness of the novel combination of radiomics and ML has been demonstrated by a large number of successful cases. It possesses outstanding strengths including non-invasiveness, low cost, and high efficiency, which may serve as a revolution to tumor assessment and emancipate workforce. However, for the extensive clinical application in the future, more efforts should be made to break down the limitations caused by technology deficiencies, inherent problems during the process of radiomic analysis, as well as the quality of present studies."
31849692,1.0,QuantiMus: A Machine Learning-Based Approach for High Precision Analysis of Skeletal Muscle Morphology,2019 Nov 29;10:1416.,"Skeletal muscle injury provokes a regenerative response, characterized by the de novo generation of myofibers that are distinguished by central nucleation and re-expression of developmentally restricted genes. In addition to these characteristics, myofiber cross-sectional area (CSA) is widely used to evaluate muscle hypertrophic and regenerative responses. Here, we introduce QuantiMus, a free software program that uses machine learning algorithms to quantify muscle morphology and molecular features with high precision and quick processing-time. The ability of QuantiMus to define and measure myofibers was compared to manual measurement or other automated software programs. QuantiMus rapidly and accurately defined total myofibers and measured CSA with comparable performance but quantified the CSA of centrally-nucleated fibers (CNFs) with greater precision compared to other software. It additionally quantified the fluorescence intensity of individual myofibers of human and mouse muscle, which was used to assess the distribution of myofiber type, based on the myosin heavy chain isoform that was expressed. Furthermore, analysis of entire quadriceps cross-sections of healthy and mdx mice showed that dystrophic muscle had an increased frequency of Evans blue dye+ injured myofibers. QuantiMus also revealed that the proportion of centrally nucleated, regenerating myofibers that express embryonic myosin heavy chain (eMyHC) or neural cell adhesion molecule (NCAM) were increased in dystrophic mice. Our findings reveal that QuantiMus has several advantages over existing software. The unique self-learning capacity of the machine learning algorithms provides superior accuracy and the ability to rapidly interrogate the complete muscle section. These qualities increase rigor and reproducibility by avoiding methods that rely on the sampling of representative areas of a section. This is of particular importance for the analysis of dystrophic muscle given the ""patchy"" distribution of muscle pathology. QuantiMus is an open source tool, allowing customization to meet investigator-specific needs and provides novel analytical approaches for quantifying muscle morphology."
31846526,2.0,"The coming 15 years in gynaecological pathology: digitisation, artificial intelligence, and new technologies",2020 Jan;76(1):171-177.,"Surgical pathology forms the cornerstone of modern oncological medicine, owing to the wealth of clinically relevant information that can be obtained from tissue morphology. Although several ancillary testing modalities have been added to surgical pathology, the way in which we view and interpret tissue morphology has remained largely unchanged since the inception of our profession. In this review, we discuss new technological advances that promise to transform the way in which we access tissue morphology and how we use it to guide patient care."
31845543,,"Artificial intelligence, machine learning and the pediatric airway",2020 Mar;30(3):264-268.,"Artificial intelligence and machine learning are rapidly expanding fields with increasing relevance in anesthesia and, in particular, airway management. The ability of artificial intelligence and machine learning algorithms to recognize patterns from large volumes of complex data makes them attractive for use in pediatric anesthesia airway management. The purpose of this review is to introduce artificial intelligence, machine learning, and deep learning to the pediatric anesthesiologist. Current evidence and developments in artificial intelligence, machine learning, and deep learning relevant to pediatric airway management are presented. We critically assess the current evidence on the use of artificial intelligence and machine learning in the assessment, diagnosis, monitoring, procedure assistance, and predicting outcomes during pediatric airway management. Further, we discuss the limitations of these technologies and offer areas for focused research that may bring pediatric airway management anesthesiology into the era of artificial intelligence and machine learning."
31845078,5.0,Landscape of HIV Implementation Research Funded by the National Institutes of Health: A Mapping Review of Project Abstracts,2020 Jun;24(6):1903-1911.,"In 2019, the requisite biomedical and behavioral interventions to eliminate new HIV infections exist. ""Ending the HIV Epidemic"" now becomes primarily a challenge of will and implementation. This review maps the extent to which implementation research (IR) has been integrated into HIV research by reviewing the recent funding portfolio of the NIH. We searched NIH RePORTER for HIV and IR-related research projects funded from January 2013 to March 2018. The 4629 unique studies identified were screened using machine learning and manual methods. 216 abstracts met the eligibility criteria of HIV and IR. Key study characteristics were then abstracted. NIH currently funds HIV studies that are either formally IR (n = 109) or preparatory for IR (n = 107). Few (13%) projects mentioned a guiding implementation model, theory, or framework, and only 56% of all studies explicitly mentioned measuring an implementation outcome. Considering the study aims along an IR continuum, 18 (8%) studies examined barriers and facilitators, 43 (20%) developed implementation strategies, 46 (21%) piloted strategies, 73 (34%) tested a single strategy, and 35 (16%) compared strategies. A higher proportion of formal IR projects involved established interventions (e.g., integrated services) compared to newer interventions (e.g., pre-exposure prophylaxis). Prioritizing HIV-related IR in NIH and other federal funding opportunity announcements and expanded training in implementation science could have a substantial impact on ending the HIV pandemic. This review serves as a baseline by which to compare funding patterns and the sophistication of IR in HIV research over time."
31841881,12.0,Artificial intelligence applications for thoracic imaging,2020 Feb;123:108774.,"Artificial intelligence is a hot topic in medical imaging. The development of deep learning methods and in particular the use of convolutional neural networks (CNNs), have led to substantial performance gain over the classic machine learning techniques. Multiple usages are currently being evaluated, especially for thoracic imaging, such as such as lung nodule evaluation, tuberculosis or pneumonia detection or quantification of diffuse lung diseases. Chest radiography is a near perfect domain for the development of deep learning algorithms for automatic interpretation, requiring large annotated datasets, in view of the high number of procedures and increasing data availability. Current algorithms are able to detect up to 14 common anomalies, when present as isolated findings. Chest computed tomography is another major field of application for artificial intelligence, especially in the perspective of large scale lung cancer screening. It is important for radiologists to apprehend, contribute actively and lead this new era of radiology powered by artificial intelligence. Such a perspective requires understanding new terms and concepts associated with machine learning. The objective of this paper is to provide useful definitions for understanding the methods used and their possibilities, and report current and future developments for thoracic imaging. Prospective validation of AI tools will be required before reaching routine clinical implementation."
31840577,2.0,Studying language in context using the temporal generalization method,2020 Feb 3;375(1791):20180531.,"The temporal generalization method (TGM) is a data analysis technique that can be used to test if the brain's representation for particular stimuli (e.g. sounds, images) is maintained, or if it changes as a function of time (King J-R, Dehaene S. 2014 Characterizing the dynamics of mental representations: the temporal generalization method. Trends Cogn. Sci. 18, 203-210. (doi:10.1016/j.tics.2014.01.002)). The TGM involves training models to predict the stimuli or condition using a time window from a recording of brain activity, and testing the resulting models at all possible time windows. This is repeated for all possible training windows to create a full matrix of accuracy for every combination of train/test window. The results of a TGM indicate when brain activity patterns are consistent (i.e. the trained model performs well even when tested on a different time window), and when they are inconsistent, allowing us to track neural representations over time. The TGM has been used to study the representation of images and sounds during a variety of tasks, but has been less readily applied to studies of language. Here, we give an overview of the method itself, discuss how the TGM has been used to analyse two studies of language in context and explore how the TGM could be applied to further our understanding of semantic composition. This article is part of the theme issue 'Towards mechanistic models of meaning composition'."
31839552,6.0,Voice patterns in schizophrenia: A systematic review and Bayesian meta-analysis,2020 Feb;216:24-40.,"Voice atypicalities have been a characteristic feature of schizophrenia since its first definitions. They are often associated with core negative symptoms such as flat affect and alogia, and with the social impairments seen in the disorder. This suggests that voice atypicalities may represent a marker of clinical features and social functioning in schizophrenia. We systematically reviewed and meta-analyzed the evidence for distinctive acoustic patterns in schizophrenia, as well as their relation to clinical features. We identified 46 articles, including 55 studies with a total of 1254 patients with schizophrenia and 699 healthy controls. Summary effect sizes (Hedges'g and Pearson's r) estimates were calculated using multilevel Bayesian modeling. We identified weak atypicalities in pitch variability (g = -0.55) related to flat affect, and stronger atypicalities in proportion of spoken time, speech rate, and pauses (g's between -0.75 and -1.89) related to alogia and flat affect. However, the effects were mostly modest (with the important exception of pause duration) compared to perceptual and clinical judgments, and characterized by large heterogeneity between studies. Moderator analyses revealed that tasks with a more demanding cognitive and social component showed larger effects both in contrasting patients and controls and in assessing symptomatology. In conclusion, studies of acoustic patterns are a promising but, yet unsystematic avenue for establishing markers of schizophrenia. We outline recommendations towards more cumulative, open, and theory-driven research."
31838165,4.0,Autoantigenomics: Holistic characterization of autoantigen repertoires for a better understanding of autoimmune diseases,2020 Feb;19(2):102450.,"Autoimmune diseases are mostly characterized by autoantibodies in the patients' serum or cerebrospinal fluid, representing diagnostic or prognostic biomarkers. For decades, research has focused on single autoantigens or panels of single autoantigens. In this article, we advocate to broaden the focus by addressing the entire autoantigen repertoire in a systemic ""omics-like"" way. This approach aims to capture the enormous biodiversity in the sets of targeted antigens and pave the way toward a more holistic understanding of the concerted character of antibody-related humoral immune responses. Ongoing technological progress permits high-throughput screenings of thousands of autoantigens in parallel, e.g., via protein microarrays, phage display, or immunoprecipitation with mass spectrometry. We argue that the time is right for combining omics and autoantibody screening approaches into ""autoantigenomics"" as a novel omics subcategory. In this article, we introduce the concept of autoantigenomics, describe its roots and application options, and demarcate the method from related holistic approaches such as systems serology or immune-related transcriptomics and proteomics. We suggest the following extendable method set to be applied to autoantigen repertoires: (1) principal component analysis, (2) hierarchical cluster analysis, (3) partial least-square discriminant analysis or orthogonal projections to latent structures discriminant analysis, (4) analysis of the repertoire sizes in disease groups and clinical subgroups, (5) overrepresentation analyses using databases like those of Gene Ontology, Reactome Pathway, or DisGeNET, (6) analysis of pathways that are significantly targeted by specific repertoires, and (7) machine learning approaches. In an unsupervised way, these methods can identify clusters of autoantigens sharing certain functional or spatial properties, or clusters of patients comprising clinical subgroups potentially useful for patient stratification. In a supervised way, these methods can lead to prediction models that may eventually assist diagnosis and prognosis. The untargeted autoantigenomics approach allows for the systematic survey of antibody-related humoral immune responses. This may enhance our understanding of autoimmune diseases in a more comprehensive way compared to current single or panel autoantibodies approaches."
31833725,9.0,Artificial intelligence and neural networks in urology: current clinical applications,2020 Feb;72(1):49-57.,"Introduction:                    As we enter the era of ""big data,"" an increasing amount of complex health-care data will become available. These data are often redundant, ""noisy,"" and characterized by wide variability. In order to offer a precise and transversal view of a clinical scenario the artificial intelligence (AI) with machine learning (ML) algorithms and Artificial neuron networks (ANNs) process were adopted, with a promising wide diffusion in the near future. The present work aims to provide a comprehensive and critical overview of the current and potential applications of AI and ANNs in urology.              Evidence acquisition:                    A non-systematic review of the literature was performed by screening Medline, PubMed, the Cochrane Database, and Embase to detect pertinent studies regarding the application of AI and ANN in Urology.              Evidence synthesis:                    The main application of AI in urology is the field of genitourinary cancers. Focusing on prostate cancer, AI was applied for the prediction of prostate biopsy results. For bladder cancer, the prediction of recurrence-free probability and diagnostic evaluation were analysed with ML algorithms. For kidney and testis cancer, anecdotal experiences were reported for staging and prediction of diseases recurrence. More recently, AI has been applied in non-oncological diseases like stones and functional urology.              Conclusions:                    AI technologies are growing their role in health care; but, up to now, their ""real-life"" implementation remains limited. However, in the near future, the potential of AI-driven era could change the clinical practice in Urology, improving overall patient outcomes."
31833636,,Density Functional Theory as a Data Science,2020 Jul;20(7):618-639.,"The development of density functional theory (DFT) functionals and physical corrections are reviewed focusing on the physical meanings and the semiempirical parameters from the viewpoint of data science. This review shows that DFT exchange-correlation functionals have been developed under many strict physical conditions with minimizing the number of the semiempirical parameters, except for some recent functionals. Major physical corrections for exchange-correlation function- als are also shown to have clear physical meanings independent of the functionals, though they inevitably require minimum semiempirical parameters dependent on the functionals combined. We, therefore, interpret that DFT functionals with physical corrections are the most sophisticated target functions that are physically legitimated, even from the viewpoint of data science."
31832112,39.0,Chronic inflammation: key player and biomarker-set to predict and prevent cancer development and progression based on individualized patient profiles,2019 Nov 20;10(4):365-381.,"A strong relationship exists between tumor and inflammation, which is the hot point in cancer research. Inflammation can promote the occurrence and development of cancer by promoting blood vessel growth, cancer cell proliferation, and tumor invasiveness, negatively regulating immune response, and changing the efficacy of certain anti-tumor drugs. It has been demonstrated that there are a large number of inflammatory factors and inflammatory cells in the tumor microenvironment, and tumor-promoting immunity and anti-tumor immunity exist simultaneously in the tumor microenvironment. The typical relationship between chronic inflammation and tumor has been presented by the relationships between Helicobacter pylori, chronic gastritis, and gastric cancer; between smoking, development of chronic pneumonia, and lung cancer; and between hepatitis virus (mainly hepatitis virus B and C), development of chronic hepatitis, and liver cancer. The prevention of chronic inflammation is a factor that can prevent cancer, so it effectively inhibits or blocks the occurrence, development, and progression of the chronic inflammation process playing important roles in the prevention of cancer. Monitoring of the causes and inflammatory factors in chronic inflammation processes is a useful way to predict cancer and assess the efficiency of cancer prevention. Chronic inflammation-based biomarkers are useful tools to predict and prevent cancer."
31831878,8.0,Drug repurposing to improve treatment of rheumatic autoimmune inflammatory diseases,2020 Jan;16(1):32-52.,"The past century has been characterized by intensive efforts, within both academia and the pharmaceutical industry, to introduce new treatments to individuals with rheumatic autoimmune inflammatory diseases (RAIDs), often by 'borrowing' treatments already employed in one RAID or previously used in an entirely different disease, a concept known as drug repurposing. However, despite sharing some clinical manifestations and immune dysregulation, disease pathogenesis and phenotype vary greatly among RAIDs, and limited understanding of their aetiology has made repurposing drugs for RAIDs challenging. Nevertheless, the past century has been characterized by different 'waves' of repurposing. Early drug repurposing occurred in academia and was based on serendipitous observations or perceived disease similarity, often driven by the availability and popularity of drug classes. Since the 1990s, most biologic therapies have been developed for one or several RAIDs and then tested among the others, with varying levels of success. The past two decades have seen data-driven repurposing characterized by signature-based approaches that rely on molecular biology and genomics. Additionally, many data-driven strategies employ computational modelling and machine learning to integrate multiple sources of data. Together, these repurposing periods have led to advances in the treatment for many RAIDs."
31830558,11.0,Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges,2020 Feb 28;471:61-71.,"Cancer is an aggressive disease with a low median survival rate. Ironically, the treatment process is long and very costly due to its high recurrence and mortality rates. Accurate early diagnosis and prognosis prediction of cancer are essential to enhance the patient's survival rate. Developments in statistics and computer engineering over the years have encouraged many scientists to apply computational methods such as multivariate statistical analysis to analyze the prognosis of the disease, and the accuracy of such analyses is significantly higher than that of empirical predictions. Furthermore, as artificial intelligence (AI), especially machine learning and deep learning, has found popular applications in clinical cancer research in recent years, cancer prediction performance has reached new heights. This article reviews the literature on the application of AI to cancer diagnosis and prognosis, and summarizes its advantages. We explore how AI assists cancer diagnosis and prognosis, specifically with regard to its unprecedented accuracy, which is even higher than that of general statistical applications in oncology. We also demonstrate ways in which these methods are advancing the field. Finally, opportunities and challenges in the clinical implementation of AI are discussed. Hence, this article provides a new perspective on how AI technology can help improve cancer diagnosis and prognosis, and continue improving human health in the future."
31824952,15.0,Deep Learning for Whole Slide Image Analysis: An Overview,2019 Nov 22;6:264.,"The widespread adoption of whole slide imaging has increased the demand for effective and efficient gigapixel image analysis. Deep learning is at the forefront of computer vision, showcasing significant improvements over previous methodologies on visual understanding. However, whole slide images have billions of pixels and suffer from high morphological heterogeneity as well as from different types of artifacts. Collectively, these impede the conventional use of deep learning. For the clinical translation of deep learning solutions to become a reality, these challenges need to be addressed. In this paper, we review work on the interdisciplinary attempt of training deep neural networks using whole slide images, and highlight the different ideas underlying these methodologies."
31824921,9.0,Comparison Study of Computational Prediction Tools for Drug-Target Binding Affinities,2019 Nov 20;7:782.,"The drug development is generally arduous, costly, and success rates are low. Thus, the identification of drug-target interactions (DTIs) has become a crucial step in early stages of drug discovery. Consequently, developing computational approaches capable of identifying potential DTIs with minimum error rate are increasingly being pursued. These computational approaches aim to narrow down the search space for novel DTIs and shed light on drug functioning context. Most methods developed to date use binary classification to predict if the interaction between a drug and its target exists or not. However, it is more informative but also more challenging to predict the strength of the binding between a drug and its target. If that strength is not sufficiently strong, such DTI may not be useful. Therefore, the methods developed to predict drug-target binding affinities (DTBA) are of great value. In this study, we provide a comprehensive overview of the existing methods that predict DTBA. We focus on the methods developed using artificial intelligence (AI), machine learning (ML), and deep learning (DL) approaches, as well as related benchmark datasets and databases. Furthermore, guidance and recommendations are provided that cover the gaps and directions of the upcoming work in this research area. To the best of our knowledge, this is the first comprehensive comparison analysis of tools focused on DTBA with reference to AI/ML/DL."
31823128,4.0,Ethical considerations about artificial intelligence for prognostication in intensive care,2019 Dec 10;7(1):70.,"Background:                    Prognosticating the course of diseases to inform decision-making is a key component of intensive care medicine. For several applications in medicine, new methods from the field of artificial intelligence (AI) and machine learning have already outperformed conventional prediction models. Due to their technical characteristics, these methods will present new ethical challenges to the intensivist.              Results:                    In addition to the standards of data stewardship in medicine, the selection of datasets and algorithms to create AI prognostication models must involve extensive scrutiny to avoid biases and, consequently, injustice against individuals or groups of patients. Assessment of these models for compliance with the ethical principles of beneficence and non-maleficence should also include quantification of predictive uncertainty. Respect for patients' autonomy during decision-making requires transparency of the data processing by AI models to explain the predictions derived from these models. Moreover, a system of continuous oversight can help to maintain public trust in this technology. Based on these considerations as well as recent guidelines, we propose a pathway to an ethical implementation of AI-based prognostication. It includes a checklist for new AI models that deals with medical and technical topics as well as patient- and system-centered issues.              Conclusion:                    AI models for prognostication will become valuable tools in intensive care. However, they require technical refinement and a careful implementation according to the standards of medical ethics."
31821153,2.0,"How New Technologies Can Improve Prediction, Assessment, and Intervention in Obsessive-Compulsive Disorder (e-OCD): Review",2019 Dec 10;6(12):e11643.,"Background:                    New technologies are set to profoundly change the way we understand and manage psychiatric disorders, including obsessive-compulsive disorder (OCD). Developments in imaging and biomarkers, along with medical informatics, may well allow for better assessments and interventions in the future. Recent advances in the concept of digital phenotype, which involves using computerized measurement tools to capture the characteristics of a given psychiatric disorder, is one paradigmatic example.              Objective:                    The impact of new technologies on health professionals' practice in OCD care remains to be determined. Recent developments could disrupt not just their clinical practices, but also their beliefs, ethics, and representations, even going so far as to question their professional culture. This study aimed to conduct an extensive review of new technologies in OCD.              Methods:                    We conducted the review by looking for titles in the PubMed database up to December 2017 that contained the following terms: [Obsessive] AND [Smartphone] OR [phone] OR [Internet] OR [Device] OR [Wearable] OR [Mobile] OR [Machine learning] OR [Artificial] OR [Biofeedback] OR [Neurofeedback] OR [Momentary] OR [Computerized] OR [Heart rate variability] OR [actigraphy] OR [actimetry] OR [digital] OR [virtual reality] OR [Tele] OR [video].              Results:                    We analyzed 364 articles, of which 62 were included. Our review was divided into 3 parts: prediction, assessment (including diagnosis, screening, and monitoring), and intervention.              Conclusions:                    The review showed that the place of connected objects, machine learning, and remote monitoring has yet to be defined in OCD. Smartphone assessment apps and the Web Screening Questionnaire demonstrated good sensitivity and adequate specificity for detecting OCD symptoms when compared with a full-length structured clinical interview. The ecological momentary assessment procedure may also represent a worthy addition to the current suite of assessment tools. In the field of intervention, CBT supported by smartphone, internet, or computer may not be more effective than that delivered by a qualified practitioner, but it is easy to use, well accepted by patients, reproducible, and cost-effective. Finally, new technologies are enabling the development of new therapies, including biofeedback and virtual reality, which focus on the learning of coping skills. For them to be used, these tools must be properly explained and tailored to individual physician and patient profiles."
31818386,,School of Block-Review of Blockchain for the Radiologists,2020 Jan;27(1):47-57.,"Blockchain, the underlying technology for Bitcoin, is a distributed digital ledger technology that enables record verification by many independent parties rather than a centralized authority, therefore making it more difficult to tamper with the data. This emerging technology has the potential to enhance various authentication and verification processes in image sharing and data security. It has the potential to promote patient-centered healthcare by giving greater control to patients over their own data. Blockchain can also be utilized for administrative tasks, such as credentialing, claims adjudication, and billing management. It can also be utilized to enhance software supporting research and clinical trials. Blockchain complements artificial intelligence (AI) and these can work synergistically to create better solutions. Although many challenges exist for increased adoption of blockchain within radiology and healthcare in general, it can play a major role in our practice and consequently, it is important for medical imaging professionals to become familiar with the technology."
31818379,1.0,Machine Learning Principles for Radiology Investigators,2020 Jan;27(1):13-25.,"Artificial intelligence and deep learning are areas of high interest for radiology investigators at present. However, the field of machine learning encompasses multiple statistics-based techniques useful for investigators, which may be complementary to deep learning approaches. After a refresher in basic statistical concepts, relevant considerations for machine learning practitioners are reviewed: regression, classification, decision boundaries, and bias-variance tradeoff. Regularization, ground truth, and populations are discussed along with compute and data management principles. Advanced statistical machine learning techniques including bootstrapping, bagging, boosting, decision trees, random forest, XGboost, and support vector machines are reviewed along with relevant examples from the radiology literature."
31815404,,Cheminformatics and Computational Approaches in Metabolomics,,"Metabolomics can be viewed as an evolved form of chemical analysis, which required an early instrumental revolution in which the technological core of spectroscopy and spectrometry was developed. This was followed by the advent of high-throughput and high-performance liquid chromatography, together with the establishment of compound libraries and database systems. The ease in the use of metabolomics platforms was coupled with an implementation of data mining methods and bioinformatics tools using machine learning approaches. Cheminformatics makes use of software packages and tools to convey workflows and to streamline data analysis. On the other hand, computational biology offers the contextual approach to the functional characterization of metabolite profiles from a dataset, providing ontologies and annotations. In this chapter, we discuss the main technical procedures used in metabolomics data acquisition, data processing and pipelines, followed by data mining and statistical approaches including machine learning, and ultimately how metabolomics data can aid in elucidating aberrant pathways and metabolic dysfunctions in disease."
31815401,,Biological Sequence Analysis,,"This chapter focuses on several biological sequence analysis techniques used in computational biology and bioinformatics. The first section provides an overview of biological sequences (nucleic acids and proteins). Bioinformatics helps us understand complex biological problems by investigating similarities and differences that exist at sequence levels in poly-nucleic acids or proteins. Alignment algorithms such as dynamic programming, basic local alignment search tool and HHblits are discussed. Artificial intelligence and machine learning methods have been used successfully in analyzing sequence data and have played an important role in elucidating many biological functions, such as protein functional classification, active site recognition, protein structural features identification, and disease prediction outcomes. This chapter discusses both supervised and unsupervised learning, neural networks, and hidden Markov models. Sequence analysis is incomplete without discussing next-generation sequencing (NGS) data. Deep sequencing is highly important due to its ability to address an increasingly diverse range of biological problems such as the ones encountered in therapeutics. A complete NGS workflow to generate a consensus sequence and haplotypes is discussed."
