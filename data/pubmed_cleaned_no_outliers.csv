,pmid,title,text,date,citations,len_text,len_title,days_live
267,22627698,Extracting biological information with computational analysis of Fourier-transform infrared (FTIR) biospectroscopy datasets: current practices to future perspectives,"Applying Fourier-transform infrared (FTIR) spectroscopy (or related technologies such as Raman spectroscopy) to biological questions (defined as biospectroscopy) is relatively novel. Potential fields of application include cytological, histological and microbial studies. This potentially provides a rapid and non-destructive approach to clinical diagnosis. Its increase in application is primarily a consequence of developing instrumentation along with computational techniques. In the coming decades, biospectroscopy is likely to become a common tool in the screening or diagnostic laboratory, or even in the general practitioner's clinic. Despite many advances in the biological application of FTIR spectroscopy, there remain challenges in sample preparation, instrumentation and data handling. We focus on the latter, where we identify in the reviewed literature, the existence of four main study goals: Pattern Finding; Biomarker Identification; Imaging; and, Diagnosis. These can be grouped into two frameworks: Exploratory; and, Diagnostic. Existing techniques in Quality Control, Pre-processing, Feature Extraction, Clustering, and Classification are critically reviewed. An aspect that is often visited is that of method choice. Based on the state-of-art, we claim that in the near future research should be focused on the challenges of dataset standardization; building information systems; development and validation of data analysis tools; and, technology transfer. A diagnostic case study using a real-world dataset is presented as an illustration. Many of the methods presented in this review are Machine Learning and Statistical techniques that are extendable to other forms of computer-based biomedical analysis, including mass spectrometry and magnetic resonance.",2012-07-01,35,1780,165,3233
259,22952238,Uncovering transcription factor modules using one- and three-dimensional analyses,"Transcriptional regulation is a critical mediator of many normal cellular processes, as well as disease progression. Transcription factors (TFs) often co-localize at cis-regulatory elements on the DNA, form protein complexes, and collaboratively regulate gene expression. Machine learning and Bayesian approaches have been used to identify TF modules in a one-dimensional context. However, recent studies using high throughput technologies have shown that TF interactions should also be considered in three-dimensional nuclear space. Here, we describe methods for identifying TF modules and discuss how moving from a one-dimensional to a three-dimensional paradigm, along with integrated experimental and computational approaches, can lead to a better understanding of TF association networks.",2012-09-01,4,793,81,3171
260,22944687,Understanding the substrate specificity of conventional calpains,"Calpains are intracellular Ca(2+)-dependent Cys proteases that play important roles in a wide range of biological phenomena via the limited proteolysis of their substrates. Genetic defects in calpain genes cause lethality and/or functional deficits in many organisms, including humans. Despite their biological importance, the mechanisms underlying the action of calpains, particularly of their substrate specificities, remain largely unknown. Studies show that certain sequence preferences influence calpain substrate recognition, and some properties of amino acids have been related successfully to substrate specificity and to the calpains' 3D structure. The full spectrum of this substrate specificity, however, has not been clarified using standard sequence analysis algorithms, e.g., the position-specific scoring-matrix method. More advanced bioinformatics techniques were used recently to identify the substrate specificities of calpains and to develop a predictor for calpain cleavage sites, demonstrating the potential of combining empirical data acquisition and machine learning. This review discusses the calpains' substrate specificities, introducing the benefits of bioinformatics applications. In conclusion, machine learning has led to the development of useful predictors for calpain cleavage sites, although the accuracy of the predictions still needs improvement. Machine learning has also elucidated information about the properties of calpains' substrate specificities, including a preference for sequences over secondary structures and the existence of a substrate specificity difference between two similar conventional calpains, which has never been indicated biochemically.",2012-09-01,19,1698,64,3171
292,22075226,Membrane protein structural bioinformatics,"Despite the increasing number of recently solved membrane protein structures, coverage of membrane protein fold space remains relatively sparse. This necessitates the use of computational strategies to investigate membrane protein structure, allowing us to further our understanding of how membrane proteins carry out their diverse range of functions, while aiding the development of novel predictive tools with which to probe uncharacterised folds. Analysis of known structures, the application of machine learning techniques, molecular dynamics simulations and protein structure prediction have enabled significant advances to be made in the field of membrane protein research. In this communication, the key bioinformatic methods that allow the characterisation of membrane proteins are reviewed, the tools available for the structural analysis of membrane proteins are presented and the contribution these tools have made to expanding our understanding of membrane protein structure, function and stability is discussed.",2012-09-01,12,1024,42,3171
257,23012584,The future of medical diagnostics: large digitized databases,"The electronic health record mandate within the American Recovery and Reinvestment Act of 2009 will have a far-reaching affect on medicine. In this article, we provide an in-depth analysis of how this mandate is expected to stimulate the production of large-scale, digitized databases of patient information. There is evidence to suggest that millions of patients and the National Institutes of Health will fully support the mining of such databases to better understand the process of diagnosing patients. This data mining likely will reaffirm and quantify known risk factors for many diagnoses. This quantification may be leveraged to further develop computer-aided diagnostic tools that weigh risk factors and provide decision support for health care providers. We expect that creation of these databases will stimulate the development of computer-aided diagnostic support tools that will become an integral part of modern medicine.",2012-09-01,10,935,60,3171
272,22528508,Paradigm shift in toxicity testing and modeling,"The limitations of traditional toxicity testing characterized by high-cost animal models with low-throughput readouts, inconsistent responses, ethical issues, and extrapolability to humans call for alternative strategies for chemical risk assessment. A new strategy using in vitro human cell-based assays has been designed to identify key toxicity pathways and molecular mechanisms leading to the prediction of an in vivo response. The emergence of quantitative high-throughput screening (qHTS) technology has proved to be an efficient way to decompose complex toxicological end points to specific pathways of targeted organs. In addition, qHTS has made a significant impact on computational toxicology in two aspects. First, the ease of mechanism of action identification brought about by in vitro assays has enhanced the simplicity and effectiveness of machine learning, and second, the high-throughput nature and high reproducibility of qHTS have greatly improved the data quality and increased the quantity of training datasets available for predictive model construction. In this review, the benefits of qHTS routinely used in the US Tox21 program will be highlighted. Quantitative structure-activity relationships models built on traditional in vivo data and new qHTS data will be compared and analyzed. In conjunction with the transition from the pilot phase to the production phase of the Tox21 program, more qHTS data will be made available that will enrich the data pool for predictive toxicology. It is perceivable that new in silico toxicity models based on high-quality qHTS data will achieve unprecedented reliability and robustness, thus becoming a valuable tool for risk assessment and drug discovery.",2012-09-01,20,1717,47,3171
279,22366294,Evaluating the state of the art in coreference resolution for electronic medical records,"Background:                    The fifth i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records conducted a systematic review on resolution of noun phrase coreference in medical records. Informatics for Integrating Biology and the Bedside (i2b2) and the Veterans Affair (VA) Consortium for Healthcare Informatics Research (CHIR) partnered to organize the coreference challenge. They provided the research community with two corpora of medical records for the development and evaluation of the coreference resolution systems. These corpora contained various record types (ie, discharge summaries, pathology reports) from multiple institutions.              Methods:                    The coreference challenge provided the community with two annotated ground truth corpora and evaluated systems on coreference resolution in two ways: first, it evaluated systems for their ability to identify mentions of concepts and to link together those mentions. Second, it evaluated the ability of the systems to link together ground truth mentions that refer to the same entity. Twenty teams representing 29 organizations and nine countries participated in the coreference challenge.              Results:                    The teams' system submissions showed that machine-learning and rule-based approaches worked best when augmented with external knowledge sources and coreference clues extracted from document structure. The systems performed better in coreference resolution when provided with ground truth mentions. Overall, the systems struggled in solving coreference resolution for cases that required domain knowledge.",2012-10-01,51,1643,88,3141
265,22752090,Risk estimation and risk prediction using machine-learning methods,"After an association between genetic variants and a phenotype has been established, further study goals comprise the classification of patients according to disease risk or the estimation of disease probability. To accomplish this, different statistical methods are required, and specifically machine-learning approaches may offer advantages over classical techniques. In this paper, we describe methods for the construction and evaluation of classification and probability estimation rules. We review the use of machine-learning approaches in this context and explain some of the machine-learning algorithms in detail. Finally, we illustrate the methodology through application to a genome-wide association analysis on rheumatoid arthritis.",2012-10-01,31,741,66,3141
264,22776068,Predicting the risk of psychosis onset: advances and prospects,"Aim:                    To conduct a systematic review of the methods and performance characteristics of models developed for predicting the onset of psychosis.              Methods:                    We performed a comprehensive literature search restricted to English articles and identified using PubMed, Medline and PsychINFO, as well as the reference lists of published studies and reviews. Inclusion criteria included the selection of more than one variable to predict psychosis or schizophrenia onset, and selection of individuals at familial risk or clinical high risk. Eighteen studies met these criteria, and we compared these studies based on the subjects selected, predictor variables used and the choice of statistical or machine learning methods.              Results:                    Quality of life and life functioning as well as structural brain imaging emerged as the most promising predictors of psychosis onset, particularly when they were coupled with appropriate dimensionality reduction methods and predictive model algorithms like the support vector machine (SVM). Balanced accuracy ranged from 100% to 78% in four studies using the SVM, and 67% to 81% in 14 studies using general linear models.              Conclusions:                    Performance of the predictive models improves with quality of life measures, life functioning measures, structural brain imaging data, as well as with the use of methods like SVM. Despite these advances, the overall performance of psychosis predictive models is still modest. In the future, performance can potentially be improved by including genetic variant and new functional imaging data in addition to the predictors that are used currently.",2012-11-01,8,1716,62,3110
266,22695048,Hierarchical reinforcement learning and decision making,"The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings.",2012-12-01,39,876,55,3080
261,22842200,Osteoarthritis year 2012 in review: biomarkers,"Purpose:                    Biomarkers provide useful diagnostic information by detecting cartilage degradation in osteoarthritis (OA), reflecting disease-relevant biological activity and predicting the course of disease progression. They also serve as surrogate endpoints in the drug discovery process. The aim of this narrative review was to focus on OA biomarker-related papers published between the osteoarthritis research society international (OARSI) 2011 meeting in San Diego and the OARSI 2012 meeting in Barcelona.              Methods:                    The PubMed/MEDLINE and SciVerse Scopus bibliographic databases were searched using the keywords: 'biomarker' and 'osteoarthritis' and/or 'biomarker' and 'proteomics'.              Results:                    Ninety-eight papers were found with the keywords 'biomarker' and 'osteoarthritis'. Fifteen papers were found with the keywords 'biomarker' and 'proteomics'. Review articles were also included. The most relevant published studies focused on extracellular matrix (ECM) molecules in body fluids. Enrichment of the deamidated epitope of cartilage oligomeric matrix protein (D-COMP) suggests that OA disease progression is associated with post-translational modifications that may show specificity for particular joint sites. Fibulin-3 peptides (Fib3-1 and Fib3-2) have been proposed as potential biomarkers of OA along with follistatin-like protein 1 (FSTL1), a new serum biomarker with the capacity to reflect the severity of joint damage. The 'membrane attack complex' (MAC) component of complement has also been implicated in OA.              Conclusion:                    Novel OA biomarkers are needed for sub-clinical disease diagnosis. Proteomic techniques are beginning to yield useful data and deliver new OA biomarkers in serum and urine. Combining biochemical markers with tissue and cell imaging techniques and bioinformatics (i.e., machine learning, clustering, data visualization) may facilitate the development of biomarker combinations enabling earlier detection of OA.",2012-12-01,42,2055,46,3080
253,23335577,Human semi-supervised learning,"Most empirical work in human categorization has studied learning in either fully supervised or fully unsupervised scenarios. Most real-world learning scenarios, however, are semi-supervised: Learners receive a great deal of unlabeled information from the world, coupled with occasional experiences in which items are directly labeled by a knowledgeable source. A large body of work in machine learning has investigated how learning can exploit both labeled and unlabeled data provided to a learner. Using equivalences between models found in human categorization and machine learning research, we explain how these semi-supervised techniques can be applied to human learning. A series of experiments are described which show that semi-supervised learning models prove useful for explaining human behavior when exposed to both labeled and unlabeled data. We then discuss some machine learning models that do not have familiar human categorization counterparts. Finally, we discuss some challenges yet to be addressed in the use of semi-supervised models for modeling human categorization.",2013-01-01,7,1087,30,3049
255,23071097,Popular computational methods to assess multiprotein complexes derived from label-free affinity purification and mass spectrometry (AP-MS) experiments,"Advances in sensitivity, resolution, mass accuracy, and throughput have considerably increased the number of protein identifications made via mass spectrometry. Despite these advances, state-of-the-art experimental methods for the study of protein-protein interactions yield more candidate interactions than may be expected biologically owing to biases and limitations in the experimental methodology. In silico methods, which distinguish between true and false interactions, have been developed and applied successfully to reduce the number of false positive results yielded by physical interaction assays. Such methods may be grouped according to: (1) the type of data used: methods based on experiment-specific measurements (e.g., spectral counts or identification scores) versus methods that extract knowledge encoded in external annotations (e.g., public interaction and functional categorisation databases); (2) the type of algorithm applied: the statistical description and estimation of physical protein properties versus predictive supervised machine learning or text-mining algorithms; (3) the type of protein relation evaluated: direct (binary) interaction of two proteins in a cocomplex versus probability of any functional relationship between two proteins (e.g., co-occurrence in a pathway, sub cellular compartment); and (4) initial motivation: elucidation of experimental data by evaluation versus prediction of novel protein-protein interaction, to be experimentally validated a posteriori. This work reviews several popular computational scoring methods and software platforms for protein-protein interactions evaluation according to their methodology, comparative strengths and weaknesses, data representation, accessibility, and availability. The scoring methods and platforms described include: CompPASS, SAINT, Decontaminator, MINT, IntAct, STRING, and FunCoup. References to related work are provided throughout in order to provide a concise but thorough introduction to a rapidly growing interdisciplinary field of investigation.",2013-01-01,17,2053,150,3049
263,22784485,Distinguishing between unipolar depression and bipolar depression: current and future clinical and neuroimaging perspectives,"Differentiating bipolar disorder (BD) from recurrent unipolar depression (UD) is a major clinical challenge. Main reasons for this include the higher prevalence of depressive relative to hypo/manic symptoms during the course of BD illness and the high prevalence of subthreshold manic symptoms in both BD and UD depression. Identifying objective markers of BD might help improve accuracy in differentiating between BD and UD depression, to ultimately optimize clinical and functional outcome for all depressed individuals. Yet, only eight neuroimaging studies to date have directly compared UD and BD depressed individuals. Findings from these studies suggest more widespread abnormalities in white matter connectivity and white matter hyperintensities in BD than UD depression, habenula volume reductions in BD but not UD depression, and differential patterns of functional abnormalities in emotion regulation and attentional control neural circuitry in the two depression types. These findings suggest different pathophysiologic processes, especially in emotion regulation, reward, and attentional control neural circuitry in BD versus UD depression. This review thereby serves as a call to action to highlight the pressing need for more neuroimaging studies, using larger samples sizes, comparing BD and UD depressed individuals. These future studies should also include dimensional approaches, studies of at-risk individuals, and more novel neuroimaging approaches, such as connectivity analysis and machine learning. Ultimately, these approaches might provide biomarkers to identify individuals at future risk for BD versus UD and biological targets for more personalized treatment and new treatment developments for BD and UD depression.",2013-01-01,74,1743,124,3049
250,23382431,Global mapping of infectious disease,"The primary aim of this review was to evaluate the state of knowledge of the geographical distribution of all infectious diseases of clinical significance to humans. A systematic review was conducted to enumerate cartographic progress, with respect to the data available for mapping and the methods currently applied. The results helped define the minimum information requirements for mapping infectious disease occurrence, and a quantitative framework for assessing the mapping opportunities for all infectious diseases. This revealed that of 355 infectious diseases identified, 174 (49%) have a strong rationale for mapping and of these only 7 (4%) had been comprehensively mapped. A variety of ambitions, such as the quantification of the global burden of infectious disease, international biosurveillance, assessing the likelihood of infectious disease outbreaks and exploring the propensity for infectious disease evolution and emergence, are limited by these omissions. An overview of the factors hindering progress in disease cartography is provided. It is argued that rapid improvement in the landscape of infectious diseases mapping can be made by embracing non-conventional data sources, automation of geo-positioning and mapping procedures enabled by machine learning and information technology, respectively, in addition to harnessing labour of the volunteer 'cognitive surplus' through crowdsourcing.",2013-02-01,68,1413,36,3018
269,22611119,Machine learning approaches for the discovery of gene-gene interactions in disease data,"Because of the complexity of gene-phenotype relationships machine learning approaches have considerable appeal as a strategy for modelling interactions. A number of such methods have been developed and applied in recent years with some modest success. Progress is hampered by the challenges presented by the complexity of the disease genetic data, including phenotypic and genetic heterogeneity, polygenic forms of inheritance and variable penetrance, combined with the analytical and computational issues arising from the enormous number of potential interactions. We review here recent and current approaches focusing, wherever possible, on applications to real data (particularly in the context of genome-wide association studies) and looking ahead to the further challenges posed by next generation sequencing data.",2013-03-01,26,819,87,2990
249,23392334,Bridging paradigms: hybrid mechanistic-discriminative predictive models,"Many disease processes are extremely complex and characterized by multiple stochastic processes interacting simultaneously. Current analytical approaches have included mechanistic models and machine learning (ML), which are often treated as orthogonal viewpoints. However, to facilitate truly personalized medicine, new perspectives may be required. This paper reviews the use of both mechanistic models and ML in healthcare as well as emerging hybrid methods, which are an exciting and promising approach for biologically based, yet data-driven advanced intelligent systems.",2013-03-01,3,575,71,2990
236,23637855,A plea for neutral comparison studies in computational sciences,"In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of ""new methods"", while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a ""tidy neutral comparison study"". R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/plea2013.",2013-04-01,14,1485,63,2959
237,23622061,Decision making: from neuroscience to psychiatry,"Adaptive behaviors increase the likelihood of survival and reproduction and improve the quality of life. However, it is often difficult to identify optimal behaviors in real life due to the complexity of the decision maker's environment and social dynamics. As a result, although many different brain areas and circuits are involved in decision making, evolutionary and learning solutions adopted by individual decision makers sometimes produce suboptimal outcomes. Although these problems are exacerbated in numerous neurological and psychiatric disorders, their underlying neurobiological causes remain incompletely understood. In this review, theoretical frameworks in economics and machine learning and their applications in recent behavioral and neurobiological studies are summarized. Examples of such applications in clinical domains are also discussed for substance abuse, Parkinson's disease, attention-deficit/hyperactivity disorder, schizophrenia, mood disorders, and autism. Findings from these studies have begun to lay the foundations necessary to improve diagnostics and treatment for various neurological and psychiatric disorders.",2013-04-01,38,1147,48,2959
252,23349097,"Toward a ""structural BLAST"": using structural relationships to infer function","We outline a set of strategies to infer protein function from structure. The overall approach depends on extensive use of homology modeling, the exploitation of a wide range of global and local geometric relationships between protein structures and the use of machine learning techniques. The combination of modeling with broad searches of protein structure space defines a ""structural BLAST"" approach to infer function with high genomic coverage. Applications are described to the prediction of protein-protein and protein-ligand interactions. In the context of protein-protein interactions, our structure-based prediction algorithm, PrePPI, has comparable accuracy to high-throughput experiments. An essential feature of PrePPI involves the use of Bayesian methods to combine structure-derived information with non-structural evidence (e.g. co-expression) to assign a likelihood for each predicted interaction. This, combined with a structural BLAST approach significantly expands the range of applications of protein structure in the annotation of protein function, including systems level biological applications where it has previously played little role.",2013-04-01,8,1160,77,2959
244,23526775,Latest developments in molecular docking: 2010-2011 in review,"The aim of docking is to accurately predict the structure of a ligand within the constraints of a receptor binding site and to correctly estimate the strength of binding. We discuss, in detail, methodological developments that occurred in the docking field in 2010 and 2011, with a particular focus on the more difficult, and sometimes controversial, aspects of this promising computational discipline. The main developments in docking in this period, covered in this review, are receptor flexibility, solvation, fragment docking, postprocessing, docking into homology models, and docking comparisons. Several new, or at least newly invigorated, advances occurred in areas such as nonlinear scoring functions, using machine-learning approaches. This review is strongly focused on docking advances in the context of drug design, specifically in virtual screening and fragment-based drug design. Where appropriate, we refer readers to exemplar case studies.",2013-05-01,69,955,61,2929
240,23552814,Mathematical modeling of infectious disease dynamics,"Over the last years, an intensive worldwide effort is speeding up the developments in the establishment of a global surveillance network for combating pandemics of emergent and re-emergent infectious diseases. Scientists from different fields extending from medicine and molecular biology to computer science and applied mathematics have teamed up for rapid assessment of potentially urgent situations. Toward this aim mathematical modeling plays an important role in efforts that focus on predicting, assessing, and controlling potential outbreaks. To better understand and model the contagious dynamics the impact of numerous variables ranging from the micro host-pathogen level to host-to-host interactions, as well as prevailing ecological, social, economic, and demographic factors across the globe have to be analyzed and thoroughly studied. Here, we present and discuss the main approaches that are used for the surveillance and modeling of infectious disease dynamics. We present the basic concepts underpinning their implementation and practice and for each category we give an annotated list of representative works.",2013-05-01,49,1126,52,2929
254,23321025,A structured approach to predictive modeling of a two-class problem using multidimensional data sets,"Biological experiments in the post-genome era can generate a staggering amount of complex data that challenges experimentalists to extract meaningful information. Increasingly, the success of an appropriately controlled experiment relies on a robust data analysis pipeline. In this paper, we present a structured approach to the analysis of multidimensional data that relies on a close, two-way communication between the bioinformatician and experimentalist. A sequential approach employing data exploration (visualization, graphical and analytical study), pre-processing, feature reduction and supervised classification using machine learning is presented. This standardized approach is illustrated by an example from a proteomic data analysis that has been used to predict the risk of infectious disease outcome. Strategies for model selection and post hoc model diagnostics are presented and applied to the case illustration. We discuss some of the practical lessons we have learned applying supervised classification to multidimensional data sets, one of which is the importance of feature reduction in achieving optimal modeling performance.",2013-05-01,7,1146,100,2929
231,23731483,Machine learning and genome annotation: a match meant to be?,"By its very nature, genomics produces large, high-dimensional datasets that are well suited to analysis by machine learning approaches. Here, we explain some key aspects of machine learning that make it useful for genome annotation, with illustrative examples from ENCODE.",2013-05-01,20,272,60,2929
243,23548026,Machine learning and social network analysis applied to Alzheimer's disease biomarkers,"Due to the fact that the number of deaths due Alzheimer is increasing, the scientists have a strong interest in early stage diagnostic of this disease. Alzheimer's patients show different kind of brain alterations, such as morphological, biochemical, functional, etc. Currently, using magnetic resonance imaging techniques is possible to obtain a huge amount of biomarkers; being difficult to appraise which of them can explain more properly how the pathology evolves instead of the normal ageing. Machine Learning methods facilitate an efficient analysis of complex data and can be used to discover which biomarkers are more informative. Moreover, automatic models can learn from historical data to suggest the diagnostic of new patients. Social Network Analysis (SNA) views social relationships in terms of network theory consisting of nodes and connections. The resulting graph-based structures are often very complex; there can be many kinds of connections between the nodes. SNA has emerged as a key technique in modern sociology. It has also gained a significant following in medicine, anthropology, biology, information science, etc., and has become a popular topic of speculation and study. This paper presents a review of machine learning and SNA techniques and then, a new approach to analyze the magnetic resonance imaging biomarkers with these techniques, obtaining relevant relationships that can explain the different phenotypes in dementia, in particular, different stages of Alzheimer's disease.",2013-06-01,2,1511,86,2898
223,23889047,ENZPRED-enzymatic protein class predicting by machine learning,"Recent times have seen flooding of biological data into the scientific community. Due to increase in large amounts of data from genome and other sequencing projects become available, being diverted on to Insilco approach for data collection and prediction has become a priority also progresses in sequencing technologies have found an exponential function rise in the number of newly found enzymes. Commonly, function of such enzymes is determined by experiments that can be time consuming and costly. As new approaches are needed to determine the functions of the proteins these genes encode. The protein parameters that can be used for an enzyme/ non-enzyme classification includes features of sequences like amino acid composition, dipeptide composition, grand Average of hydropathicity (GRAVY), probability of being in alpha helix, probability of being in beta sheet Probability of being in a turn. We show how large-scale computational analysis can help to address this challenge by help of java and support vector machine library. In this paper, a recently developed machine learning algorithm referred to as the svm library Learning Machine is used to classify protein sequences with six main classes of enzyme data downloaded from a public domain database. Comparative studies on different type of kernel methods like 1.radial basis function, 2.polynomial available in SVM library. Results show that RBF method take less time in training and give more accurate result then other kernel methods to also less training time compared to other kernel methods. The classification accuracy of RBF is also higher than various methods in respect of available sequences data.",2013-06-01,1,1673,62,2898
222,23889055,Recent advances in predicting protein classification and their applications to drug development,"With the explosion of protein sequences generated in the postgenomic era, the gap between the number of attribute- known proteins and that of uncharacterized ones has become increasingly large. Knowing the key attributes of proteins is a shortcut for prioritizing drug targets and developing novel new drugs. Unfortunately, it is both time-consuming and costly to acquire these kinds of information by purely conducting biological experiments. Therefore, it is highly desired to develop various computational tools for fast and effectively classifying proteins according to their sequence information alone. The process of developing these high throughput tools is generally involved with the following procedures: (1) constructing benchmark datasets; (2) representing a protein sequence with a discrete numerical model; (3) developing or introducing a powerful algorithm or machine learning operator to conduct the prediction; (4) estimating the anticipated accuracy with a proper and objective test method; and (5) establishing a user-friendly web-server accessible to the public. This minireview is focused on the recent progresses in identifying the types of G-protein coupled receptors (GPCRs), subcellular localization of proteins, DNA-binding proteins and their binding sites. All these identification tools may provide very useful informations for in-depth study of drug metabolism.",2013-06-01,3,1390,95,2898
233,23700999,Evolutionary computation and QSAR research,"The successful high throughput screening of molecule libraries for a specific biological property is one of the main improvements in drug discovery. The virtual molecular filtering and screening relies greatly on quantitative structure-activity relationship (QSAR) analysis, a mathematical model that correlates the activity of a molecule with molecular descriptors. QSAR models have the potential to reduce the costly failure of drug candidates in advanced (clinical) stages by filtering combinatorial libraries, eliminating candidates with a predicted toxic effect and poor pharmacokinetic profiles, and reducing the number of experiments. To obtain a predictive and reliable QSAR model, scientists use methods from various fields such as molecular modeling, pattern recognition, machine learning or artificial intelligence. QSAR modeling relies on three main steps: molecular structure codification into molecular descriptors, selection of relevant variables in the context of the analyzed activity, and search of the optimal mathematical model that correlates the molecular descriptors with a specific activity. Since a variety of techniques from statistics and artificial intelligence can aid variable selection and model building steps, this review focuses on the evolutionary computation methods supporting these tasks. Thus, this review explains the basic of the genetic algorithms and genetic programming as evolutionary computation approaches, the selection methods for high-dimensional data in QSAR, the methods to build QSAR models, the current evolutionary feature selection methods and applications in QSAR and the future trend on the joint or multi-task feature selection methods.",2013-06-01,2,1695,42,2898
2878,24290387,Motor skill in autism spectrum disorders: a subcortical view,"The earliest observable symptoms of autism spectrum disorders (ASDs) involve motor behavior. There is a growing awareness of the developmental importance of impaired motor function in ASD and its association with social skill. Compromised motor function requires increased attention, leaving fewer resources available for processing environmental stimuli and learning. This knowledge suggests that the motor system-which we know to be trainable-may be a gateway to improving outcomes of individuals living with ASD. In this review, we suggest a framework borrowed from machine learning to examine where, why, and how motor skills are different in individuals with ASD.",2013-06-01,14,668,60,2898
235,23651478,A novel integrated framework and improved methodology of computer-aided drug design,"Computer-aided drug design (CADD) is a critical initiating step of drug development, but a single model capable of covering all designing aspects remains to be elucidated. Hence, we developed a drug design modeling framework that integrates multiple approaches, including machine learning based quantitative structure-activity relationship (QSAR) analysis, 3D-QSAR, Bayesian network, pharmacophore modeling, and structure-based docking algorithm. Restrictions for each model were defined for improved individual and overall accuracy. An integration method was applied to join the results from each model to minimize bias and errors. In addition, the integrated model adopts both static and dynamic analysis to validate the intermolecular stabilities of the receptor-ligand conformation. The proposed protocol was applied to identifying HER2 inhibitors from traditional Chinese medicine (TCM) as an example for validating our new protocol. Eight potent leads were identified from six TCM sources. A joint validation system comprised of comparative molecular field analysis, comparative molecular similarity indices analysis, and molecular dynamics simulation further characterized the candidates into three potential binding conformations and validated the binding stability of each protein-ligand complex. The ligand pathway was also performed to predict the ligand ""in"" and ""exit"" from the binding site. In summary, we propose a novel systematic CADD methodology for the identification, analysis, and characterization of drug-like candidates.",2013-06-01,38,1543,83,2898
232,23722209,Cellular-resolution connectomics: challenges of dense neural circuit reconstruction,"Neuronal networks are high-dimensional graphs that are packed into three-dimensional nervous tissue at extremely high density. Comprehensively mapping these networks is therefore a major challenge. Although recent developments in volume electron microscopy imaging have made data acquisition feasible for circuits comprising a few hundreds to a few thousands of neurons, data analysis is massively lagging behind. The aim of this perspective is to summarize and quantify the challenges for data analysis in cellular-resolution connectomics and describe current solutions involving online crowd-sourcing and machine-learning approaches.",2013-06-01,81,635,83,2898
247,23410165,Predicting targeted polypharmacology for drug repositioning and multi- target drug discovery,"Prediction of polypharmacology of known drugs and new molecules against selected multiple targets is highly useful for finding new therapeutic applications of existing drugs (drug repositioning) and for discovering multi-target drugs with improved therapeutic efficacies by collective regulations of primary therapeutic targets, compensatory signalling and drug resistance mechanisms. In this review, we describe recent progresses in exploration of in-silico methods for predicting polypharmacology of known drugs and new molecules by means of structure-based (molecular docking, binding- site structural similarity, receptor-based pharmacophore searching), expression-based (expression profile/signature similarity disease-drug and drug-drug networks), ligand-based (similarity searching, side-effect similarity, QSAR, machine learning), and fragment-based approaches that have shown promising potential in facilitating drug repositioning and the discovery of multi-target drugs.",2013-06-01,16,980,92,2898
246,23431257,Prediction of deleterious nonsynonymous single-nucleotide polymorphism for human diseases,"The identification of genetic variants that are responsible for human inherited diseases is a fundamental problem in human and medical genetics. As a typical type of genetic variation, nonsynonymous single-nucleotide polymorphisms (nsSNPs) occurring in protein coding regions may alter the encoded amino acid, potentially affect protein structure and function, and further result in human inherited diseases. Therefore, it is of great importance to develop computational approaches to facilitate the discrimination of deleterious nsSNPs from neutral ones. In this paper, we review databases that collect nsSNPs and summarize computational methods for the identification of deleterious nsSNPs. We classify the existing methods for characterizing nsSNPs into three categories (sequence based, structure based, and annotation based), and we introduce machine learning models for the prediction of deleterious nsSNPs. We further discuss methods for identifying deleterious nsSNPs in noncoding variants and those for dealing with rare variants.",2013-06-01,23,1039,89,2898
2863,24489603,Machine learning approaches: from theory to application in schizophrenia,"In recent years, machine learning approaches have been successfully applied for analysis of neuroimaging data, to help in the context of disease diagnosis. We provide, in this paper, an overview of recent support vector machine-based methods developed and applied in psychiatric neuroimaging for the investigation of schizophrenia. In particular, we focus on the algorithms implemented by our group, which have been applied to classify subjects affected by schizophrenia and healthy controls, comparing them in terms of accuracy results with other recently published studies. First we give a description of the basic terminology used in pattern recognition and machine learning. Then we separately summarize and explain each study, highlighting the main features that characterize each method. Finally, as an outcome of the comparison of the results obtained applying the described different techniques, conclusions are drawn in order to understand how much automatic classification approaches can be considered a useful tool in understanding the biological underpinnings of schizophrenia. We then conclude by discussing the main implications achievable by the application of these methods into clinical practice.",2013-06-01,20,1213,72,2898
241,23549108,The role of technology and engineering models in transforming healthcare,"The healthcare system is in crisis due to challenges including escalating costs, the inconsistent provision of care, an aging population, and high burden of chronic disease related to health behaviors. Mitigating this crisis will require a major transformation of healthcare to be proactive, preventive, patient-centered, and evidence-based with a focus on improving quality-of-life. Information technology, networking, and biomedical engineering are likely to be essential in making this transformation possible with the help of advances, such as sensor technology, mobile computing, machine learning, etc. This paper has three themes: 1) motivation for a transformation of healthcare; 2) description of how information technology and engineering can support this transformation with the help of computational models; and 3) a technical overview of several research areas that illustrate the need for mathematical modeling approaches, ranging from sparse sampling to behavioral phenotyping and early detection. A key tenet of this paper concerns complementing prior work on patient-specific modeling and simulation by modeling neuropsychological, behavioral, and social phenomena. The resulting models, in combination with frequent or continuous measurements, are likely to be key components of health interventions to enhance health and wellbeing and the provision of healthcare.",2013-06-01,8,1381,72,2898
256,23016852,Anti-cancer drug development: computational strategies to identify and target proteins involved in cancer metabolism,"Cancer remains a fundamental burden to public health despite substantial efforts aimed at developing effective chemotherapeutics and significant advances in chemotherapeutic regimens. The major challenge in anti-cancer drug design is to selectively target cancer cells with high specificity. Research into treating malignancies by targeting altered metabolism in cancer cells is supported by computational approaches, which can take a leading role in identifying candidate targets for anti-cancer therapy as well as assist in the discovery and optimisation of anti-cancer agents. Natural products appear to have privileged structures for anti-cancer drug development and the bulk of this particularly valuable chemical space still remains to be explored. In this review we aim to provide a comprehensive overview of current strategies for computer-guided anti-cancer drug development. We start with a discussion of state-of-the art bioinformatics methods applied to the identification of novel anti-cancer targets, including machine learning techniques, the Connectivity Map and biological network analysis. This is followed by an extensive survey of molecular modelling and cheminformatics techniques employed to develop agents targeting proteins involved in the glycolytic, lipid, NAD+, mitochondrial (TCA cycle), amino acid and nucleic acid metabolism of cancer cells. A dedicated section highlights the most promising strategies to develop anti-cancer therapeutics from natural products and the role of metabolism and some of the many targets which are under investigation are reviewed. Recent success stories are reported for all the areas covered in this review. We conclude with a brief summary of the most interesting strategies identified and with an outlook on future directions in anti-cancer drug development.",2013-06-01,6,1821,116,2898
242,23548028,Applied computational techniques on schizophrenia using genetic mutations,"Schizophrenia is a complex disease, with both genetic and environmental influence. Machine learning techniques can be used to associate different genetic variations at different genes with a (schizophrenic or non-schizophrenic) phenotype. Several machine learning techniques were applied to schizophrenia data to obtain the results presented in this study. Considering these data, Quantitative Genotype - Disease Relationships (QDGRs) can be used for disease prediction. One of the best machine learning-based models obtained after this exhaustive comparative study was implemented online; this model is an artificial neural network (ANN). Thus, the tool offers the possibility to introduce Single Nucleotide Polymorphism (SNP) sequences in order to classify a patient with schizophrenia. Besides this comparative study, a method for variable selection, based on ANNs and evolutionary computation (EC), is also presented. This method uses half the number of variables as the original ANN and the variables obtained are among those found in other publications. In the future, QDGR models based on nucleic acid information could be expanded to other diseases.",2013-06-01,1,1157,73,2898
226,23829390,"Challenges, issues and trends in fall detection systems","Since falls are a major public health problem among older people, the number of systems aimed at detecting them has increased dramatically over recent years. This work presents an extensive literature review of fall detection systems, including comparisons among various kinds of studies. It aims to serve as a reference for both clinicians and biomedical engineers planning or conducting field investigations. Challenges, issues and trends in fall detection have been identified after the reviewing work. The number of studies using context-aware techniques is still increasing but there is a new trend towards the integration of fall detection into smartphones as well as the use of machine learning methods in the detection algorithm. We have also identified challenges regarding performance under real-life conditions, usability, and user acceptance as well as issues related to power consumption, real-time operations, sensing limitations, privacy and record of real-life falls.",2013-07-01,48,983,55,2868
245,23506300,The emerging role of photorespiration and non-photorespiratory peroxisomal metabolism in pathogen defence,"Photorespiration represents one of the major highways of primary plant metabolism and is the most prominent example of metabolic cell organelle integration, since the pathway requires the concerted action of plastidial, peroxisomal, mitochondrial and cytosolic enzymes and organellar transport proteins. Oxygenation of ribulose-1,5-bisphosphate by Rubisco leads to the formation of large amounts of 2-phosphoglycolate, which are recycled to 3-phosphoglycerate by the photorespiratory C2 cycle, concomitant with stoichiometric production rates of H2 O2 in peroxisomes. Apart from its significance for agricultural productivity, a secondary function of photorespiration in pathogen defence has emerged only recently. Here, we summarise literature data supporting the crosstalk between photorespiration and pathogen defence and perform a meta-expression analysis of photorespiratory genes during pathogen attack. Moreover, we screened Arabidopsis proteins newly predicted using machine learning methods to be targeted to peroxisomes, the central H2 O2 -producing organelle of photorespiration, for homologues of known pathogen defence proteins and analysed their expression during pathogen infection. The analyses further support the idea that photorespiration and non-photorespiratory peroxisomal metabolism play multi-faceted roles in pathogen defence beyond metabolism of reactive oxygen species.",2013-07-01,23,1396,105,2868
204,24179856,Complex biomarker discovery in neuroimaging data: Finding a needle in a haystack,"Neuropsychiatric disorders such as schizophrenia, bipolar disorder and Alzheimer's disease are major public health problems. However, despite decades of research, we currently have no validated prognostic or diagnostic tests that can be applied at an individual patient level. Many neuropsychiatric diseases are due to a combination of alterations that occur in a human brain rather than the result of localized lesions. While there is hope that newer imaging technologies such as functional and anatomic connectivity MRI or molecular imaging may offer breakthroughs, the single biomarkers that are discovered using these datasets are limited by their inability to capture the heterogeneity and complexity of most multifactorial brain disorders. Recently, complex biomarkers have been explored to address this limitation using neuroimaging data. In this manuscript we consider the nature of complex biomarkers being investigated in the recent literature and present techniques to find such biomarkers that have been developed in related areas of data mining, statistics, machine learning and bioinformatics.",2013-08-01,26,1107,80,2837
234,23686810,"Automated analysis of diabetic retinopathy images: principles, recent developments, and emerging trends","Diabetic retinopathy (DR) is a vision-threatening complication of diabetes. Timely diagnosis and intervention are essential for treatment that reduces the risk of vision loss. A good color retinal (fundus) photograph can be used as a surrogate for face-to-face evaluation of DR. The use of computers to assist or fully automate DR evaluation from retinal images has been studied for many years. Early work showed promising results for algorithms in detecting and classifying DR pathology. Newer techniques include those that adapt machine learning technology to DR image analysis. Challenges remain, however, that must be overcome before fully automatic DR detection and analysis systems become practical clinical tools.",2013-08-01,6,720,103,2837
220,23921828,Sudden event recognition: a survey,"Event recognition is one of the most active research areas in video surveillance fields. Advancement in event recognition systems mainly aims to provide convenience, safety and an efficient lifestyle for humanity. A precise, accurate and robust approach is necessary to enable event recognition systems to respond to sudden changes in various uncontrolled environments, such as the case of an emergency, physical threat and a fire or bomb alert. The performance of sudden event recognition systems depends heavily on the accuracy of low level processing, like detection, recognition, tracking and machine learning algorithms. This survey aims to detect and characterize a sudden event, which is a subset of an abnormal event in several video surveillance applications. This paper discusses the following in detail: (1) the importance of a sudden event over a general anomalous event; (2) frameworks used in sudden event recognition; (3) the requirements and comparative studies of a sudden event recognition system and (4) various decision-making approaches for sudden event recognition. The advantages and drawbacks of using 3D images from multiple cameras for real-time application are also discussed. The paper concludes with suggestions for future research directions in sudden event recognition.",2013-08-01,1,1300,34,2837
225,23848274,A review of protein function prediction under machine learning perspective,"Protein function prediction is one of the most challenging problems in the post-genomic era. The number of newly identified proteins has been exponentially increasing with the advances of the high-throughput techniques. However, the functional characterization of these new proteins was not incremented in the same proportion. To fill this gap, a large number of computational methods have been proposed in the literature. Early approaches have explored homology relationships to associate known functions to the newly discovered proteins. Nevertheless, these approaches tend to fail when a new protein is considerably different (divergent) from previously known ones. Accordingly, more accurate approaches, that use expressive data representation and explore sophisticate computational techniques are required. Regarding these points, this review provides a comprehensible description of machine learning approaches that are currently applied to protein function prediction problems. We start by defining several problems enrolled in understanding protein function aspects, and describing how machine learning can be applied to these problems. We aim to expose, in a systematical framework, the role of these techniques in protein function inference, sometimes difficult to follow up due to the rapid evolvement of the field. With this purpose in mind, we highlight the most representative contributions, the recent advancements, and provide an insightful categorization and classification of machine learning methods in functional proteomics.",2013-08-01,7,1544,74,2837
214,24025554,Five years of designing wireless sensor networks in the Doana Biological Reserve (Spain): an applications approach,"Wireless Sensor Networks (WSNs) are a technology that is becoming very popular for many applications, and environmental monitoring is one of its most important application areas. This technology solves the lack of flexibility of wired sensor installations and, at the same time, reduces the deployment costs. To demonstrate the advantages of WSN technology, for the last five years we have been deploying some prototypes in the Doana Biological Reserve, which is an important protected area in Southern Spain. These prototypes not only evaluate the technology, but also solve some of the monitoring problems that have been raised by biologists working in Doana. This paper presents a review of the work that has been developed during these five years. Here, we demonstrate the enormous potential of using machine learning in wireless sensor networks for environmental and animal monitoring because this approach increases the amount of useful information and reduces the effort that is required by biologists in an environmental monitoring task.",2013-09-01,7,1047,115,2806
216,23933976,"A review of recent literature employing electroencephalographic techniques to study the pathophysiology, phenomenology, and treatment response of schizophrenia","Clinical experience and research findings suggest that schizophrenia is a disorder comprised of multiple genetic and neurophysiological subtypes with differential response to treatment. Electroencephalography (EEG) is a non-invasive, inexpensive and useful tool for investigating the neurobiology of schizophrenia and its subtypes. EEG studies elucidate the neurophysiological mechanisms potentially underlying clinical symptomatology. In this review article recent advances in applying EEG to study pathophysiology, phenomenology, and treatment response in schizophrenia are discussed. Investigative strategies employed include: analyzing quantitative EEG (QEEG) spectral power during the resting state and cognitive tasks; applying machine learning methods to identify QEEG indicators of diagnosis and treatment response; and using the event-related brain potential (ERP) technique to characterize the neurocognitive processes underlying clinical symptoms. Studies attempting to validate potential EEG biomarkers of schizophrenia and its symptoms, which could be useful in assessing familial risk and treatment response, are also reviewed.",2013-09-01,4,1141,159,2806
218,23931841,Beyond identification: emerging and future uses for MALDI-TOF mass spectrometry in the clinical microbiology laboratory,"The routine use of matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS) has revolutionized microorganism identification in the clinical microbiology laboratory. Building from these now common microorganism identification strategies, this review explores future clinical applications of MALDI-TOF MS. This includes practical approaches for laboratorians interested in implementing direct identification processing methods for MALDI-TOF detection of microbes in bloodstream infection (BSI) and urinary tract infection (UTI), as well as, post-analytical approaches for classifying MALDI-TOF spectral data to detect characteristics other and species-level identification (e.g. strain-level classification, typing, and resistance mechanisms).",2013-09-01,12,776,119,2806
211,24099497,Neuroimaging-based biomarkers in psychiatry: clinical opportunities of a paradigm shift,"Neuroimaging research has substantiated the functional and structural abnormalities underlying psychiatric disorders but has, thus far, failed to have a significant impact on clinical practice. Recently, neuroimaging-based diagnoses and clinical predictions derived from machine learning analysis have shown significant potential for clinical translation. This review introduces the key concepts of this approach, including how the multivariate integration of patterns of brain abnormalities is a crucial component. We survey recent findings that have potential application for diagnosis, in particular early and differential diagnoses in Alzheimer disease and schizophrenia, and the prediction of clinical response to treatment in depression. We discuss the specific clinical opportunities and the challenges for developing biomarkers for psychiatry in the absence of a diagnostic gold standard. We propose that longitudinal outcomes, such as early diagnosis and prediction of treatment response, offer definite opportunities for progress. We propose that efforts should be directed toward clinically challenging predictions in which neuroimaging may have added value, compared with the existing standard assessment. We conclude that diagnostic and prognostic biomarkers will be developed through the joint application of expert psychiatric knowledge in addition to advanced methods of analysis.",2013-09-01,38,1396,87,2806
2880,24273713,"Towards the identification of imaging biomarkers in schizophrenia, using multivariate pattern classification at a single-subject level","Standard univariate analyses of brain imaging data have revealed a host of structural and functional brain alterations in schizophrenia. However, these analyses typically involve examining each voxel separately and making inferences at group-level, thus limiting clinical translation of their findings. Taking into account the fact that brain alterations in schizophrenia expand over a widely distributed network of brain regions, univariate analysis methods may not be the most suited choice for imaging data analysis. To address these limitations, the neuroimaging community has turned to machine learning methods both because of their ability to examine voxels jointly and their potential for making inferences at a single-subject level. This article provides a critical overview of the current and foreseeable applications of machine learning, in identifying imaging-based biomarkers that could be used for the diagnosis, early detection and treatment response of schizophrenia, and could, thus, be of high clinical relevance. We discuss promising future research directions and the main difficulties facing machine learning researchers as far as their potential translation into clinical practice is concerned.",2013-09-01,30,1215,134,2806
239,23559637,Pattern recognition in bioinformatics,"Pattern recognition is concerned with the development of systems that learn to solve a given problem using a set of example instances, each represented by a number of features. These problems include clustering, the grouping of similar instances; classification, the task of assigning a discrete label to a given instance; and dimensionality reduction, combining or selecting features to arrive at a more useful representation. The use of statistical pattern recognition algorithms in bioinformatics is pervasive. Classification and clustering are often applied to high-throughput measurement data arising from microarray, mass spectrometry and next-generation sequencing experiments for selecting markers, predicting phenotype and grouping objects or genes. Less explicitly, classification is at the core of a wide range of tools such as predictors of genes, protein function, functional or genetic interactions, etc., and used extensively in systems biology. A course on pattern recognition (or machine learning) should therefore be at the core of any bioinformatics education program. In this review, we discuss the main elements of a pattern recognition course, based on material developed for courses taught at the BSc, MSc and PhD levels to an audience of bioinformaticians, computer scientists and life scientists. We pay attention to common problems and pitfalls encountered in applications and in interpretation of the results obtained.",2013-09-01,9,1445,37,2806
238,23564629,Evaluating temporal relations in clinical text: 2012 i2b2 Challenge,"Background:                    The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions.              Methods:                    The challenge evaluated systems on the information extraction tasks that targeted: (1) clinically significant events, including both clinical concepts such as problems, tests, treatments, and clinical departments, and events relevant to the patient's clinical timeline, such as admissions, transfers between departments, etc; (2) temporal expressions, referring to the dates, times, durations, or frequencies phrases in the clinical text. The values of the extracted temporal expressions had to be normalized to an ISO specification standard; and (3) temporal relations, between the clinical events and temporal expressions. Participants determined pairs of events and temporal expressions that exhibited a temporal relation, and identified the temporal relation between them.              Results:                    For event detection, statistical machine learning (ML) methods consistently showed superior performance. While ML and rule based methods seemed to detect temporal expressions equally well, the best systems overwhelmingly adopted a rule based approach for value normalization. For temporal relation classification, the systems using hybrid approaches that combined ML and heuristics based methods produced the best results.",2013-10-01,99,1944,67,2776
224,23872922,Prediction of RNA binding proteins comes of age from low resolution to high resolution,"Networks of protein-RNA interactions is likely to be larger than protein-protein and protein-DNA interaction networks because RNA transcripts are encoded tens of times more than proteins (e.g. only 3% of human genome coded for proteins), have diverse function and localization, and are controlled by proteins from birth (transcription) to death (degradation). This massive network is evidenced by several recent experimental discoveries of large numbers of previously unknown RNA-binding proteins (RBPs). Meanwhile, more than 400 non-redundant protein-RNA complex structures (at 25% sequence identity or less) have been deposited into the protein databank. These sequences and structural resources for RBPs provide ample data for the development of computational techniques dedicated to RBP prediction, as experimentally determining RNA-binding functions is time-consuming and expensive. This review compares traditional machine-learning based approaches with emerging template-based methods at several levels of prediction resolution ranging from two-state binding/non-binding prediction, to binding residue prediction and protein-RNA complex structure prediction. The analysis indicates that the two approaches are complementary and their combinations may lead to further improvements.",2013-10-01,14,1287,86,2776
200,24199696,"Esophageal cancer staging: past, present, and future","TNM cancer staging, conceived 70 years ago, was first applied to the esophagus in 1977. Prior staging was neither data-driven nor harmonized with stomach cancer. Machine-learning analysis of worldwide data addressed these shortcomings in the 7th edition. The 8th edition considers 6 problems in attempting to advance esophageal cancer staging.",2013-11-01,18,343,52,2745
205,24168933,Linking brain imaging signals to visual perception,"The rapid advances in brain imaging technology over the past 20 years are affording new insights into cortical processing hierarchies in the human brain. These new data provide a complementary front in seeking to understand the links between perceptual and physiological states. Here we review some of the challenges associated with incorporating brain imaging data into such ""linking hypotheses,"" highlighting some of the considerations needed in brain imaging data acquisition and analysis. We discuss work that has sought to link human brain imaging signals to existing electrophysiological data and opened up new opportunities in studying the neural basis of complex perceptual judgments. We consider a range of approaches when using human functional magnetic resonance imaging to identify brain circuits whose activity changes in a similar manner to perceptual judgments and illustrate these approaches by discussing work that has studied the neural basis of 3D perception and perceptual learning. Finally, we describe approaches that have sought to understand the information content of brain imaging data using machine learning and work that has integrated multimodal data to overcome the limitations associated with individual brain imaging approaches. Together these approaches provide an important route in seeking to understand the links between physiological and psychological states.",2013-11-01,4,1396,50,2745
209,24126129,"Information-seeking, curiosity, and attention: computational and neural mechanisms","Intelligent animals devote much time and energy to exploring and obtaining information, but the underlying mechanisms are poorly understood. We review recent developments on this topic that have emerged from the traditionally separate fields of machine learning, eye movements in natural behavior, and studies of curiosity in psychology and neuroscience. These studies show that exploration may be guided by a family of mechanisms that range from automatic biases toward novelty or surprise to systematic searches for learning progress and information gain in curiosity-driven behavior. In addition, eye movements reflect visual information searching in multiple conditions and are amenable for cellular-level investigations. This suggests that the oculomotor system is an excellent model system for understanding information-sampling mechanisms.",2013-11-01,75,846,82,2745
230,23735485,Forecasting respiratory collapse: theory and practice for averting life-threatening infant apneas,"Apnea of prematurity is a common disorder of respiratory control among preterm infants, with potentially serious adverse consequences on infant development. We review the capability for automatically assessing apnea risk and predicting apnea episodes from multimodal physiological measurements, and for using this knowledge to provide timely therapeutic intervention. We also review other, similar clinical domains of respiratory distress assessment and prediction in the hope of gaining useful insights. We propose an algorithmic framework for constructing discriminative feature vectors from physiological measurements, and for building robust and effective statistical models for apnea assessment and prediction.",2013-11-01,10,715,97,2745
2865,24403917,GB Virus C/Hepatitis G Virus Envelope Glycoprotein E2: Computational Molecular Features and Immunoinformatics Study,"Introduction:                    GB virus C (GBV-C) or hepatitis G virus (HGV) is an enveloped, RNA positive-stranded flavivirus-like particle. E2 envelope protein of GBV-C plays an important role in virus entry into the cytosol, genotyping and as a marker for diagnosing GBV-C infections. Also, there is discussion on relations between E2 protein and gp41 protein of HIV. The purposes of our study are to multi aspect molecular evaluation of GB virus C E2 protein from its characteristics, mutations, structures and antigenicity which would help to new directions for future researches.              Evidence acquisition:                    Briefly, steps followed here were; retrieving reference sequences of E2 protein, entropy plot evaluation for finding the mutational /conservative regions, analyzing potential Glycosylation, Phosphorylation and Palmitoylation sites, prediction of primary, secondary and tertiary structures, then amino acid distributions and transmembrane topology, prediction of T and B cell epitopes, and finally visualization of epitopes and variations regions in 3D structure.              Results:                    Based on the entropy plot, 3 hypervariable regions (HVR) observed along E2 protein located in residues 133-135, 256-260 and 279-281. Analyzing primary structure of protein sequence revealed basic nature, instability, and low hydrophilicity of this protein. Transmembrane topology prediction showed that residues 257-270 presented outside, while residues 234- 256 and 271-293 were transmembrane regions. Just one N-glycosylation site, 5 potential phosphorylated peptides and two palmitoylation were found. Secondary structure revealed that this protein has 6 -helix, 12 -strand 17 Coil structures. Prediction of T-cell epitopes based on HLA-A*02:01 showed that epitope NH3-LLLDFVFVL-COOH is the best antigen icepitope. Comparative analysis for consensus B-cell epitopes regarding transmembrane topology, based on physico-chemical and machine learning approaches revealed that residue 231- 296 (NH2- EARLVPLILLLLWWWVNQLAVLGLPAVEAAVAGEVFAGPALSWCLGLPVVSMILGLANLVLYFRWL-COOH) is most effective and probable B cell epitope for E2 protein.              Conclusions:                    The comprehensive analysis of a protein with important roles has never been easy, and in case of E2 envelope glycoprotein of HGV, there is no much data on its molecular and immunological features, clinical significance and its pathogenic potential in hepatitis or any other GBV-C related diseases. So, results of the present study may explain some structural, physiological and immunological functions of this protein in GBV-C, as well as designing new diagnostic kits and besides, help to better understandingE2 protein characteristic and other members of Flavivirus family, especially HCV.",2013-12-01,3,2817,115,2715
202,24187909,Computational and theoretical methods for protein folding,"A computational approach is essential whenever the complexity of the process under study is such that direct theoretical or experimental approaches are not viable. This is the case for protein folding, for which a significant amount of data are being collected. This paper reports on the essential role of in silico methods and the unprecedented interplay of computational and theoretical approaches, which is a defining point of the interdisciplinary investigations of the protein folding process. Besides giving an overview of the available computational methods and tools, we argue that computation plays not merely an ancillary role but has a more constructive function in that computational work may precede theory and experiments. More precisely, computation can provide the primary conceptual clues to inspire subsequent theoretical and experimental work even in a case where no preexisting evidence or theoretical frameworks are available. This is cogently manifested in the application of machine learning methods to come to grips with the folding dynamics. These close relationships suggested complementing the review of computational methods within the appropriate theoretical context to provide a self-contained outlook of the basic concepts that have converged into a unified description of folding and have grown in a synergic relationship with their computational counterpart. Finally, the advantages and limitations of current computational methodologies are discussed to show how the smart analysis of large amounts of data and the development of more effective algorithms can improve our understanding of protein folding.",2013-12-01,12,1639,57,2715
203,24182747,Innovation by homologous recombination,"Swapping fragments among protein homologs can produce chimeric proteins with a wide range of properties, including properties not exhibited by the parents. Computational methods that use information from structures and sequence alignments have been used to design highly functional chimeras and chimera libraries. Recombination has generated proteins with diverse thermostability and mechanical stability, enzyme substrate specificity, and optogenetic properties. Linear regression, Gaussian processes, and support vector machine learning have been used to model sequence-function relationships and predict useful chimeras. These approaches enable engineering of protein chimeras with desired functions, as well as elucidation of the structural basis for these functions.",2013-12-01,9,771,38,2715
2870,24351646,Data mining for wearable sensors in health monitoring systems: a review of recent trends and challenges,"The past few years have witnessed an increase in the development of wearable sensors for health monitoring systems. This increase has been due to several factors such as development in sensor technology as well as directed efforts on political and stakeholder levels to promote projects which address the need for providing new methods for care given increasing challenges with an aging population. An important aspect of study in such system is how the data is treated and processed. This paper provides a recent review of the latest methods and algorithms used to analyze data from wearable sensors used for physiological monitoring of vital signs in healthcare services. In particular, the paper outlines the more common data mining tasks that have been applied such as anomaly detection, prediction and decision making when considering in particular continuous time series measurements. Moreover, the paper further details the suitability of particular data mining and machine learning methods used to process the physiological data and provides an overview of the properties of the data sets used in experimental validation. Finally, based on this literature review, a number of key challenges have been outlined for data mining methods in health monitoring systems.",2013-12-01,52,1271,103,2715
2883,24259662,Machine learning in cell biology - teaching computers to recognize phenotypes,"Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline.",2013-12-01,79,1069,77,2715
210,24116388,Application of machine learning to proteomics data: classification and biomarker identification in postgenomics biology,"Mass spectrometry is an analytical technique for the characterization of biological samples and is increasingly used in omics studies because of its targeted, nontargeted, and high throughput abilities. However, due to the large datasets generated, it requires informatics approaches such as machine learning techniques to analyze and interpret relevant data. Machine learning can be applied to MS-derived proteomics data in two ways. First, directly to mass spectral peaks and second, to proteins identified by sequence database searching, although relative protein quantification is required for the latter. Machine learning has been applied to mass spectrometry data from different biological disciplines, particularly for various cancers. The aims of such investigations have been to identify biomarkers and to aid in diagnosis, prognosis, and treatment of specific diseases. This review describes how machine learning has been applied to proteomics tandem mass spectrometry data. This includes how it can be used to identify proteins suitable for use as biomarkers of disease and for classification of samples into disease or treatment groups, which may be applicable for diagnostics. It also includes the challenges faced by such investigations, such as prediction of proteins present, protein quantification, planning for the use of machine learning, and small sample sizes.",2013-12-01,53,1381,119,2715
2871,24348517,On protocols and measures for the validation of supervised methods for the inference of biological networks,"Networks provide a natural representation of molecular biology knowledge, in particular to model relationships between biological entities such as genes, proteins, drugs, or diseases. Because of the effort, the cost, or the lack of the experiments necessary for the elucidation of these networks, computational approaches for network inference have been frequently investigated in the literature. In this paper, we examine the assessment of supervised network inference. Supervised inference is based on machine learning techniques that infer the network from a training sample of known interacting and possibly non-interacting entities and additional measurement data. While these methods are very effective, their reliable validation in silico poses a challenge, since both prediction and validation need to be performed on the basis of the same partially known network. Cross-validation techniques need to be specifically adapted to classification problems on pairs of objects. We perform a critical review and assessment of protocols and measures proposed in the literature and derive specific guidelines how to best exploit and evaluate machine learning techniques for network inference. Through theoretical considerations and in silico experiments, we analyze in depth how important factors influence the outcome of performance estimation. These factors include the amount of information available for the interacting entities, the sparsity and topology of biological networks, and the lack of experimentally verified non-interacting pairs.",2013-12-01,11,1546,107,2715
2879,24287119,An integrative meta-analysis of microRNAs in hepatocellular carcinoma,"We aimed to shed new light on the roles of microRNAs (miRNAs) in liver cancer using an integrative in silico bioinformatics analysis. A new protocol for target prediction and functional analysis is presented and applied to the 26 highly differentially deregulated miRNAs in hepatocellular carcinoma. This framework comprises: (1) the overlap of prediction results by four out of five target prediction tools, including TargetScan, PicTar, miRanda, DIANA-microT and miRDB (combining machine-learning, alignment, interaction energy and statistical tests in order to minimize false positives), (2) evidence from previous microarray analysis on the expression of these targets, (3) gene ontology (GO) and pathway enrichment analysis of the miRNA targets and their pathways and (4) linking these results to oncogenesis and cancer hallmarks. This yielded new insights into the roles of miRNAs in cancer hallmarks. Here we presented several key targets and hundreds of new targets that are significantly enriched in many new cancer-related hallmarks. In addition, we also revealed some known and new oncogenic pathways for liver cancer. These included the famous MAPK, TGF and cell cycle pathways. New insights were also provided into Wnt signaling, prostate cancer, axon guidance and oocyte meiosis pathways. These signaling and developmental pathways crosstalk to regulate stem cell transformation and implicate a role of miRNAs in hepatic stem cell deregulation and cancer development. By analyzing their complete interactome, we proposed new categorization for some of these miRNAs as either tumor-suppressors or oncomiRs with dual roles. Therefore some of these miRNAs may be addressed as therapeutic targets or used as therapeutic agents. Such dual roles thus expand the view of miRNAs as active maintainers of cellular homeostasis.",2013-12-01,15,1832,69,2715
2876,24307566,"Reverse engineering and identification in systems biology: strategies, perspectives and challenges","The interplay of mathematical modelling with experiments is one of the central elements in systems biology. The aim of reverse engineering is to infer, analyse and understand, through this interplay, the functional and regulatory mechanisms of biological systems. Reverse engineering is not exclusive of systems biology and has been studied in different areas, such as inverse problem theory, machine learning, nonlinear physics, (bio)chemical kinetics, control theory and optimization, among others. However, it seems that many of these areas have been relatively closed to outsiders. In this contribution, we aim to compare and highlight the different perspectives and contributions from these fields, with emphasis on two key questions: (i) why are reverse engineering problems so hard to solve, and (ii) what methods are available for the particular problems arising from systems biology?",2013-12-01,68,892,98,2715
2882,24262909,Creating the feedback loop: closed-loop neurostimulation,"Current DBS therapy delivers a train of electrical pulses at set stimulation parameters. This open-loop design is effective for movement disorders, but therapy may be further optimized by a closed loop design. The technology to record biosignals has outpaced our understanding of their relationship to the clinical state of the whole person. Neuronal oscillations may represent or facilitate the cooperative functioning of brain ensembles, and may provide critical information to customize neuromodulation therapy. This review addresses advances to date, not of the technology per se, but of the strategies to apply neuronal signals to trigger or modulate stimulation systems.",2014-01-01,13,676,56,2684
2877,24304044,Support vector machines for drug discovery,"Introduction:                    Support vector machines (SVMs) are supervised machine learning algorithms for binary class label prediction and regression-based prediction of property values. In recent years, SVMs have become increasingly popular for drug discovery-relevant applications such as compound classification, the search for novel active compounds and property predictions.              Areas covered:                    The authors provide the readers with a brief introduction of SVM theory and discuss the kernel functions designed for drug discovery applications. The authors also review the different types of SVM applications in drug discovery, looking at particular case studies. Furthermore, the authors discuss the recent hybrid methods developed that incorporate SVM modeling in different ways.              Expert opinion:                    SVMs are currently among the best-performing approaches for chemical and biological property prediction and the computational identification of active compounds. It is anticipated that their use in drug discovery will further increase. Indeed, this will also include the development of SVM-based meta-classifiers that combine different approaches and exploit their individual strengths and complementarity.",2014-01-01,18,1271,42,2684
2875,24320616,Beyond modules and hubs: the potential of gene coexpression networks for investigating molecular mechanisms of complex brain disorders,"In a research environment dominated by reductionist approaches to brain disease mechanisms, gene network analysis provides a complementary framework in which to tackle the complex dysregulations that occur in neuropsychiatric and other neurological disorders. Gene-gene expression correlations are a common source of molecular networks because they can be extracted from high-dimensional disease data and encapsulate the activity of multiple regulatory systems. However, the analysis of gene coexpression patterns is often treated as a mechanistic black box, in which looming 'hub genes' direct cellular networks, and where other features are obscured. By examining the biophysical bases of coexpression and gene regulatory changes that occur in disease, recent studies suggest it is possible to use coexpression networks as a multi-omic screening procedure to generate novel hypotheses for disease mechanisms. Because technical processing steps can affect the outcome and interpretation of coexpression networks, we examine the assumptions and alternatives to common patterns of coexpression analysis and discuss additional topics such as acceptable datasets for coexpression analysis, the robust identification of modules, disease-related prioritization of genes and molecular systems and network meta-analysis. To accelerate coexpression research beyond modules and hubs, we highlight some emerging directions for coexpression network research that are especially relevant to complex brain disease, including the centrality-lethality relationship, integration with machine learning approaches and network pharmacology.",2014-01-01,99,1621,134,2684
2867,24370382,Knowledge discovery in clinical decision support systems for pain management: a systematic review,"Objective:                    The occurrence of pain accounts for billions of dollars in annual medical expenditures; loss of quality of life and decreased worker productivity contribute to indirect costs. As pain is highly subjective, clinical decision support systems (CDSSs) can be critical for improving the accuracy of pain assessment and offering better support for clinical decision-making. This review is focused on computer technologies for pain management that allow CDSSs to obtain knowledge from the clinical data produced by either patients or health care professionals.              Methods and materials:                    A comprehensive literature search was conducted in several electronic databases to identify relevant articles focused on computerised systems that constituted CDSSs and include data or results related to pain symptoms from patients with acute or chronic pain, published between 1992 and 2011 in the English language. In total, thirty-nine studies were analysed; thirty-two were selected from 1245 citations, and seven were obtained from reference tracking.              Results:                    The results highlighted the following clusters of computer technologies: rule-based algorithms, artificial neural networks, nonstandard set theory, and statistical learning algorithms. In addition, several methodologies were found for content processing such as terminologies, questionnaires, and scores. The median accuracy ranged from 53% to 87.5%.              Conclusions:                    Computer technologies that have been applied in CDSSs are important but not determinant in improving the systems' accuracy and the clinical practice, as evidenced by the moderate correlation among the studies. However, these systems play an important role in the design of computerised systems oriented to a patient's symptoms as is required for pain management. Several limitations related to CDSSs were observed: the lack of integration with mobile devices, the reduced use of web-based interfaces, and scarce capabilities for data to be inserted by patients.",2014-01-01,5,2094,97,2684
2872,24338557,Artificial intelligence in medicine and cardiac imaging: harnessing big data and advanced computing to provide personalized medical diagnosis and treatment,"Although advances in information technology in the past decade have come in quantum leaps in nearly every aspect of our lives, they seem to be coming at a slower pace in the field of medicine. However, the implementation of electronic health records (EHR) in hospitals is increasing rapidly, accelerated by the meaningful use initiatives associated with the Center for Medicare & Medicaid Services EHR Incentive Programs. The transition to electronic medical records and availability of patient data has been associated with increases in the volume and complexity of patient information, as well as an increase in medical alerts, with resulting ""alert fatigue"" and increased expectations for rapid and accurate diagnosis and treatment. Unfortunately, these increased demands on health care providers create greater risk for diagnostic and therapeutic errors. In the near future, artificial intelligence (AI)/machine learning will likely assist physicians with differential diagnosis of disease, treatment options suggestions, and recommendations, and, in the case of medical imaging, with cues in image interpretation. Mining and advanced analysis of ""big data"" in health care provide the potential not only to perform ""in silico"" research but also to provide ""real time"" diagnostic and (potentially) therapeutic recommendations based on empirical data. ""On demand"" access to high-performance computing and large health care databases will support and sustain our ability to achieve personalized medicine. The IBM Jeopardy! Challenge, which pitted the best all-time human players against the Watson computer, captured the imagination of millions of people across the world and demonstrated the potential to apply AI approaches to a wide variety of subject matter, including medicine. The combination of AI, big data, and massively parallel computing offers the potential to create a revolutionary way of practicing evidence-based, personalized medicine.",2014-01-01,43,1953,155,2684
2857,24555973,The use of intelligent database systems in acute pancreatitis--a systematic review,"Introduction:                    Acute pancreatitis (AP) is a complex disease with multiple aetiological factors, wide ranging severity, and multiple challenges to effective triage and management. Databases, data mining and machine learning algorithms (MLAs), including artificial neural networks (ANNs), may assist by storing and interpreting data from multiple sources, potentially improving clinical decision-making.              Aims:                    1) Identify database technologies used to store AP data, 2) collate and categorise variables stored in AP databases, 3) identify the MLA technologies, including ANNs, used to analyse AP data, and 4) identify clinical and non-clinical benefits and obstacles in establishing a national or international AP database.              Methods:                    Comprehensive systematic search of online reference databases. The predetermined inclusion criteria were all papers discussing 1) databases, 2) data mining or 3) MLAs, pertaining to AP, independently assessed by two reviewers with conflicts resolved by a third author.              Results:                    Forty-three papers were included. Three data mining technologies and five ANN methodologies were reported in the literature. There were 187 collected variables identified. ANNs increase accuracy of severity prediction, one study showed ANNs had a sensitivity of 0.89 and specificity of 0.96 six hours after admission--compare APACHE II (cutoff score 8) with 0.80 and 0.85 respectively. Problems with databases were incomplete data, lack of clinical data, diagnostic reliability and missing clinical data.              Conclusion:                    This is the first systematic review examining the use of databases, MLAs and ANNs in the management of AP. The clinical benefits these technologies have over current systems and other advantages to adopting them are identified.",2014-02-01,3,1900,82,2653
227,23818492,Identifying driver mutations from sequencing data of heterogeneous tumors in the era of personalized genome sequencing,"Distinguishing driver mutations from passenger mutations is critical to the understanding of the molecular mechanisms of carcinogenesis and for identifying prognostic and diagnostic markers as well as therapeutic targets. We reviewed the current approaches and software for identifying driver mutations from passenger mutations including both biology-based approaches and machine-learning-based approaches. We also reviewed approaches to identify driver mutations in the context of pathways or gene sets. Finally, we discussed the challenges of predicting driver mutations considering the complexities of inter- and intra-tumor heterogeneity as well as the evolution and progression of tumors.",2014-03-01,14,693,118,2625
2874,24323524,Machine learning applications in proteomics research: how the past can boost the future,"Machine learning is a subdiscipline within artificial intelligence that focuses on algorithms that allow computers to learn solving a (complex) problem from existing data. This ability can be used to generate a solution to a particularly intractable problem, given that enough data are available to train and subsequently evaluate an algorithm on. Since MS-based proteomics has no shortage of complex problems, and since publicly available data are becoming available in ever growing amounts, machine learning is fast becoming a very popular tool in the field. We here therefore present an overview of the different applications of machine learning in proteomics that together cover nearly the entire wet- and dry-lab workflow, and that address key bottlenecks in experiment planning and design, as well as in data processing and analysis.",2014-03-01,12,839,87,2625
2847,24790546,A survey on investigating the need for intelligent power-aware load balanced routing protocols for handling critical links in MANETs,"In mobile ad hoc networks connectivity is always an issue of concern. Due to dynamism in the behavior of mobile nodes, efficiency shall be achieved only with the assumption of good network infrastructure. Presence of critical links results in deterioration which should be detected in advance to retain the prevailing communication setup. This paper discusses a short survey on the specialized algorithms and protocols related to energy efficient load balancing for critical link detection in the recent literature. This paper also suggests a machine learning based hybrid power-aware approach for handling critical nodes via load balancing.",2014-03-01,1,641,132,2625
212,24096823,Application of machine learning algorithms for clinical predictive modeling: a data-mining approach in SCT,"Data collected from hematopoietic SCT (HSCT) centers are becoming more abundant and complex owing to the formation of organized registries and incorporation of biological data. Typically, conventional statistical methods are used for the development of outcome prediction models and risk scores. However, these analyses carry inherent properties limiting their ability to cope with large data sets with multiple variables and samples. Machine learning (ML), a field stemming from artificial intelligence, is part of a wider approach for data analysis termed data mining (DM). It enables prediction in complex data scenarios, familiar to practitioners and researchers. Technological and commercial applications are all around us, gradually entering clinical research. In the following review, we would like to expose hematologists and stem cell transplanters to the concepts, clinical applications, strengths and limitations of such methods and discuss current research in HSCT. The aim of this review is to encourage utilization of the ML and DM techniques in the field of HSCT, including prediction of transplantation outcome and donor selection.",2014-03-01,21,1147,106,2625
228,23793982,Survey of encoding and decoding of visual stimulus via FMRI: an image analysis perspective,"A variety of exciting scientific achievements have been made in the last few decades in brain encoding and decoding via functional magnetic resonance imaging (fMRI). This trend continues to rise in recent years, as evidenced by the increasing number of published papers in this topic and several published survey papers addressing different aspects of research issues. Essentially, these survey articles were mainly from cognitive neuroscience and neuroimaging perspectives, although computational challenges were briefly discussed. To complement existing survey articles, this paper focuses on the survey of the variety of image analysis methodologies, such as neuroimage registration, fMRI signal analysis, ROI (regions of interest) selection, machine learning algorithms, reproducibility analysis, structural and functional connectivity, and natural image analysis, which were employed in previous brain encoding/decoding research works. This paper also provides discussions of potential limitations of those image analysis methodologies and possible future improvements. It is hoped that extensive discussions of image analysis issues could contribute to the advancements of the increasingly important brain encoding/decoding field.",2014-03-01,4,1236,90,2625
2844,24834132,Chemical named entities recognition: a review on approaches and applications,"The rapid increase in the flow rate of published digital information in all disciplines has resulted in a pressing need for techniques that can simplify the use of this information. The chemistry literature is very rich with information about chemical entities. Extracting molecules and their related properties and activities from the scientific literature to ""text mine"" these extracted data and determine contextual relationships helps research scientists, particularly those in drug development. One of the most important challenges in chemical text mining is the recognition of chemical entities mentioned in the texts. In this review, the authors briefly introduce the fundamental concepts of chemical literature mining, the textual contents of chemical documents, and the methods of naming chemicals in documents. We sketch out dictionary-based, rule-based and machine learning, as well as hybrid chemical named entity recognition approaches with their applied solutions. We end with an outlook on the pros and cons of these approaches and the types of chemical entities extracted.",2014-04-01,22,1088,76,2594
2859,24552264,An overview of data mining algorithms in drug induced toxicity prediction,"The growth in chemical diversity has increased the need to adjudicate the toxicity of different chemical compounds raising the burden on the demand of animal testing. The toxicity evaluation requires time consuming and expensive undertaking, leading to the deprivation of the methods employed for screening chemicals pointing towards the need to develop more efficient toxicity assessment systems. Computational approaches have reduced the time as well as the cost for evaluating the toxicity and kinetic behavior of any chemical. The accessibility of a large amount of data and the intense need of turning this data into useful information have attracted the attention towards data mining. Machine Learning, one of the powerful data mining techniques has evolved as the most effective and potent tool for exploring new insights on combinatorial relationships among various experimental data generated. The article accounts on some sophisticated machine learning algorithms like Artificial Neural Networks (ANN), Support Vector Machine (SVM), k-mean clustering and Self Organizing Maps (SOM) with some of the available tools used for classification, sorting and toxicological evaluation of data, clarifying, how data mining and machine learning interact cooperatively to facilitate knowledge discovery. Addressing the association of some commonly used expert systems, we briefly outline some real world applications to consider the crucial role of data set partitioning.",2014-04-01,3,1470,73,2594
215,24013948,A review of feature reduction techniques in neuroimaging,"Machine learning techniques are increasingly being used in making relevant predictions and inferences on individual subjects neuroimaging scan data. Previous studies have mostly focused on categorical discrimination of patients and matched healthy controls and more recently, on prediction of individual continuous variables such as clinical scores or age. However, these studies are greatly hampered by the large number of predictor variables (voxels) and low observations (subjects) also known as the curse-of-dimensionality or small-n-large-p problem. As a result, feature reduction techniques such as feature subset selection and dimensionality reduction are used to remove redundant predictor variables and experimental noise, a process which mitigates the curse-of-dimensionality and small-n-large-p effects. Feature reduction is an essential step before training a machine learning model to avoid overfitting and therefore improving model prediction accuracy and generalization ability. In this review, we discuss feature reduction techniques used with machine learning in neuroimaging studies.",2014-04-01,95,1101,56,2594
2861,24509098,Neural circuits as computational dynamical systems,Many recent studies of neurons recorded from cortex reveal complex temporal dynamics. How such dynamics embody the computations that ultimately lead to behavior remains a mystery. Approaching this issue requires developing plausible hypotheses couched in terms of neural dynamics. A tool ideally suited to aid in this question is the recurrent neural network (RNN). RNNs straddle the fields of nonlinear dynamical systems and machine learning and have recently seen great advances in both theory and application. I summarize recent theoretical and technological advances and highlight an example of how RNNs helped to explain perplexing high-dimensional neurophysiological data in the prefrontal cortex.,2014-04-01,39,703,50,2594
2858,24553464,"Video game play, attention, and learning: how to shape the development of attention and influence learning?","Purpose of review:                    The notion that play may facilitate learning has long been touted. Here, we review how video game play may be leveraged for enhancing attentional control, allowing greater cognitive flexibility and learning and in turn new routes to better address developmental disorders.              Recent findings:                    Video games, initially developed for entertainment, appear to enhance the behavior in domains as varied as perception, attention, task switching, or mental rotation. This surprisingly wide transfer may be mediated by enhanced attentional control, allowing increased signal-to-noise ratio and thus more informed decisions.              Summary:                    The possibility of enhancing attentional control through targeted interventions, be it computerized training or self-regulation techniques, is now well established. Embedding such training in video game play is appealing, given the astounding amount of time spent by children and adults worldwide with this media. It holds the promise of increasing compliance in patients and motivation in school children, and of enhancing the use of positive impact games. Yet for all the promises, existing research indicates that not all games are created equal: a better understanding of the game play elements that foster attention and learning as well as of the strategies developed by the players is needed. Computational models from machine learning or developmental robotics provide a rich theoretical framework to develop this work further and address its impact on developmental disorders.",2014-04-01,13,1607,107,2594
2868,24361690,Progress in computational toxicology,"Introduction:                    Computational methods have been widely applied to toxicology across pharmaceutical, consumer product and environmental fields over the past decade. Progress in computational toxicology is now reviewed.              Methods:                    A literature review was performed on computational models for hepatotoxicity (e.g. for drug-induced liver injury (DILI)), cardiotoxicity, renal toxicity and genotoxicity. In addition various publications have been highlighted that use machine learning methods. Several computational toxicology model datasets from past publications were used to compare Bayesian and Support Vector Machine (SVM) learning methods.              Results:                    The increasing amounts of data for defined toxicology endpoints have enabled machine learning models that have been increasingly used for predictions. It is shown that across many different models Bayesian and SVM perform similarly based on cross validation data.              Discussion:                    Considerable progress has been made in computational toxicology in a decade in both model development and availability of larger scale or 'big data' models. The future efforts in toxicology data generation will likely provide us with hundreds of thousands of compounds that are readily accessible for machine learning models. These models will cover relevant chemistry space for pharmaceutical, consumer product and environmental applications.",2014-04-01,29,1481,36,2594
2873,24335433,Biologically inspired intelligent decision making: a commentary on the use of artificial neural networks in bioinformatics,"Artificial neural networks (ANNs) are a class of powerful machine learning models for classification and function approximation which have analogs in nature. An ANN learns to map stimuli to responses through repeated evaluation of exemplars of the mapping. This learning approach results in networks which are recognized for their noise tolerance and ability to generalize meaningful responses for novel stimuli. It is these properties of ANNs which make them appealing for applications to bioinformatics problems where interpretation of data may not always be obvious, and where the domain knowledge required for deductive techniques is incomplete or can cause a combinatorial explosion of rules. In this paper, we provide an introduction to artificial neural network theory and review some interesting recent applications to bioinformatics problems.",2014-04-01,14,851,122,2594
2845,24808218,Predictive monitoring of mobile patients by combining clinical observations with data from wearable sensors,"The majority of patients in the hospital are ambulatory and would benefit significantly from predictive and personalized monitoring systems. Such patients are well suited to having their physiological condition monitored using low-power, minimally intrusive wearable sensors. Despite data-collection systems now being manufactured commercially, allowing physiological data to be acquired from mobile patients, little work has been undertaken on the use of the resultant data in a principled manner for robust patient care, including predictive monitoring. Most current devices generate so many false-positive alerts that devices cannot be used for routine clinical practice. This paper explores principled machine learning approaches to interpreting large quantities of continuously acquired, multivariate physiological data, using wearable patient monitors, where the goal is to provide early warning of serious physiological determination, such that a degree of predictive care may be provided. We adopt a one-class support vector machine formulation, proposing a formulation for determining the free parameters of the model using partial area under the ROC curve, a method arising from the unique requirements of performing online analysis with data from patient-worn sensors. There are few clinical evaluations of machine learning techniques in the literature, so we present results from a study at the Oxford University Hospitals NHS Trust devised to investigate the large-scale clinical use of patient-worn sensors for predictive monitoring in a ward with a high incidence of patient mortality. We show that our system can combine routine manual observations made by clinical staff with the continuous data acquired from wearable sensors. Practical considerations and recommendations based on our experiences of this clinical study are discussed, in the context of a framework for personalized monitoring.",2014-05-01,24,1911,107,2564
2849,24732754,HIV-associated neuropathogenesis: a systems biology perspective for modeling and therapy,"Despite the development of powerful antiretroviral drugs, HIV-1 associated neurological disorders (HAND) will affect approximately half of those infected with HIV-1. Combined anti-retroviral therapy (cART) targets viral replication and increases T-cell counts, but it does not always control macrophage polarization, brain infection or inflammation. Moreover, it remains difficult to identify those at risk for HAND. New therapies that focus on modulating host immune response by making use of biological pathways could prove to be more effective than cART for the treatment of neuroAIDS. Additionally, while numerous HAND biomarkers have been suggested, they are of little use without methods for appropriate data integration and a systems-level interpretation. Machine learning, could be used to develop multifactorial computational models that provide clinicians and researchers with the ability to identify which factors (in what combination and relative importance) are considered important to outcome.",2014-05-01,4,1007,88,2564
2850,24723339,The ENCODE project and perspectives on pathways,"The recently completed ENCODE project is a new source of information on metabolic activity, unveiling knowledge about evolution and similarities among species, refuting the myth that most DNA is ""junk"" and has no actual function. With this expansive resource comes a challenge: integrating these new layers of information into our current knowledge of single-nucleotide polymorphisms and previously described metabolic pathways with the aim of discovering new genes and pathways related to human diseases and traits. Further, we must determine which computational methods will be most useful in this pursuit. In this paper, we speculate over the possible methods that will emerge in this new, challenging field.",2014-05-01,14,711,47,2564
201,24197932,Applications of alignment-free methods in epigenomics,"Epigenetic mechanisms play an important role in the regulation of cell type-specific gene activities, yet how epigenetic patterns are established and maintained remains poorly understood. Recent studies have supported a role of DNA sequences in recruitment of epigenetic regulators. Alignment-free methods have been applied to identify distinct sequence features that are associated with epigenetic patterns and to predict epigenomic profiles. Here, we review recent advances in such applications, including the methods to map DNA sequence to feature space, sequence comparison and prediction models. Computational studies using these methods have provided important insights into the epigenetic regulatory mechanisms.",2014-05-01,8,718,53,2564
2813,25320644,Towards predictive resistance models for agrochemicals by combining chemical and protein similarity via proteochemometric modelling,"Resistance to pesticides is an increasing problem in agriculture. Despite practices such as phased use and cycling of 'orthogonally resistant' agents, resistance remains a major risk to national and global food security. To combat this problem, there is a need for both new approaches for pesticide design, as well as for novel chemical entities themselves. As summarized in this opinion article, a technique termed 'proteochemometric modelling' (PCM), from the field of chemoinformatics, could aid in the quantification and prediction of resistance that acts via point mutations in the target proteins of an agent. The technique combines information from both the chemical and biological domain to generate bioactivity models across large numbers of ligands as well as protein targets. PCM has previously been validated in prospective, experimental work in the medicinal chemistry area, and it draws on the growing amount of bioactivity information available in the public domain. Here, two potential applications of proteochemometric modelling to agrochemical data are described, based on previously published examples from the medicinal chemistry literature.",2014-05-01,1,1161,131,2564
207,24140287,Drug name recognition in biomedical texts: a machine-learning-based method,"Currently, there is an urgent need to develop a technology for extracting drug information automatically from biomedical texts, and drug name recognition is an essential prerequisite for extracting drug information. This article presents a machine-learning-based approach to recognize drug names in biomedical texts. In this approach, a drug name dictionary is first constructed with the external resource of DrugBank and PubMed. Then a semi-supervised learning method, feature coupling generalization, is used to filter this dictionary. Finally, the dictionary look-up and the condition random field method are combined to recognize drug names. Experimental results show that our approach achieves an F-score of 92.54% on the test set of DDIExtraction2011.",2014-05-01,5,757,74,2564
2860,24531111,Simulation of safety: a review of the state of the art in road safety simulation modelling,"Recent decades have seen considerable growth in computer capabilities, data collection technology and communication mediums. This growth has had considerable impact on our ability to replicate driver behaviour and understand the processes involved in failures in the traffic system. From time to time it is necessary to assess the level of development as a basis of determining how far we have come. This paper sets out to assess the state of the art in the use of computer models to simulate and assess the level of safety in existing and future traffic systems. It reviews developments in the area of road safety simulation models. In particular, it reviews computer models of driver and vehicle behaviour within a road context. It focuses on stochastic numerical models of traffic behaviour and how reliable these are in estimating levels of safety on the traffic network. Models of this type are commonly used in the assessment of traffic systems for capacity, delay and general performance. Adding safety to this assessment regime may allow more comprehensive assessment of future traffic systems. To date the models have focused primarily on vehicular traffic that is, cars and heavy vehicles. It has been shown that these models have potential in measuring the level of conflict on parts of the network and the measure of conflict correlated well with crash statistics. Interest in the prediction of crashes and crash severity is growing and new models are focusing on the continuum of general traffic conditions, conflict, severe conflict, crash and severe crashes. The paper also explores the general data types used to develop, calibrate and validate these models. Recent technological development in in-vehicle data collection, driver simulators and machine learning offers considerable potential for improving the behavioural base, rigour and application of road safety simulation models. The paper closes with some indication of areas of future development.",2014-05-01,2,1970,90,2564
213,24029388,Multivariate classification of blood oxygen level-dependent FMRI data with diagnostic intention: a clinical perspective,"There has been a recent upsurge of reports about applications of pattern-recognition techniques from the field of machine learning to functional MR imaging data as a diagnostic tool for systemic brain disease or psychiatric disorders. Entities studied include depression, schizophrenia, attention deficit hyperactivity disorder, and neurodegenerative disorders like Alzheimer dementia. We review these recent studies which-despite the optimism from some articles-predominantly constitute explorative efforts at the proof-of-concept level. There is some evidence that, in particular, support vector machines seem to be promising. However, the field is still far from real clinical application, and much work has to be done regarding data preprocessing, model optimization, and validation. Reporting standards are proposed to facilitate future meta-analyses or systematic reviews.",2014-05-01,24,878,119,2564
2853,24713806,Invadopodia in context,"Invadopodia are dynamic protrusions in motile tumor cells whose function is to degrade extracellular matrix so that cells can enter into new environments. Invadopodia are specifically identified by microscopy as proteolytic invasive protrusions containing TKS5 and cortactin. The increasing complexity in models for the study of invadopodia, including engineered 3D environments, explants, or animal models in vivo, entails a higher level of microenvironment complexity as well as cancer cell heterogeneity. Such experimental setups are rich in information and offer the possibility of contextualizing invadopodia and other motility-related structures. That is, they hold the promise of revealing more realistic microenvironmental conditions under which the invadopodium assembles and functions or in which tumor cells switch to a different cellular phenotype (focal adhesion, lamellipodia, proliferation, and apoptosis). For such an effort, we need a systemic approach to microscopy, which will integrate information from multiple modalities. While the individual technologies needed to achieve this are mostly available, data integration and standardization is not a trivial process. In a systems microscopy approach, microscopy is used to extract information on cell phenotypes and the microenvironment while -omics technologies assess profiles of cancer cell and microenvironment genetic, transcription, translation, and protein makeups. Data are classified and linked via in silico modeling (including statistical and mathematical models and bioinformatics). Computational considerations create predictions to be validated experimentally by perturbing the system through use of genetic manipulations and molecular biology. With such a holistic approach, a deeper understanding of function of invadopodia in vivo will be reached, opening the potential for personalized diagnostics and therapies.",2014-06-01,19,1899,22,2533
2866,25756666,Machine learning in the rational design of antimicrobial peptides,"One of the most important public health issues is the microbial and bacterial resistance to conventional antibiotics by pathogen microorganisms. In recent years, many researches have been focused on the development of new antibiotics. Among these, antimicrobial peptides (AMPs) have raised as a promising alternative to combat antibioticresistant microorganisms. For this reason, many theoretical efforts have been done in the development of new computational tools for the rational design of both better and effective AMPs. In this review, we present an overview of the rational design of AMPs using machine learning techniques and new research fields.",2014-06-01,1,653,65,2533
2856,24598104,Multiparametric Analysis of Screening Data: Growing Beyond the Single Dimension to Infinity and Beyond,"Advances in instrumentation now allow the development of screening assays that are capable of monitoring multiple readouts such as transcript or protein levels, or even multiple parameters derived from images. Such advances in assay technologies highlight the complex nature of biology and disease. Harnessing this complexity requires integration of all the different parameters that can be measured rather than just monitoring a single dimension as is commonly used. Although some of the methods used to combine multiple measurements, such as principal component analysis, are commonly used for microarray analysis, biologists are not yet using many of the tools that have been developed in other fields to address such issues. Visualization of multiparametric data sets is one of the major challenges in this field, and a depiction of the results in a manner that can be readily interpreted is essential. This article describes a number of assay systems being used to generate such data sets en masse, and the methods being applied to their visualization and analysis. We also discuss some of the challenges of applying methods developed in other fields to biology.",2014-06-01,12,1167,102,2533
206,24158807,Bioinformatics tools for predicting GPCR gene functions,"The automatic classification of GPCRs by bioinformatics methodology can provide functional information for new GPCRs in the whole 'GPCR proteome' and this information is important for the development of novel drugs. Since GPCR proteome is classified hierarchically, general ways for GPCR function prediction are based on hierarchical classification. Various computational tools have been developed to predict GPCR functions; those tools use not simple sequence searches but more powerful methods, such as alignment-free methods, statistical model methods, and machine learning methods used in protein sequence analysis, based on learning datasets. The first stage of hierarchical function prediction involves the discrimination of GPCRs from non-GPCRs and the second stage involves the classification of the predicted GPCR candidates into family, subfamily, and sub-subfamily levels. Then, further classification is performed according to their protein-protein interaction type: binding G-protein type, oligomerized partner type, etc. Those methods have achieved predictive accuracies of around 90 %. Finally, I described the future subject of research of the bioinformatics technique about functional prediction of GPCR.",2014-06-01,2,1221,55,2533
208,24136011,Improvement of adequate use of warfarin for the elderly using decision tree-based approaches,"Objectives:                    Due to the narrow therapeutic range and high drug-to-drug interactions (DDIs), improving the adequate use of warfarin for the elderly is crucial in clinical practice. This study examines whether the effectiveness of using warfarin among elderly inpatients can be improved when machine learning techniques and data from the laboratory information system are incorporated.              Methods:                    Having employed 288 validated clinical cases in the DDI group and 89 cases in the non-DDI group, we evaluate the prediction performance of seven classification techniques, with and without an Adaptive Boosting (AdaBoost) algorithm. Measures including accuracy, sensitivity, specificity and area under the curve are used to evaluate model performance.              Results:                    Decision tree-based classifiers outperform other investigated classifiers in all evaluation measures. The classifiers supplemented with AdaBoost can generally improve the performance. In addition, weight, congestive heart failure, and gender are among the top three critical variables affecting prediction accuracy for the non-DDI group, while age, ALT, and warfarin doses are the most influential factors for the DDI group.              Conclusion:                    Medical decision support systems incorporating decision tree-based approaches improve predicting performance and thus may serve as a supplementary tool in clinical practice. Information from laboratory tests and inpatients' history should not be ignored because related variables are shown to be decisive in our prediction models, especially when the DDIs exist.",2014-06-01,4,1666,92,2533
2830,25048127,Machine learning-based methods for prediction of linear B-cell epitopes,"B-cell epitope prediction facilitates immunologists in designing peptide-based vaccine, diagnostic test, disease prevention, treatment, and antibody production. In comparison with T-cell epitope prediction, the performance of variable length B-cell epitope prediction is still yet to be satisfied. Fortunately, due to increasingly available verified epitope databases, bioinformaticians could adopt machine learning-based algorithms on all curated data to design an improved prediction tool for biomedical researchers. Here, we have reviewed related epitope prediction papers, especially those for linear B-cell epitope prediction. It should be noticed that a combination of selected propensity scales and statistics of epitope residues with machine learning-based tools formulated a general way for constructing linear B-cell epitope prediction systems. It is also observed from most of the comparison results that the kernel method of support vector machine (SVM) classifier outperformed other machine learning-based approaches. Hence, in this chapter, except reviewing recently published papers, we have introduced the fundamentals of B-cell epitope and SVM techniques. In addition, an example of linear B-cell prediction system based on physicochemical features and amino acid combinations is illustrated in details.",2014-06-01,9,1320,71,2533
2881,24272434,Introduction to machine learning,"The machine learning field, which can be briefly defined as enabling computers make successful predictions using past experiences, has exhibited an impressive development recently with the help of the rapid increase in the storage capacity and processing power of computers. Together with many other disciplines, machine learning methods have been widely employed in bioinformatics. The difficulties and cost of biological analyses have led to the development of sophisticated machine learning approaches for this application area. In this chapter, we first review the fundamental concepts of machine learning such as feature assessment, unsupervised versus supervised learning and types of classification. Then, we point out the main issues of designing machine learning experiments and their performance evaluation. Finally, we introduce some supervised learning methods.",2014-06-01,31,873,32,2533
2885,24245763,Machine learning and tubercular drug target recognition,"Tuberculosis (TB) remains to be a global major public-health threat, causing millions of deaths each year. A major difficulty in dealing with TB is that the causative bacterium, Mycobacterium tuberculosis, can persist in host tissue for a long period of time even after treatment. Mycobacterial persistence has become a central research focus for developing next-generation TB drugs. Latest genomic technology has enabled a high-throughput approach for identifying potential TB drug targets. Each gene product can be screened for its uniqueness to the TB metabolism, host-pathogen discrimination, essentiality for survival, and potential for chemical binding, among other properties. However, the exhaustive search for useful drug targets over the entire genome would not be productive as expected in practice. On the other hand, the problem can be formulated as pattern recognition or inductive learning and tackled with rule-based or statistically based learning algorithms. Here we review the perspective that combines machine learning and genomics for drug discovery in tuberculosis.",2014-06-01,0,1087,55,2533
2818,25201114,Splicing code modeling,"How do cis and trans elements involved in pre-mRNA splicing come together to form a splicing ""code""? This question has been a driver of much of the research involving RNA biogenesis. The variability of splicing outcome across developmental stages and between tissues coupled with association of splicing defects with numerous diseases highlights the importance of such a code. However, the sheer number of elements involved in splicing regulation and the context-specific manner of their operation have made the derivation of such a code challenging. Recently, machine learning-based methods have been developed to infer computational models for a splicing code. These methods use high-throughput experiments measuring mRNA expression at exonic resolution and binding locations of RNA-binding proteins (RBPs) to infer what the regulatory elements that control the inclusion of a given pre-mRNA segment are. The inferred regulatory models can then be applied to genomic sequences or experimental conditions that have not been measured to predict splicing outcome. Moreover, the models themselves can be interrogated to identify new regulatory mechanisms, which can be subsequently tested experimentally. In this chapter, we survey the current state of this technology, and illustrate how it can be applied by non-computational or RNA splicing experts to study regulation of specific exons by using the AVISPA web tool.",2014-06-01,1,1417,22,2533
2851,24718104,Multivariate data analysis and machine learning in Alzheimer's disease with a focus on structural magnetic resonance imaging,"Machine learning algorithms and multivariate data analysis methods have been widely utilized in the field of Alzheimer's disease (AD) research in recent years. Advances in medical imaging and medical image analysis have provided a means to generate and extract valuable neuroimaging information. Automatic classification techniques provide tools to analyze this information and observe inherent disease-related patterns in the data. In particular, these classifiers have been used to discriminate AD patients from healthy control subjects and to predict conversion from mild cognitive impairment to AD. In this paper, recent studies are reviewed that have used machine learning and multivariate analysis in the field of AD research. The main focus is on studies that used structural magnetic resonance imaging (MRI), but studies that included positron emission tomography and cerebrospinal fluid biomarkers in addition to MRI are also considered. A wide variety of materials and methods has been employed in different studies, resulting in a range of different outcomes. Influential factors such as classifiers, feature extraction algorithms, feature selection methods, validation approaches, and cohort properties are reviewed, as well as key MRI-based and multi-modal based studies. Current and future trends are discussed.",2014-06-01,35,1325,124,2533
2839,24987556,Big data analysis using modern statistical and machine learning methods in medicine,"In this article we introduce modern statistical machine learning and bioinformatics approaches that have been used in learning statistical relationships from big data in medicine and behavioral science that typically include clinical, genomic (and proteomic) and environmental variables. Every year, data collected from biomedical and behavioral science is getting larger and more complicated. Thus, in medicine, we also need to be aware of this trend and understand the statistical tools that are available to analyze these datasets. Many statistical analyses that are aimed to analyze such big datasets have been introduced recently. However, given many different types of clinical, genomic, and environmental data, it is rather uncommon to see statistical methods that combine knowledge resulting from those different data types. To this extent, we will introduce big data in terms of clinical data, single nucleotide polymorphism and gene expression studies and their interactions with environment. In this article, we will introduce the concept of well-known regression analyses such as linear and logistic regressions that has been widely used in clinical data analyses and modern statistical models such as Bayesian networks that has been introduced to analyze more complicated data. Also we will discuss how to represent the interaction among clinical, genomic, and environmental data in using modern statistical models. We conclude this article with a promising modern statistical method called Bayesian networks that is suitable in analyzing big data sets that consists with different type of large data from clinical, genomic, and environmental data. Such statistical model form big data will provide us with more comprehensive understanding of human physiology and disease.",2014-06-01,19,1785,83,2533
2842,24895853,Structural bioinformatics of the interactome,"The past decade has seen a dramatic expansion in the number and range of techniques available to obtain genome-wide information and to analyze this information so as to infer both the functions of individual molecules and how they interact to modulate the behavior of biological systems. Here, we review these techniques, focusing on the construction of physical protein-protein interaction networks, and highlighting approaches that incorporate protein structure, which is becoming an increasingly important component of systems-level computational techniques. We also discuss how network analyses are being applied to enhance our basic understanding of biological systems and their disregulation, as well as how these networks are being used in drug development.",2014-06-01,15,764,44,2533
2825,25112429,Extending statistical boosting. An overview of recent methodological developments,"Background:                    Boosting algorithms to simultaneously estimate and select predictor effects in statistical models have gained substantial interest during the last decade.              Objectives:                    This review highlights recent methodological developments regarding boosting algorithms for statistical modelling especially focusing on topics relevant for biomedical research.              Methods:                    We suggest a unified framework for gradient boosting and likelihood-based boosting (statistical boosting) which have been addressed separately in the literature up to now.              Results:                    The methodological developments on statistical boosting during the last ten years can be grouped into three different lines of research: i) efforts to ensure variable selection leading to sparser models, ii) developments regarding different types of predictor effects and how to choose them, iii) approaches to extend the statistical boosting framework to new regression settings.              Conclusions:                    Statistical boosting algorithms have been adapted to carry out unbiased variable selection and automated model choice during the fitting process and can nowadays be applied in almost any regression setting in combination with a large amount of different types of predictor effects.",2014-06-01,8,1369,81,2533
2852,24713999,Current breathomics--a review on data pre-processing techniques and machine learning in metabolomics breath analysis,"We define breathomics as the metabolomics study of exhaled air. It is a strongly emerging metabolomics research field that mainly focuses on health-related volatile organic compounds (VOCs). Since the amount of these compounds varies with health status, breathomics holds great promise to deliver non-invasive diagnostic tools. Thus, the main aim of breathomics is to find patterns of VOCs related to abnormal (for instance inflammatory) metabolic processes occurring in the human body. Recently, analytical methods for measuring VOCs in exhaled air with high resolution and high throughput have been extensively developed. Yet, the application of machine learning methods for fingerprinting VOC profiles in the breathomics is still in its infancy. Therefore, in this paper, we describe the current state of the art in data pre-processing and multivariate analysis of breathomics data. We start with the detailed pre-processing pipelines for breathomics data obtained from gas-chromatography mass spectrometry and an ion-mobility spectrometer coupled to multi-capillary columns. The outcome of data pre-processing is a matrix containing the relative abundances of a set of VOCs for a group of patients under different conditions (e.g. disease stage, treatment). Independently of the utilized analytical method, the most important question, 'which VOCs are discriminatory?', remains the same. Answers can be given by several modern machine learning techniques (multivariate statistics) and, therefore, are the focus of this paper. We demonstrate the advantages as well the drawbacks of such techniques. We aim to help the community to understand how to profit from a particular method. In parallel, we hope to make the community aware of the existing data fusion methods, as yet unresearched in breathomics.",2014-06-01,40,1806,116,2533
2836,25002826,Machine learning methods in the computational biology of cancer,"The objectives of this Perspective paper are to review some recent advances in sparse feature selection for regression and classification, as well as compressed sensing, and to discuss how these might be used to develop tools to advance personalized cancer therapy. As an illustration of the possibilities, a new algorithm for sparse regression is presented and is applied to predict the time to tumour recurrence in ovarian cancer. A new algorithm for sparse feature selection in classification problems is presented, and its validation in endometrial cancer is briefly discussed. Some open problems are also presented.",2014-07-01,4,620,63,2503
2837,24998888,"Machine learning, medical diagnosis, and biomedical engineering research - commentary","A large number of papers are appearing in the biomedical engineering literature that describe the use of machine learning techniques to develop classifiers for detection or diagnosis of disease. However, the usefulness of this approach in developing clinically validated diagnostic techniques so far has been limited and the methods are prone to overfitting and other problems which may not be immediately apparent to the investigators. This commentary is intended to help sensitize investigators as well as readers and reviewers of papers to some potential pitfalls in the development of classifiers, and suggests steps that researchers can take to help avoid these problems. Building classifiers should be viewed not simply as an add-on statistical analysis, but as part and parcel of the experimental process. Validation of classifiers for diagnostic applications should be considered as part of a much larger process of establishing the clinical validity of the diagnostic technique.",2014-07-01,55,987,85,2503
2841,24935820,Computational validation of the motor contribution to speech perception,"Action perception and recognition are core abilities fundamental for human social interaction. A parieto-frontal network (the mirror neuron system) matches visually presented biological motion information onto observers' motor representations. This process of matching the actions of others onto our own sensorimotor repertoire is thought to be important for action recognition, providing a non-mediated ""motor perception"" based on a bidirectional flow of information along the mirror parieto-frontal circuits. State-of-the-art machine learning strategies for hand action identification have shown better performances when sensorimotor data, as opposed to visual information only, are available during learning. As speech is a particular type of action (with acoustic targets), it is expected to activate a mirror neuron mechanism. Indeed, in speech perception, motor centers have been shown to be causally involved in the discrimination of speech sounds. In this paper, we review recent neurophysiological and machine learning-based studies showing (a) the specific contribution of the motor system to speech perception and (b) that automatic phone recognition is significantly improved when motor data are used during training of classifiers (as opposed to learning from purely auditory data).",2014-07-01,1,1295,71,2503
2840,24975859,Evaluation and construction of diagnostic criteria for inclusion body myositis,"Objective:                    To use patient data to evaluate and construct diagnostic criteria for inclusion body myositis (IBM), a progressive disease of skeletal muscle.              Methods:                    The literature was reviewed to identify all previously proposed IBM diagnostic criteria. These criteria were applied through medical records review to 200 patients diagnosed as having IBM and 171 patients diagnosed as having a muscle disease other than IBM by neuromuscular specialists at 2 institutions, and to a validating set of 66 additional patients with IBM from 2 other institutions. Machine learning techniques were used for unbiased construction of diagnostic criteria.              Results:                    Twenty-four previously proposed IBM diagnostic categories were identified. Twelve categories all performed with high (97%) specificity but varied substantially in their sensitivities (11%-84%). The best performing category was European Neuromuscular Centre 2013 probable (sensitivity of 84%). Specialized pathologic features and newly introduced strength criteria (comparative knee extension/hip flexion strength) performed poorly. Unbiased data-directed analysis of 20 features in 371 patients resulted in construction of higher-performing data-derived diagnostic criteria (90% sensitivity and 96% specificity).              Conclusions:                    Published expert consensus-derived IBM diagnostic categories have uniformly high specificity but wide-ranging sensitivities. High-performing IBM diagnostic category criteria can be developed directly from principled unbiased analysis of patient data.              Classification of evidence:                    This study provides Class II evidence that published expert consensus-derived IBM diagnostic categories accurately distinguish IBM from other muscle disease with high specificity but wide-ranging sensitivities.",2014-07-01,39,1914,78,2503
2831,25046016,Automatic fall monitoring: a review,"Falls and fall-related injuries are major incidents, especially for elderly people, which often mark the onset of major deterioration of health. More than one-third of home-dwelling people aged 65 or above and two-thirds of those in residential care fall once or more each year. Reliable fall detection, as well as prevention, is an important research topic for monitoring elderly living alone in residential or hospital units. The aim of this study is to review the existing fall detection systems and some of the key research challenges faced by the research community in this field. We categorize the existing platforms into two groups: wearable and ambient devices; the classification methods are divided into rule-based and machine learning techniques. The relative merit and potential drawbacks are discussed, and we also outline some of the outstanding research challenges that emerging new platforms need to address.",2014-07-01,30,924,35,2503
2864,24478134,Probability estimation with machine learning methods for dichotomous and multicategory outcome: theory,"Probability estimation for binary and multicategory outcome using logistic and multinomial logistic regression has a long-standing tradition in biostatistics. However, biases may occur if the model is misspecified. In contrast, outcome probabilities for individuals can be estimated consistently with machine learning approaches, including k-nearest neighbors (k-NN), bagged nearest neighbors (b-NN), random forests (RF), and support vector machines (SVM). Because machine learning methods are rarely used by applied biostatisticians, the primary goal of this paper is to explain the concept of probability estimation with these methods and to summarize recent theoretical findings. Probability estimation in k-NN, b-NN, and RF can be embedded into the class of nonparametric regression learning machines; therefore, we start with the construction of nonparametric regression estimates and review results on consistency and rates of convergence. In SVMs, outcome probabilities for individuals are estimated consistently by repeatedly solving classification problems. For SVMs we review classification problem and then dichotomous probability estimation. Next we extend the algorithms for estimating probabilities using k-NN, b-NN, and RF to multicategory outcomes and discuss approaches for the multicategory probability estimation problem using SVM. In simulation studies for dichotomous and multicategory dependent variables we demonstrate the general validity of the machine learning methods and compare it with logistic regression. However, each method fails in at least one simulation scenario. We conclude with a discussion of the failures and give recommendations for selecting and tuning the methods. Applications to real data and example code are provided in a companion article (doi:10.1002/bimj.201300077).",2014-07-01,15,1817,102,2503
2820,25187715,Diagnostic and therapeutic utility of neuroimaging in depression: an overview,"A growing number of studies have used neuroimaging to further our understanding of how brain structure and function are altered in major depression. More recently, these techniques have begun to show promise for the diagnosis and treatment of depression, both as aids to conventional methods and as methods in their own right. In this review, we describe recent neuroimaging findings in the field that might aid diagnosis and improve treatment accuracy. Overall, major depression is associated with numerous structural and functional differences in neural systems involved in emotion processing and mood regulation. Furthermore, several studies have shown that the structure and function of these systems is changed by pharmacological and psychological treatments of the condition and that these changes in candidate brain regions might predict clinical response. More recently, ""machine learning"" methods have used neuroimaging data to categorize individual patients according to their diagnostic status and predict treatment response. Despite being mostly limited to group-level comparisons at present, with the introduction of new methods and more naturalistic studies, neuroimaging has the potential to become part of the clinical armamentarium and may improve diagnostic accuracy and inform treatment choice at the patient level.",2014-08-01,23,1334,77,2472
198,24231119,Review of machine learning and signal processing techniques for automated electrode selection in high-density microelectrode arrays,"Recently developed CMOS-based microprobes contain hundreds of electrodes on a single shaft with interelectrode distances as small as 30 m. So far, neuroscientists manually select a subset of those electrodes depending on their appraisal of the ""usefulness"" of the recorded signals, which makes the process subjective but more importantly too time consuming to be useable in practice. The ever-increasing number of recording electrodes on microelectrode probes calls for an automated selection of electrodes containing ""good quality signals"" or ""signals of interest."" This article reviews the different criteria for electrode selection as well as the basic signal processing steps to prepare the data to compute those criteria. We discuss three of them. The first two select the electrodes based on ""signal quality."" The first criterion computes the penalized signal-to-noise ratio (SNR); the second criterion models the neuroscientist's appraisal of signal quality. Last, our most recent work allows the selection of electrodes that capture particular anatomical cell types. The discussed algorithms perform what is called in the literature ""electronic depth control"" in contrast to the mechanical repositioning of the electrode shafts in search of ""good quality signals"" or ""signals of interest.""",2014-08-01,0,1298,131,2472
2843,24857941,Characterizing EMG data using machine-learning tools,"Effective electromyographic (EMG) signal characterization is critical in the diagnosis of neuromuscular disorders. Machine-learning based pattern classification algorithms are commonly used to produce such characterizations. Several classifiers have been investigated to develop accurate and computationally efficient strategies for EMG signal characterization. This paper provides a critical review of some of the classification methodologies used in EMG characterization, and presents the state-of-the-art accomplishments in this field, emphasizing neuromuscular pathology. The techniques studied are grouped by their methodology, and a summary of the salient findings associated with each method is presented.",2014-08-01,10,712,52,2472
2823,25123745,Managing large-scale genomic datasets and translation into clinical practice,"Objective:                    To summarize excellent current research in the field of Bioinformatics and Translational Informatics with application in the health domain.              Method:                    We provide a synopsis of the articles selected for the IMIA Yearbook 2014, from which we attempt to derive a synthetic overview of current and future activities in the field. A first step of selection was performed by querying MEDLINE with a list of MeSH descriptors completed by a list of terms adapted to the section. Each section editor evaluated independently the set of 1,851 articles and 15 articles were retained for peer-review.              Results:                    The selection and evaluation process of this Yearbook's section on Bioinformatics and Translational Informatics yielded three excellent articles regarding data management and genome medicine. In the first article, the authors present VEST (Variant Effect Scoring Tool) which is a supervised machine learning tool for prioritizing variants found in exome sequencing projects that are more likely involved in human Mendelian diseases. In the second article, the authors show how to infer surnames of male individuals by crossing anonymous publicly available genomic data from the Y chromosome and public genealogy data banks. The third article presents a statistical framework called iCluster+ that can perform pattern discovery in integrated cancer genomic data. This framework was able to determine different tumor subtypes in colon cancer.              Conclusions:                    The current research activities still attest the continuous convergence of Bioinformatics and Medical Informatics, with a focus this year on large-scale biological, genomic, and Electronic Health Records data. Indeed, there is a need for powerful tools for managing and interpreting complex data, but also a need for user-friendly tools developed for the clinicians in their daily practice. All the recent research and development efforts are contributing to the challenge of impacting clinically the results and even going towards a personalized medicine in the near future.",2014-08-01,5,2149,76,2472
2835,25008281,Text mining of cancer-related information: review of current status and future directions,"Purpose:                    This paper reviews the research literature on text mining (TM) with the aim to find out (1) which cancer domains have been the subject of TM efforts, (2) which knowledge resources can support TM of cancer-related information and (3) to what extent systems that rely on knowledge and computational methods can convert text data into useful clinical information. These questions were used to determine the current state of the art in this particular strand of TM and suggest future directions in TM development to support cancer research.              Methods:                    A review of the research on TM of cancer-related information was carried out. A literature search was conducted on the Medline database as well as IEEE Xplore and ACM digital libraries to address the interdisciplinary nature of such research. The search results were supplemented with the literature identified through Google Scholar.              Results:                    A range of studies have proven the feasibility of TM for extracting structured information from clinical narratives such as those found in pathology or radiology reports. In this article, we provide a critical overview of the current state of the art for TM related to cancer. The review highlighted a strong bias towards symbolic methods, e.g. named entity recognition (NER) based on dictionary lookup and information extraction (IE) relying on pattern matching. The F-measure of NER ranges between 80% and 90%, while that of IE for simple tasks is in the high 90s. To further improve the performance, TM approaches need to deal effectively with idiosyncrasies of the clinical sublanguage such as non-standard abbreviations as well as a high degree of spelling and grammatical errors. This requires a shift from rule-based methods to machine learning following the success of similar trends in biological applications of TM. Machine learning approaches require large training datasets, but clinical narratives are not readily available for TM research due to privacy and confidentiality concerns. This issue remains the main bottleneck for progress in this area. In addition, there is a need for a comprehensive cancer ontology that would enable semantic representation of textual information found in narrative reports.",2014-09-01,47,2303,89,2441
217,23933754,Similarity-based machine learning methods for predicting drug-target interactions: a brief review,"Computationally predicting drug-target interactions is useful to select possible drug (or target) candidates for further biochemical verification. We focus on machine learning-based approaches, particularly similarity-based methods that use drug and target similarities, which show relationships among drugs and those among targets, respectively. These two similarities represent two emerging concepts, the chemical space and the genomic space. Typically, the methods combine these two types of similarities to generate models for predicting new drug-target interactions. This process is also closely related to a lot of work in pharmacogenomics or chemical biology that attempt to understand the relationships between the chemical and genomic spaces. This background makes the similarity-based approaches attractive and promising. This article reviews the similarity-based machine learning methods for predicting drug-target interactions, which are state-of-the-art and have aroused great interest in bioinformatics. We describe each of these methods briefly, and empirically compare these methods under a uniform experimental setting to explore their advantages and limitations.",2014-09-01,69,1180,97,2441
2838,24996618,Host genetic factors predisposing to HIV-associated neurocognitive disorder,"The success of combination antiretroviral therapy (cART) in transforming the lives of HIV-infected individuals with access to these drugs is tempered by the increasing threat of HIV-associated neurocognitive disorders (HAND) to their overall health and quality of life. Intensive investigations over the past two decades have underscored the role of host immune responses, inflammation, and monocyte-derived macrophages in HAND, but the precise pathogenic mechanisms underlying HAND remain only partially delineated. Complicating research efforts and therapeutic drug development are the sheer complexity of HAND phenotypes, diagnostic imprecision, and the growing intersection of chronic immune activation with aging-related comorbidities. Yet, genetic studies still offer a powerful means of advancing individualized care for HIV-infected individuals at risk. There is an urgent need for 1) longitudinal studies using consistent phenotypic definitions of HAND in HIV-infected subpopulations at very high risk of being adversely impacted, such as children, 2) tissue studies that correlate neuropathological changes in multiple brain regions with genomic markers in affected individuals and with changes at the RNA, epigenomic, and/or protein levels, and 3) genetic association studies using more sensitive subphenotypes of HAND. The NIH Brain Initiative and Human Connectome Project, coupled with rapidly evolving systems biology and machine learning approaches for analyzing high-throughput genetic, transcriptomic and epigenetic data, hold promise for identifying actionable biological processes and gene networks that underlie HAND. This review summarizes the current state of understanding of host genetic factors predisposing to HAND in light of past challenges and suggests some priorities for future research to advance the understanding and clinical management of HAND in the cART era.",2014-09-01,22,1895,75,2441
2824,25112457,Drug-target interaction prediction via chemogenomic space: learning-based methods,"Introduction:                    Identification of the interaction between drugs and target proteins is a crucial task in genomic drug discovery. The in silico prediction is an appropriate alternative for the laborious and costly experimental process of drug-target interaction prediction. Developing a variety of computational methods opens a new direction in analyzing and detecting new drug-target pairs.              Areas covered:                    In this review, we will focus on chemogenomic methods which have established a learning framework for predicting drug-target interactions. Learning-based methods are classified into supervised and semi-supervised, and the supervised learning methods are studied as two separate parts including similarity-based methods and feature-based methods.              Expert opinion:                    In spite of many improvements for pharmacology applications by learning-based methods, there are many over simplification settings in construction of predictive models that may lead to over-optimistic results on drug-target interaction prediction.",2014-09-01,16,1096,81,2441
2821,25183786,Using high-throughput transcriptomic data for prognosis: a critical overview and perspectives,"Accurate prognosis and prediction of response to therapy are essential for personalized treatment of cancer. Even though many prognostic gene lists and predictors have been proposed, especially for breast cancer, high-throughput ""omic"" methods have so far not revolutionized clinical practice, and their clinical utility has not been satisfactorily established. Different prognostic gene lists have very few shared genes, the biological meaning of most signatures is unclear, and the published success rates are considered to be overoptimistic. This review examines critically the manner in which prognostic classifiers are derived using machine-learning methods and suggests reasons for the shortcomings and problems listed above. Two approaches that may hold hope for obtaining improved prognosis are presented. Both are based on using existing prior knowledge; one proposes combining molecular ""omic"" predictors with established clinical ones, and the second infers biologically relevant pathway deregulation scores for each tumor from expression data, and uses this representation to study and stratify individual tumors. Approaches such as the second one are referred to in the physics literature as ""phenomenology""; they will, hopefully, play a significant role in future studies of cancer. See all articles in this Cancer Research section, ""Physics in Cancer Research.""",2014-09-01,20,1376,93,2441
2810,25392680,Optimization of Network Topology in Computer-Aided Detection Schemes Using Phased Searching with NEAT in a Time-Scaled Framework,"In the field of computer-aided mammographic mass detection, many different features and classifiers have been tested. Frequently, the relevant features and optimal topology for the artificial neural network (ANN)-based approaches at the classification stage are unknown, and thus determined by trial-and-error experiments. In this study, we analyzed a classifier that evolves ANNs using genetic algorithms (GAs), which combines feature selection with the learning task. The classifier named ""Phased Searching with NEAT in a Time-Scaled Framework"" was analyzed using a dataset with 800 malignant and 800 normal tissue regions in a 10-fold cross-validation framework. The classification performance measured by the area under a receiver operating characteristic (ROC) curve was 0.856  0.029. The result was also compared with four other well-established classifiers that include fixed-topology ANNs, support vector machines (SVMs), linear discriminant analysis (LDA), and bagged decision trees. The results show that Phased Searching outperformed the LDA and bagged decision tree classifiers, and was only significantly outperformed by SVM. Furthermore, the Phased Searching method required fewer features and discarded superfluous structure or topology, thus incurring a lower feature computational and training and validation time requirement. Analyses performed on the network complexities evolved by Phased Searching indicate that it can evolve optimal network topologies based on its complexification and simplification parameter selection process. From the results, the study also concluded that the three classifiers - SVM, fixed-topology ANN, and Phased Searching with NeuroEvolution of Augmenting Topologies (NEAT) in a Time-Scaled Framework - are performing comparably well in our mammographic mass detection scheme.",2014-10-01,1,1825,128,2411
2815,25285755,Integration of biological parts toward the synthesis of a minimal cell,"Various approaches are taken to construct synthetic cells in the laboratory, a challenging goal that became experimentally imaginable over the past two decades. The construction of protocells, which explores scenarios of the origin of life, has been the original motivations for such projects. With the advent of the synthetic biology era, bottom-up engineering approaches to synthetic cells are now conceivable. The modular design emerges as the most robust framework to construct a minimal cell from natural molecular components. Although significant advances have been made for each piece making this complex puzzle, the integration of the three fundamental parts, information-metabolism-self-organization, into cell-sized liposomes capable of sustained reproduction has failed so far. Our inability to connect these three elements is also a major limitation in this research area. New methods, such as machine learning coupled to high-throughput techniques, should be exploited to accelerate the cell-free synthesis of complex biochemical systems.",2014-10-01,34,1051,70,2411
2826,25106933,A systematic review of predictive modeling for bronchiolitis,"Purpose:                    Bronchiolitis is the most common cause of illness leading to hospitalization in young children. At present, many bronchiolitis management decisions are made subjectively, leading to significant practice variation among hospitals and physicians caring for children with bronchiolitis. To standardize care for bronchiolitis, researchers have proposed various models to predict the disease course to help determine a proper management plan. This paper reviews the existing state of the art of predictive modeling for bronchiolitis. Predictive modeling for respiratory syncytial virus (RSV) infection is covered whenever appropriate, as RSV accounts for about 70% of bronchiolitis cases.              Methods:                    A systematic review was conducted through a PubMed search up to April 25, 2014. The literature on predictive modeling for bronchiolitis was retrieved using a comprehensive search query, which was developed through an iterative process. Search results were limited to human subjects, the English language, and children (birth to 18 years).              Results:                    The literature search returned 2312 references in total. After manual review, 168 of these references were determined to be relevant and are discussed in this paper. We identify several limitations and open problems in predictive modeling for bronchiolitis, and provide some preliminary thoughts on how to address them, with the hope to stimulate future research in this domain.              Conclusions:                    Many problems remain open in predictive modeling for bronchiolitis. Future studies will need to address them to achieve optimal predictive models.",2014-10-01,15,1703,60,2411
2809,25392685,Text mining in cancer gene and pathway prioritization,"Prioritization of cancer implicated genes has received growing attention as an effective way to reduce wet lab cost by computational analysis that ranks candidate genes according to the likelihood that experimental verifications will succeed. A multitude of gene prioritization tools have been developed, each integrating different data sources covering gene sequences, differential expressions, function annotations, gene regulations, protein domains, protein interactions, and pathways. This review places existing gene prioritization tools against the backdrop of an integrative Omic hierarchy view toward cancer and focuses on the analysis of their text mining components. We explain the relatively slow progress of text mining in gene prioritization, identify several challenges to current text mining methods, and highlight a few directions where more effective text mining algorithms may improve the overall prioritization task and where prioritizing the pathways may be more desirable than prioritizing only genes.",2014-10-01,14,1022,53,2411
2811,25360145,Kernel-based whole-genome prediction of complex traits: a review,"Prediction of genetic values has been a focus of applied quantitative genetics since the beginning of the 20th century, with renewed interest following the advent of the era of whole genome-enabled prediction. Opportunities offered by the emergence of high-dimensional genomic data fueled by post-Sanger sequencing technologies, especially molecular markers, have driven researchers to extend Ronald Fisher and Sewall Wright's models to confront new challenges. In particular, kernel methods are gaining consideration as a regression method of choice for genome-enabled prediction. Complex traits are presumably influenced by many genomic regions working in concert with others (clearly so when considering pathways), thus generating interactions. Motivated by this view, a growing number of statistical approaches based on kernels attempt to capture non-additive effects, either parametrically or non-parametrically. This review centers on whole-genome regression using kernel methods applied to a wide range of quantitative traits of agricultural importance in animals and plants. We discuss various kernel-based approaches tailored to capturing total genetic variation, with the aim of arriving at an enhanced predictive performance in the light of available genome annotation information. Connections between prediction machines born in animal breeding, statistics, and machine learning are revisited, and their empirical prediction performance is discussed. Overall, while some encouraging results have been obtained with non-parametric kernels, recovering non-additive genetic variation in a validation dataset remains a challenge in quantitative genetics.",2014-10-01,51,1662,64,2411
2812,25360109,Bayesian networks in neuroscience: a survey,"Bayesian networks are a type of probabilistic graphical models lie at the intersection between statistics and machine learning. They have been shown to be powerful tools to encode dependence relationships among the variables of a domain under uncertainty. Thanks to their generality, Bayesian networks can accommodate continuous and discrete variables, as well as temporal processes. In this paper we review Bayesian networks and how they can be learned automatically from data by means of structure learning algorithms. Also, we examine how a user can take advantage of these networks for reasoning by exact or approximate inference algorithms that propagate the given evidence through the graphical structure. Despite their applicability in many fields, they have been little used in neuroscience, where they have focused on specific problems, like functional connectivity analysis from neuroimaging data. Here we survey key research in neuroscience where Bayesian networks have been used with different aims: discover associations between variables, perform probabilistic reasoning over the model, and classify new observations with and without supervision. The networks are learned from data of any kind-morphological, electrophysiological, -omics and neuroimaging-, thereby broadening the scope-molecular, cellular, structural, functional, cognitive and medical- of the brain aspects to be studied.",2014-10-01,15,1403,43,2411
219,23926206,Bioinformatics approaches for improved recombinant protein production in Escherichia coli: protein solubility prediction,"The solubility of recombinant protein expressed in Escherichia coli often represents the production yield. However, up-to-date, instances of successful production of soluble recombinant proteins in E. coli expression system with high yield remain scarce. This is mainly due to the difficulties in improving the overall production capacity, as most of the well-established strategies usually involve a series of trial and error steps with unguaranteed success. One way to concurrently improve the production yield and minimize the production cost would be incorporating the potency of bioinformatics tools to conduct in silico studies, which forecasts the outcome before actual experimental work. In this article, we review and compare seven prediction tools available, which predict the solubility of protein expressed in E. coli, using the following criteria: prediction performance, usability, utility, prediction tool development and validation methodologies. This comprehensive review will be a valuable resource for researchers with limited prior experience in bioinformatics tools. As such, this will facilitate their choice of appropriate tools for studies related to enhancement of intracellular recombinant protein production in E. coli.",2014-11-01,15,1246,120,2380
2848,24769242,Computational modeling of neural plasticity for self-organization of neural networks,"Self-organization in biological nervous systems during the lifetime is known to largely occur through a process of plasticity that is dependent upon the spike-timing activity in connected neurons. In the field of computational neuroscience, much effort has been dedicated to building up computational models of neural plasticity to replicate experimental data. Most recently, increasing attention has been paid to understanding the role of neural plasticity in functional and structural neural self-organization, as well as its influence on the learning performance of neural networks for accomplishing machine learning tasks such as classification and regression. Although many ideas and hypothesis have been suggested, the relationship between the structure, dynamics and learning performance of neural networks remains elusive. The purpose of this article is to review the most important computational models for neural plasticity and discuss various ideas about neural plasticity's role. Finally, we suggest a few promising research directions, in particular those along the line that combines findings in computational neuroscience and systems biology, and their synergetic roles in understanding learning, memory and cognition, thereby bridging the gap between computational neuroscience, systems biology and computational intelligence.",2014-11-01,3,1342,84,2380
2884,24246494,An EEG Finger-Print of fMRI deep regional activation,"This work introduces a general framework for producing an EEG Finger-Print (EFP) which can be used to predict specific brain activity as measured by fMRI at a given deep region. This new approach allows for improved EEG spatial resolution based on simultaneous fMRI activity measurements. Advanced signal processing and machine learning methods were applied on EEG data acquired simultaneously with fMRI during relaxation training guided by on-line continuous feedback on changing alpha/theta EEG measure. We focused on demonstrating improved EEG prediction of activation in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model that is based on time/frequency representation of EEG data from a single electrode, can predict the amygdala related activity significantly better than a traditional theta/alpha activity sampled from the best electrode and about 1/3 of the times, significantly better than a linear combination of frequencies with a pre-defined delay. The far-reaching goal of our approach is to be able to reduce the need for fMRI scanning for probing specific sub-cortical regions such as the amygdala as the basis for brain-training procedures. On the other hand, activity in those regions can be characterized with higher temporal resolution than is obtained by fMRI alone thus revealing additional information about their processing mode.",2014-11-01,19,1393,52,2380
2808,25405022,How machine learning is shaping cognitive neuroimaging,"Functional brain images are rich and noisy data that can capture indirect signatures of neural activity underlying cognition in a given experimental setting. Can data mining leverage them to build models of cognition? Only if it is applied to well-posed questions, crafted to reveal cognitive mechanisms. Here we review how predictive models have been used on neuroimaging data to ask new questions, i.e., to uncover new aspects of cognitive organization. We also give a statistical learning perspective on these progresses and on the remaining gaping holes.",2014-11-01,20,558,54,2380
2869,24361664,"Non-negative matrix factorization of multimodal MRI, fMRI and phenotypic data reveals differential changes in default mode subnetworks in ADHD","In the multimodal neuroimaging framework, data on a single subject are collected from inherently different sources such as functional MRI, structural MRI, behavioral and/or phenotypic information. The information each source provides is not independent; a subset of features from each modality maps to one or more common latent dimensions, which can be interpreted using generative models. These latent dimensions, or ""topics,"" provide a sparse summary of the generative process behind the features for each individual. Topic modeling, an unsupervised generative model, has been used to map seemingly disparate features to a common domain. We use Non-Negative Matrix Factorization (NMF) to infer the latent structure of multimodal ADHD data containing fMRI, MRI, phenotypic and behavioral measurements. We compare four different NMF algorithms and find that the sparsest decomposition is also the most differentiating between ADHD and healthy patients. We identify dimensions that map to interpretable, recognizable dimensions such as motion, default mode network activity, and other such features of the input data. For example, structural and functional graph theory features related to default mode subnetworks clustered with the ADHD-Inattentive diagnosis. Structural measurements of the default mode network (DMN) regions such as the posterior cingulate, precuneus, and parahippocampal regions were all related to the ADHD-Inattentive diagnosis. Ventral DMN subnetworks may have more functional connections in ADHD-I, while dorsal DMN may have less. ADHD topics are dependent upon diagnostic site, suggesting diagnostic differences across geographic locations. We assess our findings in light of the ADHD-200 classification competition, and contrast our unsupervised, nominated topics with previously published supervised learning methods. Finally, we demonstrate the validity of these latent variables as biomarkers by using them for classification of ADHD in 730 patients. Cumulatively, this manuscript addresses how multimodal data in ADHD can be interpreted by latent dimensions.",2014-11-01,36,2088,142,2380
2800,25455254,Personalized Coaching Systems to support healthy behavior in people with chronic conditions,"Chronic conditions cannot be cured but daily behavior has a major effect on the severity of secondary problems and quality of life. Changing behavior however requires intensive support in daily life, which is not feasible with a human coach. A new coaching approach - so-called Personal Coaching Systems (PCSs) - use on-body sensing, combined with smart reasoning and context-aware feedback to support users in developing and maintaining a healthier behavior. Three different PCSs will be used to illustrate the different aspects of this approach: (1) Treatment of neck/shoulder pain. EMG patterns of the Trapezius muscles are used to estimate their level of relaxation. Personal vibrotactile feedback is given, to create awareness and enable learning when muscles are insufficiently relaxed. (2) Promoting a healthy activity pattern. Using a 3D accelerometer to measure activity and a smartphone to provide feedback. Timing and content of the feedback are adapted real-time, using machine-learning techniques, to optimize adherence. (3) Management of stress during daily living. The level of stress is quantified using a personal model involving a combination of different sensor signals (EMG, ECG, skin conductance, respiration). Results show that Personal Coaching Systems are feasible and a promising and challenging way forward to coach people with chronic conditions.",2014-12-01,19,1373,91,2350
1681,25733954,Neuroimaging biomarkers to predict treatment response in schizophrenia: the end of 30 years of solitude?,"Studies that have used structural magnetic resonance imaging (MRI) suggest that individuals with psychoses have brain alterations, particularly in frontal and temporal cortices, and in the white matter tracts that connect them. Furthermore, these studies suggest that brain alterations may be particularly prominent, already at illness onset, in those individuals more likely to have poorer outcomes (eg, higher number of hospital admissions, and poorer symptom remission, level of functioning, and response to the first treatment with antipsychotic drugs). The fact that, even when present, these brain alterations are subtle and distributed in nature, has limited, until now, the utility of MRI in the clinical management of these disorders. More recently, MRI approaches, such as machine learning, have suggested that these neuroanatomical biomarkers can be used for direct clinical benefits. For example, using support vector machine, MRI data obtained at illness onset have been used to predict, with significant accuracy, whether a specific individual is likely to experience a remission of symptoms later on in the course of the illness. Taken together, this evidence suggests that validated, strong neuroanatomical markers could be used not only to inform tailored intervention strategies in a single individual, but also to allow patient stratification in clinical trials for new treatments.",2014-12-01,12,1400,104,2350
2792,25566538,Toward synthesizing executable models in biology,"Over the last decade, executable models of biological behaviors have repeatedly provided new scientific discoveries, uncovered novel insights, and directed new experimental avenues. These models are computer programs whose execution mechanistically simulates aspects of the cell's behaviors. If the observed behavior of the program agrees with the observed biological behavior, then the program explains the phenomena. This approach has proven beneficial for gaining new biological insights and directing new experimental avenues. One advantage of this approach is that techniques for analysis of computer programs can be applied to the analysis of executable models. For example, one can confirm that a model agrees with experiments for all possible executions of the model (corresponding to all environmental conditions), even if there are a huge number of executions. Various formal methods have been adapted for this context, for example, model checking or symbolic analysis of state spaces. To avoid manual construction of executable models, one can apply synthesis, a method to produce programs automatically from high-level specifications. In the context of biological modeling, synthesis would correspond to extracting executable models from experimental data. We survey recent results about the usage of the techniques underlying synthesis of computer programs for the inference of biological models from experimental data. We describe synthesis of biological models from curated mutation experiment data, inferring network connectivity models from phosphoproteomic data, and synthesis of Boolean networks from gene expression data. While much work has been done on automated analysis of similar datasets using machine learning and artificial intelligence, using synthesis techniques provides new opportunities such as efficient computation of disambiguating experiments, as well as the ability to produce different kinds of models automatically from biological data.",2014-12-01,3,1976,48,2350
2797,25462334,Three-dimensional protein structure prediction: Methods and computational strategies,"A long standing problem in structural bioinformatics is to determine the three-dimensional (3-D) structure of a protein when only a sequence of amino acid residues is given. Many computational methodologies and algorithms have been proposed as a solution to the 3-D Protein Structure Prediction (3-D-PSP) problem. These methods can be divided in four main classes: (a) first principle methods without database information; (b) first principle methods with database information; (c) fold recognition and threading methods; and (d) comparative modeling methods and sequence alignment strategies. Deterministic computational techniques, optimization techniques, data mining and machine learning approaches are typically used in the construction of computational solutions for the PSP problem. Our main goal with this work is to review the methods and computational strategies that are currently used in 3-D protein prediction.",2014-12-01,16,923,84,2350
2805,27485418,Benchmarking a Wide Range of Chemical Descriptors for Drug-Target Interaction Prediction Using a Chemogenomic Approach,"The identification of drug-target interactions, or interactions between drug candidate compounds and target candidate proteins, is a crucial process in genomic drug discovery. In silico chemogenomic methods are recently recognized as a promising approach for genome-wide scale prediction of drug-target interactions, but the prediction performance depends heavily on the descriptors and similarity measures of drugs and proteins. In this paper, we investigated the performance of various descriptors and similarity measures of drugs and proteins for the drug-target interaction prediction using a chemogenomic approach. We compared the prediction accuracy of 18 chemical descriptors of drugs (e.g., ECFP, FCFP,E-state, CDK, KlekotaRoth, MACCS, PubChem, Dragon, KCF-S, and graph kernels) and 4 descriptors of proteins (e.g., amino acid composition, domain profile, local sequence similarity, and string kernel) on about one hundred thousand drug-target interactions. We examined the combinatorial effects of drug descriptors and protein descriptors using the same benchmark data under several experimental conditions. Large-scale experiments showed that our proposed KCF-S descriptor worked the best in terms of prediction accuracy. The comparative results are expected to be useful for selecting chemical descriptors in various pharmaceutical applications.",2014-12-01,12,1357,118,2350
2822,25131220,Multi-omics analysis of inflammatory bowel disease,"Crohn's disease and ulcerative colitis, known together as inflammatory bowel disease (IBD), are severe autoimmune disorders now causing gut inflammation and ulceration, among other symptoms, in up to 1 in 250 people worldwide. Incidence and prevalence of IBD have been increasing dramatically over the past several decades, although the causes for this increase are still unknown. IBD has both a complex genotype and a complex phenotype, and although it has received substantial attention from the medical research community over recent years, much of the etiology remains unexplained. Genome-wide association studies have identified a rich genetic signature of disease risk in patients with IBD, consisting of at least 163 genetic loci. Many of these loci contain genes directly involved in microbial handling, indicating that the genetic architecture of the disease has been driven by host-microbe interactions. In addition, systematic shifts in gut microbiome structure (enterotype) and function have been observed in patients with IBD. Furthermore, both the host genotype and enterotype are associated with aspects of the disease phenotype, including location of the disease. This provides strong evidence of interactions between host genotype and enterotype; however, there is a lack of published multi-omics data from IBD patients, and a lack of bioinformatics tools for modeling such systems. In this article we discuss, from a computational biologist's point of view, the potential benefits of and the challenges involved in designing and analyzing such multi-omics studies of IBD.",2014-12-01,15,1589,50,2350
2819,25199597,Discovering new indicators of fecal pollution,"Fecal pollution indicators are essential to identify and remediate contamination sources and protect public health. Historically, easily cultured facultative anaerobes such as fecal coliforms, Escherichia coli, or enterococci have been used but these indicators generally provide no information as to their source. More recently, molecular methods have targeted fecal anaerobes, which are much more abundant in humans and other mammals, and some strains appear to be associated with particular host sources. Next-generation sequencing and microbiome studies have created an unprecedented inventory of microbial communities associated with fecal sources, allowing reexamination of which taxonomic groups are best suited as informative indicators. The use of new computational methods, such as oligotyping coupled with well-established machine learning approaches, is providing new insights into patterns of host association. In this review we examine the basis for host-specificity and the rationale for using 16S rRNA gene targets for alternative indicators and highlight two taxonomic groups, Bacteroidales and Lachnospiraceae, which are rich in host-specific bacterial organisms. Finally, we discuss considerations for using alternative indicators for water quality assessments with a particular focus on detecting human sewage sources of contamination.",2014-12-01,35,1355,45,2350
2833,25016293,Text summarization in the biomedical domain: a systematic review of recent research,"Objective:                    The amount of information for clinicians and clinical researchers is growing exponentially. Text summarization reduces information as an attempt to enable users to find and understand relevant source texts more quickly and effortlessly. In recent years, substantial research has been conducted to develop and evaluate various summarization techniques in the biomedical domain. The goal of this study was to systematically review recent published research on summarization of textual documents in the biomedical domain.              Materials and methods:                    MEDLINE (2000 to October 2013), IEEE Digital Library, and the ACM digital library were searched. Investigators independently screened and abstracted studies that examined text summarization techniques in the biomedical domain. Information is derived from selected articles on five dimensions: input, purpose, output, method and evaluation.              Results:                    Of 10,786 studies retrieved, 34 (0.3%) met the inclusion criteria. Natural language processing (17; 50%) and a hybrid technique comprising of statistical, Natural language processing and machine learning (15; 44%) were the most common summarization approaches. Most studies (28; 82%) conducted an intrinsic evaluation.              Discussion:                    This is the first systematic review of text summarization in the biomedical domain. The study identified research gaps and provides recommendations for guiding future research on biomedical text summarization.              Conclusion:                    Recent research has focused on a hybrid technique comprising statistical, language processing and machine learning techniques. Further research is needed on the application and evaluation of text summarization in real research or patient care settings.",2014-12-01,28,1854,83,2350
2816,25234433,Single nucleotide variations: biological impact and theoretical interpretation,"Genome-wide association studies (GWAS) and whole-exome sequencing (WES) generate massive amounts of genomic variant information, and a major challenge is to identify which variations drive disease or contribute to phenotypic traits. Because the majority of known disease-causing mutations are exonic non-synonymous single nucleotide variations (nsSNVs), most studies focus on whether these nsSNVs affect protein function. Computational studies show that the impact of nsSNVs on protein function reflects sequence homology and structural information and predict the impact through statistical methods, machine learning techniques, or models of protein evolution. Here, we review impact prediction methods and discuss their underlying principles, their advantages and limitations, and how they compare to and complement one another. Finally, we present current applications and future directions for these methods in biological research and medical genetics.",2014-12-01,30,956,78,2350
2817,25223304,Machine learning for Big Data analytics in plants,"Rapid advances in high-throughput genomic technology have enabled biology to enter the era of 'Big Data' (large datasets). The plant science community not only needs to build its own Big-Data-compatible parallel computing and data management infrastructures, but also to seek novel analytical paradigms to extract information from the overwhelming amounts of data. Machine learning offers promising computational and analytical solutions for the integrative analysis of large, heterogeneous and unstructured datasets on the Big-Data scale, and is gradually gaining popularity in biology. This review introduces the basic concepts and procedures of machine-learning applications and envisages how machine learning could interface with Big Data technology to facilitate basic research and biotechnology in the plant sciences.",2014-12-01,22,823,49,2350
2846,24799088,Big data bioinformatics,"Recent technological advances allow for high throughput profiling of biological systems in a cost-efficient manner. The low cost of data generation is leading us to the ""big data"" era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this review, we introduce key concepts in the analysis of big data, including both ""machine learning"" algorithms as well as ""unsupervised"" and ""supervised"" examples of each. We note packages for the R programming language that are available to perform machine learning analyses. In addition to programming based solutions, we review webservers that allow users with limited or no programming background to perform these analyses on large data compendia.",2014-12-01,29,768,23,2350
1677,25764253,Neuroimaging-based methods for autism identification: a possible translational application?,"Classification methods based on machine learning (ML) techniques are becoming widespread analysis tools in neuroimaging studies. They have the potential to enhance the diagnostic power of brain data, by assigning a predictive index, either of pathology or of treatment response, to the single subject's acquisition. ML techniques are currently finding numerous applications in psychiatric illness, in addition to the widely studied neurodegenerative diseases. In this review we give a comprehensive account of the use of classification techniques applied to structural magnetic resonance images in autism spectrum disorders (ASDs). Understanding of these highly heterogeneous neurodevelopmental diseases could greatly benefit from additional descriptors of pathology and predictive indices extracted directly from brain data. A perspective is also provided on the future developments necessary to translate ML methods from the field of ASD research into the clinic.",2014-12-01,8,965,91,2350
2799,25458128,Creating a data exchange strategy for radiotherapy research: towards federated databases and anonymised public datasets,"Disconnected cancer research data management and lack of information exchange about planned and ongoing research are complicating the utilisation of internationally collected medical information for improving cancer patient care. Rapidly collecting/pooling data can accelerate translational research in radiation therapy and oncology. The exchange of study data is one of the fundamental principles behind data aggregation and data mining. The possibilities of reproducing the original study results, performing further analyses on existing research data to generate new hypotheses or developing computational models to support medical decisions (e.g. risk/benefit analysis of treatment options) represent just a fraction of the potential benefits of medical data-pooling. Distributed machine learning and knowledge exchange from federated databases can be considered as one beyond other attractive approaches for knowledge generation within ""Big Data"". Data interoperability between research institutions should be the major concern behind a wider collaboration. Information captured in electronic patient records (EPRs) and study case report forms (eCRFs), linked together with medical imaging and treatment planning data, are deemed to be fundamental elements for large multi-centre studies in the field of radiation therapy and oncology. To fully utilise the captured medical information, the study data have to be more than just an electronic version of a traditional (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability, utilisation of standards, data quality and privacy concerns, data ownership, rights to publish, data pooling architecture and storage. This paper discusses a framework for conceptual packages of ideas focused on a strategic development for international research data exchange in the field of radiation therapy and oncology.",2014-12-01,27,1888,119,2350
2804,25435482,Using what you get: dynamic physiologic signatures of critical illness,"The development and resolution of cardiopulmonary instability take time to become clinically apparent, and the treatments provided take time to have an impact. The characterization of dynamic changes in hemodynamic and metabolic variables is implicit in physiologic signatures. When primary variables are collected with high enough frequency to derive new variables, this data hierarchy can be used to develop physiologic signatures. The creation of physiologic signatures requires no new information; additional knowledge is extracted from data that already exist. It is possible to create physiologic signatures for each stage in the process of clinical decompensation and recovery to improve outcomes.",2015-01-01,6,704,70,2319
2793,25561628,Identification and computational analysis of gene regulatory elements,"Over the last two decades, advances in experimental and computational technologies have greatly facilitated genomic research. Next-generation sequencing technologies have made de novo sequencing of large genomes affordable, and powerful computational approaches have enabled accurate annotations of genomic DNA sequences. Charting functional regions in genomes must account for not only the coding sequences, but also noncoding RNAs, repetitive elements, chromatin states, epigenetic modifications, and gene regulatory elements. A mix of comparative genomics, high-throughput biological experiments, and machine learning approaches has played a major role in this truly global effort. Here we describe some of these approaches and provide an account of our current understanding of the complex landscape of the human genome. We also present overviews of different publicly available, large-scale experimental data sets and computational tools, which we hope will prove beneficial for researchers working with large and complex genomes.",2015-01-01,3,1035,69,2319
2789,25608213,A survey of online activity recognition using mobile phones,"Physical activity recognition using embedded sensors has enabled many context-aware applications in different areas, such as healthcare. Initially, one or more dedicated wearable sensors were used for such applications. However, recently, many researchers started using mobile phones for this purpose, since these ubiquitous devices are equipped with various sensors, ranging from accelerometers to magnetic field sensors. In most of the current studies, sensor data collected for activity recognition are analyzed offline using machine learning tools. However, there is now a trend towards implementing activity recognition systems on these devices in an online manner, since modern mobile phones have become more powerful in terms of available resources, such as CPU, memory and battery. The research on offline activity recognition has been reviewed in several earlier studies in detail. However, work done on online activity recognition is still in its infancy and is yet to be reviewed. In this paper, we review the studies done so far that implement activity recognition systems on mobile phones and use only their on-board sensors. We discuss various aspects of these studies. Moreover, we discuss their limitations and present various recommendations for future research.",2015-01-01,51,1279,59,2319
2788,25620954,Movement recognition technology as a method of assessing spontaneous general movements in high risk infants,"Preterm birth is associated with increased risks of neurological and motor impairments such as cerebral palsy. The risks are highest in those born at the lowest gestations. Early identification of those most at risk is challenging meaning that a critical window of opportunity to improve outcomes through therapy-based interventions may be missed. Clinically, the assessment of spontaneous general movements is an important tool, which can be used for the prediction of movement impairments in high risk infants. Movement recognition aims to capture and analyze relevant limb movements through computerized approaches focusing on continuous, objective, and quantitative assessment. Different methods of recording and analyzing infant movements have recently been explored in high risk infants. These range from camera-based solutions to body-worn miniaturized movement sensors used to record continuous time-series data that represent the dynamics of limb movements. Various machine learning methods have been developed and applied to the analysis of the recorded movement data. This analysis has focused on the detection and classification of atypical spontaneous general movements. This article aims to identify recent translational studies using movement recognition technology as a method of assessing movement in high risk infants. The application of this technology within pediatric practice represents a growing area of inter-disciplinary collaboration, which may lead to a greater understanding of the development of the nervous system in infants at high risk of motor impairment.",2015-01-01,25,1588,107,2319
1678,25759684,Computational approaches for prediction of pathogen-host protein-protein interactions,"Infectious diseases are still among the major and prevalent health problems, mostly because of the drug resistance of novel variants of pathogens. Molecular interactions between pathogens and their hosts are the key parts of the infection mechanisms. Novel antimicrobial therapeutics to fight drug resistance is only possible in case of a thorough understanding of pathogen-host interaction (PHI) systems. Existing databases, which contain experimentally verified PHI data, suffer from scarcity of reported interactions due to the technically challenging and time consuming process of experiments. These have motivated many researchers to address the problem by proposing computational approaches for analysis and prediction of PHIs. The computational methods primarily utilize sequence information, protein structure and known interactions. Classic machine learning techniques are used when there are sufficient known interactions to be used as training data. On the opposite case, transfer and multitask learning methods are preferred. Here, we present an overview of these computational approaches for predicting PHI systems, discussing their weakness and abilities, with future directions.",2015-02-01,26,1193,85,2288
2803,25440524,Applying machine learning techniques for ADME-Tox prediction: a review,"Introduction:                    Pharmacokinetics involves the study of absorption, distribution, metabolism, excretion and toxicity of xenobiotics (ADME-Tox). In this sense, the ADME-Tox profile of a bioactive compound can impact its efficacy and safety. Moreover, efficacy and safety were considered some of the major causes of clinical failures in the development of new chemical entities. In this context, machine learning (ML) techniques have been often used in ADME-Tox studies due to the existence of compounds with known pharmacokinetic properties available for generating predictive models.              Areas covered:                    This review examines the growth in the use of some ML techniques in ADME-Tox studies, in particular supervised and unsupervised techniques. Also, some critical points (e.g., size of the data set and type of output variable) must be considered during the generation of models that relate ADME-Tox properties and biological activity.              Expert opinion:                    ML techniques have been successfully employed in pharmacokinetic studies, helping the complex process of designing new drug candidates from the use of reliable ML models. An application of this procedure would be the prediction of ADME-Tox properties from studies of quantitative structure-activity relationships or the discovery of new compounds from a virtual screening using filters based on results obtained from ML techniques.",2015-02-01,24,1458,70,2288
1668,25797506,Big data in medical science--a biostatistical view,"Background:                    Inexpensive techniques for measurement and data storage now enable medical researchers to acquire far more data than can conveniently be analyzed by traditional methods. The expression ""big data"" refers to quantities on the order of magnitude of a terabyte (1012 bytes); special techniques must be used to evaluate such huge quantities of data in a scientifically meaningful way. Whether data sets of this size are useful and important is an open question that currently confronts medical science.              Methods:                    In this article, we give illustrative examples of the use of analytical techniques for big data and discuss them in the light of a selective literature review. We point out some critical aspects that should be considered to avoid errors when large amounts of data are analyzed.              Results:                    Machine learning techniques enable the recognition of potentially relevant patterns. When such techniques are used, certain additional steps should be taken that are unnecessary in more traditional analyses; for example, patient characteristics should be differentially weighted. If this is not done as a preliminary step before similarity detection, which is a component of many data analysis operations, characteristics such as age or sex will be weighted no higher than any one out of 10 000 gene expression values. Experience from the analysis of conventional observational data sets can be called upon to draw conclusions about potential causal effects from big data sets.              Conclusion:                    Big data techniques can be used, for example, to evaluate observational data derived from the routine care of entire populations, with clustering methods used to analyze therapeutically relevant patient subgroups. Such analyses can provide complementary information to clinical trials of the classic type. As big data analyses become more popular, various statistical techniques for causality analysis in observational data are becoming more widely available. This is likely to be of benefit to medical science, but specific adaptations will have to be made according to the requirements of the applications.",2015-02-01,14,2219,50,2288
2832,25029489,Kernel association for classification and prediction: a survey,"Kernel association (KA) in statistical pattern recognition used for classification and prediction have recently emerged in a machine learning and signal processing context. This survey outlines the latest trends and innovations of a kernel framework for big data analysis. KA topics include offline learning, distributed database, online learning, and its prediction. The structural presentation and the comprehensive list of references are geared to provide a useful overview of this evolving field for both specialists and relevant scholars.",2015-02-01,0,543,62,2288
1687,28347005,DNA-Protected Silver Clusters for Nanophotonics,"DNA-protected silver clusters (AgN-DNA) possess unique fluorescence properties that depend on the specific DNA template that stabilizes the cluster. They exhibit peak emission wavelengths that range across the visible and near-IR spectrum. This wide color palette, combined with low toxicity, high fluorescence quantum yields of some clusters, low synthesis costs, small cluster sizes and compatibility with DNA are enabling many applications that employ AgN-DNA. Here we review what is known about the underlying composition and structure of AgN-DNA, and how these relate to the optical properties of these fascinating, hybrid biomolecule-metal cluster nanomaterials. We place AgN-DNA in the general context of ligand-stabilized metal clusters and compare their properties to those of other noble metal clusters stabilized by small molecule ligands. The methods used to isolate pure AgN-DNA for analysis of composition and for studies of solution and single-emitter optical properties are discussed. We give a brief overview of structurally sensitive chiroptical studies, both theoretical and experimental, and review experiments on bringing silver clusters of distinct size and color into nanoscale DNA assemblies. Progress towards using DNA scaffolds to assemble multi-cluster arrays is also reviewed.",2015-02-01,18,1304,47,2288
2790,25592319,High-throughput analysis of behavior for drug discovery,"Drug testing with traditional behavioral assays constitutes a major bottleneck in the development of novel therapies. PsychoGenics developed three comprehensive high-throughput systems, SmartCube(), NeuroCube() and PhenoCube() systems, to increase the efficiency of the drug screening and phenotyping in rodents. These three systems capture different domains of behavior, namely, cognitive, motor, circadian, social, anxiety-like, gait and others, using custom-built computer vision software and machine learning algorithms for analysis. This review exemplifies the use of the three systems and explains how they can advance drug screening with their applications to phenotyping of disease models, drug screening, selection of lead candidates, behavior-driven lead optimization, and drug repurposing.",2015-03-01,11,803,55,2260
1682,25733557,The promise of reverse vaccinology,"Reverse vaccinology (RV) is a computational approach that aims to identify putative vaccine candidates in the protein coding genome (proteome) of pathogens. RV has primarily been applied to bacterial pathogens to identify proteins that can be formulated into subunit vaccines, which consist of one or more protein antigens. An RV approach based on a filtering method has already been used to construct a subunit vaccine against Neisseria meningitidis serogroup B that is now registered in several countries (Bexsero). Recently, machine learning methods have been used to improve the ability of RV approaches to identify vaccine candidates. Further improvements related to the incorporation of epitope-binding annotation and gene expression data are discussed. In the future, it is envisaged that RV approaches will facilitate rapid vaccine design with less reliance on conventional animal testing and clinical trials in order to curb the threat of antibiotic resistance or newly emerged outbreaks of bacterial origin.",2015-03-01,13,1017,34,2260
1679,25756377,An overview of the prediction of protein DNA-binding sites,"Interactions between proteins and DNA play an important role in many essential biological processes such as DNA replication, transcription, splicing, and repair. The identification of amino acid residues involved in DNA-binding sites is critical for understanding the mechanism of these biological activities. In the last decade, numerous computational approaches have been developed to predict protein DNA-binding sites based on protein sequence and/or structural information, which play an important role in complementing experimental strategies. At this time, approaches can be divided into three categories: sequence-based DNA-binding site prediction, structure-based DNA-binding site prediction, and homology modeling and threading. In this article, we review existing research on computational methods to predict protein DNA-binding sites, which includes data sets, various residue sequence/structural features, machine learning methods for comparison and selection, evaluation methods, performance comparison of different tools, and future directions in protein DNA-binding site prediction. In particular, we detail the meta-analysis of protein DNA-binding sites. We also propose specific implications that are likely to result in novel prediction methods, increased performance, or practical applications.",2015-03-01,16,1313,58,2260
1674,25773546,Potential application of machine learning in health outcomes research and some statistical cautions,"Traditional analytic methods are often ill-suited to the evolving world of health care big data characterized by massive volume, complexity, and velocity. In particular, methods are needed that can estimate models efficiently using very large datasets containing healthcare utilization data, clinical data, data from personal devices, and many other sources. Although very large, such datasets can also be quite sparse (e.g., device data may only be available for a small subset of individuals), which creates problems for traditional regression models. Many machine learning methods address such limitations effectively but are still subject to the usual sources of bias that commonly arise in observational studies. Researchers using machine learning methods such as lasso or ridge regression should assess these models using conventional specification tests.",2015-03-01,18,861,99,2260
2802,25448299,Text as data: using text-based features for proteins representation and for computational prediction of their characteristics,"The current era of large-scale biology is characterized by a fast-paced growth in the number of sequenced genomes and, consequently, by a multitude of identified proteins whose function has yet to be determined. Simultaneously, any known or postulated information concerning genes and proteins is part of the ever-growing published scientific literature, which is expanding at a rate of over a million new publications per year. Computational tools that attempt to automatically predict and annotate protein characteristics, such as function and localization patterns, are being developed along with systems that aim to support the process via text mining. Most work on protein characterization focuses on features derived directly from protein sequence data. Protein-related work that does aim to utilize the literature typically concentrates on extracting specific facts (e.g., protein interactions) from text. In the past few years we have taken a different route, treating the literature as a source of text-based features, which can be employed just as sequence-based protein-features were used in earlier work, for predicting protein subcellular location and possibly also function. We discuss here in detail the overall approach, along with results from work we have done in this area demonstrating the value of this method and its potential use.",2015-03-01,6,1353,125,2260
2829,25053743,Kernel methods for large-scale genomic data analysis,"Machine learning, particularly kernel methods, has been demonstrated as a promising new tool to tackle the challenges imposed by today's explosive data growth in genomics. They provide a practical and principled approach to learning how a large number of genetic variants are associated with complex phenotypes, to help reveal the complexity in the relationship between the genetic markers and the outcome of interest. In this review, we highlight the potential key role it will have in modern genomic data processing, especially with regard to integration with classical methods for gene prioritizing, prediction and data fusion.",2015-03-01,8,630,52,2260
1664,25859202,Multivariate cross-classification: applying machine learning techniques to characterize abstraction in neural representations,"Here we highlight an emerging trend in the use of machine learning classifiers to test for abstraction across patterns of neural activity. When a classifier algorithm is trained on data from one cognitive context, and tested on data from another, conclusions can be drawn about the role of a given brain region in representing information that abstracts across those cognitive contexts. We call this kind of analysis Multivariate Cross-Classification (MVCC), and review several domains where it has recently made an impact. MVCC has been important in establishing correspondences among neural patterns across cognitive domains, including motor-perception matching and cross-sensory matching. It has been used to test for similarity between neural patterns evoked by perception and those generated from memory. Other work has used MVCC to investigate the similarity of representations for semantic categories across different kinds of stimulus presentation, and in the presence of different cognitive demands. We use these examples to demonstrate the power of MVCC as a tool for investigating neural abstraction and discuss some important methodological issues related to its application.",2015-03-01,25,1187,125,2260
2854,24621527,Towards more accurate prediction of protein folding rates: a review of the existing Web-based bioinformatics approaches,"The understanding of protein-folding mechanisms is often considered to be an important goal that will enable structural biologists to discover the mysterious relationship between the sequence, structure and function of proteins. The ability to predict protein-folding rates without the need for actual experimental work will assist the research work of structural biologists in many ways. Many bioinformatics tools have emerged in the past decade, and each has showcased different features. In this article, we review and compare eight web-based prediction tools that are currently available and that predominantly predict the protein-folding rate. The prediction performance, usability and utility, together with the prediction tool development and validation methodologies for these tools, are critically reviewed. This article is presented in a comprehensible manner to assist readers in the process of selecting the most appropriate bioinformatics tools to meet their needs.",2015-03-01,5,978,119,2260
2801,25448759,Machine-learning approaches in drug discovery: methods and applications,"During the past decade, virtual screening (VS) has evolved from traditional similarity searching, which utilizes single reference compounds, into an advanced application domain for data mining and machine-learning approaches, which require large and representative training-set compounds to learn robust decision rules. The explosive growth in the amount of public domain-available chemical and biological data has generated huge effort to design, analyze, and apply novel learning methodologies. Here, I focus on machine-learning techniques within the context of ligand-based VS (LBVS). In addition, I analyze several relevant VS studies from recent publications, providing a detailed view of the current state-of-the-art in this field and highlighting not only the problematic issues, but also the successes and opportunities for further advances.",2015-03-01,83,849,71,2260
2834,25008538,A framework for optimal kernel-based manifold embedding of medical image data,"Kernel-based dimensionality reduction is a widely used technique in medical image analysis. To fully unravel the underlying nonlinear manifold the selection of an adequate kernel function and of its free parameters is critical. In practice, however, the kernel function is generally chosen as Gaussian or polynomial and such standard kernels might not always be optimal for a given image dataset or application. In this paper, we present a study on the effect of the kernel functions in nonlinear manifold embedding of medical image data. To this end, we first carry out a literature review on existing advanced kernels developed in the statistics, machine learning, and signal processing communities. In addition, we implement kernel-based formulations of well-known nonlinear dimensional reduction techniques such as Isomap and Locally Linear Embedding, thus obtaining a unified framework for manifold embedding using kernels. Subsequently, we present a method to automatically choose a kernel function and its associated parameters from a pool of kernel candidates, with the aim to generate the most optimal manifold embeddings. Furthermore, we show how the calculated selection measures can be extended to take into account the spatial relationships in images, or used to combine several kernels to further improve the embedding results. Experiments are then carried out on various synthetic and phantom datasets for numerical assessment of the methods. Furthermore, the workflow is applied to real data that include brain manifolds and multispectral images to demonstrate the importance of the kernel selection in the analysis of high-dimensional medical images.",2015-04-01,1,1667,77,2229
2814,25307115,Research review: Functional brain connectivity and child psychopathology--overview and methodological considerations for investigators new to the field,"Background:                    Functional connectivity MRI is an emerging technique that can be used to investigate typical and atypical brain function in developing and aging populations. Despite some of the current confounds in the field of functional connectivity MRI, the translational potential of the technique available to investigators may eventually be used to improve diagnosis, early disease detection, and therapy monitoring.              Method and scope:                    Based on a comprehensive survey of the literature, this review offers an introduction of resting-state functional connectivity for new investigators to the field of resting-state functional connectivity. We discuss a brief history of the technique, various methods of analysis, the relationship of functional networks to behavior, as well as the translational potential of functional connectivity MRI to investigate neuropsychiatric disorders. We also address some considerations and limitations with data analysis and interpretation.              Conclusions:                    The information provided in this review should serve as a foundation for investigators new to the field of resting-state functional connectivity. The discussion provides a means to better understand functional connectivity and its application to typical and atypical brain function.",2015-04-01,8,1350,151,2229
1665,25856482,The mutual inspirations of machine learning and neuroscience,"Neuroscientists are generating data sets of enormous size, which are matching the complexity of real-world classification tasks. Machine learning has helped data analysis enormously but is often not as accurate as human data analysis. Here, Helmstaedter discusses the challenges and promises of neuroscience-inspired machine learning that lie ahead.",2015-04-01,5,349,60,2229
1671,25789437,Imaging brain mechanisms in chronic visceral pain,"Chronic visceral pain syndromes are important clinical problems with largely unmet medical needs. Based on the common overlap with other chronic disorders of visceral or somatic pain, mood and affect, and their responsiveness to centrally targeted treatments, an important role of central nervous system in their pathophysiology is likely. A growing number of brain imaging studies in irritable bowel syndrome, functional dyspepsia, and bladder pain syndrome/interstitial cystitis has identified abnormalities in evoked brain responses, resting state activity, and connectivity, as well as in gray and white matter properties. Structural and functional alterations in brain regions of the salience, emotional arousal, and sensorimotor networks, as well as in prefrontal regions, are the most consistently reported findings. Some of these changes show moderate correlations with behavioral and clinical measures. Most recently, data-driven machine-learning approaches to larger data sets have been able to classify visceral pain syndromes from healthy control subjects. Future studies need to identify the mechanisms underlying the altered brain signatures of chronic visceral pain and identify targets for therapeutic interventions.",2015-04-01,36,1232,49,2229
1656,25983676,Using neurophysiological signals that reflect cognitive or affective state: six recommendations to avoid common pitfalls,"Estimating cognitive or affective state from neurophysiological signals and designing applications that make use of this information requires expertise in many disciplines such as neurophysiology, machine learning, experimental psychology, and human factors. This makes it difficult to perform research that is strong in all its aspects as well as to judge a study or application on its merits. On the occasion of the special topic ""Using neurophysiological signals that reflect cognitive or affective state"" we here summarize often occurring pitfalls and recommendations on how to avoid them, both for authors (researchers) and readers. They relate to defining the state of interest, the neurophysiological processes that are expected to be involved in the state of interest, confounding factors, inadvertently ""cheating"" with classification analyses, insight on what underlies successful state estimation, and finally, the added value of neurophysiological measures in the context of an application. We hope that this paper will support the community in producing high quality studies and well-validated, useful applications.",2015-04-01,23,1127,120,2229
1686,25704908,Identifying transcriptional cis-regulatory modules in animal genomes,"Gene expression is regulated through the activity of transcription factors (TFs) and chromatin-modifying proteins acting on specific DNA sequences, referred to as cis-regulatory elements. These include promoters, located at the transcription initiation sites of genes, and a variety of distal cis-regulatory modules (CRMs), the most common of which are transcriptional enhancers. Because regulated gene expression is fundamental to cell differentiation and acquisition of new cell fates, identifying, characterizing, and understanding the mechanisms of action of CRMs is critical for understanding development. CRM discovery has historically been challenging, as CRMs can be located far from the genes they regulate, have few readily identifiable sequence characteristics, and for many years were not amenable to high-throughput discovery methods. However, the recent availability of complete genome sequences and the development of next-generation sequencing methods have led to an explosion of both computational and empirical methods for CRM discovery in model and nonmodel organisms alike. Experimentally, CRMs can be identified through chromatin immunoprecipitation directed against TFs or histone post-translational modifications, identification of nucleosome-depleted 'open' chromatin regions, or sequencing-based high-throughput functional screening. Computational methods include comparative genomics, clustering of known or predicted TF-binding sites, and supervised machine-learning approaches trained on known CRMs. All of these methods have proven effective for CRM discovery, but each has its own considerations and limitations, and each is subject to a greater or lesser number of false-positive identifications. Experimental confirmation of predictions is essential, although shortcomings in current methods suggest that additional means of validation need to be developed. For further resources related to this article, please visit the WIREs website.              Conflict of interest:                    The authors have declared no conflicts of interest for this article.",2015-04-01,24,2091,68,2229
1648,26068757,Practical Applications of Digital Pathology,"Background:                    Virtual microscopy and advances in machine learning have paved the way for the ever-expanding field of digital pathology. Multiple image-based computing environments capable of performing automated quantitative and morphological analyses are the foundation on which digital pathology is built.              Methods:                    The applications for digital pathology in the clinical setting are numerous and are explored along with the digital software environments themselves, as well as the different analytical modalities specific to digital pathology. Prospective studies, case-control analyses, meta-analyses, and detailed descriptions of software environments were explored that pertained to digital pathology and its use in the clinical setting.              Results:                    Many different software environments have advanced platforms capable of improving digital pathology and potentially influencing clinical decisions.              Conclusions:                    The potential of digital pathology is vast, particularly with the introduction of numerous software environments available for use. With all the digital pathology tools available as well as those in development, the field will continue to advance, particularly in the era of personalized medicine, providing health care professionals with more precise prognostic information as well as helping them guide treatment decisions.",2015-04-01,1,1450,43,2229
2827,25092428,How can neuroimaging facilitate the diagnosis and stratification of patients with psychosis?,"Early diagnosis and treatment of patients with psychosis are associated with improved outcome in terms of future functioning, symptoms and treatment response. Identifying neuroimaging biomarkers for illness onset and treatment response would lead to immediate clinical benefits. In this review we discuss if neuroimaging may be utilised to diagnose patients with psychosis, predict those who will develop the illness in those at high risk, and stratify patients. State-of-the-art developments in the field are critically examined including multicentre studies, longitudinal designs, multimodal imaging and machine learning as well as some of the challenges in utilising future neuroimaging biomarkers in clinical trials. As many of these developments are already being applied in neuroimaging studies of Alzheimer's disease, we discuss what lessons have been learned from this field and how they may be applied to research in psychosis.",2015-05-01,5,936,92,2199
1634,26215861,Genes Caught In Flagranti: Integrating Renal Transcriptional Profiles With Genotypes and Phenotypes,"In the past decade, population genetics has gained tremendous success in identifying genetic variations that are statistically relevant to renal diseases and kidney function. However, it is challenging to interpret the functional relevance of the genetic variations found by population genetics studies. In this review, we discuss studies that integrate multiple levels of data, especially transcriptome profiles and phenotype data, to assign functional roles of genetic variations involved in kidney function. Furthermore, we introduce state-of-the-art machine learning algorithms, Bayesian networks, support vector machines, and Gaussian process regression, which have been applied successfully to integrating genetic, regulatory, and clinical information to predict clinical outcomes. These methods are likely to be deployed successfully in the nephrology field in the near future.",2015-05-01,1,884,99,2199
1676,25765323,Towards the automatic classification of neurons,"The classification of neurons into types has been much debated since the inception of modern neuroscience. Recent experimental advances are accelerating the pace of data collection. The resulting growth of information about morphological, physiological, and molecular properties encourages efforts to automate neuronal classification by powerful machine learning techniques. We review state-of-the-art analysis approaches and the availability of suitable data and resources, highlighting prominent challenges and opportunities. The effective solution of the neuronal classification problem will require continuous development of computational methods, high-throughput data production, and systematic metadata organization to enable cross-laboratory integration.",2015-05-01,31,761,47,2199
1652,26017444,Probabilistic machine learning and artificial intelligence,"How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.",2015-05-01,99,751,58,2199
1653,26017443,Reinforcement learning improves behaviour from evaluative feedback,"Reinforcement learning is a branch of machine learning concerned with using experience gained through interacting with the world and evaluative feedback to improve a system's ability to make behavioural decisions. It has been called the artificial intelligence problem in a microcosm because learning algorithms must act autonomously to perform well and achieve their goals. Partly driven by the increasing availability of rich data, recent years have seen exciting advances in the theory and practice of reinforcement learning, including developments in fundamental technical areas such as generalization, planning, exploration and empirical methodology, leading to increasing applicability to real-life problems.",2015-05-01,13,714,66,2199
1599,26566461,Atopic Dermatitis and Respiratory Allergy: What is the Link,"Understanding the aetiology and progression of atopic dermatitis and respiratory allergy may elucidate early preventative and management strategies aimed towards reducing the global burden of asthma and allergic disease. In this article, we review the current opinion concerning the link between atopic dermatitis and the subsequent progression of respiratory allergies during childhood and into early adolescence. Advances in machine learning and statistical methodology have facilitated the discovery of more refined definitions of phenotypes for identifying biomarkers. Understanding the role of atopic dermatitis in the development of respiratory allergy may ultimately allow us to determine more effective treatment strategies, thus reducing the patient and economic burden associated with these conditions.",2015-06-01,8,812,59,2168
1594,26587051,An Overview of Biomolecular Event Extraction from Scientific Documents,"This paper presents a review of state-of-the-art approaches to automatic extraction of biomolecular events from scientific texts. Events involving biomolecules such as genes, transcription factors, or enzymes, for example, have a central role in biological processes and functions and provide valuable information for describing physiological and pathogenesis mechanisms. Event extraction from biomedical literature has a broad range of applications, including support for information retrieval, knowledge summarization, and information extraction and discovery. However, automatic event extraction is a challenging task due to the ambiguity and diversity of natural language and higher-level linguistic phenomena, such as speculations and negations, which occur in biological texts and can lead to misunderstanding or incorrect interpretation. Many strategies have been proposed in the last decade, originating from different research areas such as natural language processing, machine learning, and statistics. This review summarizes the most representative approaches in biomolecular event extraction and presents an analysis of the current state of the art and of commonly used methods, features, and tools. Finally, current research trends and future perspectives are also discussed.",2015-06-01,0,1288,70,2168
1604,26509168,A Review of Computational Methods to Predict the Risk of Rupture of Abdominal Aortic Aneurysms,"Computational methods have played an important role in health care in recent years, as determining parameters that affect a certain medical condition is not possible in experimental conditions in many cases. Computational fluid dynamics (CFD) methods have been used to accurately determine the nature of blood flow in the cardiovascular and nervous systems and air flow in the respiratory system, thereby giving the surgeon a diagnostic tool to plan treatment accordingly. Machine learning or data mining (MLD) methods are currently used to develop models that learn from retrospective data to make a prediction regarding factors affecting the progression of a disease. These models have also been successful in incorporating factors such as patient history and occupation. MLD models can be used as a predictive tool to determine rupture potential in patients with abdominal aortic aneurysms (AAA) along with CFD-based prediction of parameters like wall shear stress and pressure distributions. A combination of these computer methods can be pivotal in bridging the gap between translational and outcomes research in medicine. This paper reviews the use of computational methods in the diagnosis and treatment of AAA.",2015-06-01,5,1218,94,2168
2807,25423479,Identifying predictive features in drug response using machine learning: opportunities and challenges,"This article reviews several techniques from machine learning that can be used to study the problem of identifying a small number of features, from among tens of thousands of measured features, that can accurately predict a drug response. Prediction problems are divided into two categories: sparse classification and sparse regression. In classification, the clinical parameter to be predicted is binary, whereas in regression, the parameter is a real number. Well-known methods for both classes of problems are briefly discussed. These include the SVM (support vector machine) for classification and various algorithms such as ridge regression, LASSO (least absolute shrinkage and selection operator), and EN (elastic net) for regression. In addition, several well-established methods that do not directly fall into machine learning theory are also reviewed, including neural networks, PAM (pattern analysis for microarrays), SAM (significance analysis for microarrays), GSEA (gene set enrichment analysis), and k-means clustering. Several references indicative of the application of these methods to cancer biology are discussed.",2015-06-01,22,1132,101,2168
1657,25982977,Open source tools for large-scale neuroscience,"New technologies for monitoring and manipulating the nervous system promise exciting biology but pose challenges for analysis and computation. Solutions can be found in the form of modern approaches to distributed computing, machine learning, and interactive visualization. But embracing these new technologies will require a cultural shift: away from independent efforts and proprietary methods and toward an open source and collaborative neuroscience.",2015-06-01,11,453,46,2168
1658,25961077,Briefing in application of machine learning methods in ion channel prediction,"In cells, ion channels are one of the most important classes of membrane proteins which allow inorganic ions to move across the membrane. A wide range of biological processes are involved and regulated by the opening and closing of ion channels. Ion channels can be classified into numerous classes and different types of ion channels exhibit different functions. Thus, the correct identification of ion channels and their types using computational methods will provide in-depth insights into their function in various biological processes. In this review, we will briefly introduce and discuss the recent progress in ion channel prediction using machine learning methods.",2015-06-01,1,672,77,2168
1649,26052377,Targeted proteomics for biomarker discovery and validation of hepatocellular carcinoma in hepatitis C infected patients,"Hepatocellular carcinoma (HCC)-related mortality is high because early detection modalities are hampered by inaccuracy, expense and inherent procedural risks. Thus there is an urgent need for minimally invasive, highly specific and sensitive biomarkers that enable early disease detection when therapeutic intervention remains practical. Successful therapeutic intervention is predicated on the ability to detect the cancer early. Similar unmet medical needs abound in most fields of medicine and require novel methodological approaches. Proteomic profiling of body fluids presents a sensitive diagnostic tool for early cancer detection. Here we describe such a strategy of comparative proteomics to identify potential serum-based biomarkers to distinguish high-risk chronic hepatitis C virus infected patients from HCC patients. In order to compensate for the extraordinary dynamic range in serum proteins, enrichment methods that compress the dynamic range without surrendering proteome complexity can help minimize the problems associated with many depletion methods. The enriched serum can be resolved using 2D-difference in-gel electrophoresis and the spots showing statistically significant changes selected for identification by liquid chromatography-tandem mass spectrometry. Subsequent quantitative verification and validation of these candidate biomarkers represent an obligatory and rate-limiting process that is greatly enabled by selected reaction monitoring (SRM). SRM is a tandem mass spectrometry method suitable for identification and quantitation of target peptides within complex mixtures independent on peptide-specific antibodies. Ultimately, multiplexed SRM and dynamic multiple reaction monitoring can be utilized for the simultaneous analysis of a biomarker panel derived from support vector machine learning approaches, which allows monitoring a specific disease state such as early HCC. Overall, this approach yields high probability biomarkers for clinical validation in large patient cohorts and represents a strategy extensible to many diseases.",2015-06-01,8,2074,119,2168
1659,25960736,On the Relationship between Variational Level Set-Based and SOM-Based Active Contours,"Most Active Contour Models (ACMs) deal with the image segmentation problem as a functional optimization problem, as they work on dividing an image into several regions by optimizing a suitable functional. Among ACMs, variational level set methods have been used to build an active contour with the aim of modeling arbitrarily complex shapes. Moreover, they can handle also topological changes of the contours. Self-Organizing Maps (SOMs) have attracted the attention of many computer vision scientists, particularly in modeling an active contour based on the idea of utilizing the prototypes (weights) of a SOM to control the evolution of the contour. SOM-based models have been proposed in general with the aim of exploiting the specific ability of SOMs to learn the edge-map information via their topology preservation property and overcoming some drawbacks of other ACMs, such as trapping into local minima of the image energy functional to be minimized in such models. In this survey, we illustrate the main concepts of variational level set-based ACMs, SOM-based ACMs, and their relationship and review in a comprehensive fashion the development of their state-of-the-art models from a machine learning perspective, with a focus on their strengths and weaknesses.",2015-06-01,0,1268,85,2168
1655,25986686,The Role of Water Occlusion for the Definition of a Protein Binding Hot-Spot,"Biological systems rely on the establishment of interactions between biomolecules, which take place in the aqueous environment of the cell. It was already demonstrated that a small set of residues at the interface, Hot-Spots(HS), contributes significantly to the binding free energy. However, these energetic determinants of affinity and specificity are still not fully understood. Moreover, the contribution of water to their HS character is also poorly characterized. In this review, we have focused on the structural data available that support the occlusion of HS from solvent, and therefore the ""O-ring theory""not only on protein-protein but also on protein-DNA complexes. We also emphasized the use of Solvent Accessible Surface Area (SASA) features in a variety of machine-learning approaches that aim to detect binding HS.",2015-06-01,4,830,76,2168
1650,26037068,Recent progresses in the exploration of machine learning methods as in-silico ADME prediction tools,"In-silico methods have been explored as potential tools for assessing ADME and ADME regulatory properties particularly in early drug discovery stages. Machine learning methods, with their ability in classifying diverse structures and complex mechanisms, are well suited for predicting ADME and ADME regulatory properties. Recent efforts have been directed at the broadening of application scopes and the improvement of predictive performance with particular focuses on the coverage of ADME properties, and exploration of more diversified training data, appropriate molecular features, and consensus modeling. Moreover, several online machine learning ADME prediction servers have emerged. Here we review these progresses and discuss the performances, application prospects and challenges of exploring machine learning methods as useful tools in predicting ADME and ADME regulatory properties.",2015-06-01,9,892,99,2168
1630,26234510,Artificial Neural Network Methods Applied to Drug Discovery for Neglected Diseases,"Among the chemometric tools used in rational drug design, we find artificial neural network methods (ANNs), a statistical learning algorithm similar to the human brain, to be quite powerful. Some ANN applications use biological and molecular data of the training series that are inserted to ensure the machine learning, and to generate robust and predictive models. In drug discovery, researchers use this methodology, looking to find new chemotherapeutic agents for various diseases. The neglected diseases are a group of tropical parasitic diseases that primarily affect poor countries in Africa, Asia, and South America. Current drugs against these diseases cause side effects, are ineffective during the chronic stages of the disease, and are often not available to the needy population, have relative high toxicity, and face developing resistance. Faced with so many problems, new chemotherapeutic agents to treat these infections are much needed. The present review reports on neural network research, which studies new ligands against Chagas' disease, sleeping sickness, malaria, tuberculosis, and leishmaniasis; a few of the neglected diseases.",2015-06-01,2,1152,82,2168
2791,25577381,Prediction of miRNA targets,"Computational methods for miRNA target prediction are currently undergoing extensive review and evaluation. There is still a great need for improvement of these tools and bioinformatics approaches are looking towards high-throughput experiments in order to validate predictions. The combination of large-scale techniques with computational tools will not only provide greater credence to computational predictions but also lead to the better understanding of specific biological questions. Current miRNA target prediction tools utilize probabilistic learning algorithms, machine learning methods and even empirical biologically defined rules in order to build models based on experimentally verified miRNA targets. Large-scale protein downregulation assays and next-generation sequencing (NGS) are now being used to validate methodologies and compare the performance of existing tools. Tools that exhibit greater correlation between computational predictions and protein downregulation or RNA downregulation are considered the state of the art. Moreover, efficiency in prediction of miRNA targets that are concurrently verified experimentally provides additional validity to computational predictions and further highlights the competitive advantage of specific tools and their efficacy in extracting biologically significant results. In this review paper, we discuss the computational methods for miRNA target prediction and provide a detailed comparison of methodologies and features utilized by each specific tool. Moreover, we provide an overview of current state-of-the-art high-throughput methods used in miRNA target prediction.",2015-06-01,13,1635,27,2168
1635,26202261,Prediction of Cell-Penetrating Peptides,"The in silico methods for the prediction of the cell-penetrating peptides are reviewed. Those include the multivariate statistical methods, machine-learning methods such as the artificial neural networks and support vector machines, and molecular modeling techniques including molecular docking and molecular dynamics.The applicability of the methods is demonstrated on the basis of the exemplary cases from the literature.",2015-06-01,0,423,39,2168
2794,25548928,Advances in protein contact map prediction based on machine learning,"A protein contact map is a simplified, two-dimensional version of the three-dimensional protein structure. Protein contact map is proved to be crucial in forming the three-dimensional structure. Contact map prediction has now become an indispensable and promising intermediate step towards final three-dimensional structure prediction, while directed sequence-structure prediction hits its bottlenecks. In this article, different evaluation scores of prediction efficiency are compared. Next, the state of the art and future perspectives of contact map methods are reviewed and special attention is paid to those relying on machine learning algorithms. Details of neural network based methods as well as a list of machine learning based methods are given. Finally, bottlenecks and potential improvements of contact map predictions are discussed.",2015-06-01,3,845,68,2168
2787,25666163,The application of in silico drug-likeness predictions in pharmaceutical research,"The concept of drug-likeness, established from the analyses of the physiochemical properties or/and structural features of existing small organic drugs or/and drug candidates, has been widely used to filter out compounds with undesirable properties, especially poor ADMET (absorption, distribution, metabolism, excretion, and toxicity) profiles. Here, we summarize various approaches for drug-likeness evaluations, including simple rules/filters based on molecular properties/structures and quantitative prediction models based on sophisticated machine learning methods, and provide a comprehensive review of recent advances in this field. Moreover, the strengths and weaknesses of these approaches are briefly outlined. Finally, the drug-likeness analyses of natural products and traditional Chinese medicines (TCM) are discussed.",2015-06-01,52,831,81,2168
2795,25548927,Some remarks on prediction of protein-protein interaction with machine learning,"Protein-protein interactions (PPIs) play a key role in many cellular processes. Uncovering the PPIs and their function within the cell is a challenge of post-genomic biology and will improve our understanding of disease and help in the development of novel methods for disease diagnosis and forensics. The experimental methods currently used to identify PPIs are both time-consuming and expensive, and high throughput experimental results have shown both high false positive beside false negative information for protein interaction. These obstacles could be overcome by developing computational approaches to predict PPIs and validate the obtained experimental results. In this work, we will describe the recent advances in predicting protein-protein interaction from the following aspects: i) the benchmark dataset construction, ii) the sequence representation approaches, iii) the common machine learning algorithms, and iv) the cross-validation test methods and assessment metrics.",2015-06-01,3,985,79,2168
1629,26234511,Bio-AIMS Collection of Chemoinformatics Web Tools based on Molecular Graph Information and Artificial Intelligence Models,"The molecular information encoding into molecular descriptors is the first step into in silico Chemoinformatics methods in Drug Design. The Machine Learning methods are a complex solution to find prediction models for specific biological properties of molecules. These models connect the molecular structure information such as atom connectivity (molecular graphs) or physical-chemical properties of an atom/group of atoms to the molecular activity (Quantitative Structure - Activity Relationship, QSAR). Due to the complexity of the proteins, the prediction of their activity is a complicated task and the interpretation of the models is more difficult. The current review presents a series of 11 prediction models for proteins, implemented as free Web tools on an Artificial Intelligence Model Server in Biosciences, Bio-AIMS (http://bio-aims.udc.es/TargetPred.php). Six tools predict protein activity, two models evaluate drug - protein target interactions and the other three calculate protein - protein interactions. The input information is based on the protein 3D structure for nine models, 1D peptide amino acid sequence for three tools and drug SMILES formulas for two servers. The molecular graph descriptor-based Machine Learning models could be useful tools for in silico screening of new peptides/proteins as future drug targets for specific treatments.",2015-06-01,0,1366,121,2168
1688,25668473,Toward high-content screening of mitochondrial morphology and membrane potential in living cells,"Mitochondria are double membrane organelles involved in various key cellular processes. Governed by dedicated protein machinery, mitochondria move and continuously fuse and divide. These ""mitochondrial dynamics"" are bi-directionally linked to mitochondrial and cell functional state in space and time. Due to the action of the electron transport chain (ETC), the mitochondrial inner membrane displays a inside-negative membrane potential (). The latter is considered a functional readout of mitochondrial ""health"" and required to sustain normal mitochondrial ATP production and mitochondrial fusion. During the last decade, live-cell microscopy strategies were developed for simultaneous quantification of  and mitochondrial morphology. This revealed that ETC dysfunction, changes in  and aberrations in mitochondrial structure often occur in parallel, suggesting they are linked potential targets for therapeutic intervention. Here we discuss how combining high-content and high-throughput strategies can be used for analysis of genetic and/or drug-induced effects at the level of individual organelles, cells and cell populations. This article is part of a Directed Issue entitled: Energy Metabolism Disorders and Therapies.",2015-06-01,9,1232,96,2168
1626,26246834,Advances in Patient Classification for Traditional Chinese Medicine: A Machine Learning Perspective,"As a complementary and alternative medicine in medical field, traditional Chinese medicine (TCM) has drawn great attention in the domestic field and overseas. In practice, TCM provides a quite distinct methodology to patient diagnosis and treatment compared to western medicine (WM). Syndrome (ZHENG or pattern) is differentiated by a set of symptoms and signs examined from an individual by four main diagnostic methods: inspection, auscultation and olfaction, interrogation, and palpation which reflects the pathological and physiological changes of disease occurrence and development. Patient classification is to divide patients into several classes based on different criteria. In this paper, from the machine learning perspective, a survey on patient classification issue will be summarized on three major aspects of TCM: sign classification, syndrome differentiation, and disease classification. With the consideration of different diagnostic data analyzed by different computational methods, we present the overview for four subfields of TCM diagnosis, respectively. For each subfield, we design a rectangular reference list with applications in the horizontal direction and machine learning algorithms in the longitudinal direction. According to the current development of objective TCM diagnosis for patient classification, a discussion of the research issues around machine learning techniques with applications to TCM diagnosis is given to facilitate the further research for TCM patient classification.",2015-06-01,8,1515,99,2168
1684,25726470,Computational prediction of riboswitches,"Riboswitches present a ubiquitous genetic regulatory mechanism for prokaryotes and have been found in HIV1, fungi, plants, and even H. sapiens. We present an overview of approaches to predict riboswitch aptamers and, more generally, RNA conformational switches.",2015-06-01,3,261,40,2168
1672,25783869,The application of machine learning to the modelling of percutaneous absorption: an overview and guide,"Machine learning (ML) methods have been applied to the analysis of a range of biological systems. This paper reviews the application of these methods to the problem domain of skin permeability and addresses critically some of the key issues. Specifically, ML methods offer great potential in both predictive ability and their ability to provide mechanistic insight to, in this case, the phenomena of skin permeation. However, they are beset by perceptions of a lack of transparency and, often, once a ML or related method has been published there is little impetus from other researchers to adopt such methods. This is usually due to the lack of transparency in some methods and the lack of availability of specific coding for running advanced ML methods. This paper reviews critically the application of ML methods to percutaneous absorption and addresses the key issue of transparency by describing in detail - and providing the detailed coding for - the process of running a ML method (in this case, a Gaussian process regression method). Although this method is applied here to the field of percutaneous absorption, it may be applied more broadly to any biological system.",2015-06-01,1,1176,102,2168
1670,25795924,Machine learning approaches to analysing textual injury surveillance data: a systematic review,"Objective:                    To synthesise recent research on the use of machine learning approaches to mining textual injury surveillance data.              Design:                    Systematic review.              Data sources:                    The electronic databases which were searched included PubMed, Cinahl, Medline, Google Scholar, and Proquest. The bibliography of all relevant articles was examined and associated articles were identified using a snowballing technique.              Selection criteria:                    For inclusion, articles were required to meet the following criteria: (a) used a health-related database, (b) focused on injury-related cases, AND used machine learning approaches to analyse textual data.              Methods:                    The papers identified through the search were screened resulting in 16 papers selected for review. Articles were reviewed to describe the databases and methodology used, the strength and limitations of different techniques, and quality assurance approaches used. Due to heterogeneity between studies meta-analysis was not performed.              Results:                    Occupational injuries were the focus of half of the machine learning studies and the most common methods described were Bayesian probability or Bayesian network based methods to either predict injury categories or extract common injury scenarios. Models were evaluated through either comparison with gold standard data or content expert evaluation or statistical measures of quality. Machine learning was found to provide high precision and accuracy when predicting a small number of categories, was valuable for visualisation of injury patterns and prediction of future outcomes. However, difficulties related to generalizability, source data quality, complexity of models and integration of content and technical knowledge were discussed.              Conclusions:                    The use of narrative text for injury surveillance has grown in popularity, complexity and quality over recent years. With advances in data mining techniques, increased capacity for analysis of large databases, and involvement of computer scientists in the injury prevention field, along with more comprehensive use and description of quality assurance methods in text mining approaches, it is likely that we will see a continued growth and advancement in knowledge of text mining in the injury field.",2015-06-01,5,2444,94,2168
1669,25796619,"In silico methods for predicting drug-drug interactions with cytochrome P-450s, transporters and beyond","Drug-drug interactions (DDIs) are associated with severe adverse effects that may lead to the patient requiring alternative therapeutics and could ultimately lead to drug withdrawal from the market if they are severe. To prevent the occurrence of DDI in the clinic, experimental systems to evaluate drug interaction have been integrated into the various stages of the drug discovery and development process. A large body of knowledge about DDI has also accumulated through these studies and pharmacovigillence systems. Much of this work to date has focused on the drug metabolizing enzymes such as cytochrome P-450s as well as drug transporters, ion channels and occasionally other proteins. This combined knowledge provides a foundation for a hypothesis-driven in silico approach, using either cheminformatics or physiologically based pharmacokinetics (PK) modeling methods to assess DDI potential. Here we review recent advances in these approaches with emphasis on hypothesis-driven mechanistic models for important protein targets involved in PK-based DDI. Recent efforts with other informatics approaches to detect DDI are highlighted. Besides DDI, we also briefly introduce drug interactions with other substances, such as Traditional Chinese Medicines to illustrate how in silico modeling can be useful in this domain. We also summarize valuable data sources and web-based tools that are available for DDI prediction. We finally explore the challenges we see faced by in silico approaches for predicting DDI and propose future directions to make these computational models more reliable, accurate, and publically accessible.",2015-06-01,4,1631,103,2168
1647,26082714,Minimal approach to neuro-inspired information processing,"To learn and mimic how the brain processes information has been a major research challenge for decades. Despite the efforts, little is known on how we encode, maintain and retrieve information. One of the hypothesis assumes that transient states are generated in our intricate network of neurons when the brain is stimulated by a sensory input. Based on this idea, powerful computational schemes have been developed. These schemes, known as machine-learning techniques, include artificial neural networks, support vector machine and reservoir computing, among others. In this paper, we concentrate on the reservoir computing (RC) technique using delay-coupled systems. Unlike traditional RC, where the information is processed in large recurrent networks of interconnected artificial neurons, we choose a minimal design, implemented via a simple nonlinear dynamical system subject to a self-feedback loop with delay. This design is not intended to represent an actual brain circuit, but aims at finding the minimum ingredients that allow developing an efficient information processor. This simple scheme not only allows us to address fundamental questions but also permits simple hardware implementations. By reducing the neuro-inspired reservoir computing approach to its bare essentials, we find that nonlinear transient responses of the simple dynamical system enable the processing of information with excellent performance and at unprecedented speed. We specifically explore different hardware implementations and, by that, we learn about the role of nonlinearity, noise, system responses, connectivity structure, and the quality of projection onto the required high-dimensional state space. Besides the relevance for the understanding of basic mechanisms, this scheme opens direct technological opportunities that could not be addressed with previous approaches.",2015-06-01,8,1868,57,2168
1642,26138575,Role of Open Source Tools and Resources in Virtual Screening for Drug Discovery,"Advancement in chemoinformatics research in parallel with availability of high performance computing platform has made handling of large scale multi-dimensional scientific data for high throughput drug discovery easier. In this study we have explored publicly available molecular databases with the help of open-source based integrated in-house molecular informatics tools for virtual screening. The virtual screening literature for past decade has been extensively investigated and thoroughly analyzed to reveal interesting patterns with respect to the drug, target, scaffold and disease space. The review also focuses on the integrated chemoinformatics tools that are capable of harvesting chemical data from textual literature information and transform them into truly computable chemical structures, identification of unique fragments and scaffolds from a class of compounds, automatic generation of focused virtual libraries, computation of molecular descriptors for structure-activity relationship studies, application of conventional filters used in lead discovery along with in-house developed exhaustive PTC (Pharmacophore, Toxicophores and Chemophores) filters and machine learning tools for the design of potential disease specific inhibitors. A case study on kinase inhibitors is provided as an example.",2015-06-01,3,1315,79,2168
1640,26145249,What was old is new again: using the host response to diagnose infectious disease,"A century of advances in infectious disease diagnosis and treatment changed the face of medicine. However, challenges continue to develop including multi-drug resistance, globalization that increases pandemic risks and high mortality from severe infections. These challenges can be mitigated through improved diagnostics, focusing on both pathogen discovery and the host response. Here, we review how 'omics' technologies improve sepsis diagnosis, early pathogen identification and personalize therapy. Such host response diagnostics are possible due to the confluence of advanced laboratory techniques (e.g., transcriptomics, metabolomics, proteomics) along with advanced mathematical modeling such as machine learning techniques. The road ahead is promising, but obstacles remain before the impact of such advanced diagnostic modalities is felt at the bedside.",2015-06-01,11,862,81,2168
1675,25769815,Prediction of drug-ABC-transporter interaction--Recent advances and future challenges,"With the discovery of P-glycoprotein (P-gp), it became evident that ABC-transporters play a vital role in bioavailability and toxicity of drugs. They prevent intracellular accumulation of toxic compounds, which renders them a major defense mechanism against xenotoxic compounds. Their expression in cells of all major barriers (intestine, blood-brain barrier, blood-placenta barrier) as well as in metabolic organs (liver, kidney) also explains their influence on the ADMET properties of drugs and drug candidates. Thus, in silico models for the prediction of the probability of a compound to interact with P-gp or analogous transporters are of high value in the early phase of the drug discovery process. Within this review, we highlight recent developments in the area, with a special focus on the molecular basis of drug-transporter interaction. In addition, with the recent availability of X-ray structures of several ABC-transporters, also structure-based design methods have been applied and will be addressed.",2015-06-01,39,1016,85,2168
1660,25958010,Prediction of cytochrome P450 mediated metabolism,"Cytochrome P450 enzymes (CYPs) form one of the most important enzyme families involved in the metabolism of xenobiotics. CYPs comprise many isoforms, which catalyze a wide variety of reactions, and potentially, a large number of different metabolites can be formed. However, it is often hard to rationalize what metabolites these enzymes generate. In recent years, many different in silico approaches have been developed to predict binding or regioselective product formation for the different CYP isoforms. These comprise ligand-based methods that are trained on experimental CYP data and structure-based methods that consider how the substrate is oriented in the active site or/and how reactive the part of the substrate that is accessible to the heme group is. We will review key aspects for various approaches that are available to predict binding and site of metabolism (SOM), what outcome can be expected from the predictions, and how they could potentially be improved.",2015-06-01,16,976,49,2168
1639,26159433,The electronic stethoscope,"Most heart diseases are associated with and reflected by the sounds that the heart produces. Heart auscultation, defined as listening to the heart sound, has been a very important method for the early diagnosis of cardiac dysfunction. Traditional auscultation requires substantial clinical experience and good listening skills. The emergence of the electronic stethoscope has paved the way for a new field of computer-aided auscultation. This article provides an in-depth study of (1) the electronic stethoscope technology, and (2) the methodology for diagnosis of cardiac disorders based on computer-aided auscultation. The paper is based on a comprehensive review of (1) literature articles, (2) market (state-of-the-art) products, and (3) smartphone stethoscope apps. It covers in depth every key component of the computer-aided system with electronic stethoscope, from sensor design, front-end circuitry, denoising algorithm, heart sound segmentation, to the final machine learning techniques. Our intent is to provide an informative and illustrative presentation of the electronic stethoscope, which is valuable and beneficial to academics, researchers and engineers in the technical field, as well as to medical professionals to facilitate its use clinically. The paper provides the technological and medical basis for the development and commercialization of a real-time integrated heart sound detection, acquisition and quantification system.",2015-07-01,23,1450,26,2138
1641,26143394,Distinguishing Asthma Phenotypes Using Machine Learning Approaches,"Asthma is not a single disease, but an umbrella term for a number of distinct diseases, each of which are caused by a distinct underlying pathophysiological mechanism. These discrete disease entities are often labelled as 'asthma endotypes'. The discovery of different asthma subtypes has moved from subjective approaches in which putative phenotypes are assigned by experts to data-driven ones which incorporate machine learning. This review focuses on the methodological developments of one such machine learning technique-latent class analysis-and how it has contributed to distinguishing asthma and wheezing subtypes in childhood. It also gives a clinical perspective, presenting the findings of studies from the past 5 years that used this approach. The identification of true asthma endotypes may be a crucial step towards understanding their distinct pathophysiological mechanisms, which could ultimately lead to more precise prevention strategies, identification of novel therapeutic targets and the development of effective personalized therapies.",2015-07-01,26,1056,66,2138
1619,26346869,Machine learning and statistical methods for the prediction of maximal oxygen uptake: recent advances,"Maximal oxygen uptake (VO2max) indicates how many milliliters of oxygen the body can consume in a state of intense exercise per minute. VO2max plays an important role in both sport and medical sciences for different purposes, such as indicating the endurance capacity of athletes or serving as a metric in estimating the disease risk of a person. In general, the direct measurement of VO2max provides the most accurate assessment of aerobic power. However, despite a high level of accuracy, practical limitations associated with the direct measurement of VO2max, such as the requirement of expensive and sophisticated laboratory equipment or trained staff, have led to the development of various regression models for predicting VO2max. Consequently, a lot of studies have been conducted in the last years to predict VO2max of various target audiences, ranging from soccer athletes, nonexpert swimmers, cross-country skiers to healthy-fit adults, teenagers, and children. Numerous prediction models have been developed using different sets of predictor variables and a variety of machine learning and statistical methods, including support vector machine, multilayer perceptron, general regression neural network, and multiple linear regression. The purpose of this study is to give a detailed overview about the data-driven modeling studies for the prediction of VO2max conducted in recent years and to compare the performance of various VO2max prediction models reported in related literature in terms of two well-known metrics, namely, multiple correlation coefficient (R) and standard error of estimate. The survey results reveal that with respect to regression methods used to develop prediction models, support vector machine, in general, shows better performance than other methods, whereas multiple linear regression exhibits the worst performance.",2015-08-01,0,1856,101,2107
1638,26172351,"Segmentation and Image Analysis of Abnormal Lungs at CT: Current Approaches, Challenges, and Future Trends","The computer-based process of identifying the boundaries of lung from surrounding thoracic tissue on computed tomographic (CT) images, which is called segmentation, is a vital first step in radiologic pulmonary image analysis. Many algorithms and software platforms provide image segmentation routines for quantification of lung abnormalities; however, nearly all of the current image segmentation approaches apply well only if the lungs exhibit minimal or no pathologic conditions. When moderate to high amounts of disease or abnormalities with a challenging shape or appearance exist in the lungs, computer-aided detection systems may be highly likely to fail to depict those abnormal regions because of inaccurate segmentation methods. In particular, abnormalities such as pleural effusions, consolidations, and masses often cause inaccurate lung segmentation, which greatly limits the use of image processing methods in clinical and research contexts. In this review, a critical summary of the current methods for lung segmentation on CT images is provided, with special emphasis on the accuracy and performance of the methods in cases with abnormalities and cases with exemplary pathologic findings. The currently available segmentation methods can be divided into five major classes: (a) thresholding-based, (b) region-based, (c) shape-based, (d) neighboring anatomy-guided, and (e) machine learning-based methods. The feasibility of each class and its shortcomings are explained and illustrated with the most common lung abnormalities observed on CT images. In an overview, practical applications and evolving technologies combining the presented approaches for the practicing radiologist are detailed.",2015-08-01,24,1709,106,2107
1621,26293849,Health Informatics via Machine Learning for the Clinical Management of Patients,"Objectives:                    To review how health informatics systems based on machine learning methods have impacted the clinical management of patients, by affecting clinical practice.              Methods:                    We reviewed literature from 2010-2015 from databases such as Pubmed, IEEE xplore, and INSPEC, in which methods based on machine learning are likely to be reported. We bring together a broad body of literature, aiming to identify those leading examples of health informatics that have advanced the methodology of machine learning. While individual methods may have further examples that might be added, we have chosen some of the most representative, informative exemplars in each case.              Results:                    Our survey highlights that, while much research is taking place in this high-profile field, examples of those that affect the clinical management of patients are seldom found. We show that substantial progress is being made in terms of methodology, often by data scientists working in close collaboration with clinical groups.              Conclusions:                    Health informatics systems based on machine learning are in their infancy and the translation of such systems into clinical management has yet to be performed at scale.",2015-08-01,7,1297,79,2107
2798,25461506,"Computational and experimental single cell biology techniques for the definition of cell type heterogeneity, interplay and intracellular dynamics","Novel technological developments enable single cell population profiling with respect to their spatial and molecular setup. These include single cell sequencing, flow cytometry and multiparametric imaging approaches and open unprecedented possibilities to learn about the heterogeneity, dynamics and interplay of the different cell types which constitute tissues and multicellular organisms. Statistical and dynamic systems theory approaches have been applied to quantitatively describe a variety of cellular processes, such as transcription and cell signaling. Machine learning approaches have been developed to define cell types, their mutual relationships, and differentiation hierarchies shaping heterogeneous cell populations, yielding insights into topics such as, for example, immune cell differentiation and tumor cell type composition. This combination of experimental and computational advances has opened perspectives towards learning predictive multi-scale models of heterogeneous cell populations.",2015-08-01,10,1010,145,2107
1624,26263424,Data mining and education,"An emerging field of educational data mining (EDM) is building on and contributing to a wide variety of disciplines through analysis of data coming from various educational technologies. EDM researchers are addressing questions of cognition, metacognition, motivation, affect, language, social discourse, etc. using data from intelligent tutoring systems, massive open online courses, educational games and simulations, and discussion forums. The data include detailed action and timing logs of student interactions in user interfaces such as graded responses to questions or essays, steps in rich problem solving environments, games or simulations, discussion forum posts, or chat dialogs. They might also include external sensors such as eye tracking, facial expression, body movement, etc. We review how EDM has addressed the research questions that surround the psychology of learning with an emphasis on assessment, transfer of learning and model discovery, the role of affect, motivation and metacognition on learning, and analysis of language data and collaborative learning. For example, we discuss (1) how different statistical assessment methods were used in a data mining competition to improve prediction of student responses to intelligent tutor tasks, (2) how better cognitive models can be discovered from data and used to improve instruction, (3) how data-driven models of student affect can be used to focus discussion in a dialog-based tutoring system, and (4) how machine learning techniques applied to discussion data can be used to produce automated agents that support student learning as they collaborate in a chat room or a discussion board.",2015-08-01,1,1665,25,2107
1662,25944500,Past and current use of walking measures for children with spina bifida: a systematic review,"Objectives:                    To describe walking measurement in children with spina bifida and to identify patterns in the use of walking measures in this population.              Data sources:                    Seven medical databases-Medline, PubMed, Embase, Scopus, Web of Science, CINAHL, and AMED-were searched from the earliest known record until March 11, 2014. Search terms encompassed 3 themes: (1) children; (2) spina bifida; and (3) walking.              Study selection:                    Articles were included if participants were children with spina bifida aged 1 to 17 years and if walking was measured. Articles were excluded if the assessment was restricted to kinematic, kinetic, or electromyographic analysis of walking. A total of 1751 abstracts were screened by 2 authors independently, and 109 articles were included in this review.              Data extraction:                    Data were extracted using standardized forms. Extracted data included study and participant characteristics and details about the walking measures used, including psychometric properties. Two authors evaluated the methodological quality of articles using a previously published framework that considers sampling method, study design, and psychometric properties of the measures used.              Data synthesis:                    Nineteen walking measures were identified. Ordinal-level rating scales (eg, Hoffer Functional Ambulation Scale) were most commonly used (57% of articles), followed by ratio-level, spatiotemporal measures, such as walking speed (18% of articles). Walking was measured for various reasons relevant to multiple health care disciplines. A machine learning analysis was used to identify patterns in the use of walking measures. The learned classifier predicted whether a spatiotemporal measure was used with 77.1% accuracy. A trend to use spatiotemporal measures in older children and those with lumbar and sacral spinal lesions was identified. Most articles were prospective studies that used samples of convenience and unblinded assessors. Few articles evaluated or considered the psychometric properties of the walking measures used.              Conclusions:                    Despite a demonstrated need to measure walking in children with spina bifida, few valid, reliable, and responsive measures have been established for this population.",2015-08-01,5,2383,92,2107
1612,26442103,A survey about methods dedicated to epistasis detection,"During the past decade, findings of genome-wide association studies (GWAS) improved our knowledge and understanding of disease genetics. To date, thousands of SNPs have been associated with diseases and other complex traits. Statistical analysis typically looks for association between a phenotype and a SNP taken individually via single-locus tests. However, geneticists admit this is an oversimplified approach to tackle the complexity of underlying biological mechanisms. Interaction between SNPs, namely epistasis, must be considered. Unfortunately, epistasis detection gives rise to analytic challenges since analyzing every SNP combination is at present impractical at a genome-wide scale. In this review, we will present the main strategies recently proposed to detect epistatic interactions, along with their operating principle. Some of these methods are exhaustive, such as multifactor dimensionality reduction, likelihood ratio-based tests or receiver operating characteristic curve analysis; some are non-exhaustive, such as machine learning techniques (random forests, Bayesian networks) or combinatorial optimization approaches (ant colony optimization, computational evolution system).",2015-09-01,24,1200,55,2076
1611,26442199,"Rare disease diagnosis: A review of web search, social media and large-scale data-mining approaches","Physicians and the general public are increasingly using web-based tools to find answers to medical questions. The field of rare diseases is especially challenging and important as shown by the long delay and many mistakes associated with diagnoses. In this paper we review recent initiatives on the use of web search, social media and data mining in data repositories for medical diagnosis. We compare the retrieval accuracy on 56 rare disease cases with known diagnosis for the web search tools google.com, pubmed.gov, omim.org and our own search tool findzebra.com. We give a detailed description of IBM's Watson system and make a rough comparison between findzebra.com and Watson on subsets of the Doctor's dilemma dataset. The recall@10 and recall@20 (fraction of cases where the correct result appears in top 10 and top 20) for the 56 cases are found to be be 29%, 16%, 27% and 59% and 32%, 18%, 34% and 64%, respectively. Thus, FindZebra has a significantly (p < 0.01) higher recall than the other 3 search engines. When tested under the same conditions, Watson and FindZebra showed similar recall@10 accuracy. However, the tests were performed on different subsets of Doctors dilemma questions. Advances in technology and access to high quality data have opened new possibilities for aiding the diagnostic process. Specialized search engines, data mining tools and social media are some of the areas that hold promise.",2015-09-01,26,1426,99,2076
1613,26442053,Crop improvement using life cycle datasets acquired under field conditions,"Crops are exposed to various environmental stresses in the field throughout their life cycle. Modern plant science has provided remarkable insights into the molecular networks of plant stress responses in laboratory conditions, but the responses of different crops to environmental stresses in the field need to be elucidated. Recent advances in omics analytical techniques and information technology have enabled us to integrate data from a spectrum of physiological metrics of field crops. The interdisciplinary efforts of plant science and data science enable us to explore factors that affect crop productivity and identify stress tolerance-related genes and alleles. Here, we describe recent advances in technologies that are key components for data driven crop design, such as population genomics, chronological omics analyses, and computer-aided molecular network prediction. Integration of the outcomes from these technologies will accelerate our understanding of crop phenology under practical field situations and identify key characteristics to represent crop stress status. These elements would help us to genetically engineer ""designed crops"" to prevent yield shortfalls because of environmental fluctuations due to future climate change.",2015-09-01,6,1251,74,2076
1685,25725305,Transcriptomics of mRNA and egg quality in farmed fish: Some recent developments and future directions,"Maternal mRNA transcripts deposited in growing oocytes regulate early development and are under intensive investigation as determinants of egg quality. The research has evolved from single gene studies to microarray and now RNA-Seq analyses in which mRNA expression by virtually every gene can be assessed and related to gamete quality. Such studies have mainly focused on genes changing two- to several-fold in expression between biological states, and have identified scores of candidate genes and a few gene networks whose functioning is related to successful development. However, ever-increasing yields of information from high throughput methods for detecting transcript abundance have far outpaced progress in methods for analyzing the massive quantities of gene expression data, and especially for meaningful relation of whole transcriptome profiles to gamete quality. We have developed a new approach to this problem employing artificial neural networks and supervised machine learning with other novel bioinformatics procedures to discover a previously unknown level of ovarian transcriptome function at which minute changes in expression of a few hundred genes is highly predictive of egg quality. In this paper, we briefly review the progress in transcriptomics of fish egg quality and discuss some future directions for this field of study.",2015-09-01,17,1353,102,2076
1632,26233633,A Review on Carotid Ultrasound Atherosclerotic Tissue Characterization and Stroke Risk Stratification in Machine Learning Framework,"Cardiovascular diseases (including stroke and heart attack) are identified as the leading cause of death in today's world. However, very little is understood about the arterial mechanics of plaque buildup, arterial fibrous cap rupture, and the role of abnormalities of the vasa vasorum. Recently, ultrasonic echogenicity characteristics and morphological characterization of carotid plaque types have been shown to have clinical utility in classification of stroke risks. Furthermore, this characterization supports aggressive and intensive medical therapy as well as procedures, including endarterectomy and stenting. This is the first state-of-the-art review to provide a comprehensive understanding of the field of ultrasonic vascular morphology tissue characterization. This paper presents fundamental and advanced ultrasonic tissue characterization and feature extraction methods for analyzing plaque. Additionally, the paper shows how the risk stratification is achieved using machine learning paradigms. More advanced methods need to be developed which can segment the carotid artery walls into multiple regions such as the bulb region and areas both proximal and distal to the bulb. Furthermore, multimodality imaging is needed for validation of such advanced methods for stroke and cardiovascular risk stratification.",2015-09-01,13,1326,131,2076
1625,26256959,Cancer Cell Line Panels Empower Genomics-Based Discovery of Precision Cancer Medicine,"Since the first human cancer cell line, HeLa, was established in the early 1950s, there has been a steady increase in the number and tumor type of available cancer cell line models. Cancer cell lines have made significant contributions to the development of various chemotherapeutic agents. Recent advances in multi-omics technologies have facilitated detailed characterizations of the genomic, transcriptomic, proteomic, and epigenomic profiles of these cancer cell lines. An increasing number of studies employ the power of a cancer cell line panel to provide predictive biomarkers for targeted and cytotoxic agents, including those that are already used in clinical practice. Different types of statistical and machine learning algorithms have been developed to analyze the large-scale data sets that have been produced. However, much work remains to address the discrepancies in drug assay results from different platforms and the frequent failures to translate discoveries from cell line models to the clinic. Nevertheless, continuous expansion of cancer cell line panels should provide unprecedented opportunities to identify new candidate targeted therapies, particularly for the so-called ""dark matter"" group of cancers, for which pharmacologically tractable driver mutations have not been identified.",2015-09-01,8,1309,85,2076
1645,26099739,Pseudo nucleotide composition or PseKNC: an effective formulation for analyzing genomic sequences,"With the avalanche of DNA/RNA sequences generated in the post-genomic age, it is urgent to develop automated methods for analyzing the relationship between the sequences and their functions. Towards this goal, a series of sequence-based methods have been proposed and applied to analyze various character-unknown DNA/RNA sequences in order for in-depth understanding their action mechanisms and processes. Compared with the classical sequence-based methods, the pseudo nucleotide composition or PseKNC approach developed very recently has the following advantages: (1) it can convert length-different DNA/RNA sequences into dimension-fixed digital vectors that can be directly handled by all the existing machine-learning algorithms or operation engines; (2) it can contain the desired features and properties according to the selection or definition of users; (3) it can cover considerable sequence pattern information, both local and global. This minireview is focused on the concept of pseudo nucleotide composition, its development and applications.",2015-10-01,61,1053,97,2046
1222,26640765,"Multivariate analyses applied to fetal, neonatal and pediatric MRI of neurodevelopmental disorders","Multivariate analysis (MVA) is a class of statistical and pattern recognition methods that involve the processing of data that contains multiple measurements per sample. MVA can be used to address a wide variety of medical neuroimaging-related challenges including identifying variables associated with a measure of clinical importance (i.e. patient outcome), creating diagnostic tests, assisting in characterizing developmental disorders, understanding disease etiology, development and progression, assisting in treatment monitoring and much more. Compared to adults, imaging of developing immature brains has attracted less attention from MVA researchers. However, remarkable MVA research growth has occurred in recent years. This paper presents the results of a systematic review of the literature focusing on MVA technologies applied to neurodevelopmental disorders in fetal, neonatal and pediatric magnetic resonance imaging (MRI) of the brain. The goal of this manuscript is to provide a concise review of the state of the scientific literature on studies employing brain MRI and MVA in a pre-adult population. Neurological developmental disorders addressed in the MVA research contained in this review include autism spectrum disorder, attention deficit hyperactivity disorder, epilepsy, schizophrenia and more. While the results of this review demonstrate considerable interest from the scientific community in applications of MVA technologies in pediatric/neonatal/fetal brain MRI, the field is still young and considerable research growth remains ahead of us.",2015-10-01,9,1570,98,2046
1667,25808539,"Improvements, trends, and new ideas in molecular docking: 2012-2013 in review","Molecular docking is a computational method for predicting the placement of ligands in the binding sites of their receptor(s). In this review, we discuss the methodological developments that occurred in the docking field in 2012 and 2013, with a particular focus on the more difficult aspects of this computational discipline. The main challenges and therefore focal points for developments in docking, covered in this review, are receptor flexibility, solvation, scoring, and virtual screening. We specifically deal with such aspects of molecular docking and its applications as selection criteria for constructing receptor ensembles, target dependence of scoring functions, integration of higher-level theory into scoring, implicit and explicit handling of solvation in the binding process, and comparison and evaluation of docking and scoring methods.",2015-10-01,47,854,77,2046
1683,25732094,A rational model of function learning,"Theories of how people learn relationships between continuous variables have tended to focus on two possibilities: one, that people are estimating explicit functions, or two that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, which provide a probabilistic basis for similarity-based function learning, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to define a rational model of human function learning that combines the strengths of both approaches and accounts for a wide variety of experimental results.",2015-10-01,6,809,37,2046
1666,25810317,Analyzing Medical Image Search Behavior: Semantics and Prediction of Query Results,"Log files of information retrieval systems that record user behavior have been used to improve the outcomes of retrieval systems, understand user behavior, and predict events. In this article, a log file of the ARRS GoldMiner search engine containing 222,005 consecutive queries is analyzed. Time stamps are available for each query, as well as masked IP addresses, which enables to identify queries from the same person. This article describes the ways in which physicians (or Internet searchers interested in medical images) search and proposes potential improvements by suggesting query modifications. For example, many queries contain only few terms and therefore are not specific; others contain spelling mistakes or non-medical terms that likely lead to poor or empty results. One of the goals of this report is to predict the number of results a query will have since such a model allows search engines to automatically propose query modifications in order to avoid result lists that are empty or too large. This prediction is made based on characteristics of the query terms themselves. Prediction of empty results has an accuracy above 88%, and thus can be used to automatically modify the query to avoid empty result sets for a user. The semantic analysis and data of reformulations done by users in the past can aid the development of better search systems, particularly to improve results for novice users. Therefore, this paper gives important ideas to better understand how people search and how to use this knowledge to improve the performance of specialized medical search engines.",2015-10-01,0,1597,82,2046
1605,26502306,Diffusion Tensor Imaging of TBI: Potentials and Challenges,"Neuroimaging plays a critical role in the setting in traumatic brain injury (TBI). Diffusion tensor imaging (DTI) is an advanced magnetic resonance imaging technique that is capable of providing rich information on the brain's neuroanatomic connectome. The purpose of this article is to systematically review the role of DTI and advanced diffusion techniques in the setting of TBI, including diffusion kurtosis imaging (DKI), neurite orientation dispersion and density imaging, diffusion spectrum imaging, and q-ball imaging. We discuss clinical applications of DTI and review the DTI literature as it pertains to TBI. Despite the continued advancements in DTI and related diffusion techniques over the past 20 years, DTI techniques are sensitive for TBI at the group level only and there is insufficient evidence that DTI plays a role at the individual level. We conclude by discussing future directions in DTI research in TBI including the role of machine learning in the pattern classification of TBI.",2015-10-01,15,1004,58,2046
1627,26241930,Automated negotiation in environmental resource management: Review and assessment,"Negotiation is an integral part of our daily life and plays an important role in resolving conflicts and facilitating human interactions. Automated negotiation, which aims at capturing the human negotiation process using artificial intelligence and machine learning techniques, is well-established in e-commerce, but its application in environmental resource management remains limited. This is due to the inherent uncertainties and complexity of environmental issues, along with the diversity of stakeholders' perspectives when dealing with these issues. The objective of this paper is to describe the main components of automated negotiation, review and compare machine learning techniques in automated negotiation, and provide a guideline for the selection of suitable methods in the particular context of stakeholders' negotiation over environmental resource issues. We advocate that automated negotiation can facilitate the involvement of stakeholders in the exploration of a plurality of solutions in order to reach a mutually satisfying agreement and contribute to informed decisions in environmental management along with the need for further studies to consolidate the potential of this modeling approach.",2015-10-01,0,1214,81,2046
1608,26468133,Toward Big Data Analytics: Review of Predictive Models in Management of Diabetes and Its Complications,"Diabetes is one of the top priorities in medical science and health care management, and an abundance of data and information is available on these patients. Whether data stem from statistical models or complex pattern recognition models, they may be fused into predictive models that combine patient information and prognostic outcome results. Such knowledge could be used in clinical decision support, disease surveillance, and public health management to improve patient care. Our aim was to review the literature and give an introduction to predictive models in screening for and the management of prevalent short- and long-term complications in diabetes. Predictive models have been developed for management of diabetes and its complications, and the number of publications on such models has been growing over the past decade. Often multiple logistic or a similar linear regression is used for prediction model development, possibly owing to its transparent functionality. Ultimately, for prediction models to prove useful, they must demonstrate impact, namely, their use must generate better patient outcomes. Although extensive effort has been put in to building these predictive models, there is a remarkable scarcity of impact studies.",2015-10-01,12,1245,102,2046
1602,26512199,Machine Learning Methods for Predicting HLA-Peptide Binding Activity,"As major histocompatibility complexes in humans, the human leukocyte antigens (HLAs) have important functions to present antigen peptides onto T-cell receptors for immunological recognition and responses. Interpreting and predicting HLA-peptide binding are important to study T-cell epitopes, immune reactions, and the mechanisms of adverse drug reactions. We review different types of machine learning methods and tools that have been used for HLA-peptide binding prediction. We also summarize the descriptors based on which the HLA-peptide binding prediction models have been constructed and discuss the limitation and challenges of the current methods. Lastly, we give a future perspective on the HLA-peptide binding prediction method based on network analysis.",2015-10-01,20,764,68,2046
1596,26579514,Prediction of Genetic Interactions Using Machine Learning and Network Properties,"A genetic interaction (GI) is a type of interaction where the effect of one gene is modified by the effect of one or several other genes. These interactions are important for delineating functional relationships among genes and their corresponding proteins, as well as elucidating complex biological processes and diseases. An important type of GI - synthetic sickness or synthetic lethality - involves two or more genes, where the loss of either gene alone has little impact on cell viability, but the combined loss of all genes leads to a severe decrease in fitness (sickness) or cell death (lethality). The identification of GIs is an important problem for it can help delineate pathways, protein complexes, and regulatory dependencies. Synthetic lethal interactions have important clinical and biological significance, such as providing therapeutically exploitable weaknesses in tumors. While near systematic high-content screening for GIs is possible in single cell organisms such as yeast, the systematic discovery of GIs is extremely difficult in mammalian cells. Therefore, there is a great need for computational approaches to reliably predict GIs, including synthetic lethal interactions, in these organisms. Here, we review the state-of-the-art approaches, strategies, and rigorous evaluation methods for learning and predicting GIs, both under general (healthy/standard laboratory) conditions and under specific contexts, such as diseases.",2015-10-01,9,1451,80,2046
1221,26648850,Control Capabilities of Myoelectric Robotic Prostheses by Hand Amputees: A Scientific Research and Market Overview,"Hand amputation can dramatically affect the capabilities of a person. Cortical reorganization occurs in the brain, but the motor and somatosensorial cortex can interact with the remnant muscles of the missing hand even many years after the amputation, leading to the possibility to restore the capabilities of hand amputees through myoelectric prostheses. Myoelectric hand prostheses with many degrees of freedom are commercially available and recent advances in rehabilitation robotics suggest that their natural control can be performed in real life. The first commercial products exploiting pattern recognition to recognize the movements have recently been released, however the most common control systems are still usually unnatural and must be learned through long training. Dexterous and naturally controlled robotic prostheses can become reality in the everyday life of amputees but the path still requires many steps. This mini-review aims to improve the situation by giving an overview of the advancements in the commercial and scientific domains in order to outline the current and future chances in this field and to foster the integration between market and scientific research.",2015-11-01,21,1191,114,2015
1211,26759786,Studying depression using imaging and machine learning methods,"Depression is a complex clinical entity that can pose challenges for clinicians regarding both accurate diagnosis and effective timely treatment. These challenges have prompted the development of multiple machine learning methods to help improve the management of this disease. These methods utilize anatomical and physiological data acquired from neuroimaging to create models that can identify depressed patients vs. non-depressed patients and predict treatment outcomes. This article (1) presents a background on depression, imaging, and machine learning methodologies; (2) reviews methodologies of past studies that have used imaging and machine learning to study depression; and (3) suggests directions for future depression-related studies.",2015-11-01,22,746,62,2015
1609,26460190,Computational prediction of protein interfaces: A review of data driven methods,"Reliably pinpointing which specific amino acid residues form the interface(s) between a protein and its binding partner(s) is critical for understanding the structural and physicochemical determinants of protein recognition and binding affinity, and has wide applications in modeling and validating protein interactions predicted by high-throughput methods, in engineering proteins, and in prioritizing drug targets. Here, we review the basic concepts, principles and recent advances in computational approaches to the analysis and prediction of protein-protein interfaces. We point out caveats for objectively evaluating interface predictors, and discuss various applications of data-driven interface predictors for improving energy model-driven protein-protein docking. Finally, we stress the importance of exploiting binding partner information in reliably predicting interfaces and highlight recent advances in this emerging direction.",2015-11-01,32,939,79,2015
1616,26386547,Mid-level image representations for real-time heart view plane classification of echocardiograms,"In this paper, we explore mid-level image representations for real-time heart view plane classification of 2D echocardiogram ultrasound images. The proposed representations rely on bags of visual words, successfully used by the computer vision community in visual recognition problems. An important element of the proposed representations is the image sampling with large regions, drastically reducing the execution time of the image characterization procedure. Throughout an extensive set of experiments, we evaluate the proposed approach against different image descriptors for classifying four heart view planes. The results show that our approach is effective and efficient for the target problem, making it suitable for use in real-time setups. The proposed representations are also robust to different image transformations, e.g., downsampling, noise filtering, and different machine learning classifiers, keeping classification accuracy above 90%. Feature extraction can be performed in 30 fps or 60 fps in some cases. This paper also includes an in-depth review of the literature in the area of automatic echocardiogram view classification giving the reader a through comprehension of this field of study.",2015-11-01,4,1213,96,2015
1601,26540053,Computational Prediction of RNA-Binding Proteins and Binding Sites,"Proteins and RNA interaction have vital roles in many cellular processes such as protein synthesis, sequence encoding, RNA transfer, and gene regulation at the transcriptional and post-transcriptional levels. Approximately 6%-8% of all proteins are RNA-binding proteins (RBPs). Distinguishing these RBPs or their binding residues is a major aim of structural biology. Previously, a number of experimental methods were developed for the determination of protein-RNA interactions. However, these experimental methods are expensive, time-consuming, and labor-intensive. Alternatively, researchers have developed many computational approaches to predict RBPs and protein-RNA binding sites, by combining various machine learning methods and abundant sequence and/or structural features. There are three kinds of computational approaches, which are prediction from protein sequence, prediction from protein structure, and protein-RNA docking. In this paper, we review all existing studies of predictions of RNA-binding sites and RBPs and complexes, including data sets used in different approaches, sequence and structural features used in several predictors, prediction method classifications, performance comparisons, evaluation methods, and future directions.",2015-11-01,24,1256,66,2015
1600,26556470,Applicability of gene expression and systems biology to develop pharmacogenetic predictors; antipsychotic-induced extrapyramidal symptoms as an example,"Pharmacogenetics has been driven by a candidate gene approach. The disadvantage of this approach is that is limited by our current understanding of the mechanisms by which drugs act. Gene expression could help to elucidate the molecular signatures of antipsychotic treatments searching for dysregulated molecular pathways and the relationships between gene products, especially protein-protein interactions. To embrace the complexity of drug response, machine learning methods could help to identify gene-gene interactions and develop pharmacogenetic predictors of drug response. The present review summarizes the applicability of the topics presented here (gene expression, network analysis and gene-gene interactions) in pharmacogenetics. In order to achieve this, we present an example of identifying genetic predictors of extrapyramidal symptoms induced by antipsychotic.",2015-11-01,2,875,151,2015
1646,26092722,Spatial and temporal epidemiological analysis in the Big Data era,"Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.",2015-11-01,16,2634,65,2015
1654,26014110,The role of machine learning in neuroimaging for drug discovery and development,"Neuroimaging has been identified as a potentially powerful probe for the in vivo study of drug effects on the brain with utility across several phases of drug development spanning preclinical and clinical investigations. Specifically, neuroimaging can provide insight into drug penetration and distribution, target engagement, pharmacodynamics, mechanistic action and potential indicators of clinical efficacy. In this review, we focus on machine learning approaches for neuroimaging which enable us to make predictions at the individual level based on the distributed effects across the whole brain. Crucially, these approaches can be trained on data from one study and applied to an independent study and, unlike group-level statistics, can be readily use to assess the generalisability to unseen data. In this review, we present examples and suggestions for how machine learning could help answer fundamental questions spanning the drug discovery pipeline: (1) Who should I recruit for this study? (2) What should I measure and when should I measure it? (3) How does the pharmacological agent behave using an experimental medicine model?, and (4) How does a compound differ from and/or resemble existing compounds? Specifically, we present studies from the literature and we suggest areas for the focus of future development. Further refinement and tailoring of machine learning techniques may help realise their tremendous potential for drug discovery and drug validation.",2015-11-01,12,1476,79,2015
1590,26636071,Gradient Matching Methods for Computational Inference in Mechanistic Models for Systems Biology: A Review and Comparative Analysis,"Parameter inference in mathematical models of biological pathways, expressed as coupled ordinary differential equations (ODEs), is a challenging problem in contemporary systems biology. Conventional methods involve repeatedly solving the ODEs by numerical integration, which is computationally onerous and does not scale up to complex systems. Aimed at reducing the computational costs, new concepts based on gradient matching have recently been proposed in the computational statistics and machine learning literature. In a preliminary smoothing step, the time series data are interpolated; then, in a second step, the parameters of the ODEs are optimized, so as to minimize some metric measuring the difference between the slopes of the tangents to the interpolants, and the time derivatives from the ODEs. In this way, the ODEs never have to be solved explicitly. This review provides a concise methodological overview of the current state-of-the-art methods for gradient matching in ODEs, followed by an empirical comparative evaluation based on a set of widely used and representative benchmark data.",2015-11-01,2,1105,130,2015
1620,26298193,Learning with hidden variables,"Learning and inferring features that generate sensory input is a task continuously performed by cortex. In recent years, novel algorithms and learning rules have been proposed that allow neural network models to learn such features from natural images, written text, audio signals, etc. These networks usually involve deep architectures with many layers of hidden neurons. Here we review recent advancements in this area emphasizing, amongst other things, the processing of dynamical inputs by networks with hidden nodes and the role of single neuron models. These points and the questions they arise can provide conceptual advancements in understanding of learning in the cortex and the relationship between machine learning approaches to learning with hidden nodes and those in cortical circuits.",2015-12-01,1,798,30,1985
1595,26581149,Machine Learning Approaches for Predicting Radiation Therapy Outcomes: A Clinician's Perspective,"Radiation oncology has always been deeply rooted in modeling, from the early days of isoeffect curves to the contemporary Quantitative Analysis of Normal Tissue Effects in the Clinic (QUANTEC) initiative. In recent years, medical modeling for both prognostic and therapeutic purposes has exploded thanks to increasing availability of electronic data and genomics. One promising direction that medical modeling is moving toward is adopting the same machine learning methods used by companies such as Google and Facebook to combat disease. Broadly defined, machine learning is a branch of computer science that deals with making predictions from complex data through statistical models. These methods serve to uncover patterns in data and are actively used in areas such as speech recognition, handwriting recognition, face recognition, ""spam"" filtering (junk email), and targeted advertising. Although multiple radiation oncology research groups have shown the value of applied machine learning (ML), clinical adoption has been slow due to the high barrier to understanding these complex models by clinicians. Here, we present a review of the use of ML to predict radiation therapy outcomes from the clinician's point of view with the hope that it lowers the ""barrier to entry"" for those without formal training in ML. We begin by describing 7 principles that one should consider when evaluating (or creating) an ML model in radiation oncology. We next introduce 3 popular ML methods--logistic regression (LR), support vector machine (SVM), and artificial neural network (ANN)--and critique 3 seminal papers in the context of these principles. Although current studies are in exploratory stages, the overall methodology has progressively matured, and the field is ready for larger-scale further investigation.",2015-12-01,48,1808,96,1985
1215,26690135,Bioinformatics Mining and Modeling Methods for the Identification of Disease Mechanisms in Neurodegenerative Disorders,"Since the decoding of the Human Genome, techniques from bioinformatics, statistics, and machine learning have been instrumental in uncovering patterns in increasing amounts and types of different data produced by technical profiling technologies applied to clinical samples, animal models, and cellular systems. Yet, progress on unravelling biological mechanisms, causally driving diseases, has been limited, in part due to the inherent complexity of biological systems. Whereas we have witnessed progress in the areas of cancer, cardiovascular and metabolic diseases, the area of neurodegenerative diseases has proved to be very challenging. This is in part because the aetiology of neurodegenerative diseases such as Alzheimers disease or Parkinsons disease is unknown, rendering it very difficult to discern early causal events. Here we describe a panel of bioinformatics and modeling approaches that have recently been developed to identify candidate mechanisms of neurodegenerative diseases based on publicly available data and knowledge. We identify two complementary strategies-data mining techniques using genetic data as a starting point to be further enriched using other data-types, or alternatively to encode prior knowledge about disease mechanisms in a model based framework supporting reasoning and enrichment analysis. Our review illustrates the challenges entailed in integrating heterogeneous, multiscale and multimodal information in the area of neurology in general and neurodegeneration in particular. We conclude, that progress would be accelerated by increasing efforts on performing systematic collection of multiple data-types over time from each individual suffering from neurodegenerative disease. The work presented here has been driven by project AETIONOMY; a project funded in the course of the Innovative Medicines Initiative (IMI); which is a public-private partnership of the European Federation of Pharmaceutical Industry Associations (EFPIA) and the European Commission (EC).",2015-12-01,20,2012,118,1985
1633,26225918,Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/UTHealth shared task Track 1,"The 2014 i2b2/UTHealth Natural Language Processing (NLP) shared task featured four tracks. The first of these was the de-identification track focused on identifying protected health information (PHI) in longitudinal clinical narratives. The longitudinal nature of clinical narratives calls particular attention to details of information that, while benign on their own in separate records, can lead to identification of patients in combination in longitudinal records. Accordingly, the 2014 de-identification track addressed a broader set of entities and PHI than covered by the Health Insurance Portability and Accountability Act - the focus of the de-identification shared task that was organized in 2006. Ten teams tackled the 2014 de-identification task and submitted 22 system outputs for evaluation. Each team was evaluated on their best performing system output. Three of the 10 systems achieved F1 scores over .90, and seven of the top 10 scored over .75. The most successful systems combined conditional random fields and hand-written rules. Our findings indicate that automated systems can be very effective for this task, but that de-identification is not yet a solved problem.",2015-12-01,51,1188,131,1985
1618,26358617,Quantitative structure-activity relationship: promising advances in drug discovery platforms,"Introduction:                    Quantitative structure-activity relationship (QSAR) modeling is one of the most popular computer-aided tools employed in medicinal chemistry for drug discovery and lead optimization. It is especially powerful in the absence of 3D structures of specific drug targets. QSAR methods have been shown to draw public attention since they were first introduced.              Areas covered:                    In this review, the authors provide a brief discussion of the basic principles of QSAR, model development and model validation. They also highlight the current applications of QSAR in different fields, particularly in virtual screening, rational drug design and multi-target QSAR. Finally, in view of recent controversies, the authors detail the challenges faced by QSAR modeling and the relevant solutions. The aim of this review is to show how QSAR modeling can be applied in novel drug discovery, design and lead optimization.              Expert opinion:                    QSAR should intentionally be used as a powerful tool for fragment-based drug design platforms in the field of drug discovery and design. Although there have been an increasing number of experimentally determined protein structures in recent years, a great number of protein structures cannot be easily obtained (i.e., membrane transport proteins and G-protein coupled receptors). Fragment-based drug discovery, such as QSAR, could be applied further and have a significant role in dealing with these problems. Moreover, along with the development of computer software and hardware, it is believed that QSAR will be increasingly important.",2015-12-01,18,1651,92,1985
1606,26499213,The identification of cis-regulatory elements: A review from a machine learning perspective,"The majority of the human genome consists of non-coding regions that have been called junk DNA. However, recent studies have unveiled that these regions contain cis-regulatory elements, such as promoters, enhancers, silencers, insulators, etc. These regulatory elements can play crucial roles in controlling gene expressions in specific cell types, conditions, and developmental stages. Disruption to these regions could contribute to phenotype changes. Precisely identifying regulatory elements is key to deciphering the mechanisms underlying transcriptional regulation. Cis-regulatory events are complex processes that involve chromatin accessibility, transcription factor binding, DNA methylation, histone modifications, and the interactions between them. The development of next-generation sequencing techniques has allowed us to capture these genomic features in depth. Applied analysis of genome sequences for clinical genetics has increased the urgency for detecting these regions. However, the complexity of cis-regulatory events and the deluge of sequencing data require accurate and efficient computational approaches, in particular, machine learning techniques. In this review, we describe machine learning approaches for predicting transcription factor binding sites, enhancers, and promoters, primarily driven by next-generation sequencing data. Data sources are provided in order to facilitate testing of novel methods. The purpose of this review is to attract computational experts and data scientists to advance this field.",2015-12-01,9,1539,91,1985
1614,26429153,Identification of drug candidates and repurposing opportunities through compound-target interaction networks,"Introduction:                    System-wide identification of both on- and off-targets of chemical probes provides improved understanding of their therapeutic potential and possible adverse effects, thereby accelerating and de-risking drug discovery process. Given the high costs of experimental profiling of the complete target space of drug-like compounds, computational models offer systematic means for guiding these mapping efforts. These models suggest the most potent interactions for further experimental or pre-clinical evaluation both in cell line models and in patient-derived material.              Areas covered:                    The authors focus here on network-based machine learning models and their use in the prediction of novel compound-target interactions both in target-based and phenotype-based drug discovery applications. While currently being used mainly in complementing the experimentally mapped compound-target networks for drug repurposing applications, such as extending the target space of already approved drugs, these network pharmacology approaches may also suggest completely unexpected and novel investigational probes for drug development.              Expert opinion:                    Although the studies reviewed here have already demonstrated that network-centric modeling approaches have the potential to identify candidate compounds and selective targets in disease networks, many challenges still remain. In particular, these challenges include how to incorporate the cellular context and genetic background into the disease networks to enable more stratified and selective target predictions, as well as how to make the prediction models more realistic for the practical drug discovery and therapeutic applications.",2015-12-01,15,1766,108,1985
1631,26233900,Machine learning in burn care and research: A systematic review of the literature,"Background:                    To date, there are no reviews on machine learning (ML) in burn care. Considering the growth of ML in medicine and the complexities and challenges of burn care, this review specializes on ML applications in burn care. The objective was to examine the features and impact of applications in targeting various aspects of burn care and research.              Methods:                    MEDLINE, the Cochrane Database of Systematic Reviews, ScienceDirect, and citation review of relevant primary and review articles were searched for studies involving burn care/research and machine learning. Data were abstracted on study design, study size, year, population, application of burn care/research, ML technique(s), and algorithm performance.              Results:                    15 retrospective observational studies involving burn patients met inclusion criteria. In total 5105 patients with acute thermal injury, 171 clinical burn wounds, 180 9-mer peptides, and 424 12-mer peptides were included in the studies. Studies focused on burn diagnosis (n=5), aminoglycoside response (n=3), hospital length of stay (n=2), survival/mortality (n=4), burn healing time (n=1), and antimicrobial peptides in burn patients (n=1). Of these 15 studies, 11 used artificial neural networks. Importantly, all studies demonstrated the benefits of ML in burn care/research and superior performance over traditional statistical methods. However, algorithm performance was assessed differently by different authors. Feature selection varied among studies, but studies with similar applications shared specific features including age, gender, presence of inhalation injury, total body surface area burned, and when available, various degrees of burns, infections, and previous histories/conditions of burn patients.              Conclusion:                    A common feature base may be determined for ML in burn care/research, but the impact of ML will require further validation in prospective observational studies and randomized clinical trials, establishment of common performance metrics, and high quality evidence about clinical and economic impacts. Only then can ML applications be advanced and accepted widely in burn care/research.",2015-12-01,3,2254,81,1985
1213,26696900,Prediction of Druggable Proteins Using Machine Learning and Systems Biology: A Mini-Review,"The emergence of -omics technologies has allowed the collection of vast amounts of data on biological systems. Although, the pace of such collection has been exponential, the impact of these data remains small on many critical biomedical applications such as drug development. Limited resources, high costs, and low hit-to-lead ratio have led researchers to search for more cost effective methodologies. A possible alternative is to incorporate computational methods of potential drug target prediction early during drug discovery workflow. Computational methods based on systems approaches have the advantage of taking into account the global properties of a molecule not limited to its sequence, structure or function. Machine learning techniques are powerful tools that can extract relevant information from massive and noisy data sets. In recent years the scientific community has explored the combined power of these fields to propose increasingly accurate and low cost methods to propose interesting drug targets. In this mini-review, we describe promising approaches based on the simultaneous use of systems biology and machine learning to access gene and protein druggability. Moreover, we discuss the state-of-the-art of this emerging and interdisciplinary field, discussing data sources, algorithms and the performance of the different methodologies. Finally, we indicate interesting avenues of research and some remaining open challenges.",2015-12-01,10,1449,90,1985
1178,27110292,Machine-learning scoring functions to improve structure-based binding affinity prediction and virtual screening,"Docking tools to predict whether and how a small molecule binds to a target can be applied if a structural model of such target is available. The reliability of docking depends, however, on the accuracy of the adopted scoring function (SF). Despite intense research over the years, improving the accuracy of SFs for structure-based binding affinity prediction or virtual screening has proven to be a challenging task for any class of method. New SFs based on modern machine-learning regression models, which do not impose a predetermined functional form and thus are able to exploit effectively much larger amounts of experimental data, have recently been introduced. These machine-learning SFs have been shown to outperform a wide range of classical SFs at both binding affinity prediction and virtual screening. The emerging picture from these studies is that the classical approach of using linear regression with a small number of expert-selected structural features can be strongly improved by a machine-learning approach based on nonlinear regression allied with comprehensive data-driven feature selection. Furthermore, the performance of classical SFs does not grow with larger training datasets and hence this performance gap is expected to widen as more training data becomes available in the future. Other topics covered in this review include predicting the reliability of a SF on a particular target class, generating synthetic data to improve predictive performance and modeling guidelines for SF development. WIREs Comput Mol Sci 2015, 5:405-424. doi: 10.1002/wcms.1225 For further resources related to this article, please visit the WIREs website.",2015-12-01,47,1663,111,1985
1205,26834576,"Multivariate Analyses Applied to Healthy Neurodevelopment in Fetal, Neonatal, and Pediatric MRI","Multivariate analysis (MVA) is a class of statistical and pattern recognition techniques that involve the processing of data that contains multiple measurements per sample. MVA can be used to address a wide variety of neurological medical imaging related challenges including the evaluation of healthy brain development, the automated analysis of brain tissues and structures through image segmentation, evaluating the effects of genetic and environmental factors on brain development, evaluating sensory stimulation's relationship with functional brain activity and much more. Compared to adult imaging, pediatric, neonatal and fetal imaging have attracted less attention from MVA researchers, however, recent years have seen remarkable MVA research growth in pre-adult populations. This paper presents the results of a systematic review of the literature focusing on MVA applied to healthy subjects in fetal, neonatal and pediatric magnetic resonance imaging (MRI) of the brain. While the results of this review demonstrate considerable interest from the scientific community in applications of MVA technologies in brain MRI, the field is still young and significant research growth will continue into the future.",2016-01-01,3,1215,95,1954
1141,27491648,Deep Learning in Drug Discovery,"Artificial neural networks had their first heyday in molecular informatics and drug discovery approximately two decades ago. Currently, we are witnessing renewed interest in adapting advanced neural network architectures for pharmaceutical research by borrowing from the field of ""deep learning"". Compared with some of the other life sciences, their application in drug discovery is still limited. Here, we provide an overview of this emerging field of molecular informatics, present the basic concepts of prominent deep learning methods and offer motivation to explore these techniques for their usefulness in computer-assisted drug discovery and design. We specifically emphasize deep neural networks, restricted Boltzmann machine networks and convolutional networks.",2016-01-01,89,769,31,1954
1218,26674745,Standardized data collection to build prediction models in oncology: a prototype for rectal cancer,"The advances in diagnostic and treatment technology are responsible for a remarkable transformation in the internal medicine concept with the establishment of a new idea of personalized medicine. Inter- and intra-patient tumor heterogeneity and the clinical outcome and/or treatment's toxicity's complexity, justify the effort to develop predictive models from decision support systems. However, the number of evaluated variables coming from multiple disciplines: oncology, computer science, bioinformatics, statistics, genomics, imaging, among others could be very large thus making traditional statistical analysis difficult to exploit. Automated data-mining processes and machine learning approaches can be a solution to organize the massive amount of data, trying to unravel important interaction. The purpose of this paper is to describe the strategy to collect and analyze data properly for decision support and introduce the concept of an 'umbrella protocol' within the framework of 'rapid learning healthcare'.",2016-01-01,12,1018,98,1954
1663,25935161,A comprehensive comparative review of sequence-based predictors of DNA- and RNA-binding residues,"Motivated by the pressing need to characterize protein-DNA and protein-RNA interactions on large scale, we review a comprehensive set of 30 computational methods for high-throughput prediction of RNA- or DNA-binding residues from protein sequences. We summarize these predictors from several significant perspectives including their design, outputs and availability. We perform empirical assessment of methods that offer web servers using a new benchmark data set characterized by a more complete annotation that includes binding residues transferred from the same or similar proteins. We show that predictors of DNA-binding (RNA-binding) residues offer relatively strong predictive performance but they are unable to properly separate DNA- from RNA-binding residues. We design and empirically assess several types of consensuses and demonstrate that machine learning (ML)-based approaches provide improved predictive performance when compared with the individual predictors of DNA-binding residues or RNA-binding residues. We also formulate and execute first-of-its-kind study that targets combined prediction of DNA- and RNA-binding residues. We design and test three types of consensuses for this prediction and conclude that this novel approach that relies on ML design provides better predictive quality than individual predictors when tested on prediction of DNA- and RNA-binding residues individually. It also substantially improves discrimination between these two types of nucleic acids. Our results suggest that development of a new generation of predictors would benefit from using training data sets that combine both RNA- and DNA-binding proteins, designing new inputs that specifically target either DNA- or RNA-binding residues and pursuing combined prediction of DNA- and RNA-binding residues.",2016-01-01,24,1809,96,1954
1206,26819529,Diagnosing gastrointestinal illnesses using fecal headspace volatile organic compounds,"Volatile organic compounds (VOCs) emitted from stool are the components of the smell of stool representing the end products of microbial activity and metabolism that can be used to diagnose disease. Despite the abundance of hydrogen, carbon dioxide, and methane that have already been identified in human flatus, the small portion of trace gases making up the VOCs emitted from stool include organic acids, alcohols, esters, heterocyclic compounds, aldehydes, ketones, and alkanes, among others. These are the gases that vary among individuals in sickness and in health, in dietary changes, and in gut microbial activity. Electronic nose devices are analytical and pattern recognition platforms that can utilize mass spectrometry or electrochemical sensors to detect these VOCs in gas samples. When paired with machine-learning and pattern recognition algorithms, this can identify patterns of VOCs, and thus patterns of smell, that can be used to identify disease states. In this review, we provide a clinical background of VOC identification, electronic nose development, and review gastroenterology applications toward diagnosing disease by the volatile headspace analysis of stool.",2016-01-01,11,1185,86,1954
1220,26651007,Functional neuroimaging of psychotherapeutic processes in anxiety and depression: from mechanisms to predictions,"Purpose of review:                    The review provides an update of functional neuroimaging studies that identify neural processes underlying psychotherapy and predict outcomes following psychotherapeutic treatment in anxiety and depressive disorders. Following current developments in this field, studies were classified as 'mechanistic' or 'predictor' studies (i.e., informing neurobiological models about putative mechanisms versus aiming to provide predictive information).              Recent findings:                    Mechanistic evidence points toward a dual-process model of psychotherapy in anxiety disorders with abnormally increased limbic activation being decreased, while prefrontal activity is increased. Partly overlapping findings are reported for depression, albeit with a stronger focus on prefrontal activation following treatment. No studies directly comparing neural pathways of psychotherapy between anxiety and depression were detected. Consensus is accumulating for an overarching role of the anterior cingulate cortex in modulating treatment response across disorders. When aiming to quantify clinical utility, the need for single-subject predictions is increasingly recognized and predictions based on machine learning approaches show high translational potential.              Summary:                    Present findings encourage the search for predictors providing clinically meaningful information for single patients. However, independent validation as a crucial prerequisite for clinical use is still needed. Identifying nonresponders a priori creates the need for alternative treatment options that can be developed based on an improved understanding of those neural mechanisms underlying effective interventions.",2016-01-01,7,1753,112,1954
1195,26941766,Changing the Game: Using Integrative Genomics to Probe Virulence Mechanisms of the Stem Rust Pathogen Puccinia graminis f. sp. tritici,"The recent resurgence of wheat stem rust caused by new virulent races of Puccinia graminis f. sp. tritici (Pgt) poses a threat to food security. These concerns have catalyzed an extensive global effort toward controlling this disease. Substantial research and breeding programs target the identification and introduction of new stem rust resistance (Sr) genes in cultivars for genetic protection against the disease. Such resistance genes typically encode immune receptor proteins that recognize specific components of the pathogen, known as avirulence (Avr) proteins. A significant drawback to deploying cultivars with single Sr genes is that they are often overcome by evolution of the pathogen to escape recognition through alterations in Avr genes. Thus, a key element in achieving durable rust control is the deployment of multiple effective Sr genes in combination, either through conventional breeding or transgenic approaches, to minimize the risk of resistance breakdown. In this situation, evolution of pathogen virulence would require changes in multiple Avr genes in order to bypass recognition. However, choosing the optimal Sr gene combinations to deploy is a challenge that requires detailed knowledge of the pathogen Avr genes with which they interact and the virulence phenotypes of Pgt existing in nature. Identifying specific Avr genes from Pgt will provide screening tools to enhance pathogen virulence monitoring, assess heterozygosity and propensity for mutation in pathogen populations, and confirm individual Sr gene functions in crop varieties carrying multiple effective resistance genes. Toward this goal, much progress has been made in assembling a high quality reference genome sequence for Pgt, as well as a Pan-genome encompassing variation between multiple field isolates with diverse virulence spectra. In turn this has allowed prediction of Pgt effector gene candidates based on known features of Avr genes in other plant pathogens, including the related flax rust fungus. Upregulation of gene expression in haustoria and evidence for diversifying selection are two useful parameters to identify candidate Avr genes. Recently, we have also applied machine learning approaches to agnostically predict candidate effectors. Here, we review progress in stem rust pathogenomics and approaches currently underway to identify Avr genes recognized by wheat Sr genes.",2016-02-01,11,2392,134,1923
2806,25428267,Acoustic sequences in non-human animals: a tutorial review and prospectus,"Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise - let alone understand - the significance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be defined and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the fields of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, 'Analysing vocal sequences in animals'. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this field, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different fields, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality.",2016-02-01,33,2246,73,1923
1176,27114900,Computational analysis in epilepsy neuroimaging: A survey of features and methods,"Epilepsy affects 65 million people worldwide, a third of whom have seizures that are resistant to anti-epileptic medications. Some of these patients may be amenable to surgical therapy or treatment with implantable devices, but this usually requires delineation of discrete structural or functional lesion(s), which is challenging in a large percentage of these patients. Advances in neuroimaging and machine learning allow semi-automated detection of malformations of cortical development (MCDs), a common cause of drug resistant epilepsy. A frequently asked question in the field is what techniques currently exist to assist radiologists in identifying these lesions, especially subtle forms of MCDs such as focal cortical dysplasia (FCD) Type I and low grade glial tumors. Below we introduce some of the common lesions encountered in patients with epilepsy and the common imaging findings that radiologists look for in these patients. We then review and discuss the computational techniques introduced over the past 10 years for quantifying and automatically detecting these imaging findings. Due to large variations in the accuracy and implementation of these studies, specific techniques are traditionally used at individual centers, often guided by local expertise, as well as selection bias introduced by the varying prevalence of specific patient populations in different epilepsy centers. We discuss the need for a multi-institutional study that combines features from different imaging modalities as well as computational techniques to definitively assess the utility of specific automated approaches to epilepsy imaging. We conclude that sharing and comparing these different computational techniques through a common data platform provides an opportunity to rigorously test and compare the accuracy of these tools across different patient populations and geographical locations. We propose that these kinds of tools, quantitative imaging analysis methods and open data platforms for aggregating and sharing data and algorithms, can play a vital role in reducing the cost of care, the risks of invasive treatments, and improve overall outcomes for patients with epilepsy.",2016-02-01,15,2182,81,1923
1651,26031189,"The significance of Ciona intestinalis as a stem organism in integrative studies of functional evolution of the chordate endocrine, neuroendocrine, and nervous systems","Ascidians are the closest phylogenetic neighbors to vertebrates and are believed to conserve the evolutionary origin in chordates of the endocrine, neuroendocrine, and nervous systems involving neuropeptides and peptide hormones. Ciona intestinalis harbors various homologs or prototypes of vertebrate neuropeptides and peptide hormones including gonadotropin-releasing hormones (GnRHs), tachykinins (TKs), and calcitonin, as well as Ciona-specific neuropeptides such as Ciona vasopressin, LF, and YFV/L peptides. Moreover, molecular and functional studies on Ciona tachykinin (Ci-TK) have revealed the novel molecular mechanism of inducing oocyte growth via up-regulation of vitellogenesis-associated protease activity, which is expected to be conserved in vertebrates. Furthermore, a series of studies on Ciona GnRH receptor paralogs have verified the species-specific regulation of GnRHergic signaling including unique signaling control via heterodimerization among multiple GnRH receptors. These findings confirm the remarkable significance of ascidians in investigations of the evolutionary processes of the peptidergic systems in chordates, leading to the promising advance in the research on Ciona peptides in the next stage based on the recent development of emerging technologies including genome-editing techniques, peptidomics-based multi-color staining, machine-learning prediction, and next-generation sequencing. These technologies and bioinformatic integration of the resultant ""multi-omics"" data will provide unprecedented insights into the comprehensive understanding of molecular and functional regulatory mechanisms of the Ciona peptides, and will eventually enable the exploration of both conserved and diversified endocrine, neuroendocrine, and nervous systems in the evolutionary lineage of chordates.",2016-02-01,7,1823,167,1923
1194,26952574,Collective-Intelligence Recommender Systems: Advancing Computer Tailoring for Health Behavior Change Into the 21st Century,"Background:                    What is the next frontier for computer-tailored health communication (CTHC) research? In current CTHC systems, study designers who have expertise in behavioral theory and mapping theory into CTHC systems select the variables and develop the rules that specify how the content should be tailored, based on their knowledge of the targeted population, the literature, and health behavior theories. In collective-intelligence recommender systems (hereafter recommender systems) used by Web 2.0 companies (eg, Netflix and Amazon), machine learning algorithms combine user profiles and continuous feedback ratings of content (from themselves and other users) to empirically tailor content. Augmenting current theory-based CTHC with empirical recommender systems could be evaluated as the next frontier for CTHC.              Objective:                    The objective of our study was to uncover barriers and challenges to using recommender systems in health promotion.              Methods:                    We conducted a focused literature review, interviewed subject experts (n=8), and synthesized the results.              Results:                    We describe (1) limitations of current CTHC systems, (2) advantages of incorporating recommender systems to move CTHC forward, and (3) challenges to incorporating recommender systems into CTHC. Based on the evidence presented, we propose a future research agenda for CTHC systems.              Conclusions:                    We promote discussion of ways to move CTHC into the 21st century by incorporation of recommender systems.",2016-03-01,11,1615,122,1894
1644,26108231,A roadmap to multifactor dimensionality reduction methods,"Complex diseases are defined to be determined by multiple genetic and environmental factors alone as well as in interactions. To analyze interactions in genetic data, many statistical methods have been suggested, with most of them relying on statistical regression models. Given the known limitations of classical methods, approaches from the machine-learning community have also become attractive. From this latter family, a fast-growing collection of methods emerged that are based on the Multifactor Dimensionality Reduction (MDR) approach. Since its first introduction, MDR has enjoyed great popularity in applications and has been extended and modified multiple times. Based on a literature search, we here provide a systematic and comprehensive overview of these suggested methods. The methods are described in detail, and the availability of implementations is listed. Most recent approaches offer to deal with large-scale data sets and rare variants, which is why we expect these methods to even gain in popularity.",2016-03-01,18,1023,57,1894
1185,27014079,Predicting Essential Genes and Proteins Based on Machine Learning and Network Topological Features: A Comprehensive Review,"Essential proteins/genes are indispensable to the survival or reproduction of an organism, and the deletion of such essential proteins will result in lethality or infertility. The identification of essential genes is very important not only for understanding the minimal requirements for survival of an organism, but also for finding human disease genes and new drug targets. Experimental methods for identifying essential genes are costly, time-consuming, and laborious. With the accumulation of sequenced genomes data and high-throughput experimental data, many computational methods for identifying essential proteins are proposed, which are useful complements to experimental methods. In this review, we show the state-of-the-art methods for identifying essential genes and proteins based on machine learning and network topological features, point out the progress and limitations of current methods, and discuss the challenges and directions for further research.",2016-03-01,23,969,122,1894
1617,26358759,Bioinformatics approaches to single-cell analysis in developmental biology,"Individual cells within the same population show various degrees of heterogeneity, which may be better handled with single-cell analysis to address biological and clinical questions. Single-cell analysis is especially important in developmental biology as subtle spatial and temporal differences in cells have significant associations with cell fate decisions during differentiation and with the description of a particular state of a cell exhibiting an aberrant phenotype. Biotechnological advances, especially in the area of microfluidics, have led to a robust, massively parallel and multi-dimensional capturing, sorting, and lysis of single-cells and amplification of related macromolecules, which have enabled the use of imaging and omics techniques on single cells. There have been improvements in computational single-cell image analysis in developmental biology regarding feature extraction, segmentation, image enhancement and machine learning, handling limitations of optical resolution to gain new perspectives from the raw microscopy images. Omics approaches, such as transcriptomics, genomics and epigenomics, targeting gene and small RNA expression, single nucleotide and structural variations and methylation and histone modifications, rely heavily on high-throughput sequencing technologies. Although there are well-established bioinformatics methods for analysis of sequence data, there are limited bioinformatics approaches which address experimental design, sample size considerations, amplification bias, normalization, differential expression, coverage, clustering and classification issues, specifically applied at the single-cell level. In this review, we summarize biological and technological advancements, discuss challenges faced in the aforementioned data acquisition and analysis issues and present future prospects for application of single-cell analyses to developmental biology.",2016-03-01,5,1910,74,1894
1196,26936700,"Vision 20/20: Magnetic resonance imaging-guided attenuation correction in PET/MRI: Challenges, solutions, and opportunities","Attenuation correction is an essential component of the long chain of data correction techniques required to achieve the full potential of quantitative positron emission tomography (PET) imaging. The development of combined PET/magnetic resonance imaging (MRI) systems mandated the widespread interest in developing novel strategies for deriving accurate attenuation maps with the aim to improve the quantitative accuracy of these emerging hybrid imaging systems. The attenuation map in PET/MRI should ideally be derived from anatomical MR images; however, MRI intensities reflect proton density and relaxation time properties of biological tissues rather than their electron density and photon attenuation properties. Therefore, in contrast to PET/computed tomography, there is a lack of standardized global mapping between the intensities of MRI signal and linear attenuation coefficients at 511 keV. Moreover, in standard MRI sequences, bones and lung tissues do not produce measurable signals owing to their low proton density and short transverse relaxation times. MR images are also inevitably subject to artifacts that degrade their quality, thus compromising their applicability for the task of attenuation correction in PET/MRI. MRI-guided attenuation correction strategies can be classified in three broad categories: (i) segmentation-based approaches, (ii) atlas-registration and machine learning methods, and (iii) emission/transmission-based approaches. This paper summarizes past and current state-of-the-art developments and latest advances in PET/MRI attenuation correction. The advantages and drawbacks of each approach for addressing the challenges of MR-based attenuation correction are comprehensively described. The opportunities brought by both MRI and PET imaging modalities for deriving accurate attenuation maps and improving PET quantification will be elaborated. Future prospects and potential clinical applications of these techniques and their integration in commercial systems will also be discussed.",2016-03-01,30,2030,123,1894
1182,27053448,"Outcome modeling techniques for prostate cancer radiotherapy: Data, models, and validation","Prostate cancer is a frequently diagnosed malignancy worldwide and radiation therapy is a first-line approach in treating localized as well as locally advanced cases. The limiting factor in modern radiotherapy regimens is dose to normal structures, an excess of which can lead to aberrant radiation-induced toxicities. Conversely, dose reduction to spare adjacent normal structures risks underdosing target volumes and compromising local control. As a result, efforts aimed at predicting the effects of radiotherapy could invaluably optimize patient treatments by mitigating such toxicities and simultaneously maximizing biochemical control. In this work, we review the types of data, frameworks and techniques used for prostate radiotherapy outcome modeling. Consideration is given to clinical and dose-volume metrics, such as those amassed by the QUANTEC initiative, and also to newer methods for the integration of biological and genetic factors to improve prediction performance. We furthermore highlight trends in machine learning that may help to elucidate the complex pathophysiological mechanisms of tumor control and radiation-induced normal tissue side effects.",2016-03-01,3,1171,90,1894
1187,27011184,"MicroRNAs as Biomarkers for Diagnosis, Prognosis and Theranostics in Prostate Cancer","Prostate cancer (PC) includes several phenotypes, from indolent to highly aggressive cancer. Actual diagnostic and prognostic tools have several limitations, and there is a need for new biomarkers to stratify patients and assign them optimal therapies by taking into account potential genetic and epigenetic differences. MicroRNAs (miRNAs) are small sequences of non-coding RNA regulating specific genes involved in the onset and development of PC. Stable miRNAs have been found in biofluids, such as serum and plasma; thus, the measurement of PC-associated miRNAs is emerging as a non-invasive tool for PC detection and monitoring. In this study, we conduct an in-depth literature review focusing on miRNAs that may contribute to the diagnosis and prognosis of PC. The role of miRNAs as a potential theranostic tool in PC is discussed. Using a meta-analysis approach, we found a group of 29 miRNAs with diagnostic properties and a group of seven miRNAs with prognostic properties, which were found already expressed in both biofluids and PC tissues. We tested the two miRNA groups on The Cancer Genome Atlas dataset of PC tissue samples with a machine-learning approach. Our results suggest that these 29 miRNAs should be considered as potential panel of biomarkers for the diagnosis of PC, both as in vivo non-invasive test and ex vivo confirmation test.",2016-03-01,46,1356,84,1894
1607,26484733,Building cell models and simulations from microscope images,"The use of fluorescence microscopy has undergone a major revolution over the past twenty years, both with the development of dramatic new technologies and with the widespread adoption of image analysis and machine learning methods. Many open source software tools provide the ability to use these methods in a wide range of studies, and many molecular and cellular phenotypes can now be automatically distinguished. This article presents the next major challenge in microscopy automation, the creation of accurate models of cell organization directly from images, and reviews the progress that has been made towards this challenge.",2016-03-01,7,631,59,1894
1191,26995379,Rationale and methodology of a collaborative learning project in congenital cardiac care,"Background:                    Collaborative learning is a technique through which individuals or teams learn together by capitalizing on one another's knowledge, skills, resources, experience, and ideas. Clinicians providing congenital cardiac care may benefit from collaborative learning given the complexity of the patient population and team approach to patient care.              Rationale and development:                    Industrial system engineers first performed broad-based time-motion and process analyses of congenital cardiac care programs at 5 Pediatric Heart Network core centers. Rotating multidisciplinary team site visits to each center were completed to facilitate deep learning and information exchange. Through monthly conference calls and an in-person meeting, we determined that duration of mechanical ventilation following infant cardiac surgery was one key variation that could impact a number of clinical outcomes. This was underscored by one participating center's practice of early extubation in the majority of its patients. A consensus clinical practice guideline using collaborative learning was developed and implemented by multidisciplinary teams from the same 5 centers. The 1-year prospective initiative was completed in May 2015, and data analysis is under way.              Conclusion:                    Collaborative learning that uses multidisciplinary team site visits and information sharing allows for rapid structured fact-finding and dissemination of expertise among institutions. System modeling and machine learning approaches objectively identify and prioritize focused areas for guideline development. The collaborative learning framework can potentially be applied to other components of congenital cardiac care and provide a complement to randomized clinical trials as a method to rapidly inform and improve the care of children with congenital heart disease.",2016-04-01,13,1913,88,1863
1174,27130797,IBM Watson: How Cognitive Computing Can Be Applied to Big Data Challenges in Life Sciences Research,"Life sciences researchers are under pressure to innovate faster than ever. Big data offer the promise of unlocking novel insights and accelerating breakthroughs. Ironically, although more data are available than ever, only a fraction is being integrated, understood, and analyzed. The challenge lies in harnessing volumes of data, integrating the data from hundreds of sources, and understanding their various formats. New technologies such as cognitive computing offer promise for addressing this challenge because cognitive solutions are specifically designed to integrate and analyze big datasets. Cognitive solutions can understand different types of data such as lab values in a structured database or the text of a scientific publication. Cognitive solutions are trained to understand technical, industry-specific content and use advanced reasoning, predictive modeling, and machine learning techniques to advance research faster. Watson, a cognitive computing technology, has been configured to support life sciences research. This version of Watson includes medical literature, patents, genomics, and chemical and pharmacological data that researchers would typically use in their work. Watson has also been developed with specific comprehension of scientific terminology so it can make novel connections in millions of pages of text. Watson has been applied to a few pilot studies in the areas of drug target identification and drug repurposing. The pilot results suggest that Watson can accelerate identification of novel drug candidates and novel drug targets by harnessing the potential of big data.",2016-04-01,43,1611,99,1863
1209,26800334,Prototype-based models in machine learning,"An overview is given of prototype-based models in machine learning. In this framework, observations, i.e., data, are stored in terms of typical representatives. Together with a suitable measure of similarity, the systems can be employed in the context of unsupervised and supervised analysis of potentially high-dimensional, complex datasets. We discuss basic schemes of competitive vector quantization as well as the so-called neural gas approach and Kohonen's topology-preserving self-organizing map. Supervised learning in prototype systems is exemplified in terms of learning vector quantization. Most frequently, the familiar Euclidean distance serves as a dissimilarity measure. We present extensions of the framework to nonstandard measures and give an introduction to the use of adaptive distances in relevance learning.",2016-04-01,3,828,42,1863
1202,26868042,"Expanding perspectives on cognition in humans, animals, and machines","Over the past decade neuroscience has been attacking the problem of cognition with increasing vigor. Yet, what exactly is cognition, beyond a general signifier of anything seemingly complex the brain does? Here, we briefly review attempts to define, describe, explain, build, enhance and experience cognition. We highlight perspectives including psychology, molecular biology, computation, dynamical systems, machine learning, behavior and phenomenology. This survey of the landscape reveals not a clear target for explanation but a pluralistic and evolving scene with diverse opportunities for grounding future research. We argue that rather than getting to the bottom of it, over the next century, by deconstructing and redefining cognition, neuroscience will and should expand rather than merely reduce our concept of the mind.",2016-04-01,4,830,68,1863
1200,26898163,Painful Issues in Pain Prediction,"How perception of pain emerges from neural activity is largely unknown. Identifying a neural 'pain signature' and deriving a way to predict perceived pain from brain activity would have enormous basic and clinical implications. Researchers are increasingly turning to functional brain imaging, often applying machine-learning algorithms to infer that pain perception occurred. Yet, such sophisticated analyses are fraught with interpretive difficulties. Here, we highlight some common and troublesome problems in the literature, and suggest methods to ensure researchers draw accurate conclusions from their results. Since functional brain imaging is increasingly finding practical applications with real-world consequences, it is critical to interpret brain scans accurately, because decisions based on neural data will only be as good as the science behind them.",2016-04-01,25,864,33,1863
1203,26867072,A Biobehavioral Framework to Address the Emerging Challenge of Multimorbidity,"Multimorbidity, the co-occurrence of multiple physical or psychological illnesses, is prevalent particularly among older adults. The number of Americans with multiple chronic diseases is projected to increase from 57 million in 2000 to 81 million in 2020. However, behavioral medicine and health psychology, while focusing on the co-occurrence of psychological/psychiatric disorders with primary medical morbidities, have historically tended to ignore the co-occurrence of primary medical comorbidities, such as diabetes and cancer, and their biopsychosocial implications. This approach may hinder our ecologically valid understanding of the etiology, prevention, and treatment for individual patients with multimorbidity. In this selective review, we propose a heuristic behavioral framework for the etiology of multimorbidity. More acknowledgment and systematic research on multiple, co-existing disorders in behavioral medicine are consistent with the biopsychosocial model's emphasis on treating the ""whole person,"" which means not considering any single illness, its symptoms, risk factors, or mechanisms, in isolation. As systems analytics, big data, machine learning, and mixed-model trajectory analyses, among others, come online and become more widely available, we may be able to tackle multimorbidity more holistically, efficiently, and satisfactorily.",2016-04-01,10,1363,77,1863
1192,26979668,On the convergence of nanotechnology and Big Data analysis for computer-aided diagnosis,"An overview is provided of the challenges involved in building computer-aided diagnosis systems capable of precise medical diagnostics based on integration and interpretation of data from different sources and formats. The availability of massive amounts of data and computational methods associated with the Big Data paradigm has brought hope that such systems may soon be available in routine clinical practices, which is not the case today. We focus on visual and machine learning analysis of medical data acquired with varied nanotech-based techniques and on methods for Big Data infrastructure. Because diagnosis is essentially a classification task, we address the machine learning techniques with supervised and unsupervised classification, making a critical assessment of the progress already made in the medical field and the prospects for the near future. We also advocate that successful computer-aided diagnosis requires a merge of methods and concepts from nanotechnology and Big Data analysis.",2016-04-01,4,1007,87,1863
1610,26450107,Surgical robotics beyond enhanced dexterity instrumentation: a survey of machine learning techniques and their role in intelligent and autonomous surgical actions,"Purpose:                    Advances in technology and computing play an increasingly important role in the evolution of modern surgical techniques and paradigms. This article reviews the current role of machine learning (ML) techniques in the context of surgery with a focus on surgical robotics (SR). Also, we provide a perspective on the future possibilities for enhancing the effectiveness of procedures by integrating ML in the operating room.              Methods:                    The review is focused on ML techniques directly applied to surgery, surgical robotics, surgical training and assessment. The widespread use of ML methods in diagnosis and medical image computing is beyond the scope of the review. Searches were performed on PubMed and IEEE Explore using combinations of keywords: ML, surgery, robotics, surgical and medical robotics, skill learning, skill analysis and learning to perceive.              Results:                    Studies making use of ML methods in the context of surgery are increasingly being reported. In particular, there is an increasing interest in using ML for developing tools to understand and model surgical skill and competence or to extract surgical workflow. Many researchers begin to integrate this understanding into the control of recent surgical robots and devices.              Conclusion:                    ML is an expanding field. It is popular as it allows efficient processing of vast amounts of data for interpreting and real-time decision making. Already widely used in imaging and diagnosis, it is believed that ML will also play an important role in surgery and interventional treatments. In particular, ML could become a game changer into the conception of cognitive surgical robots. Such robots endowed with cognitive skills would assist the surgical team also on a cognitive level, such as possibly lowering the mental load of the team. For example, ML could help extracting surgical skill, learned through demonstration by human experts, and could transfer this to robotic skills. Such intelligent surgical assistance would significantly surpass the state of the art in surgical robotics. Current devices possess no intelligence whatsoever and are merely advanced and expensive instruments.",2016-04-01,13,2264,162,1863
1170,27171499,Discovery and Optimization of Materials Using Evolutionary Approaches,"Materials science is undergoing a revolution, generating valuable new materials such as flexible solar panels, biomaterials and printable tissues, new catalysts, polymers, and porous materials with unprecedented properties. However, the number of potentially accessible materials is immense. Artificial evolutionary methods such as genetic algorithms, which explore large, complex search spaces very efficiently, can be applied to the identification and optimization of novel materials more rapidly than by physical experiments alone. Machine learning models can augment experimental measurements of materials fitness to accelerate identification of useful and novel materials in vast materials composition or property spaces. This review discusses the problems of large materials spaces, the types of evolutionary algorithms employed to identify or optimize materials, and how materials can be represented mathematically as genomes, describes fitness landscapes and mutation operators commonly employed in materials evolution, and provides a comprehensive summary of published research on the use of evolutionary methods to generate new catalysts, phosphors, and a range of other materials. The review identifies the potential for evolutionary methods to revolutionize a wide range of manufacturing, medical, and materials based industries.",2016-05-01,21,1341,69,1833
1175,27119951,Big Data and Machine Learning in Plastic Surgery: A New Frontier in Surgical Innovation,"Medical decision-making is increasingly based on quantifiable data. From the moment patients come into contact with the health care system, their entire medical history is recorded electronically. Whether a patient is in the operating room or on the hospital ward, technological advancement has facilitated the expedient and reliable measurement of clinically relevant health metrics, all in an effort to guide care and ensure the best possible clinical outcomes. However, as the volume and complexity of biomedical data grow, it becomes challenging to effectively process ""big data"" using conventional techniques. Physicians and scientists must be prepared to look beyond classic methods of data processing to extract clinically relevant information. The purpose of this article is to introduce the modern plastic surgeon to machine learning and computational interpretation of large data sets. What is machine learning? Machine learning, a subfield of artificial intelligence, can address clinically relevant problems in several domains of plastic surgery, including burn surgery; microsurgery; and craniofacial, peripheral nerve, and aesthetic surgery. This article provides a brief introduction to current research and suggests future projects that will allow plastic surgeons to explore this new frontier of surgical science.",2016-05-01,18,1330,87,1833
1168,27213397,Contextualising Water Use in Residential Settings: A Survey of Non-Intrusive Techniques and Approaches,"Water monitoring in households is important to ensure the sustainability of fresh water reserves on our planet. It provides stakeholders with the statistics required to formulate optimal strategies in residential water management. However, this should not be prohibitive and appliance-level water monitoring cannot practically be achieved by deploying sensors on every faucet or water-consuming device of interest due to the higher hardware costs and complexity, not to mention the risk of accidental leakages that can derive from the extra plumbing needed. Machine learning and data mining techniques are promising techniques to analyse monitored data to obtain non-intrusive water usage disaggregation. This is because they can discern water usage from the aggregated data acquired from a single point of observation. This paper provides an overview of water usage disaggregation systems and related techniques adopted for water event classification. The state-of-the art of algorithms and testbeds used for fixture recognition are reviewed and a discussion on the prominent challenges and future research are also included.",2016-05-01,1,1126,102,1833
1148,27440790,"Digital technology in respiratory diseases: Promises, (no) panacea and time for a new paradigm","In a world where digital technology has revolutionized the way we work, shop and manage our finances it is unsurprising that digital systems are suggested as potential solutions to delivering clinically and cost-effective care for an aging population with one or more long-term conditions. However, recent evidence suggesting that telehealth may not be quite the panacea that was promised, has led to discussions on the mechanisms and role of digital technology in respiratory care. Implementation in rural and remote settings offers significant benefits in terms of convenient access to care, but is contingent on technical and organizational infrastructure. Telemonitoring systems rely on algorithms to detect deterioration and trigger alerts; machine learning may enable telemonitoring of the future to develop personalized systems that are sensitive to clinical status whilst reducing false alerts. By providing access to information, offering convenient and flexible modes of communication and enabling the transfer of monitoring data to support professional assessment, telehealth can support self-management. At present, all too often, expensive 'off the shelf' systems are purchased and given to clinicians to use. It is time for the paradigm to shift. As clinicians we should identify the specific challenges we face in delivering care, and expect flexible systems that can be customized to individual patients' requirements and adapted to our diverse healthcare contexts.",2016-05-01,4,1481,94,1833
1184,27017830,Computational Analysis and Simulation of Empathic Behaviors: a Survey of Empathy Modeling with Behavioral Signal Processing Framework,"Empathy is an important psychological process that facilitates human communication and interaction. Enhancement of empathy has profound significance in a range of applications. In this paper, we review emerging directions of research on computational analysis of empathy expression and perception as well as empathic interactions, including their simulation. We summarize the work on empathic expression analysis by the targeted signal modalities (e.g., text, audio, and facial expressions). We categorize empathy simulation studies into theory-based emotion space modeling or application-driven user and context modeling. We summarize challenges in computational study of empathy including conceptual framing and understanding of empathy, data availability, appropriate use and validation of machine learning techniques, and behavior signal processing. Finally, we propose a unified view of empathy computation and offer a series of open problems for future research.",2016-05-01,0,968,133,1833
1147,27446888,Pre-Adult MRI of Brain Cancer and Neurological Injury: Multivariate Analyses,"Brain cancer and neurological injuries, such as stroke, are life-threatening conditions for which further research is needed to overcome the many challenges associated with providing optimal patient care. Multivariate analysis (MVA) is a class of pattern recognition technique involving the processing of data that contains multiple measurements per sample. MVA can be used to address a wide variety of neuroimaging challenges, including identifying variables associated with patient outcomes; understanding an injury's etiology, development, and progression; creating diagnostic tests; assisting in treatment monitoring; and more. Compared to adults, imaging of the developing brain has attracted less attention from MVA researchers, however, remarkable MVA growth has occurred in recent years. This paper presents the results of a systematic review of the literature focusing on MVA technologies applied to brain injury and cancer in neurological fetal, neonatal, and pediatric magnetic resonance imaging (MRI). With a wide variety of MRI modalities providing physiologically meaningful biomarkers and new biomarker measurements constantly under development, MVA techniques hold enormous potential toward combining available measurements toward improving basic research and the creation of technologies that contribute to improving patient care.",2016-06-01,2,1347,76,1802
1155,27379211,Big Data Analytics for Prostate Radiotherapy,"Radiation therapy is a first-line treatment option for localized prostate cancer and radiation-induced normal tissue damage are often the main limiting factor for modern radiotherapy regimens. Conversely, under-dosing of target volumes in an attempt to spare adjacent healthy tissues limits the likelihood of achieving local, long-term control. Thus, the ability to generate personalized data-driven risk profiles for radiotherapy outcomes would provide valuable prognostic information to help guide both clinicians and patients alike. Big data applied to radiation oncology promises to deliver better understanding of outcomes by harvesting and integrating heterogeneous data types, including patient-specific clinical parameters, treatment-related dose-volume metrics, and biological risk factors. When taken together, such variables make up the basis for a multi-dimensional space (the ""RadoncSpace"") in which the presented modeling techniques search in order to identify significant predictors. Herein, we review outcome modeling and big data-mining techniques for both tumor control and radiotherapy-induced normal tissue effects. We apply many of the presented modeling approaches onto a cohort of hypofractionated prostate cancer patients taking into account different data types and a large heterogeneous mix of physical and biological parameters. Cross-validation techniques are also reviewed for the refinement of the proposed framework architecture and checking individual model performance. We conclude by considering advanced modeling techniques that borrow concepts from big data analytics, such as machine learning and artificial intelligence, before discussing the potential future impact of systems radiobiology approaches.",2016-06-01,13,1740,44,1802
1603,26511342,"Computer-Aided Optimization of Combined Anti-Retroviral Therapy for HIV: New Drugs, New Drug Targets and Drug Resistance","Background:                    Resistance to antiretroviral drugs is a complex and evolving area with relevant implications in the treatment of human immunodeficiency virus (HIV) infection. Several rules, algorithms and full-fledged computer programs have been developed to assist the HIV specialist in the choice of the best patient-tailored therapy.              Methods:                    Experts' rules and statistical/machine learning algorithms for interpreting HIV drug resistance, along with their program implementations, were retrieved from PubMed and other on-line resources to be critically reviewed in terms of technical approach, performance, usability, update, and evolution (i.e. inclusion of novel drugs or expansion to other viral agents).              Results:                    Several drug resistance prediction algorithms for the nucleotide/nucleoside/non-nucleoside reverse transcriptase, protease and integrase inhibitors as well as coreceptor antagonists are currently available, routinely used, and have been validated thoroughly in independent studies. Computer tools that combine single-drug genotypic/phenotypic resistance interpretation and optimize combination antiretroviral therapy have been also developed and implemented as web applications. Most of the systems have been updated timely to incorporate new drugs and few have recently been expanded to meet a similar need in the Hepatitis C area. Prototype systems aiming at predicting virological response from both virus and patient indicators have been recently developed but they are not yet being routinely used.              Conclusion:                    Computing HIV genotype to predict drug susceptibility in vitro or response to combination antiretroviral therapy in vivo is a continuous and productive research field, translating into successful treatment decision support tools, an essential component of the management of HIV patients.",2016-06-01,5,1935,120,1802
2736,28042575,Long Noncoding RNA Identification: Comparing Machine Learning Based Tools for Long Noncoding Transcripts Discrimination,"Long noncoding RNA (lncRNA) is a kind of noncoding RNA with length more than 200 nucleotides, which aroused interest of people in recent years. Lots of studies have confirmed that human genome contains many thousands of lncRNAs which exert great influence over some critical regulators of cellular process. With the advent of high-throughput sequencing technologies, a great quantity of sequences is waiting for exploitation. Thus, many programs are developed to distinguish differences between coding and long noncoding transcripts. Different programs are generally designed to be utilised under different circumstances and it is sensible and practical to select an appropriate method according to a certain situation. In this review, several popular methods and their advantages, disadvantages, and application scopes are summarised to assist people in employing a suitable method and obtaining a more reliable result.",2016-06-01,13,920,119,1802
1598,26567735,Frontiers for the Early Diagnosis of AD by Means of MRI Brain Imaging and Support Vector Machines,"The emergence of Alzheimer's Disease (AD) as a consequence of increasing aging population makes urgent the availability of methods for the early and accurate diagnosis. Magnetic Resonance Imaging (MRI) could be used as in vivo, non invasive tool to identify sensitive and specific markers of very early AD progression. In recent years, multivariate pattern analysis (MVPA) and machine- learning algorithms have attracted strong interest within the neuroimaging community, as they allow automatic classification of imaging data with higher performance than univariate statistical analysis. An exhaustive search of PubMed, Web of Science and Medline records was performed in this work, in order to retrieve studies focused on the potential role of MRI in aiding the clinician in early diagnosis of AD by using Support Vector Machines (SVMs) as MVPA automated classification method. A total of 30 studies emerged, published from 2008 to date. This review aims to give a state-of-the-art overview about SVM for the early and differential diagnosis of AD-related pathologies by means of MRI data, starting from preliminary steps such as image pre-processing, feature extraction and feature selection, and ending with classification, validation strategies and extraction of MRI-related biomarkers. The main advantages and drawbacks of the different techniques were explored. Results obtained by the reviewed studies were reported in terms of classification performance and biomarker outcomes, in order to shed light on the parameters that accompany normal and pathological aging. Unresolved issues and possible future directions were finally pointed out.",2016-06-01,23,1648,97,1802
1197,26924521,Recent advances in quantitative high throughput and high content data analysis,"Introduction:                    High throughput screening has become a basic technique with which to explore biological systems. Advances in technology, including increased screening capacity, as well as methods that generate multiparametric readouts, are driving the need for improvements in the analysis of data sets derived from such screens.              Areas covered:                    This article covers the recent advances in the analysis of high throughput screening data sets from arrayed samples, as well as the recent advances in the analysis of cell-by-cell data sets derived from image or flow cytometry application. Screening multiple genomic reagents targeting any given gene creates additional challenges and so methods that prioritize individual gene targets have been developed. The article reviews many of the open source data analysis methods that are now available and which are helping to define a consensus on the best practices to use when analyzing screening data.              Expert opinion:                    As data sets become larger, and more complex, the need for easily accessible data analysis tools will continue to grow. The presentation of such complex data sets, to facilitate quality control monitoring and interpretation of the results will require the development of novel visualizations. In addition, advanced statistical and machine learning algorithms that can help identify patterns, correlations and the best features in massive data sets will be required. The ease of use for these tools will be important, as they will need to be used iteratively by laboratory scientists to improve the outcomes of complex analyses.",2016-06-01,1,1669,78,1802
2768,27800282,Digital Suicide Prevention: Can Technology Become a Game-changer?,"Suicide continues to be a leading cause of death and has been recognized as a significant public health issue. Rapid advances in data science can provide us with useful tools for suicide prevention, and help to dynamically assess suicide risk in quantitative data-driven ways. In this article, the authors highlight the most current international research in digital suicide prevention, including the use of machine learning, smartphone applications, and wearable sensor driven systems. The authors also discuss future opportunities for digital suicide prevention, and propose a novel Sensor-driven Mental State Assessment System.",2016-06-01,10,630,65,1802
2770,27784247,Current Trends in Drug Sensitivity Prediction,"Cancer cell line panels have proved useful disease models to, among others, identify genomic markers of drug sensitivity and to develop new anticancer drugs. The increasing availability of in vitro sensitivity and cell line profiling data sets raises the question of whether this information could be used, and to which extent, to predict the activity of drugs in cancer cell lines and, ultimately, in patients tumors. Drug sensitivity prediction embraces those approaches aiming at predicting in vitro drug activity on cancer cell lines by integrating genomic and/or chemical information using machine learning models. In this review, we summarize the cytotoxicity assays generally used to determine in vitro activity on cultured cell lines, and revisit the drug sensitivity prediction studies that have leveraged chemical and cell line profiling data from the NCI60, Cancer Cell Line Encyclopedia (CCLE) and Genomics of Drug Sensitivity in Cancer (GDSC) projects. A section outlining current limitations and future perspectives in the field closes the review.",2016-06-01,2,1061,45,1802
1592,26626626,Linking Essential Tremor to the Cerebellum-Neuroimaging Evidence,"Essential tremor (ET) is the most common pathological tremor disorder in the world, and post-mortem evidence has shown that the cerebellum is the most consistent area of pathology in ET. In the last few years, advanced neuroimaging has tried to confirm this evidence. The aim of the present review is to discuss to what extent the evidence provided by this field of study may be generalised. We performed a systematic literature search combining the terms ET with the following keywords: MRI, VBM, MRS, DTI, fMRI, PET and SPECT. We summarised and discussed each study and placed the results in the context of existing knowledge regarding the cerebellar involvement in ET. A total of 51 neuroimaging studies met our search criteria, roughly divided into 19 structural and 32 functional studies. Despite clinical and methodological differences, both functional and structural imaging studies showed similar findings but without defining a clear topography of neurodegeneration. Indeed, the vast majority of studies found functional and structural abnormalities in several parts of the anterior and posterior cerebellar lobules, but it remains to be established to what degree these neural changes contribute to clinical symptoms of ET. Currently, advanced neuroimaging has confirmed the involvement of the cerebellum in pathophysiological processes of ET, although a high variability in results persists. For this reason, the translation of this knowledge into daily clinical practice is again partially limited, although new advanced multivariate neuroimaging approaches (machine-learning) are proving interesting changes of perspective.",2016-06-01,29,1636,64,1802
1622,26286210,Prediction and Dissection of Protein-RNA Interactions by Molecular Descriptors,"Protein-RNA interactions play crucial roles in numerous biological processes. However, detecting the interactions and binding sites between protein and RNA by traditional experiments is still time consuming and labor costing. Thus, it is of importance to develop bioinformatics methods for predicting protein-RNA interactions and binding sites. Accurate prediction of protein-RNA interactions and recognitions will highly benefit to decipher the interaction mechanisms between protein and RNA, as well as to improve the RNA-related protein engineering and drug design. In this work, we summarize the current bioinformatics strategies of predicting protein-RNA interactions and dissecting protein-RNA interaction mechanisms from local structure binding motifs. In particular, we focus on the feature-based machine learning methods, in which the molecular descriptors of protein and RNA are extracted and integrated as feature vectors of representing the interaction events and recognition residues. In addition, the available methods are classified and compared comprehensively. The molecular descriptors are expected to elucidate the binding mechanisms of protein-RNA interaction and reveal the functional implications from structural complementary perspective.",2016-06-01,1,1261,78,1802
2723,28132481,Tackling the problem of HIV drug resistance,"The virally-encoded HIV-1 protease is an effective target for antiviral drugs, however, treatment for HIV infections is limited by the prevalence of drug resistant viral mutants. In this review, we describe our three-pronged approach to analyze and combat drug resistance. Understanding the molecular basis for resistance due to protease inhibitors is a key initial step in this approach. This knowledge is being employed for the design of new, improved inhibitors with high affinity for resistant mutants as well as wild type enzyme. In parallel with experimental studies of diverse mutants and inhibitory compounds, we are developing efficient algorithms to predict drug resistance phenotype from genotype data. This approach has important practical applications in the clinic where genotyping is recommended for individuals with new infections.",2016-06-01,3,847,43,1802
1204,26846174,Models and Data Sources Used in Systems Medicine. A Systematic Literature Review,"Background:                    Systems medicine is a new approach for the development and selection of treatment strategies for patients with complex diseases. It is often referred to as the application of systems biology methods for decision making in patient care. For systems medicine computer applications, many different data sources have to be integrated and included into models. This is a challenging task for Medical Informatics since the approach exceeds traditional systems like Electronic Health Records. To prioritize research activities for systems medicine applications, it is necessary to get an overview over modelling methods and data sources already used in this field.              Objectives:                    We performed a systematic literature review with the objective to capture current use of 1) modelling methods and 2) data sources in systems medicine related research projects.              Methods:                    We queried the MEDLINE and ScienceDirect databases for papers associated with the search term systems medicine and related terms. Papers were screened and assessed in full text in a two-step process according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement guidelines.              Results:                    The queries returned 698 articles of which 34 papers were finally included into the study. A multitude of modelling approaches such as machine learning and network analysis was identified and classified. Since these approaches are also used in other domains, no methods specific for systems medicine could be identified. Omics data are the most widely used data types followed by clinical data. Most studies only include a rather limited number of data sources.              Conclusions:                    Currently, many different modelling approaches are used in systems medicine. Thus, highly flexible modular solutions are necessary for systems medicine clinical applications. However, the number of data sources included into the models is limited and most projects currently focus on prognosis. To leverage the potential of systems medicine further, it will be necessary to focus on treatment strategies for patients and consider a broader range of data.",2016-06-01,3,2267,80,1802
1217,26677181,Systems Medicine in Pharmaceutical Research and Development,"The development of new drug therapies requires substantial and ever increasing investments from the pharmaceutical company. Ten years ago, the average time from early target identification and optimization until initial market authorization of a new drug compound took more than 10 years and involved costs in the order of one billion US dollars. Recent studies indicate even a significant growth of costs in the meanwhile, mainly driven by the increasing complexity of diseases addressed by pharmaceutical research.Modeling and simulation are proven approaches to handle highly complex systems; hence, systems medicine is expected to control the spiral of complexity of diseases and increasing costs. Today, the main focus of systems medicine applications in industry is on mechanistic modeling. Biological mechanisms are represented by explicit equations enabling insight into the cooperation of all relevant mechanisms. Mechanistic modeling is widely accepted in pharmacokinetics, but prediction from cell behavior to patients is rarely possible due to lacks in our understanding of the controlling mechanisms. Data-driven modeling aims to compensate these lacks by the use of advanced statistical and machine learning methods. Future progress in pharmaceutical research and development will require integrated hybrid modeling technologies allowing realization of the benefits of both mechanistic and data-driven modeling. In this chapter, we sketch typical industrial application areas for both modeling techniques and derive the requirements for future technology development.",2016-06-01,0,1581,59,1802
1135,27525223,Computerized techniques pave the way for drug-drug interaction prediction and interpretation,"Introduction:                    Health care industry also patients penalized by medical errors that are inevitable but highly preventable. Vast majority of medical errors are related to adverse drug reactions, while drug-drug interactions (DDIs) are the main cause of adverse drug reactions (ADRs). DDIs and ADRs have mainly been reported by haphazard case studies. Experimental in vivo and in vitro researches also reveals DDI pairs. Laboratory and experimental researches are valuable but also expensive and in some cases researchers may suffer from limitations.              Methods:                    In the current investigation, the latest published works were studied to analyze the trend and pattern of the DDI modelling and the impacts of machine learning methods. Applications of computerized techniques were also investigated for the prediction and interpretation of DDIs.              Results:                    Computerized data-mining in pharmaceutical sciences and related databases provide new key transformative paradigms that can revolutionize the treatment of diseases and hence medical care. Given that various aspects of drug discovery and pharmacotherapy are closely related to the clinical and molecular/biological information, the scientifically sound databases (e.g., DDIs, ADRs) can be of importance for the success of pharmacotherapy modalities.              Conclusion:                    A better understanding of DDIs not only provides a robust means for designing more effective medicines but also grantees patient safety.",2016-06-01,3,1556,92,1802
1169,27207370,Bioimage Informatics for Big Data,"Bioimage informatics is a field wherein high-throughput image informatics methods are used to solve challenging scientific problems related to biology and medicine. When the image datasets become larger and more complicated, many conventional image analysis approaches are no longer applicable. Here, we discuss two critical challenges of large-scale bioimage informatics applications, namely, data accessibility and adaptive data analysis. We highlight case studies to show that these challenges can be tackled based on distributed image computing as well as machine learning of image examples in a multidimensional environment.",2016-06-01,5,629,33,1802
1189,26996942,Current Approaches in Computational Drug Resistance Prediction in HIV,"Background:                    Today a broad range of antiretroviral drug regimens are applicable for the successful suppression of virus replication in human immunodeficiency virus (HIV) infected people. However, there still remains an obstacle in therapy: the high mutation rate of the HI virus under drug pressure leads to resistant variants causing failure of permanent and effective treatment. Therefore, resistance testing is therefore inevitable to administer appropriate antiviral drugs to infected patients.              Methods:                    By means of current high-throughput sequencing technologies, computational models have recently constituted important assistance in drug resistance prediction and can guide the choice of medical treatment. Several machine learning algorithms, e.g. support-vector machines, random forests, as well as statistical methods have been already applied to genotypic data and structural information to predict drug resistance.              Results:                    In this review, we provide an overview of existing approaches in computational drug resistance prediction in HIV. We further highlight the challenges and limitations of current methods, e.g. time complexity and prediction of non-B subtypes.              Conclusion:                    Moreover, we give a perspective on multi-label and multi-instance classification techniques that potentially tackle the problem of cross-resistances among drugs.",2016-06-01,6,1464,69,1802
1216,26689499,Providing data science support for systems pharmacology and its implications to drug discovery,"Introduction:                    The conventional one-drug-one-target-one-disease drug discovery process has been less successful in tracking multi-genic, multi-faceted complex diseases. Systems pharmacology has emerged as a new discipline to tackle the current challenges in drug discovery. The goal of systems pharmacology is to transform huge, heterogeneous, and dynamic biological and clinical data into interpretable and actionable mechanistic models for decision making in drug discovery and patient treatment. Thus, big data technology and data science will play an essential role in systems pharmacology.              Areas covered:                    This paper critically reviews the impact of three fundamental concepts of data science on systems pharmacology: similarity inference, overfitting avoidance, and disentangling causality from correlation. The authors then discuss recent advances and future directions in applying the three concepts of data science to drug discovery, with a focus on proteome-wide context-specific quantitative drug target deconvolution and personalized adverse drug reaction prediction.              Expert opinion:                    Data science will facilitate reducing the complexity of systems pharmacology modeling, detecting hidden correlations between complex data sets, and distinguishing causation from correlation. The power of data science can only be fully realized when integrated with mechanism-based multi-scale modeling that explicitly takes into account the hierarchical organization of biological systems from nucleic acid to proteins, to molecular interaction networks, to cells, to tissues, to patients, and to populations.",2016-06-01,13,1686,94,1802
1173,27157416,Data-driven Approach to Detect and Predict Adverse Drug Reactions,"Background:                    Many factors that directly or indirectly cause adverse drug reaction (ADRs) varying from pharmacological, immunological and genetic factors to ethnic, age, gender, social factors as well as drug and disease related ones. On the other hand, advanced methods of statistics, machine learning and data mining allow the users to more effectively analyze the data for descriptive and predictive purposes. The fast changes in this field make it difficult to follow the research progress and context on ADR detection and prediction.              Methods:                    A large amount of articles on ADRs in the last twenty years is collected. These articles are grouped by recent data types used to study ADRs: omics, social media and electronic medical records (EMRs), and reviewed in terms of the problem addressed, the datasets used and methods.              Results:                    Corresponding three tables are established providing brief information on the research for ADRs detection and prediction.              Conclusion:                    The data-driven approach has shown to be powerful in ADRs detection and prediction. The review helps researchers and pharmacists to have a quick overview on the current status of ADRs detection and prediction.",2016-06-01,9,1293,65,1802
2781,27656117,Pattern Recognition Approaches for Breast Cancer DCE-MRI Classification: A Systematic Review,"We performed a systematic review of several pattern analysis approaches for classifying breast lesions using dynamic, morphological, and textural features in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Several machine learning approaches, namely artificial neural networks (ANN), support vector machines (SVM), linear discriminant analysis (LDA), tree-based classifiers (TC), and Bayesian classifiers (BC), and features used for classification are described. The findings of a systematic review of 26 studies are presented. The sensitivity and specificity are respectively 91 and 83 % for ANN, 85 and 82 % for SVM, 96 and 85 % for LDA, 92 and 87 % for TC, and 82 and 85 % for BC. The sensitivity and specificity are respectively 82 and 74 % for dynamic features, 93 and 60 % for morphological features, 88 and 81 % for textural features, 95 and 86 % for a combination of dynamic and morphological features, and 88 and 84 % for a combination of dynamic, morphological, and other features. LDA and TC have the best performance. A combination of dynamic and morphological features gives the best performance.",2016-06-01,15,1125,92,1802
1172,27157417,Probing the Hypothesis of SAR Continuity Restoration by the Removal of Activity Cliffs Generators in QSAR,"In this work we report the first attempt to study the effect of activity cliffs over the generalization ability of machine learning (ML) based QSAR classifiers, using as study case a previously reported diverse and noisy dataset focused on drug induced liver injury (DILI) and more than 40 ML classification algorithms. Here, the hypothesis of structure-activity relationship (SAR) continuity restoration by activity cliffs removal is tested as a potential solution to overcome such limitation. Previously, a parallelism was established between activity cliffs generators (ACGs) and instances that should be misclassified (ISMs), a related concept from the field of machine learning. Based on this concept we comparatively studied the classification performance of multiple machine learning classifiers as well as the consensus classifier derived from predictive classifiers obtained from training sets including or excluding ACGs. The influence of the removal of ACGs from the training set over the virtual screening performance was also studied for the respective consensus classifiers algorithms. In general terms, the removal of the ACGs from the training process slightly decreased the overall accuracy of the ML classifiers and multi-classifiers, improving their sensitivity (the weakest feature of ML classifiers trained with ACGs) but decreasing their specificity. Although these results do not support a positive effect of the removal of ACGs over the classification performance of ML classifiers, the ""balancing effect"" of ACG removal demonstrated to positively influence the virtual screening performance of multi-classifiers based on valid base ML classifiers. Specially, the early recognition ability was significantly favored after ACGs removal. The results presented and discussed in this work represent the first step towards the application of a remedial solution to the activity cliffs problem in QSAR studies.",2016-06-01,1,1928,105,1802
1208,26806341,Computer vision for high content screening,"High Content Screening (HCS) technologies that combine automated fluorescence microscopy with high throughput biotechnology have become powerful systems for studying cell biology and drug screening. These systems can produce more than 100 000 images per day, making their success dependent on automated image analysis. In this review, we describe the steps involved in quantifying microscopy images and different approaches for each step. Typically, individual cells are segmented from the background using a segmentation algorithm. Each cell is then quantified by extracting numerical features, such as area and intensity measurements. As these feature representations are typically high dimensional (>500), modern machine learning algorithms are used to classify, cluster and visualize cells in HCS experiments. Machine learning algorithms that learn feature representations, in addition to the classification or clustering task, have recently advanced the state of the art on several benchmarking tasks in the computer vision community. These techniques have also recently been applied to HCS image analysis.",2016-06-01,13,1111,42,1802
2745,27975231,Statistical Approaches to Candidate Biomarker Panel Selection,"The statistical analysis of robust biomarker candidates is a complex process, and is involved in several key steps in the overall biomarker development pipeline (see Fig. 22.1, Chap. 19 ). Initially, data visualization (Sect. 22.1, below) is important to determine outliers and to get a feel for the nature of the data and whether there appear to be any differences among the groups being examined. From there, the data must be pre-processed (Sect. 22.2) so that outliers are handled, missing values are dealt with, and normality is assessed. Once the processed data has been cleaned and is ready for downstream analysis, hypothesis tests (Sect. 22.3) are performed, and proteins that are differentially expressed are identified. Since the number of differentially expressed proteins is usually larger than warrants further investigation (50+ proteins versus just a handful that will be considered for a biomarker panel), some sort of feature reduction (Sect. 22.4) should be performed to narrow the list of candidate biomarkers down to a more reasonable number. Once the list of proteins has been reduced to those that are likely most useful for downstream classification purposes, unsupervised or supervised learning is performed (Sects. 22.5 and 22.6, respectively).",2016-06-01,2,1269,61,1802
1210,26776761,Information and communication technology solutions for outdoor navigation in dementia,"Introduction:                    Information and communication technology (ICT) is potentially mature enough to empower outdoor and social activities in dementia. However, actual ICT-based devices have limited functionality and impact, mainly limited to safety. What is an ideal operational framework to enhance this field to support outdoor and social activities?              Methods:                    Review of literature and cross-disciplinary expert discussion.              Results:                    A situation-aware ICT requires a flexible fine-tuning by stakeholders of system usability and complexity of function, and of user safety and autonomy. It should operate by artificial intelligence/machine learning and should reflect harmonized stakeholder values, social context, and user residual cognitive functions. ICT services should be proposed at the prodromal stage of dementia and should be carefully validated within the life space of users in terms of quality of life, social activities, and costs.              Discussion:                    The operational framework has the potential to produce ICT and services with high clinical impact but requires substantial investment.",2016-06-01,8,1197,85,1802
1207,26814169,Use of machine learning approaches for novel drug discovery,"Introduction:                    The use of computational tools in the early stages of drug development has increased in recent decades. Machine learning (ML) approaches have been of special interest, since they can be applied in several steps of the drug discovery methodology, such as prediction of target structure, prediction of biological activity of new ligands through model construction, discovery or optimization of hits, and construction of models that predict the pharmacokinetic and toxicological (ADMET) profile of compounds.              Areas covered:                    This article presents an overview on some applications of ML techniques in drug design. These techniques can be employed in ligand-based drug design (LBDD) and structure-based drug design (SBDD) studies, such as similarity searches, construction of classification and/or prediction models of biological activity, prediction of secondary structures and binding sites docking and virtual screening.              Expert opinion:                    Successful cases have been reported in the literature, demonstrating the efficiency of ML techniques combined with traditional approaches to study medicinal chemistry problems. Some ML techniques used in drug design are: support vector machine, random forest, decision trees and artificial neural networks. Currently, an important application of ML techniques is related to the calculation of scoring functions used in docking and virtual screening assays from a consensus, combining traditional and ML techniques in order to improve the prediction of binding sites and docking solutions.",2016-06-01,32,1619,59,1802
1162,27315762,What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated,"We update complementary learning systems (CLS) theory, which holds that intelligent agents must possess two learning systems, instantiated in mammalians in neocortex and hippocampus. The first gradually acquires structured knowledge representations while the second quickly learns the specifics of individual experiences. We broaden the role of replay of hippocampal memories in the theory, noting that replay allows goal-dependent weighting of experience statistics. We also address recent challenges to the theory and extend it by showing that recurrent activation of hippocampal traces can support some forms of generalization and that neocortical learning can be rapid for information that is consistent with known structure. Finally, we note the relevance of the theory to the design of artificial intelligent agents, highlighting connections between neuroscience and machine learning.",2016-07-01,61,890,95,1772
1177,27113568,Use of systems biology to decipher host-pathogen interaction networks and predict biomarkers,"In systems biology, researchers aim to understand complex biological systems as a whole, which is often achieved by mathematical modelling and the analyses of high-throughput data. In this review, we give an overview of medical applications of systems biology approaches with special focus on host-pathogen interactions. After introducing general ideas of systems biology, we focus on (1) the detection of putative biomarkers for improved diagnosis and support of therapeutic decisions, (2) network modelling for the identification of regulatory interactions between cellular molecules to reveal putative drug targets and (3) module discovery for the detection of phenotype-specific modules in molecular interaction networks. Biomarker detection applies supervised machine learning methods utilizing high-throughput data (e.g. single nucleotide polymorphism (SNP) detection, RNA-seq, proteomics) and clinical data. We demonstrate structural analysis of molecular networks, especially by identification of disease modules as a novel strategy, and discuss possible applications to host-pathogen interactions. Pioneering work was done to predict molecular host-pathogen interactions networks based on dual RNA-seq data. However, currently this network modelling is restricted to a small number of genes. With increasing number and quality of databases and data repositories, the prediction of large-scale networks will also be feasible that can used for multidimensional diagnosis and decision support for prevention and therapy of diseases. Finally, we outline further perspective issues such as support of personalized medicine with high-throughput data and generation of multiscale host-pathogen interaction models.",2016-07-01,10,1715,92,1772
1161,27322705,Machine learning approaches in MALDI-MSI: clinical applications,"Introduction:                    Despite the unquestionable advantages of Matrix-Assisted Laser Desorption/Ionization Mass Spectrometry Imaging in visualizing the spatial distribution and the relative abundance of biomolecules directly on-tissue, the yielded data is complex and high dimensional. Therefore, analysis and interpretation of this huge amount of information is mathematically, statistically and computationally challenging.              Areas covered:                    This article reviews some of the challenges in data elaboration with particular emphasis on machine learning techniques employed in clinical applications, and can be useful in general as an entry point for those who want to study the computational aspects. Several characteristics of data processing are described, enlightening advantages and disadvantages. Different approaches for data elaboration focused on clinical applications are also provided. Practical tutorial based upon Orange Canvas and Weka software is included, helping familiarization with the data processing. Expert commentary: Recently, MALDI-MSI has gained considerable attention and has been employed for research and diagnostic purposes, with successful results. Data dimensionality constitutes an important issue and statistical methods for information-preserving data reduction represent one of the most challenging aspects. The most common data reduction methods are characterized by collecting independent observations into a single table. However, the incorporation of relational information can improve the discriminatory capability of the data.",2016-07-01,4,1607,63,1772
1146,27451435,Natural language processing in pathology: a scoping review,"Background:                    Encoded pathology data are key for medical registries and analyses, but pathology information is often expressed as free text.              Objective:                    We reviewed and assessed the use of NLP (natural language processing) for encoding pathology documents.              Materials and methods:                    Papers addressing NLP in pathology were retrieved from PubMed, Association for Computing Machinery (ACM) Digital Library and Association for Computational Linguistics (ACL) Anthology. We reviewed and summarised the study objectives; NLP methods used and their validation; software implementations; the performance on the dataset used and any reported use in practice.              Results:                    The main objectives of the 38 included papers were encoding and extraction of clinically relevant information from pathology reports. Common approaches were word/phrase matching, probabilistic machine learning and rule-based systems. Five papers (13%) compared different methods on the same dataset. Four papers did not specify the method(s) used. 18 of the 26 studies that reported F-measure, recall or precision reported values of over 0.9. Proprietary software was the most frequently mentioned category (14 studies); General Architecture for Text Engineering (GATE) was the most applied architecture overall. Practical system use was reported in four papers. Most papers used expert annotation validation.              Conclusions:                    Different methods are used in NLP research in pathology, and good performances, that is, high precision and recall, high retrieval/removal rates, are reported for all of these. Lack of validation and of shared datasets precludes performance comparison. More comparative analysis and validation are needed to provide better insight into the performance and merits of these methods.",2016-07-01,5,1904,58,1772
1180,27090952,Computational Analysis of Behavior,"In this review, we discuss the emerging field of computational behavioral analysis-the use of modern methods from computer science and engineering to quantitatively measure animal behavior. We discuss aspects of experiment design important to both obtaining biologically relevant behavioral data and enabling the use of machine vision and learning techniques for automation. These two goals are often in conflict. Restraining or restricting the environment of the animal can simplify automatic behavior quantification, but it can also degrade the quality or alter important aspects of behavior. To enable biologists to design experiments to obtain better behavioral measurements, and computer scientists to pinpoint fruitful directions for algorithm improvement, we review known effects of artificial manipulation of the animal on behavior. We also review machine vision and learning techniques for tracking, feature extraction, automated behavior classification, and automated behavior discovery, the assumptions they make, and the types of data they work best with.",2016-07-01,38,1067,34,1772
1136,27525157,New Evaluation Vector through the Stanford Mobile Inquiry-Based Learning Environment (SMILE) for Participatory Action Research,"Objectives:                    This article reviews an evaluation vector model driven from a participatory action research leveraging a collective inquiry system named SMILE (Stanford Mobile Inquiry-based Learning Environment).              Methods:                    SMILE has been implemented in a diverse set of collective inquiry generation and analysis scenarios including community health care-specific professional development sessions and community-based participatory action research projects. In each scenario, participants are given opportunities to construct inquiries around physical and emotional health-related phenomena in their own community.              Results:                    Participants formulated inquiries as well as potential clinical treatments and hypothetical scenarios to address health concerns or clarify misunderstandings or misdiagnoses often found in their community practices. From medical universities to rural village health promotion organizations, all participatory inquiries and potential solutions can be collected and analyzed. The inquiry and solution sets represent an evaluation vector which helps educators better understand community health issues at a much deeper level.              Conclusions:                    SMILE helps collect problems that are most important and central to their community health concerns. The evaluation vector, consisting participatory and collective inquiries and potential solutions, helps the researchers assess the participants' level of understanding on issues around health concerns and practices while helping the community adequately formulate follow-up action plans. The method used in SMILE requires much further enhancement with machine learning and advanced data visualization.",2016-07-01,1,1772,126,1772
1171,27168345,Neurobiological markers predicting treatment response in anxiety disorders: A systematic review and implications for clinical application,"Anxiety disorders constitute the largest group of mental disorders with a high individual and societal burden. Neurobiological markers of treatment response bear potential to improve response rates by informing stratified medicine approaches. A systematic review was performed on the current evidence of the predictive value of genetic, neuroimaging and other physiological markers for treatment response (pharmacological and/or psychotherapeutic treatment) in anxiety disorders. Studies published until March 2015 were selected through search in PubMed, Web of Science, PsycINFO, Embase, and CENTRAL. Sixty studies were included, among them 27 on genetic, 17 on neuroimaging and 16 on other markers. Preliminary evidence was found for the functional 5-HTTLPR/rs25531 genotypes, anterior cingulate cortex function and cardiovascular flexibility to modulate treatment outcome. Studies varied considerably in methodological quality. Application of more stringent study methodology, predictions on the individual patient level and cross-validation in independent samples are recommended to set the next stage of biomarker research and to avoid flawed conclusions in the emerging field of ""Mental Health Predictomics"".",2016-07-01,20,1214,137,1772
1130,27559342,Data Mining and Pattern Recognition Models for Identifying Inherited Diseases: Challenges and Implications,"Data mining and pattern recognition methods reveal interesting findings in genetic studies, especially on how the genetic makeup is associated with inherited diseases. Although researchers have proposed various data mining models for biomedical approaches, there remains a challenge in accurately prioritizing the single nucleotide polymorphisms (SNP) associated with the disease. In this commentary, we review the state-of-art data mining and pattern recognition models for identifying inherited diseases and deliberate the need of binary classification- and scoring-based prioritization methods in determining causal variants. While we discuss the pros and cons associated with these methods known, we argue that the gene prioritization methods and the protein interaction (PPI) methods in conjunction with the K nearest neighbors' could be used in accurately categorizing the genetic factors in disease causation.",2016-08-01,1,916,106,1741
1152,27423136,Protein function in precision medicine: deep understanding with machine learning,"Precision medicine and personalized health efforts propose leveraging complex molecular, medical and family history, along with other types of personal data toward better life. We argue that this ambitious objective will require advanced and specialized machine learning solutions. Simply skimming some low-hanging results off the data wealth might have limited potential. Instead, we need to better understand all parts of the system to define medically relevant causes and effects: how do particular sequence variants affect particular proteins and pathways? How do these effects, in turn, cause the health or disease-related phenotype? Toward this end, deeper understanding will not simply diffuse from deeper machine learning, but from more explicit focus on understanding protein function, context-specific protein interaction networks, and impact of variation on both.",2016-08-01,17,874,80,1741
1133,27529225,Deep Artificial Neural Networks and Neuromorphic Chips for Big Data Analysis: Pharmaceutical and Bioinformatics Applications,"Over the past decade, Deep Artificial Neural Networks (DNNs) have become the state-of-the-art algorithms in Machine Learning (ML), speech recognition, computer vision, natural language processing and many other tasks. This was made possible by the advancement in Big Data, Deep Learning (DL) and drastically increased chip processing abilities, especially general-purpose graphical processing units (GPGPUs). All this has created a growing interest in making the most of the potential offered by DNNs in almost every field. An overview of the main architectures of DNNs, and their usefulness in Pharmacology and Bioinformatics are presented in this work. The featured applications are: drug design, virtual screening (VS), Quantitative Structure-Activity Relationship (QSAR) research, protein structure prediction and genomics (and other omics) data mining. The future need of neuromorphic hardware for DNNs is also discussed, and the two most advanced chips are reviewed: IBM TrueNorth and SpiNNaker. In addition, this review points out the importance of considering not only neurons, as DNNs and neuromorphic chips should also include glial cells, given the proven importance of astrocytes, a type of glial cell which contributes to information processing in the brain. The Deep Artificial Neuron-Astrocyte Networks (DANAN) could overcome the difficulties in architecture design, learning process and scalability of the current ML methods.",2016-08-01,11,1441,124,1741
1165,27282231,A review of the applications of data mining and machine learning for the prediction of biomedical properties of nanoparticles,"This article presents a comprehensive review of applications of data mining and machine learning for the prediction of biomedical properties of nanoparticles of medical interest. The papers reviewed here present the results of research using these techniques to predict the biological fate and properties of a variety of nanoparticles relevant to their biomedical applications. These include the influence of particle physicochemical properties on cellular uptake, cytotoxicity, molecular loading, and molecular release in addition to manufacturing properties like nanoparticle size, and polydispersity. Overall, the results are encouraging and suggest that as more systematic data from nanoparticles becomes available, machine learning and data mining would become a powerful aid in the design of nanoparticles for biomedical applications. There is however the challenge of great heterogeneity in nanoparticles, which will make these discoveries more challenging than for traditional small molecule drug design.",2016-08-01,4,1012,125,1741
2701,28268284,Intelligent hearing aids: the next revolution,"The first revolution in hearing aids came from nonlinear amplification, which allows better compensation for both soft and loud sounds. The second revolution stemmed from the introduction of digital signal processing, which allows better programmability and more sophisticated algorithms. The third revolution in hearing aids is wireless, which allows seamless connectivity between a pair of hearing aids and with more and more external devices. Each revolution has fundamentally transformed hearing aids and pushed the entire industry forward significantly. Machine learning has received significant attention in recent years and has been applied in many other industries, e.g., robotics, speech recognition, genetics, and crowdsourcing. We argue that the next revolution in hearing aids is machine intelligence. In fact, this revolution is already quietly happening. We will review the development in at least three major areas: applications of machine learning in speech enhancement; applications of machine learning in individualization and customization of signal processing algorithms; applications of machine learning in improving the efficiency and effectiveness of clinical tests. With the advent of the internet of things, the above developments will accelerate. This revolution will bring patient satisfactions to a new level that has never been seen before.",2016-08-01,1,1369,45,1741
1167,27224846,Using online social networks to track a pandemic: A systematic review,"Background:                    The popularity and proliferation of online social networks (OSNs) have created massive social interaction among users that generate an extensive amount of data. An OSN offers a unique opportunity for studying and understanding social interaction and communication among far larger populations now more than ever before. Recently, OSNs have received considerable attention as a possible tool to track a pandemic because they can provide an almost real-time surveillance system at a less costly rate than traditional surveillance systems.              Methods:                    A systematic literature search for studies with the primary aim of using OSN to detect and track a pandemic was conducted. We conducted an electronic literature search for eligible English articles published between 2004 and 2015 using PUBMED, IEEExplore, ACM Digital Library, Google Scholar, and Web of Science. First, the articles were screened on the basis of titles and abstracts. Second, the full texts were reviewed. All included studies were subjected to quality assessment.              Result:                    OSNs have rich information that can be utilized to develop an almost real-time pandemic surveillance system. The outcomes of OSN surveillance systems have demonstrated high correlations with the findings of official surveillance systems. However, the limitation in using OSN to track pandemic is in collecting representative data with sufficient population coverage. This challenge is related to the characteristics of OSN data. The data are dynamic, large-sized, and unstructured, thus requiring advanced algorithms and computational linguistics.              Conclusions:                    OSN data contain significant information that can be used to track a pandemic. Different from traditional surveys and clinical reports, in which the data collection process is time consuming at costly rates, OSN data can be collected almost in real time at a cheaper cost. Additionally, the geographical and temporal information can provide exploratory analysis of spatiotemporal dynamics of infectious disease spread. However, on one hand, an OSN-based surveillance system requires comprehensive adoption, enhanced geographical identification system, and advanced algorithms and computational linguistics to eliminate its limitations and challenges. On the other hand, OSN is probably to never replace traditional surveillance, but it can offer complementary data that can work best when integrated with traditional data.",2016-08-01,18,2546,69,1741
2786,27610328,Big data and tactical analysis in elite soccer: future challenges and opportunities for sports science,"Until recently tactical analysis in elite soccer were based on observational data using variables which discard most contextual information. Analyses of team tactics require however detailed data from various sources including technical skill, individual physiological performance, and team formations among others to represent the complex processes underlying team tactical behavior. Accordingly, little is known about how these different factors influence team tactical behavior in elite soccer. In parts, this has also been due to the lack of available data. Increasingly however, detailed game logs obtained through next-generation tracking technologies in addition to physiological training data collected through novel miniature sensor technologies have become available for research. This leads however to the opposite problem where the shear amount of data becomes an obstacle in itself as methodological guidelines as well as theoretical modelling of tactical decision making in team sports is lacking. The present paper discusses how big data and modern machine learning technologies may help to address these issues and aid in developing a theoretical model for tactical decision making in team sports. As experience from medical applications show, significant organizational obstacles regarding data governance and access to technologies must be overcome first. The present work discusses these issues with respect to tactical analyses in elite soccer and propose a technological stack which aims to introduce big data technologies into elite soccer research. The proposed approach could also serve as a guideline for other sports science domains as increasing data size is becoming a wide-spread phenomenon.",2016-08-01,29,1720,102,1741
1142,27488403,Progress in Biomedical Knowledge Discovery: A 25-year Retrospective,"Objectives:                    We sought to explore, via a systematic review of the literature, the state of the art of knowledge discovery in biomedical databases as it existed in 1992, and then now, 25 years later, mainly focused on supervised learning.              Methods:                    We performed a rigorous systematic search of PubMed and latent Dirichlet allocation to identify themes in the literature and trends in the science of knowledge discovery in and between time periods and compare these trends. We restricted the result set using a bracket of five years previous, such that the 1992 result set was restricted to articles published between 1987 and 1992, and the 2015 set between 2011 and 2015. This was to reflect the current literature available at the time to researchers and others at the target dates of 1992 and 2015. The search term was framed as: Knowledge Discovery OR Data Mining OR Pattern Discovery OR Pattern Recognition, Automated.              Results:                    A total 538 and 18,172 documents were retrieved for 1992 and 2015, respectively. The number and type of data sources increased dramatically over the observation period, primarily due to the advent of electronic clinical systems. The period 1992- 2015 saw the emergence of new areas of research in knowledge discovery, and the refinement and application of machine learning approaches that were nascent or unknown in 1992.              Conclusions:                    Over the 25 years of the observation period, we identified numerous developments that impacted the science of knowledge discovery, including the availability of new forms of data, new machine learning algorithms, and new application domains. Through a bibliometric analysis we examine the striking changes in the availability of highly heterogeneous data resources, the evolution of new algorithmic approaches to knowledge discovery, and we consider from legal, social, and political perspectives possible explanations of the growth of the field. Finally, we reflect on the achievements of the past 25 years to consider what the next 25 years will bring with regard to the availability of even more complex data and to the methods that could be, and are being now developed for the discovery of new knowledge in biomedical data.",2016-08-01,1,2307,67,1741
1126,27586752,Learnability of prosodic boundaries: Is infant-directed speech easier?,"This study explores the long-standing hypothesis that the acoustic cues to prosodic boundaries in infant-directed speech (IDS) make those boundaries easier to learn than those in adult-directed speech (ADS). Three cues (pause duration, nucleus duration, and pitch change) were investigated, by means of a systematic review of the literature, statistical analyses of a corpus of Japanese, and machine learning experiments. The review of previous work revealed that the effect of register on boundary cues is less well established than previously thought, and that results often vary across studies for certain cues. Statistical analyses run on a large database of mother-child and mother-interviewer interactions showed that the duration of a pause and the duration of the syllable nucleus preceding the boundary are two cues which are enhanced in IDS, while f0 change is actually degraded in IDS. Supervised and unsupervised machine learning techniques applied to these acoustic cues revealed that IDS boundaries were consistently better classified than ADS ones, regardless of the learning method used. The role of the cues examined in this study and the importance of these findings in the more general context of early linguistic structure acquisition is discussed.",2016-08-01,0,1268,70,1741
1164,27295548,A renaissance of neural networks in drug discovery,"Introduction:                    Neural networks are becoming a very popular method for solving machine learning and artificial intelligence problems. The variety of neural network types and their application to drug discovery requires expert knowledge to choose the most appropriate approach.              Areas covered:                    In this review, the authors discuss traditional and newly emerging neural network approaches to drug discovery. Their focus is on backpropagation neural networks and their variants, self-organizing maps and associated methods, and a relatively new technique, deep learning. The most important technical issues are discussed including overfitting and its prevention through regularization, ensemble and multitask modeling, model interpretation, and estimation of applicability domain. Different aspects of using neural networks in drug discovery are considered: building structure-activity models with respect to various targets; predicting drug selectivity, toxicity profiles, ADMET and physicochemical properties; characteristics of drug-delivery systems and virtual screening.              Expert opinion:                    Neural networks continue to grow in importance for drug discovery. Recent developments in deep learning suggests further improvements may be gained in the analysis of large chemical data sets. It's anticipated that neural networks will be more widely used in drug discovery in the future, and applied in non-traditional areas such as drug delivery systems, biologically compatible materials, and regenerative medicine.",2016-08-01,33,1586,50,1741
1179,27097559,Areas of controversy in neuroprogression in bipolar disorder,"Objective:                    We aimed to review clinical features and biological underpinnings related to neuroprogression in bipolar disorder (BD). Also, we discussed areas of controversy and future research in the field.              Method:                    We systematically reviewed the extant literature pertaining to neuroprogression and BD by searching PubMed and EMBASE for articles published up to March 2016.              Results:                    A total of 114 studies were included. Neuroimaging and clinical evidence from cross-sectional and longitudinal studies show that a subset of patients with BD presents a neuroprogressive course with brain changes and unfavorable outcomes. Risk factors associated with these unfavorable outcomes are number of mood episodes, early trauma, and psychiatric and clinical comorbidity.              Conclusion:                    Illness trajectories are largely variable, and illness progression is not a general rule in BD. The number of manic episodes seems to be the clinical marker more robustly associated with neuroprogression in BD. However, the majority of the evidence came from cross-sectional studies that are prone to bias. Longitudinal studies may help to identify signatures of neuroprogression and integrate findings from the field of neuroimaging, neurocognition, and biomarkers.",2016-08-01,31,1353,60,1741
2700,28268288,Difficulty understanding speech in noise by the hearing impaired: underlying causes and technological solutions,"A primary complaint of hearing-impaired individuals involves poor speech understanding when background noise is present. Hearing aids and cochlear implants often allow good speech understanding in quiet backgrounds. But hearing-impaired individuals are highly noise intolerant, and existing devices are not very effective at combating background noise. As a result, speech understanding in noise is often quite poor. In accord with the significance of the problem, considerable effort has been expended toward understanding and remedying this issue. Fortunately, our understanding of the underlying issues is reasonably good. In sharp contrast, effective solutions have remained elusive. One solution that seems promising involves a single-microphone machine-learning algorithm to extract speech from background noise. Data from our group indicate that the algorithm is capable of producing vast increases in speech understanding by hearing-impaired individuals. This paper will first provide an overview of the speech-in-noise problem and outline why hearing-impaired individuals are so noise intolerant. An overview of our approach to solving this problem will follow.",2016-08-01,1,1170,111,1741
1154,27383691,Alien Mindscapes-A Perspective on the Search for Extraterrestrial Intelligence,"Advances in planetary and space sciences, astrobiology, and life and cognitive sciences, combined with developments in communication theory, bioneural computing, machine learning, and big data analysis, create new opportunities to explore the probabilistic nature of alien life. Brought together in a multidisciplinary approach, they have the potential to support an integrated and expanded Search for Extraterrestrial Intelligence (SETI (1) ), a search that includes looking for life as we do not know it. This approach will augment the odds of detecting a signal by broadening our understanding of the evolutionary and systemic components in the search for extraterrestrial intelligence (ETI), provide more targets for radio and optical SETI, and identify new ways of decoding and coding messages using universal markers.              Key words:                    SETI-Astrobiology-Coevolution of Earth and life-Planetary habitability and biosignatures. Astrobiology 16, 661-676.",2016-09-01,3,982,78,1710
1181,27072840,Regional infarction identification from cardiac CT images: a computer-aided biomechanical approach,"Purpose:                    Regional infarction identification is important for heart disease diagnosis and management, and myocardial deformation has been shown to be effective for this purpose. Although tagged and strain-encoded MR images can provide such measurements, they are uncommon in clinical routine. On the contrary, cardiac CT images are more available with lower costs, but they only provide motion of cardiac boundaries and additional constraints are required to obtain the myocardial strains. The goal of this study is to verify the potential of contrast-enhanced CT images on computer-aided regional infarction identification.              Methods:                    We propose a biomechanical approach combined with machine learning algorithms. A hyperelastic biomechanical model is used with deformable image registration to estimate 3D myocardial strains from CT images. The regional strains and CT image intensities are input to a classifier for regional infarction identification. Cross-validations on ten canine image sequences with artificially induced infarctions were used to study the performances of using different feature combinations and machine learning algorithms.              Results:                    Radial strain, circumferential strain, first principal strain, and image intensity were shown to be discriminative features. The highest identification accuracy ([Formula: see text] %) was achieved when combining radial strain with image intensity. Random forests gave better results than support vector machines on less discriminative features. Random forests also performed better when all strains were used together.              Conclusion:                    Although CT images cannot directly measure myocardial deformation, with the use of a biomechanical model, the estimated strains can provide promising identification results especially when combined with CT image intensity.",2016-09-01,6,1925,98,1710
1140,27501026,"Machine learning for large-scale wearable sensor data in Parkinson's disease: Concepts, promises, pitfalls, and futures","For the treatment and monitoring of Parkinson's disease (PD) to be scientific, a key requirement is that measurement of disease stages and severity is quantitative, reliable, and repeatable. The last 50 years in PD research have been dominated by qualitative, subjective ratings obtained by human interpretation of the presentation of disease signs and symptoms at clinical visits. More recently, ""wearable,"" sensor-based, quantitative, objective, and easy-to-use systems for quantifying PD signs for large numbers of participants over extended durations have been developed. This technology has the potential to significantly improve both clinical diagnosis and management in PD and the conduct of clinical studies. However, the large-scale, high-dimensional character of the data captured by these wearable sensors requires sophisticated signal processing and machine-learning algorithms to transform it into scientifically and clinically meaningful information. Such algorithms that ""learn"" from data have shown remarkable success in making accurate predictions for complex problems in which human skill has been required to date, but they are challenging to evaluate and apply without a basic understanding of the underlying logic on which they are based. This article contains a nontechnical tutorial review of relevant machine-learning algorithms, also describing their limitations and how these can be overcome. It discusses implications of this technology and a practical road map for realizing the full potential of this technology in PD research and practice.  2016 International Parkinson and Movement Disorder Society.",2016-09-01,41,1631,119,1710
1128,29560870,Using Electroencephalography for Treatment Guidance in Major Depressive Disorder,"Given the high prevalence of treatment-resistant depression and the long delays in finding effective treatments via trial and error, valid biomarkers of treatment outcome with the ability to guide treatment selection represent one of the most important unmet needs in mood disorders. A large body of research has investigated, for this purpose, biomarkers derived from electroencephalography (EEG), using resting state EEG or evoked potentials. Most studies have focused on specific EEG features (or combinations thereof), whereas more recently machine-learning approaches have been used to define the EEG features with the best predictive abilities without a priori hypotheses. While reviewing these different approaches, we have focused on the predictor characteristics and the quality of the supporting evidence.",2016-09-01,13,815,80,1710
2782,27652181,An overview of topic modeling and its current applications in bioinformatics,"Background:                    With the rapid accumulation of biological datasets, machine learning methods designed to automate data analysis are urgently needed. In recent years, so-called topic models that originated from the field of natural language processing have been receiving much attention in bioinformatics because of their interpretability. Our aim was to review the application and development of topic models for bioinformatics.              Description:                    This paper starts with the description of a topic model, with a focus on the understanding of topic modeling. A general outline is provided on how to build an application in a topic model and how to develop a topic model. Meanwhile, the literature on application of topic models to biological data was searched and analyzed in depth. According to the types of models and the analogy between the concept of document-topic-word and a biological object (as well as the tasks of a topic model), we categorized the related studies and provided an outlook on the use of topic models for the development of bioinformatics applications.              Conclusion:                    Topic modeling is a useful method (in contrast to the traditional means of data reduction in bioinformatics) and enhances researchers' ability to interpret biological information. Nevertheless, due to the lack of topic models optimized for specific biological data, the studies on topic modeling in biological data still have a long and challenging road ahead. We believe that topic models are a promising method for various applications in bioinformatics research.",2016-09-01,20,1627,76,1710
2784,27649151,Omics-Based Strategies in Precision Medicine: Toward a Paradigm Shift in Inborn Errors of Metabolism Investigations,"The rise of technologies that simultaneously measure thousands of data points represents the heart of systems biology. These technologies have had a huge impact on the discovery of next-generation diagnostics, biomarkers, and drugs in the precision medicine era. Systems biology aims to achieve systemic exploration of complex interactions in biological systems. Driven by high-throughput omics technologies and the computational surge, it enables multi-scale and insightful overviews of cells, organisms, and populations. Precision medicine capitalizes on these conceptual and technological advancements and stands on two main pillars: data generation and data modeling. High-throughput omics technologies allow the retrieval of comprehensive and holistic biological information, whereas computational capabilities enable high-dimensional data modeling and, therefore, accessible and user-friendly visualization. Furthermore, bioinformatics has enabled comprehensive multi-omics and clinical data integration for insightful interpretation. Despite their promise, the translation of these technologies into clinically actionable tools has been slow. In this review, we present state-of-the-art multi-omics data analysis strategies in a clinical context. The challenges of omics-based biomarker translation are discussed. Perspectives regarding the use of multi-omics approaches for inborn errors of metabolism (IEM) are presented by introducing a new paradigm shift in addressing IEM investigations in the post-genomic era.",2016-09-01,39,1523,115,1710
1127,29560871,The Clinical Added Value of Imaging: A Perspective From Outcome Prediction,"Objective measures of psychiatric health would be of benefit in clinical practice. Despite considerable research in the area of psychiatric neuroimaging outcome prediction, translating putative neuroimaging markers (neuromarkers) of a disorder into clinical practice has proven challenging. We reviewed studies that used neuroimaging measures to predict treatment response and disease outcomes in major depressive disorder, substance use, autism spectrum disorder, psychosis, and dementia. The majority of studies sought to predict psychiatric outcomes rather than develop a specific biological index of future disease trajectory. Studies varied widely with respect to sample size and quantification of out-of-sample prediction model performance. Many studies were able to predict psychiatric outcomes with moderate accuracy, with neuroimaging data often augmenting the prediction compared to clinical or psychometric data alone. We make recommendations for future research with respect to methods that can increase the generalizability and reproducibility of predictions. Large sample sizes in conjunction with machine learning methods, such as feature selection, cross-validation, and random label permutation, provide significant improvement to and quantification of generalizability. Further refinement of neuroimaging protocols and analysis methods will likely facilitate the clinical applicability of predictive imaging markers in psychiatry. Such clinically relevant neuromarkers need not necessarily be grounded in the pathophysiology of the disease, but identifying these neuromarkers may suggest targets for future research into disease mechanisms. The ability of imaging prediction models to augment clinical judgments will ultimately depend on the personal and economic costs and benefits to the patient.",2016-09-01,7,1816,74,1710
1198,26911811,Extracting information from the text of electronic medical records to improve case detection: a systematic review,"Background:                    Electronic medical records (EMRs) are revolutionizing health-related research. One key issue for study quality is the accurate identification of patients with the condition of interest. Information in EMRs can be entered as structured codes or unstructured free text. The majority of research studies have used only coded parts of EMRs for case-detection, which may bias findings, miss cases, and reduce study quality. This review examines whether incorporating information from text into case-detection algorithms can improve research quality.              Methods:                    A systematic search returned 9659 papers, 67 of which reported on the extraction of information from free text of EMRs with the stated purpose of detecting cases of a named clinical condition. Methods for extracting information from text and the technical accuracy of case-detection algorithms were reviewed.              Results:                    Studies mainly used US hospital-based EMRs, and extracted information from text for 41 conditions using keyword searches, rule-based algorithms, and machine learning methods. There was no clear difference in case-detection algorithm accuracy between rule-based and machine learning methods of extraction. Inclusion of information from text resulted in a significant improvement in algorithm sensitivity and area under the receiver operating characteristic in comparison to codes alone (median sensitivity 78% (codes + text) vs 62% (codes), P = .03; median area under the receiver operating characteristic 95% (codes + text) vs 88% (codes), P = .025).              Conclusions:                    Text in EMRs is accessible, especially with open source information extraction algorithms, and significantly improves case detection when combined with codes. More harmonization of reporting within EMR studies is needed, particularly standardized reporting of algorithm accuracy metrics like positive predictive value (precision) and sensitivity (recall).",2016-09-01,78,2018,113,1710
1193,26962757,Propensity score analysis with missing data,"Propensity score analysis is a method that equates treatment and control groups on a comprehensive set of measured confounders in observational (nonrandomized) studies. A successful propensity score analysis reduces bias in the estimate of the average treatment effect in a nonrandomized study, making the estimate more comparable with that obtained from a randomized experiment. This article reviews and discusses an important practical issue in propensity analysis, in which the baseline covariates (potential confounders) and the outcome have missing values (incompletely observed). We review the statistical theory of propensity score analysis and estimation methods for propensity scores with incompletely observed covariates. Traditional logistic regression and modern machine learning methods (e.g., random forests, generalized boosted modeling) as estimation methods for incompletely observed covariates are reviewed. Balance diagnostics and equating methods for incompletely observed covariates are briefly described. Using an empirical example, the propensity score estimation methods for incompletely observed covariates are illustrated and compared. (PsycINFO Database Record",2016-09-01,8,1187,43,1710
1153,27406289,"Machine learning, statistical learning and the future of biological research in psychiatry","Psychiatric research has entered the age of 'Big Data'. Datasets now routinely involve thousands of heterogeneous variables, including clinical, neuroimaging, genomic, proteomic, transcriptomic and other 'omic' measures. The analysis of these datasets is challenging, especially when the number of measurements exceeds the number of individuals, and may be further complicated by missing data for some subjects and variables that are highly correlated. Statistical learning-based models are a natural extension of classical statistical approaches but provide more effective methods to analyse very large datasets. In addition, the predictive capability of such models promises to be useful in developing decision support systems. That is, methods that can be introduced to clinical settings and guide, for example, diagnosis classification or personalized treatment. In this review, we aim to outline the potential benefits of statistical learning methods in clinical research. We first introduce the concept of Big Data in different environments. We then describe how modern statistical learning models can be used in practice on Big Datasets to extract relevant information. Finally, we discuss the strengths of using statistical learning in psychiatric studies, from both research and practical clinical points of view.",2016-09-01,48,1322,90,1710
1160,27340949,Multiscale modeling of brain dynamics: from single neurons and networks to mathematical tools,"The extreme complexity of the brain naturally requires mathematical modeling approaches on a large variety of scales; the spectrum ranges from single neuron dynamics over the behavior of groups of neurons to neuronal network activity. Thus, the connection between the microscopic scale (single neuron activity) to macroscopic behavior (emergent behavior of the collective dynamics) and vice versa is a key to understand the brain in its complexity. In this work, we attempt a review of a wide range of approaches, ranging from the modeling of single neuron dynamics to machine learning. The models include biophysical as well as data-driven phenomenological models. The discussed models include Hodgkin-Huxley, FitzHugh-Nagumo, coupled oscillators (Kuramoto oscillators, Rssler oscillators, and the Hindmarsh-Rose neuron), Integrate and Fire, networks of neurons, and neural field equations. In addition to the mathematical models, important mathematical methods in multiscale modeling and reconstruction of the causal connectivity are sketched. The methods include linear and nonlinear tools from statistics, data analysis, and time series analysis up to differential equations, dynamical systems, and bifurcation theory, including Granger causal connectivity analysis, phase synchronization connectivity analysis, principal component analysis (PCA), independent component analysis (ICA), and manifold learning algorithms such as ISOMAP, and diffusion maps and equation-free techniques. WIREs Syst Biol Med 2016, 8:438-458. doi: 10.1002/wsbm.1348 For further resources related to this article, please visit the WIREs website.",2016-09-01,3,1627,93,1710
2783,27650405,Systematic review of blood transcriptome profiling in neuropsychiatric disorders: guidelines for biomarker discovery,"Introduction:                    The utility of blood for genome-wide gene expression profiling and biomarker discovery has received much attention in patients diagnosed with major neuropsychiatric disorders. While numerous studies have been conducted, statistical rigor and clarity in terms of blood-based biomarker discovery, validation, and testing are needed.              Methods:                    We conducted a systematic review of the literature to investigate methodological approaches and to assess the value of blood transcriptome profiling in research on mental disorders. We were particularly interested in statistical considerations related to machine learning, gene network analyses, and convergence across different disorders.              Results:                    A total of 108 peripheral blood transcriptome studies across 15 disorders were surveyed: 25 studies used a variety of machine learning techniques to assess putative clinical viability of the candidate biomarkers; 11 leveraged a higher-order systems-level perspective to identify gene module-based biomarkers; and nine performed analyses across two or more neuropsychiatric phenotypes. Notably, ~50% of the surveyed studies included fewer than 50 samples (cases and controls), while ~75% included less than 100.              Conclusions:                    Detailed consideration of statistical analysis in the early stages of experimental planning is critical to ensure blood-based biomarker discovery and validation. Statistical guidelines are presented to enhance implementation and reproducibility of machine learning and gene network analyses across independent studies. Future studies capitalizing on larger sample sizes and emerging next-generation technologies set the stage for moving the field forwards. Copyright  2016 John Wiley & Sons, Ltd.",2016-09-01,4,1839,116,1710
1615,26411473,Correct machine learning on protein sequences: a peer-reviewing perspective,"Machine learning methods are becoming increasingly popular to predict protein features from sequences. Machine learning in bioinformatics can be powerful but carries also the risk of introducing unexpected biases, which may lead to an overestimation of the performance. This article espouses a set of guidelines to allow both peer reviewers and authors to avoid common machine learning pitfalls. Understanding biology is necessary to produce useful data sets, which have to be large and diverse. Separating the training and test process is imperative to avoid over-selling method performance, which is also dependent on several hidden parameters. A novel predictor has always to be compared with several existing methods, including simple baseline strategies. Using the presented guidelines will help nonspecialists to appreciate the critical issues in machine learning.",2016-09-01,14,870,75,1710
1145,27470504,A review on host-pathogen interactions: classification and prediction,"The research on host-pathogen interactions is an ever-emerging and evolving field. Every other day a new pathogen gets discovered, along with comes the challenge of its prevention and cure. As the intelligent human always vies for prevention, which is better than cure, understanding the mechanisms of host-pathogen interactions gets prior importance. There are many mechanisms involved from the pathogen as well as the host sides while an interaction happens. It is a vis-a-vis fight of the counter genes and proteins from both sides. Who wins depends on whether a host gets an infection or not. Moreover, a higher level of complexity arises when the pathogens evolve and become resistant to a host's defense mechanisms. Such pathogens pose serious challenges for treatment. The entire human population is in danger of such long-lasting persistent infections. Some of these infections even increase the rate of mortality. Hence there is an immediate emergency to understand how the pathogens interact with their host for successful invasion. It may lead to discovery of appropriate preventive measures, and the development of rational therapeutic measures and medication against such infections and diseases. This review, a state-of-the-art updated scenario of host-pathogen interaction research, has been done by keeping in mind this urgency. It covers the biological and computational aspects of host-pathogen interactions, classification of the methods by which the pathogens interact with their hosts, different machine learning techniques for prediction of host-pathogen interactions, and future scopes of this research field.",2016-10-01,8,1632,69,1680
2775,27718010,"From ""ear"" to there: a review of biorobotic models of auditory processing in lizards","The peripheral auditory system of lizards has been extensively studied, because of its remarkable directionality. In this paper, we review the research that has been performed on this system using a biorobotic approach. The various robotic implementations developed to date, both wheeled and legged, of the auditory model exhibit strong phonotactic performance for two types of steering mechanisms-a simple threshold decision model and Braitenberg sensorimotor cross-couplings. The Braitenberg approach removed the need for a decision model, but produced relatively inefficient robot trajectories. Introducing various asymmetries in the auditory model reduced the efficiency of the robot trajectories, but successful phonotaxis was maintained. Relatively loud noise distractors degraded the trajectory efficiency and above-threshold noise resulted in unsuccessful phonotaxis. Machine learning techniques were applied to successfully compensate for asymmetries as well as noise distractors. Such techniques were also successfully used to construct a representation of auditory space, which is crucial for sound localisation while remaining stationary as opposed to phonotaxis-based localisation. The peripheral auditory model was furthermore found to adhere to an auditory scaling law governing the variation in frequency response with respect to physical ear separation. Overall, the research to date paves the way towards investigating the more fundamental topic of auditory metres versus auditory maps, and the existing robotic implementations can act as tools to compare the two approaches.",2016-10-01,5,1593,84,1680
1134,27528421,Computer-aided diagnosis of breast cancer using cytological images: A systematic review,"Cytological evaluation by microscopic image-based characterization [imprint cytology (IC) and fine needle aspiration cytology (FNAC)] plays an integral role in primary screening/detection of breast cancer. The sensitivity of IC and FNAC as a screening tool is dependent on the image quality and the pathologist's level of expertise. Computer-aided diagnosis (CAD) is used to assists the pathologists by developing various machine learning and image processing algorithms. This study reviews the various manual and computer-aided techniques used so far in breast cytology. Diagnostic applications were studied to estimate the role of CAD in breast cancer diagnosis. This paper presents an overview of image processing and pattern recognition techniques that have been used to address several issues in breast cytology-based CAD including slide preparation, staining, microscopic imaging, pre-processing, segmentation, feature extraction and diagnostic classification. This review provides better insights to readers regarding the state of the art the knowledge on CAD-based breast cancer diagnosis to date.",2016-10-01,6,1105,87,1680
1129,27568202,Connectivity Changes in Parkinson's Disease,"Parkinson's disease (PD) is a chronic and progressive movement disorder of the central nervous system characterized by widespread alterations in several non-motor aspects such as mood, sleep, olfactory, and cognition in addition to motor dysfunctions. Advanced neuroimaging using functional connectivity reconstruction of the human brain has provided a vast knowledge on the pathophysiological mechanisms underlying this disorder, but this, however, does not cover the overall inter-/intra-individual variability of PD phenotypes. The present review is aimed at discussing to what extent the evidence provided by group-based neuroimaging analysis in this field of study (using seed-based, network-based, or graph theory approaches) may be generalized. In particular, we summarized the literature on the application of resting-state functional connectivity studies to explore different neural correlates of motor and non-motor symptoms of PD and the neural mechanisms involved in treatment effects: effects of levodopa or deep brain stimulation. The lesson learnt from one decade of studies provides consistent evidence on the role of the altered communication between the striato-frontal pathways as a marker of PD-related motor degeneration, whereas in the non-motor domain, several missing pieces of a complex puzzle are provided. However, the main target is to present a new era of intelligent neuroimaging applications, where automated multivariate analysis of functional connectivity data may be used for moving from group-level statistical results to personalized predictions in a clinical setting. Although in its relative infancy, the evidence gathered so far suggests a new era of clinical neuroimaging is starting.",2016-10-01,15,1724,43,1680
1150,27435734,Walking through the statistical black boxes of plant breeding,"The main statistical procedures in plant breeding are based on Gaussian process and can be computed through mixed linear models. Intelligent decision making relies on our ability to extract useful information from data to help us achieve our goals more efficiently. Many plant breeders and geneticists perform statistical analyses without understanding the underlying assumptions of the methods or their strengths and pitfalls. In other words, they treat these statistical methods (software and programs) like black boxes. Black boxes represent complex pieces of machinery with contents that are not fully understood by the user. The user sees the inputs and outputs without knowing how the outputs are generated. By providing a general background on statistical methodologies, this review aims (1) to introduce basic concepts of machine learning and its applications to plant breeding; (2) to link classical selection theory to current statistical approaches; (3) to show how to solve mixed models and extend their application to pedigree-based and genomic-based prediction; and (4) to clarify how the algorithms of genome-wide association studies work, including their assumptions and limitations.",2016-10-01,6,1199,61,1680
1163,27306552,Transforming the care of atrial fibrillation with mobile health,"Atrial fibrillation (AF) is a multifaceted and highly variable disease that is often difficult to manage within the traditional health-care model. The conventional model of regular or pre-scheduled appointments with physicians or allied health professionals is poorly suited to the unpredictable and often urgent clinical needs of patients with AF. Mobile health (mHealth) has the potential to dramatically transform the delivery and quality of AF care. In this brief review, we summarize the current limitations and evidence gaps in treating patients with AF. We then describe the current mHealth landscape, changes in telehealth coverage and reimbursement, and recent technological advances of smartphones, mobile applications, and connected wearable devices. We also describe important barriers and challenges, such as clinical management of large volumes of data, application of predictive analytics/machine learning, and the need for high-quality randomized clinical trials.",2016-10-01,16,979,63,1680
2774,27479316,Molecular interaction fingerprint approaches for GPCR drug discovery,"Protein-ligand interaction fingerprints (IFPs) are binary 1D representations of the 3D structure of protein-ligand complexes encoding the presence or absence of specific interactions between the binding pocket amino acids and the ligand. Various implementations of IFPs have been developed and successfully applied for post-processing molecular docking results for G Protein-Coupled Receptor (GPCR) ligand binding mode prediction and virtual ligand screening. Novel interaction fingerprint methods enable structural chemogenomics and polypharmacology predictions by complementing the increasing amount of GPCR structural data. Machine learning methods are increasingly used to derive relationships between bioactivity data and fingerprint descriptors of chemical and structural information of binding sites, ligands, and protein-ligand interactions. Factors that influence the application of IFPs include structure preparation, binding site definition, fingerprint similarity assessment, and data processing and these factors pose challenges as well possibilities to optimize interaction fingerprint methods for GPCR drug discovery.",2016-10-01,13,1132,68,1680
1157,27349830,Cardiac image modelling: Breadth and depth in heart disease,"With the advent of large-scale imaging studies and big health data, and the corresponding growth in analytics, machine learning and computational image analysis methods, there are now exciting opportunities for deepening our understanding of the mechanisms and characteristics of heart disease. Two emerging fields are computational analysis of cardiac remodelling (shape and motion changes due to disease) and computational analysis of physiology and mechanics to estimate biophysical properties from non-invasive imaging. Many large cohort studies now underway around the world have been specifically designed based on non-invasive imaging technologies in order to gain new information about the development of heart disease from asymptomatic to clinical manifestations. These give an unprecedented breadth to the quantification of population variation and disease development. Also, for the individual patient, it is now possible to determine biophysical properties of myocardial tissue in health and disease by interpreting detailed imaging data using computational modelling. For these population and patient-specific computational modelling methods to develop further, we need open benchmarks for algorithm comparison and validation, open sharing of data and algorithms, and demonstration of clinical efficacy in patient management and care. The combination of population and patient-specific modelling will give new insights into the mechanisms of cardiac disease, in particular the development of heart failure, congenital heart disease, myocardial infarction, contractile dysfunction and diastolic dysfunction.",2016-10-01,10,1619,59,1680
2780,27656787,Artificial consciousness and the consciousness-attention dissociation,"Artificial Intelligence is at a turning point, with a substantial increase in projects aiming to implement sophisticated forms of human intelligence in machines. This research attempts to model specific forms of intelligence through brute-force search heuristics and also reproduce features of human perception and cognition, including emotions. Such goals have implications for artificial consciousness, with some arguing that it will be achievable once we overcome short-term engineering challenges. We believe, however, that phenomenal consciousness cannot be implemented in machines. This becomes clear when considering emotions and examining the dissociation between consciousness and attention in humans. While we may be able to program ethical behavior based on rules and machine learning, we will never be able to reproduce emotions or empathy by programming such control systems-these will be merely simulations. Arguments in favor of this claim include considerations about evolution, the neuropsychological aspects of emotions, and the dissociation between attention and consciousness found in humans. Ultimately, we are far from achieving artificial consciousness.",2016-10-01,3,1176,69,1680
2703,28263938,Analysis of Machine Learning Techniques for Heart Failure Readmissions,"Background:                    The current ability to predict readmissions in patients with heart failure is modest at best. It is unclear whether machine learning techniques that address higher dimensional, nonlinear relationships among variables would enhance prediction. We sought to compare the effectiveness of several machine learning algorithms for predicting readmissions.              Methods and results:                    Using data from the Telemonitoring to Improve Heart Failure Outcomes trial, we compared the effectiveness of random forests, boosting, random forests combined hierarchically with support vector machines or logistic regression (LR), and Poisson regression against traditional LR to predict 30- and 180-day all-cause readmissions and readmissions because of heart failure. We randomly selected 50% of patients for a derivation set, and a validation set comprised the remaining patients, validated using 100 bootstrapped iterations. We compared C statistics for discrimination and distributions of observed outcomes in risk deciles for predictive range. In 30-day all-cause readmission prediction, the best performing machine learning model, random forests, provided a 17.8% improvement over LR (mean C statistics, 0.628 and 0.533, respectively). For readmissions because of heart failure, boosting improved the C statistic by 24.9% over LR (mean C statistic 0.678 and 0.543, respectively). For 30-day all-cause readmission, the observed readmission rates in the lowest and highest deciles of predicted risk with random forests (7.8% and 26.2%, respectively) showed a much wider separation than LR (14.2% and 16.4%, respectively).              Conclusions:                    Machine learning methods improved the prediction of readmission after hospitalization for heart failure compared with LR and provided the greatest predictive range in observed readmission rates.",2016-11-01,56,1901,70,1649
2750,27917107,The Berlin Brain-Computer Interface: Progress Beyond Communication and Control,"The combined effect of fundamental results about neurocognitive processes and advancements in decoding mental states from ongoing brain signals has brought forth a whole range of potential neurotechnological applications. In this article, we review our developments in this area and put them into perspective. These examples cover a wide range of maturity levels with respect to their applicability. While we assume we are still a long way away from integrating Brain-Computer Interface (BCI) technology in general interaction with computers, or from implementing neurotechnological measures in safety-critical workplaces, results have already now been obtained involving a BCI as research tool. In this article, we discuss the reasons why, in some of the prospective application domains, considerable effort is still required to make the systems ready to deal with the full complexity of the real world.",2016-11-01,39,904,78,1649
2762,27830251,Biomechanisms of Comorbidity: Reviewing Integrative Analyses of Multi-omics Datasets and Electronic Health Records,"Objectives:                    Disease comorbidity is a pervasive phenomenon impacting patients' health outcomes, disease management, and clinical decisions. This review presents past, current and future research directions leveraging both phenotypic and molecular information to uncover disease similarity underpinning the biology and etiology of disease comorbidity.              Methods:                    We retrieved ~130 publications and retained 59, ranging from 2006 to 2015, that comprise a minimum number of five diseases and at least one type of biomolecule. We surveyed their methods, disease similarity metrics, and calculation of comorbidities in the electronic health records, if present.              Results:                    Among the surveyed studies, 44% generated or validated disease similarity metrics in context of comorbidity, with 60% being published in the last two years. As inputs, 87% of studies utilized intragenic loci and proteins while 13% employed RNA (mRNA, LncRNA or miRNA). Network modeling was predominantly used (35%) followed by statistics (28%) to impute similarity between these biomolecules and diseases. Studies with large numbers of biomolecules and diseases used network models or nave overlap of disease-molecule associations, while machine learning, statistics, and information retrieval were utilized in smaller and moderate sized studies. Multiscale computations comprising shared function, network topology, and phenotypes were performed exclusively on proteins.              Conclusion:                    This review highlighted the growing methods for identifying the molecular mechanisms underpinning comorbidities that leverage multiscale molecular information and patterns from electronic health records. The survey unveiled that intergenic polymorphisms have been overlooked for similarity imputation compared to their intragenic counterparts, offering new opportunities to bridge the mechanistic and similarity gaps of comorbidity.",2016-11-01,2,1995,114,1649
2777,27698035,Big data need big theory too,"The current interest in big data, machine learning and data analytics has generated the widespread impression that such methods are capable of solving most problems without the need for conventional scientific methods of inquiry. Interest in these methods is intensifying, accelerated by the ease with which digitized data can be acquired in virtually all fields of endeavour, from science, healthcare and cybersecurity to economics, social sciences and the humanities. In multiscale modelling, machine learning appears to provide a shortcut to reveal correlations of arbitrary complexity between processes at the atomic, molecular, meso- and macroscales. Here, we point out the weaknesses of pure big data approaches with particular focus on biology and medicine, which fail to provide conceptual accounts for the processes to which they are applied. No matter their 'depth' and the sophistication of data-driven methods, such as artificial neural nets, in the end they merely fit curves to existing data. Not only do these methods invariably require far larger quantities of data than anticipated by big data aficionados in order to produce statistically reliable results, but they can also fail in circumstances beyond the range of the data used to train them because they are not designed to model the structural characteristics of the underlying system. We argue that it is vital to use theory as a guide to experimental design for maximal efficiency of data collection and to produce reliable predictive models and conceptual knowledge. Rather than continuing to fund, pursue and promote 'blind' big data projects with massive budgets, we call for more funding to be allocated to the elucidation of the multiscale and stochastic processes controlling the behaviour of complex systems, including those of life, medicine and healthcare.This article is part of the themed issue 'Multiscale modelling at the physics-chemistry-biology interface'.",2016-11-01,28,1947,28,1649
2748,27942354,"Heart Failure: Diagnosis, Severity Estimation and Prediction of Adverse Events Through Machine Learning Techniques","Heart failure is a serious condition with high prevalence (about 2% in the adult population in developed countries, and more than 8% in patients older than 75 years). About 3-5% of hospital admissions are linked with heart failure incidents. Heart failure is the first cause of admission by healthcare professionals in their clinical practice. The costs are very high, reaching up to 2% of the total health costs in the developed countries. Building an effective disease management strategy requires analysis of large amount of data, early detection of the disease, assessment of the severity and early prediction of adverse events. This will inhibit the progression of the disease, will improve the quality of life of the patients and will reduce the associated medical costs. Toward this direction machine learning techniques have been employed. The aim of this paper is to present the state-of-the-art of the machine learning methodologies applied for the assessment of heart failure. More specifically, models predicting the presence, estimating the subtype, assessing the severity of heart failure and predicting the presence of adverse events, such as destabilizations, re-hospitalizations, and mortality are presented. According to the authors' knowledge, it is the first time that such a comprehensive review, focusing on all aspects of the management of heart failure, is presented.",2016-11-01,17,1391,114,1649
1124,27599991,The Next Era: Deep Learning in Pharmaceutical Research,"Over the past decade we have witnessed the increasing sophistication of machine learning algorithms applied in daily use from internet searches, voice recognition, social network software to machine vision software in cameras, phones, robots and self-driving cars. Pharmaceutical research has also seen its fair share of machine learning developments. For example, applying such methods to mine the growing datasets that are created in drug discovery not only enables us to learn from the past but to predict a molecule's properties and behavior in future. The latest machine learning algorithm garnering significant attention is deep learning, which is an artificial neural network with multiple hidden layers. Publications over the last 3 years suggest that this algorithm may have advantages over previous machine learning methods and offer a slight but discernable edge in predictive performance. The time has come for a balanced review of this technique but also to apply machine learning methods such as deep learning across a wider array of endpoints relevant to pharmaceutical research for which the datasets are growing such as physicochemical property prediction, formulation prediction, absorption, distribution, metabolism, excretion and toxicity (ADME/Tox), target prediction and skin permeation, etc. We also show that there are many potential applications of deep learning beyond cheminformatics. It will be important to perform prospective testing (which has been carried out rarely to date) in order to convince skeptics that there will be benefits from investing in this technique.",2016-11-01,43,1599,54,1649
2772,27241666,Big Data and machine learning in radiation oncology: State of the art and future prospects,"Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.",2016-11-01,54,668,90,1649
2769,30645788,The Diagnostic Imagination in Radiology: Part 1,"*Machines that dream, the restless impulse for technical change that has marked radiology from its beginning and forays into deep neural networks, will no doubt unsettle long-held institu- tional practices in radiology. *A willingness to collaborate and puzzle through machine intelligence has come from those who have not accepted the status quo. A certain form of scientific curiosity has been a guiding principle in their work. *In radiology, machine intelligence has been extremely useful and built into just about every major technical innovation. But it has only been the last several years that a subfield of Al, machine learning, has begun to show remarkably fast development due to faster comput- er processing capabilities and advanced modeling and results emerging from the application of deep learning.",2016-11-01,0,814,47,1649
2771,27452181,Clinical chemistry in higher dimensions: Machine-learning and enhanced prediction from routine clinical chemistry data,"Big Data is having an impact on many areas of research, not the least of which is biomedical science. In this review paper, big data and machine learning are defined in terms accessible to the clinical chemistry community. Seven myths associated with machine learning and big data are then presented, with the aim of managing expectation of machine learning amongst clinical chemists. The myths are illustrated with four examples investigating the relationship between biomarkers in liver function tests, enhanced laboratory prediction of hepatitis virus infection, the relationship between bilirubin and white cell count, and the relationship between red cell distribution width and laboratory prediction of anaemia.",2016-11-01,3,717,118,1649
1591,26634919,Progress and challenges in bioinformatics approaches for enhancer identification,"Enhancers are cis-acting DNA elements that play critical roles in distal regulation of gene expression. Identifying enhancers is an important step for understanding distinct gene expression programs that may reflect normal and pathogenic cellular conditions. Experimental identification of enhancers is constrained by the set of conditions used in the experiment. This requires multiple experiments to identify enhancers, as they can be active under specific cellular conditions but not in different cell types/tissues or cellular states. This has opened prospects for computational prediction methods that can be used for high-throughput identification of putative enhancers to complement experimental approaches. Potential functions and properties of predicted enhancers have been catalogued and summarized in several enhancer-oriented databases. Because the current methods for the computational prediction of enhancers produce significantly different enhancer predictions, it will be beneficial for the research community to have an overview of the strategies and solutions developed in this field. In this review, we focus on the identification and analysis of enhancers by bioinformatics approaches. First, we describe a general framework for computational identification of enhancers, present relevant data types and discuss possible computational solutions. Next, we cover over 30 existing computational enhancer identification methods that were developed since 2000. Our review highlights advantages, limitations and potentials, while suggesting pragmatic guidelines for development of more efficient computational enhancer prediction methods. Finally, we discuss challenges and open problems of this topic, which require further consideration.",2016-11-01,25,1753,80,1649
2773,26670115,Machine learning and new vital signs monitoring in civilian en route care: A systematic review of the literature and future implications for the military,"Background:                    Although air transport medical services are today an integral part of trauma systems in most developed countries, to date, there are no reviews on recent innovations in civilian en route care. The purpose of this systematic review was to identify potential machine learning and new vital signs monitoring technologies in civilian en route care that could help close civilian and military capability gaps in monitoring and the early detection and treatment of various trauma injuries.              Methods:                    MEDLINE, the Cochrane Database of Systematic Reviews, and citation review of relevant primary and review articles were searched for studies involving civilian en route care, air medical transport, and technologies from January 2005 to November 2015. Data were abstracted on study design, population, year, sponsors, innovation category, details of technologies, and outcomes.              Results:                    Thirteen observational studies involving civilian medical transport met inclusion criteria. Studies either focused on machine learning and software algorithms (n = 5), new vital signs monitoring (n = 6), or both (n = 2). Innovations involved continuous digital acquisition of physiologic data and parameter extraction. Importantly, all studies (n = 13) demonstrated improved outcomes where applicable and potential use during civilian and military en route care. However, almost all studies required further validation in prospective and/or randomized controlled trials.              Conclusion:                    Potential machine learning technologies and monitoring of novel vital signs such as heart rate variability and complexity in civilian en route care could help enhance en route care for our nation's war fighters. In a complex global environment, they could potentially fill capability gaps such as monitoring and the early detection and treatment of various trauma injuries. However, the impact of these innovations and technologies will require further validation before widespread acceptance and prehospital use.              Level of evidence:                    Systematic review, level V.",2016-11-01,0,2180,153,1649
2776,27698038,A perspective on bridging scales and design of models using low-dimensional manifolds and data-driven model inference,"Systems in nature capable of collective behaviour are nonlinear, operating across several scales. Yet our ability to account for their collective dynamics differs in physics, chemistry and biology. Here, we briefly review the similarities and differences between mathematical modelling of adaptive living systems versus physico-chemical systems. We find that physics-based chemistry modelling and computational neuroscience have a shared interest in developing techniques for model reductions aiming at the identification of a reduced subsystem or slow manifold, capturing the effective dynamics. By contrast, as relations and kinetics between biological molecules are less characterized, current quantitative analysis under the umbrella of bioinformatics focuses on signal extraction, correlation, regression and machine-learning analysis. We argue that model reduction analysis and the ensuing identification of manifolds bridges physics and biology. Furthermore, modelling living systems presents deep challenges as how to reconcile rich molecular data with inherent modelling uncertainties (formalism, variables selection and model parameters). We anticipate a new generative data-driven modelling paradigm constrained by identified governing principles extracted from low-dimensional manifold analysis. The rise of a new generation of models will ultimately connect biology to quantitative mechanistic descriptions, thereby setting the stage for investigating the character of the model language and principles driving living systems.This article is part of the themed issue 'Multiscale modelling at the physics-chemistry-biology interface'.",2016-11-01,4,1646,117,1649
2737,28035989,"Neuroblastoma, a Paradigm for Big Data Science in Pediatric Oncology","Pediatric cancers rarely exhibit recurrent mutational events when compared to most adult cancers. This poses a challenge in understanding how cancers initiate, progress, and metastasize in early childhood. Also, due to limited detected driver mutations, it is difficult to benchmark key genes for drug development. In this review, we use neuroblastoma, a pediatric solid tumor of neural crest origin, as a paradigm for exploring ""big data"" applications in pediatric oncology. Computational strategies derived from big data science-network- and machine learning-based modeling and drug repositioning-hold the promise of shedding new light on the molecular mechanisms driving neuroblastoma pathogenesis and identifying potential therapeutics to combat this devastating disease. These strategies integrate robust data input, from genomic and transcriptomic studies, clinical data, and in vivo and in vitro experimental models specific to neuroblastoma and other types of cancers that closely mimic its biological characteristics. We discuss contexts in which ""big data"" and computational approaches, especially network-based modeling, may advance neuroblastoma research, describe currently available data and resources, and propose future models of strategic data collection and analyses for neuroblastoma and other related diseases.",2016-12-01,15,1330,68,1619
1137,27518905,Imaging and machine learning techniques for diagnosis of Alzheimer's disease,"Alzheimer's disease (AD) is a common health problem in elderly people. There has been considerable research toward the diagnosis and early detection of this disease in the past decade. The sensitivity of biomarkers and the accuracy of the detection techniques have been defined to be the key to an accurate diagnosis. This paper presents a state-of-the-art review of the research performed on the diagnosis of AD based on imaging and machine learning techniques. Different segmentation and machine learning techniques used for the diagnosis of AD are reviewed including thresholding, supervised and unsupervised learning, probabilistic techniques, Atlas-based approaches, and fusion of different image modalities. More recent and powerful classification techniques such as the enhanced probabilistic neural network of Ahmadlou and Adeli should be investigated with the goal of improving the diagnosis accuracy. A combination of different image modalities can help improve the diagnosis accuracy rate. Research is needed on the combination of modalities to discover multi-modal biomarkers.",2016-12-01,6,1088,76,1619
2719,28144341,Computational methods in drug discovery,"The process for drug discovery and development is challenging, time consuming and expensive. Computer-aided drug discovery (CADD) tools can act as a virtual shortcut, assisting in the expedition of this long process and potentially reducing the cost of research and development. Today CADD has become an effective and indispensable tool in therapeutic development. The human genome project has made available a substantial amount of sequence data that can be used in various drug discovery projects. Additionally, increasing knowledge of biological structures, as well as increasing computer power have made it possible to use computational methods effectively in various phases of the drug discovery and development pipeline. The importance of in silico tools is greater than ever before and has advanced pharmaceutical research. Here we present an overview of computational methods used in different facets of drug discovery and highlight some of the recent successes. In this review, both structure-based and ligand-based drug discovery methods are discussed. Advances in virtual high-throughput screening, protein structure prediction methods, protein-ligand docking, pharmacophore modeling and QSAR techniques are reviewed.",2016-12-01,72,1228,39,1619
2742,27999256,Recent Progress in Machine Learning-Based Methods for Protein Fold Recognition,"Knowledge on protein folding has a profound impact on understanding the heterogeneity and molecular function of proteins, further facilitating drug design. Predicting the 3D structure (fold) of a protein is a key problem in molecular biology. Determination of the fold of a protein mainly relies on molecular experimental methods. With the development of next-generation sequencing techniques, the discovery of new protein sequences has been rapidly increasing. With such a great number of proteins, the use of experimental techniques to determine protein folding is extremely difficult because these techniques are time consuming and expensive. Thus, developing computational prediction methods that can automatically, rapidly, and accurately classify unknown protein sequences into specific fold categories is urgently needed. Computational recognition of protein folds has been a recent research hotspot in bioinformatics and computational biology. Many computational efforts have been made, generating a variety of computational prediction methods. In this review, we conduct a comprehensive survey of recent computational methods, especially machine learning-based methods, for protein fold recognition. This review is anticipated to assist researchers in their pursuit to systematically understand the computational recognition of protein folds.",2016-12-01,15,1351,78,1619
1125,27592382,Feature selection methods for big data bioinformatics: A survey from the search perspective,"This paper surveys main principles of feature selection and their recent applications in big data bioinformatics. Instead of the commonly used categorization into filter, wrapper, and embedded approaches to feature selection, we formulate feature selection as a combinatorial optimization or search problem and categorize feature selection methods into exhaustive search, heuristic search, and hybrid methods, where heuristic search methods may further be categorized into those with or without data-distilled feature ranking measures.",2016-12-01,26,535,91,1619
2756,27863190,Human Papillomavirus Drives Tumor Development Throughout the Head and Neck: Improved Prognosis Is Associated With an Immune Response Largely Restricted to the Oropharynx,"Purpose In squamous cell carcinomas of the head and neck (HNSCC), the increasing incidence of oropharyngeal squamous cell carcinomas (OPSCCs) is attributable to human papillomavirus (HPV) infection. Despite commonly presenting at late stage, HPV-driven OPSCCs are associated with improved prognosis compared with HPV-negative disease. HPV DNA is also detectable in nonoropharyngeal (non-OPSCC), but its pathogenic role and clinical significance are unclear. The objectives of this study were to determine whether HPV plays a causal role in non-OPSCC and to investigate whether HPV confers a survival benefit in these tumors. Methods Meta-analysis was used to build a cross-tissue gene-expression signature for HPV-driven cancer. Classifiers trained by machine-learning approaches were used to predict the HPV status of 520 HNSCCs profiled by The Cancer Genome Atlas project. DNA methylation data were similarly used to classify 464 HNSCCs and these analyses were integrated with genomic, histopathology, and survival data to permit a comprehensive comparison of HPV transcript-positive OPSCC and non-OPSCC. Results HPV-driven tumors accounted for 4.1% of non-OPSCCs. Regardless of anatomic site, HPV+ HNSCCs shared highly similar gene expression and DNA methylation profiles; nonkeratinizing, basaloid histopathological features; and lack of TP53 or CDKN2A alterations. Improved overall survival, however, was largely restricted to HPV-driven OPSCCs, which were associated with increased levels of tumor-infiltrating lymphocytes compared with HPV-driven non-OPSCCs. Conclusion Our analysis identified a causal role for HPV in transcript-positive non-OPSCCs throughout the head and neck. Notably, however, HPV-driven non-OPSCCs display a distinct immune microenvironment and clinical behavior compared with HPV-driven OPSCCs.",2016-12-01,43,1824,169,1619
2755,27870246,Materials Informatics: Statistical Modeling in Material Science,"Material informatics is engaged with the application of informatic principles to materials science in order to assist in the discovery and development of new materials. Central to the field is the application of data mining techniques and in particular machine learning approaches, often referred to as Quantitative Structure Activity Relationship (QSAR) modeling, to derive predictive models for a variety of materials-related ""activities"". Such models can accelerate the development of new materials with favorable properties and provide insight into the factors governing these properties. Here we provide a comparison between medicinal chemistry/drug design and materials-related QSAR modeling and highlight the importance of developing new, materials-specific descriptors. We survey some of the most recent QSAR models developed in materials science with focus on energetic materials and on solar cells. Finally we present new examples of material-informatic analyses of solar cells libraries produced from metal oxides using combinatorial material synthesis. Different analyses lead to interesting physical insights as well as to the design of new cells with potentially improved photovoltaic parameters.",2016-12-01,1,1210,63,1619
309,28830106,Digital Pharmacovigilance and Disease Surveillance: Combining Traditional and Big-Data Systems for Better Public Health,"The digital revolution has contributed to very large data sets (ie, big data) relevant for public health. The two major data sources are electronic health records from traditional health systems and patient-generated data. As the two data sources have complementary strengths-high veracity in the data from traditional sources and high velocity and variety in patient-generated data-they can be combined to build more-robust public health systems. However, they also have unique challenges. Patient-generated data in particular are often completely unstructured and highly context dependent, posing essentially a machine-learning challenge. Some recent examples from infectious disease surveillance and adverse drug event monitoring demonstrate that the technical challenges can be solved. Despite these advances, the problem of verification remains, and unless traditional and digital epidemiologic approaches are combined, these data sources will be constrained by their intrinsic limits.",2016-12-01,12,990,119,1619
1183,27039698,Using neuroimaging to help predict the onset of psychosis,"The aim of this review is to assess the potential for neuroimaging measures to facilitate prediction of the onset of psychosis. Research in this field has mainly involved people at 'ultra-high risk' (UHR) of psychosis, who have a very high risk of developing a psychotic disorder within a few years of presentation to mental health services. The review details the key findings and developments in this area to date and examines the methodological and logistical challenges associated with making predictions in an individual subject in a clinical setting.",2017-01-01,11,556,57,1588
2722,28133810,Systems serology for evaluation of HIV vaccine trials,"The scale and scope of the global epidemic, coupled to challenges with traditional vaccine development approaches, point toward a need for novel methodologies for HIV vaccine research. While the development of vaccines able to induce broadly neutralizing antibodies remains the ultimate goal, to date, vaccines continue to fail to induce these rare humoral immune responses. Conversely, growing evidence across vaccine platforms in both non-human primates and humans points to a role for polyclonal vaccine-induced antibody responses in protection from infection. These candidate vaccines, despite employing disparate viral vectors and immunization strategies, consistently identify a role for functional or non-traditional antibody activities as correlates of immunity. However, the precise mechanism(s) of action of these ""binding"" antibodies, their specific characteristics, and their ability to be selectively induced and/or potentiated to result in complete protection merits parallel investigation to neutralizing antibody-based vaccine design approaches. Ultimately, while neutralizing and functional antibody-based vaccine strategies need not be mutually exclusive, defining the specific characteristics of ""protective"" functional antibodies may provide a target immune profile to potentially induce more robust immunity against HIV. Specifically, one approach to guide the development of functional antibody-based vaccine strategies, termed ""systems serology"", offers an unbiased and comprehensive approach to systematically survey humoral immune responses, capturing the array of functions and humoral response characteristics that may be induced following vaccination with high resolution. Coupled to machine learning tools, large datasets that explore the ""antibody-ome"" offer a means to step back from anticipated correlates and mechanisms of protection and toward a more fundamental understanding of coordinated aspects of humoral immune responses, to more globally differentiate among vaccine candidates, and most critically, to identify the features of humoral immunity that distinguish protective from non-protective responses. Overall, the systematic serological approach described here aimed at broadly capturing the enormous biodiversity in antibody profiles that may emerge following vaccination, complements the existing cutting edge tools in the cellular immunology space that survey vaccine-induced polyfunctional cellular activity by flow cytometry, transcriptional profiling, epigenetic, and metabolomic analysis to offer a means to develop both a more nuanced and a more complete understanding of correlates of protection to support the design of functional vaccine strategies.",2017-01-01,34,2704,53,1588
2721,28138367,Machine Learning and Data Mining Methods in Diabetes Research,"The remarkable advances in biotechnology and health sciences have led to a significant production of data, such as high throughput genetic data and clinical information, generated from large Electronic Health Records (EHRs). To this end, application of machine learning and data mining methods in biosciences is presently, more than ever before, vital and indispensable in efforts to transform intelligently all available information into valuable knowledge. Diabetes mellitus (DM) is defined as a group of metabolic disorders exerting significant pressure on human health worldwide. Extensive research in all aspects of diabetes (diagnosis, etiopathophysiology, therapy, etc.) has led to the generation of huge amounts of data. The aim of the present study is to conduct a systematic review of the applications of machine learning, data mining techniques and tools in the field of diabetes research with respect to a) Prediction and Diagnosis, b) Diabetic Complications, c) Genetic Background and Environment, and e) Health Care and Management with the first category appearing to be the most popular. A wide range of machine learning algorithms were employed. In general, 85% of those used were characterized by supervised learning approaches and 15% by unsupervised ones, and more specifically, association rules. Support vector machines (SVM) arise as the most successful and widely used algorithm. Concerning the type of data, clinical datasets were mainly used. The title applications in the selected articles project the usefulness of extracting valuable knowledge leading to new hypotheses targeting deeper understanding and further investigation in DM.",2017-01-01,78,1661,61,1588
1593,26592808,Group-regularized individual prediction: theory and application to pain,"Multivariate pattern analysis (MVPA) has become an important tool for identifying brain representations of psychological processes and clinical outcomes using fMRI and related methods. Such methods can be used to predict or 'decode' psychological states in individual subjects. Single-subject MVPA approaches, however, are limited by the amount and quality of individual-subject data. In spite of higher spatial resolution, predictive accuracy from single-subject data often does not exceed what can be accomplished using coarser, group-level maps, because single-subject patterns are trained on limited amounts of often-noisy data. Here, we present a method that combines population-level priors, in the form of biomarker patterns developed on prior samples, with single-subject MVPA maps to improve single-subject prediction. Theoretical results and simulations motivate a weighting based on the relative variances of biomarker-based prediction-based on population-level predictive maps from prior groups-and individual-subject, cross-validated prediction. Empirical results predicting pain using brain activity on a trial-by-trial basis (single-trial prediction) across 6 studies (N=180 participants) confirm the theoretical predictions. Regularization based on a population-level biomarker-in this case, the Neurologic Pain Signature (NPS)-improved single-subject prediction accuracy compared with idiographic maps based on the individuals' data alone. The regularization scheme that we propose, which we term group-regularized individual prediction (GRIP), can be applied broadly to within-person MVPA-based prediction. We also show how GRIP can be used to evaluate data quality and provide benchmarks for the appropriateness of population-level maps like the NPS for a given individual or study.",2017-01-01,22,1801,71,1588
2720,28141576,Mathematical modelling of the electric sense of fish: the role of multi-frequency measurements and movement,"Understanding active electrolocation in weakly electric fish remains a challenging issue. In this article we propose a mathematical formulation of this problem, in terms of partial differential equations. This allows us to detail two algorithms: one for localizing a target using the multi-frequency aspect of the signal, and another one for identifying the shape of this target. Shape recognition is designed in a machine learning point of view, and takes advantage of both the multi-frequency setup and the movement of the fish around its prey. Numerical simulations are shown for the computation of the electric field emitted and sensed by the fish; they are then used as an input for the two algorithms.",2017-01-01,1,707,107,1588
1201,26873661,"The recent progress in proteochemometric modelling: focusing on target descriptors, cross-term descriptors and application scope","As an extension of the conventional quantitative structure activity relationship models, proteochemometric (PCM) modelling is a computational method that can predict the bioactivity relations between multiple ligands and multiple targets. Traditional PCM modelling includes three essential elements: descriptors (including target descriptors, ligand descriptors and cross-term descriptors), bioactivity data and appropriate learning functions that link the descriptors to the bioactivity data. Since its appearance, PCM modelling has developed rapidly over the past decade by taking advantage of the progress of different descriptors and machine learning techniques, along with the increasing amounts of available bioactivity data. Specifically, the new emerging target descriptors and cross-term descriptors not only significantly increased the performance of PCM modelling but also expanded its application scope from traditional protein-ligand interaction to more abundant interactions, including protein-peptide, protein-DNA and even protein-protein interactions. In this review, target descriptors and cross-term descriptors, as well as the corresponding application scope, are intensively summarized. Additionally, we look forward to seeing PCM modelling extend into new application scopes, such as Target-Catalyst-Ligand systems, with the further development of descriptors, machine learning techniques and increasing amounts of available bioactivity data.",2017-01-01,9,1463,128,1588
2724,28127429,Machine learning and systems genomics approaches for multi-omics data,"In light of recent advances in biomedical computing, big data science, and precision medicine, there is a mammoth demand for establishing algorithms in machine learning and systems genomics (MLSG), together with multi-omics data, to weigh probable phenotype-genotype relationships. Software frameworks in MLSG are extensively employed to analyze hundreds of thousands of multi-omics data by high-throughput technologies. In this study, we reviewed the MLSG software frameworks and future directions with respect to multi-omics data analysis and integration. Our review was targeted at researching recent approaches and technical solutions for the MLSG software frameworks using multi-omics platforms.",2017-01-01,34,700,69,1588
1138,27502048,Ten problems and solutions when predicting individual outcome from lesion site after stroke,"In this paper, we consider solutions to ten of the challenges faced when trying to predict an individual's functional outcome after stroke on the basis of lesion site. A primary goal is to find lesion-outcome associations that are consistently observed in large populations of stroke patients because consistent associations maximise confidence in future individualised predictions. To understand and control multiple sources of inter-patient variability, we need to systematically investigate each contributing factor and how each factor depends on other factors. This requires very large cohorts of patients, who differ from one another in typical and measurable ways, including lesion site, lesion size, functional outcome and time post stroke (weeks to decades). These multivariate investigations are complex, particularly when the contributions of different variables interact with one another. Machine learning algorithms can help to identify the most influential variables and indicate dependencies between different factors. Multivariate lesion analyses are needed to understand how the effect of damage to one brain region depends on damage or preservation in other brain regions. Such data-led investigations can reveal predictive relationships between lesion site and outcome. However, to understand and improve the predictions we need explanatory models of the neural networks and degenerate pathways that support functions of interest. This will entail integrating the results of lesion analyses with those from functional imaging (fMRI, MEG), transcranial magnetic stimulation (TMS) and diffusor tensor imaging (DTI) studies of healthy participants and patients.",2017-01-01,28,1676,91,1588
1214,26690804,"""Look at my classifier's result"": Disentangling unresponsive from (minimally) conscious patients","Given the fact that clinical bedside examinations can have a high rate of misdiagnosis, machine learning techniques based on neuroimaging and electrophysiological measurements are increasingly being considered for comatose patients and patients with unresponsive wakefulness syndrome, a minimally conscious state or locked-in syndrome. Machine learning techniques have the potential to move from group-level statistical results to personalized predictions in a clinical setting. They have been applied for the purpose of (1) detecting changes in brain activation during functional tasks, equivalent to a behavioral command-following test and (2) estimating signs of consciousness by analyzing measurement data obtained from multiple subjects in resting state. In this review, we provide a comprehensive overview of the literature on both approaches and discuss the translation of present findings to clinical practice. We found that most studies struggle with the difficulty of establishing a reliable behavioral assessment and fluctuations in the patient's levels of arousal. Both these factors affect the training and validation of machine learning methods to a considerable degree. In studies involving more than 50 patients, small to moderate evidence was found for the presence of signs of consciousness or good outcome, where one study even showed strong evidence for good outcome.",2017-01-01,14,1387,96,1588
2785,27641093,"Computational inference of gene regulatory networks: Approaches, limitations and opportunities","Gene regulatory networks lie at the core of cell function control. In E. coli and S. cerevisiae, the study of gene regulatory networks has led to the discovery of regulatory mechanisms responsible for the control of cell growth, differentiation and responses to environmental stimuli. In plants, computational rendering of gene regulatory networks is gaining momentum, thanks to the recent availability of high-quality genomes and transcriptomes and development of computational network inference approaches. Here, we review current techniques, challenges and trends in gene regulatory network inference and highlight challenges and opportunities for plant science. We provide plant-specific application examples to guide researchers in selecting methodologies that suit their particular research questions. Given the interdisciplinary nature of gene regulatory network inference, we tried to cater to both biologists and computer scientists to help them engage in a dialogue about concepts and caveats in network inference. Specifically, we discuss problems and opportunities in heterogeneous data integration for eukaryotic organisms and common caveats to be considered during network model evaluation. This article is part of a Special Issue entitled: Plant Gene Regulatory Mechanisms and Networks, edited by Dr. Erich Grotewold and Dr. Nathan Springer.",2017-01-01,17,1356,94,1588
2718,28144827,Biomarkers for Musculoskeletal Pain Conditions: Use of Brain Imaging and Machine Learning,"Chronic musculoskeletal pain condition often shows poor correlations between tissue abnormalities and clinical pain. Therefore, classification of pain conditions like chronic low back pain, osteoarthritis, and fibromyalgia depends mostly on self report and less on objective findings like X-ray or magnetic resonance imaging (MRI) changes. However, recent advances in structural and functional brain imaging have identified brain abnormalities in chronic pain conditions that can be used for illness classification. Because the analysis of complex and multivariate brain imaging data is challenging, machine learning techniques have been increasingly utilized for this purpose. The goal of machine learning is to train specific classifiers to best identify variables of interest on brain MRIs (i.e., biomarkers). This report describes classification techniques capable of separating MRI-based brain biomarkers of chronic pain patients from healthy controls with high accuracy (70-92%) using machine learning, as well as critical scientific, practical, and ethical considerations related to their potential clinical application. Although self-report remains the gold standard for pain assessment, machine learning may aid in the classification of chronic pain disorders like chronic back pain and fibromyalgia as well as provide mechanistic information regarding their neural correlates.",2017-01-01,9,1386,89,1588
2749,27940887,Machine learning and computer vision approaches for phenotypic profiling,"With recent advances in high-throughput, automated microscopy, there has been an increased demand for effective computational strategies to analyze large-scale, image-based data. To this end, computer vision approaches have been applied to cell segmentation and feature extraction, whereas machine-learning approaches have been developed to aid in phenotypic classification and clustering of data acquired from biological images. Here, we provide an overview of the commonly used computer vision and machine-learning methods for generating and categorizing phenotypic profiles, highlighting the general biological utility of each approach.",2017-01-01,31,639,72,1588
1158,27346545,Computational neuroimaging strategies for single patient predictions,"Neuroimaging increasingly exploits machine learning techniques in an attempt to achieve clinically relevant single-subject predictions. An alternative to machine learning, which tries to establish predictive links between features of the observed data and clinical variables, is the deployment of computational models for inferring on the (patho)physiological and cognitive mechanisms that generate behavioural and neuroimaging responses. This paper discusses the rationale behind a computational approach to neuroimaging-based single-subject inference, focusing on its potential for characterising disease mechanisms in individual subjects and mapping these characterisations to clinical predictions. Following an overview of two main approaches - Bayesian model selection and generative embedding - which can link computational models to individual predictions, we review how these methods accommodate heterogeneity in psychiatric and neurological spectrum disorders, help avoid erroneous interpretations of neuroimaging data, and establish a link between a mechanistic, model-based approach and the statistical perspectives afforded by machine learning.",2017-01-01,38,1156,68,1588
2753,27889391,Review of fall detection techniques: A data availability perspective,"A fall is an abnormal activity that occurs rarely; however, missing to identify falls can have serious health and safety implications on an individual. Due to the rarity of occurrence of falls, there may be insufficient or no training data available for them. Therefore, standard supervised machine learning methods may not be directly applied to handle this problem. In this paper, we present a taxonomy for the study of fall detection from the perspective of availability of fall data. The proposed taxonomy is independent of the type of sensors used and specific feature extraction/selection methods. The taxonomy identifies different categories of classification methods for the study of fall detection based on the availability of their data during training the classifiers. Then, we present a comprehensive literature review within those categories and identify the approach of treating a fall as an abnormal activity to be a plausible research direction. We conclude our paper by discussing several open research problems in the field and pointers for future research.",2017-01-01,16,1075,68,1588
2744,27979318,Molecular Diagnostics of Ageing and Tackling Age-related Disease,"As average life expectancy increases there is a greater focus on health-span and, in particular, how to treat or prevent chronic age-associated diseases. Therapies which were able to control 'biological age' with the aim of postponing chronic and costly diseases of old age require an entirely new approach to drug development. Molecular technologies and machine-learning methods have already yielded diagnostics that help guide cancer treatment and cardiovascular procedures. Discovery of valid and clinically informative diagnostics of human biological age (combined with disease-specific biomarkers) has the potential to alter current drug-discovery strategies, aid clinical trial recruitment and maximize healthy ageing. I will review some basic principles that govern the development of 'ageing' diagnostics, how such assays could be used during the drug-discovery or development process. Important logistical and statistical considerations are illustrated by reviewing recent biomarker activity in the field of Alzheimer's disease, as dementia represents the most pressing of priorities for the pharmaceutical industry, as well as the chronic disease in humans most associated with age.",2017-01-01,3,1192,64,1588
1156,27362387,Machine Learning Techniques in Clinical Vision Sciences,"This review presents and discusses the contribution of machine learning techniques for diagnosis and disease monitoring in the context of clinical vision science. Many ocular diseases leading to blindness can be halted or delayed when detected and treated at its earliest stages. With the recent developments in diagnostic devices, imaging and genomics, new sources of data for early disease detection and patients' management are now available. Machine learning techniques emerged in the biomedical sciences as clinical decision-support techniques to improve sensitivity and specificity of disease detection and monitoring, increasing objectively the clinical decision-making process. This manuscript presents a review in multimodal ocular disease diagnosis and monitoring based on machine learning approaches. In the first section, the technical issues related to the different machine learning approaches will be present. Machine learning techniques are used to automatically recognize complex patterns in a given dataset. These techniques allows creating homogeneous groups (unsupervised learning), or creating a classifier predicting group membership of new cases (supervised learning), when a group label is available for each case. To ensure a good performance of the machine learning techniques in a given dataset, all possible sources of bias should be removed or minimized. For that, the representativeness of the input dataset for the true population should be confirmed, the noise should be removed, the missing data should be treated and the data dimensionally (i.e., the number of parameters/features and the number of cases in the dataset) should be adjusted. The application of machine learning techniques in ocular disease diagnosis and monitoring will be presented and discussed in the second section of this manuscript. To show the clinical benefits of machine learning in clinical vision sciences, several examples will be presented in glaucoma, age-related macular degeneration, and diabetic retinopathy, these ocular pathologies being the major causes of irreversible visual impairment.",2017-01-01,20,2108,55,1588
2746,27966278,Computational resources and tools for antimicrobial peptides,"Antimicrobial peptides (AMPs), as evolutionarily conserved components of innate immune system, protect against pathogens including bacteria, fungi, viruses, and parasites. In general, AMPs are relatively small peptides (<10 kDa) with cationic nature and amphipathic structure and have modes of action different from traditional antibiotics. Up to now, there are more than 19 000 AMPs that have been reported, including those isolated from nature sources or by synthesis. They have been considered to be promising substitutes of conventional antibiotics in the quest to address the increasing occurrence of antibiotic resistance. However, most AMPs have modest direct antimicrobial activity, and their mechanisms of action, as well as their structure-activity relationships, are still poorly understood. Computational strategies are invaluable assets to provide insight into the activity of AMPs and thus exploit their potential as a new generation of antimicrobials. This article reviews the advances of AMP databases and computational tools for the prediction and design of new active AMPs. Copyright  2016 European Peptide Society and John Wiley & Sons, Ltd.",2017-01-01,12,1161,60,1588
2767,27814027,Harnessing Big Data for Systems Pharmacology,"Systems pharmacology aims to holistically understand mechanisms of drug actions to support drug discovery and clinical practice. Systems pharmacology modeling (SPM) is data driven. It integrates an exponentially growing amount of data at multiple scales (genetic, molecular, cellular, organismal, and environmental). The goal of SPM is to develop mechanistic or predictive multiscale models that are interpretable and actionable. The current explosions in genomics and other omics data, as well as the tremendous advances in big data technologies, have already enabled biologists to generate novel hypotheses and gain new knowledge through computational models of genome-wide, heterogeneous, and dynamic data sets. More work is needed to interpret and predict a drug response phenotype, which is dependent on many known and unknown factors. To gain a comprehensive understanding of drug actions, SPM requires close collaborations between domain experts from diverse fields and integration of heterogeneous models from biophysics, mathematics, statistics, machine learning, and semantic webs. This creates challenges in model management, model integration, model translation, and knowledge integration. In this review, we discuss several emergent issues in SPM and potential solutions using big data technology and analytics. The concurrent development of high-throughput techniques, cloud computing, data science, and the semantic web will likely allow SPM to be findable, accessible, interoperable, reusable, reliable, interpretable, and actionable.",2017-01-01,19,1550,44,1588
2732,28057825,Machine vision methods for analyzing social interactions,"Recent developments in machine vision methods for automatic, quantitative analysis of social behavior have immensely improved both the scale and level of resolution with which we can dissect interactions between members of the same species. In this paper, we review these methods, with a particular focus on how biologists can apply them to their own work. We discuss several components of machine vision-based analyses: methods to record high-quality video for automated analyses, video-based tracking algorithms for estimating the positions of interacting animals, and machine learning methods for recognizing patterns of interactions. These methods are extremely general in their applicability, and we review a subset of successful applications of them to biological questions in several model systems with very different types of social behaviors.",2017-01-01,29,851,56,1588
2754,27886184,A view of the current and future role of optical coherence tomography in the management of age-related macular degeneration,"Optical coherence tomography (OCT) has become an established diagnostic technology in the clinical management of age-related macular degeneration (AMD). OCT is being used for primary diagnosis, evaluation of therapeutic efficacy, and long-term monitoring. Computer-based advances in image analysis provide complementary imaging tools such as OCT angiography, further novel automated analysis methods as well as feature detection and prediction of prognosis in disease and therapy by machine learning. In early AMD, pathognomonic features such as drusen, pseudodrusen, and abnormalities of the retinal pigment epithelium (RPE) can be imaged in a qualitative and quantitative way to identify early signs of disease activity and define the risk of progression. In advanced AMD, disease activity can be monitored clearly by qualitative and quantified analyses of fluid pooling, such as intraretinal cystoid fluid, subretinal fluid, and pigment epithelial detachment (PED). Moreover, machine learning methods detect a large spectrum of new biomarkers. Evaluation of treatment efficacy and definition of optimal therapeutic regimens are an important aim in managing neovascular AMD. In atrophic AMD hallmarked by geographic atrophy (GA), advanced spectral domain (SD)-OCT imaging largely replaces conventional fundus autofluorescence (FAF) as it adds insight into the condition of the neurosensory layers and associated alterations at the level of the RPE and choroid. Exploration of imaging features by computerized methods has just begun but has already opened relevant and reliable horizons for the optimal use of OCT imaging for individualized and population-based management of AMD-the leading retinal epidemic of modern times.",2017-01-01,28,1726,123,1588
2747,27965365,Novel tools for quantifying secondary growth,"Secondary growth occurs in dicotyledons and gymnosperms, and results in an increased girth of plant organs. It is driven primarily by the vascular cambium, which produces thousands of cells throughout the life of several plant species. For instance, even in the small herbaceous model plant Arabidopsis, manual quantification of this massive process is impractical. Here, we provide a comprehensive overview of current methods used to measure radial growth. We discuss the issues and problematics related to its quantification. We highlight recent advances and tools developed for automated cellular phenotyping and its future applications.",2017-01-01,4,640,44,1588
2758,27862002,Distributed data networks: a blueprint for Big Data sharing and healthcare analytics,"This paper defines the attributes of distributed data networks and outlines the data and analytic infrastructure needed to build and maintain a successful network. We use examples from one successful implementation of a large-scale, multisite, healthcare-related distributed data network, the U.S. Food and Drug Administration-sponsored Sentinel Initiative. Analytic infrastructure-development concepts are discussed from the perspective of promoting six pillars of analytic infrastructure: consistency, reusability, flexibility, scalability, transparency, and reproducibility. This paper also introduces one use case for machine learning algorithm development to fully utilize and advance the portfolio of population health analytics, particularly those using multisite administrative data sources.",2017-01-01,4,799,84,1588
1132,27537363,Biomechanical Studies on Patterns of Cranial Bone Fracture Using the Immature Porcine Model,"This review was prepared for the American Society of Mechanical Engineers Lissner Medal. It specifically discusses research performed in the Orthopaedic Biomechanics Laboratories on pediatric cranial bone mechanics and patterns of fracture in collaboration with the Forensic Anthropology Laboratory at Michigan State University. Cranial fractures are often an important element seen by forensic anthropologists during the investigation of pediatric trauma cases litigated in courts. While forensic anthropologists and forensic biomechanists are often called on to testify in these cases, there is little basic science developed in support of their testimony. The following is a review of studies conducted in the above laboratories and supported by the National Institute of Justice to begin an understanding of the mechanics and patterns of pediatric cranial bone fracture. With the lack of human pediatric specimens, the studies utilize an immature porcine model. Because much case evidence involves cranial bone fracture, the studies described below focus on determining input loading based on the resultant bone fracture pattern. The studies involve impact to the parietal bone, the most often fractured cranial bone, and begin with experiments on entrapped heads, progressing to those involving free-falling heads. The studies involve head drops onto different types and shapes of interfaces with variations of impact energy. The studies show linear fractures initiating from sutural boundaries, away from the impact site, for flat surface impacts, in contrast to depressed fractures for more focal impacts. The results have been incorporated into a ""Fracture Printing Interface (FPI),"" using machine learning and pattern recognition algorithms. The interface has been used to help interpret mechanisms of injury in pediatric death cases collected from medical examiner offices. The ultimate aim of this program of study is to develop a ""Human Fracture Printing Interface"" that can be used by forensic investigators in determining mechanisms of pediatric cranial bone fracture.",2017-02-01,2,2082,91,1557
2714,28190189,Towards Precision in HF Pharmacotherapy,"Purpose of review:                    Heart failure (HF) is a disease state with great heterogeneity, which complicates the therapeutic process. Identifying more precise HF phenotypes will allow for the development of more targeted therapies and improvement in patient outcomes. This review explores the future for precision medicine in HF treatment.              Recent findings:                    Rather than a continuous disease spectrum with a uniform pathogenesis, HF has phenotypes with different underlying pathophysiologic features. The challenge is to establish clinical phenotypic characterizations to direct therapy. Phenomapping, a process of using machine learning algorithms applied to clinical data sets, has been used to identify phenotypically distinct and clinically meaningful HF groups. As powerful technologies extend our knowledge, future analyses may be able to compile more comprehensive phenotypic profiles using genetic, epigenetic, proteomic, and metabolomic measurements. Identifying clinical characterizations of particular HF patients that would be uniquely or disproportionately responsive to a specific treatment would allow for more direct selection of optimal therapy, reduce trial-and-error prescribing, and help avoid adverse drug reactions.",2017-02-01,1,1278,39,1557
2715,28183433,Mechanisms of the Development of Allergy (MeDALL): Introducing novel concepts in allergy phenotypes,"Asthma, rhinitis, and eczema are complex diseases with multiple genetic and environmental factors interlinked through IgE-associated and non-IgE-associated mechanisms. Mechanisms of the Development of ALLergy (MeDALL; EU FP7-CP-IP; project no: 261357; 2010-2015) studied the complex links of allergic diseases at the clinical and mechanistic levels by linking epidemiologic, clinical, and mechanistic research, including in vivo and in vitro models. MeDALL integrated 14 European birth cohorts, including 44,010 participants and 160 cohort follow-ups between pregnancy and age 20 years. Thirteen thousand children were prospectively followed after puberty by using a newly standardized MeDALL Core Questionnaire. A microarray developed for allergen molecules with increased IgE sensitivity was obtained for 3,292 children. Estimates of air pollution exposure from previous studies were available for 10,000 children. Omics data included those from historical genome-wide association studies (23,000 children) and DNA methylation (2,173), targeted multiplex biomarker (1,427), and transcriptomic (723) studies. Using classical epidemiology and machine-learning methods in 16,147 children aged 4 years and 11,080 children aged 8 years, MeDALL showed the multimorbidity of eczema, rhinitis, and asthma and estimated that only 38% of multimorbidity was attributable to IgE sensitization. MeDALL has proposed a new vision of multimorbidity independent of IgE sensitization, and has shown that monosensitization and polysensitization represent 2 distinct phenotypes. The translational component of MeDALL is shown by the identification of a novel allergic phenotype characterized by polysensitization and multimorbidity, which is associated with the frequency, persistence, and severity of allergic symptoms. The results of MeDALL will help integrate personalized, predictive, preventative, and participatory approaches in allergic diseases.",2017-02-01,41,1935,99,1557
2716,28154052,Predicting human behavior: The next frontiers,"Machine learning has provided researchers with new tools for understanding human behavior. In this article, we briefly describe some successes in predicting behaviors and describe the challenges over the next few years.",2017-02-01,4,219,45,1557
2717,28154050,Beyond prediction: Using big data for policy problems,"Machine-learning prediction methods have been extremely productive in applications ranging from medicine to allocating fire and health inspectors in cities. However, there are a number of gaps between making a prediction and making a decision, and underlying assumptions need to be understood in order to optimize data-driven decision-making.",2017-02-01,15,342,53,1557
2738,28034409,Visualizing the knowledge structure and evolution of big data research in healthcare informatics,"Background:                    In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.              Methods:                    To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers' production trends in the field and the trend of each paper's co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.              Results:                    By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People's Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).              Conclusion:                    This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.",2017-02-01,20,1979,96,1557
2710,28230848,Computational approaches to fMRI analysis,"Analysis methods in cognitive neuroscience have not always matched the richness of fMRI data. Early methods focused on estimating neural activity within individual voxels or regions, averaged over trials or blocks and modeled separately in each participant. This approach mostly neglected the distributed nature of neural representations over voxels, the continuous dynamics of neural activity during tasks, the statistical benefits of performing joint inference over multiple participants and the value of using predictive models to constrain analysis. Several recent exploratory and theory-driven methods have begun to pursue these opportunities. These methods highlight the importance of computational techniques in fMRI analysis, especially machine learning, algorithmic optimization and parallel computing. Adoption of these techniques is enabling a new generation of experiments and analyses that could transform our understanding of some of the most complex-and distinctly human-signals in the brain: acts of cognition such as thoughts, intentions and memories.",2017-02-01,46,1068,41,1557
2778,27693712,Design of efficient computational workflows for in silico drug repurposing,"Here, we provide a comprehensive overview of the current status of in silico repurposing methods by establishing links between current technological trends, data availability and characteristics of the algorithms used in these methods. Using the case of the computational repurposing of fasudil as an alternative autophagy enhancer, we suggest a generic modular organization of a repurposing workflow. We also review 3D structure-based, similarity-based, inference-based and machine learning (ML)-based methods. We summarize the advantages and disadvantages of these methods to emphasize three current technical challenges. We finish by discussing current directions of research, including possibilities offered by new methods, such as deep learning.",2017-02-01,32,750,74,1557
2733,28057585,An Overview of data science uses in bioimage informatics,"This review aims at providing a practical overview of the use of statistical features and associated data science methods in bioimage informatics. To achieve a quantitative link between images and biological concepts, one typically replaces an object coming from an image (a segmented cell or intracellular object, a pattern of expression or localisation, even a whole image) by a vector of numbers. They range from carefully crafted biologically relevant measurements to features learnt through deep neural networks. This replacement allows for the use of practical algorithms for visualisation, comparison and inference, such as the ones from machine learning or multivariate statistics. While originating mainly, for biology, in high content screening, those methods are integral to the use of data science for the quantitative analysis of microscopy images to gain biological insight, and they are sure to gather more interest as the need to make sense of the increasing amount of acquired imaging data grows more pressing.",2017-02-01,7,1027,56,1557
2706,28242295,"Analysis of live cell images: Methods, tools and opportunities","Advances in optical microscopy, biosensors and cell culturing technologies have transformed live cell imaging. Thanks to these advances live cell imaging plays an increasingly important role in basic biology research as well as at all stages of drug development. Image analysis methods are needed to extract quantitative information from these vast and complex data sets. The aim of this review is to provide an overview of available image analysis methods for live cell imaging, in particular required preprocessing image segmentation, cell tracking and data visualisation methods. The potential opportunities recent advances in machine learning, especially deep learning, and computer vision provide are being discussed. This review includes overview of the different available software packages and toolkits.",2017-02-01,17,811,62,1557
2707,28241858,Machine learning identifies a compact gene set for monitoring the circadian clock in human blood,"Background:                    The circadian clock and the daily rhythms it produces are crucial for human health, but are often disrupted by the modern environment. At the same time, circadian rhythms may influence the efficacy and toxicity of therapeutics and the metabolic response to food intake. Developing treatments for circadian dysfunction, as well as optimizing the daily timing of treatments for other health conditions, will require a simple and accurate method to monitor the molecular state of the circadian clock.              Methods:                    Here we used a recently developed method called ZeitZeiger to predict circadian time (CT, time of day according to the circadian clock) from genome-wide gene expression in human blood.              Results:                    In cross-validation on 498 samples from 60 individuals across three publicly available datasets, ZeitZeiger predicted CT in single samples with a median absolute error of 2.1 h. The predictor trained on all 498 samples used 15 genes, only two of which are part of the core circadian clock. By then applying ZeitZeiger to 475 additional samples from the same three datasets, we quantified how the circadian clock in the blood was affected by various perturbations to the sleep-wake and light-dark cycles. Finally, we extended ZeitZeiger (1) to handle intra-individual variation by making predictions based on multiple samples taken a known time apart, and (2) to handle inter-individual variation by personalizing predictions based on samples from the respective individual. Each of these strategies improved prediction of CT by ~20%.              Conclusions:                    Our results are an important step towards precision circadian medicine. In addition, our generalizable extensions to ZeitZeiger may be applicable to the growing number of biological datasets that contain multiple observations per individual.",2017-02-01,20,1916,96,1557
374,28451550,Improving the Prediction of Survival in Cancer Patients by Using Machine Learning Techniques: Experience of Gene Expression Data: A Narrative Review,"Background:                    Today, despite the many advances in early detection of diseases, cancer patients have a poor prognosis and the survival rates in them are low. Recently, microarray technologies have been used for gathering thousands data about the gene expression level of cancer cells. These types of data are the main indicators in survival prediction of cancer. This study highlights the improvement of survival prediction based on gene expression data by using machine learning techniques in cancer patients.              Methods:                    This review article was conducted by searching articles between 2000 to 2016 in scientific databases and e-Journals. We used keywords such as machine learning, gene expression data, survival and cancer.              Results:                    Studies have shown the high accuracy and effectiveness of gene expression data in comparison with clinical data in survival prediction. Because of bewildering and high volume of such data, studies have highlighted the importance of machine learning algorithms such as Artificial Neural Networks (ANN) to find out the distinctive signatures of gene expression in cancer patients. These algorithms improve the efficiency of probing and analyzing gene expression in cancer profiles for survival prediction of cancer.              Conclusion:                    By attention to the capabilities of machine learning techniques in proteomics and genomics applications, developing clinical decision support systems based on these methods for analyzing gene expression data can prevent potential errors in survival estimation, provide appropriate and individualized treatments to patients and improve the prognosis of cancers.",2017-02-01,7,1730,148,1557
2704,28254083,Classification techniques on computerized systems to predict and/or to detect Apnea: A systematic review,"Background and objective:                    Sleep apnea syndrome (SAS), which can significantly decrease the quality of life is associated with a major risk factor of health implications such as increased cardiovascular disease, sudden death, depression, irritability, hypertension, and learning difficulties. Thus, it is relevant and timely to present a systematic review describing significant applications in the framework of computational intelligence-based SAS, including its performance, beneficial and challenging effects, and modeling for the decision-making on multiple scenarios.              Methods:                    This study aims to systematically review the literature on systems for the detection and/or prediction of apnea events using a classification model.              Results:                    Forty-five included studies revealed a combination of classification techniques for the diagnosis of apnea, such as threshold-based (14.75%) and machine learning (ML) models (85.25%). In addition, the ML models, were clustered in a mind map, include neural networks (44.26%), regression (4.91%), instance-based (11.47%), Bayesian algorithms (1.63%), reinforcement learning (4.91%), dimensionality reduction (8.19%), ensemble learning (6.55%), and decision trees (3.27%).              Conclusions:                    A classification model should provide an auto-adaptive and no external-human action dependency. In addition, the accuracy of the classification models is related with the effective features selection. New high-quality studies based on randomized controlled trials and validation of models using a large and multiple sample of data are recommended.",2017-03-01,4,1685,104,1529
2779,27659841,Neuroprogression and illness trajectories in bipolar disorder,"The longitudinal course of bipolar disorder is highly variable, and a subset of patients seems to present a progressive course associated with brain changes and functional impairment. Areas covered: We discuss the theory of neuroprogression in bipolar disorder. This concept considers the systemic stress response that occurs within mood episodes and late-stage deficits in functioning and cognition as well as neuroanatomic changes. We also discuss treatment refractoriness that may take place in some cases of bipolar disorder. We searched PubMed for articles published in any language up to June 4th, 2016. We found 315 abstracts and included 87 studies in our review. Expert commentary: We are of the opinion that the use of specific pharmacological strategies and functional remediation may be potentially useful in bipolar patients at late-stages. New analytic approaches using multimodal data hold the potential to help in identifying signatures of subgroups of patients who will develop a neuroprogressive course.",2017-03-01,7,1021,61,1529
2699,28272367,How Open Data Shapes In Silico Transporter Modeling,"Chemical compound bioactivity and related data are nowadays easily available from open data sources and the open medicinal chemistry literature for many transmembrane proteins. Computational ligand-based modeling of transporters has therefore experienced a shift from local (quantitative) models to more global, qualitative, predictive models. As the size and heterogeneity of the data set rises, careful data curation becomes even more important. This includes, for example, not only a tailored cutoff setting for the generation of binary classes, but also the proper assessment of the applicability domain. Powerful machine learning algorithms (such as multi-label classification) now allow the simultaneous prediction of multiple related targets. However, the more complex, the less interpretable these models will get. We emphasize that transmembrane transporters are very peculiar, some of which act as off-targets rather than as real drug targets. Thus, careful selection of the right modeling technique is important, as well as cautious interpretation of results. We hope that, as more and more data will become available, we will be able to ameliorate and specify our models, coming closer towards function elucidation and the development of safer medicine.",2017-03-01,0,1265,51,1529
1139,27501063,"""Is voice a marker for Autism spectrum disorder? A systematic review and meta-analysis""","Individuals with Autism Spectrum Disorder (ASD) tend to show distinctive, atypical acoustic patterns of speech. These behaviors affect social interactions and social development and could represent a non-invasive marker for ASD. We systematically reviewed the literature quantifying acoustic patterns in ASD. Search terms were: (prosody OR intonation OR inflection OR intensity OR pitch OR fundamental frequency OR speech rate OR voice quality OR acoustic) AND (autis* OR Asperger). Results were filtered to include only: empirical studies quantifying acoustic features of vocal production in ASD, with a sample size >2, and the inclusion of a neurotypical comparison group and/or correlations between acoustic measures and severity of clinical features. We identified 34 articles, including 30 univariate studies and 15 multivariate machine-learning studies. We performed meta-analyses of the univariate studies, identifying significant differences in mean pitch and pitch range between individuals with ASD and comparison participants (Cohen's d of 0.4-0.5 and discriminatory accuracy of about 61-64%). The multivariate studies reported higher accuracies than the univariate studies (63-96%). However, the methods used and the acoustic features investigated were too diverse for performing meta-analysis. We conclude that multivariate studies of acoustic patterns are a promising but yet unsystematic avenue for establishing ASD markers. We outline three recommendations for future studies: open data, open methods, and theory-driven research. Autism Res 2017, 10: 384-407.  2016 International Society for Autism Research, Wiley Periodicals, Inc.",2017-03-01,14,1649,87,1529
390,28348527,Tuning Up the Old Brain with New Tricks: Attention Training via Neurofeedback,"Neurofeedback (NF) is a form of biofeedback that uses real-time (RT) modulation of brain activity to enhance brain function and behavioral performance. Recent advances in Brain-Computer Interfaces (BCI) and cognitive training (CT) have provided new tools and evidence that NF improves cognitive functions, such as attention and working memory (WM), beyond what is provided by traditional CT. More published studies have demonstrated the efficacy of NF, particularly for treating attention deficit hyperactivity disorder (ADHD) in children. In contrast, there have been fewer studies done in older adults with or without cognitive impairment, with some notable exceptions. The focus of this review is to summarize current success in RT NF training of older brains aiming to match those of younger brains during attention/WM tasks. We also outline potential future advances in RT brainwave-based NF for improving attention training in older populations. The rapid growth in wireless recording of brain activity, machine learning classification and brain network analysis provides new tools for combating cognitive decline and brain aging in older adults. We optimistically conclude that NF, combined with new neuro-markers (event-related potentials and connectivity) and traditional features, promises to provide new hope for brain and CT in the growing older population.",2017-03-01,9,1369,77,1529
385,28363883,Methods for Coding Tobacco-Related Twitter Data: A Systematic Review,"Background:                    As Twitter has grown in popularity to 313 million monthly active users, researchers have increasingly been using it as a data source for tobacco-related research.              Objective:                    The objective of this systematic review was to assess the methodological approaches of categorically coded tobacco Twitter data and make recommendations for future studies.              Methods:                    Data sources included PsycINFO, Web of Science, PubMed, ABI/INFORM, Communication Source, and Tobacco Regulatory Science. Searches were limited to peer-reviewed journals and conference proceedings in English from January 2006 to July 2016. The initial search identified 274 articles using a Twitter keyword and a tobacco keyword. One coder reviewed all abstracts and identified 27 articles that met the following inclusion criteria: (1) original research, (2) focused on tobacco or a tobacco product, (3) analyzed Twitter data, and (4) coded Twitter data categorically. One coder extracted data collection and coding methods.              Results:                    E-cigarettes were the most common type of Twitter data analyzed, followed by specific tobacco campaigns. The most prevalent data sources were Gnip and Twitter's Streaming application programming interface (API). The primary methods of coding were hand-coding and machine learning. The studies predominantly coded for relevance, sentiment, theme, user or account, and location of user.              Conclusions:                    Standards for data collection and coding should be developed to be able to more easily compare and replicate tobacco-related Twitter results. Additional recommendations include the following: sample Twitter's databases multiple times, make a distinction between message attitude and emotional tone for sentiment, code images and URLs, and analyze user profiles. Being relatively novel and widely used among adolescents and black and Hispanic individuals, Twitter could provide a rich source of tobacco surveillance data among vulnerable populations.",2017-03-01,14,2097,68,1529
2730,28087243,Using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: Methods and applications,"Deep learning (DL) is a family of machine learning methods that has gained considerable attention in the scientific community, breaking benchmark records in areas such as speech and visual recognition. DL differs from conventional machine learning methods by virtue of its ability to learn the optimal representation from the raw data through consecutive nonlinear transformations, achieving increasingly higher levels of abstraction and complexity. Given its ability to detect abstract and complex patterns, DL has been applied in neuroimaging studies of psychiatric and neurological disorders, which are characterised by subtle and diffuse alterations. Here we introduce the underlying concepts of DL and review studies that have used this approach to classify brain-based disorders. The results of these studies indicate that DL could be a powerful tool in the current search for biomarkers of psychiatric and neurologic disease. We conclude our review by discussing the main promises and challenges of using DL to elucidate brain-based disorders, as well as possible directions for future research.",2017-03-01,74,1102,130,1529
2743,27997811,Macromolecular target prediction by self-organizing feature maps,"Rational drug discovery would greatly benefit from a more nuanced appreciation of the activity of pharmacologically active compounds against a diverse panel of macromolecular targets. Already, computational target-prediction models assist medicinal chemists in library screening, de novo molecular design, optimization of active chemical agents, drug re-purposing, in the spotting of potential undesired off-target activities, and in the 'de-orphaning' of phenotypic screening hits. The self-organizing map (SOM) algorithm has been employed successfully for these and other purposes. Areas covered: The authors recapitulate contemporary artificial neural network methods for macromolecular target prediction, and present the basic SOM algorithm at a conceptual level. Specifically, they highlight consensus target-scoring by the employment of multiple SOMs, and discuss the opportunities and limitations of this technique. Expert opinion: Self-organizing feature maps represent a straightforward approach to ligand clustering and classification. Some of the appeal lies in their conceptual simplicity and broad applicability domain. Despite known algorithmic shortcomings, this computational target prediction concept has been proven to work in prospective settings with high success rates. It represents a prototypic technique for future advances in the in silico identification of the modes of action and macromolecular targets of bioactive molecules.",2017-03-01,7,1453,64,1529
2688,28319238,Behavioral and neural constraints on hierarchical representations,"Central to behavior and cognition is the way that sensory stimuli are represented in neural systems. The distributions over such stimuli enjoy rich structure; however, how the brain captures and exploits these regularities is unclear. Here, we consider different sources of perhaps the most prevalent form of structure, namely hierarchies, in one of its most prevalent cases, namely the representation of images. We review experimental approaches across a range of subfields, spanning inference, memory recall, and visual adaptation, to investigate how these constrain hierarchical representations. We also discuss progress in building hierarchical models of the representation of images-this has the potential to clarify how the structure of the world is reflected in biological systems. We suggest there is a need for a closer embedding of recent advances in machine learning and computer vision into the design and interpretation of experiments, notably by utilizing the understanding of the structure of natural scenes and through the creation of hierarchically structured synthetic stimuli.",2017-03-01,1,1095,65,1529
2713,28211015,"Fifty years of computer analysis in chest imaging: rule-based, machine learning, deep learning","Half a century ago, the term ""computer-aided diagnosis"" (CAD) was introduced in the scientific literature. Pulmonary imaging, with chest radiography and computed tomography, has always been one of the focus areas in this field. In this study, I describe how machine learning became the dominant technology for tackling CAD in the lungs, generally producing better results than do classical rule-based approaches, and how the field is now rapidly changing: in the last few years, we have seen how even better results can be obtained with deep learning. The key differences among rule-based processing, machine learning, and deep learning are summarized and illustrated for various applications of CAD in the chest.",2017-03-01,31,713,94,1529
2708,28236531,Neuroadaptive Bayesian Optimization and Hypothesis Testing,"Cognitive neuroscientists are often interested in broad research questions, yet use overly narrow experimental designs by considering only a small subset of possible experimental conditions. This limits the generalizability and reproducibility of many research findings. Here, we propose an alternative approach that resolves these problems by taking advantage of recent developments in real-time data analysis and machine learning. Neuroadaptive Bayesian optimization is a powerful strategy to efficiently explore more experimental conditions than is currently possible with standard methodology. We argue that such an approach could broaden the hypotheses considered in cognitive science, improving the generalizability of findings. In addition, Bayesian optimization can be combined with preregistration to cover exploration, mitigating researcher bias more broadly and improving reproducibility.",2017-03-01,9,899,58,1529
2697,28277804,Cardiac imaging: working towards fully-automated machine analysis & interpretation,"Non-invasive imaging plays a critical role in managing patients with cardiovascular disease. Although subjective visual interpretation remains the clinical mainstay, quantitative analysis facilitates objective, evidence-based management, and advances in clinical research. This has driven developments in computing and software tools aimed at achieving fully automated image processing and quantitative analysis. In parallel, machine learning techniques have been used to rapidly integrate large amounts of clinical and quantitative imaging data to provide highly personalized individual patient-based conclusions. Areas covered: This review summarizes recent advances in automated quantitative imaging in cardiology and describes the latest techniques which incorporate machine learning principles. The review focuses on the cardiac imaging techniques which are in wide clinical use. It also discusses key issues and obstacles for these tools to become utilized in mainstream clinical practice. Expert commentary: Fully-automated processing and high-level computer interpretation of cardiac imaging are becoming a reality. Application of machine learning to the vast amounts of quantitative data generated per scan and integration with clinical data also facilitates a move to more patient-specific interpretation. These developments are unlikely to replace interpreting physicians but will provide them with highly accurate tools to detect disease, risk-stratify, and optimize patient-specific treatment. However, with each technological advance, we move further from human dependence and closer to fully-automated machine interpretation.",2017-03-01,23,1640,82,1529
2757,27862943,Blood transcriptomic comparison of individuals with and without autism spectrum disorder: A combined-samples mega-analysis,"Blood-based microarray studies comparing individuals affected with autism spectrum disorder (ASD) and typically developing individuals help characterize differences in circulating immune cell functions and offer potential biomarker signal. We sought to combine the subject-level data from previously published studies by mega-analysis to increase the statistical power. We identified studies that compared ex vivo blood or lymphocytes from ASD-affected individuals and unrelated comparison subjects using Affymetrix or Illumina array platforms. Raw microarray data and clinical meta-data were obtained from seven studies, totaling 626 affected and 447 comparison subjects. Microarray data were processed using uniform methods. Covariate-controlled mixed-effect linear models were used to identify gene transcripts and co-expression network modules that were significantly associated with diagnostic status. Permutation-based gene-set analysis was used to identify functionally related sets of genes that were over- and under-expressed among ASD samples. Our results were consistent with diminished interferon-, EGF-, PDGF-, PI3K-AKT-mTOR-, and RAS-MAPK-signaling cascades, and increased ribosomal translation and NK-cell related activity in ASD. We explored evidence for sex-differences in the ASD-related transcriptomic signature. We also demonstrated that machine-learning classifiers using blood transcriptome data perform with moderate accuracy when data are combined across studies. Comparing our results with those from blood-based studies of protein biomarkers (e.g., cytokines and trophic factors), we propose that ASD may feature decoupling between certain circulating signaling proteins (higher in ASD samples) and the transcriptional cascades which they typically elicit within circulating immune cells (lower in ASD samples). These findings provide insight into ASD-related transcriptional differences in circulating immune cells.  2016 Wiley Periodicals, Inc.",2017-04-01,18,1973,122,1498
381,28405579,A survey of methods and tools to detect recent and strong positive selection,"Positive selection occurs when an allele is favored by natural selection. The frequency of the favored allele increases in the population and due to genetic hitchhiking the neighboring linked variation diminishes, creating so-called selective sweeps. Detecting traces of positive selection in genomes is achieved by searching for signatures introduced by selective sweeps, such as regions of reduced variation, a specific shift of the site frequency spectrum, and particular LD patterns in the region. A variety of methods and tools can be used for detecting sweeps, ranging from simple implementations that compute summary statistics such as Tajima's D, to more advanced statistical approaches that use combinations of statistics, maximum likelihood, machine learning etc. In this survey, we present and discuss summary statistics and software tools, and classify them based on the selective sweep signature they detect, i.e., SFS-based vs. LD-based, as well as their capacity to analyze whole genomes or just subgenomic regions. Additionally, we summarize the results of comparisons among four open-source software releases (SweeD, SweepFinder, SweepFinder2, and OmegaPlus) regarding sensitivity, specificity, and execution times. In equilibrium neutral models or mild bottlenecks, both SFS- and LD-based methods are able to detect selective sweeps accurately. Methods and tools that rely on LD exhibit higher true positive rates than SFS-based ones under the model of a single sweep or recurrent hitchhiking. However, their false positive rate is elevated when a misspecified demographic model is used to represent the null hypothesis. When the correct (or similar to the correct) demographic model is used instead, the false positive rates are considerably reduced. The accuracy of detecting the true target of selection is decreased in bottleneck scenarios. In terms of execution time, LD-based methods are typically faster than SFS-based methods, due to the nature of required arithmetic.",2017-04-01,28,1994,76,1498
2695,28285459,A Critical Review for Developing Accurate and Dynamic Predictive Models Using Machine Learning Methods in Medicine and Health Care,"Recently, Artificial Intelligence (AI) has been used widely in medicine and health care sector. In machine learning, the classification or prediction is a major field of AI. Today, the study of existing predictive models based on machine learning methods is extremely active. Doctors need accurate predictions for the outcomes of their patients' diseases. In addition, for accurate predictions, timing is another significant factor that influences treatment decisions. In this paper, existing predictive models in medicine and health care have critically reviewed. Furthermore, the most famous machine learning methods have explained, and the confusion between a statistical approach and machine learning has clarified. A review of related literature reveals that the predictions of existing predictive models differ even when the same dataset is used. Therefore, existing predictive models are essential, and current methods must be improved.",2017-04-01,19,943,130,1498
2702,28265788,A review of supervised machine learning applied to ageing research,"Broadly speaking, supervised machine learning is the computational task of learning correlations between variables in annotated data (the training set), and using this information to create a predictive model capable of inferring annotations for new data, whose annotations are not known. Ageing is a complex process that affects nearly all animal species. This process can be studied at several levels of abstraction, in different organisms and with different objectives in mind. Not surprisingly, the diversity of the supervised machine learning algorithms applied to answer biological questions reflects the complexities of the underlying ageing processes being studied. Many works using supervised machine learning to study the ageing process have been recently published, so it is timely to review these works, to discuss their main findings and weaknesses. In summary, the main findings of the reviewed papers are: the link between specific types of DNA repair and ageing; ageing-related proteins tend to be highly connected and seem to play a central role in molecular pathways; ageing/longevity is linked with autophagy and apoptosis, nutrient receptor genes, and copper and iron ion transport. Additionally, several biomarkers of ageing were found by machine learning. Despite some interesting machine learning results, we also identified a weakness of current works on this topic: only one of the reviewed papers has corroborated the computational results of machine learning algorithms through wet-lab experiments. In conclusion, supervised machine learning has contributed to advance our knowledge and has provided novel insights on ageing, yet future work should have a greater emphasis in validating the predictions.",2017-04-01,22,1730,66,1498
2711,28222333,Immunoprofiling as a predictor of patient's response to cancer therapy-promises and challenges,"Immune cell infiltration is common to many tumors and has been recognized by pathologists for more than 100 years. The application of digital imaging and objective assessment software allowed a concise determination of the type and quantity of immune cells and their location relative to the tumor and, in the case of colon cancer, characterized overall survival better than AJCC TNM staging. Subsequently, expression of PD-L1, by 50% or more tumor cells, identified NSCLC patients with double the response rate to anti-PD-1. Soon, automated staining methods will improve reproducibility of multiplex staining and allow for CLIA standards so that multiplex staining can be used to make clinical decisions. Ultimately, machine-learning algorithms will help interpret data from tissue images and lead to improved delivery of precision medicine.",2017-04-01,15,842,94,1498
2726,28125274,Implementing Machine Learning in Radiology Practice and Research,"Objective:                    The purposes of this article are to describe concepts that radiologists should understand to evaluate machine learning projects, including common algorithms, supervised as opposed to unsupervised techniques, statistical pitfalls, and data considerations for training and evaluation, and to briefly describe ethical dilemmas and legal risk.              Conclusion:                    Machine learning includes a broad class of computer programs that improve with experience. The complexity of creating, training, and monitoring machine learning indicates that the success of the algorithms will require radiologist involvement for years to come, leading to engagement rather than replacement.",2017-04-01,61,722,64,1498
392,28342697,Recent publications from the Alzheimer's Disease Neuroimaging Initiative: Reviewing progress toward improved AD clinical trials,"Introduction:                    The Alzheimer's Disease Neuroimaging Initiative (ADNI) has continued development and standardization of methodologies for biomarkers and has provided an increased depth and breadth of data available to qualified researchers. This review summarizes the over 400 publications using ADNI data during 2014 and 2015.              Methods:                    We used standard searches to find publications using ADNI data.              Results:                    (1) Structural and functional changes, including subtle changes to hippocampal shape and texture, atrophy in areas outside of hippocampus, and disruption to functional networks, are detectable in presymptomatic subjects before hippocampal atrophy; (2) In subjects with abnormal -amyloid deposition (A+), biomarkers become abnormal in the order predicted by the amyloid cascade hypothesis; (3) Cognitive decline is more closely linked to tau than A deposition; (4) Cerebrovascular risk factors may interact with A to increase white-matter (WM) abnormalities which may accelerate Alzheimer's disease (AD) progression in conjunction with tau abnormalities; (5) Different patterns of atrophy are associated with impairment of memory and executive function and may underlie psychiatric symptoms; (6) Structural, functional, and metabolic network connectivities are disrupted as AD progresses. Models of prion-like spreading of A pathology along WM tracts predict known patterns of cortical A deposition and declines in glucose metabolism; (7) New AD risk and protective gene loci have been identified using biologically informed approaches; (8) Cognitively normal and mild cognitive impairment (MCI) subjects are heterogeneous and include groups typified not only by ""classic"" AD pathology but also by normal biomarkers, accelerated decline, and suspected non-Alzheimer's pathology; (9) Selection of subjects at risk of imminent decline on the basis of one or more pathologies improves the power of clinical trials; (10) Sensitivity of cognitive outcome measures to early changes in cognition has been improved and surrogate outcome measures using longitudinal structural magnetic resonance imaging may further reduce clinical trial cost and duration; (11) Advances in machine learning techniques such as neural networks have improved diagnostic and prognostic accuracy especially in challenges involving MCI subjects; and (12) Network connectivity measures and genetic variants show promise in multimodal classification and some classifiers using single modalities are rivaling multimodal classifiers.              Discussion:                    Taken together, these studies fundamentally deepen our understanding of AD progression and its underlying genetic basis, which in turn informs and improves clinical trial design.",2017-04-01,69,2817,127,1498
394,28324301,Neuroimaging in Epilepsy,"In recent years, the field of neuroimaging has undergone dramatic development. Specifically, of importance for clinicians and researchers managing patients with epilepsies, new methods of brain imaging in search of the seizure-producing abnormalities have been implemented, and older methods have undergone additional refinement. Methodology to predict seizure freedom and cognitive outcome has also rapidly progressed. In general, the image data processing methods are very different and more complicated than even a decade ago. In this review, we identify the recent developments in neuroimaging that are aimed at improved management of epilepsy patients. Advances in structural imaging, diffusion imaging, fMRI, structural and functional connectivity, hybrid imaging methods, quantitative neuroimaging, and machine-learning are discussed. We also briefly summarize the potential new developments that may shape the field of neuroimaging in the near future and may advance not only our understanding of epileptic networks as the source of treatment-resistant seizures but also better define the areas that need to be treated in order to provide the patients with better long-term outcomes.",2017-04-01,10,1191,24,1498
2739,28032396,Computational neuroscience approach to biomarkers and treatments for mental disorders,"Psychiatry research has long experienced a stagnation stemming from a lack of understanding of the neurobiological underpinnings of phenomenologically defined mental disorders. Recently, the application of computational neuroscience to psychiatry research has shown great promise in establishing a link between phenomenological and pathophysiological aspects of mental disorders, thereby recasting current nosology in more biologically meaningful dimensions. In this review, we highlight recent investigations into computational neuroscience that have undertaken either theory- or data-driven approaches to quantitatively delineate the mechanisms of mental disorders. The theory-driven approach, including reinforcement learning models, plays an integrative role in this process by enabling correspondence between behavior and disorder-specific alterations at multiple levels of brain organization, ranging from molecules to cells to circuits. Previous studies have explicated a plethora of defining symptoms of mental disorders, including anhedonia, inattention, and poor executive function. The data-driven approach, on the other hand, is an emerging field in computational neuroscience seeking to identify disorder-specific features among high-dimensional big data. Remarkably, various machine-learning techniques have been applied to neuroimaging data, and the extracted disorder-specific features have been used for automatic case-control classification. For many disorders, the reported accuracies have reached 90% or more. However, we note that rigorous tests on independent cohorts are critically required to translate this research into clinical applications. Finally, we discuss the utility of the disorder-specific features found by the data-driven approach to psychiatric therapies, including neurofeedback. Such developments will allow simultaneous diagnosis and treatment of mental disorders using neuroimaging, thereby establishing 'theranostics' for the first time in clinical psychiatry.",2017-04-01,24,2004,85,1498
2735,28055887,EEG-Based Strategies to Detect Motor Imagery for Control and Rehabilitation,"Advances in brain-computer interface (BCI) technology have facilitated the detection of Motor Imagery (MI) from electroencephalography (EEG). First, we present three strategies of using BCI to detect MI from EEG: operant conditioning that employed a fixed model, machine learning that employed a subject-specific model computed from calibration, and adaptive strategy that continuously compute the subject-specific model. Second, we review prevailing works that employed the operant conditioning and machine learning strategies. Third, we present our past work on six stroke patients who underwent a BCI rehabilitation clinical trial with averaged accuracies of 79.8% during calibration and 69.5% across 18 online feedback sessions. Finally, we perform an offline study in this paper on our work employing the adaptive strategy. The results yielded significant improvements of 12% (p < 0.001) and 9% (p < 0.001) using all the data and using limited preceding data respectively in the feedback accuracies. The results showed an increase in the amount of training data yielded improvements. Nevertheless, results of using limited preceding data showed a larger part of the improvement was due to the adaptive strategy and changing subject-specific models did not deteriorate the accuracies. Hence the adaptive strategy is effective in addressing the non-stationarity between calibration and feedback sessions.",2017-04-01,19,1407,75,1498
359,28538142,An Introduction to Infinite HMMs for Single-Molecule Data Analysis,"The hidden Markov model (HMM) has been a workhorse of single-molecule data analysis and is now commonly used as a stand-alone tool in time series analysis or in conjunction with other analysis methods such as tracking. Here, we provide a conceptual introduction to an important generalization of the HMM, which is poised to have a deep impact across the field of biophysics: the infinite HMM (iHMM). As a modeling tool, iHMMs can analyze sequential data without a priori setting a specific number of states as required for the traditional (finite) HMM. Although the current literature on the iHMM is primarily intended for audiences in statistics, the idea is powerful and the iHMM's breadth in applicability outside machine learning and data science warrants a careful exposition. Here, we explain the key ideas underlying the iHMM, with a special emphasis on implementation, and provide a description of a code we are making freely available. In a companion article, we provide an important extension of the iHMM to accommodate complications such as drift.",2017-05-01,11,1058,66,1468
351,28588416,Mathematics delivering the advantage: the role of mathematicians in manufacturing and beyond,"Much has been written about the benefits that mathematics can bring to the UK economy and the manufacturing sector in particular, but less on the value of mathematicians and a mathematical training. This article, written from an industry perspective, considers the value of mathematicians to the UK's industrial base and the importance to the UK economy of encouraging young people in the UK to choose to study mathematics at school as a gateway to a wide range of careers. The points are illustrated using examples from the author's 20 years' experience in the security and intelligence and manufacturing sectors.",2017-05-01,0,614,92,1468
380,28405937,New Cardiac Imaging Algorithms to Diagnose Constrictive Pericarditis Versus Restrictive Cardiomyopathy,"Purpose of review:                    Echocardiography is the mainstay in the diagnostic evaluation of constrictive pericarditis (CP) and restrictive cardiomyopathy (RCM), but no single echocardiographic parameter is sufficiently robust to accurately distinguish between the two conditions. The present review summarizes the recent advances in echocardiography that promise to improve its diagnostic performance for this purpose. The role of other imaging modalities such as cardiac computed tomography, magnetic resonance imaging, and invasive hemodynamic assessment in the overall diagnostic approach is also discussed briefly.              Recent findings:                    A recent study has demonstrated improved diagnostic accuracy of echocardiography with integration of multiple conventional echocardiographic parameters in to a step-wise algorithm. Concurrently, the studies using speckle-tracking echocardiography have revealed distinct and disparate patterns of myocardial mechanical abnormalities in CP and RCM with their ability to distinguish between the two conditions. The incorporation of machine-learning algorithms into echocardiography workflow permits easy integration of the wealth of the diagnostic data available and promises to further enhance the diagnostic accuracy of echocardiography. New imaging algorithms are continuously being evolved to permit accurate distinction between CP and RCM. Further research is needed to validate the accuracy of these newer algorithms and to define their place in the overall diagnostic approach for this purpose.",2017-05-01,4,1577,102,1468
391,28344110,Toward a systematic exploration of nano-bio interactions,"Many studies of nanomaterials make non-systematic alterations of nanoparticle physicochemical properties. Given the immense size of the property space for nanomaterials, such approaches are not very useful in elucidating fundamental relationships between inherent physicochemical properties of these materials and their interactions with, and effects on, biological systems. Data driven artificial intelligence methods such as machine learning algorithms have proven highly effective in generating models with good predictivity and some degree of interpretability. They can provide a viable method of reducing or eliminating animal testing. However, careful experimental design with the modelling of the results in mind is a proven and efficient way of exploring large materials spaces. This approach, coupled with high speed automated experimental synthesis and characterization technologies now appearing, is the fastest route to developing models that regulatory bodies may find useful. We advocate greatly increased focus on systematic modification of physicochemical properties of nanoparticles combined with comprehensive biological evaluation and computational analysis. This is essential to obtain better mechanistic understanding of nano-bio interactions, and to derive quantitatively predictive and robust models for the properties of nanomaterials that have useful domains of applicability.",2017-05-01,5,1401,56,1468
2741,28011145,A Functional Genomic Meta-Analysis of Clinical Trials in Systemic Sclerosis: Toward Precision Medicine and Combination Therapy,"Systemic sclerosis is an orphan, systemic autoimmune disease with no FDA-approved treatments. Its heterogeneity and rarity often result in underpowered clinical trials making the analysis and interpretation of associated molecular data challenging. We performed a meta-analysis of gene expression data from skin biopsies of patients with systemic sclerosis treated with five therapies: mycophenolate mofetil, rituximab, abatacept, nilotinib, and fresolimumab. A common clinical improvement criterion of -20% or -5 modified Rodnan skin score was applied to each study. We applied a machine learning approach that captured features beyond differential expression and was better at identifying targets of therapies than the differential expression alone. Regardless of treatment mechanism, abrogation of inflammatory pathways accompanied clinical improvement in multiple studies suggesting that high expression of immune-related genes indicates active and targetable disease. Our framework allowed us to compare different trials and ask if patients who failed one therapy would likely improve on a different therapy, based on changes in gene expression. Genes with high expression at baseline in fresolimumab nonimprovers were downregulated in mycophenolate mofetil improvers, suggesting that immunomodulatory or combination therapy may have benefitted these patients. This approach can be broadly applied to increase tissue specificity and sensitivity of differential expression results.",2017-05-01,8,1485,126,1468
383,28375728,Personal Sensing: Understanding Mental Health Using Ubiquitous Sensors and Machine Learning,"Sensors in everyday devices, such as our phones, wearables, and computers, leave a stream of digital traces. Personal sensing refers to collecting and analyzing data from sensors embedded in the context of daily life with the aim of identifying human behaviors, thoughts, feelings, and traits. This article provides a critical review of personal sensing research related to mental health, focused principally on smartphones, but also including studies of wearables, social media, and computers. We provide a layered, hierarchical model for translating raw sensor data into markers of behaviors and states related to mental health. Also discussed are research methods as well as challenges, including privacy and problems of dimensionality. Although personal sensing is still in its infancy, it holds great promise as a method for conducting mental health research and as a clinical tool for monitoring at-risk populations and providing the foundation for the next generation of mobile health (or mHealth) interventions.",2017-05-01,86,1019,91,1468
2347,29463985,Involvement of Machine Learning for Breast Cancer Image Classification: A Survey,"Breast cancer is one of the largest causes of women's death in the world today. Advance engineering of natural image classification techniques and Artificial Intelligence methods has largely been used for the breast-image classification task. The involvement of digital image classification allows the doctor and the physicians a second opinion, and it saves the doctors' and physicians' time. Despite the various publications on breast image classification, very few review papers are available which provide a detailed description of breast cancer image classification techniques, feature extraction and selection procedures, classification measuring parameterizations, and image classification findings. We have put a special emphasis on the Convolutional Neural Network (CNN) method for breast image classification. Along with the CNN method we have also described the involvement of the conventional Neural Network (NN), Logic Based classifiers such as the Random Forest (RF) algorithm, Support Vector Machines (SVM), Bayesian methods, and a few of the semisupervised and unsupervised methods which have been used for breast image classification.",2017-06-01,7,1151,80,1437
354,28570593,A review of active learning approaches to experimental design for uncovering biological networks,"Various types of biological knowledge describe networks of interactions among elementary entities. For example, transcriptional regulatory networks consist of interactions among proteins and genes. Current knowledge about the exact structure of such networks is highly incomplete, and laboratory experiments that manipulate the entities involved are conducted to test hypotheses about these networks. In recent years, various automated approaches to experiment selection have been proposed. Many of these approaches can be characterized as active machine learning algorithms. Active learning is an iterative process in which a model is learned from data, hypotheses are generated from the model to propose informative experiments, and the experiments yield new data that is used to update the model. This review describes the various models, experiment selection strategies, validation techniques, and successful applications described in the literature; highlights common themes and notable distinctions among methods; and identifies likely directions of future research and open problems in the area.",2017-06-01,12,1102,96,1437
326,28694872,"Biomimetic molecular design tools that learn, evolve, and adapt","A dominant hallmark of living systems is their ability to adapt to changes in the environment by learning and evolving. Nature does this so superbly that intensive research efforts are now attempting to mimic biological processes. Initially this biomimicry involved developing synthetic methods to generate complex bioactive natural products. Recent work is attempting to understand how molecular machines operate so their principles can be copied, and learning how to employ biomimetic evolution and learning methods to solve complex problems in science, medicine and engineering. Automation, robotics, artificial intelligence, and evolutionary algorithms are now converging to generate what might broadly be called in silico-based adaptive evolution of materials. These methods are being applied to organic chemistry to systematize reactions, create synthesis robots to carry out unit operations, and to devise closed loop flow self-optimizing chemical synthesis systems. Most scientific innovations and technologies pass through the well-known ""S curve"", with slow beginning, an almost exponential growth in capability, and a stable applications period. Adaptive, evolving, machine learning-based molecular design and optimization methods are approaching the period of very rapid growth and their impact is already being described as potentially disruptive. This paper describes new developments in biomimetic adaptive, evolving, learning computational molecular design methods and their potential impacts in chemistry, engineering, and medicine.",2017-06-01,1,1549,63,1437
352,28585183,Precision Medicine for Heart Failure with Preserved Ejection Fraction: An Overview,"There are few proven therapies for heart failure with preserved ejection fraction (HFpEF). The lack of therapies, along with increased recognition of the disorder and its underlying pathophysiology, has led to the acknowledgement that HFpEF is heterogeneous and is not likely to respond to a one-size-fits-all approach. Thus, HFpEF is a prime candidate to benefit from a precision medicine approach. For this reason, we have assembled a compendium of papers on the topic of precision medicine in HFpEF in the Journal of Cardiovascular Translational Research. These papers cover a variety of topics relevant to precision medicine in HFpEF, including automated identification of HFpEF patients; machine learning, novel molecular approaches, genomics, and deep phenotyping of HFpEF; and clinical trial designs that can be used to advance precision medicine in HFpEF. In this introductory article, we provide an overview of precision medicine in HFpEF with the hope that the work described here and in the other papers in this special theme issue will stimulate investigators and clinicians to advance a more targeted approach to HFpEF classification and treatment.",2017-06-01,15,1161,82,1437
308,28831290,An Update on Statistical Boosting in Biomedicine,"Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression, and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.",2017-06-01,1,865,48,1437
1212,26728066,Predicting Subcellular Localization of Proteins by Bioinformatic Algorithms,"When predicting the subcellular localization of proteins from their amino acid sequences, there are basically three approaches: signal-based, global property-based, and homology-based. Each of these has its advantages and drawbacks, and it is important when comparing methods to know which approach was used. Various statistical and machine learning algorithms are used with all three approaches, and various measures and standards are employed when reporting the performances of the developed methods. This chapter presents a number of available methods for prediction of sorting signals and subcellular localization, but rather than providing a checklist of which predictors to use, it aims to function as a guide for critical assessment of prediction methods.",2017-06-01,3,762,75,1437
683,28980044,"Analysis, Recognition, and Classification of Biological Membrane Images","Biological membrane images contain a variety of objects and patterns, which convey information about the underlying biological structures and mechanisms. The field of image analysis includes methods of computation which convert features and objects identified in images into quantitative information about biological structures represented in these images. Microscopy images are complex, noisy, and full of artifacts and consequently require multiple image processing steps for the extraction of meaningful quantitative information. This review is focused on methods of analysis of images of cells and biological membranes such as detection, segmentation, classification and machine learning, registration, tracking, and visualization. These methods could make possible, for example, to automatically identify defects in the cell membrane which affect physiological processes. Detailed analysis of membrane images could facilitate understanding of the underlying physiological structures or help in the interpretation of biological experiments.",2017-06-01,0,1044,71,1437
347,28641107,The Persistence and Transience of Memory,"The predominant focus in the neurobiological study of memory has been on remembering (persistence). However, recent studies have considered the neurobiology of forgetting (transience). Here we draw parallels between neurobiological and computational mechanisms underlying transience. We propose that it is the interaction between persistence and transience that allows for intelligent decision-making in dynamic, noisy environments. Specifically, we argue that transience (1) enhances flexibility, by reducing the influence of outdated information on memory-guided decision-making, and (2) prevents overfitting to specific past events, thereby promoting generalization. According to this view, the goal of memory is not the transmission of information through time, per se. Rather, the goal of memory is to optimize decision-making. As such, transience is as important as persistence in mnemonic systems.",2017-06-01,51,904,40,1437
346,28641555,Supervised Machine Learning Methods Applied to Predict Ligand- Binding Affinity,"Background:                    Calculation of ligand-binding affinity is an open problem in computational medicinal chemistry. The ability to computationally predict affinities has a beneficial impact in the early stages of drug development, since it allows a mathematical model to assess protein-ligand interactions. Due to the availability of structural and binding information, machine learning methods have been applied to generate scoring functions with good predictive power.              Objective:                    Our goal here is to review recent developments in the application of machine learning methods to predict ligand-binding affinity.              Method:                    We focus our review on the application of computational methods to predict binding affinity for protein targets. In addition, we also describe the major available databases for experimental binding constants and protein structures. Furthermore, we explain the most successful methods to evaluate the predictive power of scoring functions.              Results:                    Association of structural information with ligand-binding affinity makes it possible to generate scoring functions targeted to a specific biological system. Through regression analysis, this data can be used as a base to generate mathematical models to predict ligandbinding affinities, such as inhibition constant, dissociation constant and binding energy.              Conclusion:                    Experimental biophysical techniques were able to determine the structures of over 120,000 macromolecules. Considering also the evolution of binding affinity information, we may say that we have a promising scenario for development of scoring functions, making use of machine learning techniques. Recent developments in this area indicate that building scoring functions targeted to the biological systems of interest shows superior predictive performance, when compared with other approaches.",2017-06-01,7,1969,79,1437
343,30062151,Enabling Precision Cardiology Through Multiscale Biology and Systems Medicine,"The traditional paradigm of cardiovascular disease research derives insight from large-scale, broadly inclusive clinical studies of well-characterized pathologies. These insights are then put into practice according to standardized clinical guidelines. However, stagnation in the development of new cardiovascular therapies and variability in therapeutic response implies that this paradigm is insufficient for reducing the cardiovascular disease burden. In this state-of-the-art review, we examine 3 interconnected ideas we put forth as key concepts for enabling a transition to precision cardiology: 1) precision characterization of cardiovascular disease with machine learning methods; 2) the application of network models of disease to embrace disease complexity; and 3) using insights from the previous 2 ideas to enable pharmacology and polypharmacology systems for more precise drug-to-patient matching and patient-disease stratification. We conclude by exploring the challenges of applying a precision approach to cardiology, which arise from a deficit of the required resources and infrastructure, and emerging evidence for the clinical effectiveness of this nascent approach.",2017-06-01,13,1185,77,1437
341,28656130,"Tuberculosis control, and the where and why of artificial intelligence","Countries aiming to reduce their tuberculosis (TB) burden by 2035 to the levels envisaged by the World Health Organization End TB Strategy need to innovate, with approaches such as digital health (electronic and mobile health) in support of patient care, surveillance, programme management, training and communication. Alongside the large-scale roll-out required for such interventions to make a significant impact, products must stay abreast of advancing technology over time. The integration of artificial intelligence into new software promises to make processes more effective and efficient, endowing them with a potential hitherto unimaginable. Users can benefit from artificial intelligence-enabled pattern recognition software for tasks ranging from reading radiographs to adverse event monitoring, sifting through vast datasets to personalise a patient's care plan or to customise training materials. Many experts forecast the imminent transformation of the delivery of healthcare services. We discuss how artificial intelligence and machine learning could revolutionise the management of TB.",2017-06-01,5,1100,70,1437
310,28828993,"General Machine Learning Model, Review, and Experimental-Theoretic Study of Magnolol Activity in Enterotoxigenic Induced Oxidative Stress","This study evaluated the antioxidative effects of magnolol based on the mouse model induced by Enterotoxigenic Escherichia coli (E. coli, ETEC). All experimental mice were equally treated with ETEC suspensions (3.45109 CFU/ml) after oral administration of magnolol for 7 days at the dose of 0, 100, 300 and 500 mg/kg Body Weight (BW), respectively. The oxidative metabolites and antioxidases for each sample (organism of mouse) were determined: Malondialdehyde (MDA), Nitric Oxide (NO), Glutathione (GSH), Myeloperoxidase (MPO), Catalase (CAT), Superoxide Dismutase (SOD), and Glutathione Peroxidase (GPx). In addition, we also determined the corresponding mRNA expressions of CAT, SOD and GPx as well as the Total Antioxidant Capacity (T-AOC). The experiment was completed with a theoretical study that predicts a series of 79 ChEMBL activities of magnolol with 47 proteins in 18 organisms using a Quantitative Structure- Activity Relationship (QSAR) classifier based on the Moving Averages (MAs) of Rcpi descriptors in three types of experimental conditions (biological activity with specific units, protein target and organisms). Six Machine Learning methods from Weka software were tested and the best QSAR classification model was provided by Random Forest with True Positive Rate (TPR) of 0.701 and Area under Receiver Operating Characteristic (AUROC) of 0.790 (test subset, 10-fold crossvalidation). The model is predicting if the new ChEMBL activities are greater or lower than the average values for the magnolol targets in different organisms.",2017-06-01,3,1554,137,1437
339,28662070,Machine learning and microsimulation techniques on the prognosis of dementia: A systematic literature review,"Background:                    Dementia is a complex disorder characterized by poor outcomes for the patients and high costs of care. After decades of research little is known about its mechanisms. Having prognostic estimates about dementia can help researchers, patients and public entities in dealing with this disorder. Thus, health data, machine learning and microsimulation techniques could be employed in developing prognostic estimates for dementia.              Objective:                    The goal of this paper is to present evidence on the state of the art of studies investigating and the prognosis of dementia using machine learning and microsimulation techniques.              Method:                    To achieve our goal we carried out a systematic literature review, in which three large databases-Pubmed, Socups and Web of Science were searched to select studies that employed machine learning or microsimulation techniques for the prognosis of dementia. A single backward snowballing was done to identify further studies. A quality checklist was also employed to assess the quality of the evidence presented by the selected studies, and low quality studies were removed. Finally, data from the final set of studies were extracted in summary tables.              Results:                    In total 37 papers were included. The data summary results showed that the current research is focused on the investigation of the patients with mild cognitive impairment that will evolve to Alzheimer's disease, using machine learning techniques. Microsimulation studies were concerned with cost estimation and had a populational focus. Neuroimaging was the most commonly used variable.              Conclusions:                    Prediction of conversion from MCI to AD is the dominant theme in the selected studies. Most studies used ML techniques on Neuroimaging data. Only a few data sources have been recruited by most studies and the ADNI database is the one most commonly used. Only two studies have investigated the prediction of epidemiological aspects of Dementia using either ML or MS techniques. Finally, care should be taken when interpreting the reported accuracy of ML techniques, given studies' different contexts.",2017-06-01,13,2243,108,1437
2751,27896760,Opportunities and Challenges of Multiplex Assays: A Machine Learning Perspective,"Multiplex assays that allow the simultaneous measurement of multiple analytes in small sample quantities have developed into a widely used technology. Their implementation spans across multiple assay systems and can provide readouts of similar quality as the respective single-plex measures, albeit at far higher throughput. Multiplex assay systems are therefore an important element for biomarker discovery and development strategies but analysis of the derived data can face substantial challenges that may limit the possibility of identifying meaningful biological markers. This chapter gives an overview of opportunities and challenges of multiplexed biomarker analysis, in particular from the perspective of machine learning aimed at identification of predictive biological signatures.",2017-06-01,1,790,80,1437
331,28681133,Innovative Clinical Trial Designs for Precision Medicine in Heart Failure with Preserved Ejection Fraction,"A major challenge in the care of patients with heart failure and preserved ejection fraction (HFpEF) is the lack of proven therapies due to disappointing results from randomized controlled trials (RCTs). The heterogeneity of the HFpEF syndrome and the use of conventional RCT designs are possible reasons underlying the failure of these trials. There are several factors-including the widespread adoption of electronic health records, decreasing costs of obtaining high-dimensional data, and the availability of a wide variety of potential therapeutics-that have evolved to enable more innovative clinical trial designs in HFpEF. Here, we review the current landscape of HFpEF RCTs and present several innovative RCT designs that could be implemented in HFpEF, including enrichment trials, adaptive trials, umbrella trials, basket trials, and machine learning-based trials (including examples for each). Our hope is that the description of the aforementioned innovative trial designs will stimulate new approaches to clinical trials in HFpEF.",2017-06-01,11,1042,106,1437
332,28672838,Recent Advances in Conotoxin Classification by Using Machine Learning Methods,"Conotoxins are disulfide-rich small peptides, which are invaluable peptides that target ion channel and neuronal receptors. Conotoxins have been demonstrated as potent pharmaceuticals in the treatment of a series of diseases, such as Alzheimer's disease, Parkinson's disease, and epilepsy. In addition, conotoxins are also ideal molecular templates for the development of new drug lead compounds and play important roles in neurobiological research as well. Thus, the accurate identification of conotoxin types will provide key clues for the biological research and clinical medicine. Generally, conotoxin types are confirmed when their sequence, structure, and function are experimentally validated. However, it is time-consuming and costly to acquire the structure and function information by using biochemical experiments. Therefore, it is important to develop computational tools for efficiently and effectively recognizing conotoxin types based on sequence information. In this work, we reviewed the current progress in computational identification of conotoxins in the following aspects: (i) construction of benchmark dataset; (ii) strategies for extracting sequence features; (iii) feature selection techniques; (iv) machine learning methods for classifying conotoxins; (v) the results obtained by these methods and the published tools; and (vi) future perspectives on conotoxin classification. The paper provides the basis for in-depth study of conotoxins and drug therapy research.",2017-06-01,14,1490,77,1437
337,28663166,Researching Mental Health Disorders in the Era of Social Media: Systematic Review,"Background:                    Mental illness is quickly becoming one of the most prevalent public health problems worldwide. Social network platforms, where users can express their emotions, feelings, and thoughts, are a valuable source of data for researching mental health, and techniques based on machine learning are increasingly used for this purpose.              Objective:                    The objective of this review was to explore the scope and limits of cutting-edge techniques that researchers are using for predictive analytics in mental health and to review associated issues, such as ethical concerns, in this area of research.              Methods:                    We performed a systematic literature review in March 2017, using keywords to search articles on data mining of social network data in the context of common mental health disorders, published between 2010 and March 8, 2017 in medical and computer science journals.              Results:                    The initial search returned a total of 5386 articles. Following a careful analysis of the titles, abstracts, and main texts, we selected 48 articles for review. We coded the articles according to key characteristics, techniques used for data collection, data preprocessing, feature extraction, feature selection, model construction, and model verification. The most common analytical method was text analysis, with several studies using different flavors of image analysis and social interaction graph analysis.              Conclusions:                    Despite an increasing number of studies investigating mental health issues using social network data, some common problems persist. Assembling large, high-quality datasets of social media users with mental disorder is problematic, not only due to biases associated with the collection methods, but also with regard to managing consent and selecting appropriate analytics techniques.",2017-06-01,33,1932,81,1437
336,28663789,Molecular and phenotypic biomarkers of aging,"Individuals of the same age may not age at the same rate. Quantitative biomarkers of aging are valuable tools to measure physiological age, assess the extent of 'healthy aging', and potentially predict health span and life span for an individual. Given the complex nature of the aging process, the biomarkers of aging are multilayered and multifaceted. Here, we review the phenotypic and molecular biomarkers of aging. Identifying and using biomarkers of aging to improve human health, prevent age-associated diseases, and extend healthy life span are now facilitated by the fast-growing capacity of multilevel cross-sectional and longitudinal data acquisition, storage, and analysis, particularly for data related to general human populations. Combined with artificial intelligence and machine learning techniques, reliable panels of biomarkers of aging will have tremendous potential to improve human health in aging societies.",2017-06-01,32,929,44,1437
334,28667600,Protein Sorting Prediction,"Many computational methods are available for predicting protein sorting in bacteria. When comparing them, it is important to know that they can be grouped into three fundamentally different approaches: signal-based, global-property-based and homology-based prediction. In this chapter, the strengths and drawbacks of each of these approaches is described through many examples of methods that predict secretion, integration into membranes, or subcellular locations in general. The aim of this chapter is to provide a user-level introduction to the field with a minimum of computational theory.",2017-06-01,2,593,26,1437
2752,27896759,Identification and Clinical Translation of Biomarker Signatures: Statistical Considerations,"Powerful machine learning tools exist to extract biological patterns for diagnosis or prediction from high-dimensional datasets. Simultaneous advances in high-throughput profiling technologies have led to a rapid acceleration of biomarker discovery investigations across all areas of medicine. However, the translation of biomarker signatures into clinically useful tools has thus far been difficult. In this chapter, several important considerations are discussed that influence such translation in the context of classifier design. These include aspects of variable selection that go beyond classification accuracy, as well as effects of variability on assay stability and sample size. The consideration of such factors may lead to an adaptation of biomarker discovery approaches, aimed at an optimal balance of performance and clinical translatability.",2017-06-01,0,855,91,1437
2725,28125523,Machine Learning-Based Classification of 38 Years of Spine-Related Literature Into 100 Research Topics,"Study design:                    Retrospective review.              Objective:                    To identify the top 100 spine research topics.              Summary of background data:                    Recent advances in ""machine learning,"" or computers learning without explicit instructions, have yielded broad technological advances. Topic modeling algorithms can be applied to large volumes of text to discover quantifiable themes and trends.              Methods:                    Abstracts were extracted from the National Library of Medicine PubMed database from five prominent peer-reviewed spine journals (European Spine Journal [ESJ], The Spine Journal [SpineJ], Spine, Journal of Spinal Disorders and Techniques [JSDT], Journal of Neurosurgery: Spine [JNS]). Each abstract was entered into a latent Dirichlet allocation model specified to discover 100 topics, resulting in each abstract being assigned a probability of belonging in a topic. Topics were named using the five most frequently appearing terms within that topic. Significance of increasing (""hot"") or decreasing (""cold"") topic popularity over time was evaluated with simple linear regression.              Results:                    From 1978 to 2015, 25,805 spine-related research articles were extracted and classified into 100 topics. Top two most published topics included ""clinical, surgeons, guidelines, information, care"" (n = 496 articles) and ""pain, back, low, treatment, chronic"" (424). Top two hot trends included ""disc, cervical, replacement, level, arthroplasty"" (+0.05%/yr, P < 0.001), and ""minimally, invasive, approach, technique"" (+0.05%/yr, P < 0.001). By journal, the most published topics were ESJ-""operative, surgery, postoperative, underwent, preoperative""; SpineJ-""clinical, surgeons, guidelines, information, care""; Spine-""pain, back, low, treatment, chronic""; JNS- ""tumor, lesions, rare, present, diagnosis""; JSDT-""cervical, anterior, plate, fusion, ACDF.""              Conclusion:                    Topics discovered through latent Dirichlet allocation modeling represent unbiased meaningful themes relevant to spine care. Topic dynamics can provide historical context and direction for future research for aspiring investigators and trainees interested in spine careers. Please explore https://singdc.shinyapps.io/spinetopics.              Level of evidence:                    N A.",2017-06-01,4,2389,102,1437
360,28530547,Application of Machine Learning Approaches for Protein-protein Interactions Prediction,"Background:                    Proteomics endeavors to study the structures, functions and interactions of proteins. Information of the protein-protein interactions (PPIs) helps to improve our knowledge of the functions and the 3D structures of proteins. Thus determining the PPIs is essential for the study of the proteomics.              Objective:                    In this review, in order to study the application of machine learning in predicting PPI, some machine learning approaches such as support vector machine (SVM), artificial neural networks (ANNs) and random forest (RF) were selected, and the examples of its applications in PPIs were listed.              Results:                    SVM and RF are two commonly used methods. Nowadays, more researchers predict PPIs by combining more than two methods.              Conclusion:                    This review presents the application of machine learning approaches in predicting PPI. Many examples of success in identification and prediction in the area of PPI prediction have been discussed, and the PPIs research is still in progress.",2017-06-01,4,1102,86,1437
2727,28116551,Tensor Factorization for Precision Medicine in Heart Failure with Preserved Ejection Fraction,"Heart failure with preserved ejection fraction (HFpEF) is a heterogeneous clinical syndrome that may benefit from improved subtyping in order to better characterize its pathophysiology and to develop novel targeted therapies. The United States Precision Medicine Initiative comes amid the rapid growth in quantity and modality of clinical data for HFpEF patients ranging from deep phenotypic to trans-omic data. Tensor factorization, a form of machine learning, allows for the integration of multiple data modalities to derive clinically relevant HFpEF subtypes that may have significant differences in underlying pathophysiology and differential response to therapies. Tensor factorization also allows for better interpretability by supporting dimensionality reduction and identifying latent groups of data for meaningful summarization of both features and disease outcomes. In this narrative review, we analyze the modest literature on the application of tensor factorization to related biomedical fields including genotyping and phenotyping. Based on the cited work including work of our own, we suggest multiple tensor factorization formulations capable of integrating the deep phenotypic and trans-omic modalities of data for HFpEF, or accounting for interactions between genetic variants at different omic hierarchies. We encourage extensive experimental studies to tackle challenges in applying tensor factorization for precision medicine in HFpEF, including effectively incorporating existing medical knowledge, properly accounting for uncertainty, and efficiently enforcing sparsity for better interpretability.",2017-06-01,13,1620,93,1437
2728,28092576,Multiple-Instance Learning for Medical Image and Video Analysis,"Multiple-instance learning (MIL) is a recent machine-learning paradigm that is particularly well suited to medical image and video analysis (MIVA) tasks. Based solely on class labels assigned globally to images or videos, MIL algorithms learn to detect relevant patterns locally in images or videos. These patterns are then used for classification at a global level. Because supervision relies on global labels, manual segmentations are not needed to train MIL algorithms, unlike traditional single-instance learning (SIL) algorithms. Consequently, these solutions are attracting increasing interest from the MIVA community: since the term was coined by Dietterich et al. in 1997, 73 research papers about MIL have been published in the MIVA literature. This paper reviews the existing strategies for modeling MIVA tasks as MIL problems, recommends general-purpose MIL algorithms for each type of MIVA tasks, and discusses MIVA-specific MIL algorithms. Various experiments performed in medical image and video datasets are compiled in order to back up these discussions. This meta-analysis shows that, besides being more convenient than SIL solutions, MIL algorithms are also more accurate in many cases. In other words, MIL is the ideal solution for many MIVA tasks. Recent trends are discussed, and future directions are proposed for this emerging paradigm.",2017-06-01,6,1359,63,1437
395,28319831,Structure-based prediction of host-pathogen protein interactions,"The discovery, validation, and characterization of protein-based interactions from different species are crucial for translational research regarding a variety of pathogens, ranging from the malaria parasite Plasmodium falciparum to HIV-1. Here, we review recent advances in the prediction of host-pathogen protein interfaces using structural information. In particular, we observe that current methods chiefly perform machine learning on sequence and domain information to produce large sets of candidate interactions that are further assessed and pruned to generate final, highly probable sets. Structure-based studies have also emphasized the electrostatic properties and evolutionary transformations of pathogenic interfaces, supplying crucial insight into antigenic determinants and the ways pathogens compete for host protein binding. Advancements in spectroscopic and crystallographic methods complement the aforementioned techniques, permitting the rigorous study of true positives at a molecular level. Together, these approaches illustrate how protein structure on a variety of levels functions coordinately and dynamically to achieve host takeover.",2017-06-01,7,1159,64,1437
1149,27436868,Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges,"Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning.",2017-06-01,57,1390,127,1437
2705,28251040,Analytic Complexity and Challenges in Identifying Mixtures of Exposures Associated with Phenotypes in the Exposome Era,"Purpose of review:                    Mixtures, or combinations and interactions between multiple environmental exposures, are hypothesized to be causally linked with disease and health-related phenotypes. Established and emerging molecular measurement technologies to assay the exposome, the comprehensive battery of exposures encountered from birth to death, promise a new way of identifying mixtures in disease in the epidemiological setting. In this opinion, we describe the analytic complexity and challenges in identifying mixtures associated with phenotype and disease.              Recent findings:                    Existing and emerging machine-learning methods and data analytic approaches (e.g., ""environment-wide association studies"" [EWASs]), as well as large cohorts may enhance possibilities to identify mixtures of correlated exposures associated with phenotypes; however, the analytic complexity of identifying mixtures is immense.              Summary:                    If the exposome concept is realized, new analytical methods and large sample sizes will be required to ascertain how mixtures are associated with disease. The author recommends documenting prevalent correlated exposures and replicated main effects prior to identifying mixtures.",2017-06-01,14,1270,118,1437
389,28353154,Physiome approach for the analysis of vascular flow reserve in the heart and brain,"This work reviews the key aspects of coronary and neurovascular flow reserves with an emphasis on physiomic modeling characteristics by the use of a variety of numerical approaches. First, we explain the definition of fractional flow reserve (FFR) in coronary artery and introduce its clinical significance. Then, computational researches for obtaining FFR are reviewed, and their clinical outcomes are compared. In the case of cerebrovascular reserve (CVR), in spite of substantial progress in the simulation of cerebral hemodynamics, only a few computational studies exist. Thus, we discuss the limitations of CVR simulation study and suggest the challenging issue to overcome these. Also, the future direction of physiomic researches for the flow reserves in coronary arteries and cerebral arteries is described. Also, we introduce a machine learning algorithm trained by the existing physiomic simulation data of flow reserve and suggest a prospective research direction related to this.",2017-06-01,2,991,82,1437
2698,28272810,Deep learning for computational chemistry,"The rise and fall of artificial neural networks is well documented in the scientific literature of both computer science and computational chemistry. Yet almost two decades later, we are now seeing a resurgence of interest in deep learning, a machine learning algorithm based on multilayer neural networks. Within the last few years, we have seen the transformative impact of deep learning in many domains, particularly in speech recognition and computer vision, to the extent that the majority of expert practitioners in those field are now regularly eschewing prior established models in favor of deep learning models. In this review, we provide an introductory overview into the theory of deep neural networks and their unique properties that distinguish them from traditional machine learning algorithms used in cheminformatics. By providing an overview of the variety of emerging applications of deep neural networks, we highlight its ubiquity and broad applicability to a wide range of challenges in the field, including quantitative structure activity relationship, virtual screening, protein structure prediction, quantum chemistry, materials design, and property prediction. In reviewing the performance of deep neural networks, we observed a consistent outperformance against non-neural networks state-of-the-art models across disparate research topics, and deep neural network-based models often exceeded the ""glass ceiling"" expectations of their respective tasks. Coupled with the maturity of GPU-accelerated computing for training deep neural networks and the exponential growth of chemical data on which to train these networks on, we anticipate that deep learning algorithms will be a valuable tool for computational chemistry.  2017 Wiley Periodicals, Inc.",2017-06-01,57,1773,41,1437
388,28353229,The Utility of Multiplex Assays for Identification of Proteomic Signatures in Psychiatry,"As substantial efforts are being made to identify biological markers of psychiatric illnesses, it is becoming clear that clinically useful accuracy will require larger sets of readouts that potentially span different technological platforms. For discovery of proteomic biomarkers, simultaneous measurement of analytes in small sample quantities has developed into a widely used technology of similar quality as the respective single-plex assays. Multiplex assay systems therefore hold promise for biomarker discovery and development in many complex disease areas including psychiatry. However, analysis of the derived data is subject to substantial challenges that may impede the possibility of obtaining meaningful findings. This chapter discusses potential applications of multiplexed assay technologies during biomarker development and highlights potential challenges for machine learning analysis of derived data.",2017-06-01,1,917,88,1437
387,28356908,Random Deep Belief Networks for Recognizing Emotions from Speech Signals,"Now the human emotions can be recognized from speech signals using machine learning methods; however, they are challenged by the lower recognition accuracies in real applications due to lack of the rich representation ability. Deep belief networks (DBN) can automatically discover the multiple levels of representations in speech signals. To make full of its advantages, this paper presents an ensemble of random deep belief networks (RDBN) method for speech emotion recognition. It firstly extracts the low level features of the input speech signal and then applies them to construct lots of random subspaces. Each random subspace is then provided for DBN to yield the higher level features as the input of the classifier to output an emotion label. All outputted emotion labels are then fused through the majority voting to decide the final emotion label for the input speech signal. The conducted experimental results on benchmark speech emotion databases show that RDBN has better accuracy than the compared methods for speech emotion recognition.",2017-06-01,5,1051,72,1437
2693,28301733,Big Data Analytics in Chemical Engineering,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.",2017-06-01,3,768,42,1437
382,28377102,Can We Predict Gene Expression by Understanding Proximal Promoter Architecture?,"We review computational predictions of expression from the promoter architecture - the set of transcription factors that can bind the proximal promoter. We focus on spatial expression patterns in animals with complex body plans and many distinct tissue types. This field is ripe for change as functional genomics datasets accumulate for both expression and protein-DNA interactions. While there has been some success in predicting the breadth of expression (i.e., the fraction of tissue types a gene is expressed in), predicting tissue specificity remains challenging. We discuss how progress can be achieved through either machine learning or complementary combinatorial data mining. The likely impact of single-cell expression data is considered. Finally, we discuss the design of artificial promoters as a practical application.",2017-06-01,7,831,79,1437
2764,27823566,Parallel Computing for Brain Simulation,"Background:                    The human brain is the most complex system in the known universe, it is therefore one of the greatest mysteries. It provides human beings with extraordinary abilities. However, until now it has not been understood yet how and why most of these abilities are produced.              Aims:                    For decades, researchers have been trying to make computers reproduce these abilities, focusing on both understanding the nervous system and, on processing data in a more efficient way than before. Their aim is to make computers process information similarly to the brain. Important technological developments and vast multidisciplinary projects have allowed creating the first simulation with a number of neurons similar to that of a human brain.              Conclusion:                    This paper presents an up-to-date review about the main research projects that are trying to simulate and/or emulate the human brain. They employ different types of computational models using parallel computing: digital models, analog models and hybrid models. This review includes the current applications of these works, as well as future trends. It is focused on various works that look for advanced progress in Neuroscience and still others which seek new discoveries in Computer Science (neuromorphic hardware, machine learning techniques). Their most outstanding characteristics are summarized and the latest advances and future plans are presented. In addition, this review points out the importance of considering not only neurons: Computational models of the brain should also include glial cells, given the proven importance of astrocytes in information processing.",2017-06-01,0,1704,39,1437
377,28434256,A bioinformatics roadmap for the human vaccines project,"Biomedical research has become a data intensive science in which high throughput experimentation is producing comprehensive data about biological systems at an ever-increasing pace. The Human Vaccines Project is a new public-private partnership, with the goal of accelerating development of improved vaccines and immunotherapies for global infectious diseases and cancers by decoding the human immune system. To achieve its mission, the Project is developing a Bioinformatics Hub as an open-source, multidisciplinary effort with the overarching goal of providing an enabling infrastructure to support the data processing, analysis and knowledge extraction procedures required to translate high throughput, high complexity human immunology research data into biomedical knowledge, to determine the core principles driving specific and durable protective immune responses.",2017-06-01,2,870,55,1437
2689,28315224,An Overview of Bioinformatics Tools and Resources in Allergy,"The rapidly increasing number of characterized allergens has created huge demands for advanced information storage, retrieval, and analysis. Bioinformatics and machine learning approaches provide useful tools for the study of allergens and epitopes prediction, which greatly complement traditional laboratory techniques. The specific applications mainly include identification of B- and T-cell epitopes, and assessment of allergenicity and cross-reactivity. In order to facilitate the work of clinical and basic researchers who are not familiar with bioinformatics, we review in this chapter the most important databases, bioinformatic tools, and methods with relevance to the study of allergens.",2017-06-01,1,696,60,1437
2759,27848884,Understanding the Structural Basis for Inhibition of Cyclin-Dependent Kinases. New Pieces in the Molecular Puzzle,"Background:                    Cyclin-dependent kinases (CDKs) comprise an important protein family for development of drugs, mostly aimed for use in treatment of cancer but there is also potential for development of drugs for neurodegenerative diseases and diabetes. Since the early 1990s, structural studies have been carried out on CDKs, in order to determine the structural basis for inhibition of this protein target.              Objective:                    Our goal here is to review recent structural studies focused on CDKs. We concentrate on latest developments in the understanding of the structural basis for inhibition of CDKs, relating structures and ligand-binding information.              Method:                    Protein crystallography has been successfully applied to elucidate over 400 CDK structures. Most of these structures are complexed with inhibitors. We use this richness of structural information to describe the major structural features determining the inhibition of this enzyme.              Results:                    Structures of CDK1, 2, 4-9, 12 13, and 16 have been elucidated. Analysis of these structures in complex with a wide range of different competitive inhibitors, strongly indicate some common features that can be used to guide the development of CDK inhibitors, such as a pattern of hydrogen bonding and the presence of halogen atoms in the ligand structure.              Conclusion:                    Nowadays we have structural information for hundreds of CDKs. Combining the structural and functional information we may say that a pattern of intermolecular hydrogen bonds is of pivotal importance for inhibitor specificity. In addition, machine learning techniques have shown improvements in predicting binding affinity for CDKs.",2017-06-01,4,1786,113,1437
304,28845200,Nature-Inspired Chemical Reaction Optimisation Algorithms,Nature-inspired meta-heuristic algorithms have dominated the scientific literature in the areas of machine learning and cognitive computing paradigm in the last three decades. Chemical reaction optimisation (CRO) is a population-based meta-heuristic algorithm based on the principles of chemical reaction. A chemical reaction is seen as a process of transforming the reactants (or molecules) through a sequence of reactions into products. This process of transformation is implemented in the CRO algorithm to solve optimisation problems. This article starts with an overview of the chemical reactions and how it is applied to the optimisation problem. A review of CRO and its variants is presented in the paper. Guidelines from the literature on the effective choice of CRO parameters for solution of optimisation problems are summarised.,2017-06-01,0,838,57,1437
370,28490262,Methods for safety signal detection in healthcare databases: a literature review,"With increasing availability, the use of healthcare databases as complementary data source for drug safety signal detection has been explored to circumvent the limitations inherent in spontaneous reporting. Areas covered: To review the methods proposed for safety signal detection in healthcare databases and their performance. Expert opinion: Fifteen different data mining methods were identified. They are based on disproportionality analysis, traditional pharmacoepidemiological designs (e.g. self-controlled designs), sequence symmetry analysis (SSA), sequential statistical testing, temporal association rules, supervised machine learning (SML), and the tree-based scan statistic. When considering the performance of these methods, the self-controlled designs, the SSA, and the SML seemed the most interesting approaches. In the perspective of routine signal detection from healthcare databases, pragmatic aspects such as the need for stakeholders to understand the method in order to be confident in the results must be considered. From this point of view, the SSA could appear as the most suitable method for signal detection in healthcare databases owing to its simple principle and its ability to provide a risk estimate. However, further developments, such as automated prioritization, are needed to help stakeholders handle the multiplicity of signals.",2017-06-01,15,1363,80,1437
369,28494123,Ecological momentary interventions for depression and anxiety,"Ecological momentary interventions (EMIs) are becoming more popular and more powerful resources for the treatment and prevention of depression and anxiety due to advances in technological capacity and analytic sophistication. Previous work has demonstrated that EMIs can be effective at reducing symptoms of depression and anxiety as well as related outcomes of stress and at increasing positive psychological functioning. In this review, we highlight the differences between EMIs and other forms of treatment due to the nature of EMIs to be deeply integrated into the fabric of people's day-to-day lives. EMIs require unique considerations in their design, deployment, and evaluation. Furthermore, given that EMIs have been advanced by changes in technologies and that the use of behavioral intervention technologies for mental health has been increasing, we discuss how technologies and analytics might usher in a new era of EMIs. Future EMIs might reduce user burden and increase intervention personalization and sophistication by leveraging digital sensors and advances in natural language processing and machine learning. Thus, although current EMIs are effective, the EMIs of the future might be more engaging, responsive, and adaptable to different people and different contexts.",2017-06-01,28,1286,61,1437
368,28494725,Computational Methods for Predicting ncRNA-protein Interactions,"Background:                    RNA-protein interactions (RPIs) play an important role in many cellular processes. In particular, noncoding RNA-protein interactions (ncRPIs) are involved in various gene regulations and human complex diseases. High-throughput experiments have provided a large number of valuable information about ncRPIs, but these experiments are expensive and timeconsuming. Therefore, some computational approaches have been developed to predict ncRPIs efficiently and effectively.              Methods:                    In this work, we will describe the recent advance of predicting ncRPIs from the following aspects: i) the dataset construction; ii) the sequence and structural feature representation, and iii) the machine learning algorithm.              Results:                    The current methods have successfully predicted ncRPIs, but most of them trained and tested on the small benchmark datasets derived from ncRNA-protein complexes in PDB database. The generalization performance and robust of these existing methods need to be further improved.              Conclusion:                    Concomitant with the large numbers of ncRPIs generated by high-throughput technologies, three future directions for predicting ncRPIs with machine learning should be paid attention. One direction is that how to effectively construct the negative sample set. Another is the selection of novel and effective features from the sequences and structures of ncRNAs and proteins. The third is the design of powerful predictor.",2017-06-01,3,1545,63,1437
646,29098674,Development of New Diagnostic Techniques - Machine Learning,"Traditional diagnoses on addiction reply on the patients' self-reports, which are easy to be dampened by false memory or malingering. Machine learning (ML) is a data-driven procedure that learns algorithms from training data and makes predictions. It is quickly developed and is more and more utilized into clinical applications including diagnoses of addiction. This chapter reviewed the basic concepts and processes of ML. Some studies utilizing ML to classify addicts and non-addicts, separate different types of addiction, and evaluate the effects of treatment are also reviewed. Both advantages and shortcomings of ML in diagnoses of addiction are discussed.",2017-06-01,2,663,59,1437
365,28508479,Prediction of complex traits: Conciliating genetics and statistics,"This review focuses on methods used to predict complex traits. Main characteristics of prediction approaches are given: the deterministic or stochastic nature of prediction, the objects of prediction, the sources of information and the main statistical methods. Sources of information discussed are the traditional genealogies and phenotypes, nucleotide sequences, expression data and epigenetics marks. Statistical methods are presented as successive degrees of generalization from the definition of the conditional expectation as the prediction rule, to best linear unbiased prediction, then Bayesian and, recently, machine learning methods, including meta-methods. We highlight the contributions of Daniel Gianola to this methodological evolution.",2017-06-01,2,750,66,1437
655,29058219,LEMRG: Decision Rule Generation Algorithm for Mining MicroRNA Expression Data,"Recently, research on mining microRNA (or miRNA) expression data has received a lot of attention, mainly because of its role in gene regulation. However, such type of data - usually saved in the form of microarrays - are very specific, because they contain only a small number of cases (often less than 100) compared with large number of attributes (equal to several hundreds or even tens of thousand). The small number of cases available during the learning process can cause instability of the newly created classifiers. Secondly, the huge number of attributes imposes the necessity of selecting only a few dominant attributes strongly correlated with the decision. Thus, an application of fundamental machine learning approaches of mining microarray data and its further classification is problematic or even could just fail.Thus, the main goal of our research is to develop the generalized algorithm of mining microarray data (including miRNA data sets), mainly to improve stability and, consequently, accuracy of classification for the newly created learning classifiers. The main concept of the novel approach is based on iteratively inducing many subsequent decision rule sets - called decision rule generations - instead of inducing only a single decision rule set, as it is done routinely. The decision rules have been chosen as the baseline classifiers of the newly developed LEMRG (Learning from Examples Module based on Rule Generations) algorithm mainly because the decision rule-based knowledge representation is easier for humans to comprehend, rather than other learning models. In our research we used a miRNA expression level learning data set describing 11 types of human cancers, while the testing data set contained poorly differentiated cases of only four types of cancers. As expected, our new classifiers - saved in the form of so-called cumulative decision rule sets - had better stability and accuracy of classification than single decision rule sets induced in the traditional manner. Furthermore, the LEMRG was compared with other machine learning models. It was proven that only 3 out of all 16 tested classifiers enabled so effective classification as our newly developed approach. Thus, using our cumulative set of decision rules, all cases of cancer from two selected concepts - colon and ovary - were correctly classified. Furthermore, we showed the role of these selected miRNAs as the potential biomarkers for diagnosis of tumors.A preliminary result of our research on decision rule generations was initially presented at the first International Conference of Digital Medicine and Medical 3D Printing (17-19.06.2016, Nanjing, China).",2017-06-01,0,2669,77,1437
358,28540688,"Machine Learning Techniques in Exploring MicroRNA Gene Discovery, Targets, and Functions","In recent years, the role of miRNAs in post-transcriptional gene regulation has provided new insights into the understanding of several types of cancers and neurological disorders. Although miRNA research has gathered great momentum since its discovery, traditional biological methods for finding miRNA genes and targets continue to remain a huge challenge due to the laborious tasks and extensive time involved. Fortunately, advances in computational methods have yielded considerable improvements in miRNA studies. This literature review briefly discusses recent machine learning-based techniques applied in the discovery of miRNAs, prediction of miRNA targets, and inference of miRNA functions. We also discuss the limitations of how these approaches have been elucidated in previous studies.",2017-06-01,0,795,88,1437
312,28812013,Intelligent Techniques Using Molecular Data Analysis in Leukaemia: An Opportunity for Personalized Medicine Support System,"The use of intelligent techniques in medicine has brought a ray of hope in terms of treating leukaemia patients. Personalized treatment uses patient's genetic profile to select a mode of treatment. This process makes use of molecular technology and machine learning, to determine the most suitable approach to treating a leukaemia patient. Until now, no reviews have been published from a computational perspective concerning the development of personalized medicine intelligent techniques for leukaemia patients using molecular data analysis. This review studies the published empirical research on personalized medicine in leukaemia and synthesizes findings across studies related to intelligence techniques in leukaemia, with specific attention to particular categories of these studies to help identify opportunities for further research into personalized medicine support systems in chronic myeloid leukaemia. A systematic search was carried out to identify studies using intelligence techniques in leukaemia and to categorize these studies based on leukaemia type and also the task, data source, and purpose of the studies. Most studies used molecular data analysis for personalized medicine, but future advancement for leukaemia patients requires molecular models that use advanced machine-learning methods to automate decision-making in treatment management to deliver supportive medical information to the patient in clinical practice.",2017-06-01,2,1444,122,1437
372,28456584,Inference in the age of big data: Future perspectives on neuroscience,"Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative datasets of unprecedented breadth (e.g., microanatomy, synaptic connections, and optogenetic brain-behavior assays) and size (e.g., cognition, brain imaging, and genetics). While growing data availability and information granularity have been amply discussed, we direct attention to a less explored question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more important to distill neurobiological knowledge from healthy and pathological brain measurements. We argue that large-scale data analysis will use more statistical models that are non-parametric, generative, and mixing frequentist and Bayesian aspects, while supplementing classical hypothesis testing with out-of-sample predictions.",2017-07-01,44,1032,69,1407
2765,27817211,Asthma phenotypes in childhood,"Asthma is no longer thought of as a single disease, but rather a collection of varying symptoms expressing different disease patterns. One of the ongoing challenges is understanding the underlying pathophysiological mechanisms that may be responsible for the varying responses to treatment. Areas Covered: This review provides an overview of our current understanding of the asthma phenotype concept in childhood and describes key findings from both conventional and data-driven methods. Expert Commentary: With the vast amounts of data generated from cohorts, there is hope that we can elucidate distinct pathophysiological mechanisms, or endotypes. In return, this would lead to better patient stratification and disease management, thereby providing true personalised medicine.",2017-07-01,7,780,30,1407
375,28436837,"Passive BCI in Operational Environments: Insights, Recent Advances, and Future Trends","Goal:                    This minireview aims to highlight recent important aspects to consider and evaluate when passive brain-computer interface (pBCI) systems would be developed and used in operational environments, and remarks future directions of their applications.              Methods:                    Electroencephalography (EEG) based pBCI has become an important tool for real-time analysis of brain activity since it could potentially provide covertly-without distracting the user from the main task-and objectively-not affected by the subjective judgment of an observer or the user itself-information about the operator cognitive state.              Results:                    Different examples of pBCI applications in operational environments and new adaptive interface solutions have been presented and described. In addition, a general overview regarding the correct use of machine learning techniques (e.g., which algorithm to use, common pitfalls to avoid, etc.) in the pBCI field has been provided.              Conclusion:                    Despite recent innovations on algorithms and neurotechnology, pBCI systems are not completely ready to enter the market yet, mainly due to limitations of the EEG electrodes technology, and algorithms reliability and capability in real settings.              Significance:                    High complexity and safety critical systems (e.g., airplanes, ATM interfaces) should adapt their behaviors and functionality accordingly to the user' actual mental state. Thus, technologies (i.e., pBCIs) able to measure in real time the user's mental states would result very useful in such ""high risk"" environments to enhance human machine interaction, and so increase the overall safety.",2017-07-01,28,1747,85,1407
362,28524769,Machine learning for epigenetics and future medical applications,Understanding epigenetic processes holds immense promise for medical applications. Advances in Machine Learning (ML) are critical to realize this promise. Previous studies used epigenetic data sets associated with the germline transmission of epigenetic transgenerational inheritance of disease and novel ML approaches to predict genome-wide locations of critical epimutations. A combination of Active Learning (ACL) and Imbalanced Class Learning (ICL) was used to address past problems with ML to develop a more efficient feature selection process and address the imbalance problem in all genomic data sets. The power of this novel ML approach and our ability to predict epigenetic phenomena and associated disease is suggested. The current approach requires extensive computation of features over the genome. A promising new approach is to introduce Deep Learning (DL) for the generation and simultaneous computation of novel genomic features tuned to the classification task. This approach can be used with any genomic or biological data set applied to medicine. The application of molecular epigenetic data in advanced machine learning analysis to medicine is the focus of this review.,2017-07-01,14,1189,64,1407
379,28414186,A review on neuroimaging-based classification studies and associated feature extraction methods for Alzheimer's disease and its prodromal stages,"Neuroimaging has made it possible to measure pathological brain changes associated with Alzheimer's disease (AD) in vivo. Over the past decade, these measures have been increasingly integrated into imaging signatures of AD by means of classification frameworks, offering promising tools for individualized diagnosis and prognosis. We reviewed neuroimaging-based studies for AD and mild cognitive impairment classification, selected after online database searches in Google Scholar and PubMed (January, 1985-June, 2016). We categorized these studies based on the following neuroimaging modalities (and sub-categorized based on features extracted as a post-processing step from these modalities): i) structural magnetic resonance imaging [MRI] (tissue density, cortical surface, and hippocampal measurements), ii) functional MRI (functional coherence of different brain regions, and the strength of the functional connectivity), iii) diffusion tensor imaging (patterns along the white matter fibers), iv) fluorodeoxyglucose positron emission tomography (FDG-PET) (metabolic rate of cerebral glucose), and v) amyloid-PET (amyloid burden). The studies reviewed indicate that the classification frameworks formulated on the basis of these features show promise for individualized diagnosis and prediction of clinical progression. Finally, we provided a detailed account of AD classification challenges and addressed some future research directions.",2017-07-01,89,1443,144,1407
348,28631139,A Systematic Review of Wearable Patient Monitoring Systems - Current Challenges and Opportunities for Clinical Adoption,"The aim of this review is to investigate barriers and challenges of wearable patient monitoring (WPM) solutions adopted by clinicians in acute, as well as in community, care settings. Currently, healthcare providers are coping with ever-growing healthcare challenges including an ageing population, chronic diseases, the cost of hospitalization, and the risk of medical errors. WPM systems are a potential solution for addressing some of these challenges by enabling advanced sensors, wearable technology, and secure and effective communication platforms between the clinicians and patients. A total of 791 articles were screened and 20 were selected for this review. The most common publication venue was conference proceedings (13, 54%). This review only considered recent studies published between 2015 and 2017. The identified studies involved chronic conditions (6, 30%), rehabilitation (7, 35%), cardiovascular diseases (4, 20%), falls (2, 10%) and mental health (1, 5%). Most studies focussed on the system aspects of WPM solutions including advanced sensors, wireless data collection, communication platform and clinical usability based on a specific area or disease. The current studies are progressing with localized sensor-software integration to solve a specific use-case/health area using non-scalable and 'silo' solutions. There is further work required regarding interoperability and clinical acceptance challenges. The advancement of wearable technology and possibilities of using machine learning and artificial intelligence in healthcare is a concept that has been investigated by many studies. We believe future patient monitoring and medical treatments will build upon efficient and affordable solutions of wearable technology.",2017-07-01,36,1747,119,1407
386,28359573,"Next-Generation Global Biomonitoring: Large-scale, Automated Reconstruction of Ecological Networks","We foresee a new global-scale, ecological approach to biomonitoring emerging within the next decade that can detect ecosystem change accurately, cheaply, and generically. Next-generation sequencing of DNA sampled from the Earth's environments would provide data for the relative abundance of operational taxonomic units or ecological functions. Machine-learning methods would then be used to reconstruct the ecological networks of interactions implicit in the raw NGS data. Ultimately, we envision the development of autonomous samplers that would sample nucleic acids and upload NGS sequence data to the cloud for network reconstruction. Large numbers of these samplers, in a global array, would allow sensitive automated biomonitoring of the Earth's major ecosystems at high spatial and temporal resolution, revolutionising our understanding of ecosystem change.",2017-07-01,9,864,98,1407
330,30202589,Big Data in traumatic brain injury; promise and challenges,"Traumatic brain injury (TBI) is a spectrum disease of overwhelming complexity, the research of which generates enormous amounts of structured, semi-structured and unstructured data. This resulting big data has tremendous potential to be mined for valuable information regarding the ""most complex disease of the most complex organ"". Big data analyses require specialized big data analytics applications, machine learning and artificial intelligence platforms to reveal associations, trends, correlations and patterns not otherwise realized by current analytical approaches. The intersection of potential data sources between experimental TBI and clinical TBI research presents inherent challenges for setting parameters for the generation of common data elements and to mine existing legacy data that would allow highly translatable big data analyses. In order to successfully utilize big data analyses in TBI, we must be willing to accept the messiness of data, collect and store all data and give up causation for correlation. In this context, coupling the big data approach to established clinical and pre-clinical data sources will transform current practices for triage, diagnosis, treatment and prognosis into highly integrated evidence-based patient care.",2017-07-01,5,1261,58,1407
2694,28294138,Machine learning applications in cell image analysis,"Machine learning (ML) refers to a set of automatic pattern recognition methods that have been successfully applied across various problem domains, including biomedical image analysis. This review focuses on ML applications for image analysis in light microscopy experiments with typical tasks of segmenting and tracking individual cells, and modelling of reconstructed lineage trees. After describing a typical image analysis pipeline and highlighting challenges of automatic analysis (for example, variability in cell morphology, tracking in presence of clutters) this review gives a brief historical outlook of ML, followed by basic concepts and definitions required for understanding examples. This article then presents several example applications at various image processing stages, including the use of supervised learning methods for improving cell segmentation, and the application of active learning for tracking. The review concludes with remarks on parameter setting and future directions.",2017-07-01,26,1001,52,1407
340,28657906,Radiogenomics and radiotherapy response modeling,"Advances in patient-specific information and biotechnology have contributed to a new era of computational medicine. Radiogenomics has emerged as a new field that investigates the role of genetics in treatment response to radiation therapy. Radiation oncology is currently attempting to embrace these recent advances and add to its rich history by maintaining its prominent role as a quantitative leader in oncologic response modeling. Here, we provide an overview of radiogenomics starting with genotyping, data aggregation, and application of different modeling approaches based on modifying traditional radiobiological methods or application of advanced machine learning techniques. We highlight the current status and potential for this new field to reshape the landscape of outcome modeling in radiotherapy and drive future advances in computational oncology.",2017-08-01,17,863,48,1376
2934,29872707,Network-based machine learning and graph theory algorithms for precision oncology,"Network-based analytics plays an increasingly important role in precision oncology. Growing evidence in recent studies suggests that cancer can be better understood through mutated or dysregulated pathways or networks rather than individual mutations and that the efficacy of repositioned drugs can be inferred from disease modules in molecular networks. This article reviews network-based machine learning and graph theory algorithms for integrative analysis of personal genomic data and biomedical knowledge bases to identify tumor-specific molecular mechanisms, candidate targets and repositioned drugs for personalized treatment. The review focuses on the algorithmic design and mathematical formulation of these methods to facilitate applications and implementations of network-based analysis in the practice of precision oncology. We review the methods applied in three scenarios to integrate genomic data and network models in different analysis pipelines, and we examine three categories of network-based approaches for repositioning drugs in drug-disease-gene networks. In addition, we perform a comprehensive subnetwork/pathway analysis of mutations in 31 cancer genome projects in the Cancer Genome Atlas and present a detailed case study on ovarian cancer. Finally, we discuss interesting observations, potential pitfalls and future directions in network-based precision oncology.",2017-08-01,22,1392,81,1376
361,28527522,Precision Medicine: Genomic Profiles to Individualize Therapy,"Precision medicine is the application of genotypic and Omics biomarkers to determine the most appropriate, outcome-driven therapy for individual patients. To determine the best choice of therapy, institutions use significant information technology-enabled data from imaging, electronic medical records, sensors in the clinic/hospitals, and wearable sensors to determine treatment response. With genomic profiling, targets to affect a disease course are continuing to be developed. As clonal mutational prevalence continues to be understood, information can be communicated to patients to inform them that resistance is common, requiring collection of more genetic mutations from patients with further biopsies or blood collection.",2017-08-01,4,730,61,1376
327,28693857,How Not To Drown in Data: A Guide for Biomaterial Engineers,"High-throughput assays that produce hundreds of measurements per sample are powerful tools for quantifying cell-material interactions. With advances in automation and miniaturization in material fabrication, hundreds of biomaterial samples can be rapidly produced, which can then be characterized using these assays. However, the resulting deluge of data can be overwhelming. To the rescue are computational methods that are well suited to these problems. Machine learning techniques provide a vast array of tools to make predictions about cell-material interactions and to find patterns in cellular responses. Computational simulations allow researchers to pose and test hypotheses and perform experiments in silico. This review describes approaches from these two domains that can be brought to bear on the problem of analyzing biomaterial screening data.",2017-08-01,5,857,59,1376
324,28711211,Advanced Tissue Characterization and Texture Analysis Using Dual-Energy Computed Tomography: Horizons and Emerging Applications,"In the last article of this issue, advanced analysis capabilities of DECT is reviewed, including spectral Hounsfield unit attenuation curves, virtual monochromatic images, material decomposition maps, tissue effective Z determination, and other advanced post-processing DECT tools, followed by different methods of analysis of the attenuation curves generated using DECT. The article concludes with exciting future horizons and potential applications, such as the use of the rich quantitative data in dual energy CT scans for texture or radiomic analysis and the use of machine learning methods for generation of prediction models using spectral data.",2017-08-01,6,651,127,1376
654,29062966,Bacterial cell-free expression technology to in vitro systems engineering and optimization,"Cell-free expression system is a technology for the synthesis of proteins in vitro. The system is a platform for several bioengineering projects, e.g. cell-free metabolic engineering, evolutionary design of experiments, and synthetic minimal cell construction. Bacterial cell-free protein synthesis system (CFPS) is a robust tool for synthetic biology. The bacteria lysate, the DNA, and the energy module, which are the three optimized sub-systems for in vitro protein synthesis, compose the integrated system. Currently, an optimized E. coli cell-free expression system can produce up to 2.3 mg/mL of a fluorescent reporter protein. Herein, I will describe the features of ATP-regeneration systems for in vitro protein synthesis, and I will present a machine-learning experiment for optimizing the protein yield of E. coli cell-free protein synthesis systems. Moreover, I will introduce experiments on the synthesis of a minimal cell using liposomes as dynamic containers, and E. coli cell-free expression system as biochemical platform for metabolism and gene expression. CFPS can be further integrated with other technologies for novel applications in environmental, medical and material science.",2017-08-01,1,1200,90,1376
652,29063553,Contributions from the 2016 Literature on Clinical Decision Support,"Objectives: To summarize recent research and select the best papers published in 2016 in the field of computerized clinical decision support for the Decision Support section of the IMIA yearbook. Methods: A literature review was performed by searching two bibliographic databases for papers related to clinical decision support systems (CDSSs). The aim was to identify a list of candidate best papers from the retrieved papers that were then peer-reviewed by external reviewers. A consensus meeting of the IMIA editorial team finally selected the best papers on the basis of all reviews and section editor evaluation. Results: Among the 1,145 retrieved papers, the entire review process resulted in the selection of four best papers. The first paper describes machine learning models used to predict breast cancer multidisciplinary team decisions and compares them with two predictors based on guideline knowledge. The second paper introduces a linked-data approach for publication, discovery, and interoperability of CDSSs. The third paper assessed the variation in high-priority drug-drug interaction (DDI) alerts across 14 Electronic Health Record systems, operating in different institutions in the US. The fourth paper proposes a generic framework for modeling multiple concurrent guidelines and detecting their recommendation interactions using semantic web technologies. Conclusions: The process of identifying and selecting best papers in the domain of CDSSs demonstrated that the research in this field is very active concerning diverse dimensions, such as the types of CDSSs, e.g. guideline-based, machine-learning-based, knowledge-fusion-based, etc., and addresses challenging areas, such as the concurrent application of multiple guidelines for comorbid patients, the resolution of interoperability issues, and the evaluation of CDSSs. Nevertheless, this process also showed that CDSSs are not yet fully part of the digitalized healthcare ecosystem. Many challenges remain to be faced with regard to the evidence of their output, the dissemination of their technologies, as well as their adoption for better and safer healthcare delivery.",2017-08-01,3,2150,67,1376
651,29063560,Secondary Use of Recorded or Self-expressed Personal Data: Consumer Health Informatics and Education in the Era of Social Media and Health Apps,"Objective: To summarize the state of the art during the year 2016 in the areas related to consumer health informatics and education with a special emphasis in secondary use of patient data. Methods: We conducted a systematic review of articles published in 2016, using PubMed with a predefined set of queries. We identified over 320 potential articles for review. Papers were considered according to their relevance for the topic of the section. Using consensus, we selected the 15 most representative papers, which were submitted to external reviewers for full review and scoring. Based on the scoring and quality criteria, five papers were finally selected as best papers Results: The five best papers can be grouped in two major areas: 1) methods and tools to identify and collect formal requirements for secondary use of data, and 2) innovative topics highlighting the interest of carrying on ""secondary"" studies on patient data, more specifically on the data self-expressed by patients through social media tools. Regarding the formal requirements about informed consent, the selected papers report a comparison of legal aspects in European countries to find a common and unified grammar around the concept of ""data donation"". Regarding innovative approaches to value patient data, the selected papers report machine learning algorithms to extract knowledge from patient experience and satisfaction with health care delivery, drug and medication use, treatment compliance and barriers during cancer disease, or acceptation of public health actions such as vaccination. Conclusions: Secondary use of patient data (apart from personal health care record data) can be expressed according to many ways. Requirements to allow this secondary use have to be harmonized between countries, and social media platforms can be efficiently used to explore and create knowledge on patient experience with health problems or activities. Machine learning algorithms can explore those massive amounts of data to support health care professionals, and institutions provide more accurate knowledge about use and usage, behaviour, sentiment, or satisfaction about health care delivery.",2017-08-01,2,2170,143,1376
650,29063566,Clinical Research Informatics: Contributions from 2016,"Objectives: To summarize key contributions to current research in the field of Clinical Research Informatics (CRI) and to select the best papers published in 2016. Methods: A bibliographic search using a combination of MeSH and free terms on CRI was performed using PubMed, followed by a double-blind review in order to select a list of candidate best papers to be then peer-reviewed by external reviewers. A consensus meeting between the two section editors and the editorial team was organized to finally conclude on the selection of best papers. Results: Among the 452 papers published in 2016 in the various areas of CRI and returned by the query, the full review process selected four best papers. The authors of the first paper utilized a comprehensive representation of the patient medical record and semi-automatically labeled training sets to create phenotype models via a machine learning process. The second selected paper describes an open source tool chain securely connecting ResearchKit compatible applications (Apps) to the widely-used clinical research infrastructure Informatics for Integrating Biology and the Bedside (i2b2). The third selected paper describes the FAIR Guiding Principles for scientific data management and stewardship. The fourth selected paper focuses on the evaluation of the risk of privacy breaches in releasing genomics datasets. Conclusions: A major trend in the 2016 publications is the variety of research on ""real-world data"" - healthcare-generated data, person health data, and patient-reported outcomes -highlighting the opportunities provided by new machine learning techniques as well as new potential risks of privacy breaches.",2017-08-01,0,1678,54,1376
366,28505027,Automated surveillance of healthcare-associated infections: state of the art,"Purpose of review:                    This review describes recent advances in the field of automated surveillance of healthcare-associated infections (HAIs), with a focus on data sources and the development of semiautomated or fully automated algorithms.              Recent findings:                    The availability of high-quality data in electronic health records and a well-designed information technology (IT) infrastructure to access these data are indispensable for successful implementation of automated HAI surveillance. Previous studies have demonstrated that reliance on stand-alone administrative data is generally unsuited as sole case-finding strategy. Recent attempts to combine multiple administrative and clinical data sources in algorithms yielded more reliable results. Current surveillance practices are mostly limited to single healthcare facilities, but future linkage of multiple databases in a network may allow interfacility surveillance. Although prior surveillance algorithms were often straightforward decision trees based on structured data, recent studies have used a wide variety of techniques for case-finding, including logistic regression and various machine learning methods. In the future, natural language processing may enable the use of unstructured narrative data.              Summary:                    Developments in healthcare IT are rapidly changing the landscape of HAI surveillance. The electronic availability and incorporation of routine care data in surveillance algorithms enhances the reliability, efficiency and standardization of surveillance practices.",2017-08-01,4,1614,76,1376
318,28738313,High throughput phenotyping to accelerate crop breeding and monitoring of diseases in the field,"Effective implementation of technology that facilitates accurate and high-throughput screening of thousands of field-grown lines is critical for accelerating crop improvement and breeding strategies for higher yield and disease tolerance. Progress in the development of field-based high throughput phenotyping methods has advanced considerably in the last 10 years through technological progress in sensor development and high-performance computing. Here, we review recent advances in high throughput field phenotyping technologies designed to inform the genetics of quantitative traits, including crop yield and disease tolerance. Successful application of phenotyping platforms to advance crop breeding and identify and monitor disease requires: (1) high resolution of imaging and environmental sensors; (2) quality data products that facilitate computer vision, machine learning and GIS; (3) capacity infrastructure for data management and analysis; and (4) automated environmental data collection. Accelerated breeding for agriculturally relevant crop traits is key to the development of improved varieties and is critically dependent on high-resolution, high-throughput field-scale phenotyping technologies that can efficiently discriminate better performing lines within a larger population and across multiple environments.",2017-08-01,42,1330,95,1376
353,28577131,Deep Learning for Brain MRI Segmentation: State of the Art and Future Directions,"Quantitative analysis of brain MRI is routine for many neurological diseases and conditions and relies on accurate segmentation of structures of interest. Deep learning-based segmentation approaches for brain MRI are gaining interest due to their self-learning and generalization ability over large amounts of data. As the deep learning architectures are becoming more mature, they gradually outperform previous state-of-the-art classical machine learning algorithms. This review aims to provide an overview of current deep learning-based segmentation approaches for quantitative brain MRI. First we review the current deep learning architectures used for segmentation of anatomical brain structures and brain lesions. Next, the performance, speed, and properties of deep learning approaches are summarized and discussed. Finally, we provide a critical assessment of the current state and identify likely future developments and trends.",2017-08-01,99,936,80,1376
311,28812204,Multivariable Adaptive Artificial Pancreas System in Type 1 Diabetes,"Purpose of review:                    The review summarizes the current state of the artificial pancreas (AP) systems and introduces various new modules that should be included in future AP systems.              Recent findings:                    A fully automated AP must be able to detect and mitigate the effects of meals, exercise, stress and sleep on blood glucose concentrations. This can only be achieved by using a multivariable approach that leverages information from wearable devices that provide real-time streaming data about various physiological variables that indicate imminent changes in blood glucose concentrations caused by meals, exercise, stress and sleep. The development of a fully automated AP will necessitate the design of multivariable and adaptive systems that use information from wearable devices in addition to glucose sensors and modify the models used in their model-predictive alarm and control systems to adapt to the changes in the metabolic state of the user. These AP systems will also integrate modules for controller performance assessment, fault detection and diagnosis, machine learning and classification to interpret various signals and achieve fault-tolerant control. Advances in wearable devices, computational power, and safe and secure communications are enabling the development of fully automated multivariable AP systems.",2017-08-01,8,1374,68,1376
2690,28315069,Toolkits and Libraries for Deep Learning,"Deep learning is an important new area of machine learning which encompasses a wide range of neural network architectures designed to complete various tasks. In the medical imaging domain, example tasks include organ segmentation, lesion detection, and tumor classification. The most popular network architecture for deep learning for images is the convolutional neural network (CNN). Whereas traditional machine learning requires determination and calculation of features from which the algorithm learns, deep learning approaches learn the important features as well as the proper weighting of those features to make predictions for new data. In this paper, we will describe some of the libraries and tools that are available to aid in the construction and efficient execution of deep learning as applied to medical images.",2017-08-01,12,824,40,1376
298,28892073,Progress and roadblocks in the search for brain-based biomarkers of autism and attention-deficit/hyperactivity disorder,"Children with neurodevelopmental disorders benefit most from early interventions and treatments. The development and validation of brain-based biomarkers to aid in objective diagnosis can facilitate this important clinical aim. The objective of this review is to provide an overview of current progress in the use of neuroimaging to identify brain-based biomarkers for autism spectrum disorder (ASD) and attention-deficit/hyperactivity disorder (ADHD), two prevalent neurodevelopmental disorders. We summarize empirical work that has laid the foundation for using neuroimaging to objectively quantify brain structure and function in ways that are beginning to be used in biomarker development, noting limitations of the data currently available. The most successful machine learning methods that have been developed and applied to date are discussed. Overall, there is increasing evidence that specific features (for example, functional connectivity, gray matter volume) of brain regions comprising the salience and default mode networks can be used to discriminate ASD from typical development. Brain regions contributing to successful discrimination of ADHD from typical development appear to be more widespread, however there is initial evidence that features derived from frontal and cerebellar regions are most informative for classification. The identification of brain-based biomarkers for ASD and ADHD could potentially assist in objective diagnosis, monitoring of treatment response and prediction of outcomes for children with these neurodevelopmental disorders. At present, however, the field has yet to identify reliable and reproducible biomarkers for these disorders, and must address issues related to clinical heterogeneity, methodological standardization and cross-site validation before further progress can be achieved.",2017-08-01,37,1838,119,1376
2696,28283186,Imaging Genetics and Genomics in Psychiatry: A Critical Review of Progress and Potential,"Imaging genetics and genomics research has begun to provide insight into the molecular and genetic architecture of neural phenotypes and the neural mechanisms through which genetic risk for psychopathology may emerge. As it approaches its third decade, imaging genetics is confronted by many challenges, including the proliferation of studies using small sample sizes and diverse designs, limited replication, problems with harmonization of neural phenotypes for meta-analysis, unclear mechanisms, and evidence that effect sizes may be more modest than originally posited, with increasing evidence of polygenicity. These concerns have encouraged the field to grow in many new directions, including the development of consortia and large-scale data collection projects and the use of novel methods (e.g., polygenic approaches, machine learning) that enhance the quality of imaging genetic studies but also introduce new challenges. We critically review progress in imaging genetics and offer suggestions and highlight potential pitfalls of novel approaches. Ultimately, the strength of imaging genetics and genomics lies in their translational and integrative potential with other research approaches (e.g., nonhuman animal models, psychiatric genetics, pharmacologic challenge) to elucidate brain-based pathways that give rise to the vast individual differences in behavior as well as risk for psychopathology.",2017-08-01,42,1410,88,1376
300,28879175,Computer Vision Malaria Diagnostic Systems-Progress and Prospects,"Accurate malaria diagnosis is critical to prevent malaria fatalities, curb overuse of antimalarial drugs, and promote appropriate management of other causes of fever. While several diagnostic tests exist, the need for a rapid and highly accurate malaria assay remains. Microscopy and rapid diagnostic tests are the main diagnostic modalities available, yet they can demonstrate poor performance and accuracy. Automated microscopy platforms have the potential to significantly improve and standardize malaria diagnosis. Based on image recognition and machine learning algorithms, these systems maintain the benefits of light microscopy and provide improvements such as quicker scanning time, greater scanning area, and increased consistency brought by automation. While these applications have been in development for over a decade, recently several commercial platforms have emerged. In this review, we discuss the most advanced computer vision malaria diagnostic technologies and investigate several of their features which are central to field use. Additionally, we discuss the technological and policy barriers to implementing these technologies in low-resource settings world-wide.",2017-08-01,4,1185,65,1376
329,28689314,Overview of deep learning in medical imaging,"The use of machine learning (ML) has been increasing rapidly in the medical imaging field, including computer-aided diagnosis (CAD), radiomics, and medical image analysis. Recently, an ML area called deep learning emerged in the computer vision field and became very popular in many fields. It started from an event in late 2012, when a deep-learning approach based on a convolutional neural network (CNN) won an overwhelming victory in the best-known worldwide computer vision competition, ImageNet Classification. Since then, researchers in virtually all fields, including medical imaging, have started actively participating in the explosively growing field of deep learning. In this paper, the area of deep learning in medical imaging is overviewed, including (1) what was changed in machine learning before and after the introduction of deep learning, (2) what is the source of the power of deep learning, (3) two major deep-learning models: a massive-training artificial neural network (MTANN) and a convolutional neural network (CNN), (4) similarities and differences between the two models, and (5) their applications to medical imaging. This review shows that ML with feature input (or feature-based ML) was dominant before the introduction of deep learning, and that the major and essential difference between ML before and after deep learning is the learning of image data directly without object segmentation or feature extraction; thus, it is the source of the power of deep learning, although the depth of the model is an important attribute. The class of ML with image input (or image-based ML) including deep learning has a long history, but recently gained popularity due to the use of the new terminology, deep learning. There are two major models in this class of ML in medical imaging, MTANN and CNN, which have similarities as well as several differences. In our experience, MTANNs were substantially more efficient in their development, had a higher performance, and required a lesser number of training cases than did CNNs. ""Deep learning"", or ML with image input, in medical imaging is an explosively growing, promising field. It is expected that ML with image input will be the mainstream area in the field of medical imaging in the next few decades.",2017-09-01,84,2275,44,1345
626,29184889,The conceptualization of a Just-In-Time Adaptive Intervention (JITAI) for the reduction of sedentary behavior in older adults,"Low physical activity and high sedentary behavior in older adults can be addressed with interventions that are delivered through modern technology. Just-In-Time Adaptive Interventions (JITAIs) are an emerging technology-driven behavior-change intervention type and capitalize on data that is collected via mobile sensing technology (e.g., smartphones) to trigger appropriate support in real-life. In this paper we integrated behavior change and aging theory and research as well as knowledge around older adult's technology use to conceptualize a JITAI targeting the reduction of sedentary behavior in older adults. The JITAIs ultimate goal is to encourage older adults to take regular activity breaks from prolonged sitting. As a proximal outcome, we suggest the number of daily activity breaks from sitting. Support provided to interrupt sitting time can be based on tailoring variables: (I) the current accumulated sitting time; (II) the location of the individual; (III) the time of the day; (IV) the frequency of daily support prompts; and (V) the response to previous support prompts. Data on these variables can be collected using sensors that are commonly inbuilt into smartphones (e.g., accelerometer, GPS). Support prompts might be best delivered via traditional text messages as older adults are usually familiar and comfortable with this function. The content of the prompts should encourage breaks from prolonged sitting by highlighting immediate benefits of sitting time interruptions. Additionally, light physical activities that could be done during the breaks should also be presented (e.g., walking into the kitchen to prepare a cup of tea). Although the conceptualized JITAI can be developed and implemented to test its efficacy, more work is required to identify ways to collect, aggregate, organize and immediately use dense data on the proposed and other potentially important tailoring variables. Machine learning and other computational modelling techniques commonly used by computer scientists and engineers appear promising. With this, to develop powerful JITAIs and to actualize the full potential of modern sensing technologies transdisciplinary approaches are required.",2017-09-01,8,2198,125,1345
335,28666178,A systematic review of gait analysis methods based on inertial sensors and adaptive algorithms,"The conventional methods to assess human gait are either expensive or complex to be applied regularly in clinical practice. To reduce the cost and simplify the evaluation, inertial sensors and adaptive algorithms have been utilized, respectively. This paper aims to summarize studies that applied adaptive also called artificial intelligence (AI) algorithms to gait analysis based on inertial sensor data, verifying if they can support the clinical evaluation. Articles were identified through searches of the main databases, which were encompassed from 1968 to October 2016. We have identified 22 studies that met the inclusion criteria. The included papers were analyzed due to their data acquisition and processing methods with specific questionnaires. Concerning the data acquisition, the mean score is 6.11.62, what implies that 13 of 22 papers failed to report relevant outcomes. The quality assessment of AI algorithms presents an above-average rating (8.21.84). Therefore, AI algorithms seem to be able to support gait analysis based on inertial sensor data. Further research, however, is necessary to enhance and standardize the application in patients, since most of the studies used distinct methods to evaluate healthy subjects.",2017-09-01,34,1242,94,1345
666,29037014,Deep into the Brain: Artificial Intelligence in Stroke Imaging,"Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.",2017-09-01,30,860,62,1345
320,28728937,The impact of machine learning techniques in the study of bipolar disorder: A systematic review,"Machine learning techniques provide new methods to predict diagnosis and clinical outcomes at an individual level. We aim to review the existing literature on the use of machine learning techniques in the assessment of subjects with bipolar disorder. We systematically searched PubMed, Embase and Web of Science for articles published in any language up to January 2017. We found 757 abstracts and included 51 studies in our review. Most of the included studies used multiple levels of biological data to distinguish the diagnosis of bipolar disorder from other psychiatric disorders or healthy controls. We also found studies that assessed the prediction of clinical outcomes and studies using unsupervised machine learning to build more consistent clinical phenotypes of bipolar disorder. We concluded that given the clinical heterogeneity of samples of patients with BD, machine learning techniques may provide clinicians and researchers with important insights in fields such as diagnosis, personalized treatment and prognosis orientation.",2017-09-01,24,1043,95,1345
687,32309594,Artificial Intelligence versus Doctors' Intelligence: A Glance on Machine Learning Benefaction in Electrocardiography,"Computational machine learning, especially self-enhancing algorithms, prove remarkable effectiveness in applications, including cardiovascular medicine. This review summarizes and cross-compares the current machine learning algorithms applied to electrocardiogram interpretation. In practice, continuous real-time monitoring of electrocardiograms is still difficult to realize. Furthermore, automated ECG interpretation by implementing specific artificial intelligence algorithms is even more challenging. By collecting large datasets from one individual, computational approaches can assure an efficient personalized treatment strategy, such as a correct prediction on patient-specific disease progression, therapeutic success rate and limitations of certain interventions, thus reducing the hospitalization costs and physicians' workload. Clearly such aims can be achieved by a perfect symbiosis of a multidisciplinary team involving clinicians, researchers and computer scientists. Summarizing, continuous cross-examination between machine intelligence and human intelligence is a combination of precision, rationale and high-throughput scientific engine integrated into a challenging framework of big data science.",2017-09-01,0,1218,117,1345
674,29018336,Encoding and Decoding Models in Cognitive Electrophysiology,"Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of ""Encoding"" models, in which stimulus features are used to model brain activity, and ""Decoding"" models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aim is to provide a practical understanding of predictive modeling of human brain data and to propose best-practices in conducting these analyses.",2017-09-01,23,1791,59,1345
357,28544059,Objective assessment of the evolutionary action equation for the fitness effect of missense mutations across CAGI-blinded contests,"A major challenge in genome interpretation is to estimate the fitness effect of coding variants of unknown significance (VUS). Labor, limited understanding of protein functions, and lack of assays generally limit direct experimental assessment of VUS, and make robust and accurate computational approaches a necessity. Often, however, algorithms that predict mutational effect disagree among themselves and with experimental data, slowing their adoption for clinical diagnostics. To objectively assess such methods, the Critical Assessment of Genome Interpretation (CAGI) community organizes contests to predict unpublished experimental data, available only to CAGI assessors. We review here the CAGI performance of evolutionary action (EA) predictions of mutational impact. EA models the fitness effect of coding mutations analytically, as a product of the gradient of the fitness landscape times the perturbation size. In practice, these terms are computed from phylogenetic considerations as the functional sensitivity of the mutated site and as the magnitude of amino acid substitution, respectively, and yield the percentage loss of wild-type activity. In five CAGI challenges, EA consistently performed on par or better than sophisticated machine learning approaches. This objective assessment suggests that a simple differential model of evolution can interpret the fitness effect of coding variations, opening diverse clinical applications.",2017-09-01,11,1448,130,1345
315,28791894,Decoding HIV resistance: from genotype to therapy,"Genetic variation in HIV poses a major challenge for prevention and treatment of the AIDS pandemic. Resistance occurs by mutations in the target proteins that lower affinity for the drug or alter the protein dynamics, thereby enabling viral replication in the presence of the drug. Due to the prevalence of drug-resistant strains, monitoring the genotype of the infecting virus is recommended. Computational approaches for predicting resistance from genotype data and guiding therapy are discussed. Many prediction methods rely on rules derived from known resistance-associated mutations, however, statistical or machine learning can improve the classification accuracy and assess unknown mutations. Adding classifiers such as information on the atomic structure of the protein can further enhance the predictions.",2017-09-01,3,814,49,1345
384,28366290,Microbiome Tools for Forensic Science,"Microbes are present at every crime scene and have been used as physical evidence for over a century. Advances in DNA sequencing and computational approaches have led to recent breakthroughs in the use of microbiome approaches for forensic science, particularly in the areas of estimating postmortem intervals (PMIs), locating clandestine graves, and obtaining soil and skin trace evidence. Low-cost, high-throughput technologies allow us to accumulate molecular data quickly and to apply sophisticated machine-learning algorithms, building generalizable predictive models that will be useful in the criminal justice system. In particular, integrating microbiome and metabolomic data has excellent potential to advance microbial forensics.",2017-09-01,18,739,37,1345
2729,28092510,Unobtrusive and Wearable Systems for Automatic Dietary Monitoring,"The threat of obesity, diabetes, anorexia, and bulimia in our society today has motivated extensive research on dietary monitoring. Standard self-report methods such as 24-h recall and food frequency questionnaires are expensive, burdensome, and unreliable to handle the growing health crisis. Long-term activity monitoring in daily living is a promising approach to provide individuals with quantitative feedback that can encourage healthier habits. Although several studies have attempted automating dietary monitoring using wearable, handheld, smart-object, and environmental systems, it remains an open research problem. This paper aims to provide a comprehensive review of wearable and hand-held approaches from 2004 to 2016. Emphasis is placed on sensor types used, signal analysis and machine learning methods, as well as a benchmark of state-of-the art work in this field. Key issues, challenges, and gaps are highlighted to motivate future work toward development of effective, reliable, and robust dietary monitoring systems.",2017-09-01,7,1035,65,1345
664,29044470,Analysis of Gene-Gene Interactions,"The goal of this unit is to introduce epistasis, or gene-gene interactions, as a significant contributor to the genetic architecture of complex traits, including disease susceptibility. This unit begins with an historical overview of the concept of epistasis and the challenges inherent in the identification of potential gene-gene interactions. Then, it reviews statistical and machine learning methods for discovering epistasis in the context of genetic studies of quantitative and categorical traits. This unit concludes with a discussion of meta-analysis, replication, and other topics of active research.  2017 by John Wiley & Sons, Inc.",2017-10-01,5,643,34,1315
645,29114182,Application of Deep Learning in Automated Analysis of Molecular Images in Cancer: A Survey,"Molecular imaging enables the visualization and quantitative analysis of the alterations of biological procedures at molecular and/or cellular level, which is of great significance for early detection of cancer. In recent years, deep leaning has been widely used in medical imaging analysis, as it overcomes the limitations of visual assessment and traditional machine learning techniques by extracting hierarchical features with powerful representation capability. Research on cancer molecular images using deep learning techniques is also increasing dynamically. Hence, in this paper, we review the applications of deep learning in molecular imaging in terms of tumor lesion segmentation, tumor classification, and survival prediction. We also outline some future directions in which researchers may develop more powerful deep learning models for better performance in the applications in cancer molecular imaging.",2017-10-01,6,916,90,1315
364,28520235,First Principles Neural Network Potentials for Reactive Simulations of Large Molecular and Condensed Systems,"Modern simulation techniques have reached a level of maturity which allows a wide range of problems in chemistry and materials science to be addressed. Unfortunately, the application of first principles methods with predictive power is still limited to rather small systems, and despite the rapid evolution of computer hardware no fundamental change in this situation can be expected. Consequently, the development of more efficient but equally reliable atomistic potentials to reach an atomic level understanding of complex systems has received considerable attention in recent years. A promising new development has been the introduction of machine learning (ML) methods to describe the atomic interactions. Once trained with electronic structure data, ML potentials can accelerate computer simulations by several orders of magnitude, while preserving quantum mechanical accuracy. This Review considers the methodology of an important class of ML potentials that employs artificial neural networks.",2017-10-01,24,1000,108,1315
306,28838801,Optimization of infobutton design and Implementation: A systematic review,"Objective:                    Infobuttons are clinical decision tools embedded in the electronic health record that attempt to link clinical data with context sensitive knowledge resources. We systematically reviewed technical approaches that contribute to improved infobutton design, implementation and functionality.              Methods:                    We searched databases including MEDLINE, EMBASE, and the Cochrane Library database from inception to March 1, 2016 for studies describing the use of infobuttons. We selected full review comparative studies, usability studies, and qualitative studies examining infobutton design and implementation. We abstracted usability measures such as user satisfaction, impact, and efficiency, as well as prediction accuracy of infobutton content retrieval algorithms and infobutton adoption/interoperability.              Results:                    We found 82 original research studies on infobuttons. Twelve studies met criteria for detailed abstraction. These studies investigated infobutton interoperability (1 study); tools to help tailor infobutton functionality (1 study); interventions to improve user experience (7 studies); and interventions to improve content retrieval by improving prediction of relevant knowledge resources and information needs (3 studies). In-depth interviews with implementers showed the Health Level Seven (HL7) Infobutton standard to be simple and easy to implement. A usability study demonstrated the feasibility of a tool to help medical librarians tailor infobutton functionality. User experience studies showed that access to resources with which users are familiar increased user satisfaction ratings; and that links to specific subsections of drug monographs increased information seeking efficiency. However, none of the user experience improvements led to increased usage uptake. Recommender systems based on machine learning algorithms outperformed hand-crafted rules in the prediction of relevant resources and clinicians' information needs in a laboratory setting, but no studies were found using these techniques in clinical settings. Improved content indexing in one study led to improved content retrieval across three health care organizations.              Conclusion:                    Best practice technical approaches to ensure optimal infobutton functionality, design and implementation remain understudied. The HL7 Infobutton standard has supported wide adoption of infobutton functionality among clinical information systems and knowledge resources. Limited evidence supports infobutton enhancements such as links to specific subtopics, configuration of optimal resources for specific tasks and users, and improved indexing and content coverage. Further research is needed to investigate user experience improvements to increase infobutton use and effectiveness.",2017-10-01,4,2871,73,1315
656,29056906,Random Forest Algorithm for the Classification of Neuroimaging Data in Alzheimer's Disease: A Systematic Review,"Objective: Machine learning classification has been the most important computational development in the last years to satisfy the primary need of clinicians for automatic early diagnosis and prognosis. Nowadays, Random Forest (RF) algorithm has been successfully applied for reducing high dimensional and multi-source data in many scientific realms. Our aim was to explore the state of the art of the application of RF on single and multi-modal neuroimaging data for the prediction of Alzheimer's disease. Methods: A systematic review following PRISMA guidelines was conducted on this field of study. In particular, we constructed an advanced query using boolean operators as follows: (""random forest"" OR ""random forests"") AND neuroimaging AND (""alzheimer's disease"" OR alzheimer's OR alzheimer) AND (prediction OR classification). The query was then searched in four well-known scientific databases: Pubmed, Scopus, Google Scholar and Web of Science. Results: Twelve articles-published between the 2007 and 2017-have been included in this systematic review after a quantitative and qualitative selection. The lesson learnt from these works suggest that when RF was applied on multi-modal data for prediction of Alzheimer's disease (AD) conversion from the Mild Cognitive Impairment (MCI), it produces one of the best accuracies to date. Moreover, the RF has important advantages in terms of robustness to overfitting, ability to handle highly non-linear data, stability in the presence of outliers and opportunity for efficient parallel processing mainly when applied on multi-modality neuroimaging data, such as, MRI morphometric, diffusion tensor imaging, and PET images. Conclusions: We discussed the strengths of RF, considering also possible limitations and by encouraging further studies on the comparisons of this algorithm with other commonly used classification approaches, particularly in the early prediction of the progression from MCI to AD.",2017-10-01,40,1955,111,1315
657,29056896,Classical Statistics and Statistical Learning in Imaging Neuroscience,"Brain-imaging research has predominantly generated insight by means of classical statistics, including regression-type analyses and null-hypothesis testing using t-test and ANOVA. Throughout recent years, statistical learning methods enjoy increasing popularity especially for applications in rich and complex data, including cross-validated out-of-sample prediction using pattern classification and sparsity-inducing regression. This concept paper discusses the implications of inferential justifications and algorithmic methodologies in common data analysis scenarios in neuroimaging. It is retraced how classical statistics and statistical learning originated from different historical contexts, build on different theoretical foundations, make different assumptions, and evaluate different outcome metrics to permit differently nuanced conclusions. The present considerations should help reduce current confusion between model-driven classical hypothesis testing and data-driven learning algorithms for investigating the brain with imaging techniques.",2017-10-01,25,1055,69,1315
667,29035422,The Use of Telemedicine and Mobile Technology to Promote Population Health and Population Management for Psychiatric Disorders,"Purpose of review:                    This article discusses recent applications in telemedicine to promote the goals of population health and population management for people suffering psychiatric disorders.              Recent findings:                    The use of telemedicine to promote collaborative care, self-monitoring and chronic disease management, and population screening has demonstrated broad applicability and effectiveness. Collaborative care using videoconferencing to facilitate mental health specialty consults has demonstrated effectiveness in the treatment of depression, PTSD, and also ADHD in pediatric populations. Mobile health is currently being harnessed to monitor patient symptom trajectories with the goal of using machine learning algorithms to predict illness relapse. Patient portals serve as a bridge between patients and providers. They provide an electronically secure shared space for providers and patients to collaborate and optimize care. To date, research has supported the effectiveness of telemedicine in promoting population health. Future endeavors should focus on developing the most effective clinical protocols for using these technologies to ensure long-term use and maximum effectiveness in reducing population burden of mental health.",2017-10-01,6,1287,126,1315
2709,28232122,"Functional connectomics from a ""big data"" perspective","In the last decade, explosive growth regarding functional connectome studies has been observed. Accumulating knowledge has significantly contributed to our understanding of the brain's functional network architectures in health and disease. With the development of innovative neuroimaging techniques, the establishment of large brain datasets and the increasing accumulation of published findings, functional connectomic research has begun to move into the era of ""big data"", which generates unprecedented opportunities for discovery in brain science and simultaneously encounters various challenging issues, such as data acquisition, management and analyses. Big data on the functional connectome exhibits several critical features: high spatial and/or temporal precision, large sample sizes, long-term recording of brain activity, multidimensional biological variables (e.g., imaging, genetic, demographic, cognitive and clinic) and/or vast quantities of existing findings. We review studies regarding functional connectomics from a big data perspective, with a focus on recent methodological advances in state-of-the-art image acquisition (e.g., multiband imaging), analysis approaches and statistical strategies (e.g., graph theoretical analysis, dynamic network analysis, independent component analysis, multivariate pattern analysis and machine learning), as well as reliability and reproducibility validations. We highlight the novel findings in the application of functional connectomic big data to the exploration of the biological mechanisms of cognitive functions, normal development and aging and of neurological and psychiatric disorders. We advocate the urgent need to expand efforts directed at the methodological challenges and discuss the direction of applications in this field.",2017-10-01,23,1796,53,1315
2766,27815038,"Phenotypes in obstructive sleep apnea: A definition, examples and evolution of approaches","Obstructive sleep apnea (OSA) is a complex and heterogeneous disorder and the apnea hypopnea index alone can not capture the diverse spectrum of the condition. Enhanced phenotyping can improve prognostication, patient selection for clinical trials, understanding of mechanisms, and personalized treatments. In OSA, multiple condition characteristics have been termed ""phenotypes."" To help classify patients into relevant prognostic and therapeutic categories, an OSA phenotype can be operationally defined as: ""A category of patients with OSA distinguished from others by a single or combination of disease features, in relation to clinically meaningful attributes (symptoms, response to therapy, health outcomes, quality of life)."" We review approaches to clinical phenotyping in OSA, citing examples of increasing analytic complexity. Although clinical feature based OSA phenotypes with significant prognostic and treatment implications have been identified (e.g., excessive daytime sleepiness OSA), many current categorizations lack association with meaningful outcomes. Recent work focused on pathophysiologic risk factors for OSA (e.g., arousal threshold, craniofacial morphology, chemoreflex sensitivity) appears to capture heterogeneity in OSA, but requires clinical validation. Lastly, we discuss the use of machine learning as a promising phenotyping strategy that can integrate multiple types of data (genomic, molecular, cellular, clinical) to identify unique, meaningful OSA phenotypes.",2017-10-01,49,1498,89,1315
686,28971278,A Systematic Review of Wearable Systems for Cancer Detection: Current State and Challenges,"Rapid growth of sensor and computing platforms have introduced the wearable systems. In recent years, wearable systems have led to new applications across all medical fields. The aim of this review is to present current state-of-the-art approach in the field of wearable system based cancer detection and identify key challenges that resist it from clinical adoption. A total of 472 records were screened and 11 were finally included in this study. Two types of records were studied in this context that includes 45% research articles and 55% manufactured products. The review was performed per PRISMA guidelines where considerations was given to records that were published or reported between 2009 and 2017. The identified records included 4 cancer detecting wearable systems such as breast cancer (36.3%), skin cancer (36.3%), prostate cancer (18.1%), and multi-type cancer (9%). Most works involved sensor based smart systems comprising of microcontroller, Bluetooth module, and smart phone. Few demonstrated Ultra-Wide Band (i.e. UWB) antenna based wearable systems. Skin cancer detecting wearable systems were most comprehensible ones. The current works are gradually progressing with seamless integration of sensory units along with smart networking. However, they lack in cloud computing and long-range communication paradigms. Artificial intelligence and machine learning are key ports that need to be attached with current wearable systems. Further, clinical inertia, lack of awareness, and high cost are altogether pulling back the actual growth of such system. It is well comprehended that upon sincere orientation of all identified challenges, wearable systems would emerge as vital alternative to futuristic cancer detection.",2017-10-01,8,1739,90,1315
671,29027093,Long-Term Prognostic Value of Coronary Computed Tomography Angiography,"Coronary CT angiography (CTA) is a highly accurate test for the diagnosis of coronary artery disease (CAD), with its use guided by numerous contemporary appropriate use criteria and clinical guidelines. Unique among non-invasive tests for CAD, coronary CTA provides direct visualization of coronary atherosclerosis for the assessment of angiographic stenosis, as well as validated measures of plaque vulnerability. Long-term studies now clearly demonstrate that the absence of CAD on coronary CTA identifies a patient that is at very low risk for future cardiovascular events. Conversely, the presence, location, and severity of CAD as measured on coronary CTA provide powerful prognostic information that is superior to traditional risk factors and other clinical variables. Observational studies and data obtained from clinical trials suggest that the anatomic information derived from coronary CTA significantly increases the utilization of statins and aspirin. Furthermore, these changes are associated with reductions in the risk for mortality, revascularizations, and incident myocardial infarctions among subjects with coronary atherosclerosis. As a result, current societal consensus statements have attempted to standardize coronary CTA reporting, to include incorporation of vulnerable plaque features and recommendations on the use of preventive therapies, such as statins, so to more consistently link important prognostic findings on coronary CTA to appropriate preventive and therapeutic interventions. Automated measures of total coronary plaque volume, machine learning, and CT-derived fractional flow reserve may further refine the prognostic accuracy of coronary CTA. Herein, we summarize recently published literature that reports the long-term ( 5 years of follow-up) prognostic usefulness of coronary CTA.",2017-10-01,1,1827,70,1315
378,28418201,"Chemoinformatics at IFP Energies Nouvelles: Applications in the Fields of Energy, Transport, and Environment","The objective of the present paper is to summarize chemoinformatics based research, and more precisely, the development of quantitative structure property relationships performed at IFP Energies nouvelles (IFPEN) during the last decade. A special focus is proposed on research activities performed in the ""Thermodynamics and Molecular Simulation"" department, i. e. the use of multiscale molecular simulation methods in responses to projects. Molecular simulation techniques can be envisaged to supplement dataset when experimental information lacks, thus the review includes a section dedicated to molecular simulation codes, development of intermolecular potentials, and some of their possible applications. Know-how and feedback from our experiences in terms of machine learning application for thermophysical property predictions are included in a section dealing with methodological aspects. The generic character of chemoinformatics is emphasized through applications in the fields of energy, transport, and environment, with illustrations for three IFPEN business units: ""Transports"", ""Energy Resources"", and ""Processes"". More precisely, the review focus on different challenges such as the prediction of properties for alternative fuels, the prediction of fuel compatibility with polymeric materials, the prediction of properties for surfactants usable in chemical enhanced oil recovery, and the prediction of guest-host interactions between gases and nanoporous materials in the frame of carbon dioxide capture or gas separation activities.",2017-10-01,0,1548,108,1315
322,28716627,Membrane proteins structures: A review on computational modeling tools,"Background:                    Membrane proteins (MPs) play diverse and important functions in living organisms. They constitute 20% to 30% of the known bacterial, archaean and eukaryotic organisms' genomes. In humans, their importance is emphasized as they represent 50% of all known drug targets. Nevertheless, experimental determination of their three-dimensional (3D) structure has proven to be both time consuming and rather expensive, which has led to the development of computational algorithms to complement the available experimental methods and provide valuable insights.              Scope of review:                    This review highlights the importance of membrane proteins and how computational methods are capable of overcoming challenges associated with their experimental characterization. It covers various MP structural aspects, such as lipid interactions, allostery, and structure prediction, based on methods such as Molecular Dynamics (MD) and Machine-Learning (ML).              Major conclusions:                    Recent developments in algorithms, tools and hybrid approaches, together with the increase in both computational resources and the amount of available data have resulted in increasingly powerful and trustworthy approaches to model MPs.              General significance:                    Even though MPs are elementary and important in nature, the determination of their 3D structure has proven to be a challenging endeavor. Computational methods provide a reliable alternative to experimental methods. In this review, we focus on computational techniques to determine the 3D structure of MP and characterize their binding interfaces. We also summarize the most relevant databases and software programs available for the study of MPs.",2017-10-01,19,1779,70,1315
625,29184897,"Deep learning for cardiac computer-aided diagnosis: benefits, issues & solutions","Cardiovascular diseases are one of the top causes of deaths worldwide. In developing nations and rural areas, difficulties with diagnosis and treatment are made worse due to the deficiency of healthcare facilities. A viable solution to this issue is telemedicine, which involves delivering health care and sharing medical knowledge at a distance. Additionally, mHealth, the utilization of mobile devices for medical care, has also proven to be a feasible choice. The integration of telemedicine, mHealth and computer-aided diagnosis systems with the fields of machine and deep learning has enabled the creation of effective services that are adaptable to a multitude of scenarios. The objective of this review is to provide an overview of heart disease diagnosis and management, especially within the context of rural healthcare, as well as discuss the benefits, issues and solutions of implementing deep learning algorithms to improve the efficacy of relevant medical applications.",2017-10-01,3,982,80,1315
693,28918672,"Protein complexes, big data, machine learning and integrative proteomics: lessons learned over a decade of systematic analysis of protein interaction networks","Elucidation of the networks of physical (functional) interactions present in cells and tissues is fundamental for understanding the molecular organization of biological systems, the mechanistic basis of essential and disease-related processes, and for functional annotation of previously uncharacterized proteins (via guilt-by-association or -correlation). After a decade in the field, we felt it timely to document our own experiences in the systematic analysis of protein interaction networks. Areas covered: Researchers worldwide have contributed innovative experimental and computational approaches that have driven the rapidly evolving field of 'functional proteomics'. These include mass spectrometry-based methods to characterize macromolecular complexes on a global-scale and sophisticated data analysis tools - most notably machine learning - that allow for the generation of high-quality protein association maps. Expert commentary: Here, we recount some key lessons learned, with an emphasis on successful workflows, and challenges, arising from our own and other groups' ongoing efforts to generate, interpret and report proteome-scale interaction networks in increasingly diverse biological contexts.",2017-10-01,6,1213,158,1315
676,28990839,A review of intelligent systems for heart sound signal analysis,"Intelligent computer-aided diagnosis (CAD) systems can enhance the diagnostic capabilities of physicians and reduce the time required for accurate diagnosis. CAD systems could provide physicians with a suggestion about the diagnostic of heart diseases. The objective of this paper is to review the recent published preprocessing, feature extraction and classification techniques and their state of the art of phonocardiogram (PCG) signal analysis. Published literature reviewed in this paper shows the potential of machine learning techniques as a design tool in PCG CAD systems and reveals that the CAD systems for PCG signal analysis are still an open problem. Related studies are compared to their datasets, feature extraction techniques and the classifiers they used. Current achievements and limitations in developing CAD systems for PCG signal analysis using machine learning techniques are presented and discussed. In the light of this review, a number of future research directions for PCG signal analysis are provided.",2017-10-01,3,1027,63,1315
688,28956772,Machine Learning Approaches in Cardiovascular Imaging,"Cardiovascular imaging technologies continue to increase in their capacity to capture and store large quantities of data. Modern computational methods, developed in the field of machine learning, offer new approaches to leveraging the growing volume of imaging data available for analyses. Machine learning methods can now address data-related problems ranging from simple analytic queries of existing measurement data to the more complex challenges involved in analyzing raw images. To date, machine learning has been used in 2 broad and highly interconnected areas: automation of tasks that might otherwise be performed by a human and generation of clinically important new knowledge. Most cardiovascular imaging studies have focused on task-oriented problems, but more studies involving algorithms aimed at generating new clinical insights are emerging. Continued expansion in the size and dimensionality of cardiovascular imaging databases is driving strong interest in applying powerful deep learning methods, in particular, to analyze these data. Overall, the most effective approaches will require an investment in the resources needed to appropriately prepare such large data sets for analyses. Notwithstanding current technical and logistical challenges, machine learning and especially deep learning methods have much to offer and will substantially impact the future practice and science of cardiovascular imaging.",2017-10-01,20,1425,53,1315
349,28624633,From flamingo dance to (desirable) drug discovery: a nature-inspired approach,"The therapeutic effects of drugs are well known to result from their interaction with multiple intracellular targets. Accordingly, the pharma industry is currently moving from a reductionist approach based on a 'one-target fixation' to a holistic multitarget approach. However, many drug discovery practices are still procedural abstractions resulting from the attempt to understand and address the action of biologically active compounds while preventing adverse effects. Here, we discuss how drug discovery can benefit from the principles of evolutionary biology and report two real-life case studies. We do so by focusing on the desirability principle, and its many features and applications, such as machine learning-based multicriteria virtual screening.",2017-10-01,4,759,77,1315
328,28691131,A survey of context recognition in surgery,"With the introduction of operating rooms of the future context awareness has gained importance in the surgical environment. This paper organizes and reviews different approaches for recognition of context in surgery. Major electronic research databases were queried to obtain relevant publications submitted between the years 2010 and 2015. Three different types of context were identified: (i) the surgical workflow context, (ii) surgeon's cognitive and (iii) technical state context. A total of 52 relevant studies were identified and grouped based on the type of context detected and sensors used. Different approaches were summarized to provide recommendations for future research. There is still room for improvement in terms of methods used and evaluations performed. Machine learning should be used more extensively to uncover hidden relationships between different properties of the surgeon's state, particularly when performing cognitive context recognition. Furthermore, validation protocols should be improved by performing more evaluations in situ and with a higher number of unique participants. The paper also provides a structured outline of recent context recognition methods to facilitate development of new generation context-aware surgical support systems.",2017-10-01,1,1275,42,1315
680,28985932,Machine Learning Applications to Resting-State Functional MR Imaging Analysis,"Machine learning is one of the most exciting and rapidly expanding fields within computer science. Academic and commercial research entities are investing in machine learning methods, especially in personalized medicine via patient-level classification. There is great promise that machine learning methods combined with resting state functional MR imaging will aid in diagnosis of disease and guide potential treatment for conditions thought to be impossible to identify based on imaging alone, such as psychiatric disorders. We discuss machine learning methods and explore recent advances.",2017-11-01,1,591,77,1284
653,29063093,NMR window of molecular complexity showing homeostasis in superorganisms,"NMR offers tremendous advantages in the analyses of molecular complexity, such as crude bio-fluids, bio-extracts, and intact cells and tissues. Here we introduce recent applications of NMR approaches, as well as next generation sequencing (NGS), for the evaluation of human and environmental health (i.e., maintenance of a homeostatic state) based on metabolic and microbial profiling and data science. We describe useful databases and web tools that are used to support these studies by facilitating the characterization of metabolites from complex NMR spectra. Because the NMR spectra of metabolic mixtures can produce numerical matrix data (e.g., chemical shift versus intensity) with high reproducibility and inter-institution convertibility, advanced data science approaches, such as multivariate analysis and machine learning, are desirable; therefore, we also introduce informatics techniques derived from heterogeneously measured data, such as environmental microbiota, for the extraction of submerged information using data science approaches. We summarize recent studies of microbiomes that are based on these techniques and show that, particularly in human studies, NMR-based metabolic characterization of non-invasive samples, such as feces, can provide a large quantity of beneficial information regarding human health and disease.",2017-11-01,2,1344,72,1284
299,28881183,From machine learning to deep learning: progress in machine intelligence for rational drug discovery,"Machine intelligence, which is normally presented as artificial intelligence, refers to the intelligence exhibited by computers. In the history of rational drug discovery, various machine intelligence approaches have been applied to guide traditional experiments, which are expensive and time-consuming. Over the past several decades, machine-learning tools, such as quantitative structure-activity relationship (QSAR) modeling, were developed that can identify potential biological active molecules from millions of candidate compounds quickly and cheaply. However, when drug discovery moved into the era of 'big' data, machine learning approaches evolved into deep learning approaches, which are a more powerful and efficient way to deal with the massive amounts of data generated from modern drug discovery approaches. Here, we summarize the history of machine learning and provide insight into recently developed deep learning approaches and their applications in rational drug discovery. We suggest that this evolution of machine intelligence now provides a guide for early-stage drug design and discovery in the current big data era.",2017-11-01,51,1139,100,1284
679,28985937,Applications of Resting State Functional MR Imaging to Traumatic Brain Injury,"Traumatic brain injury (TBI) is an important public health issue. TBI includes a broad spectrum of injury severities and abnormalities. Functional MR imaging (fMR imaging), both resting state (rs) and task, has been used often in research to study the effects of TBI. Although rs-fMR imaging is not currently applicable in clinical diagnosis of TBI, computer-aided tools are making this a possibility for the future. Specifically, graph theory is being used to study the change in networks after TBI. Machine learning methods allow researchers to build models capable of predicting injury severity and recovery trajectories.",2017-11-01,2,624,77,1284
684,28974388,(Machine-)Learning to analyze in vivo microscopy: Support vector machines,"The development of new microscopy techniques for super-resolved, long-term monitoring of cellular and subcellular dynamics in living organisms is revealing new fundamental aspects of tissue development and repair. However, new microscopy approaches present several challenges. In addition to unprecedented requirements for data storage, the analysis of high resolution, time-lapse images is too complex to be done manually. Machine learning techniques are ideally suited for the (semi-)automated analysis of multidimensional image data. In particular, support vector machines (SVMs), have emerged as an efficient method to analyze microscopy images obtained from animals. Here, we discuss the use of SVMs to analyze in vivo microscopy data. We introduce the mathematical framework behind SVMs, and we describe the metrics used by SVMs and other machine learning approaches to classify image data. We discuss the influence of different SVM parameters in the context of an algorithm for cell segmentation and tracking. Finally, we describe how the application of SVMs has been critical to study protein localization in yeast screens, for lineage tracing in C. elegans, or to determine the developmental stage of Drosophila embryos to investigate gene expression dynamics. We propose that SVMs will become central tools in the analysis of the complex image data that novel microscopy modalities have made possible. This article is part of a Special Issue entitled: Biophysics in Canada, edited by Lewis Kay, John Baenziger, Albert Berghuis and Peter Tieleman.",2017-11-01,2,1556,73,1284
677,28990168,Automated analysis of seizure semiology and brain electrical activity in presurgery evaluation of epilepsy: A focused survey,"Epilepsy being one of the most prevalent neurological disorders, affecting approximately 50 million people worldwide, and with almost 30-40% of patients experiencing partial epilepsy being nonresponsive to medication, epilepsy surgery is widely accepted as an effective therapeutic option. Presurgical evaluation has advanced significantly using noninvasive techniques based on video monitoring, neuroimaging, and electrophysiological and neuropsychological tests; however, certain clinical settings call for invasive intracranial recordings such as stereoelectroencephalography (SEEG), aiming to accurately map the eloquent brain networks involved during a seizure. Most of the current presurgical evaluation procedures focus on semiautomatic techniques, where surgery diagnosis relies immensely on neurologists' experience and their time-consuming subjective interpretation of semiology or the manifestations of epilepsy and their correlation with the brain's electrical activity. Because surgery misdiagnosis reaches a rate of 30%, and more than one-third of all epilepsies are poorly understood, there is an evident keen interest in improving diagnostic precision using computer-based methodologies that in the past few years have shown near-human performance. Among them, deep learning has excelled in many biological and medical applications, but has advanced insufficiently in epilepsy evaluation and automated understanding of neural bases of semiology. In this paper, we systematically review the automatic applications in epilepsy for human motion analysis, brain electrical activity, and the anatomoelectroclinical correlation to attribute anatomical localization of the epileptogenic network to distinctive epilepsy patterns. Notably, recent advances in deep learning techniques will be investigated in the contexts of epilepsy to address the challenges exhibited by traditional machine learning techniques. Finally, we discuss and propose future research on epilepsy surgery assessment that can jointly learn across visually observed semiologic patterns and recorded brain electrical activity.",2017-11-01,1,2106,124,1284
690,28949520,"Interpretation of Quantitative Structure-Activity Relationship Models: Past, Present, and Future","This paper is an overview of the most significant and impactful interpretation approaches of quantitative structure-activity relationship (QSAR) models, their development, and application. The evolution of the interpretation paradigm from ""model  descriptors  (structure)"" to ""model  structure"" is indicated. The latter makes all models interpretable regardless of machine learning methods or descriptors used for modeling. This opens wide prospects for application of corresponding interpretation approaches to retrieve structure-property relationships captured by any models. Issues of separate approaches are discussed as well as general issues and prospects of QSAR model interpretation.",2017-11-01,14,694,96,1284
660,29048559,Plant phenomics: an overview of image acquisition technologies and image data analysis algorithms,"The study of phenomes or phenomics has been a central part of biology. The field of automatic phenotype acquisition technologies based on images has seen an important advance in the last years. As with other high-throughput technologies, it addresses a common set of problems, including data acquisition and analysis. In this review, we give an overview of the main systems developed to acquire images. We give an in-depth analysis of image processing with its major issues and the algorithms that are being used or emerging as useful to obtain data out of images in an automatic fashion.",2017-11-01,24,588,97,1284
345,28643174,Natural Language Processing for EHR-Based Pharmacovigilance: A Structured Review,"The goal of pharmacovigilance is to detect, monitor, characterize and prevent adverse drug events (ADEs) with pharmaceutical products. This article is a comprehensive structured review of recent advances in applying natural language processing (NLP) to electronic health record (EHR) narratives for pharmacovigilance. We review methods of varying complexity and problem focus, summarize the current state-of-the-art in methodology advancement, discuss limitations and point out several promising future directions. The ability to accurately capture both semantic and syntactic structures in clinical narratives becomes increasingly critical to enable efficient and accurate ADE detection. Significant progress has been made in algorithm development and resource construction since 2000. Since 2012, statistical analysis and machine learning methods have gained traction in automation of ADE mining from EHR narratives. Current state-of-the-art methods for NLP-based ADE detection from EHRs show promise regarding their integration into production pharmacovigilance systems. In addition, integrating multifaceted, heterogeneous data sources has shown promise in improving ADE detection and has become increasingly adopted. On the other hand, challenges and opportunities remain across the frontier of NLP application to EHR-based pharmacovigilance, including proper characterization of ADE context, differentiation between off- and on-label drug-use ADEs, recognition of the importance of polypharmacy-induced ADEs, better integration of heterogeneous data sources, creation of shared corpora, and organization of shared-task challenges to advance the state-of-the-art.",2017-11-01,33,1668,80,1284
325,28699534,Discovery and Development of ATP-Competitive mTOR Inhibitors Using Computational Approaches,"The mammalian target of rapamycin (mTOR) is a central controller of cell growth, proliferation, metabolism, and angiogenesis. This protein is an attractive target for new anticancer drug development. Significant progress has been made in hit discovery, lead optimization, drug candidate development and determination of the three-dimensional (3D) structure of mTOR. Computational methods have been applied to accelerate the discovery and development of mTOR inhibitors helping to model the structure of mTOR, screen compound databases, uncover structure-activity relationship (SAR) and optimize the hits, mine the privileged fragments and design focused libraries. Besides, computational approaches were also applied to study protein-ligand interactions mechanisms and in natural product-driven drug discovery. Herein, we survey the most recent progress on the application of computational approaches to advance the discovery and development of compounds targeting mTOR. Future directions in the discovery of new mTOR inhibitors using computational methods are also discussed.",2017-11-01,1,1076,91,1284
350,28614702,De-identification of psychiatric intake records: Overview of 2016 CEGS N-GRID shared tasks Track 1,"The 2016 CEGS N-GRID shared tasks for clinical records contained three tracks. Track 1 focused on de-identification of a new corpus of 1000 psychiatric intake records. This track tackled de-identification in two sub-tracks: Track 1.A was a ""sight unseen"" task, where nine teams ran existing de-identification systems, without any modifications or training, on 600 new records in order to gauge how well systems generalize to new data. The best-performing system for this track scored an F1 of 0.799. Track 1.B was a traditional Natural Language Processing (NLP) shared task on de-identification, where 15 teams had two months to train their systems on the new data, then test it on an unannotated test set. The best-performing system from this track scored an F1 of 0.914. The scores for Track 1.A show that unmodified existing systems do not generalize well to new data without the benefit of training data. The scores for Track 1.B are slightly lower than the 2014 de-identification shared task (which was almost identical to 2016 Track 1.B), indicating that these new psychiatric records pose a more difficult challenge to NLP systems. Overall, de-identification is still not a solved problem, though it is important to the future of clinical NLP.",2017-11-01,26,1250,98,1284
638,29147562,Cognitive computing and eScience in health and life science research: artificial intelligence and obesity intervention programs,"Objective:                    To present research models based on artificial intelligence and discuss the concept of cognitive computing and eScience as disruptive factors in health and life science research methodologies.              Methods:                    The paper identifies big data as a catalyst to innovation and the development of artificial intelligence, presents a framework for computer-supported human problem solving and describes a transformation of research support models. This framework includes traditional computer support; federated cognition using machine learning and cognitive agents to augment human intelligence; and a semi-autonomous/autonomous cognitive model, based on deep machine learning, which supports eScience.              Results:                    The paper provides a forward view of the impact of artificial intelligence on our human-computer support and research methods in health and life science research.              Conclusions:                    By augmenting or amplifying human task performance with artificial intelligence, cognitive computing and eScience research models are discussed as novel and innovative systems for developing more effective adaptive obesity intervention programs.",2017-11-01,3,1245,127,1284
637,29149080,Novel Tactile Sensor Technology and Smart Tactile Sensing Systems: A Review,"During the last decades, smart tactile sensing systems based on different sensing techniques have been developed due to their high potential in industry and biomedical engineering. However, smart tactile sensing technologies and systems are still in their infancy, as many technological and system issues remain unresolved and require strong interdisciplinary efforts to address them. This paper provides an overview of smart tactile sensing systems, with a focus on signal processing technologies used to interpret the measured information from tactile sensors and/or sensors for other sensory modalities. The tactile sensing transduction and principles, fabrication and structures are also discussed with their merits and demerits. Finally, the challenges that tactile sensing technology needs to overcome are highlighted.",2017-11-01,13,824,75,1284
629,29179711,A systematic review of data mining and machine learning for air pollution epidemiology,"Background:                    Data measuring airborne pollutants, public health and environmental factors are increasingly being stored and merged. These big datasets offer great potential, but also challenge traditional epidemiological methods. This has motivated the exploration of alternative methods to make predictions, find patterns and extract information. To this end, data mining and machine learning algorithms are increasingly being applied to air pollution epidemiology.              Methods:                    We conducted a systematic literature review on the application of data mining and machine learning methods in air pollution epidemiology. We carried out our search process in PubMed, the MEDLINE database and Google Scholar. Research articles applying data mining and machine learning methods to air pollution epidemiology were queried and reviewed.              Results:                    Our search queries resulted in 400 research articles. Our fine-grained analysis employed our inclusion/exclusion criteria to reduce the results to 47 articles, which we separate into three primary areas of interest: 1) source apportionment; 2) forecasting/prediction of air pollution/quality or exposure; and 3) generating hypotheses. Early applications had a preference for artificial neural networks. In more recent work, decision trees, support vector machines, k-means clustering and the APRIORI algorithm have been widely applied. Our survey shows that the majority of the research has been conducted in Europe, China and the USA, and that data mining is becoming an increasingly common tool in environmental health. For potential new directions, we have identified that deep learning and geo-spacial pattern mining are two burgeoning areas of data mining that have good potential for future applications in air pollution epidemiology.              Conclusions:                    We carried out a systematic review identifying the current trends, challenges and new directions to explore in the application of data mining methods to air pollution epidemiology. This work shows that data mining is increasingly being applied in air pollution epidemiology. The potential to support air pollution epidemiology continues to grow with advancements in data mining related to temporal and geo-spacial mining, and deep learning. This is further supported by new sensors and storage mediums that enable larger, better quality data. This suggests that many more fruitful applications can be expected in the future.",2017-11-01,16,2525,86,1284
373,28455151,Symptom severity prediction from neuropsychiatric clinical records: Overview of 2016 CEGS N-GRID shared tasks Track 2,"The second track of the CEGS N-GRID 2016 natural language processing shared tasks focused on predicting symptom severity from neuropsychiatric clinical records. For the first time, initial psychiatric evaluation records have been collected, de-identified, annotated and shared with the scientific community. One-hundred-ten researchers organized in twenty-four teams participated in this track and submitted sixty-five system runs for evaluation. The top ten teams each achieved an inverse normalized macro-averaged mean absolute error score over 0.80. The top performing system employed an ensemble of six different machine learning-based classifiers to achieve a score 0.86. The task resulted to be generally easy with the exception of two specific classes of records: records with very few but crucial positive valence signals, and records describing patients predominantly affected by negative rather than positive valence. Those cases proved to be very challenging for most of the systems. Further research is required to consider the task solved. Overall, the results of this track demonstrate the effectiveness of data-driven approaches to the task of symptom severity classification.",2017-11-01,15,1191,117,1284
691,28932174,Internet-based computer technology on radiotherapy,"Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.",2017-12-01,0,1241,50,1254
695,28905541,The Role of New Imaging Methods in Managing Age-Related Macular Degeneration,"The use of imaging for age-related macular degeneration (AMD) depends on how it benefits clinical management and on reimbursement. The latter should relate to the former. This review assesses how different forms of AMD can be imaged and what information this provides. For nonneovascular AMD high-resolution optical coherence tomography (OCT), autofluorescence, and near infrared imaging can identify the type of drusen, such as reticular pseudodrusen, which influences prognosis, and the amount of atrophy, for which phase 3 trials are underway. Clarifying the correct diagnosis for late-onset Stargardt and macular telangiectasia, if treatment becomes available, will be especially important. Choroidal thickness can be measured and changes with antivascular endothelial growth factor treatment, but how this influences management is less clear. The finding of a thick choroid may alter the diagnosis to pachychoroid neovasculopathy, which may have a different treatment response. Peripheral retinal changes are commonly found on ultrawide-field imaging but their importance is not yet determined. The mainstay of imaging is OCT, which can detect neovascular AMD by detecting thickening and be used for follow-up, as the presence or absence of thickening is the main determinant of treatment. Higher resolution systems and now OCT angiography are able to distinguish neovascular type, especially type 2 choroidal neovascularization but also polypoidal choroidal vasculopathy and retinal angiomatous proliferation. Fundus fluorescein and indocyanine green angiographies still have a role, although that partly depends on whether photodynamic therapy is being considered. Automated image analysis and machine learning will be increasingly important in supporting clinician decisions.",2017-12-01,3,1784,76,1254
670,29029029,A manifesto for cardiovascular imaging: addressing the human factor,"Our use of modern cardiovascular imaging tools has not kept pace with their technological development. Diagnostic errors are common but seldom investigated systematically. Rather than more impressive pictures, our main goal should be more precise tests of function which we select because their appropriate use has therapeutic implications which in turn have a beneficial impact on morbidity or mortality. We should practise analytical thinking, use checklists to avoid diagnostic pitfalls, and apply strategies that will reduce biases and avoid overdiagnosis. We should develop normative databases, so that we can apply diagnostic algorithms that take account of variations with age and risk factors and that allow us to calculate pre-test probability and report the post-test probability of disease. We should report the imprecision of a test, or its confidence limits, so that reference change values can be considered in daily clinical practice. We should develop decision support tools to improve the quality and interpretation of diagnostic imaging, so that we choose the single best test irrespective of modality. New imaging tools should be evaluated rigorously, so that their diagnostic performance is established before they are widely disseminated; this should be a shared responsibility of manufacturers with clinicians, leading to cost-effective implementation. Trials should evaluate diagnostic strategies against independent reference criteria. We should exploit advances in machine learning to analyse digital data sets and identify those features that best predict prognosis or responses to treatment. Addressing these human factors will reap benefit for patients, while technological advances continue unpredictably.",2017-12-01,2,1734,67,1254
694,28914640,Machine learning: novel bioinformatics approaches for combating antimicrobial resistance,"Purpose of review:                    Antimicrobial resistance (AMR) is a threat to global health and new approaches to combating AMR are needed. Use of machine learning in addressing AMR is in its infancy but has made promising steps. We reviewed the current literature on the use of machine learning for studying bacterial AMR.              Recent findings:                    The advent of large-scale data sets provided by next-generation sequencing and electronic health records make applying machine learning to the study and treatment of AMR possible. To date, it has been used for antimicrobial susceptibility genotype/phenotype prediction, development of AMR clinical decision rules, novel antimicrobial agent discovery and antimicrobial therapy optimization.              Summary:                    Application of machine learning to studying AMR is feasible but remains limited. Implementation of machine learning in clinical settings faces barriers to uptake with concerns regarding model interpretability and data quality.Future applications of machine learning to AMR are likely to be laboratory-based, such as antimicrobial susceptibility phenotype prediction.",2017-12-01,9,1176,88,1254
367,28497663,Mortality risk prediction models for coronary artery bypass graft surgery: current scenario and future direction,"Introduction:                    Many risk prediction models are currently in use for predicting short-term mortality following coronary artery bypass graft (CABG) surgery. This review critically appraised the methods that were used for developing these models to assess their applicability in current practice setting as well as for the necessity of up-gradation.              Evidence acquisition:                    Medline via Ovid was searched for articles published between 1946 and 2016 and EMBASE via Ovid between 1974 and 2016 to identify risk prediction models for CABG. Article selection and data extraction was conducted using the CHARMS checklist for review of prediction model studies. Association between model development methods and model's discrimination was assessed using Kruskal-Wallis one-way analysis of variance and Mann-Whitney U-test.              Evidence synthesis:                    A total of 53 risk prediction models for short-term mortality following CABG were identified. The review found a wide variation in development methodology of risk prediction models in the field. Ambiguous predictor and outcome definition, sub-optimum sample size, inappropriate handling of missing data and inefficient predictor selection technique are major issues identified in the review. Quantitative synthesis in the review showed ""missing value imputation"" and ""adopting machine learning algorithms"" may result in better discrimination power of the models.              Conclusions:                    There are aspects in current risk modeling, where there is room for improvement to reflect current clinical practice. Future risk modelling needs to adopt a standardized approach to defining both outcome and predictor variables, rational treatment of missing data and robust statistical techniques to enhance performance of the mortality risk prediction.",2017-12-01,2,1875,112,1254
618,29220074,Prediction of Protein-Protein Interactions,"The authors provide an overview of physical protein-protein interaction prediction, covering the main strategies for predicting interactions, approaches for assessing predictions, and online resources for accessing predictions. This unit focuses on the main advancements in each of these areas over the last decade. The methods and resources that are presented here are not an exhaustive set, but characterize the current state of the field-highlighting key challenges and achievements.  2017 by John Wiley & Sons, Inc.",2017-12-01,5,520,42,1254
2359,29430456,Text-mining analysis of mHealth research,"In recent years, because of the advancements in communication and networking technologies, mobile technologies have been developing at an unprecedented rate. mHealth, the use of mobile technologies in medicine, and the related research has also surged parallel to these technological advancements. Although there have been several attempts to review mHealth research through manual processes such as systematic reviews, the sheer magnitude of the number of studies published in recent years makes this task very challenging. The most recent developments in machine learning and text mining offer some potential solutions to address this challenge by allowing analyses of large volumes of texts through semi-automated processes. The objective of this study is to analyze the evolution of mHealth research by utilizing text-mining and natural language processing (NLP) analyses. The study sample included abstracts of 5,644 mHealth research articles, which were gathered from five academic search engines by using search terms such as mobile health, and mHealth. The analysis used the Text Explorer module of JMP Pro 13 and an iterative semi-automated process involving tokenizing, phrasing, and terming. After developing the document term matrix (DTM) analyses such as single value decomposition (SVD), topic, and hierarchical document clustering were performed, along with the topic-informed document clustering approach. The results were presented in the form of word-clouds and trend analyses. There were several major findings regarding research clusters and trends. First, our results confirmed time-dependent nature of terminology use in mHealth research. For example, in earlier versus recent years the use of terminology changed from ""mobile phone"" to ""smartphone"" and from ""applications"" to ""apps"". Second, ten clusters for mHealth research were identified including (I) Clinical Research on Lifestyle Management, (II) Community Health, (III) Literature Review, (IV) Medical Interventions, (V) Research Design, (VI) Infrastructure, (VII) Applications, (VIII) Research and Innovation in Health Technologies, (IX) Sensor-based Devices and Measurement Algorithms, (X) Survey-based Research. Third, the trend analyses indicated the infrastructure cluster as the highest percentage researched area until 2014. The Research and Innovation in Health Technologies cluster experienced the largest increase in numbers of publications in recent years, especially after 2014. This study is unique because it is the only known study utilizing text-mining analyses to reveal the streams and trends for mHealth research. The fast growth in mobile technologies is expected to lead to higher numbers of studies focusing on mHealth and its implications for various healthcare outcomes. Findings of this study can be utilized by researchers in identifying areas for future studies.",2017-12-01,1,2870,40,1254
616,29234465,Ten quick tips for machine learning in computational biology,"Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences.",2017-12-01,82,748,60,1254
604,29312134,Multivariate Analysis and Machine Learning in Cerebral Palsy Research,"Cerebral palsy (CP), a common pediatric movement disorder, causes the most severe physical disability in children. Early diagnosis in high-risk infants is critical for early intervention and possible early recovery. In recent years, multivariate analytic and machine learning (ML) approaches have been increasingly used in CP research. This paper aims to identify such multivariate studies and provide an overview of this relatively young field. Studies reviewed in this paper have demonstrated that multivariate analytic methods are useful in identification of risk factors, detection of CP, movement assessment for CP prediction, and outcome assessment, and ML approaches have made it possible to automatically identify movement impairments in high-risk infants. In addition, outcome predictors for surgical treatments have been identified by multivariate outcome studies. To make the multivariate and ML approaches useful in clinical settings, further research with large samples is needed to verify and improve these multivariate methods in risk factor identification, CP detection, movement assessment, and outcome evaluation or prediction. As multivariate analysis, ML and data processing technologies advance in the era of Big Data of this century, it is expected that multivariate analysis and ML will play a bigger role in improving the diagnosis and treatment of CP to reduce mortality and morbidity rates, and enhance patient care for children with CP.",2017-12-01,1,1463,69,1254
297,28893404,3D Quantitative Chemical Imaging of Tissues by Spectromics,"Mid-infrared (IR), Raman, and X-ray fluorescence (XRF) spectroscopy methods, as well as mass spectrometry (MS), can be used for 3D chemical imaging. These techniques offer an invaluable opportunity to access chemical features of biological samples in a nonsupervised way. The global chemical information they provide enables the exploitation of a large array of chemical species or parameters, so-called 'spectromics'. Extracting chemical data from spectra is critical for the high-quality chemical analysis of biosamples. Furthermore, these are the only currently available techniques that can quantitatively analyze tissue content (e.g., molecular concentrations) and substructures (e.g., cells or blood vessels). The development of chemical-derived biological metadata appears to be a new way to exploit spectral information with machine learning algorithms.",2017-12-01,2,861,58,1254
301,28867500,Review of combinations of experimental and computational techniques to identify and understand genes involved in innate immunity and effector-triggered defence,"The innate immune system includes a first layer of defence that recognises conserved pathogen-associated molecular patterns that are essential for microbial fitness. Resistance (R) gene-based recognition of pathogen effectors, which function in modulation or avoidance of host immunity, activates a second layer of plant defence. In this review, experimental and computational techniques are considered to improve understanding of the plant immune system. Biocomputation contributes to discovery of the molecular genetic basis of host resistance against pathogens. Sequenced genomes have been used to identify R genes in plants. Resistance gene enrichment sequencing based on conserved protein domains has increased the number of R genes with nucleotide-binding site and leucine-rich repeat domains. Network analysis will contribute to an improved understanding of the innate immune system and identify novel genes for partial disease resistance. Machine learning algorithms are expected to become important in defining aspects of the immune system that are less well characterised, including identification of R genes that lack conserved protein domains.",2017-12-01,1,1155,159,1254
319,28729156,Multivariate pattern analysis utilizing structural or functional MRI-In individuals with musculoskeletal pain and healthy controls: A systematic review,"Objective:                    The purpose of this systematic review is to systematically review the evidence relating to findings generated by multivariate pattern analysis (MVPA) following structural or functional magnetic resonance imaging (fMRI) to determine if this analysis is able to: a) Discriminate between individuals with musculoskeletal pain and healthy controls, b) Predict pain perception in healthy individuals stimulated with a noxious stimulus compared to those stimulated with a non-noxious stimulus.              Methods:                    MEDLINE, CINAHL, Embase, PEDro, Google Scholar, Cochrane library and Web of Science were systematically screened for relevant literature using different combinations of keywords regarding structural and functional MRI analysed with MVPA, both in individuals with musculoskeletal pain and healthy controls. Reference lists of included articles were hand-searched for additional literature. Eligible articles were assessed on risk of bias and reviewed by two independent researchers.              Results:                    The search query returned 18 articles meeting the inclusion criteria. Methodological quality varied from poor to good. Seven studies investigated the ability of machine-learning algorithms to differentiate patient groups from healthy control participants. Overall, the review demonstrated that MVPA can discriminate between individuals with MSK pain and healthy controls with an overall accuracy ranging from 53% to 94%. Twelve studies utilized healthy control participants (using them as their own controls), during experimental pain paradigms aimed to investigate the ability of machine-learning to differentiate individuals stimulated with noxious stimuli from those stimulated with non-noxious stimuli, with 'pain' detection rates ranging from 60% to 94%. However, significant heterogeneity in patient conditions, study methodology and brain imaging techniques resulted in various findings that make study comparisons and formal conclusions challenging.              Conclusion:                    There is preliminary and emerging evidence that MVPA analyses of structural or functional MRI are able to discriminate between patients and healthy controls, and also discriminate between noxious and non-noxious stimulation. No prospective studies were found in this review to allow determination of the prognostic or diagnostic capabilities or treatment responsiveness of these analyses. Future studies would also benefit from combining various behavioural, genotype and phenotype data into analyses to assist with development of sensitive and specific signatures that could guide future individualized patient treatment options and evaluate how treatments exert their effects.",2017-12-01,3,2762,151,1254
614,29249829,A polygenic score for schizophrenia predicts glycemic control,"Schizophrenia is substantially comorbid with type 2 diabetes (T2D), but the molecular basis of this effect is incompletely understood. Here, we show that a cortical schizophrenia expression score predicts glycemic control from pancreatic islet cell expression. We used machine learning to identify a cortical expression signature in 212 schizophrenia patients and controls, which explained ~25% of the illness-associated variance. The algorithm was predicted in expression data from 51 subjects (9 with T2D), explained up to 26.3% of the variance in the glycemic control indicator HbA1c and could significantly differentiate T2D patients from controls. The cross-tissue prediction was driven by processes previously linked to diabetes. Genes contributing to this prediction were involved in the electron transport chain as well as kidney development and support oxidative stress as a molecular process underlying the comorbidity between both conditions. Together, the present results suggest a molecular commonality between schizophrenia and glycemic markers of type 2 diabetes.",2017-12-01,3,1078,61,1254
628,32923746,Designing and interpreting 'multi-omic' experiments that may change our understanding of biology,"Most biological mechanisms involve more than one type of biomolecule, and hence operate not solely at the level of either genome, transcriptome, proteome, metabolome or ionome. Datasets resulting from single-omic analysis are rapidly increasing in throughput and quality, rendering multi-omic studies feasible. These should offer a comprehensive, structured and interactive overview of a biological mechanism. However, combining single-omic datasets in a meaningful manner has so far proved challenging, and the discovery of new biological information lags behind expectation. One reason is that experiments conducted in different laboratories can typically not to be combined without restriction. Second, the interpretation of multi-omic datasets represents a significant challenge by nature, as the biological datasets are heterogeneous not only for technical, but also for biological, chemical, and physical reasons. Here, multi-layer network theory and methods of artificial intelligence might contribute to solve these problems. For the efficient application of machine learning however, biological datasets need to become more systematic, more precise - and much larger. We conclude our review with basic guidelines for the successful set-up of a multi-omic experiment.",2017-12-01,9,1275,96,1254
635,29153163,A 100-Year Review: Methods and impact of genetic selection in dairy cattle-From daughter-dam comparisons to deep learning algorithms,"In the early 1900s, breed society herdbooks had been established and milk-recording programs were in their infancy. Farmers wanted to improve the productivity of their cattle, but the foundations of population genetics, quantitative genetics, and animal breeding had not been laid. Early animal breeders struggled to identify genetically superior families using performance records that were influenced by local environmental conditions and herd-specific management practices. Daughter-dam comparisons were used for more than 30 yr and, although genetic progress was minimal, the attention given to performance recording, genetic theory, and statistical methods paid off in future years. Contemporary (herdmate) comparison methods allowed more accurate accounting for environmental factors and genetic progress began to accelerate when these methods were coupled with artificial insemination and progeny testing. Advances in computing facilitated the implementation of mixed linear models that used pedigree and performance data optimally and enabled accurate selection decisions. Sequencing of the bovine genome led to a revolution in dairy cattle breeding, and the pace of scientific discovery and genetic progress accelerated rapidly. Pedigree-based models have given way to whole-genome prediction, and Bayesian regression models and machine learning algorithms have joined mixed linear models in the toolbox of modern animal breeders. Future developments will likely include elucidation of the mechanisms of genetic inheritance and epigenetic modification in key biological pathways, and genomic data will be used with data from on-farm sensors to facilitate precision management on modern dairy farms.",2017-12-01,9,1707,132,1254
639,29147555,"What can machine learning do for antimicrobial peptides, and what can antimicrobial peptides do for machine learning?","Antimicrobial peptides (AMPs) are a diverse class of well-studied membrane-permeating peptides with important functions in innate host defense. In this short review, we provide a historical overview of AMPs, summarize previous applications of machine learning to AMPs, and discuss the results of our studies in the context of the latest AMP literature. Much work has been recently done in leveraging computational tools to design new AMP candidates with high therapeutic efficacies for drug-resistant infections. We show that machine learning on AMPs can be used to identify essential physico-chemical determinants of AMP functionality, and identify and design peptide sequences to generate membrane curvature. In a broader scope, we discuss the implications of our findings for the discovery of membrane-active peptides in general, and uncovering membrane activity in new and existing peptide taxonomies.",2017-12-01,21,905,117,1254
2380,29375355,Computational Foundations of Natural Intelligence,"New developments in AI and neuroscience are revitalizing the quest to understanding natural intelligence, offering insight about how to equip machines with human-like capabilities. This paper reviews some of the computational principles relevant for understanding natural intelligence and, ultimately, achieving strong AI. After reviewing basic principles, a variety of computational modeling approaches is discussed. Subsequently, I concentrate on the use of artificial neural networks as a framework for modeling cognitive processes. This paper ends by outlining some of the challenges that remain to fulfill the promise of machines that show human-like intelligence.",2017-12-01,5,669,49,1254
643,29127902,A survey of machine learning applications in HIV clinical research and care,"A wealth of genetic, demographic, clinical and biomarker data is collected from routine clinical care of HIV patients and exists in the form of medical records available among the medical care and research communities. Machine learning (ML) methods have the ability to identify and discover patterns in complex datasets and predict future outcomes of HIV treatment. We survey published studies that make use of ML techniques in HIV clinical research and care. An advanced search relevant to the use of ML in HIV research was conducted in the PubMed biomedical database. The survey outcomes of interest include data sources, ML techniques, ML tasks and ML application paradigms. A growing trend in application of ML in HIV research was observed. The application paradigm has diversified to include practical clinical application, but statistical analysis remains the most dominant application. There is an increase in the use of genomic sources of data and high performance non-parametric ML methods with a focus on combating resistance to antiretroviral therapy (ART). There is need for improvement in collection of health records data and increased training in ML so as to translate ML research into clinical application in HIV management.",2017-12-01,5,1240,75,1254
647,29089139,Towards In Silico Prediction of the Immune-Checkpoint Blockade Response,"Cancer immunotherapy with immune-checkpoint blockade (ICB) is considered a promising strategy for cancer treatment. Identifying predictive biomarkers and developing efficient computational models to predict the ICB response are important issues for successful immunotherapy. Here, we present a concise and intuitive survey of the computational issues for ICB response prediction, providing a summary of the available predictive biomarkers and building of one-stop machine-learning models that integrate biomarkers calculable from high-throughput sequencing (HTS) data. Several points for discussion are highlighted to inspire further research for improving ICB treatment. Continuing efforts are required to improve ICB response prediction and to identify novel predictive biomarkers by taking advantage of the rapid development of computational models and HTS techniques for effective and personalized cancer immunotherapy.",2017-12-01,3,923,71,1254
649,29074032,Predicting Age Using Neuroimaging: Innovative Brain Ageing Biomarkers,"The brain changes as we age and these changes are associated with functional deterioration and neurodegenerative disease. It is vital that we better understand individual differences in the brain ageing process; hence, techniques for making individualised predictions of brain ageing have been developed. We present evidence supporting the use of neuroimaging-based 'brain age' as a biomarker of an individual's brain health. Increasingly, research is showing how brain disease or poor physical health negatively impacts brain age. Importantly, recent evidence shows that having an 'older'-appearing brain relates to advanced physiological and cognitive ageing and the risk of mortality. We discuss controversies surrounding brain age and highlight emerging trends such as the use of multimodality neuroimaging and the employment of 'deep learning' methods.",2017-12-01,89,857,69,1254
634,29155639,When Machines Think: Radiology's Next Frontier,"Artificial intelligence (AI), machine learning, and deep learning are terms now seen frequently, all of which refer to computer algorithms that change as they are exposed to more data. Many of these algorithms are surprisingly good at recognizing objects in images. The combination of large amounts of machine-consumable digital data, increased and cheaper computing power, and increasingly sophisticated statistical models combine to enable machines to find patterns in data in ways that are not only cost-effective but also potentially beyond humans' abilities. Building an AI algorithm can be surprisingly easy. Understanding the associated data structures and statistics, on the other hand, is often difficult and obscure. Converting the algorithm into a sophisticated product that works consistently in broad, general clinical use is complex and incompletely understood. To show how these AI products reduce costs and improve outcomes will require clinical translation and industrial-grade integration into routine workflow. Radiology has the chance to leverage AI to become a center of intelligently aggregated, quantitative, diagnostic information. Centaur radiologists, formed as a synergy of human plus computer, will provide interpretations using data extracted from images by humans and image-analysis computer algorithms, as well as the electronic health record, genomics, and other disparate sources. These interpretations will form the foundation of precision health care, or care customized to an individual patient.  RSNA, 2017.",2017-12-01,41,1545,46,1254
603,29312932,Fetal Cardiac Doppler Signal Processing Techniques: Challenges and Future Research Directions,"The fetal Doppler Ultrasound (DUS) is commonly used for monitoring fetal heart rate and can also be used for identifying the event timings of fetal cardiac valve motions. In early-stage fetuses, the detected Doppler signal suffers from noise and signal loss due to the fetal movements and changing fetal location during the measurement procedure. The fetal cardiac intervals, which can be estimated by measuring the fetal cardiac event timings, are the most important markers of fetal development and well-being. To advance DUS-based fetal monitoring methods, several powerful and well-advanced signal processing and machine learning methods have recently been developed. This review provides an overview of the existing techniques used in fetal cardiac activity monitoring and a comprehensive survey on fetal cardiac Doppler signal processing frameworks. The review is structured with a focus on their shortcomings and advantages, which helps in understanding fetal Doppler cardiogram signal processing methods and the related Doppler signal analysis procedures by providing valuable clinical information. Finally, a set of recommendations are suggested for future research directions and the use of fetal cardiac Doppler signal analysis, processing, and modeling to address the underlying challenges.",2017-12-01,6,1302,93,1254
2369,29405647,Deep Generative Models for Molecular Science,"Generative deep machine learning models now rival traditional quantum-mechanical computations in predicting properties of new structures, and they come with a significantly lower computational cost, opening new avenues in computational molecular science. In the last few years, a variety of deep generative models have been proposed for modeling molecules, which differ in both their model structure and choice of input features. We review these recent advances within deep generative models for predicting molecular properties, with particular focus on models based on the probabilistic autoencoder (or variational autoencoder, VAE) approach in which the molecular structure is embedded in a latent vector space from which its properties can be predicted and its structure can be restored.",2018-01-01,4,790,44,1223
342,28648568,Application of machine learning classification for structural brain MRI in mood disorders: Critical review from a clinical perspective,"Mood disorders are a highly prevalent group of mental disorders causing substantial socioeconomic burden. There are various methodological approaches for identifying the underlying mechanisms of the etiology, symptomatology, and therapeutics of mood disorders; however, neuroimaging studies have provided the most direct evidence for mood disorder neural substrates by visualizing the brains of living individuals. The prefrontal cortex, hippocampus, amygdala, thalamus, ventral striatum, and corpus callosum are associated with depression and bipolar disorder. Identifying the distinct and common contributions of these anatomical regions to depression and bipolar disorder have broadened and deepened our understanding of mood disorders. However, the extent to which neuroimaging research findings contribute to clinical practice in the real-world setting is unclear. As traditional or non-machine learning MRI studies have analyzed group-level differences, it is not possible to directly translate findings from research to clinical practice; the knowledge gained pertains to the disorder, but not to individuals. On the other hand, a machine learning approach makes it possible to provide individual-level classifications. For the past two decades, many studies have reported on the classification accuracy of machine learning-based neuroimaging studies from the perspective of diagnosis and treatment response. However, for the application of a machine learning-based brain MRI approach in real world clinical settings, several major issues should be considered. Secondary changes due to illness duration and medication, clinical subtypes and heterogeneity, comorbidities, and cost-effectiveness restrict the generalization of the current machine learning findings. Sophisticated classification of clinical and diagnostic subtypes is needed. Additionally, as the approach is inevitably limited by sample size, multi-site participation and data-sharing are needed in the future.",2018-01-01,7,1982,134,1223
2357,29434508,Quantum machine learning: a classical perspective,"Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning (ML) techniques to impressive results in regression, classification, data generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets is motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed up classical ML algorithms. Here we review the literature in quantum ML and discuss perspectives for a mixed readership of classical ML and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in ML are identified as promising directions for the field. Practical questions, such as how to upload classical data into quantum form, will also be addressed.",2018-01-01,7,1127,49,1223
692,28930544,Neural signatures of attention: insights from decoding population activity patterns,"Understanding brain function and the computations that individual neurons and neuronal ensembles carry out during cognitive functions is one of the biggest challenges in neuroscientific research. To this end, invasive electrophysiological studies have provided important insights by recording the activity of single neurons in behaving animals. To average out noise, responses are typically averaged across repetitions and across neurons that are usually recorded on different days. However, the brain makes decisions on short time scales based on limited exposure to sensory stimulation by interpreting responses of populations of neurons on a moment to moment basis. Recent studies have employed machine-learning algorithms in attention and other cognitive tasks to decode the information content of distributed activity patterns across neuronal ensembles on a single trial basis. Here, we review results from studies that have used pattern-classification decoding approaches to explore the population representation of cognitive functions. These studies have offered significant insights into population coding mechanisms. Moreover, we discuss how such advances can aid the development of cognitive brain-computer interfaces.",2018-01-01,2,1228,83,1223
2377,29378578,"Artificial intelligence on the identification of risk groups for osteoporosis, a general review","Introduction:                    The goal of this paper is to present a critical review on the main systems that use artificial intelligence to identify groups at risk for osteoporosis or fractures. The systems considered for this study were those that fulfilled the following requirements: range of coverage in diagnosis, low cost and capability to identify more significant somatic factors.              Methods:                    A bibliographic research was done in the databases, PubMed, IEEExplorer Latin American and Caribbean Center on Health Sciences Information (LILACS), Medical Literature Analysis and Retrieval System Online (MEDLINE), Cumulative Index to Nursing and Allied Health Literature (CINAHL), Scopus, Web of Science, and Science Direct searching the terms ""Neural Network"", ""Osteoporosis Machine Learning"" and ""Osteoporosis Neural Network"". Studies with titles not directly related to the research topic and older data that reported repeated strategies were excluded. The search was carried out with the descriptors in German, Spanish, French, Italian, Mandarin, Portuguese and English; but only studies written in English were found to meet the established criteria. Articles covering the period 2000-2017 were selected; however, articles prior to this period with great relevance were included in this study.              Discussion:                    Based on the collected research, it was identified that there are several methods in the use of artificial intelligence to help the screening of risk groups of osteoporosis or fractures. However, such systems were limited to a specific ethnic group, gender or age. For future research, new challenges are presented.              Conclusions:                    It is necessary to develop research with the unification of different databases and grouping of the various attributes and clinical factors, in order to reach a greater comprehensiveness in the identification of risk groups of osteoporosis. For this purpose, the use of any predictive tool should be performed in different populations with greater participation of male patients and inclusion of a larger age range for the ones involved. The biggest challenge is to deal with all the data complexity generated by this unification, developing evidence-based standards for the evaluation of the most significant risk factors.",2018-01-01,5,2363,95,1223
2360,29430271,Point-of-care testing in the early diagnosis of acute pesticide intoxication: The example of paraquat,"Acute pesticide intoxication is a common method of suicide globally. This article reviews current diagnostic methods and makes suggestions for future development. In the case of paraquat intoxication, it is characterized by multi-organ failure, causing substantial mortality and morbidity. Early diagnosis may save the life of a paraquat intoxication patient. Conventional paraquat intoxication diagnostic methods, such as symptom review and urine sodium dithionite assay, are time-consuming and impractical in resource-scarce areas where most intoxication cases occur. Several experimental and clinical studies have shown the potential of portable Surface Enhanced Raman Scattering (SERS), paper-based devices, and machine learning for paraquat intoxication diagnosis. Portable SERS and new SERS substrates maintain the sensitivity of SERS while being less costly and more convenient than conventional SERS. Paper-based devices provide the advantages of price and portability. Machine learning algorithms can be implemented as a mobile phone application and facilitate diagnosis in resource-limited areas. Although these methods have not yet met all features of an ideal diagnostic method, the combination and development of these methods offer much promise.",2018-01-01,1,1259,101,1223
606,29290259,Radiomics and radiogenomics in lung cancer: A review for the clinician,"Lung cancer is responsible for a large proportion of cancer-related deaths across the globe, with delayed detection being perhaps the most significant factor for its high mortality rate. Though the National Lung Screening Trial argues for screening of certain at-risk populations, the practical implementation of these screening efforts has not yet been successful and remains in high demand. Radiomics refers to the computerized extraction of data from radiologic images, and provides unique potential for making lung cancer screening more rapid and accurate using machine learning algorithms. The quantitative features analyzed express subvisual characteristics of images which correlate with pathogenesis of diseases. These features are broadly classified into four categories: intensity, structure, texture/gradient, and wavelet, based on the types of image attributes they capture. Many studies have been done to show correlation between these features and the malignant potential of a nodule on a chest CT. In cancer patients, these nodules also have features that can be correlated with prognosis and mutation status. The major limitations of radiomics are the lack of standardization of acquisition parameters, inconsistent radiomic methods, and lack of reproducibility. Researchers are working on overcoming these limitations, which would make radiomics more acceptable in the medical community.",2018-01-01,86,1404,70,1223
675,28992412,Protein Science by DNA Sequencing: How Advances in Molecular Biology Are Accelerating Biochemistry,"A fundamental goal of protein biochemistry is to determine the sequence-function relationship, but the vastness of sequence space makes comprehensive evaluation of this landscape difficult. However, advances in DNA synthesis and sequencing now allow researchers to assess the functional impact of every single mutation in many proteins, but challenges remain in library construction and the development of general assays applicable to a diverse range of protein functions. This Perspective briefly outlines the technical innovations in DNA manipulation that allow massively parallel protein biochemistry and then summarizes the methods currently available for library construction and the functional assays of protein variants. Areas in need of future innovation are highlighted with a particular focus on assay development and the use of computational analysis with machine learning to effectively traverse the sequence-function landscape. Finally, applications in the fundamentals of protein biochemistry, disease prediction, and protein engineering are presented.",2018-01-01,3,1066,98,1223
682,28981352,Imaging Genetic Heterogeneity in Glioblastoma and Other Glial Tumors: Review of Current Methods and Future Directions,"Objective:                    The purpose of this review is to summarize advances in the molecular analysis of gliomas, the role genetics plays in MRI features, and how machine-learning approaches can be used to survey the tumoral environment.              Conclusion:                    The genetic profile of gliomas influences the course of treatment and clinical outcomes. Though biopsy is the reference standard for determining tumor genetics, it can suffer diagnostic delays due to surgical planning and pathologic assessment. Radiogenomics may allow rapid, low-risk characterization of genetic heterogeneity.",2018-01-01,13,615,117,1223
612,29261360,Precision Medicine: Functional Advancements,"Precision medicine was conceptualized on the strength of genomic sequence analysis. High-throughput functional metrics have enhanced sequence interpretation and clinical precision. These technologies include metabolomics, magnetic resonance imaging, and I rhythm (cardiac monitoring), among others. These technologies are discussed and placed in clinical context for the medical specialties of internal medicine, pediatrics, obstetrics, and gynecology. Publications in these fields support the concept of a higher level of precision in identifying disease risk. Precise disease risk identification has the potential to enable intervention with greater specificity, resulting in disease prevention-an important goal of precision medicine.",2018-01-01,4,737,43,1223
2382,29367240,Quantitative approaches to energy and glucose homeostasis: machine learning and modelling for precision understanding and prediction,"Obesity is a major global public health problem. Understanding how energy homeostasis is regulated, and can become dysregulated, is crucial for developing new treatments for obesity. Detailed recording of individual behaviour and new imaging modalities offer the prospect of medically relevant models of energy homeostasis that are both understandable and individually predictive. The profusion of data from these sources has led to an interest in applying machine learning techniques to gain insight from these large, relatively unstructured datasets. We review both physiological models and machine learning results across a diverse range of applications in energy homeostasis, and highlight how modelling and machine learning can work together to improve predictive ability. We collect quantitative details in a comprehensive mathematical supplement. We also discuss the prospects of forecasting homeostatic behaviour and stress the importance of characterizing stochasticity within and between individuals in order to provide practical, tailored forecasts and guidance to combat the spread of obesity.",2018-01-01,4,1105,132,1223
623,29192299,Machine learning to detect signatures of disease in liquid biopsies - a user's guide,"New technologies that measure sparse molecular biomarkers from easily accessible bodily fluids (e.g. blood, urine, and saliva) are revolutionizing disease diagnostics and precision medicine. Microchip devices can measure more disease biomarkers with better sensitivity and specificity each year, but clinical interpretation of these biomarkers remains a challenge. Single biomarkers in 'liquid biopsy' often cannot accurately predict the state of a disease due to heterogeneity in phenotype and disease expression across individuals. To address this challenge, investigators are combining multiplexed measurements of different biomarkers that together define robust signatures for specific disease states. Machine learning is a useful tool to automatically discover and detect these signatures, especially as new technologies output increasing quantities of molecular data. In this paper, we review the state of the field of machine learning applied to molecular diagnostics and provide practical guidance to use this tool effectively and to avoid common pitfalls.",2018-01-01,17,1064,84,1223
685,28971622,"Perspectives, potentials and trends of ex vivo and in vivo optical molecular pathology","It is pivotal for medical applications, such as noninvasive histopathologic characterization of tissue, to realize label-free and molecule-specific representation of morphologic and biochemical composition in real-time with subcellular spatial resolution. This unmet clinical need requires new approaches for rapid and reliable real-time assessment of pathologies to complement established diagnostic tools. Photonic imaging combined with digitalization offers the potential to provide the clinician the requested information both under in vivo and ex vivo conditions. This report summarizes photonic approaches and their use in combination with image processing, machine learning and augmented virtual reality that might solve current challenges in modern medicine. Details are given for pathology, intraoperative diagnosis in head and neck cancer and endoscopic diagnosis in gastroenterology.",2018-01-01,2,894,86,1223
602,29320410,Advances in Non-Destructive Early Assessment of Fruit Ripeness towards Defining Optimal Time of Harvest and Yield Prediction-A Review,"Global food security for the increasing world population not only requires increased sustainable production of food but a significant reduction in pre- and post-harvest waste. The timing of when a fruit is harvested is critical for reducing waste along the supply chain and increasing fruit quality for consumers. The early in-field assessment of fruit ripeness and prediction of the harvest date and yield by non-destructive technologies have the potential to revolutionize farming practices and enable the consumer to eat the tastiest and freshest fruit possible. A variety of non-destructive techniques have been applied to estimate the ripeness or maturity but not all of them are applicable for in situ (field or glasshouse) assessment. This review focuses on the non-destructive methods which are promising for, or have already been applied to, the pre-harvest in-field measurements including colorimetry, visible imaging, spectroscopy and spectroscopic imaging. Machine learning and regression models used in assessing ripeness are also discussed.",2018-01-01,5,1054,133,1223
636,29150140,Visual pathways from the perspective of cost functions and multi-task deep neural networks,"Vision research has been shaped by the seminal insight that we can understand the higher-tier visual cortex from the perspective of multiple functional pathways with different goals. In this paper, we try to give a computational account of the functional organization of this system by reasoning from the perspective of multi-task deep neural networks. Machine learning has shown that tasks become easier to solve when they are decomposed into subtasks with their own cost function. We hypothesize that the visual system optimizes multiple cost functions of unrelated tasks and this causes the emergence of a ventral pathway dedicated to vision for perception, and a dorsal pathway dedicated to vision for action. To evaluate the functional organization in multi-task deep neural networks, we propose a method that measures the contribution of a unit towards each task, applying it to two networks that have been trained on either two related or two unrelated tasks, using an identical stimulus set. Results show that the network trained on the unrelated tasks shows a decreasing degree of feature representation sharing towards higher-tier layers while the network trained on related tasks uniformly shows high degree of sharing. We conjecture that the method we propose can be used to analyze the anatomical and functional organization of the visual system and beyond. We predict that the degree to which tasks are related is a good descriptor of the degree to which they share downstream cortical-units.",2018-01-01,1,1506,90,1223
641,29134342,An introduction and overview of machine learning in neurosurgical care,"Background:                    Machine learning (ML) is a branch of artificial intelligence that allows computers to learn from large complex datasets without being explicitly programmed. Although ML is already widely manifest in our daily lives in various forms, the considerable potential of ML has yet to find its way into mainstream medical research and day-to-day clinical care. The complex diagnostic and therapeutic modalities used in neurosurgery provide a vast amount of data that is ideally suited for ML models. This systematic review explores ML's potential to assist and improve neurosurgical care.              Method:                    A systematic literature search was performed in the PubMed and Embase databases to identify all potentially relevant studies up to January 1, 2017. All studies were included that evaluated ML models assisting neurosurgical treatment.              Results:                    Of the 6,402 citations identified, 221 studies were selected after subsequent title/abstract and full-text screening. In these studies, ML was used to assist surgical treatment of patients with epilepsy, brain tumors, spinal lesions, neurovascular pathology, Parkinson's disease, traumatic brain injury, and hydrocephalus. Across multiple paradigms, ML was found to be a valuable tool for presurgical planning, intraoperative guidance, neurophysiological monitoring, and neurosurgical outcome prediction.              Conclusions:                    ML has started to find applications aimed at improving neurosurgical care by increasing the efficiency and precision of perioperative decision-making. A thorough validation of specific ML models is essential before implementation in clinical neurosurgical care. To bridge the gap between research and clinical care, practical and ethical issues should be considered parallel to the development of these techniques.",2018-01-01,0,1891,70,1223
601,29321268,Computational techniques for ECG analysis and interpretation in light of their contribution to medical advances,"Widely developed for clinical screening, electrocardiogram (ECG) recordings capture the cardiac electrical activity from the body surface. ECG analysis can therefore be a crucial first step to help diagnose, understand and predict cardiovascular disorders responsible for 30% of deaths worldwide. Computational techniques, and more specifically machine learning techniques and computational modelling are powerful tools for classification, clustering and simulation, and they have recently been applied to address the analysis of medical data, especially ECG data. This review describes the computational methods in use for ECG analysis, with a focus on machine learning and 3D computer simulations, as well as their accuracy, clinical implications and contributions to medical advances. The first section focuses on heartbeat classification and the techniques developed to extract and classify abnormal from regular beats. The second section focuses on patient diagnosis from whole recordings, applied to different diseases. The third section presents real-time diagnosis and applications to wearable devices. The fourth section highlights the recent field of personalized ECG computer simulations and their interpretation. Finally, the discussion section outlines the challenges of ECG analysis and provides a critical assessment of the methods presented. The computational methods reported in this review are a strong asset for medical discoveries and their translation to the clinical world may lead to promising advances.",2018-01-01,19,1526,111,1223
599,29324649,Machine Learning Methods for Analysis of Metabolic Data and Metabolic Pathway Modeling,"Machine learning uses experimental data to optimize clustering or classification of samples or features, or to develop, augment or verify models that can be used to predict behavior or properties of systems. It is expected that machine learning will help provide actionable knowledge from a variety of big data including metabolomics data, as well as results of metabolism models. A variety of machine learning methods has been applied in bioinformatics and metabolism analyses including self-organizing maps, support vector machines, the kernel machine, Bayesian networks or fuzzy logic. To a lesser extent, machine learning has also been utilized to take advantage of the increasing availability of genomics and metabolomics data for the optimization of metabolic network models and their analysis. In this context, machine learning has aided the development of metabolic networks, the calculation of parameters for stoichiometric and kinetic models, as well as the analysis of major features in the model for the optimal application of bioreactors. Examples of this very interesting, albeit highly complex, application of machine learning for metabolism modeling will be the primary focus of this review presenting several different types of applications for model optimization, parameter determination or system analysis using models, as well as the utilization of several different types of machine learning technologies.",2018-01-01,20,1426,86,1223
2391,29348728,Systems-level mechanisms of action of Panax ginseng: a network pharmacological approach,"Panax ginseng has been used since ancient times based on the traditional Asian medicine theory and clinical experiences, and currently, is one of the most popular herbs in the world. To date, most of the studies concerning P. ginseng have focused on specific mechanisms of action of individual constituents. However, in spite of many studies on the molecular mechanisms of P. ginseng, it still remains unclear how multiple active ingredients of P. ginseng interact with multiple targets simultaneously, giving the multidimensional effects on various conditions and diseases. In order to decipher the systems-level mechanism of multiple ingredients of P. ginseng, a novel approach is needed beyond conventional reductive analysis. We aim to review the systems-level mechanism of P. ginseng by adopting novel analytical framework-network pharmacology. Here, we constructed a compound-target network of P. ginseng using experimentally validated and machine learning-based prediction results. The targets of the network were analyzed in terms of related biological process, pathways, and diseases. The majority of targets were found to be related with primary metabolic process, signal transduction, nitrogen compound metabolic process, blood circulation, immune system process, cell-cell signaling, biosynthetic process, and neurological system process. In pathway enrichment analysis of targets, mainly the terms related with neural activity showed significant enrichment and formed a cluster. Finally, relative degrees analysis for the target-disease association of P. ginseng revealed several categories of related diseases, including respiratory, psychiatric, and cardiovascular diseases.",2018-01-01,11,1689,87,1223
669,29031831,Large-scale retrieval for medical image analytics: A comprehensive review,"Over the past decades, medical image analytics was greatly facilitated by the explosion of digital imaging techniques, where huge amounts of medical images were produced with ever-increasing quality and diversity. However, conventional methods for analyzing medical images have achieved limited success, as they are not capable to tackle the huge amount of image data. In this paper, we review state-of-the-art approaches for large-scale medical image analysis, which are mainly based on recent advances in computer vision, machine learning and information retrieval. Specifically, we first present the general pipeline of large-scale retrieval, summarize the challenges/opportunities of medical image analytics on a large-scale. Then, we provide a comprehensive review of algorithms and techniques relevant to major processes in the pipeline, including feature representation, feature indexing, searching, etc. On the basis of existing work, we introduce the evaluation protocols and multiple applications of large-scale medical image retrieval, with a variety of exploratory and diagnostic scenarios. Finally, we discuss future directions of large-scale retrieval, which can further improve the performance of medical image analysis.",2018-01-01,14,1235,73,1223
673,29020316,Machine Learning for Healthcare: On the Verge of a Major Shift in Healthcare Epidemiology,"The increasing availability of electronic health data presents a major opportunity in healthcare for both discovery and practical applications to improve healthcare. However, for healthcare epidemiologists to best use these data, computational techniques that can handle large complex datasets are required. Machine learning (ML), the study of tools and methods for identifying patterns in data, can help. The appropriate application of ML to these data promises to transform patient risk stratification broadly in the field of medicine and especially in infectious diseases. This, in turn, could lead to targeted interventions that reduce the spread of healthcare-associated pathogens. In this review, we begin with an introduction to the basics of ML. We then move on to discuss how ML can transform healthcare epidemiology, providing examples of successful applications. Finally, we present special considerations for those healthcare epidemiologists who want to use and apply ML.",2018-01-01,39,983,89,1223
665,29042216,Mapping human brain lesions and their functional consequences,"Neuroscience has a long history of inferring brain function by examining the relationship between brain injury and subsequent behavioral impairments. The primary advantage of this method over correlative methods is that it can tell us if a certain brain region is necessary for a given cognitive function. In addition, lesion-based analyses provide unique insights into clinical deficits. In the last decade, statistical voxel-based lesion behavior mapping (VLBM) emerged as a powerful method for understanding the architecture of the human brain. This review illustrates how VLBM improves our knowledge of functional brain architecture, as well as how it is inherently limited by its mass-univariate approach. A wide array of recently developed methods appear to supplement traditional VLBM. This paper provides an overview of these new methods, including the use of specialized imaging modalities, the combination of structural imaging with normative connectome data, as well as multivariate analyses of structural imaging data. We see these new methods as complementing rather than replacing traditional VLBM, providing synergistic tools to answer related questions. Finally, we discuss the potential for these methods to become established in cognitive neuroscience and in clinical applications.",2018-01-01,23,1299,61,1223
678,28986230,Machine Learning and Neurosurgical Outcome Prediction: A Systematic Review,"Objective:                    Accurate measurement of surgical outcomes is highly desirable to optimize surgical decision-making. An important element of surgical decision making is identification of the patient cohort that will benefit from surgery before the intervention. Machine learning (ML) enables computers to learn from previous data to make accurate predictions on new data. In this systematic review, we evaluate the potential of ML for neurosurgical outcome prediction.              Methods:                    A systematic search in the PubMed and Embase databases was performed to identify all potential relevant studies up to January 1, 2017.              Results:                    Thirty studies were identified that evaluated ML algorithms used as prediction models for survival, recurrence, symptom improvement, and adverse events in patients undergoing surgery for epilepsy, brain tumor, spinal lesions, neurovascular disease, movement disorders, traumatic brain injury, and hydrocephalus. Depending on the specific prediction task evaluated and the type of input features included, ML models predicted outcomes after neurosurgery with a median accuracy and area under the receiver operating curve of 94.5% and 0.83, respectively. Compared with logistic regression, ML models performed significantly better and showed a median absolute improvement in accuracy and area under the receiver operating curve of 15% and 0.06, respectively. Some studies also demonstrated a better performance in ML models compared with established prognostic indices and clinical experts.              Conclusions:                    In the research setting, ML has been studied extensively, demonstrating an excellent performance in outcome prediction for a wide range of neurosurgical conditions. However, future studies should investigate how ML can be implemented as a practical tool supporting neurosurgical care.",2018-01-01,30,1917,74,1223
2367,29409736,Biosignature Discovery for Substance Use Disorders Using Statistical Learning,"There are limited biomarkers for substance use disorders (SUDs). Traditional statistical approaches are identifying simple biomarkers in large samples, but clinical use cases are still being established. High-throughput clinical, imaging, and 'omic' technologies are generating data from SUD studies and may lead to more sophisticated and clinically useful models. However, analytic strategies suited for high-dimensional data are not regularly used. We review strategies for identifying biomarkers and biosignatures from high-dimensional data types. Focusing on penalized regression and Bayesian approaches, we address how to leverage evidence from existing studies and knowledge bases, using nicotine metabolism as an example. We posit that big data and machine learning approaches will considerably advance SUD biomarker discovery. However, translation to clinical practice, will require integrated scientific efforts.",2018-02-01,3,921,77,1192
609,29275361,Applications of Support Vector Machine (SVM) Learning in Cancer Genomics,"Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.",2018-02-01,62,724,72,1192
2366,29415726,Advances in intelligent diagnosis methods for pulmonary ground-glass opacity nodules,"Pulmonary nodule is one of the important lesions of lung cancer, mainly divided into two categories of solid nodules and ground glass nodules. The improvement of diagnosis of lung cancer has significant clinical significance, which could be realized by machine learning techniques. At present, there have been a lot of researches focusing on solid nodules. But the research on ground glass nodules started late, and lacked research results. This paper summarizes the research progress of the method of intelligent diagnosis for pulmonary nodules since 2014. It is described in details from four aspects: nodular signs, data analysis methods, prediction models and system evaluation. This paper aims to provide the research material for researchers of the clinical diagnosis and intelligent analysis of lung cancer, and further improve the precision of pulmonary ground glass nodule diagnosis.",2018-02-01,4,892,84,1192
2362,29427011,Self-learning computers for surgical planning and prediction of postoperative alignment,"Purpose:                    In past decades, the role of sagittal alignment has been widely demonstrated in the setting of spinal conditions. As several parameters can be affected, identifying the driver of the deformity is the cornerstone of a successful treatment approach. Despite the importance of restoring sagittal alignment for optimizing outcome, this task remains challenging. Self-learning computers and optimized algorithms are of great interest in spine surgery as in that they facilitate better planning and prediction of postoperative alignment. Nowadays, computer-assisted tools are part of surgeons' daily practice; however, the use of such tools remains to be time-consuming.              Methods:                    NARRATIVE REVIEW AND RESULTS: Computer-assisted methods for the prediction of postoperative alignment consist of a three step analysis: identification of anatomical landmark, definition of alignment objectives, and simulation of surgery. Recently, complex rules for the prediction of alignment have been proposed. Even though this kind of work leads to more personalized objectives, the number of parameters involved renders it difficult for clinical use, stressing the importance of developing computer-assisted tools. The evolution of our current technology, including machine learning and other types of advanced algorithms, will provide powerful tools that could be useful in improving surgical outcomes and alignment prediction. These tools can combine different types of advanced technologies, such as image recognition and shape modeling, and using this technique, computer-assisted methods are able to predict spinal shape. The development of powerful computer-assisted methods involves the integration of several sources of information such as radiographic parameters (X-rays, MRI, CT scan, etc.), demographic information, and unusual non-osseous parameters (muscle quality, proprioception, gait analysis data). In using a larger set of data, these methods will aim to mimic what is actually done by spine surgeons, leading to real tailor-made solutions.              Conclusion:                    Integrating newer technology can change the current way of planning/simulating surgery. The use of powerful computer-assisted tools that are able to integrate several parameters and learn from experience can change the traditional way of selecting treatment pathways and counseling patients. However, there is still much work to be done to reach a desired level as noted in other orthopedic fields, such as hip surgery. Many of these tools already exist in non-medical fields and their adaptation to spine surgery is of considerable interest.",2018-02-01,4,2684,87,1192
476,30275936,Machine Learning Methods for Histopathological Image Analysis,"Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.",2018-02-01,78,472,61,1192
2346,29467659,Transfer and Multi-task Learning in QSAR Modeling: Advances and Challenges,"Medicinal chemistry projects involve some steps aiming to develop a new drug, such as the analysis of biological targets related to a given disease, the discovery and the development of drug candidates for these targets, performing parallel biological tests to validate the drug effectiveness and side effects. Approaches as quantitative study of activity-structure relationships (QSAR) involve the construction of predictive models that relate a set of descriptors of a chemical compound series and its biological activities with respect to one or more targets in the human body. Datasets used to perform QSAR analyses are generally characterized by a small number of samples and this makes them more complex to build accurate predictive models. In this context, transfer and multi-task learning techniques are very suitable since they take information from other QSAR models to the same biological target, reducing efforts and costs for generating new chemical compounds. Therefore, this review will present the main features of transfer and multi-task learning studies, as well as some applications and its potentiality in drug design projects.",2018-02-01,10,1147,74,1192
2316,29594137,Artificial Intelligence for the Artificial Kidney: Pointers to the Future of a Personalized Hemodialysis Therapy,"Background:                    Current dialysis devices are not able to react when unexpected changes occur during dialysis treatment or to learn about experience for therapy personalization. Furthermore, great efforts are dedicated to develop miniaturized artificial kidneys to achieve a continuous and personalized dialysis therapy, in order to improve the patient's quality of life. These innovative dialysis devices will require a real-time monitoring of equipment alarms, dialysis parameters, and patient-related data to ensure patient safety and to allow instantaneous changes of the dialysis prescription for the assessment of their adequacy. The analysis and evaluation of the resulting large-scale data sets enters the realm of ""big data"" and will require real-time predictive models. These may come from the fields of machine learning and computational intelligence, both included in artificial intelligence, a branch of engineering involved with the creation of devices that simulate intelligent behavior. The incorporation of artificial intelligence should provide a fully new approach to data analysis, enabling future advances in personalized dialysis therapies. With the purpose to learn about the present and potential future impact on medicine from experts in artificial intelligence and machine learning, a scientific meeting was organized in the Hospital Universitari Bellvitge (L'Hospitalet, Barcelona). As an outcome of that meeting, the aim of this review is to investigate artificial intel ligence experiences on dialysis, with a focus on potential barriers, challenges, and prospects for future applications of these technologies.              Summary and key messages:                    Artificial intelligence research on dialysis is still in an early stage, and the main challenge relies on interpretability and/or comprehensibility of data models when applied to decision making. Artificial neural networks and medical decision support systems have been used to make predictions about anemia, total body water, or intradialysis hypotension and are promising approaches for the prescription and monitoring of hemodialysis therapy. Current dialysis machines are continuously improving due to innovative technological developments, but patient safety is still a key challenge. Real-time monitoring systems, coupled with automatic instantaneous biofeedback, will allow changing dialysis prescriptions continuously. The integration of vital sign monitoring with dialysis parameters will produce large data sets that will require the use of data analysis techniques, possibly from the area of machine learning, in order to make better decisions and increase the safety of patients.",2018-02-01,6,2704,112,1192
615,29249344,Computational methods for corpus callosum segmentation on MRI: A systematic literature review,"Background and objective:                    The corpus callosum (CC) is the largest white matter structure in the brain and has a significant role in central nervous system diseases. Its volume correlates with the severity and/or extent of neurodegenerative disease. Even though the CC's role has been extensively studied over the last decades, and different algorithms and methods have been published regarding CC segmentation and parcellation, no reviews or surveys covering such developments have been reported so far. To bridge this gap, this paper presents a systematic literature review of computational methods focusing on CC segmentation and parcellation acquired on magnetic resonance imaging.              Methods:                    IEEExplore, PubMed, EBSCO Host, and Scopus database were searched with the following search terms: ((Segmentation OR Parcellation) AND (Corpus Callosum) AND (DTI OR MRI OR Diffusion Tensor Imag* OR Diffusion Tractography OR Magnetic Resonance Imag*)), resulting in 802 publications. Two reviewers independently evaluated all articles and 36 studies were selected through the systematic literature review process.              Results:                    This work reviewed four main segmentation methods groups: model-based, region-based, thresholding, and machine learning; 32 different validity metrics were reported. Even though model-based techniques are the most recurrently used for the segmentation task (13 articles), machine learning approaches achieved better outcomes of 95% when analyzing mean values for segmentation and classification metrics results. Moreover, CC segmentation is better established in T1-weighted images, having more methods implemented and also being tested in larger datasets, compared with diffusion tensor images.              Conclusions:                    The analyzed computational methods used to perform CC segmentation on magnetic resonance imaging have not yet overcome all presented challenges owing to metrics variability and lack of traceable materials.",2018-02-01,1,2045,93,1192
2354,29439490,Chemical Sensor Systems and Associated Algorithms for Fire Detection: A Review,"Indoor fire detection using gas chemical sensing has been a subject of investigation since the early nineties. This approach leverages the fact that, for certain types of fire, chemical volatiles appear before smoke particles do. Hence, systems based on chemical sensing can provide faster fire alarm responses than conventional smoke-based fire detectors. Moreover, since it is known that most casualties in fires are produced from toxic emissions rather than actual burns, gas-based fire detection could provide an additional level of safety to building occupants. In this line, since the 2000s, electrochemical cells for carbon monoxide sensing have been incorporated into fire detectors. Even systems relying exclusively on gas sensors have been explored as fire detectors. However, gas sensors respond to a large variety of volatiles beyond combustion products. As a result, chemical-based fire detectors require multivariate data processing techniques to ensure high sensitivity to fires and false alarm immunity. In this paper, we the survey toxic emissions produced in fires and defined standards for fire detection systems. We also review the state of the art of chemical sensor systems for fire detection and the associated signal and data processing algorithms. We also examine the experimental protocols used for the validation of the different approaches, as the complexity of the test measurements also impacts on reported sensitivity and specificity measures. All in all, further research and extensive test under different fire and nuisance scenarios are still required before gas-based fire detectors penetrate largely into the market. Nevertheless, the use of dynamic features and multivariate models that exploit sensor correlations seems imperative.",2018-02-01,7,1769,78,1192
2353,29441154,e-PTSD: an overview on how new technologies can improve prediction and assessment of Posttraumatic Stress Disorder (PTSD),"Background: New technologies may profoundly change our way of understanding psychiatric disorders including posttraumatic stress disorder (PTSD). Imaging and biomarkers, along with technological and medical informatics developments, might provide an answer regarding at-risk patient's identification. Recent advances in the concept of 'digital phenotype', which refers to the capture of characteristics of a psychiatric disorder by computerized measurement tools, is one paradigmatic example. Objective: The impact of the new technologies on health professionals practice in PTSD care remains to be determined. The recent evolutions could disrupt the clinical practices and practitioners in their beliefs, ethics and representations, going as far as questioning their professional culture. In the present paper, we conducted an extensive search to highlight the articles which reflect the potential of these new technologies. Method: We conducted an overview by querying PubMed database with the terms [PTSD] [Posttraumatic stress disorder] AND [Computer] OR [Computerized] OR [Mobile] OR [Automatic] OR [Automated] OR [Machine learning] OR [Sensor] OR [Heart rate variability] OR [HRV] OR [actigraphy] OR [actimetry] OR [digital] OR [motion] OR [temperature] OR [virtual reality]. Results: We summarized the synthesized literature in two categories: prediction and assessment (including diagnostic, screening and monitoring). Two independent reviewers screened, extracted data and quality appraised the sources. Results were synthesized narratively. Conclusions: This overview shows that many studies are underway allowing researchers to start building a PTSD digital phenotype using passive data obtained by biometric sensors. Active data obtained from Ecological Momentary Assessment (EMA) could allow clinicians to assess PTSD patients. The place of connected objects, Artificial Intelligence and remote monitoring of patients with psychiatric pathology remains to be defined. These tools must be explained and adapted to the different profiles of physicians and patients. The involvement of patients, caregivers and health professionals is essential to the design and evaluation of these new tools.",2018-02-01,12,2203,121,1192
314,28798198,Wearable ballistocardiogram and seismocardiogram systems for health and performance,"Cardiovascular diseases (CVDs) are prevalent in the US, and many forms of CVD primarily affect the mechanical aspects of heart function. Wearable technologies for monitoring the mechanical health of the heart and vasculature could enable proactive management of CVDs through titration of care based on physiological status as well as preventative wellness monitoring to help promote lifestyle choices that reduce the overall risk of developing CVDs. Additionally, such wearable technologies could be used to optimize human performance in austere environments. This review describes our progress in developing wearable ballistocardiogram (BCG)- and seismocardiogram-based systems for monitoring relative changes in cardiac output, contractility, and blood pressure. Our systems use miniature, low-noise accelerometers to measure the movements of the body in response to the heartbeat and novel machine learning algorithms to provide robustness against motion artifacts and sensor misplacement. Moreover, we have mathematically related wearable BCG signals-representing local, cardiogenic movements of a point on the body-to better understood whole body BCG signals, and thereby improved estimation of key health parameters. We validated these systems with experiments in healthy subjects, studies in patients with heart failure, and measurements in austere environments such as water immersion. The systems can be used in future work as a tool for clinicians and physiologists to measure the mechanical aspects of cardiovascular function outside of clinical settings, and to thereby titrate care for patients with CVDs, provide preventative screening, and optimize performance in austere environments by providing real-time in-depth information regarding performance and risk.",2018-02-01,11,1775,83,1192
317,28751369,Wearable technology for compensatory reserve to sense hypovolemia,"Traditional monitoring technologies fail to provide accurate or early indications of hypovolemia-mediated extremis because physiological systems (as measured by vital signs) effectively compensate until circulatory failure occurs. Hypovolemia is the most life-threatening physiological condition associated with circulatory shock in hemorrhage or sepsis, and it impairs one's ability to sustain physical exertion during heat stress. This review focuses on the physiology underlying the development of a novel noninvasive wearable technology that allows for real-time evaluation of the cardiovascular system's ability to compensate to hypovolemia, or its compensatory reserve, which provides an individualized estimate of impending circulatory collapse. Compensatory reserve is assessed by real-time changes (sampled millions of times per second) in specific features (hundreds of features) of arterial waveform analog signals that can be obtained from photoplethysmography using machine learning and feature extraction techniques. Extensive experimental evidence employing acute reductions in central blood volume (using lower-body negative pressure, blood withdrawal, heat stress, dehydration) demonstrate that compensatory reserve provides the best indicator for early and accurate assessment for compromises in blood pressure, tissue perfusion, and oxygenation in resting human subjects. Engineering challenges exist for the development of a ruggedized wearable system that can measure signals from multiple sites, improve signal-to-noise ratios, be customized for use in austere conditions (e.g., battlefield, patient transport), and be worn during strenuous physical activity.",2018-02-01,4,1681,65,1192
2348,29463885,Machine learning for the meta-analyses of microbial pathogens' volatile signatures,"Non-invasive and fast diagnostic tools based on volatolomics hold great promise in the control of infectious diseases. However, the tools to identify microbial volatile organic compounds (VOCs) discriminating between human pathogens are still missing. Artificial intelligence is increasingly recognised as an essential tool in health sciences. Machine learning algorithms based in support vector machines and features selection tools were here applied to find sets of microbial VOCs with pathogen-discrimination power. Studies reporting VOCs emitted by human microbial pathogens published between 1977 and 2016 were used as source data. A set of 18 VOCs is sufficient to predict the identity of 11 microbial pathogens with high accuracy (77%), and precision (62-100%). There is one set of VOCs associated with each of the 11 pathogens which can predict the presence of that pathogen in a sample with high accuracy and precision (86-90%). The implemented pathogen classification methodology supports future database updates to include new pathogen-VOC data, which will enrich the classifiers. The sets of VOCs identified potentiate the improvement of the selectivity of non-invasive infection diagnostics using artificial olfaction devices.",2018-02-01,12,1239,82,1192
344,28646349,Modeling Pain Using fMRI: From Regions to Biomarkers,"Pain is a subjective and complex phenomenon. Its complexity is related to its heterogeneity: multiple component processes, including sensation, affect, and cognition, contribute to pain experience and reporting. These components are likely to be encoded in distributed brain networks that interact to create pain experience and pain-related decision-making. Therefore, to understand pain, we must identify these networks and build models of these interactions that yield testable predictions about pain-related outcomes. We have developed several such models or 'signatures' of pain, by (1) integrating activity across multiple systems, and (2) using pattern-recognition to identify processes related to pain experience. One model, the Neurologic Pain Signature, is sensitive and specific to pain in individuals, involves brain regions that receive nociceptive afferents, and shows little effect of expectation or self-regulation in tests to date. Another, the 'Stimulus Intensity-Independent Pain Signature', explains substantial additional variation in trial-to-trial pain reports. It involves many brain regions that do not show increased activity in proportion to noxious stimulus intensity, including medial and lateral prefrontal cortex, nucleus accumbens, and hippocampus. Responses in this system mediate expectancy and perceived control effects in several studies. Overall, this approach provides a pathway to understanding pain by identifying multiple systems that track different aspects of pain. Such componential models can be combined in unique ways on a subject-by-subject basis to explain an individual's pain experience.",2018-02-01,25,1637,52,1192
2368,29405981,Environmental metabolomics with data science for investigating ecosystem homeostasis,"A natural ecosystem can be viewed as the interconnections between complex metabolic reactions and environments. Humans, a part of these ecosystems, and their activities strongly affect the environments. To account for human effects within ecosystems, understanding what benefits humans receive by facilitating the maintenance of environmental homeostasis is important. This review describes recent applications of several NMR approaches to the evaluation of environmental homeostasis by metabolic profiling and data science. The basic NMR strategy used to evaluate homeostasis using big data collection is similar to that used in human health studies. Sophisticated metabolomic approaches (metabolic profiling) are widely reported in the literature. Further challenges include the analysis of complex macromolecular structures, and of the compositions and interactions of plant biomass, soil humic substances, and aqueous particulate organic matter. To support the study of these topics, we also discuss sample preparation techniques and solid-state NMR approaches. Because NMR approaches can produce a number of data with high reproducibility and inter-institution compatibility, further analysis of such data using machine learning approaches is often worthwhile. We also describe methods for data pretreatment in solid-state NMR and for environmental feature extraction from heterogeneously-measured spectroscopic data by machine learning approaches.",2018-02-01,9,1453,84,1192
2387,29362138,Texture analysis and machine learning to characterize suspected thyroid nodules and differentiated thyroid cancer: Where do we stand?,"In thyroid imaging, ""texture"" refers to the echographic appearence of the parenchyma or a nodule. However, definition of the image characteristics is operator dependent and influenced by the operator's experience. In a more objective texture analysis, a variety of mathematical methods are used to describe image inhomogeneity, allowing assessment of an image by means of quantitative parameters. Moreover, this approach may be used to develop an efficient computer-aided diagnosis (CAD) system to yield a second opinion when differentiating malignant and benign thyroid lesions. The aim of this review is to summarize the available literature data on texture analysis, with and without CAD, in patients with suspected thyroid nodules or differentiated thyroid cancer, and to assess the current state of the approach.",2018-02-01,23,817,133,1192
681,28982791,"Radiomics in Brain Tumor: Image Assessment, Quantitative Feature Descriptors, and Machine-Learning Approaches","Radiomics describes a broad set of computational methods that extract quantitative features from radiographic images. The resulting features can be used to inform imaging diagnosis, prognosis, and therapy response in oncology. However, major challenges remain for methodologic developments to optimize feature extraction and provide rapid information flow in clinical settings. Equally important, to be clinically useful, predictive radiomic properties must be clearly linked to meaningful biologic characteristics and qualitative imaging properties familiar to radiologists. Here we use a cross-disciplinary approach to highlight studies in radiomics. We review brain tumor radiologic studies (eg, imaging interpretation) through computational models (eg, computer vision and machine learning) that provide novel clinical insights. We outline current quantitative image feature extraction and prediction strategies with different levels of available clinical classes for supporting clinical decision-making. We further discuss machine-learning challenges and data opportunities to advance radiomic studies.",2018-02-01,63,1107,109,1192
303,28861708,Breast cancer cell nuclei classification in histopathology images using deep neural networks,"Purpose:                    Cell nuclei classification in breast cancer histopathology images plays an important role in effective diagnose since breast cancer can often be characterized by its expression in cell nuclei. However, due to the small and variant sizes of cell nuclei, and heavy noise in histopathology images, traditional machine learning methods cannot achieve desirable recognition accuracy. To address this challenge, this paper aims to present a novel deep neural network which performs representation learning and cell nuclei recognition in an end-to-end manner.              Methods:                    The proposed model hierarchically maps raw medical images into a latent space in which robustness is achieved by employing a stacked denoising autoencoder. A supervised classifier is further developed to improve the discrimination of the model by maximizing inter-subject separability in the latent space. The proposed method involves a cascade model which jointly learns a set of nonlinear mappings and a classifier from the given raw medical images. Such an on-the-shelf learning strategy makes obtaining discriminative features possible, thus leading to better recognition performance.              Results:                    Extensive experiments with benign and malignant breast cancer datasets are conducted to verify the effectiveness of the proposed method. Better performance was obtained when compared with other feature extraction methods, and higher recognition rate was achieved when compared with other seven classification methods.              Conclusions:                    We propose an end-to-end DNN model for cell nuclei and non-nuclei classification of histopathology images. It demonstrates that the proposed method can achieve promising performance in cell nuclei classification, and the proposed method is suitable for the cell nuclei classification task.",2018-02-01,3,1904,92,1192
2381,29374138,A Shared Vision for Machine Learning in Neuroscience,"With ever-increasing advancements in technology, neuroscientists are able to collect data in greater volumes and with finer resolution. The bottleneck in understanding how the brain works is consequently shifting away from the amount and type of data we can collect and toward what we actually do with the data. There has been a growing interest in leveraging this vast volume of data across levels of analysis, measurement techniques, and experimental paradigms to gain more insight into brain function. Such efforts are visible at an international scale, with the emergence of big data neuroscience initiatives, such as the BRAIN initiative (Bargmann et al., 2014), the Human Brain Project, the Human Connectome Project, and the National Institute of Mental Health's Research Domain Criteria initiative. With these large-scale projects, much thought has been given to data-sharing across groups (Poldrack and Gorgolewski, 2014; Sejnowski et al., 2014); however, even with such data-sharing initiatives, funding mechanisms, and infrastructure, there still exists the challenge of how to cohesively integrate all the data. At multiple stages and levels of neuroscience investigation, machine learning holds great promise as an addition to the arsenal of analysis tools for discovering how the brain works.",2018-02-01,15,1305,52,1192
2386,29362150,CT and MR imaging for solid renal mass characterization,"As our understanding has expanded that relatively large fraction of incidentally discovered renal masses, especially in small size, are benign or indolent even if malignant, there is growing acceptance of more conservative management including active surveillance for small renal masses. As for advanced renal cell carcinomas (RCCs), nonsurgical and subtype specific treatment options such as immunotherapy and targeted therapy is developing. On these backgrounds, renal mass characterization including differentiation of benign from malignant tumors, RCC subtyping and prediction of RCC aggressiveness is receiving much attention and a variety of imaging techniques and analytic methods are being investigated. In addition to conventional imaging techniques, integration of texture analysis, functional imaging (i.e. diffusion weighted and perfusion imaging) and multivariate diagnostic methods including machine learning have provided promising results for these purposes in research fields, although standardization and external, multi-institutional validations are needed.",2018-02-01,10,1076,55,1192
627,29183655,Predicting Violent Behavior: What Can Neuroscience Add?,"The ability to accurately predict violence and other forms of serious antisocial behavior would provide important societal benefits, and there is substantial enthusiasm for the potential predictive accuracy of neuroimaging techniques. Here, we review the current status of violence prediction using actuarial and clinical methods, and assess the current state of neuroprediction. We then outline several questions that need to be addressed by future studies of neuroprediction if neuroimaging and other neuroscientific markers are to be successfully translated into public policy.",2018-02-01,17,580,55,1192
2329,29515993,In Silico Prediction of Chemical Toxicity for Drug Design Using Machine Learning Methods and Structural Alerts,"During drug development, safety is always the most important issue, including a variety of toxicities and adverse drug effects, which should be evaluated in preclinical and clinical trial phases. This review article at first simply introduced the computational methods used in prediction of chemical toxicity for drug design, including machine learning methods and structural alerts. Machine learning methods have been widely applied in qualitative classification and quantitative regression studies, while structural alerts can be regarded as a complementary tool for lead optimization. The emphasis of this article was put on the recent progress of predictive models built for various toxicities. Available databases and web servers were also provided. Though the methods and models are very helpful for drug design, there are still some challenges and limitations to be improved for drug safety assessment in the future.",2018-02-01,20,923,110,1192
2330,29515569,Computational Strategies for Dissecting the High-Dimensional Complexity of Adaptive Immune Repertoires,"The adaptive immune system recognizes antigens via an immense array of antigen-binding antibodies and T-cell receptors, the immune repertoire. The interrogation of immune repertoires is of high relevance for understanding the adaptive immune response in disease and infection (e.g., autoimmunity, cancer, HIV). Adaptive immune receptor repertoire sequencing (AIRR-seq) has driven the quantitative and molecular-level profiling of immune repertoires, thereby revealing the high-dimensional complexity of the immune receptor sequence landscape. Several methods for the computational and statistical analysis of large-scale AIRR-seq data have been developed to resolve immune repertoire complexity and to understand the dynamics of adaptive immunity. Here, we review the current research on (i) diversity, (ii) clustering and network, (iii) phylogenetic, and (iv) machine learning methods applied to dissect, quantify, and compare the architecture, evolution, and specificity of immune repertoires. We summarize outstanding questions in computational immunology and propose future directions for systems immunology toward coupling AIRR-seq with the computational discovery of immunotherapeutics, vaccines, and immunodiagnostics.",2018-02-01,41,1225,102,1192
2895,29977480,An Artificial Neural Network Integrated Pipeline for Biomarker Discovery Using Alzheimer's Disease as a Case Study,"The field of machine learning has allowed researchers to generate and analyse vast amounts of data using a wide variety of methodologies. Artificial Neural Networks (ANN) are some of the most commonly used statistical models and have been successful in biomarker discovery studies in multiple disease types. This review seeks to explore and evaluate an integrated ANN pipeline for biomarker discovery and validation in Alzheimer's disease, the most common form of dementia worldwide with no proven cause and no available cure. The proposed pipeline consists of analysing public data with a categorical and continuous stepwise algorithm and further examination through network inference to predict gene interactions. This methodology can reliably generate novel markers and further examine known ones and can be used to guide future research in Alzheimer's disease.",2018-02-01,5,864,114,1192
644,29126825,Artificial Intelligence in Medical Practice: The Question to the Answer?,"Computer science advances and ultra-fast computing speeds find artificial intelligence (AI) broadly benefitting modern society-forecasting weather, recognizing faces, detecting fraud, and deciphering genomics. AI's future role in medical practice remains an unanswered question. Machines (computers) learn to detect patterns not decipherable using biostatistics by processing massive datasets (big data) through layered mathematical models (algorithms). Correcting algorithm mistakes (training) adds to AI predictive model confidence. AI is being successfully applied for image analysis in radiology, pathology, and dermatology, with diagnostic speed exceeding, and accuracy paralleling, medical experts. While diagnostic confidence never reaches 100%, combining machines plus physicians reliably enhances system performance. Cognitive programs are impacting medical practice by applying natural language processing to read the rapidly expanding scientific literature and collate years of diverse electronic medical records. In this and other ways, AI may optimize the care trajectory of chronic disease patients, suggest precision therapies for complex illnesses, reduce medical errors, and improve subject enrollment into clinical trials.",2018-02-01,62,1240,72,1192
2335,29497285,Review of Statistical Learning Methods in Integrated Omics Studies (An Integrated Information Science),"Integrated omics is becoming a new channel for investigating the complex molecular system in modern biological science and sets a foundation for systematic learning for precision medicine. The statistical/machine learning methods that have emerged in the past decade for integrated omics are not only innovative but also multidisciplinary with integrated knowledge in biology, medicine, statistics, machine learning, and artificial intelligence. Here, we review the nontrivial classes of learning methods from the statistical aspects and streamline these learning methods within the statistical learning framework. The intriguing findings from the review are that the methods used are generalizable to other disciplines with complex systematic structure, and the integrated omics is part of an integrated information science which has collated and integrated different types of information for inferences and decision making. We review the statistical learning methods of exploratory and supervised learning from 42 publications. We also discuss the strengths and limitations of the extended principal component analysis, cluster analysis, network analysis, and regression methods. Statistical techniques such as penalization for sparsity induction when there are fewer observations than the number of features and using Bayesian approach when there are prior knowledge to be integrated are also included in the commentary. For the completeness of the review, a table of currently available software and packages from 23 publications for omics are summarized in the appendix.",2018-02-01,18,1575,102,1192
2379,29376234,Multimodal Imaging in Diabetic Macular Edema,"Throughout ophthalmic history it has been shown that progress has gone hand in hand with technological breakthroughs. In the past, fluorescein angiography and fundus photographs were the most commonly used imaging modalities in the management of diabetic macular edema (DME). Today, despite the moderate correlation between macular thickness and functional outcomes, spectral domain optical coherence tomography (SD-OCT) has become the DME workhorse in clinical practice. Several SD-OCT biomarkers have been looked at including presence of epiretinal membrane, vitreomacular adhesion, disorganization of the inner retinal layers, central macular thickness, integrity of the ellipsoid layer, and subretinal fluid, among others. Emerging imaging modalities include fundus autofluorescence, macular pigment optical density, fluorescence lifetime imaging ophthalmoscopy, OCT angiography, and adaptive optics. Technological advances in imaging of the posterior segment of the eye have enabled ophthalmologists to develop hypotheses about pathological mechanisms of disease, monitor disease progression, and assess response to treatment. Spectral domain OCT is the most commonly performed imaging modality in the management of DME. However, reliable biomarkers have yet to be identified. Machine learning may provide treatment algorithms based on multimodal imaging.",2018-02-01,2,1360,44,1192
640,29136580,Advancing the large-scale CCS database for metabolomics and lipidomics at the machine-learning era,"Metabolomics and lipidomics aim to comprehensively measure the dynamic changes of all metabolites and lipids that are present in biological systems. The use of ion mobility-mass spectrometry (IM-MS) for metabolomics and lipidomics has facilitated the separation and the identification of metabolites and lipids in complex biological samples. The collision cross-section (CCS) value derived from IM-MS is a valuable physiochemical property for the unambiguous identification of metabolites and lipids. However, CCS values obtained from experimental measurement and computational modeling are limited available, which significantly restricts the application of IM-MS. In this review, we will discuss the recently developed machine-learning based prediction approach, which could efficiently generate precise CCS databases in a large scale. We will also highlight the applications of CCS databases to support metabolomics and lipidomics.",2018-02-01,11,934,98,1192
2337,29490932,Emerging Technologies for Molecular Diagnosis of Sepsis,"Rapid and accurate profiling of infection-causing pathogens remains a significant challenge in modern health care. Despite advances in molecular diagnostic techniques, blood culture analysis remains the gold standard for diagnosing sepsis. However, this method is too slow and cumbersome to significantly influence the initial management of patients. The swift initiation of precise and targeted antibiotic therapies depends on the ability of a sepsis diagnostic test to capture clinically relevant organisms along with antimicrobial resistance within 1 to 3 h. The administration of appropriate, narrow-spectrum antibiotics demands that such a test be extremely sensitive with a high negative predictive value. In addition, it should utilize small sample volumes and detect polymicrobial infections and contaminants. All of this must be accomplished with a platform that is easily integrated into the clinical workflow. In this review, we outline the limitations of routine blood culture testing and discuss how emerging sepsis technologies are converging on the characteristics of the ideal sepsis diagnostic test. We include seven molecular technologies that have been validated on clinical blood specimens or mock samples using human blood. In addition, we discuss advances in machine learning technologies that use electronic medical record data to provide contextual evaluation support for clinical decision-making.",2018-02-01,43,1421,55,1192
2332,29508152,A Systematic Literature Review of Technologies for Suicidal Behavior Prevention,"Suicide is the second cause of death in young people. The use of technologies as tools facilitates the detection of individuals at risk of suicide thus allowing early intervention and efficacy. Suicide can be prevented in many cases. Technology can help people at risk of suicide and their families. It could prevent situations of risk of suicide with the technological evolution that is increasing. This work is a systematic review of research papers published in the last ten years on technology for suicide prevention. In September 2017, the consultation was carried out in the scientific databases PubMed, ScienceDirect, PsycINFO, The Cochrane Library and Google Scholar. A general search was conducted with the terms ""prevention"" AND ""suicide"" AND ""technology. More specific searches included technologies such as ""Web"", ""mobile"", ""social networks"", and others terms related to technologies. The number of articles found following the methodology proposed was 90, but only 30 are focused on the objective of this work. Most of them were Web technologies (51.61%), mobile solutions (22.58%), social networks (12.90%), machine learning (3.23%) and other technologies (9.68%). According to the results obtained, although there are technological solutions that help the prevention of suicide, much remains to be done in this field. Collaboration among technologists, psychiatrists, patients, and family members is key to advancing the development of new technology-based solutions that can help save lives.",2018-03-01,13,1507,79,1164
2363,29426712,Getting Momentum: From Biocatalysis to Advanced Synthetic Biology,"Applied biocatalysis is driven by environmental and economic incentives for using enzymes in the synthesis of various pharmaceutical and industrially important chemicals. Protein engineering is used to tailor the properties of enzymes to catalyze desired chemical transformations, and some engineered enzymes now outperform the best chemocatalytic alternatives by orders of magnitude. Unfortunately, custom engineering of a robust biocatalyst is still a time-consuming process, but an understanding of how enzyme function depends on amino acid sequence will speed up the process. This review demonstrates how recent advances in ultrahigh-throughput screening, mutational scanning, DNA synthesis, metagenomics, and machine learning will soon make it possible to model, predict, and manipulate the relationship between protein sequence and function, accelerating the tailor design of novel biocatalysts.",2018-03-01,11,901,65,1164
2361,29428074,Machine learning techniques for breast cancer computer aided diagnosis using different image modalities: A systematic review,"Background and objective:                    The high incidence of breast cancer in women has increased significantly in the recent years. Physician experience of diagnosing and detecting breast cancer can be assisted by using some computerized features extraction and classification algorithms. This paper presents the conduction and results of a systematic review (SR) that aims to investigate the state of the art regarding the computer aided diagnosis/detection (CAD) systems for breast cancer.              Methods:                    The SR was conducted using a comprehensive selection of scientific databases as reference sources, allowing access to diverse publications in the field. The scientific databases used are Springer Link (SL), Science Direct (SD), IEEE Xplore Digital Library, and PubMed. Inclusion and exclusion criteria were defined and applied to each retrieved work to select those of interest. From 320 studies retrieved, 154 studies were included. However, the scope of this research is limited to scientific and academic works and excludes commercial interests.              Results:                    This survey provides a general analysis of the current status of CAD systems according to the used image modalities and the machine learning based classifiers. Potential research studies have been discussed to create a more objective and efficient CAD systems.",2018-03-01,15,1390,124,1164
2340,29486863,Machine Learning for Precision Psychiatry: Opportunities and Challenges,"The nature of mental illness remains a conundrum. Traditional disease categories are increasingly suspected to misrepresent the causes underlying mental disturbance. Yet psychiatrists and investigators now have an unprecedented opportunity to benefit from complex patterns in brain, behavior, and genes using methods from machine learning (e.g., support vector machines, modern neural-network algorithms, cross-validation procedures). Combining these analysis techniques with a wealth of data from consortia and repositories has the potential to advance a biologically grounded redefinition of major psychiatric disorders. Increasing evidence suggests that data-derived subgroups of psychiatric patients can better predict treatment outcomes than DSM/ICD diagnoses can. In a new era of evidence-based psychiatry tailored to single patients, objectively measurable endophenotypes could allow for early disease detection, individualized treatment selection, and dosage adjustment to reduce the burden of disease. This primer aims to introduce clinicians and researchers to the opportunities and challenges in bringing machine intelligence into psychiatric practice.",2018-03-01,76,1163,71,1164
2385,29366598,Translational Radiomics: Defining the Strategy Pipeline and Considerations for Application-Part 2: From Clinical Implementation to Enterprise,"Enterprise imaging has channeled various technological innovations to the field of clinical radiology, ranging from advanced imaging equipment and postacquisition iterative reconstruction tools to image analysis and computer-aided detection tools. More recently, the advancement in the field of quantitative image analysis coupled with machine learning-based data analytics, classification, and integration has ushered in the era of radiomics, a paradigm shift that holds tremendous potential in clinical decision support as well as drug discovery. However, there are important issues to consider to incorporate radiomics into a clinically applicable system and a commercially viable solution. In this two-part series, we offer insights into the development of the translational pipeline for radiomics from methodology to clinical implementation (Part 1) and from that point to enterprise development (Part 2). In Part 2 of this two-part series, we study the components of the strategy pipeline, from clinical implementation to building enterprise solutions.",2018-03-01,3,1058,141,1164
2325,29534053,Calibration of Minimally Invasive Continuous Glucose Monitoring Sensors: State-of-The-Art and Current Perspectives,"Minimally invasive continuous glucose monitoring (CGM) sensors are wearable medical devices that provide real-time measurement of subcutaneous glucose concentration. This can be of great help in the daily management of diabetes. Most of the commercially available CGM devices have a wire-based sensor, usually placed in the subcutaneous tissue, which measures a ""raw"" current signal via a glucose-oxidase electrochemical reaction. This electrical signal needs to be translated in real-time to glucose concentration through a calibration process. For such a scope, the first commercialized CGM sensors implemented simple linear regression techniques to fit reference glucose concentration measurements periodically collected by fingerprick. On the one hand, these simple linear techniques required several calibrations per day, with the consequent patient's discomfort. On the other, only a limited accuracy was achieved. This stimulated researchers to propose, over the last decade, more sophisticated algorithms to calibrate CGM sensors, resorting to suitable signal processing, modelling, and machine-learning techniques. This review paper will first contextualize and describe the calibration problem and its implementation in the first generation of CGM sensors, and then present the most recently-proposed calibration algorithms, with a perspective on how these new techniques can influence future CGM products in terms of accuracy improvement and calibration reduction.",2018-03-01,14,1475,114,1164
2384,29366600,Translational Radiomics: Defining the Strategy Pipeline and Considerations for Application-Part 1: From Methodology to Clinical Implementation,"Enterprise imaging has channeled various technological innovations to the field of clinical radiology, ranging from advanced imaging equipment and postacquisition iterative reconstruction tools to image analysis and computer-aided detection tools. More recently, the advancements in the field of quantitative image analysis coupled with machine learning-based data analytics, classification, and integration have ushered us into the era of radiomics, which has tremendous potential in clinical decision support as well as drug discovery. There are important issues to consider to incorporate radiomics as a clinically applicable system and a commercially viable solution. In this two-part series, we offer insights into the development of the translational pipeline for radiomics from methodology to clinical implementation (Part 1) and from that to enterprise development (Part 2).",2018-03-01,3,882,142,1164
2352,29441463,Cumulative Risk and Impact Modeling on Environmental Chemical and Social Stressors,"Purpose of review:                    The goal of this review is to identify cumulative modeling methods used to evaluate combined effects of exposures to environmental chemicals and social stressors. The specific review question is: What are the existing quantitative methods used to examine the cumulative impacts of exposures to environmental chemical and social stressors on health?              Recent findings:                    There has been an increase in literature that evaluates combined effects of exposures to environmental chemicals and social stressors on health using regression models; very few studies applied other data mining and machine learning techniques to this problem. The majority of studies we identified used regression models to evaluate combined effects of multiple environmental and social stressors. With proper study design and appropriate modeling assumptions, additional data mining methods may be useful to examine combined effects of environmental and social stressors.",2018-03-01,8,1009,82,1164
2311,29603063,Deep Learning for Drug Design: an Artificial Intelligence Paradigm for Drug Discovery in the Big Data Era,"Over the last decade, deep learning (DL) methods have been extremely successful and widely used to develop artificial intelligence (AI) in almost every domain, especially after it achieved its proud record on computational Go. Compared to traditional machine learning (ML) algorithms, DL methods still have a long way to go to achieve recognition in small molecular drug discovery and development. And there is still lots of work to do for the popularization and application of DL for research purpose, e.g., for small molecule drug research and development. In this review, we mainly discussed several most powerful and mainstream architectures, including the convolutional neural network (CNN), recurrent neural network (RNN), and deep auto-encoder networks (DAENs), for supervised learning and nonsupervised learning; summarized most of the representative applications in small molecule drug design; and briefly introduced how DL methods were used in those applications. The discussion for the pros and cons of DL methods as well as the main challenges we need to tackle were also emphasized.",2018-03-01,27,1095,105,1164
2321,29545756,e-Addictology: An Overview of New Technologies for Assessing and Intervening in Addictive Behaviors,"Background:                    New technologies can profoundly change the way we understand psychiatric pathologies and addictive disorders. New concepts are emerging with the development of more accurate means of collecting live data, computerized questionnaires, and the use of passive data. Digital phenotyping, a paradigmatic example, refers to the use of computerized measurement tools to capture the characteristics of different psychiatric disorders. Similarly, machine learning-a form of artificial intelligence-can improve the classification of patients based on patterns that clinicians have not always considered in the past. Remote or automated interventions (web-based or smartphone-based apps), as well as virtual reality and neurofeedback, are already available or under development.              Objective:                    These recent changes have the potential to disrupt practices, as well as practitioners' beliefs, ethics and representations, and may even call into question their professional culture. However, the impact of new technologies on health professionals' practice in addictive disorder care has yet to be determined. In the present paper, we therefore present an overview of new technology in the field of addiction medicine.              Method:                    Using the keywords [e-health], [m-health], [computer], [mobile], [smartphone], [wearable], [digital], [machine learning], [ecological momentary assessment], [biofeedback] and [virtual reality], we searched the PubMed database for the most representative articles in the field of assessment and interventions in substance use disorders.              Results:                    We screened 595 abstracts and analyzed 92 articles, dividing them into seven categories: e-health program and web-based interventions, machine learning, computerized adaptive testing, wearable devices and digital phenotyping, ecological momentary assessment, biofeedback, and virtual reality.              Conclusion:                    This overview shows that new technologies can improve assessment and interventions in the field of addictive disorders. The precise role of connected devices, artificial intelligence and remote monitoring remains to be defined. If they are to be used effectively, these tools must be explained and adapted to the different profiles of physicians and patients. The involvement of patients, caregivers and other health professionals is essential to their design and assessment.",2018-03-01,18,2492,99,1164
2308,29614729,"Imaging, Tracking and Computational Analyses of Virus Entry and Egress with the Cytoskeleton","Viruses have a dual nature: particles are &ldquo;passive substances&rdquo; lacking chemical energy transformation, whereas infected cells are &ldquo;active substances&rdquo; turning-over energy. How passive viral substances convert to active substances, comprising viral replication and assembly compartments has been of intense interest to virologists, cell and molecular biologists and immunologists. Infection starts with virus entry into a susceptible cell and delivers the viral genome to the replication site. This is a multi-step process, and involves the cytoskeleton and associated motor proteins. Likewise, the egress of progeny virus particles from the replication site to the extracellular space is enhanced by the cytoskeleton and associated motor proteins. This overcomes the limitation of thermal diffusion, and transports virions and virion components, often in association with cellular organelles. This review explores how the analysis of viral trajectories informs about mechanisms of infection. We discuss the methodology enabling researchers to visualize single virions in cells by fluorescence imaging and tracking. Virus visualization and tracking are increasingly enhanced by computational analyses of virus trajectories as well as in silico modeling. Combined approaches reveal previously unrecognized features of virus-infected cells. Using select examples of complementary methodology, we highlight the role of actin filaments and microtubules, and their associated motors in virus infections. In-depth studies of single virion dynamics at high temporal and spatial resolutions thereby provide deep insight into virus infection processes, and are a basis for uncovering underlying mechanisms of how cells function.",2018-03-01,20,1741,92,1164
2293,29687000,Machine Learning in Ultrasound Computer-Aided Diagnostic Systems: A Survey,"The ultrasound imaging is one of the most common schemes to detect diseases in the clinical practice. There are many advantages of ultrasound imaging such as safety, convenience, and low cost. However, reading ultrasound imaging is not easy. To support the diagnosis of clinicians and reduce the load of doctors, many ultrasound computer-aided diagnosis (CAD) systems are proposed. In recent years, the success of deep learning in the image classification and segmentation led to more and more scholars realizing the potential of performance improvement brought by utilizing the deep learning in the ultrasound CAD system. This paper summarized the research which focuses on the ultrasound CAD system utilizing machine learning technology in recent years. This study divided the ultrasound CAD system into two categories. One is the traditional ultrasound CAD system which employed the manmade feature and the other is the deep learning ultrasound CAD system. The major feature and the classifier employed by the traditional ultrasound CAD system are introduced. As for the deep learning ultrasound CAD, newest applications are summarized. This paper will be useful for researchers who focus on the ultrasound CAD system.",2018-03-01,21,1221,74,1164
2358,29431517,Informatics and machine learning to define the phenotype,"For the past decade, the focus of complex disease research has been the genotype. From technological advancements to the development of analysis methods, great progress has been made. However, advances in our definition of the phenotype have remained stagnant. Phenotype characterization has recently emerged as an exciting area of informatics and machine learning. The copious amounts of diverse biomedical data that have been collected may be leveraged with data-driven approaches to elucidate trait-related features and patterns. Areas covered: In this review, the authors discuss the phenotype in traditional genetic associations and the challenges this has imposed.Approaches for phenotype refinement that can aid in more accurate characterization of traits are also discussed. Further, the authors highlight promising machine learning approaches for establishing a phenotype and the challenges of electronic health record (EHR)-derived data. Expert commentary: The authors hypothesize that through unsupervised machine learning, data-driven approaches can be used to define phenotypes rather than relying on expert clinician knowledge. Through the use of machine learning and an unbiased set of features extracted from clinical repositories, researchers will have the potential to further understand complex traits and identify patient subgroups. This knowledge may lead to more preventative and precise clinical care.",2018-03-01,9,1424,56,1164
2373,29396321,Conceptual endophenotypes: A strategy to advance the impact of psychoneuroendocrinology in precision medicine,"Psychobiological research has generated a tremendous amount of findings on the psychological, neuroendocrine, molecular and environmental processes that are directly relevant for mental and physical health, but have overwhelmed our capacity to meaningfully absorb, integrate, and utilize this knowledge base. Here, we reflect about suitable strategies to improve the translational success of psychoneuroendocrinological research in the era of precision medicine. Following a strategy advocated by the National Research Council and the tradition of endophenotype-based research, we advance here a new approach, termed ""conceptual endophenotypes"". We define the contextual and formal criteria of conceptual endophenotypes, outline criteria for filtering and selecting information, and describe how conceptual endophenotypes can be validated and implemented at the bedside. As proof-of-concept, we describe some of our findings from research that has adopted this approach in the context of stress-related disorders. We argue that conceptual endophenotypes engineer a bridge between the bench and the bedside. This approach readily lends itself to being continuously developed and implemented. Recent methodological advances, including digital phenotyping, machine learning, grassroots collaboration, and a learning healthcare system, may accelerate the development and implementation of this conceptual endophenotype approach.",2018-03-01,2,1424,109,1164
296,28902409,Binswanger's disease: biomarkers in the inflammatory form of vascular cognitive impairment and dementia,"Vascular cognitive impairment and dementia (VCID) is a major public health concern because of the increased incidence of vascular disease in the aging population and the impact of vascular disease on Alzheimer's disease. VCID is a heterogeneous group of diseases for which there are no proven treatments. Biomarkers can be used to select more homogeneous populations. Small vessel disease is the most prevalent form of VCID and is the optimal form for treatment trials because there is a progressive course with characteristic pathological changes. Subcortical ischemic vascular disease of the Binswanger type (SIVD-BD) has a characteristic set of features that can be used both to identify patients and to follow treatment. SIVD-BD patients have clinical, neuropsychological, cerebrospinal fluid (CSF) and imaging features that can be used as biomarkers. No one feature is diagnostic, but a multimodal approach defines the SIVD-BD spectrum disorder. The most important features are large white matter lesions with axonal damage, blood-brain barrier disruption as shown by magnetic resonance imaging and CSF, and neuropsychological evidence of executive dysfunction. We have used these features to create a Binswanger Disease Scale and a probability of SIVD-BD, using a machine-learning algorithm. The patients discussed in this review are derived from published studies. Biomarkers not only aid in early diagnosis before the disease process has progressed too far for treatment, but also can indicate response to treatment. Refining the use of biomarkers will allow dementia treatment to enter the era of precision medicine. This article is part of the Special Issue ""Vascular Dementia"".",2018-03-01,10,1688,103,1164
2740,28011753,A review on machine learning principles for multi-view biological data integration,"Driven by high-throughput sequencing techniques, modern genomic and clinical studies are in a strong need of integrative machine learning models for better use of vast volumes of heterogeneous information in the deep understanding of biological systems and the development of predictive models. How data from multiple sources (called multi-view data) are incorporated in a learning system is a key step for successful analysis. In this article, we provide a comprehensive review on omics and clinical data integration techniques, from a machine learning perspective, for various analyses such as prediction, clustering, dimension reduction and association. We shall show that Bayesian models are able to use prior information and model measurements with various distributions; tree-based methods can either build a tree with all features or collectively make a final decision based on trees learned from each view; kernel methods fuse the similarity matrices learned from individual views together for a final similarity matrix or learning model; network-based fusion methods are capable of inferring direct and indirect associations in a heterogeneous network; matrix factorization models have potential to learn interactions among features from different views; and a range of deep neural networks can be integrated in multi-modal learning for capturing the complex mechanism of biological systems.",2018-03-01,75,1400,82,1164
668,29031836,Optofluidic time-stretch quantitative phase microscopy,"Innovations in optical microscopy have opened new windows onto scientific research, industrial quality control, and medical practice over the last few decades. One of such innovations is optofluidic time-stretch quantitative phase microscopy - an emerging method for high-throughput quantitative phase imaging that builds on the interference between temporally stretched signal and reference pulses by using dispersive properties of light in both spatial and temporal domains in an interferometric configuration on a microfluidic platform. It achieves the continuous acquisition of both intensity and phase images with a high throughput of more than 10,000 particles or cells per second by overcoming speed limitations that exist in conventional quantitative phase imaging methods. Applications enabled by such capabilities are versatile and include characterization of cancer cells and microalgal cultures. In this paper, we review the principles and applications of optofluidic time-stretch quantitative phase microscopy and discuss its future perspective.",2018-03-01,3,1058,54,1164
622,29194052,Machine learning in heart failure: ready for prime time,"Purpose of review:                    The aim of this review is to present an up-to-date overview of the application of machine learning methods in heart failure including diagnosis, classification, readmissions and medication adherence.              Recent findings:                    Recent studies have shown that the application of machine learning techniques may have the potential to improve heart failure outcomes and management, including cost savings by improving existing diagnostic and treatment support systems. Recently developed deep learning methods are expected to yield even better performance than traditional machine learning techniques in performing complex tasks by learning the intricate patterns hidden in big medical data.              Summary:                    The review summarizes the recent developments in the application of machine and deep learning methods in heart failure management.",2018-03-01,13,919,55,1164
302,28864356,Lensless digital holographic microscopy and its applications in biomedicine and environmental monitoring,"Optical compound microscope has been a major tool in biomedical imaging for centuries. Its performance relies on relatively complicated, bulky and expensive lenses and alignment mechanics. In contrast, the lensless microscope digitally reconstructs microscopic images of specimens without using any lenses, as a result of which it can be made much smaller, lighter and lower-cost. Furthermore, the limited space-bandwidth product of objective lenses in a conventional microscope can be significantly surpassed by a lensless microscope. Such lensless imaging designs have enabled high-resolution and high-throughput imaging of specimens using compact, portable and cost-effective devices to potentially address various point-of-care, global-health and telemedicine related challenges. In this review, we discuss the operation principles and the methods behind lensless digital holographic on-chip microscopy. We also go over various applications that are enabled by cost-effective and compact implementations of lensless microscopy, including some recent work on air quality monitoring, which utilized machine learning for high-throughput and accurate quantification of particulate matter in air. Finally, we conclude with a brief future outlook of this computational imaging technology.",2018-03-01,12,1286,104,1164
620,29215763,Breast cancer: The translation of big genomic data to cancer precision medicine,"Cancer is a complex genetic disease that develops from the accumulation of genomic alterations in which germline variations predispose individuals to cancer and somatic alterations initiate and trigger the progression of cancer. For the past 2 decades, genomic research has advanced remarkably, evolving from single-gene to whole-genome screening by using genome-wide association study and next-generation sequencing that contributes to big genomic data. International collaborative efforts have contributed to curating these data to identify clinically significant alterations that could be used in clinical settings. Focusing on breast cancer, the present review summarizes the identification of genomic alterations with high-throughput screening as well as the use of genomic information in clinical trials that match cancer patients to therapies, which further leads to cancer precision medicine. Furthermore, cancer screening and monitoring were enhanced greatly by the use of liquid biopsies. With the growing data complexity and size, there is much anticipation in exploiting deep machine learning and artificial intelligence to curate integrative ""-omics"" data to refine the current medical practice to be applied in the near future.",2018-03-01,18,1241,79,1164
600,29324298,Calibration of raw accelerometer data to measure physical activity: A systematic review,"Most of calibration studies based on accelerometry were developed using count-based analyses. In contrast, calibration studies based on raw acceleration signals are relatively recent and their evidences are incipient. The aim of the current study was to systematically review the literature in order to summarize methodological characteristics and results from raw data calibration studies. The review was conducted up to May 2017 using four databases: PubMed, Scopus, SPORTDiscus and Web of Science. Methodological quality of the included studies was evaluated using the Landis and Koch's guidelines. Initially, 1669 titles were identified and, after assessing titles, abstracts and full-articles, 20 studies were included. All studies were conducted in high-income countries, most of them with relatively small samples and specific population groups. Physical activity protocols were different among studies and the indirect calorimetry was the criterion measure mostly used. High mean values of sensitivity, specificity and accuracy from the intensity thresholds of cut-point-based studies were observed (93.7%, 91.9% and 95.8%, respectively). The most frequent statistical approach applied was machine learning-based modelling, in which the mean coefficient of determination was 0.70 to predict physical activity energy expenditure. Regarding the recognition of physical activity types, the mean values of accuracy for sedentary, household and locomotive activities were 82.9%, 55.4% and 89.7%, respectively. In conclusion, considering the construct of physical activity that each approach assesses, linear regression, machine-learning and cut-point-based approaches presented promising validity parameters.",2018-03-01,13,1711,87,1164
316,28751371,Wearable knee health system employing novel physiological biomarkers,"Knee injuries and chronic disorders, such as arthritis, affect millions of Americans, leading to missed workdays and reduced quality of life. Currently, after an initial diagnosis, there are few quantitative technologies available to provide sensitive subclinical feedback to patients regarding improvements or setbacks to their knee health status; instead, most assessments are qualitative, relying on patient-reported symptoms, performance during functional tests, and physical examinations. Recent advances have been made with wearable technologies for assessing the health status of the knee (and potentially other joints) with the goal of facilitating personalized rehabilitation of injuries and care for chronic conditions. This review describes our progress in developing wearable sensing technologies that enable quantitative physiological measurements and interpretation of knee health status. Our sensing system enables longitudinal quantitative measurements of knee sounds, swelling, and activity context during clinical and field situations. Importantly, we leverage machine-learning algorithms to fuse the low-level signal and feature data of the measured time series waveforms into higher level metrics of joint health. This paper summarizes the engineering validation, baseline physiological experiments, and human subject studies-both cross-sectional and longitudinal-that demonstrate the efficacy of using such systems for robust knee joint health assessment. We envision our sensor system complementing and advancing present-day practices to reduce joint reinjury risk, to optimize rehabilitation recovery time for a quicker return to activity, and to reduce health care costs.",2018-03-01,2,1695,68,1164
2372,29398494,Machine Learning in Medical Imaging,"Advances in both imaging and computers have synergistically led to a rapid rise in the potential use of artificial intelligence in various radiological imaging tasks, such as risk assessment, detection, diagnosis, prognosis, and therapy response, as well as in multi-omics disease discovery. A brief overview of the field is given here, allowing the reader to recognize the terminology, the various subfields, and components of machine learning, as well as the clinical potential. Radiomics, an expansion of computer-aided diagnosis, has been defined as the conversion of images to minable data. The ultimate benefit of quantitative radiomics is to (1) yield predictive image-based phenotypes of disease for precision medicine or (2) yield quantitative image-based phenotypes for data mining with other -omics for discovery (ie, imaging genomics). For deep learning in radiology to succeed, note that well-annotated large data sets are needed since deep networks are complex, computer software and hardware are evolving constantly, and subtle differences in disease states are more difficult to perceive than differences in everyday objects. In the future, machine learning in radiology is expected to have a substantial clinical impact with imaging examinations being routinely obtained in clinical practice, providing an opportunity to improve decision support in medical image interpretation. The term of note is decision support, indicating that computers will augment human decision making, making it more effective and efficient. The clinical impact of having computers in the routine clinical practice may allow radiologists to further integrate their knowledge with their clinical colleagues in other medical specialties and allow for precision medicine.",2018-03-01,53,1762,35,1164
658,29055936,Machine learning in laboratory medicine: waiting for the flood?,"This review focuses on machine learning and on how methods and models combining data analytics and artificial intelligence have been applied to laboratory medicine so far. Although still in its infancy, the potential for applying machine learning to laboratory data for both diagnostic and prognostic purposes deserves more attention by the readership of this journal, as well as by physician-scientists who will want to take advantage of this new computer-based support in pathology and laboratory medicine.",2018-03-01,9,508,63,1164
613,29251699,Artificial intelligence in diagnosis of obstructive lung disease: current status and future potential,"Purpose of review:                    The application of artificial intelligence in the diagnosis of obstructive lung diseases is an exciting phenomenon. Artificial intelligence algorithms work by finding patterns in data obtained from diagnostic tests, which can be used to predict clinical outcomes or to detect obstructive phenotypes. The purpose of this review is to describe the latest trends and to discuss the future potential of artificial intelligence in the diagnosis of obstructive lung diseases.              Recent findings:                    Machine learning has been successfully used in automated interpretation of pulmonary function tests for differential diagnosis of obstructive lung diseases. Deep learning models such as convolutional neural network are state-of-the art for obstructive pattern recognition in computed tomography. Machine learning has also been applied in other diagnostic approaches such as forced oscillation test, breath analysis, lung sound analysis and telemedicine with promising results in small-scale studies.              Summary:                    Overall, the application of artificial intelligence has produced encouraging results in the diagnosis of obstructive lung diseases. However, large-scale studies are still required to validate current findings and to boost its adoption by the medical community.",2018-03-01,14,1358,101,1164
630,29175265,Digital image analysis in breast pathology-from image processing techniques to artificial intelligence,"Breast cancer is the most common malignant disease in women worldwide. In recent decades, earlier diagnosis and better adjuvant therapy have substantially improved patient outcome. Diagnosis by histopathology has proven to be instrumental to guide breast cancer treatment, but new challenges have emerged as our increasing understanding of cancer over the years has revealed its complex nature. As patient demand for personalized breast cancer therapy grows, we face an urgent need for more precise biomarker assessment and more accurate histopathologic breast cancer diagnosis to make better therapy decisions. The digitization of pathology data has opened the door to faster, more reproducible, and more precise diagnoses through computerized image analysis. Software to assist diagnostic breast pathology through image processing techniques have been around for years. But recent breakthroughs in artificial intelligence (AI) promise to fundamentally change the way we detect and treat breast cancer in the near future. Machine learning, a subfield of AI that applies statistical methods to learn from data, has seen an explosion of interest in recent years because of its ability to recognize patterns in data with less need for human instruction. One technique in particular, known as deep learning, has produced groundbreaking results in many important problems including image classification and speech recognition. In this review, we will cover the use of AI and deep learning in diagnostic breast pathology, and other recent developments in digital image analysis.",2018-04-01,30,1573,102,1133
619,29216413,Quantum Machine Learning in Chemical Compound Space,"Rather than numerically solving the computationally demanding equations of quantum or statistical mechanics, machine learning methods can infer approximate solutions, interpolating previously acquired property data sets of molecules and materials. The case is made for quantum machine learning: An inductive molecular modeling approach which can be applied to quantum chemistry problems.",2018-04-01,18,387,51,1133
2292,29695070,The Novel Roles of Connexin Channels and Tunneling Nanotubes in Cancer Pathogenesis,"Neoplastic growth and cellular differentiation are critical hallmarks of tumor development. It is well established that cell-to-cell communication between tumor cells and ""normal"" surrounding cells regulates tumor differentiation and proliferation, aggressiveness, and resistance to treatment. Nevertheless, the mechanisms that result in tumor growth and spread as well as the adaptation of healthy surrounding cells to the tumor environment are poorly understood. A major component of these communication systems is composed of connexin (Cx)-containing channels including gap junctions (GJs), tunneling nanotubes (TNTs), and hemichannels (HCs). There are hundreds of reports about the role of Cx-containing channels in the pathogenesis of cancer, and most of them demonstrate a downregulation of these proteins. Nonetheless, new data demonstrate that a localized communication via Cx-containing GJs, HCs, and TNTs plays a key role in tumor growth, differentiation, and resistance to therapies. Moreover, the type and downstream effects of signals communicated between the different populations of tumor cells are still unknown. However, new approaches such as artificial intelligence (AI) and machine learning (ML) could provide new insights into these signals communicated between connected cells. We propose that the identification and characterization of these new communication systems and their associated signaling could provide new targets to prevent or reduce the devastating consequences of cancer.",2018-04-01,16,1508,83,1133
610,29274047,Quantitative Analysis of Uncertainty in Medical Reporting: Creating a Standardized and Objective Methodology,"Uncertainty in text-based medical reports has long been recognized as problematic, frequently resulting in misunderstanding and miscommunication. One strategy for addressing the negative clinical ramifications of report uncertainty would be the creation of a standardized methodology for characterizing and quantifying uncertainty language, which could provide both the report author and reader with context related to the perceived level of diagnostic confidence and accuracy. A number of computerized strategies could be employed in the creation of this analysis including string search, natural language processing and understanding, histogram analysis, topic modeling, and machine learning. The derived uncertainty data offers the potential to objectively analyze report uncertainty in real time and correlate with outcomes analysis for the purpose of context and user-specific decision support at the point of care, where intervention would have the greatest clinical impact.",2018-04-01,2,980,108,1133
607,29288495,PanCancer insights from The Cancer Genome Atlas: the pathologist's perspective,"The Cancer Genome Atlas (TCGA) represents one of several international consortia dedicated to performing comprehensive genomic and epigenomic analyses of selected tumour types to advance our understanding of disease and provide an open-access resource for worldwide cancer research. Thirty-three tumour types (selected by histology or tissue of origin, to include both common and rare diseases), comprising >11 000 specimens, were subjected to DNA sequencing, copy number and methylation analysis, and transcriptomic, proteomic and histological evaluation. Each cancer type was analysed individually to identify tissue-specific alterations, and make correlations across different molecular platforms. The final dataset was then normalized and combined for the PanCancer Initiative, which seeks to identify commonalities across different cancer types or cells of origin/lineage, or within anatomically or morphologically related groups. An important resource generated along with the rich molecular studies is an extensive digital pathology slide archive, composed of frozen section tissue directly related to the tissues analysed as part of TCGA, and representative formalin-fixed paraffin-embedded, haematoxylin and eosin (H&E)-stained diagnostic slides. These H&E image resources have primarily been used to verify diagnoses and histological subtypes with some limited extraction of standard pathological variables such as mitotic activity, grade, and lymphocytic infiltrates. Largely overlooked is the richness of these scanned images for more sophisticated feature extraction approaches coupled with machine learning, and ultimately correlation with molecular features and clinical endpoints. Here, we document initial attempts to exploit TCGA imaging archives, and describe some of the tools, and the rapidly evolving image analysis/feature extraction landscape. Our hope is to inform, and ultimately inspire and challenge, the pathology and cancer research communities to exploit these imaging resources so that the full potential of this integral platform of TCGA can be used to complement and enhance the insightful integrated analyses from the genomic and epigenomic platforms. Copyright  2017 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.",2018-04-01,36,2291,78,1133
2336,29492605,"Machine learning for medical ultrasound: status, methods, and future opportunities","Ultrasound (US) imaging is the most commonly performed cross-sectional diagnostic imaging modality in the practice of medicine. It is low-cost, non-ionizing, portable, and capable of real-time image acquisition and display. US is a rapidly evolving technology with significant challenges and opportunities. Challenges include high inter- and intra-operator variability and limited image quality control. Tremendous opportunities have arisen in the last decade as a result of exponential growth in available computational power coupled with progressive miniaturization of US devices. As US devices become smaller, enhanced computational capability can contribute significantly to decreasing variability through advanced image processing. In this paper, we review leading machine learning (ML) approaches and research directions in US, with an emphasis on recent ML advances. We also present our outlook on future opportunities for ML techniques to further improve clinical workflow and US-based disease diagnosis and characterization.",2018-04-01,12,1033,82,1133
2944,29849612,Aquatic Toxic Analysis by Monitoring Fish Behavior Using Computer Vision: A Recent Progress,"Video tracking based biological early warning system achieved a great progress with advanced computer vision and machine learning methods. Ability of video tracking of multiple biological organisms has been largely improved in recent years. Video based behavioral monitoring has become a common tool for acquiring quantified behavioral data for aquatic risk assessment. Investigation of behavioral responses under chemical and environmental stress has been boosted by rapidly developed machine learning and artificial intelligence. In this paper, we introduce the fundamental of video tracking and present the pioneer works in precise tracking of a group of individuals in 2D and 3D space. Technical and practical issues suffered in video tracking are explained. Subsequently, the toxic analysis based on fish behavioral data is summarized. Frequently used computational methods and machine learning are explained with their applications in aquatic toxicity detection and abnormal pattern analysis. Finally, advantages of recent developed deep learning approach in toxic prediction are presented.",2018-04-01,1,1096,91,1133
597,29331490,Supervised Machine Learning for Population Genetics: A New Paradigm,"As population genomic datasets grow in size, researchers are faced with the daunting task of making sense of a flood of information. To keep pace with this explosion of data, computational methodologies for population genetic inference are rapidly being developed to best utilize genomic sequence data. In this review we discuss a new paradigm that has emerged in computational population genomics: that of supervised machine learning (ML). We review the fundamentals of ML, discuss recent applications of supervised ML to population genetics that outperform competing methods, and describe promising future directions in this area. Ultimately, we argue that supervised ML is an important and underutilized tool that has considerable potential for the world of evolutionary genomics.",2018-04-01,65,783,67,1133
2341,29483348,What is the natural measurement unit of temperament: single traits or profiles?,"There is fundamental doubt about whether the natural unit of measurement for temperament and personality corresponds to single traits or to multi-trait profiles that describe the functioning of a whole person. Biogenetic researchers of temperament usually assume they need to focus on individual traits that differ between individuals. Recent research indicates that a shift of emphasis to understand processes within the individual is crucial for identifying the natural building blocks of temperament. Evolution and development operate on adaptation of whole organisms or persons, not on individual traits or categories. Adaptive functioning generally depends on feedback among many variable processes in ways that are characteristic of complex adaptive systems, not machines with separate parts. Advanced methods of unsupervised machine learning can now be applied to genome-wide association studies and brain imaging in order to uncover the genotypic-phenotypic architecture of traits like temperament, which are strongly influenced by complex interactions, such as genetic epistasis, pleiotropy and gene-environment interactions. We have found that the heritability of temperament can be nearly fully explained by a large number of genetic variants that are unique for multi-trait profiles, not single traits. The implications of this finding for research design and precision medicine are discussed.This article is part of the theme issue 'Diverse perspectives on diversity: multi-disciplinary approaches to taxonomies of individual differences'.",2018-04-01,8,1552,79,1133
2343,29477048,Simulations meet machine learning in structural biology,"Classical molecular dynamics (MD) simulations will be able to reach sampling in the second timescale within five years, producing petabytes of simulation data at current force field accuracy. Notwithstanding this, MD will still be in the regime of low-throughput, high-latency predictions with average accuracy. We envisage that machine learning (ML) will be able to solve both the accuracy and time-to-prediction problem by learning predictive models using expensive simulation data. The synergies between classical, quantum simulations and ML methods, such as artificial neural networks, have the potential to drastically reshape the way we make predictions in computational structural biology and drug discovery.",2018-04-01,4,715,55,1133
611,29261408,"Big Data in Public Health: Terminology, Machine Learning, and Privacy","The digital world is generating data at a staggering and still increasing rate. While these ""big data"" have unlocked novel opportunities to understand public health, they hold still greater potential for research and practice. This review explores several key issues that have arisen around big data. First, we propose a taxonomy of sources of big data to clarify terminology and identify threads common across some subtypes of big data. Next, we consider common public health research and practice uses for big data, including surveillance, hypothesis-generating research, and causal inference, while exploring the role that machine learning may play in each use. We then consider the ethical implications of the big data revolution with particular emphasis on maintaining appropriate care for privacy in a world in which technology is rapidly changing social norms regarding the need for (and even the meaning of) privacy. Finally, we make suggestions regarding structuring teams and training to succeed in working with big data in research and practice.",2018-04-01,34,1056,69,1133
2331,29513400,The Molecular Industrial Revolution: Automated Synthesis of Small Molecules,"Today we are poised for a transition from the highly customized crafting of specific molecular targets by hand to the increasingly general and automated assembly of different types of molecules with the push of a button. Creating machines that are capable of making many different types of small molecules on demand, akin to that which has been achieved on the macroscale with 3D printers, is challenging. Yet important progress is being made toward this objective with two complementary approaches: 1) Automation of customized synthesis routes to different targets by machines that enable the use of many reactions and starting materials, and 2) automation of generalized platforms that make many different targets using common coupling chemistry and building blocks. Continued progress in these directions has the potential to shift the bottleneck in molecular innovation from synthesis to imagination, and thereby help drive a new industrial revolution on the molecular scale.",2018-04-01,20,979,75,1133
313,28807870,"An open, multi-vendor, multi-field-strength brain MR dataset and analysis of publicly available skull stripping methods agreement","This paper presents an open, multi-vendor, multi-field strength magnetic resonance (MR) T1-weighted volumetric brain imaging dataset, named Calgary-Campinas-359 (CC-359). The dataset is composed of images of older healthy adults (29-80 years) acquired on scanners from three vendors (Siemens, Philips and General Electric) at both 1.5 T and 3 T. CC-359 is comprised of 359 datasets, approximately 60 subjects per vendor and magnetic field strength. The dataset is approximately age and gender balanced, subject to the constraints of the available images. It provides consensus brain extraction masks for all volumes generated using supervised classification. Manual segmentation results for twelve randomly selected subjects performed by an expert are also provided. The CC-359 dataset allows investigation of 1) the influences of both vendor and magnetic field strength on quantitative analysis of brain MR; 2) parameter optimization for automatic segmentation methods; and potentially 3) machine learning classifiers with big data, specifically those based on deep learning methods, as these approaches require a large amount of data. To illustrate the utility of this dataset, we compared to the results of a supervised classifier, the results of eight publicly available skull stripping methods and one publicly available consensus algorithm. A linear mixed effects model analysis indicated that vendor (p-value<0.001) and magnetic field strength (p-value<0.001) have statistically significant impacts on skull stripping results.",2018-04-01,7,1533,129,1133
2328,29517418,SNPs affecting the clinical outcomes of regularly used immunosuppressants,"Recent studies have suggested that genomic diversity may play a key role in different clinical outcomes, and the importance of SNPs is becoming increasingly clear. In this article, we summarize the bioactivity of SNPs that may affect the sensitivity to or possibility of drug reactions that occur among the signaling pathways of regularly used immunosuppressants, such as glucocorticoids, azathioprine, tacrolimus, mycophenolate mofetil, cyclophosphamide and methotrexate. The development of bioinformatics, including machine learning models, has enabled prediction of the proper immunosuppressant dosage with minimal adverse drug reactions for patients after organ transplantation or for those with autoimmune diseases. This article provides a theoretical basis for the personalized use of immunosuppressants in the future.",2018-04-01,3,824,73,1133
2920,29904616,Artificial Intelligence in Medicine and Radiation Oncology,Artifical Intelligence (AI) was reviewed with a focus on its potential applicability to radiation oncology. The improvement of process efficiencies and the prevention of errors were found to be the most significant contributions of AI to radiation oncology. It was found that the prevention of errors is most effective when data transfer processes were automated and operational decisions were based on logical or learned evaluations by the system. It was concluded that AI could greatly improve the efficiency and accuracy of radiation oncology operations.,2018-04-01,3,557,58,1133
2389,29352978,Use of multimodality imaging and artificial intelligence for diagnosis and prognosis of early stages of Alzheimer's disease,"Alzheimer's disease (AD) is a major neurodegenerative disease and the most common cause of dementia. Currently, no treatment exists to slow down or stop the progression of AD. There is converging belief that disease-modifying treatments should focus on early stages of the disease, that is, the mild cognitive impairment (MCI) and preclinical stages. Making a diagnosis of AD and offering a prognosis (likelihood of converting to AD) at these early stages are challenging tasks but possible with the help of multimodality imaging, such as magnetic resonance imaging (MRI), fluorodeoxyglucose (FDG)-positron emission topography (PET), amyloid-PET, and recently introduced tau-PET, which provides different but complementary information. This article is a focused review of existing research in the recent decade that used statistical machine learning and artificial intelligence methods to perform quantitative analysis of multimodality image data for diagnosis and prognosis of AD at the MCI or preclinical stages. We review the existing work in 3 subareas: diagnosis, prognosis, and methods for handling modality-wise missing data-a commonly encountered problem when using multimodality imaging for prediction or classification. Factors contributing to missing data include lack of imaging equipment, cost, difficulty of obtaining patient consent, and patient drop-off (in longitudinal studies). Finally, we summarize our major findings and provide some recommendations for potential future research directions.",2018-04-01,12,1512,123,1133
2370,29405289,Role of artificial intelligence in the care of patients with nonsmall cell lung cancer,"Background:                    Lung cancer is the leading cause of cancer death worldwide. In up to 57% of patients, it is diagnosed at an advanced stage and the 5-year survival rate ranges between 10%-16%. There has been a significant amount of research using machine learning to generate tools using patient data to improve outcomes.              Methods:                    This narrative review is based on research material obtained from PubMed up to Nov 2017. The search terms include ""artificial intelligence,"" ""machine learning,"" ""lung cancer,"" ""Nonsmall Cell Lung Cancer (NSCLC),"" ""diagnosis"" and ""treatment.""              Results:                    Recent studies support the use of computer-aided systems and the use of radiomic features to help diagnose lung cancer earlier. Other studies have looked at machine learning (ML) methods that offer prognostic tools to doctors and help them in choosing personalized treatment options for their patients based on molecular, genetics and histological features. Combining artificial intelligence approaches into health care may serve as a beneficial tool for patients with NSCLC, and this review outlines these benefits and current shortcomings throughout the continuum of care.              Conclusion:                    We present a review of the various applications of ML methods in NSCLC as it relates to improving diagnosis, treatment and outcomes.",2018-04-01,6,1411,86,1133
2983,29700073,Biomedical Informatics on the Cloud: A Treasure Hunt for Advancing Cardiovascular Medicine,"In the digital age of cardiovascular medicine, the rate of biomedical discovery can be greatly accelerated by the guidance and resources required to unearth potential collections of knowledge. A unified computational platform leverages metadata to not only provide direction but also empower researchers to mine a wealth of biomedical information and forge novel mechanistic insights. This review takes the opportunity to present an overview of the cloud-based computational environment, including the functional roles of metadata, the architecture schema of indexing and search, and the practical scenarios of machine learning-supported molecular signature extraction. By introducing several established resources and state-of-the-art workflows, we share with our readers a broadly defined informatics framework to phenotype cardiovascular health and disease.",2018-04-01,8,860,90,1133
2388,29360430,Image analysis and machine learning for detecting malaria,"Malaria remains a major burden on global health, with roughly 200 million cases worldwide and more than 400,000 deaths per year. Besides biomedical research and political efforts, modern information technology is playing a key role in many attempts at fighting the disease. One of the barriers toward a successful mortality reduction has been inadequate malaria diagnosis in particular. To improve diagnosis, image analysis software and machine learning methods have been used to quantify parasitemia in microscopic blood slides. This article gives an overview of these techniques and discusses the current developments in image analysis and machine learning for microscopic malaria diagnosis. We organize the different approaches published in the literature according to the techniques used for imaging, image preprocessing, parasite detection and cell segmentation, feature computation, and automatic cell classification. Readers will find the different techniques listed in tables, with the relevant articles cited next to them, for both thin and thick blood smear images. We also discussed the latest developments in sections devoted to deep learning and smartphone technology for future malaria diagnosis.",2018-04-01,28,1210,57,1133
2921,29900088,Improving the Standard for Deep Brain Stimulation Therapy: Target Structures and Feedback Signals for Adaptive Stimulation. Current Perspectives and Future Directions,"Deep brain stimulation (DBS) is an established therapeutic option for the treatment of various neurological disorders and has been used successfully in movement disorders for over 25 years. However, the standard stimulation schemes have not changed substantially. Two major points of interest for the further development of DBS are target-structures and novel adaptive stimulation techniques integrating feedback signals. We describe recent research results on target structures and on neural and behavioural feedback signals for adaptive deep brain stimulation (aDBS), as well as outline future directions.",2018-04-01,2,607,166,1133
2731,28060710,A Review on Methods for Detecting SNP Interactions in High-Dimensional Genomic Data,"In this era of genome-wide association studies (GWAS), the quest for understanding the genetic architecture of complex diseases is rapidly increasing more than ever before. The development of high throughput genotyping and next generation sequencing technologies enables genetic epidemiological analysis of large scale data. These advances have led to the identification of a number of single nucleotide polymorphisms (SNPs) responsible for disease susceptibility. The interactions between SNPs associated with complex diseases are increasingly being explored in the current literature. These interaction studies are mathematically challenging and computationally complex. These challenges have been addressed by a number of data mining and machine learning approaches. This paper reviews the current methods and the related software packages to detect the SNP interactions that contribute to diseases. The issues that need to be considered when developing these models are addressed in this review. The paper also reviews the achievements in data simulation to evaluate the performance of these models. Further, it discusses the future of SNP interaction analysis.",2018-04-01,5,1165,83,1133
2351,29445894,Sparse QSAR modelling methods for therapeutic and regenerative medicine,"The quantitative structure-activity relationships method was popularized by Hansch and Fujita over 50 years ago. The usefulness of the method for drug design and development has been shown in the intervening years. As it was developed initially to elucidate which molecular properties modulated the relative potency of putative agrochemicals, and at a time when computing resources were scarce, there is much scope for applying modern mathematical methods to improve the QSAR method and to extending the general concept to the discovery and optimization of bioactive molecules and materials more broadly. I describe research over the past two decades where we have rebuilt the unit operations of the QSAR method using improved mathematical techniques, and have applied this valuable platform technology to new important areas of research and industry such as nanoscience, omics technologies, advanced materials, and regenerative medicine. This paper was presented as the 2017 ACS Herman Skolnik lecture.",2018-04-01,1,1003,71,1133
2502,30627231,Radiomics - the value of the numbers in present and future radiology,"Radiomics is a new concept that has been functioning in medicine for only a few years. This idea, created recently, relies on processing innumerable quantities of metadata acquired from every examination, followed by extraction thereof from relevant imaging examinations, such as computer tomography (CT), magnetic resonance imaging (MRI), or positron emission tomography (PET) images, by means of appropriate created algorithms. The extracted results have great potential and broad possibilities of application. Thanks to these we can verify efficiency of treatment, predict locations of metastases of tumours, correlate results with histopathological examinations, or define the type of cancer more precisely. In effect, we obtain more personalised treatment for each patient, which is extremely important and highly recommendable in the tests and applicable treatment therapies conducted nowadays. Radiomics is a non-invasive and high efficiency post-processing method. This article is intended to explain the idea of radiomics, the mechanisms of data acquisition, existing possibilities, and the challenges incurred by radiologists and physicians at the stage of making diagnosis or conducting treatment.",2018-04-01,1,1208,68,1133
2349,29452923,Statistical and machine learning approaches to predicting protein-ligand interactions,"Data driven computational approaches to predicting protein-ligand binding are currently achieving unprecedented levels of accuracy on held-out test datasets. Up until now, however, this has not led to corresponding breakthroughs in our ability to design novel ligands for protein targets of interest. This review summarizes the current state of the art in this field, emphasizing the recent development of deep neural networks for predicting protein-ligand binding. We explain the major technical challenges that have caused difficulty with predicting novel ligands, including the problems of sampling noise and the challenge of using benchmark datasets that are sufficiently unbiased that they allow the model to extrapolate to new regimes.",2018-04-01,8,741,85,1133
2963,29770293,Current monitoring and innovative predictive modeling to improve care in the pediatric cardiac intensive care unit,"The objectives of this review are (I) to describe the challenges associated with monitoring patients in the pediatric cardiac intensive care unit (PCICU) and (II) to discuss the use of innovative statistical and artificial intelligence (AI) software programs to attempt to predict significant clinical events. Patients cared for in the PCICU are clinically fragile and at risk for fatal decompensation. Current monitoring modalities are often ineffective, sometimes inaccurate, and fail to detect a deteriorating clinical status in a timely manner. Predictive models created by AI and machine learning may lead to earlier detection of patients at risk for clinical decompensation and thereby improve care for critically ill pediatric cardiac patients.",2018-04-01,3,751,114,1133
689,28954825,Deep learning guided stroke management: a review of clinical applications,"Stroke is a leading cause of long-term disability, and outcome is directly related to timely intervention. Not all patients benefit from rapid intervention, however. Thus a significant amount of attention has been paid to using neuroimaging to assess potential benefit by identifying areas of ischemia that have not yet experienced cellular death. The perfusion-diffusion mismatch, is used as a simple metric for potential benefit with timely intervention, yet penumbral patterns provide an inaccurate predictor of clinical outcome. Machine learning research in the form of deep learning (artificial intelligence) techniques using deep neural networks (DNNs) excel at working with complex inputs. The key areas where deep learning may be imminently applied to stroke management are image segmentation, automated featurization (radiomics), and multimodal prognostication. The application of convolutional neural networks, the family of DNN architectures designed to work with images, to stroke imaging data is a perfect match between a mature deep learning technique and a data type that is naturally suited to benefit from deep learning's strengths. These powerful tools have opened up exciting opportunities for data-driven stroke management for acute intervention and for guiding prognosis. Deep learning techniques are useful for the speed and power of results they can deliver and will become an increasingly standard tool in the modern stroke specialist's arsenal for delivering personalized medicine to patients with ischemic stroke.",2018-04-01,20,1539,73,1133
2299,29662559,Deep Learning in Nuclear Medicine and Molecular Imaging: Current Perspectives and Future Directions,"Recent advances in deep learning have impacted various scientific and industrial fields. Due to the rapid application of deep learning in biomedical data, molecular imaging has also started to adopt this technique. In this regard, it is expected that deep learning will potentially affect the roles of molecular imaging experts as well as clinical decision making. This review firstly offers a basic overview of deep learning particularly for image data analysis to give knowledge to nuclear medicine physicians and researchers. Because of the unique characteristics and distinctive aims of various types of molecular imaging, deep learning applications can be different from other fields. In this context, the review deals with current perspectives of deep learning in molecular imaging particularly in terms of development of biomarkers. Finally, future challenges of deep learning application for molecular imaging and future roles of experts in molecular imaging will be discussed.",2018-04-01,13,985,99,1133
2298,29670029,Concepts in Light Microscopy of Viruses,"Viruses threaten humans, livestock, and plants, and are difficult to combat. Imaging of viruses by light microscopy is key to uncover the nature of known and emerging viruses in the quest for finding new ways to treat viral disease and deepening the understanding of virus&ndash;host interactions. Here, we provide an overview of recent technology for imaging cells and viruses by light microscopy, in particular fluorescence microscopy in static and live-cell modes. The review lays out guidelines for how novel fluorescent chemical probes and proteins can be used in light microscopy to illuminate cells, and how they can be used to study virus infections. We discuss advantages and opportunities of confocal and multi-photon microscopy, selective plane illumination microscopy, and super-resolution microscopy. We emphasize the prevalent concepts in image processing and data analyses, and provide an outlook into label-free digital holographic microscopy for virus research.",2018-04-01,7,978,39,1133
2326,29528822,"Multimedia-enhanced Radiology Reports: Concept, Components, and Challenges","Multimedia-enhanced radiology report (MERR) development is defined and described from an informatics perspective, in which the MERR is seen as a superior information-communicating entity. Recent technical advances, such as the hyperlinking of report text directly to annotated images, improve MERR information content and accessibility compared with text-only reports. The MERR is analyzed by its components, which include hypertext, tables, graphs, embedded images, and their interconnections. The authors highlight the advantages of each component for improving the radiologist's communication of report content information and the user's ability to extract information. Requirements for MERR implementation (eg, integration of picture archiving and communication systems, radiology information systems, and electronic medical record systems) and the authors' initial experiences and challenges in MERR implementation at the National Institutes of Health are reviewed. The transition to MERRs has provided advantages over use of traditional text-only radiology reports because of the capacity to include hyperlinked report text that directs clinicians to image annotations, images, tables, and graphs. A framework is provided for thinking about the MERR from the user's perspective. Additional applications of emerging technologies (eg, artificial intelligence and machine learning) are described in the crafting of what the authors believe is the radiology report of the future. RSNA, 2018.",2018-04-01,4,1494,74,1133
2319,29555807,Blessing of dimensionality: mathematical foundations of the statistical physics of data,"The concentrations of measure phenomena were discovered as the mathematical background to statistical mechanics at the end of the nineteenth/beginning of the twentieth century and have been explored in mathematics ever since. At the beginning of the twenty-first century, it became clear that the proper utilization of these phenomena in machine learning might transform the curse of dimensionality into the blessing of dimensionality This paper summarizes recently discovered phenomena of measure concentration which drastically simplify some machine learning problems in high dimension, and allow us to correct legacy artificial intelligence systems. The classical concentration of measure theorems state that i.i.d. random points are concentrated in a thin layer near a surface (a sphere or equators of a sphere, an average or median-level set of energy or another Lipschitz function, etc.). The new stochastic separation theorems describe the thin structure of these thin layers: the random points are not only concentrated in a thin layer but are all linearly separable from the rest of the set, even for exponentially large random sets. The linear functionals for separation of points can be selected in the form of the linear Fisher's discriminant. All artificial intelligence systems make errors. Non-destructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behaviour by a simple and robust classifier. The stochastic separation theorems provide us with such classifiers and determine a non-iterative (one-shot) procedure for their construction.This article is part of the theme issue 'Hilbert's sixth problem'.",2018-04-01,9,1687,87,1133
2976,29728226,Radiomics in radiooncology - Challenging the medical physicist,"Purpose:                    Noticing the fast growing translation of artificial intelligence (AI) technologies to medical image analysis this paper emphasizes the future role of the medical physicist in this evolving field. Specific challenges are addressed when implementing big data concepts with high-throughput image data processing like radiomics and machine learning in a radiooncology environment to support clinical decisions.              Methods:                    Based on the experience of our interdisciplinary radiomics working group, techniques for processing minable data, extracting radiomics features and associating this information with clinical, physical and biological data for the development of prediction models are described. A special emphasis was placed on the potential clinical significance of such an approach.              Results:                    Clinical studies demonstrate the role of radiomics analysis as an additional independent source of information with the potential to influence the radiooncology practice, i.e. to predict patient prognosis, treatment response and underlying genetic changes. Extending the radiomics approach to integrate imaging, clinical, genetic and dosimetric data ('panomics') challenges the medical physicist as member of the radiooncology team.              Conclusions:                    The new field of big data processing in radiooncology offers opportunities to support clinical decisions, to improve predicting treatment outcome and to stimulate fundamental research on radiation response both of tumor and normal tissue. The integration of physical data (e.g. treatment planning, dosimetric, image guidance data) demands an involvement of the medical physicist in the radiomics approach of radiooncology. To cope with this challenge national and international organizations for medical physics should organize more training opportunities in artificial intelligence technologies in radiooncology.",2018-04-01,19,1975,62,1133
393,28341746,Leveraging sequence-based faecal microbial community survey data to identify a composite biomarker for colorectal cancer,"Objective:                    Colorectal cancer (CRC) is the second leading cause of cancer-associated mortality in the USA. The faecal microbiome may provide non-invasive biomarkers of CRC and indicate transition in the adenoma-carcinoma sequence. Re-analysing raw sequence and metadata from several studies uniformly, we sought to identify a composite and generalisable microbial marker for CRC.              Design:                    Raw 16S rRNA gene sequence data sets from nine studies were processed with two pipelines, (1) QIIME closed reference (QIIME-CR) or (2) a strain-specific method herein termed SS-UP (Strain Select, UPARSE bioinformatics pipeline). A total of 509 samples (79 colorectal adenoma, 195 CRC and 235 controls) were analysed. Differential abundance, meta-analysis random effects regression and machine learning analyses were carried out to determine the consistency and diagnostic capabilities of potential microbial biomarkers.              Results:                    Definitive taxa, including Parvimonas micra ATCC 33270, Streptococcus anginosus and yet-to-be-cultured members of Proteobacteria, were frequently and significantly increased in stools from patients with CRC compared with controls across studies and had high discriminatory capacity in diagnostic classification. Microbiome-based CRC versus control classification produced an area under receiver operator characteristic (AUROC) curve of 76.6% in QIIME-CR and 80.3% in SS-UP. Combining clinical and microbiome markers gave a diagnostic AUROC of 83.3% for QIIME-CR and 91.3% for SS-UP.              Conclusions:                    Despite technological differences across studies and methods, key microbial markers emerged as important in classifying CRC cases and such could be used in a universal diagnostic for the disease. The choice of bioinformatics pipeline influenced accuracy of classification. Strain-resolved microbial markers might prove crucial in providing a microbial diagnostic for CRC.",2018-05-01,51,1998,120,1103
323,28715343,Deep Belief Networks for Electroencephalography: A Review of Recent Contributions and Future Outlooks,"Deep learning, a relatively new branch of machine learning, has been investigated for use in a variety of biomedical applications. Deep learning algorithms have been used to analyze different physiological signals and gain a better understanding of human physiology for automated diagnosis of abnormal conditions. In this paper, we provide an overview of deep learning approaches with a focus on deep belief networks in electroencephalography applications. We investigate the state-of-the-art algorithms for deep belief networks and then cover the application of these algorithms and their performances in electroencephalographic applications. We covered various applications of electroencephalography in medicine, including emotion recognition, sleep stage classification, and seizure detection, in order to understand how deep learning algorithms could be modified to better suit the tasks desired. This review is intended to provide researchers with a broad overview of the currently existing deep belief network methodology for electroencephalography signals, as well as to highlight potential challenges for future research.",2018-05-01,9,1129,101,1103
2938,29861451,Bio-Signal Complexity Analysis in Epileptic Seizure Monitoring: A Topic Review,"Complexity science has provided new perspectives and opportunities for understanding a variety of complex natural or social phenomena, including brain dysfunctions like epilepsy. By delving into the complexity in electrophysiological signals and neuroimaging, new insights have emerged. These discoveries have revealed that complexity is a fundamental aspect of physiological processes. The inherent nonlinearity and non-stationarity of physiological processes limits the methods based on simpler underlying assumptions to point out the pathway to a more comprehensive understanding of their behavior and relation with certain diseases. The perspective of complexity may benefit both the research and clinical practice through providing novel data analytics tools devoted for the understanding of and the intervention about epilepsies. This review aims to provide a sketchy overview of the methods derived from different disciplines lucubrating to the complexity of bio-signals in the field of epilepsy monitoring. Although the complexity of bio-signals is still not fully understood, bundles of new insights have been already obtained. Despite the promising results about epileptic seizure detection and prediction through offline analysis, we are still lacking robust, tried-and-true real-time applications. Multidisciplinary collaborations and more high-quality data accessible to the whole community are needed for reproducible research and the development of such applications.",2018-05-01,2,1482,78,1103
2937,29867348,Critical Issues in BDNF Val66Met Genetic Studies of Neuropsychiatric Disorders,"Neurotrophins have been implicated in the pathophysiology of many neuropsychiatric diseases. Brain-derived neurotrophic factor (BDNF) is the most abundant and widely distributed neurotrophin in the brain. Its Val66Met polymorphism (refSNP Cluster Report: rs6265) is a common and functional single-nucleotide polymorphism (SNP) affecting the activity-dependent release of BDNF. BDNF Val66Met transgenic mice have been generated, which may provide further insight into the functional impact of this polymorphism in the brain. Considering the important role of BDNF in brain function, more than 1,100 genetic studies have investigated this polymorphism in the past 15 years. Although these studies have reported some encouraging positive findings initially, most of the findings cannot be replicated in following studies. These inconsistencies in BDNF Val66Met genetic studies may be attributed to many factors such as age, sex, environmental factors, ethnicity, genetic model used for analysis, and gene-gene interaction, which are discussed in this review. We also discuss the results of recent studies that have reported the novel functions of this polymorphism. Because many BDNF polymorphisms and non-genetic factors have been implicated in the complex traits of neuropsychiatric diseases, the conventional genetic association-based method is limited to address these complex interactions. Future studies should apply data mining and machine learning techniques to determine the genetic role of BDNF in neuropsychiatric diseases.",2018-05-01,17,1531,78,1103
2302,29655580,Canadian Association of Radiologists White Paper on Artificial Intelligence in Radiology,"Artificial intelligence (AI) is rapidly moving from an experimental phase to an implementation phase in many fields, including medicine. The combination of improved availability of large datasets, increasing computing power, and advances in learning algorithms has created major performance breakthroughs in the development of AI applications. In the last 5 years, AI techniques known as deep learning have delivered rapidly improving performance in image recognition, caption generation, and speech recognition. Radiology, in particular, is a prime candidate for early adoption of these techniques. It is anticipated that the implementation of AI in radiology over the next decade will significantly improve the quality, value, and depth of radiology's contribution to patient care and population health, and will revolutionize radiologists' workflows. The Canadian Association of Radiologists (CAR) is the national voice of radiology committed to promoting the highest standards in patient-centered imaging, lifelong learning, and research. The CAR has created an AI working group with the mandate to discuss and deliberate on practice, policy, and patient care issues related to the introduction and implementation of AI in imaging. This white paper provides recommendations for the CAR derived from deliberations between members of the AI working group. This white paper on AI in radiology will inform CAR members and policymakers on key terminology, educational needs of members, research and development, partnerships, potential clinical applications, implementation, structure and governance, role of radiologists, and potential impact of AI on radiology in Canada.",2018-05-01,41,1672,88,1103
2303,29648622,Deep learning of genomic variation and regulatory network data,"The human genome is now investigated through high-throughput functional assays, and through the generation of population genomic data. These advances support the identification of functional genetic variants and the prediction of traits (e.g. deleterious variants and disease). This review summarizes lessons learned from the large-scale analyses of genome and exome data sets, modeling of population data and machine-learning strategies to solve complex genomic sequence regions. The review also portrays the rapid adoption of artificial intelligence/deep neural networks in genomics; in particular, deep learning approaches are well suited to model the complex dependencies in the regulatory landscape of the genome, and to provide predictors for genetic variant calling and interpretation.",2018-05-01,11,792,62,1103
2322,29544791,"Blood vessel segmentation algorithms - Review of methods, datasets and evaluation metrics","Background:                    Blood vessel segmentation is a topic of high interest in medical image analysis since the analysis of vessels is crucial for diagnosis, treatment planning and execution, and evaluation of clinical outcomes in different fields, including laryngology, neurosurgery and ophthalmology. Automatic or semi-automatic vessel segmentation can support clinicians in performing these tasks. Different medical imaging techniques are currently used in clinical practice and an appropriate choice of the segmentation algorithm is mandatory to deal with the adopted imaging technique characteristics (e.g. resolution, noise and vessel contrast).              Objective:                    This paper aims at reviewing the most recent and innovative blood vessel segmentation algorithms. Among the algorithms and approaches considered, we deeply investigated the most novel blood vessel segmentation including machine learning, deformable model, and tracking-based approaches.              Methods:                    This paper analyzes more than 100 articles focused on blood vessel segmentation methods. For each analyzed approach, summary tables are presented reporting imaging technique used, anatomical region and performance measures employed. Benefits and disadvantages of each method are highlighted.              Discussion:                    Despite the constant progress and efforts addressed in the field, several issues still need to be overcome. A relevant limitation consists in the segmentation of pathological vessels. Unfortunately, not consistent research effort has been addressed to this issue yet. Research is needed since some of the main assumptions made for healthy vessels (such as linearity and circular cross-section) do not hold in pathological tissues, which on the other hand require new vessel model formulations. Moreover, image intensity drops, noise and low contrast still represent an important obstacle for the achievement of a high-quality enhancement. This is particularly true for optical imaging, where the image quality is usually lower in terms of noise and contrast with respect to magnetic resonance and computer tomography angiography.              Conclusion:                    No single segmentation approach is suitable for all the different anatomical region or imaging modalities, thus the primary goal of this review was to provide an up to date source of information about the state of the art of the vessel segmentation algorithms so that the most suitable methods can be chosen according to the specific task.",2018-05-01,32,2582,89,1103
2304,29637384,Stacked generalization: an introduction to super learning,"Stacked generalization is an ensemble method that allows researchers to combine several different prediction algorithms into one. Since its introduction in the early 1990s, the method has evolved several times into a host of methods among which is the ""Super Learner"". Super Learner uses V-fold cross-validation to build the optimal weighted combination of predictions from a library of candidate algorithms. Optimality is defined by a user-specified objective function, such as minimizing mean squared error or maximizing the area under the receiver operating characteristic curve. Although relatively simple in nature, use of Super Learner by epidemiologists has been hampered by limitations in understanding conceptual and technical details. We work step-by-step through two examples to illustrate concepts and address common concerns.",2018-05-01,12,838,57,1103
2320,29551544,Radiogenomics and IR,"Radiogenomics involves the integration of mineable data from imaging phenotypes with genomic and clinical data to establish predictive models using machine learning. As a noninvasive surrogate for a tumor's in vivo genetic profile, radiogenomics may potentially provide data for patient treatment stratification. Radiogenomics may also supersede the shortcomings associated with genomic research, such as the limited availability of high-quality tissue and restricted sampling of tumoral subpopulations. Interventional radiologists are well suited to circumvent these obstacles through advancements in image-guided tissue biopsies and intraprocedural imaging. Comprehensive understanding of the radiogenomic process is crucial for interventional radiologists to contribute to this evolving field.",2018-05-01,3,796,20,1103
2318,29566172,Biomedical informatics and machine learning for clinical genomics,"While tens of thousands of pathogenic variants are used to inform the many clinical applications of genomics, there remains limited information on quantitative disease risk for the majority of variants used in clinical practice. At the same time, rising demand for genetic counselling has prompted a growing need for computational approaches that can help interpret genetic variation. Such tasks include predicting variant pathogenicity and identifying variants that are too common to be penetrant. To address these challenges, researchers are increasingly turning to integrative informatics approaches. These approaches often leverage vast sources of data, including electronic health records and population-level allele frequency databases (e.g. gnomAD), as well as machine learning techniques such as support vector machines and deep learning. In this review, we highlight recent informatics and machine learning approaches that are improving our understanding of pathogenic variation and discuss obstacles that may limit their emerging role in clinical genomics.",2018-05-01,4,1066,65,1103
2945,29848472,Artificial Intelligence for Diabetes Management and Decision Support: Literature Review,"Background:                    Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis.              Objective:                    The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges.              Methods:                    A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review.              Results:                    We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results.              Conclusions:                    We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients' quality of life.",2018-05-01,33,1748,87,1103
457,30310652,Big-data and machine learning to revamp computational toxicology and its use in risk assessment,"The creation of large toxicological databases and advances in machine-learning techniques have empowered computational approaches in toxicology. Work with these large databases based on regulatory data has allowed reproducibility assessment of animal models, which highlight weaknesses in traditional in vivo methods. This should lower the bars for the introduction of new approaches and represents a benchmark that is achievable for any alternative method validated against these methods. Quantitative Structure Activity Relationships (QSAR) models for skin sensitization, eye irritation, and other human health hazards based on these big databases, however, also have made apparent some of the challenges facing computational modeling, including validation challenges, model interpretation issues, and model selection issues. A first implementation of machine learning-based predictions termed REACHacross achieved unprecedented sensitivities of >80% with specificities >70% in predicting the six most common acute and topical hazards covering about two thirds of the chemical universe. While this is awaiting formal validation, it demonstrates the new quality introduced by big data and modern data-mining technologies. The rapid increase in the diversity and number of computational models, as well as the data they are based on, create challenges and opportunities for the use of computational methods.",2018-05-01,9,1407,95,1103
2985,29697304,Deep learning in pharmacogenomics: from gene regulation to patient stratification,"This Perspective provides examples of current and future applications of deep learning in pharmacogenomics, including: identification of novel regulatory variants located in noncoding domains of the genome and their function as applied to pharmacoepigenomics; patient stratification from medical records; and the mechanistic prediction of drug response, targets and their interactions. Deep learning encapsulates a family of machine learning algorithms that has transformed many important subfields of artificial intelligence over the last decade, and has demonstrated breakthrough performance improvements on a wide range of tasks in biomedicine. We anticipate that in the future, deep learning will be widely used to predict personalized drug response and optimize medication selection and dosing, using knowledge extracted from large and complex molecular, epidemiological, clinical and demographic datasets.",2018-05-01,17,911,81,1103
2978,29725101,Core microbiomes for sustainable agroecosystems,"In an era of ecosystem degradation and climate change, maximizing microbial functions in agroecosystems has become a prerequisite for the future of global agriculture. However, managing species-rich communities of plant-associated microbiomes remains a major challenge. Here, we propose interdisciplinary research strategies to optimize microbiome functions in agroecosystems. Informatics now allows us to identify members and characteristics of 'core microbiomes', which may be deployed to organize otherwise uncontrollable dynamics of resident microbiomes. Integration of microfluidics, robotics and machine learning provides novel ways to capitalize on core microbiomes for increasing resource-efficiency and stress-resistance of agroecosystems.",2018-05-01,66,748,47,1103
2970,29749590,"Imaging, Health Record, and Artificial Intelligence: Hype or Hope?","Purpose of review:                    The review is focused on ""digital health"", which means advanced analytics based on multi-modal data. The ""Health Care Internet of Things"", which uses sensors, apps, and remote monitoring could provide continuous clinical information in the cloud that enables clinicians to access the information they need to care for patients everywhere. Greater standardization of acquisition protocols will be needed to maximize the potential gains from automation and machine learning.              Recent findings:                    Recent artificial intelligence applications on cardiac imaging will not be diagnosing patients and replacing doctors but will be augmenting their ability to find key relevant data they need to care for a patient and present it in a concise, easily digestible format. Risk stratification will transition from oversimplified population-based risk scores to machine learning-based metrics incorporating a large number of patient-specific clinical and imaging variables in real-time beyond the limits of human cognition. This will deliver highly accurate and individual personalized risk assessments and facilitate tailored management plans.",2018-05-01,6,1197,66,1103
2371,29401044,Machine Learning Approaches for Clinical Psychology and Psychiatry,"Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined.",2018-05-01,77,1025,66,1103
2964,29769297,"Mechanistic models versus machine learning, a fight worth fighting for the biological community?","Ninety per cent of the world's data have been generated in the last 5 years (Machine learning: the power and promise of computers that learn by example Report no. DES4702. Issued April 2017. Royal Society). A small fraction of these data is collected with the aim of validating specific hypotheses. These studies are led by the development of mechanistic models focused on the causality of input-output relationships. However, the vast majority is aimed at supporting statistical or correlation studies that bypass the need for causality and focus exclusively on prediction. Along these lines, there has been a vast increase in the use of machine learning models, in particular in the biomedical and clinical sciences, to try and keep pace with the rate of data generation. Recent successes now beg the question of whether mechanistic models are still relevant in this area. Said otherwise, why should we try to understand the mechanisms of disease progression when we can use machine learning tools to directly predict disease outcome?",2018-05-01,23,1036,96,1103
2958,29786659,Deep Learning to Predict Falls in Older Adults Based on Daily-Life Trunk Accelerometry,"Early detection of high fall risk is an essential component of fall prevention in older adults. Wearable sensors can provide valuable insight into daily-life activities; biomechanical features extracted from such inertial data have been shown to be of added value for the assessment of fall risk. Body-worn sensors such as accelerometers can provide valuable insight into fall risk. Currently, biomechanical features derived from accelerometer data are used for the assessment of fall risk. Here, we studied whether deep learning methods from machine learning are suited to automatically derive features from raw accelerometer data that assess fall risk. We used an existing dataset of 296 older adults. We compared the performance of three deep learning model architectures (convolutional neural network (CNN), long short-term memory (LSTM) and a combination of these two (ConvLSTM)) to each other and to a baseline model with biomechanical features on the same dataset. The results show that the deep learning models in a single-task learning mode are strong in recognition of identity of the subject, but that these models only slightly outperform the baseline method on fall risk assessment. When using multi-task learning, with gender and age as auxiliary tasks, deep learning models perform better. We also found that preprocessing of the data resulted in the best performance (AUC = 0.75). We conclude that deep learning models, and in particular multi-task learning, effectively assess fall risk on the basis of wearable sensor data.",2018-05-01,11,1541,86,1103
2973,29741258,Image processing and machine learning in the morphological analysis of blood cells,"Introduction:                    This review focuses on how image processing and machine learning can be useful for the morphological characterization and automatic recognition of cell images captured from peripheral blood smears.              Methods:                    The basics of the 3 core elements (segmentation, quantitative features, and classification) are outlined, and recent literature is discussed. Although red blood cells are a significant part of this context, this study focuses on malignant lymphoid cells and blast cells.              Results:                    There is no doubt that these technologies may help the cytologist to perform efficient, objective, and fast morphological analysis of blood cells. They may also help in the interpretation of some morphological features and may serve as learning and survey tools.              Conclusion:                    Although research is still needed, it is important to define screening strategies to exploit the potential of image-based automatic recognition systems integrated in the daily routine of laboratories along with other analysis methodologies.",2018-05-01,7,1131,82,1103
2364,29426065,A review of machine learning in obesity,"Rich sources of obesity-related data arising from sensors, smartphone apps, electronic medical health records and insurance data can bring new insights for understanding, preventing and treating obesity. For such large datasets, machine learning provides sophisticated and elegant tools to describe, classify and predict obesity-related risks and outcomes. Here, we review machine learning methods that predict and/or classify such as linear and logistic regression, artificial neural networks, deep learning and decision tree analysis. We also review methods that describe and characterize data such as cluster analysis, principal component analysis, network science and topological data analysis. We introduce each method with a high-level overview followed by examples of successful applications. The algorithms were then applied to National Health and Nutrition Examination Survey to demonstrate methodology, utility and outcomes. The strengths and limitations of each method were also evaluated. This summary of machine learning algorithms provides a unique overview of the state of data analysis applied specifically to obesity.",2018-05-01,18,1134,39,1103
632,29173233,Fact or fiction: reducing the proportion and impact of false positives,"False positive findings in science are inevitable, but are they particularly common in psychology and psychiatry? The evidence that we review suggests that while not restricted to our field, the problem is acute. We describe the concept of researcher 'degrees-of-freedom' to explain how many false-positive findings arise, and how the various strategies of registration, pre-specification, and reporting standards that are being adopted both reduce and make these visible. We review possible benefits and harms of proposed statistical solutions, from tougher requirements for significance, to Bayesian and machine learning approaches to analysis. Finally we consider the organisation and methods for replication and systematic review in psychology and psychiatry.",2018-05-01,2,763,70,1103
2980,29723481,The potential for machine learning algorithms to improve and reduce the cost of 3-dimensional printing for surgical planning,"Introduction:                    3D-printed anatomical models play an important role in medical and research settings. The recent successes of 3D anatomical models in healthcare have led many institutions to adopt the technology. However, there remain several issues that must be addressed before it can become more wide-spread. Of importance are the problems of cost and time of manufacturing. Machine learning (ML) could be utilized to solve these issues by streamlining the 3D modeling process through rapid medical image segmentation and improved patient selection and image acquisition. The current challenges, potential solutions, and future directions for ML and 3D anatomical modeling in healthcare are discussed.              Areas covered:                    This review covers research articles in the field of machine learning as related to 3D anatomical modeling. Topics discussed include automated image segmentation, cost reduction, and related time constraints.              Expert commentary:                    ML-based segmentation of medical images could potentially improve the process of 3D anatomical modeling. However, until more research is done to validate these technologies in clinical practice, their impact on patient outcomes will remain unknown. We have the necessary computational tools to tackle the problems discussed. The difficulty now lies in our ability to collect sufficient data.",2018-05-01,0,1420,124,1103
2960,29781047,A Survey on Coronary Atherosclerotic Plaque Tissue Characterization in Intravascular Optical Coherence Tomography,"Purpose of review:                    Atherosclerotic plaque deposition within the coronary vessel wall leads to arterial stenosis and severe catastrophic events over time. Identification of these atherosclerotic plaque components is essential to pre-estimate the risk of cardiovascular disease (CVD) and stratify them as a high or low risk. The characterization and quantification of coronary plaque components are not only vital but also a challenging task which can be possible using high-resolution imaging techniques.              Recent finding:                    Atherosclerotic plaque components such as thin cap fibroatheroma (TCFA), fibrous cap, macrophage infiltration, large necrotic core, and thrombus are the microstructural plaque components that can be detected with only high-resolution imaging modalities such as intravascular ultrasound (IVUS) and optical coherence tomography (OCT). Light-based OCT provides better visualization of plaque tissue layers of coronary vessel walls as compared to IVUS. Three dominant paradigms have been identified to characterize atherosclerotic plaque components based on optical attenuation coefficients, machine learning algorithms, and deep learning techniques. This review (condensation of 126 papers after downloading 150 articles) presents a detailed comparison among various methodologies utilized for plaque tissue characterization, classification, and arterial measurements in OCT. Furthermore, this review presents the different ways to predict and stratify the risk associated with the CVD based on plaque characterization and measurements in OCT. Moreover, this review discovers three different paradigms for plaque characterization and their pros and cons. Among all of the techniques, a combination of machine learning and deep learning techniques is a best possible solution that provides improved OCT-based risk stratification.",2018-05-01,12,1896,113,1103
2323,29536448,Machine Learning-Based Modeling of Drug Toxicity,"Toxicity is an important reason for the failure of drug research and development (R&D). The traditional experimental testings for chemical toxicity profile are costly and time-consuming. Therefore, it is attractive to develop the effective and accurate alternatives, such as in silico prediction models. In this review, we discuss the practical use of some prediction models on three toxicity end points, including acute toxicity, carcinogenicity, and inhibition of the human ether-a-go-go-related gene ion channel (hERG). Special emphasis is put on the machine learning methods for developing in silico models, and their advantages and weaknesses are discussed. We conclude that machine learning methods are valuable for helping the process of designing new candidates with low toxicity in drug R&D studies. In the future, much still needs to be done to understand more completely the biological mechanisms for toxicity and to develop more accurate prediction models to screen compounds.",2018-06-01,4,988,48,1072
2324,29536444,Revisit of Machine Learning Supported Biological and Biomedical Studies,"Generally, machine learning includes many in silico methods to transform the principles underlying natural phenomenon to human understanding information, which aim to save human labor, to assist human judge, and to create human knowledge. It should have wide application potential in biological and biomedical studies, especially in the era of big biological data. To look through the application of machine learning along with biological development, this review provides wide cases to introduce the selection of machine learning methods in different practice scenarios involved in the whole biological and biomedical study cycle and further discusses the machine learning strategies for analyzing omics data in some cutting-edge biological studies. Finally, the notes on new challenges for machine learning due to small-sample high-dimension are summarized from the key points of sample unbalance, white box, and causality.",2018-06-01,0,925,71,1072
2933,29873832,Multimodal seizure detection: A review,"A review is given on the combined use of multiple modalities in non electroencephalography (EEG)-based detection of motor seizures in children and adults. A literature search of papers was done on multimodal seizure detection with extraction of data on type of modalities, study design and algorithm, sensitivity, false detection rate, and seizure types. Evidence of superiority was sought for using multiple instead of single modalities. Seven papers were found from 2010 to 2017, mostly using contact sensors such as accelerometers (n = 5), electromyography (n = 2), heart rate (n = 2), electrodermal activity (n = 1), and oximetry (n = 1). Remote sensors included video, radar, movement, and sound. All studies but one were in-hospital, with video-EEG as a gold standard. Algorithms were based on physiology and supervised machine learning, but did not always include a separate test dataset. Sensitivity ranged from 4% to 100% and false detection rate from 0.25 to 20 per 8 hours. Tonic-clonic seizure detection performed best. False detections tended to be restricted to a minority (16%-30%) of patients. Use of multiple sensors increased sensitivity; false detections decreased in one study, but increased in another. These preliminary studies suggest that detection of tonic-clonic seizures might be feasible, but larger field studies are required under more rigorous design that precludes bias. Generic algorithms probably suffice for the majority of patients.",2018-06-01,10,1468,38,1072
149,30095052,"Perturbation Theory Machine Learning Models: Theory, Regulatory Issues, and Applications to Organic Synthesis, Medicinal Chemistry, Protein Research, and Technology","Machine Learning (ML) models are very useful to predict physicochemical properties of small organic molecules, proteins, proteomes, and complex systems. These methods may be useful to reduce the cost of research in terms of materials resources, time, and laboratory animal sacrifice. Recently different authors have reported Perturbation Theory (PT) methods combined with ML to obtain PTML (PT + ML) models. They have applied PTML models to the study of different biological systems and in technology as well. Here, we present one state-of- the-art review about the different applications of PTML models in Organic Synthesis, Medicinal Chemistry, Protein Research, and Technology. In this work, we also embrace an overview of regulatory issues for acceptance and validation of both: the Cheminformatics models, and the characterization of new Biomaterials. This is a main question in order to make scientific result self for humans and environment.",2018-06-01,0,948,164,1072
2894,29977864,Machine Learning and Radiogenomics: Lessons Learned and Future Directions,"Due to the rapid increase in the availability of patient data, there is significant interest in precision medicine that could facilitate the development of a personalized treatment plan for each patient on an individual basis. Radiation oncology is particularly suited for predictive machine learning (ML) models due to the enormous amount of diagnostic data used as input and therapeutic data generated as output. An emerging field in precision radiation oncology that can take advantage of ML approaches is radiogenomics, which is the study of the impact of genomic variations on the sensitivity of normal and tumor tissue to radiation. Currently, patients undergoing radiotherapy are treated using uniform dose constraints specific to the tumor and surrounding normal tissues. This is suboptimal in many ways. First, the dose that can be delivered to the target volume may be insufficient for control but is constrained by the surrounding normal tissue, as dose escalation can lead to significant morbidity and rare. Second, two patients with nearly identical dose distributions can have substantially different acute and late toxicities, resulting in lengthy treatment breaks and suboptimal control, or chronic morbidities leading to poor quality of life. Despite significant advances in radiogenomics, the magnitude of the genetic contribution to radiation response far exceeds our current understanding of individual risk variants. In the field of genomics, ML methods are being used to extract harder-to-detect knowledge, but these methods have yet to fully penetrate radiogenomics. Hence, the goal of this publication is to provide an overview of ML as it applies to radiogenomics. We begin with a brief history of radiogenomics and its relationship to precision medicine. We then introduce ML and compare it to statistical hypothesis testing to reflect on shared lessons and to avoid common pitfalls. Current ML approaches to genome-wide association studies are examined. The application of ML specifically to radiogenomics is next presented. We end with important lessons for the proper integration of ML into radiogenomics.",2018-06-01,18,2134,73,1072
2500,30628866,A review on machine learning methods for in silico toxicity prediction,"In silico toxicity prediction plays an important role in the regulatory decision making and selection of leads in drug design as in vitro/vivo methods are often limited by ethics, time, budget, and other resources. Many computational methods have been employed in predicting the toxicity profile of chemicals. This review provides a detailed end-to-end overview of the application of machine learning algorithms to Structure-Activity Relationship (SAR)-based predictive toxicology. From raw data to model validation, the importance of data quality is stressed as it greatly affects the predictive power of derived models. Commonly overlooked challenges such as data imbalance, activity cliff, model evaluation, and definition of applicability domain are highlighted, and plausible solutions for alleviating these challenges are discussed.",2018-06-01,6,838,70,1072
155,30084331,Advances in Computational Studies of Potential Drug Targets in Mycobacterium tuberculosis,"Tuberculosis continues to remain as one of the leading causes of death worldwide, in spite of significant progress being made in the last twenty years through increased compliance to treatment. The current review gives an overview of the recent efforts made in the endeavor to identify novel inhibitors and promising drug targets for Mycobacterium tuberculosis with structure and ligand-based approaches along with bioinformatics studies following complete sequencing of its genome. A large number of these studies target biomolecules in metabolic pathways that are vital for the survival of the microorganism. A discussion on efforts to study metalloproteins as relatively underexplored targets in the context of their druggability is also presented.",2018-06-01,1,751,89,1072
2936,29869300,A Review of the Evolution of Vision-Based Motion Analysis and the Integration of Advanced Computer Vision Methods Towards Developing a Markerless System,"Background:                    The study of human movement within sports biomechanics and rehabilitation settings has made considerable progress over recent decades. However, developing a motion analysis system that collects accurate kinematic data in a timely, unobtrusive and externally valid manner remains an open challenge.              Main body:                    This narrative review considers the evolution of methods for extracting kinematic information from images, observing how technology has progressed from laborious manual approaches to optoelectronic marker-based systems. The motion analysis systems which are currently most widely used in sports biomechanics and rehabilitation do not allow kinematic data to be collected automatically without the attachment of markers, controlled conditions and/or extensive processing times. These limitations can obstruct the routine use of motion capture in normal training or rehabilitation environments, and there is a clear desire for the development of automatic markerless systems. Such technology is emerging, often driven by the needs of the entertainment industry, and utilising many of the latest trends in computer vision and machine learning. However, the accuracy and practicality of these systems has yet to be fully scrutinised, meaning such markerless systems are not currently in widespread use within biomechanics.              Conclusions:                    This review aims to introduce the key state-of-the-art in markerless motion capture research from computer vision that is likely to have a future impact in biomechanics, while considering the challenges with accuracy and robustness that are yet to be addressed.",2018-06-01,17,1697,152,1072
2296,29676964,Advanced Imaging Techniques in Evaluation of Colorectal Cancer,"Imaging techniques are clinical decision-making tools in the evaluation of patients with colorectal cancer (CRC). The aim of this article is to discuss the potential of recent advances in imaging for diagnosis, prognosis, therapy planning, and assessment of response to treatment of CRC. Recent developments and new clinical applications of conventional imaging techniques such as virtual colonoscopy, dual-energy spectral computed tomography, elastography, advanced computing techniques (including volumetric rendering techniques and machine learning), magnetic resonance (MR) imaging-based magnetization transfer, and new liver imaging techniques, which may offer additional clinical information in patients with CRC, are summarized. In addition, the clinical value of functional and molecular imaging techniques such as diffusion-weighted MR imaging, dynamic contrast material-enhanced imaging, blood oxygen level-dependent imaging, lymphography with contrast agents, positron emission tomography with different radiotracers, and MR spectroscopy is reviewed, and the advantages and disadvantages of these modalities are evaluated. Finally, the future role of imaging-based analysis of tumor heterogeneity and multiparametric imaging, the development of radiomics and radiogenomics, and future challenges for imaging of patients with CRC are discussed. Online supplemental material is available for this article. RSNA, 2018.",2018-06-01,12,1427,62,1072
2295,29679305,"The New Possibilities from ""Big Data"" to Overlooked Associations Between Diabetes, Biochemical Parameters, Glucose Control, and Osteoporosis","Purpose of review:                    To review current practices and technologies within the scope of ""Big Data"" that can further our understanding of diabetes mellitus and osteoporosis from large volumes of data. ""Big Data"" techniques involving supervised machine learning, unsupervised machine learning, and deep learning image analysis are presented with examples of current literature.              Recent findings:                    Supervised machine learning can allow us to better predict diabetes-induced osteoporosis and understand relative predictor importance of diabetes-affected bone tissue. Unsupervised machine learning can allow us to understand patterns in data between diabetic pathophysiology and altered bone metabolism. Image analysis using deep learning can allow us to be less dependent on surrogate predictors and use large volumes of images to classify diabetes-induced osteoporosis and predict future outcomes directly from images. ""Big Data"" techniques herald new possibilities to understand diabetes-induced osteoporosis and ascertain our current ability to classify, understand, and predict this condition.",2018-06-01,2,1138,140,1072
2972,29745270,QSAR modelling: a therapeutic patent review 2010-present,"Introduction:                    Quantitative Structure-Activity Relationship (QSAR) models are becoming one of the most interesting fields for developing therapeutics and therapeutics related patents. At present, QSAR methodologies comprise a series of possibilities, including joining forces with machine learning methods and increasing even more the swiftness they might bring to the prospective development of therapeutics in the Health Sciences scope.              Areas covered:                    After evaluating the period from 2010 to early 2018, the areas covered by the reviewed QSAR based therapeutics patents comprise three main fields (drug development, risk assessment and novel QSAR methodologies), and several areas, from cancer and cancer related symptomatology to neurodegenerative diseases, such as Parkinson's disease, or even monitoring several chemical particles carrier-mediums or interface frontiers.              Expert opinion:                    Among the several conclusions drawn from this reviewing, some pertain to the near future of investigative research on QSAR based inventions for therapeutic purposes, while others include the prospective of an even more grown interest on cytotoxicity assessment with in silico models and protocols. Further, the type of compounds described in these types of patents is likely to see an increase in neurodegenerative diseases therapeutics, as the novel methodologies meet the challenging global health needs as human life expectancy increases.",2018-06-01,4,1516,56,1072
2294,29681257,Implementation of novel statistical procedures and other advanced approaches to improve analysis of CASA data,"Computer-aided sperm analysis (CASA) produces a wealth of data that is frequently ignored. The use of multiparametric statistical methods can help explore these datasets, unveiling the subpopulation structure of sperm samples. In this review we analyse the significance of the internal heterogeneity of sperm samples and its relevance. We also provide a brief description of the statistical tools used for extracting sperm subpopulations from the datasets, namely unsupervised clustering (with non-hierarchical, hierarchical and two-step methods) and the most advanced supervised methods, based on machine learning. The former method has allowed exploration of subpopulation patterns in many species, whereas the latter offering further possibilities, especially considering functional studies and the practical use of subpopulation analysis. We also consider novel approaches, such as the use of geometric morphometrics or imaging flow cytometry. Finally, although the data provided by CASA systems provides valuable information on sperm samples by applying clustering analyses, there are several caveats. Protocols for capturing and analysing motility or morphometry should be standardised and adapted to each experiment, and the algorithms should be open in order to allow comparison of results between laboratories. Moreover, we must be aware of new technology that could change the paradigm for studying sperm motility and morphology.",2018-06-01,5,1439,109,1072
2971,29746776,From Cancer to Pain Target by Automated Selectivity Inversion of a Clinical Candidate,"Elimination of inadvertent binding is crucial for inhibitor design targeting conserved protein classes like kinases. Compounds in clinical trials provide a rich source for initiating drug design efforts by exploiting such secondary binding events. Considering both aspects, we shifted the selectivity of tozasertib, originally developed against AurA as cancer target, toward the pain target TrkA. First, selectivity-determining features in binding pockets were identified by fusing interaction grids of several key and off-target conformations. A focused library was subsequently created and prioritized using a multiobjective selection scheme that filters for selective and highly active compounds based on orthogonal methods grounded in computational chemistry and machine learning. Eighteen high-ranking compounds were synthesized and experimentally tested. The top-ranked compound has 10000-fold improved selectivity versus AurA, nanomolar cellular activity, and is highly selective in a kinase panel. This was achieved in a single round of automated in silico optimization, highlighting the power of recent advances in computer-aided drug design to automate design and selection processes.",2018-06-01,0,1194,85,1072
2584,30465503,Intelligently Applying Artificial Intelligence in Chemoinformatics,"The intertwining of chemoinformatics with artificial intelligence (AI) has given a tremendous fillip to the field of drug discovery. With the rapid growth of chemical data from high throughput screening and combinatorial synthesis, AI has become an indispensable tool for drug designers to mine chemical information from large compound databases for developing drugs at a much faster rate as never before. The applications of AI have gone beyond bioactivity predictions and have shown promise in addressing diverse problems in drug discovery like de novo molecular design, synthesis prediction and biological image analysis. In this article, we provide an overview of all the algorithms under the umbrella of AI, enlist the tools/frameworks required for implementing these algorithms as well as present a compendium of web servers, databases and open-source platforms implicated in drug discovery, Quantitative Structure-Activity Relationship (QSAR), data mining, solvation free energy and molecular graph mining.",2018-06-01,2,1013,66,1072
633,29173045,Multiplicity issues in exploratory subgroup analysis,"The general topic of subgroup identification has attracted much attention in the clinical trial literature due to its important role in the development of tailored therapies and personalized medicine. Subgroup search methods are commonly used in late-phase clinical trials to identify subsets of the trial population with certain desirable characteristics. Post-hoc or exploratory subgroup exploration has been criticized for being extremely unreliable. Principled approaches to exploratory subgroup analysis based on recent advances in machine learning and data mining have been developed to address this criticism. These approaches emphasize fundamental statistical principles, including the importance of performing multiplicity adjustments to account for selection bias inherent in subgroup search. This article provides a detailed review of multiplicity issues arising in exploratory subgroup analysis. Multiplicity corrections in the context of principled subgroup search will be illustrated using the family of SIDES (subgroup identification based on differential effect search) methods. A case study based on a Phase III oncology trial will be presented to discuss the details of subgroup search algorithms with resampling-based multiplicity adjustment procedures.",2018-06-01,4,1272,52,1072
456,30311557,Machine-Learning Prediction of Drug-Induced Cardiac Arrhythmia: Analysis of Gene Expression and Clustering,"A marked delay in the electrical repolarization of heart ventricles is characterized by prolongation of the Q-T wave (QT) interval on a surface electrocardiogram. Such a delay can lead to potentially life-threatening cardiac arrhythmia (torsades de pointes). Such prolongation is also a widely accepted cardiac safety biomarker in drug development. Current preclinical drug-safety assays include patch clamp analysis to evaluate drug-related blockade of cardiac repolarizing ion currents. Recently reported patch clamp assay results have shown predictive sensitivities and specificities in the ranges of 64%-82% and 75%-88%, respectively. In this project, we use a support vector machine classifier to find mean sensitivities and specificities of 85% and 90%, respectively, across 77 drug subclassifications. Clustering by gene expression profile similarities shows that drugs known to prolong the QT interval do not always form distinct groups, but the number of groups is limited. The most common biological network links associated with these groups involve genes linked with fatty acid metabolism, G proteins, intracellular glutathione, immune responses, apoptosis, mitochondrial function, electron transport, and mitogen-activated protein kinases. These results suggest that machine-learning analysis of gene expression and clustering may augment cardiac safety predictions for improving drug-safety assessments.",2018-06-01,0,1417,106,1072
2898,29970965,Cancer risk assessment in modern radiotherapy workflow with medical big data,"Modern radiotherapy (RT) is being enriched by big digital data and intensive technology. Multimodality image registration, intelligence-guided planning, real-time tracking, image-guided RT (IGRT), and automatic follow-up surveys are the products of the digital era. Enormous digital data are created in the process of treatment, including benefits and risks. Generally, decision making in RT tries to balance these two aspects, which is based on the archival and retrieving of data from various platforms. However, modern risk-based analysis shows that many errors that occur in radiation oncology are due to failures in workflow. These errors can lead to imbalance between benefits and risks. In addition, the exact mechanism and dose-response relationship for radiation-induced malignancy are not well understood. The cancer risk in modern RT workflow continues to be a problem. Therefore, in this review, we develop risk assessments based on our current knowledge of IGRT and provide strategies for cancer risk reduction. Artificial intelligence (AI) such as machine learning is also discussed because big data are transforming RT via AI.",2018-06-01,3,1141,76,1072
2912,29934910,Predictive Systems Toxicology,"In this review we address to what extent computational techniques can augment our ability to predict toxicity. The first section provides a brief history of empirical observations on toxicity dating back to the dawn of Sumerian civilization. Interestingly, the concept of dose emerged very early on, leading up to the modern emphasis on kinetic properties, which in turn encodes the insight that toxicity is not solely a property of a compound but instead depends on the interaction with the host organism. The next logical step is the current conception of evaluating drugs from a personalized medicine point of view. We review recent work on integrating what could be referred to as classical pharmacokinetic analysis with emerging systems biology approaches incorporating multiple omics data. These systems approaches employ advanced statistical analytical data processing complemented with machine learning techniques and use both pharmacokinetic and omics data. We find that such integrated approaches not only provide improved predictions of toxicity but also enable mechanistic interpretations of the molecular mechanisms underpinning toxicity and drug resistance. We conclude the chapter by discussing some of the main challenges, such as how to balance the inherent tension between the predicitive capacity of models, which in practice amounts to constraining the number of features in the models versus allowing for rich mechanistic interpretability, i.e., equipping models with numerous molecular features. This challenge also requires patient-specific predictions on toxicity, which in turn requires proper stratification of patients as regards how they respond, with or without adverse toxic effects. In summary, the transformation of the ancient concept of dose is currently successfully operationalized using rich integrative data encoded in patient-specific models.",2018-06-01,0,1881,29,1072
2423,30821199,Deep learning for predicting toxicity of chemicals: a mini review,"Humans and wildlife inhabit a world with panoply of natural and synthetic chemicals. Alarmingly, only a limited number of chemicals have undergone comprehensive toxicological evaluation due to limitations of traditional toxicity testing. High-throughput screening assays provide a higher-speed alternative for conventional toxicity testing. Advancement of high-throughput bioassay technology has greatly increased chemical toxicity data volumes in the past decade, pushing toxicology research into a ""big data"" era. However, traditional data analysis methods fail to effectively process large data volumes, presenting both a challenge and an opportunity for toxicologists. Deep learning, a machine learning method leveraging deep neural networks (DNNs), is a proven useful tool for building quantitative structure-activity relationship (QSAR) models for toxicity prediction utilizing these new large datasets. In this mini review, a brief technical background on DNNs is provided, and the current state of chemical toxicity prediction models built with DNNs is reviewed. In addition, relevant toxicity data sources are summarized, possible limitations are discussed, and perspectives on DNN utilization in chemical toxicity prediction are given.",2018-06-01,2,1245,65,1072
2313,29601321,Predicting adverse hemodynamic events in critically ill patients,"Purpose of review:                    The art of predicting future hemodynamic instability in the critically ill has rapidly become a science with the advent of advanced analytical processed based on computer-driven machine learning techniques. How these methods have progressed beyond severity scoring systems to interface with decision-support is summarized.              Recent findings:                    Data mining of large multidimensional clinical time-series databases using a variety of machine learning tools has led to our ability to identify alert artifact and filter it from bedside alarms, display real-time risk stratification at the bedside to aid in clinical decision-making and predict the subsequent development of cardiorespiratory insufficiency hours before these events occur. This fast evolving filed is primarily limited by linkage of high-quality granular to physiologic rationale across heterogeneous clinical care domains.              Summary:                    Using advanced analytic tools to glean knowledge from clinical data streams is rapidly becoming a reality whose clinical impact potential is great.",2018-06-01,2,1140,64,1072
2314,29600766,Discovering Synergistic Drug Combination from a Computational Perspective,"Synergistic drug combinations play an important role in the treatment of complex diseases. The identification of effective drug combination is vital to further reduce the side effects and improve therapeutic efficiency. In previous years, in vitro method has been the main route to discover synergistic drug combinations. However, many limitations of time and resource consumption lie within the in vitro method. Therefore, with the rapid development of computational models and the explosive growth of large and phenotypic data, computational methods for discovering synergistic drug combinations are an efficient and promising tool and contribute to precision medicine. It is the key of computational methods how to construct the computational model. Different computational strategies generate different performance. In this review, the recent advancements in computational methods for predicting effective drug combination are concluded from multiple aspects. First, various datasets utilized to discover synergistic drug combinations are summarized. Second, we discussed feature-based approaches and partitioned these methods into two classes including feature-based methods in terms of similarity measure, and feature-based methods in terms of machine learning. Third, we discussed network-based approaches for uncovering synergistic drug combinations. Finally, we analyzed and prospected computational methods for predicting effective drug combinations.",2018-06-01,0,1460,73,1072
112,30182829,A Brief Review on Software Tools in Generating Chou's Pseudo-factor Representations for All Types of Biological Sequences,"Background:                    In the post-genome age, it is more urgent to understand the functions of genes and proteins. Since experimental methods are usually costly and time consuming, computational predictions are recognized as an alternative approach. In developing a predictive method for functional genomics and proteomics, one of the most important steps is to represent biological sequences with a fixed length numerical form, which can be further analyzed using machine learning algorithms. Chou's pseudo-amino acid compositions and the pseudo k-nucleotide compositions are algorithms for this purpose.              Conclusion:                    Since the appearance of these algorithms, several software tools have been developed as implementations. These software tools facilitate the application of these algorithms. As these software tools are developed with different technologies and for different application scenarios, we will briefly review the technical aspect of these software tools in this short review.",2018-06-01,3,1029,121,1072
2906,29953863,Phenotypic Image Analysis Software Tools for Exploring and Understanding Big Image Data from Cell-Based Assays,"Phenotypic image analysis is the task of recognizing variations in cell properties using microscopic image data. These variations, produced through a complex web of interactions between genes and the environment, may hold the key to uncover important biological phenomena or to understand the response to a drug candidate. Today, phenotypic analysis is rarely performed completely by hand. The abundance of high-dimensional image data produced by modern high-throughput microscopes necessitates computational solutions. Over the past decade, a number of software tools have been developed to address this need. They use statistical learning methods to infer relationships between a cell's phenotype and data from the image. In this review, we examine the strengths and weaknesses of non-commercial phenotypic image analysis software, cover recent developments in the field, identify challenges, and give a perspective on future possibilities.",2018-06-01,18,942,110,1072
2905,29956014,A Survey of Data Mining and Deep Learning in Bioinformatics,"The fields of medicine science and health informatics have made great progress recently and have led to in-depth analytics that is demanded by generation, collection and accumulation of massive data. Meanwhile, we are entering a new period where novel technologies are starting to analyze and explore knowledge from tremendous amount of data, bringing limitless potential for information growth. One fact that cannot be ignored is that the techniques of machine learning and deep learning applications play a more significant role in the success of bioinformatics exploration from biological data point of view, and a linkage is emphasized and established to bridge these two data analytics techniques and bioinformatics in both industry and academia. This survey concentrates on the review of recent researches using data mining and deep learning approaches for analyzing the specific domain knowledge of bioinformatics. The authors give a brief but pithy summarization of numerous data mining algorithms used for preprocessing, classification and clustering as well as various optimized neural network architectures in deep learning methods, and their advantages and disadvantages in the practical applications are also discussed and compared in terms of their industrial usage. It is believed that in this review paper, valuable insights are provided for those who are dedicated to start using data analytics methods in bioinformatics.",2018-06-01,15,1438,59,1072
2977,29725961,Hello World Deep Learning in Medical Imaging,"There is recent popularity in applying machine learning to medical imaging, notably deep learning, which has achieved state-of-the-art performance in image analysis and processing. The rapid adoption of deep learning may be attributed to the availability of machine learning frameworks and libraries to simplify their use. In this tutorial, we provide a high-level overview of how to build a deep neural network for medical image classification, and provide code that can help those new to the field begin their informatics projects.",2018-06-01,8,533,44,1072
2315,29595111,Looking for New Inhibitors for the Epidermal Growth Factor Receptor,"Epidermal Growth Factor Receptor (EGFR) is still the main target of the Head and Neck Squamous Cell Cancer (HNSCC) because its overexpression has been detected in more than 90% of this type of cancer. This overexpression is usually linked with more aggressive disease, increased resistance to chemotherapy and radiotherapy, increased metastasis, inhibition of apoptosis, promotion of neoplastic angiogenesis, and, finally, poor prognosis and decreased survival. Due to this reason, the main target in the search of new drugs and inhibitors candidates is to downturn this overexpression. Quantitative Structure-Activity Relationship (QSAR) is one of the most widely used approaches while looking for new and more active inhibitors drugs. In this contest, a lot of authors used this technique, combined with others, to find new drugs or enhance the activity of well-known inhibitors. In this paper, on one hand, we will review the most important QSAR approaches developed in the last fifteen years, spacing from classical 1D approaches until more sophisticated 3D; the first paper is dated 2003 while the last one is from 2017. On the other hand, we will present a completely new QSAR approach aimed at the prediction of new EGFR inhibitors drugs. The model presented here has been developed over a dataset consisting of more than 1000 compounds using various molecular descriptors calculated with the DRAGON 7.0 software.",2018-06-01,4,1421,67,1072
2904,29956120,"Smartphones, Sensors, and Machine Learning to Advance Real-Time Prediction and Interventions for Suicide Prevention: a Review of Current Progress and Next Steps","Purpose of review:                    As rates of suicide continue to rise, there is urgent need for innovative approaches to better understand, predict, and care for those at high risk of suicide. Numerous mobile and sensor technology solutions have already been proposed, are in development, or are already available today. This review seeks to assess their clinical evidence and help the reader understand the current state of the field.              Recent findings:                    Advances in smartphone sensing, machine learning methods, and mobile apps directed towards reducing suicide offer promising evidence; however, most of these innovative approaches are still nascent. Further replication and validation of preliminary results is needed. Whereas numerous promising mobile and sensor technology based solutions for real time understanding, predicting, and caring for those at highest risk of suicide are being studied today, their clinical utility remains largely unproven. However, given both the rapid pace and vast scale of current research efforts, we expect clinicians will soon see useful and impactful digital tools for this space within the next 2 to 5 years.",2018-06-01,33,1185,160,1072
2691,28302041,Promises of Machine Learning Approaches in Prediction of Absorption of Compounds,"The Machine Learning (ML) is one of the fastest developing techniques in the prediction and evaluation of important pharmacokinetic properties such as absorption, distribution, metabolism and excretion. The availability of a large number of robust validation techniques for prediction models devoted to pharmacokinetics has significantly enhanced the trust and authenticity in ML approaches. There is a series of prediction models generated and used for rapid screening of compounds on the basis of absorption in last one decade. Prediction of absorption of compounds using ML models has great potential across the pharmaceutical industry as a non-animal alternative to predict absorption. However, these prediction models still have to go far ahead to develop the confidence similar to conventional experimental methods for estimation of drug absorption. Some of the general concerns are selection of appropriate ML methods and validation techniques in addition to selecting relevant descriptors and authentic data sets for the generation of prediction models. The current review explores published models of ML for the prediction of absorption using physicochemical properties as descriptors and their important conclusions. In addition, some critical challenges in acceptance of ML models for absorption are also discussed.",2018-06-01,0,1326,80,1072
2930,29880128,Artificial Intelligence in Cardiology,"Artificial intelligence and machine learning are poised to influence nearly every aspect of the human condition, and cardiology is not an exception to this trend. This paper provides a guide for clinicians on relevant aspects of artificial intelligence and machine learning, reviews selected applications of these methods in cardiology to date, and identifies how cardiovascular medicine could incorporate artificial intelligence in the future. In particular, the paper first reviews predictive modeling concepts relevant to cardiology such as feature selection and frequent pitfalls such as improper dichotomization. Second, it discusses common algorithms used in supervised learning and reviews selected applications in cardiology and related disciplines. Third, it describes the advent of deep learning and related methods collectively called unsupervised learning, provides contextual examples both in general medicine and in cardiovascular medicine, and then explains how these methods could be applied to enable precision cardiology and improve patient outcomes.",2018-06-01,87,1068,37,1072
2931,29879881,Machine Learning-based Virtual Screening and Its Applications to Alzheimer's Drug Discovery: A Review,"Background:                    Virtual Screening (VS) has emerged as an important tool in the drug development process, as it conducts efficient in silico searches over millions of compounds, ultimately increasing yields of potential drug leads. As a subset of Artificial Intelligence (AI), Machine Learning (ML) is a powerful way of conducting VS for drug leads. ML for VS generally involves assembling a filtered training set of compounds, comprised of known actives and inactives. After training the model, it is validated and, if sufficiently accurate, used on previously unseen databases to screen for novel compounds with desired drug target binding activity.              Objective:                    The study aims to review ML-based methods used for VS and applications to Alzheimer's Disease (AD) drug discovery.              Methods:                    To update the current knowledge on ML for VS, we review thorough backgrounds, explanations, and VS applications of the following ML techniques: Nave Bayes (NB), k-Nearest Neighbors (kNN), Support Vector Machines (SVM), Random Forests (RF), and Artificial Neural Networks (ANN).              Results:                    All techniques have found success in VS, but the future of VS is likely to lean more largely toward the use of neural networks - and more specifically, Convolutional Neural Networks (CNN), which are a subset of ANN that utilize convolution. We additionally conceptualize a work flow for conducting ML-based VS for potential therapeutics for AD, a complex neurodegenerative disease with no known cure and prevention. This both serves as an example of how to apply the concepts introduced earlier in the review and as a potential workflow for future implementation.              Conclusion:                    Different ML techniques are powerful tools for VS, and they have advantages and disadvantages albeit. ML-based VS can be applied to AD drug development.",2018-06-01,10,1945,101,1072
2932,29874824,Changing Trends in Computational Drug Repositioning,"Efforts to maximize the indications potential and revenue from drugs that are already marketed are largely motivated by what Sir James Black, a Nobel Prize-winning pharmacologist advocated-""The most fruitful basis for the discovery of a new drug is to start with an old drug"". However, rational design of drug mixtures poses formidable challenges because of the lack of or limited information about in vivo cell regulation, mechanisms of genetic pathway activation, and in vivo pathway interactions. Hence, most of the successfully repositioned drugs are the result of ""serendipity"", discovered during late phase clinical studies of unexpected but beneficial findings. The connections between drug candidates and their potential adverse drug reactions or new applications are often difficult to foresee because the underlying mechanism associating them is largely unknown, complex, or dispersed and buried in silos of information. Discovery of such multi-domain pharmacomodules-pharmacologically relevant sub-networks of biomolecules and/or pathways-from collection of databases by independent/simultaneous mining of multiple datasets is an active area of research. Here, while presenting some of the promising bioinformatics approaches and pipelines, we summarize and discuss the current and evolving landscape of computational drug repositioning.",2018-06-01,19,1348,51,1072
2306,29626206,High-throughput mouse phenomics for characterizing mammalian gene function,"We are entering a new era of mouse phenomics, driven by large-scale and economical generation of mouse mutants coupled with increasingly sophisticated and comprehensive phenotyping. These studies are generating large, multidimensional gene-phenotype data sets, which are shedding new light on the mammalian genome landscape and revealing many hitherto unknown features of mammalian gene function. Moreover, these phenome resources provide a wealth of disease models and can be integrated with human genomics data as a powerful approach for the interpretation of human genetic variation and its relationship to disease. In the future, the development of novel phenotyping platforms allied to improved computational approaches, including machine learning, for the analysis of phenotype data will continue to enhance our ability to develop a comprehensive and powerful model of mammalian gene-phenotype space.",2018-06-01,17,906,74,1072
461,30306477,3D Ultrasound for Orthopedic Interventions,"Ultrasound is a real-time, non-radiation-based imaging modality with an ability to acquire two-dimensional (2D) and three-dimensional (3D) data. Due to these capabilities, research has been carried out in order to incorporate it as an intraoperative imaging modality for various orthopedic surgery procedures. However, high levels of noise, different imaging artifacts, and bone surfaces appearing blurred with several mm in thickness have prohibited the widespread use of ultrasound as a standard of care imaging modality in orthopedics. In this chapter, we provided a detailed overview of numerous applications of 3D ultrasound in the domain of orthopedic surgery. Specifically, we discuss the advantages and disadvantages of methods proposed for segmentation and enhancement of bone ultrasound data and the successful application of these methods in clinical domain. Finally, a number of challenges are identified which need to be overcome in order for ultrasound to become a preferred imaging modality in orthopedics.",2018-06-01,0,1021,42,1072
617,29222764,"Optimization of Multi-Omic Genome-Scale Models: Methodologies, Hands-on Tutorial, and Perspectives","Genome-scale metabolic models are valuable tools for assessing the metabolic potential of living organisms. Being downstream of gene expression, metabolism is increasingly being used as an indicator of the phenotypic outcome for drugs and therapies. We here present a review of the principal methods used for constraint-based modelling in systems biology, and explore how the integration of multi-omic data can be used to improve phenotypic predictions of genome-scale metabolic models. We believe that the large-scale comparison of the metabolic response of an organism to different environmental conditions will be an important challenge for genome-scale models. Therefore, within the context of multi-omic methods, we describe a tutorial for multi-objective optimization using the metabolic and transcriptomics adaptation estimator (METRADE), implemented in MATLAB. METRADE uses microarray and codon usage data to model bacterial metabolic response to environmental conditions (e.g., antibiotics, temperatures, heat shock). Finally, we discuss key considerations for the integration of multi-omic networks into metabolic models, towards automatically extracting knowledge from such models.",2018-06-01,6,1192,98,1072
2334,29498975,The State of Technology in Craniosynostosis,"Introduction:                    Craniosynostosis, the premature fusion of 1 cranial sutures, is the leading cause of pediatric skull deformities, affecting 1 of every 2000 to 2500 live births worldwide. Technologies used for the management of craniofacial conditions, specifically in craniosynostosis, have been advancing dramatically. This article highlights the most recent technological advances in craniosynostosis surgery through a systematic review of the literature.              Methods:                    A systematic electronic search was performed using the PubMed database. Search terms used were ""craniosynostosis"" AND ""technology"" OR ""innovation"" OR ""novel.' Two independent reviewers subsequently reviewed the resultant articles based on strict inclusion and exclusion criteria. Selected manuscripts deemed novel by the senior authors were grouped by procedure categories.              Results:                    Following review of the PubMed database, 28 of 536 articles were retained. Of the 28 articles, 20 articles consisting of 21 technologies were deemed as being novel by the senior authors. The technologies were categorized as diagnostic imaging (n = 6), surgical planning (n = 4), cranial vault evaluation (n = 4), machine learning (n = 3), ultrasound pinning (n = 3), and near-infrared spectroscopy (n = 1).              Conclusion:                    Multiple technological advances have impacted the treatment of craniosynostosis. These innovations include improvement in diagnosis and objective measurement of craniosynostosis, preoperative planning, intraoperative procedures, communication between both surgeons and patients, and surgical education.",2018-06-01,1,1685,43,1072
195,29994590,A Review of Signal Processing Techniques for Electrocardiogram Signal Quality Assessment,"Electrocardiogram (ECG) signal quality assessment (SQA) plays a vital role in significantly improving the diagnostic accuracy and reliability of unsupervised ECG analysis systems. In practice, the ECG signal is often corrupted with different kinds of noises and artifacts. Therefore, numerous SQA methods were presented based on the ECG signal and/or noise features and the machine learning classifiers and/or heuristic decision rules. This paper presents an overview of current state-of-the-art SQA methods and highlights the practical limitations of the existing SQA methods. Based upon past and our studies, it is noticed that a lightweight ECG noise analysis framework is highly demanded for real-time detection, localization, and classification of single and combined ECG noises within the context of wearable ECG monitoring devices which are often resource constrained.",2018-06-01,9,875,88,1072
2966,29754806,Machine learning in cardiac CT: Basic concepts and contemporary data,"Propelled by the synergy of the groundbreaking advancements in the ability to analyze high-dimensional datasets and the increasing availability of imaging and clinical data, machine learning (ML) is poised to transform the practice of cardiovascular medicine. Owing to the growing body of literature validating both the diagnostic performance as well as the prognostic implications of anatomic and physiologic findings, coronary computed tomography angiography (CCTA) is now a well-established non-invasive modality for the assessment of cardiovascular disease. ML has been increasingly utilized to optimize performance as well as extract data from CCTA as well as non-contrast enhanced cardiac CT scans. The purpose of this review is to describe the contemporary state of ML based algorithms applied to cardiac CT, as well as to provide clinicians with an understanding of its benefits and associated limitations.",2018-06-01,13,914,68,1072
307,28831921,In Silico Studies in Drug Research Against Neurodegenerative Diseases,"Background:                    Neurodegenerative diseases such as Alzheimer's disease (AD), amyotrophic lateral sclerosis, Parkinson's disease (PD), spinal cerebellar ataxias, and spinal and bulbar muscular atrophy are described by slow and selective degeneration of neurons and axons in the central nervous system (CNS) and constitute one of the major challenges of modern medicine. Computeraided or in silico drug design methods have matured into powerful tools for reducing the number of ligands that should be screened in experimental assays.              Methods:                    In the present review, the authors provide a basic background about neurodegenerative diseases and in silico techniques in the drug research. Furthermore, they review the various in silico studies reported against various targets in neurodegenerative diseases, including homology modeling, molecular docking, virtual high-throughput screening, quantitative structure activity relationship (QSAR), hologram quantitative structure activity relationship (HQSAR), 3D pharmacophore mapping, proteochemometrics modeling (PCM), fingerprints, fragment-based drug discovery, Monte Carlo simulation, molecular dynamic (MD) simulation, quantum-mechanical methods for drug design, support vector machines, and machine learning approaches.              Results:                    Detailed analysis of the recently reported case studies revealed that the majority of them use a sequential combination of ligand and structure-based virtual screening techniques, with particular focus on pharmacophore models and the docking approach.              Conclusion:                    Neurodegenerative diseases have a multifactorial pathoetiological origin, so scientists have become persuaded that a multi-target therapeutic strategy aimed at the simultaneous targeting of multiple proteins (and therefore etiologies) involved in the development of a disease is recommended in future.",2018-06-01,5,1953,69,1072
100,30209997,A Brief Survey of Machine Learning Application in Cancerlectin Identification,"Proteins with at least one carbohydrate recognition domain are lectins that can identify and reversibly interact with glycan moiety of glycoconjugates or a soluble carbohydrate. It has been proved that lectins can play various vital roles in mediating signal transduction, cell-cell recognition and interaction, immune defense, and so on. Most organisms can synthesize and secret lectins. A portion of lectins closely related to diverse cancers, called cancerlectins, are involved in tumor initiation, growth and recrudescence. Cancerlectins have been investigated for their applications in the laboratory study, clinical diagnosis and therapy, and drug delivery and targeting of cancers. The identification of cancerlectin genes from a lot of lectins is helpful for dissecting cancers. Several cancerlectin prediction tools based on machine learning approaches have been established and have become an excellent complement to experimental methods. In this review, we comprehensively summarize and expound the indispensable materials for implementing cancerlectin prediction models. We hope that this review will contribute to understanding cancerlectins and provide valuable clues for the study of cancerlectins. Novel systems for cancerlectin gene identification are expected to be developed for clinical applications and gene therapy.",2018-06-01,9,1337,77,1072
321,28728899,Machine learning-enabled discovery and design of membrane-active peptides,"Antimicrobial peptides are a class of membrane-active peptides that form a critical component of innate host immunity and possess a diversity of sequence and structure. Machine learning approaches have been profitably employed to efficiently screen sequence space and guide experiment towards promising candidates with high putative activity. In this mini-review, we provide an introduction to antimicrobial peptides and summarize recent advances in machine learning-enabled antimicrobial peptide discovery and design with a focus on a recent work Lee et al. Proc. Natl. Acad. Sci. USA 2016;113(48):13588-13593. This study reports the development of a support vector machine classifier to aid in the design of membrane active peptides. We use this model to discover membrane activity as a multiplexed function in diverse peptide families and provide interpretable understanding of the physicochemical properties and mechanisms governing membrane activity. Experimental validation of the classifier reveals it to have learned membrane activity as a unifying signature of antimicrobial peptides with diverse modes of action. Some of the discriminating rules by which it performs classification are in line with existing ""human learned"" understanding, but it also unveils new previously unknown determinants and multidimensional couplings governing membrane activity. Integrating machine learning with targeted experimentation can guide both antimicrobial peptide discovery and design and new understanding of the properties and mechanisms underpinning their modes of action.",2018-06-01,22,1572,73,1072
338,28662371,Modeling food matrix effects on chemical reactivity: Challenges and perspectives,"The same chemical reaction may be different in terms of its position of the equilibrium (i.e., thermodynamics) and its kinetics when studied in different foods. The diversity in the chemical composition of food and in its structural organization at macro-, meso-, and microscopic levels, that is, the food matrix, is responsible for this difference. In this viewpoint paper, the multiple, and interconnected ways the food matrix can affect chemical reactivity are summarized. Moreover, mechanistic and empirical approaches to explain and predict the effect of food matrix on chemical reactivity are described. Mechanistic models aim to quantify the effect of food matrix based on a detailed understanding of the chemical and physical phenomena occurring in food. Their applicability is limited at the moment to very simple food systems. Empirical modeling based on machine learning combined with data-mining techniques may represent an alternative, useful option to predict the effect of the food matrix on chemical reactivity and to identify chemical and physical properties to be further tested. In such a way the mechanistic understanding of the effect of the food matrix on chemical reactions can be improved.",2018-06-01,4,1213,80,1072
371,28481395,A Systematic Review on Machine Learning in Neurosurgery: The Future of Decision-Making in Patient Care,"Current practice of neurosurgery depends on clinical practice guidelines and evidence-based research publications that derive results using statistical methods. However, statistical analysis methods have some limitations such as the inability to analyze nonlinear variables, requiring setting a level of significance, being impractical for analyzing large amounts of data and the possibility of human bias. Machine learning is an emerging method for analyzing massive amounts of complex data which relies on algorithms that allow computers to learn and make accurate predictions. During the past decade, machine learning has been increasingly implemented in medical research as well as neurosurgical publications. This systematical review aimed to assemble the current neurosurgical literature that machine learning has been utilized, and to inform neurosurgeons on this novel method of data analysis.",2018-06-01,5,901,102,1072
2344,29476532,Research Imaging of Brain Structure and Function After Concussion,"Even when concussions are associated with prolonged physical and cognitive sequelae, concussions are typically ""invisible"" on diagnostic brain imaging, indicating that the neuropathology associated with concussion lies under the detection threshold of routine imaging. However, data from brain structural and functional research imaging studies using diffusion tensor imaging, resting-state functional magnetic resonance imaging, and brain perfusion imaging indicate that these imaging sequences have a role in identifying concussion-related neuropathology. These advanced imaging techniques provide insights into concussion neuropathology and might be useful for differentiating concussed patients from healthy controls. In this review article, we provide an overview of research findings from brain structural and functional imaging studies of concussion, and discuss the accuracy of classification models developed via machine-learning algorithms for identifying individual patients with concussion based on imaging data.",2018-06-01,5,1024,65,1072
398,30421670,Machine Learning Methods in Precision Medicine Targeting Epigenetic Diseases,"Background:                    On a tide of big data, machine learning is coming to its day. Referring to huge amounts of epigenetic data coming from biological experiments and clinic, machine learning can help in detecting epigenetic features in genome, finding correlations between phenotypes and modifications in histone or genes, accelerating the screen of lead compounds targeting epigenetics diseases and many other aspects around the study on epigenetics, which consequently realizes the hope of precision medicine.              Methods:                    In this minireview, we will focus on reviewing the fundamentals and applications of machine learning methods which are regularly used in epigenetics filed and explain their features. Their advantages and disadvantages will also be discussed.              Results:                    Machine learning algorithms have accelerated studies in precision medicine targeting epigenetics diseases.              Conclusion:                    In order to make full use of machine learning algorithms, one should get familiar with the pros and cons of them, which will benefit from big data by choosing the most suitable method(s).",2018-06-01,0,1185,76,1072
402,30417778,Convolutional Neural Networks for ATC Classification,"Background:                    Anatomical Therapeutic Chemical (ATC) classification of unknown compound has raised high significance for both drug development and basic research. The ATC system is a multi-label classification system proposed by the World Health Organization (WHO), which categorizes drugs into classes according to their therapeutic effects and characteristics. This system comprises five levels and includes several classes in each level; the first level includes 14 main overlapping classes. The ATC classification system simultaneously considers anatomical distribution, therapeutic effects, and chemical characteristics, the prediction for an unknown compound of its ATC classes is an essential problem, since such a prediction could be used to deduce not only a compound's possible active ingredients but also its therapeutic, pharmacological, and chemical properties. Nevertheless, the problem of automatic prediction is very challenging due to the high variability of the samples and the presence of overlapping among classes, resulting in multiple predictions and making machine learning extremely difficult.              Methods:                    In this paper, we propose a multi-label classifier system based on deep learned features to infer the ATC classification. The system is based on a 2D representation of the samples: first a 1D feature vector is obtained extracting information about a compound's chemical-chemical interaction and its structural and fingerprint similarities to other compounds belonging to the different ATC classes, then the original 1D feature vector is reshaped to obtain a 2D matrix representation of the compound. Finally, a convolutional neural network (CNN) is trained and used as a feature extractor. Two general purpose classifiers designed for multi-label classification are trained using the deep learned features and resulting scores are fused by the average rule.              Results:                    Experimental evaluation based on rigorous cross-validation demonstrates the superior prediction quality of this method compared to other state-of-the-art approaches developed for this problem.              Conclusion:                    Extensive experiments demonstrate that the new predictor, based on CNN, outperforms other existing predictors in the literature in almost all the five metrics used to examine the performance for multi-label systems, particularly in the ""absolute true"" rate and the ""absolute false"" rate, the two most significant indexes. Matlab code will be available at https://github.com/LorisNanni.",2018-06-01,3,2596,52,1072
421,30378021,Prediction of Peroxisomal Matrix Proteins in Plants,"Our knowledge of the proteome of plant peroxisomes is far from being complete, and the functional complexity and plasticity of this cell organelle are amazingly high particularly in plants, as exemplified by the model species Arabidopsis thaliana. Plant-specific peroxisome functions that have been uncovered only recently include, for instance, the participation of peroxisomes in phylloquinone and biotin biosynthesis. Experimental proteome studies have been proved very successful in defining the proteome of Arabidopsis peroxisomes but this approach also faces significant challenges and limitations. Complementary to experimental approaches, computational methods have emerged as important powerful tools to define the proteome of soluble matrix proteins of plant peroxisomes. Compared to other cell organelles such as mitochondria, plastids and the ER, the simultaneous operation of two major import pathways for soluble proteins in peroxisomes is rather atypical. Novel machine learning prediction approaches have been developed for peroxisome targeting signals type 1 (PTS1) and revealed high sensitivity and specificity, as validated by in vivo subcellular targeting analyses in diverse transient plant expression systems. Accordingly, the algorithms allow the correct prediction of many novel peroxisome-targeted proteins from plant genome sequences and the discovery of additional organelle functions. In contrast, the prediction of PTS2 proteins largely remains restricted to genome searches by conserved patterns contrary to more advanced machine learning methods. Here, we summarize and discuss the capabilities and accuracies of available prediction algorithms for PTS1 and PTS2 carrying proteins.",2018-06-01,2,1712,51,1072
462,30306468,Computer-Aided Orthopaedic Surgery: State-of-the-Art and Future Perspectives,"Introduced more than two decades ago, computer-aided orthopaedic surgery (CAOS) has emerged as a new and independent area, due to the importance of treatment of musculoskeletal diseases in orthopaedics and traumatology, increasing availability of different imaging modalities and advances in analytics and navigation tools. The aim of this chapter is to present the basic elements of CAOS devices and to review state-of-the-art examples of different imaging modalities used to create the virtual representations, of different position tracking devices for navigation systems, of different surgical robots, of different methods for registration and referencing, and of CAOS modules that have been realized for different surgical procedures. Future perspectives will be outlined. It is expected that the recent advancement on smart instrumentation, medical robotics, artificial intelligence, machine learning, and deep learning techniques, in combination with big data analytics, may lead to smart CAOS systems and intelligent orthopaedics in the near future.",2018-06-01,1,1057,76,1072
433,30360722,Elucidating Protein-protein Interactions Through Computational Approaches and Designing Small Molecule Inhibitors Against them for Various Diseases,"Background:                    To carry out wide range of cellular functionalities, proteins often associate with one or more proteins in a phenomenon known as Protein-Protein Interaction (PPI). Experimental and computational approaches were applied on PPIs in order to determine the interacting partners, and also to understand how an abnormality in such interactions can become the principle cause of a disease.              Objective:                    This review aims to elucidate the case studies where PPIs involved in various human diseases have been proven or validated with computational techniques, and also to elucidate how small molecule inhibitors of PPIs have been designed computationally to act as effective therapeutic measures against certain diseases.              Results:                    Computational techniques to predict PPIs are emerging rapidly in the modern day. They not only help in predicting new PPIs, but also generate outputs that substantiate the experimentally determined results. Moreover, computation has aided in the designing of novel inhibitor molecules disrupting the PPIs. Some of them are already being tested in the clinical trials.              Conclusion:                    This review delineated the classification of computational tools that are essential to investigate PPIs. Furthermore, the review shed light on how indispensable computational tools have become in the field of medicine to analyze the interaction networks and to design novel inhibitors efficiently against dreadful diseases in a shorter time span.",2018-06-01,1,1572,147,1072
2760,27842478,A Review of Computational Methods for Predicting Drug Targets,"Drug discovery and development is not only a time-consuming and labor-intensive process but also full of risk. Identifying targets of small molecules helps evaluate safety of drugs and find new therapeutic applications. The biotechnology measures a wide variety of properties related to drug and targets from different perspectives, thus generating a large body of data. This undoubtedly provides a solid foundation to explore relationships between drugs and targets. A large number of computational techniques have recently been developed for drug target prediction. In this paper, we summarize these computational methods and classify them into structure-based, molecular activity-based, side-effectbased and multi-omics-based predictions according to the used data for inference. The multi-omicsbased methods are further grouped into two types: classifier-based and network-based predictions. Furthermore the advantages and limitations of each type of methods are discussed. Finally, we point out the future directions of computational predictions for drug targets.",2018-06-01,1,1069,61,1072
2763,27829350,Drug-Target Interactions: Prediction Methods and Applications,"Identifying the interactions between drugs and target proteins is a key step in drug discovery. This not only aids to understand the disease mechanism, but also helps to identify unexpected therapeutic activity or adverse side effects of drugs. Hence, drug-target interaction prediction becomes an essential tool in the field of drug repurposing. The availability of heterogeneous biological data on known drug-target interactions enabled many researchers to develop various computational methods to decipher unknown drug-target interactions. This review provides an overview on these computational methods for predicting drug-target interactions along with available webservers and databases for drug-target interactions. Further, the applicability of drug-target interactions in various diseases for identifying lead compounds has been outlined.",2018-06-01,4,847,61,1072
2345,29476392,"Collaborative and Reproducible Research: Goals, Challenges, and Strategies","Combining imaging biomarkers with genomic and clinical phenotype data is the foundation of precision medicine research efforts. Yet, biomedical imaging research requires unique infrastructure compared with principally text-driven clinical electronic medical record (EMR) data. The issues are related to the binary nature of the file format and transport mechanism for medical images as well as the post-processing image segmentation and registration needed to combine anatomical and physiological imaging data sources. The SiiM Machine Learning Committee was formed to analyze the gaps and challenges surrounding research into machine learning in medical imaging and to find ways to mitigate these issues. At the 2017 annual meeting, a whiteboard session was held to rank the most pressing issues and develop strategies to meet them. The results, and further reflections, are summarized in this paper.",2018-06-01,2,901,74,1072
193,29998104,Machine Learning in Orthopedics: A Literature Review,"In this paper we present the findings of a systematic literature review covering the articles published in the last two decades in which the authors described the application of a machine learning technique and method to an orthopedic problem or purpose. By searching both in the Scopus and Medline databases, we retrieved, screened and analyzed the content of 70 journal articles, and coded these resources following an iterative method within a Grounded Theory approach. We report the survey findings by outlining the articles' content in terms of the main machine learning techniques mentioned therein, the orthopedic application domains, the source data and the quality of their predictive performance.",2018-06-01,14,706,52,1072
197,29992885,Toward Reproducible Results from Targeted Metabolomic Studies: Perspectives for Data Pre-processing and a Basis for Analytic Pipeline Development,"Contemporary metabolomics experiments generate a rich array of complex high-dimensional data. Consequently, there have been concurrent efforts to develop methodological standards and analytical workflows to streamline the generation of meaningful biochemical and clinical inferences from raw data generated using an analytical platform like mass spectrometry. While such considerations have been frequently addressed in untargeted metabolomics (i.e., the broad survey of all distinguishable metabolites within a sample of interest), this methodological scrutiny has seldom been applied to data generated using commercial, targeted metabolomics kits. We suggest that this may, in part, account for past and more recent incomplete replications of previously specified biomarker panels. Herein, we identify common impediments challenging the analysis of raw, targeted metabolomic abundance data from a commercial kit and review methods to remedy these issues. In doing so, we propose an analytical pipeline suitable for the pre-processing of data for downstream biomarker discovery. Operational and statistical considerations for integrating targeted data sets across experimental sites and analytical batches are discussed, as are best practices for developing predictive models relating pre-processed metabolomic data to associated phenotypic information.",2018-06-01,6,1354,145,1072
2946,29847214,The Human Vaccines Project: Towards a comprehensive understanding of the human immune response to immunization,"Although the success of vaccination to date has been unprecedented, our inadequate understanding of the details of the human immune response to immunization has resulted in several recent vaccine failures and significant delays in the development of high-need vaccines for global infectious diseases and cancer. Because of the need to better understand the immense complexity of the human immune system, the Human Vaccines Project was launched in 2015 with the mission to decode the human immune response to accelerate development of vaccines and immunotherapies for major diseases. The Project currently has three programs: 1) The Human Immunome Program, with the goal of deciphering the complete repertoire of B and T cell receptors across the human population, termed the Human Immunome, 2) The Rules of Immunogenicity Program, with the goal of understanding the key principles of how a vaccine elicits a protective and durable response using a system immunology approach, and 3) The Universal Influenza Vaccine Initiative (UIVI), with the goal of conducting experimental clinical trials to understand the influence of influenza pre-exposures on subsequent influenza immunization and the mechanisms of protection. Given the dramatic advances in computational and systems biology, genomics, immune monitoring, bioinformatics and machine learning, there is now an unprecedented opportunity to unravel the intricacies of the human immune response to immunization, ushering in a new era in vaccine development.",2018-06-01,3,1509,110,1072
173,30051792,Applications of Machine Learning Methods in Drug Toxicity Prediction,"Toxicity evaluation is an important part of the preclinical safety assessment of new drugs, which is directly related to human health and the fate of drugs. It is of importance to study how to evaluate drug toxicity accurately and economically. The traditional in vitro and in vivo toxicity tests are laborious, time-consuming, highly expensive, and even involve animal welfare issues. Computational methods developed for drug toxicity prediction can compensate for the shortcomings of traditional methods and have been considered useful in the early stages of drug development. Numerous drug toxicity prediction models have been developed using a variety of computational methods. With the advance of the theory of machine learning and molecular representation, more and more drug toxicity prediction models are developed using a variety of machine learning methods, such as support vector machine, random forest, naive Bayesian, back propagation neural network. And significant advances have been made in many toxicity endpoints, such as carcinogenicity, mutagenicity, and hepatotoxicity. In this review, we aimed to provide a comprehensive overview of the machine learning based drug toxicity prediction studies conducted in recent years. In addition, we compared the performance of the models proposed in these studies in terms of accuracy, sensitivity, and specificity, providing a view of the current state-of-the-art in this field and highlighting the issues in the current studies.",2018-06-01,9,1489,68,1072
174,30051410,"Big-Data Analysis, Cluster Analysis, and Machine-Learning Approaches","Medicine will experience many changes in the coming years because the so-called ""medicine of the future"" will be increasingly proactive, featuring four basic elements: predictive, personalized, preventive, and participatory. Drivers for these changes include the digitization of data in medicine and the availability of computational tools that deal with massive volumes of data. Thus, the need to apply machine-learning methods to medicine has increased dramatically in recent years while facing challenges related to an unprecedented large number of clinically relevant features and highly specific diagnostic tests. Advances regarding data-storage technology and the progress concerning genome studies have enabled collecting vast amounts of patient clinical details, thus permitting the extraction of valuable information. In consequence, big-data analytics is becoming a mandatory technology to be used in the clinical domain.Machine learning and big-data analytics can be used in the field of cardiology, for example, for the prediction of individual risk factors for cardiovascular disease, for clinical decision support, and for practicing precision medicine using genomic information. Several projects employ machine-learning techniques to address the problem of classification and prediction of heart failure (HF) subtypes and unbiased clustering analysis using dense phenomapping to identify phenotypically distinct HF categories. In this chapter, these ideas are further presented, and a computerized model allowing the distinction between two major HF phenotypes on the basis of ventricular-volume data analysis is discussed in detail.",2018-06-01,7,1648,68,1072
175,30050769,"Computer Aided Nodule Analysis and Risk Yield (CANARY) characterization of adenocarcinoma: radiologic biopsy, risk stratification and future directions","The majority of incidentally and screen-detected lung cancers are adenocarcinomas. Optimal management of these tumors is clinically challenging due to variability in tumor histopathology and behavior. Invasive adenocarcinoma (IA) is generally aggressive while adenocarcinoma in situ (AIS) and minimally invasive adenocarcinoma (MIA) may be extremely indolent. Computer Aided Nodule Analysis and Risk Yield (CANARY) is a quantitative computed tomography (CT) analysis tool that allows non-invasive assessment of tumor characteristics. This analysis may obviate the need for tissue biopsy and facilitate the risk stratification of adenocarcinoma of the lung. CANARY was developed by unsupervised machine learning techniques using CT data of histopathologically-characterized adenocarcinomas of the lung. This technique identified 9 distinct exemplars that constitute the spectrum of CT features found in adenocarcinoma of the lung. The distributions of these features in a nodule correlate with histopathology. Further automated clustering of CANARY nodules defined three distinct groups that have distinctly different post-resection disease free survival (DFS). CANARY has been validated within the NLST cohort and multiple other cohorts. Using semi-automated segmentation as input to CANARY, there is excellent repeatability and interoperator correlation of results. Confirmation and longitudinal tracking of indolent adenocarcinoma with CANARY may ultimately add decision support in nuanced cases where surgery may not be in the best interest of the patient due to competing comorbidity. Currently under investigation is CANARY's role in detecting differing driver mutations and tumor response to targeted chemotherapeutics. Combining the results from CANARY analysis with clinical information and other quantitative techniques such as analysis of the tumor-free surrounding lung may aid in building more powerful predictive models. The next step in CANARY investigation will be its prospective application, both in selecting low-risk stage 1 adenocarcinoma for active surveillance and investigation in selecting high-risk early stage adenocarcinoma for adjuvant therapy.",2018-06-01,1,2172,151,1072
176,30050768,Lung cancer prediction using machine learning and advanced imaging techniques,"Machine learning based lung cancer prediction models have been proposed to assist clinicians in managing incidental or screen detected indeterminate pulmonary nodules. Such systems may be able to reduce variability in nodule classification, improve decision making and ultimately reduce the number of benign nodules that are needlessly followed or worked-up. In this article, we provide an overview of the main lung cancer prediction approaches proposed to date and highlight some of their relative strengths and weaknesses. We discuss some of the challenges in the development and validation of such techniques and outline the path to clinical adoption.",2018-06-01,4,654,77,1072
598,29327813,Computational Tools for the Identification and Interpretation of Sequence Motifs in Immunopeptidomes,"Recent advances in proteomics and mass-spectrometry have widely expanded the detectable peptide repertoire presented by major histocompatibility complex (MHC) molecules on the cell surface, collectively known as the immunopeptidome. Finely characterizing the immunopeptidome brings about important basic insights into the mechanisms of antigen presentation, but can also reveal promising targets for vaccine development and cancer immunotherapy. This report describes a number of practical and efficient approaches to analyze immunopeptidomics data, discussing the identification of meaningful sequence motifs in various scenarios and considering current limitations. Guidelines are provided for the filtering of false hits and contaminants, and to address the problem of motif deconvolution in cell lines expressing multiple MHC alleles, both for the MHC class I and class II systems. Finally, it is demonstrated how machine learning can be readily employed by non-expert users to generate accurate prediction models directly from mass-spectrometry eluted ligand data sets.",2018-06-01,17,1074,100,1072
596,29344895,Bioinformatics Approaches to Predict Drug Responses from Genomic Sequencing,"Fulfilling the promises of precision medicine will depend on our ability to create patient-specific treatment regimens. Therefore, being able to translate genomic sequencing into predicting how a patient will respond to a given drug is critical. In this chapter, we review common bioinformatics approaches that aim to use sequencing data to predict sample-specific drug susceptibility. First, we explain the importance of customized drug regimens to the future of medical care. Second, we discuss the different public databases and community efforts that can be leveraged to develop new methods for identifying new predictive biomarkers. Third, we cover the basic methods that are currently used to identify markers or signatures of drug response, without any prior knowledge of the drug's mechanism of action. We further discuss how one can integrate knowledge about drug targets, mechanisms, and predictive markers to better estimate drug response in a diverse set of samples. We begin this section with a primer on popular methods to identify targets and mechanism of action for new small molecules. This discussion also includes a set of computational methods that incorporate other drug features, which do not relate to drug-induced genetic changes or sequencing data such as drug structures, side-effects, and efficacy profiles. Those additional drug properties can aid in gaining higher accuracy for the identification of drug target and mechanism of action. We then progress to discuss using these targets in combination with disease-specific expression patterns, known pathways, and genetic interaction networks to aid drug choice. Finally, we conclude this chapter with a general overview of machine learning methods that can integrate multiple pieces of sequencing data along with prior drug or biological knowledge to drastically improve response prediction.",2018-06-01,2,1870,75,1072
192,30008798,Information-Based Medicine in Glioma Patients: A Clinical Perspective,"Glioma constitutes the most common type of primary brain tumor with a dismal survival, often measured in terms of months or years. The thin line between treatment effectiveness and patient harm underpins the importance of tailoring clinical management to the individual patient. Randomized trials have laid the foundation for many neuro-oncological guidelines. Despite this, their findings focus on group-level estimates. Given our current tools, we are limited in our ability to guide patients on what therapy is best for them as individuals, or even how long they should expect to survive. Machine learning, however, promises to provide the analytical support for personalizing treatment decisions, and deep learning allows clinicians to unlock insight from the vast amount of unstructured data that is collected on glioma patients. Although these novel techniques have achieved astonishing results across a variety of clinical applications, significant hurdles remain associated with the implementation of them in clinical practice. Future challenges include the assembly of well-curated cross-institutional datasets, improvement of the interpretability of machine learning models, and balancing novel evidence-based decision-making with the associated liability of automated inference. Although artificial intelligence already exceeds clinical expertise in a variety of applications, clinicians remain responsible for interpreting the implications of, and acting upon, each prediction.",2018-06-01,2,1489,69,1072
2338,29488902,A review of classification algorithms for EEG-based brain-computer interfaces: a 10 year update,"Objective:                    Most current electroencephalography (EEG)-based brain-computer interfaces (BCIs) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify EEG signals in BCIs. The time is therefore ripe for an updated review of EEG classification algorithms for BCIs.              Approach:                    We surveyed the BCI and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design BCIs. We synthesize these studies in order to present such algorithms, to report how they were used for BCIs, what were the outcomes, and to identify their pros and cons.              Main results:                    We found that the recently designed classification algorithms for EEG-based BCIs can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple BCI problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art BCI methods.              Significance:                    This paper provides a comprehensive overview of the modern classification algorithms used in EEG-based BCIs, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance EEG classification in BCI.",2018-06-01,93,2158,95,1072
2917,29915783,Computational Chemical Synthesis Analysis and Pathway Design,"With the idea of retrosynthetic analysis, which was raised in the 1960s, chemical synthesis analysis and pathway design have been transformed from a complex problem to a regular process of structural simplification. This review aims to summarize the developments of computer-assisted synthetic analysis and design in recent years, and how machine-learning algorithms contributed to them. LHASA system started the pioneering work of designing semi-empirical reaction modes in computers, with its following rule-based and network-searching work not only expanding the databases, but also building new approaches to indicating reaction rules. Programs like ARChem Route Designer replaced hand-coded reaction modes with automatically-extracted rules, and programs like Chematica changed traditional designing into network searching. Afterward, with the help of machine learning, two-step models which combine reaction rules and statistical methods became the main stream. Recently, fully data-driven learning methods using deep neural networks which even do not require any prior knowledge, were applied into this field. Up to now, however, these methods still cannot replace experienced human organic chemists due to their relatively low accuracies. Future new algorithms with the aid of powerful computational hardware will make this topic promising and with good prospects.",2018-06-01,3,1372,60,1072
188,30013818,Development of a Behavioral Health Stigma Measure and Application of Machine Learning for Classification,"Objective: Given the growing public health importance of measuring the change in mental health stigma over time, the goal of this study was to demonstrate the potential for using machine learning as a tool to analyze patterns of social stigma as a complement to traditional research methods. Methods: A total of 1,904 participants were recruited through Sona Systems, Ltd (Tallinn, Estonia), an experiment management system for online research, to complete a self-reported survey. The collected data were used to develop a new measure of mental (behavioral) health stigma. To build a classification predictive model of stigma, a decision tree was used as the data mining tool, wherein a set of classification rules was generated and tested for its ability to examine the prevalence of stigma. Results: A three-factor stigma model was supported and confirmed. Results indicate that the measure is content-valid and internally consistent. Performance evaluation of the machine learning-based classification algorithm revealed a sufficient inter-rater reliability with a predictive accuracy of 92.4 percent. Conclusion: This study illustrates the potential for applying machine learning to derive a data-driven understanding of the extent to which stigma is prevalent in society. It establishes a framework for the development of an index to track stigma over time and to assist healthcare decision-makers with improving the health of populations and the experience of care for patients.",2018-06-01,1,1484,104,1072
2913,29928237,Neuromarkers for Mental Disorders: Harnessing Population Neuroscience,"Despite abundant research into the neurobiology of mental disorders, to date neurobiological insights have had very little impact on psychiatric diagnosis or treatment. In this review, we contend that the search for neuroimaging biomarkers-neuromarkers-of mental disorders is a highly promising avenue toward improved psychiatric healthcare. However, many of the traditional tools used for psychiatric neuroimaging are inadequate for the identification of neuromarkers. Specifically, we highlight the need for larger samples and for multivariate analysis. Approaches such as machine learning are likely to be beneficial for interrogating high-dimensional neuroimaging data. We suggest that broad, population-based study designs will be important for developing neuromarkers of mental disorders, and will facilitate a move away from a phenomenological definition of mental disorder categories and toward psychiatric nosology based on biological evidence. We provide an outline of how the development of neuromarkers should occur, emphasizing the need for tests of external and construct validity, and for collaborative research efforts. Finally, we highlight some concerns regarding the development, and use of, neuromarkers in psychiatric healthcare.",2018-06-01,6,1250,69,1072
186,30034462,Mining Big Neuron Morphological Data,"The advent of automatic tracing and reconstruction technology has led to a surge in the number of neurons 3D reconstruction data and consequently the neuromorphology research. However, the lack of machine-driven annotation schema to automatically detect the types of the neurons based on their morphology still hinders the development of this branch of science. Neuromorphology is important because of the interplay between the shape and functionality of neurons and the far-reaching impact on the diagnostics and therapeutics in neurological disorders. This survey paper provides a comprehensive research in the field of automatic neurons classification and presents the existing challenges, methods, tools, and future directions for automatic neuromorphology analytics. We summarize the major automatic techniques applicable in the field and propose a systematic data processing pipeline for automatic neuron classification, covering data capturing, preprocessing, analyzing, classification, and retrieval. Various techniques and algorithms in machine learning are illustrated and compared to the same dataset to facilitate ongoing research in the field.",2018-06-01,0,1156,36,1072
2952,29792109,Computational functional genomics-based approaches in analgesic drug discovery and repurposing,"Persistent pain is a major healthcare problem affecting a fifth of adults worldwide with still limited treatment options. The search for new analgesics increasingly includes the novel research area of functional genomics, which combines data derived from various processes related to DNA sequence, gene expression or protein function and uses advanced methods of data mining and knowledge discovery with the goal of understanding the relationship between the genome and the phenotype. Its use in drug discovery and repurposing for analgesic indications has so far been performed using knowledge discovery in gene function and drug target-related databases; next-generation sequencing; and functional proteomics-based approaches. Here, we discuss recent efforts in functional genomics-based approaches to analgesic drug discovery and repurposing and highlight the potential of computational functional genomics in this field including a demonstration of the workflow using a novel R library 'dbtORA'.",2018-06-01,6,999,94,1072
2949,29800865,"Cryptic binding sites on proteins: definition, detection, and druggability","Many proteins in their unbound structures lack surface pockets appropriately sized for drug binding. Hence, a variety of experimental and computational tools have been developed for the identification of cryptic sites that are not evident in the unbound protein but form upon ligand binding, and can provide tractable drug target sites. The goal of this review is to discuss the definition, detection, and druggability of such sites, and their potential value for drug discovery. Novel methods based on molecular dynamics simulations are particularly promising and yield a large number of transient pockets, but it has been shown that only a minority of such sites are generally capable of binding ligands with substantial affinity. Based on recent studies, current methodology can be improved by combining molecular dynamics with fragment docking and machine learning approaches.",2018-06-01,19,880,74,1072
2957,29787940,Survey on deep learning for radiotherapy,"More than 50% of cancer patients are treated with radiotherapy, either exclusively or in combination with other methods. The planning and delivery of radiotherapy treatment is a complex process, but can now be greatly facilitated by artificial intelligence technology. Deep learning is the fastest-growing field in artificial intelligence and has been successfully used in recent years in many domains, including medicine. In this article, we first explain the concept of deep learning, addressing it in the broader context of machine learning. The most common network architectures are presented, with a more specific focus on convolutional neural networks. We then present a review of the published works on deep learning methods that can be applied to radiotherapy, which are classified into seven categories related to the patient workflow, and can provide some insights of potential future applications. We have attempted to make this paper accessible to both radiotherapy and deep learning communities, and hope that it will inspire new collaborations between these two communities to develop dedicated radiotherapy applications.",2018-07-01,33,1135,40,1042
2943,29852943,PET/MRI Hybrid Systems,"Over the last decade, the combination of PET and MRI in one system has proven to be highly successful in basic preclinical research, as well as in clinical research. Nowadays, PET/MRI systems are well established in preclinical imaging and are progressing into clinical applications to provide further insights into specific diseases, therapeutic assessments, and biological pathways. Certain challenges in terms of hardware had to be resolved concurrently with the development of new techniques to be able to reach the full potential of both combined techniques. This review provides an overview of these challenges and describes the opportunities that simultaneous PET/MRI systems can exploit in comparison with stand-alone or other combined hybrid systems. New approaches were developed for simultaneous PET/MRI systems to correct for attenuation of 511 keV photons because MRI does not provide direct information on gamma photon attenuation properties. Furthermore, new algorithms to correct for motion were developed, because MRI can accurately detect motion with high temporal resolution. The additional information gained by the MRI can be employed to correct for partial volume effects as well. The development of new detector designs in combination with fast-decaying scintillator crystal materials enabled time-of-flight detection and incorporation in the reconstruction algorithms. Furthermore, this review lists the currently commercially available systems both for preclinical and clinical imaging and provides an overview of applications in both fields. In this regard, special emphasis has been placed on data analysis and the potential for both modalities to evolve with advanced image analysis tools, such as cluster analysis and machine learning.",2018-07-01,8,1764,22,1042
2390,29352006,Machine learning in cardiovascular medicine: are we there yet?,"Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.",2018-07-01,53,1752,62,1042
2940,29859766,"Computational morphogenesis - Embryogenesis, cancer research and digital pathology","Overall survival benefits of cancer therapies have, in general, fallen short of what was expected from them. By examining the many parallels that exist between embryogenesis and carcinogenesis, we discuss how clinical and fundamental cancer research can benefit from computational morphogenesis (CM) insofar as carcinogenesis is causally related to altered mechanisms underlying embryogenesis and post-embryonic tissue homeostasis. We also discuss about the critical role played by digital pathology (DP) since it constitutes the main source of data for the model-fitting and validation of CM-generated virtual tissues. Conversely, we outline how CM can provide support to DP by generating annotated synthetic 2D and 3D data that can be fed into machine-learning methods dedicated to the automated diagnosis of DP slides.",2018-07-01,1,821,82,1042
2979,29724864,Empowering thyroid hormone research in human subjects using OMICs technologies,"OMICs subsume different physiological layers including the genome, transcriptome, proteome and metabolome. Recent advances in analytical techniques allow for the exhaustive determination of biomolecules in all OMICs levels from less invasive human specimens such as blood and urine. Investigating OMICs in deeply characterized population-based or experimental studies has led to seminal improvement of our understanding of genetic determinants of thyroid function, identified putative thyroid hormone target genes and thyroid hormone-induced shifts in the plasma protein and metabolite content. Consequently, plasma biomolecules have been suggested as surrogates of tissue-specific action of thyroid hormones. This review provides a brief introduction to OMICs in thyroid research with a particular focus on metabolomics studies in humans elucidating the important role of thyroid hormones for whole body metabolism in adults.",2018-07-01,3,926,78,1042
2935,29871778,Internet of Health Things: Toward intelligent vital signs monitoring in hospital wards,"Background:                    Large amounts of patient data are routinely manually collected in hospitals by using standalone medical devices, including vital signs. Such data is sometimes stored in spreadsheets, not forming part of patients' electronic health records, and is therefore difficult for caregivers to combine and analyze. One possible solution to overcome these limitations is the interconnection of medical devices via the Internet using a distributed platform, namely the Internet of Things. This approach allows data from different sources to be combined in order to better diagnose patient health status and identify possible anticipatory actions.              Methods:                    This work introduces the concept of the Internet of Health Things (IoHT), focusing on surveying the different approaches that could be applied to gather and combine data on vital signs in hospitals. Common heuristic approaches are considered, such as weighted early warning scoring systems, and the possibility of employing intelligent algorithms is analyzed.              Results:                    As a result, this article proposes possible directions for combining patient data in hospital wards to improve efficiency, allow the optimization of resources, and minimize patient health deterioration.              Conclusion:                    It is concluded that a patient-centered approach is critical, and that the IoHT paradigm will continue to provide more optimal solutions for patient management in hospital wards.",2018-07-01,6,1534,86,1042
2526,30576418,Research progress in protein posttranslational modification site prediction,"Posttranslational modifications (PTMs) play an important role in regulating protein folding, activity and function and are involved in almost all cellular processes. Identification of PTMs of proteins is the basis for elucidating the mechanisms of cell biology and disease treatments. Compared with the laboriousness of equivalent experimental work, PTM prediction using various machine-learning methods can provide accurate, simple and rapid research solutions and generate valuable information for further laboratory studies. In this review, we manually curate most of the bioinformatics tools published since 2008. We also summarize the approaches for predicting ubiquitination sites and glycosylation sites. Moreover, we discuss the challenges of current PTM bioinformatics tools and look forward to future research possibilities.",2018-07-01,3,834,75,1042
2962,29771496,What Can Pleiotropic Proteins in Innate Immunity Teach Us about Bioconjugation and Molecular Design?,"A common bioengineering strategy to add function to a given molecule is by conjugation of a new moiety onto that molecule. Adding multiple functions in this way becomes increasingly challenging and leads to composite molecules with larger molecular weights. In this review, we attempt to gain a new perspective by looking at this problem in reverse, by examining nature's strategies of multiplexing different functions into the same pleiotropic molecule using emerging analysis techniques such as machine learning. We concentrate on examples from the innate immune system, which employs a finite repertoire of molecules for a broad range of tasks. An improved understanding of how diverse functions are multiplexed into a single molecule can inspire new approaches for the deterministic design of multifunctional molecules.",2018-07-01,6,823,100,1042
2376,29389679,Artificial Intelligence in Surgery: Promises and Perils,"Objective:                    The aim of this review was to summarize major topics in artificial intelligence (AI), including their applications and limitations in surgery. This paper reviews the key capabilities of AI to help surgeons understand and critically evaluate new AI applications and to contribute to new developments.              Summary background data:                    AI is composed of various subfields that each provide potential solutions to clinical problems. Each of the core subfields of AI reviewed in this piece has also been used in other industries such as the autonomous car, social networks, and deep learning computers.              Methods:                    A review of AI papers across computer science, statistics, and medical sources was conducted to identify key concepts and techniques within AI that are driving innovation across industries, including surgery. Limitations and challenges of working with AI were also reviewed.              Results:                    Four main subfields of AI were defined: (1) machine learning, (2) artificial neural networks, (3) natural language processing, and (4) computer vision. Their current and future applications to surgical practice were introduced, including big data analytics and clinical decision support systems. The implications of AI for surgeons and the role of surgeons in advancing the technology to optimize clinical effectiveness were discussed.              Conclusions:                    Surgeons are well positioned to help integrate AI into modern practice. Surgeons should partner with data scientists to capture data across phases of care and to provide clinical context, for AI has the potential to revolutionize the way surgery is taught and practiced with the promise of a future optimized for the highest quality patient care.",2018-07-01,51,1836,55,1042
2928,29882974,Neuroimaging Studies Illustrate the Commonalities Between Ageing and Brain Diseases,"The lack of specificity in neuroimaging studies of neurological and psychiatric diseases suggests that these different diseases have more in common than is generally considered. Potentially, features that are secondary effects of different pathological processes may share common neurobiological underpinnings. Intriguingly, many of these mechanisms are also observed in studies of normal (i.e., non-pathological) brain ageing. Different brain diseases may be causing premature or accelerated ageing to the brain, an idea that is supported by a line of ""brain ageing"" research that combines neuroimaging data with machine learning analysis. In reviewing this field, I conclude that such observations could have important implications, suggesting that we should shift experimental paradigm: away from characterizing the average case-control brain differences resulting from a disease toward methods that place individuals in their age-appropriate context. This will also lead naturally to clinical applications, whereby neuroimaging can contribute to a personalized-medicine approach to improve brain health.",2018-07-01,4,1107,83,1042
2953,29791959,"Sarcopenia: Beyond Muscle Atrophy and into the New Frontiers of Opportunistic Imaging, Precision Medicine, and Machine Learning","As populations continue to age worldwide, the impact of sarcopenia on public health will continue to grow. The clinically relevant and increasingly common diagnosis of sarcopenia is at the confluence of three tectonic shifts in medicine: opportunistic imaging, precision medicine, and machine learning. This review focuses on the state-of-the-art imaging of sarcopenia and provides context for such imaging by discussing the epidemiology, pathophysiology, consequences, and future directions in the field of sarcopenia.",2018-07-01,18,519,127,1042
2301,29656681,Progress with modeling activity landscapes in drug discovery,"Activity landscapes (ALs) are representations and models of compound data sets annotated with a target-specific activity. In contrast to quantitative structure-activity relationship (QSAR) models, ALs aim at characterizing structure-activity relationships (SARs) on a large-scale level encompassing all active compounds for specific targets. The popularity of AL modeling has grown substantially with the public availability of large activity-annotated compound data sets. AL modeling crucially depends on molecular representations and similarity metrics used to assess structural similarity. Areas covered: The concepts of AL modeling are introduced and its basis in quantitatively assessing molecular similarity is discussed. The different types of AL modeling approaches are introduced. AL designs can broadly be divided into three categories: compound-pair based, dimensionality reduction, and network approaches. Recent developments for each of these categories are discussed focusing on the application of mathematical, statistical, and machine learning tools for AL modeling. AL modeling using chemical space networks is covered in more detail. Expert opinion: AL modeling has remained a largely descriptive approach for the analysis of SARs. Beyond mere visualization, the application of analytical tools from statistics, machine learning and network theory has aided in the sophistication of AL designs and provides a step forward in transforming ALs from descriptive to predictive tools. To this end, optimizing representations that encode activity relevant features of molecules might prove to be a crucial step.",2018-07-01,4,1623,60,1042
180,30049182,Diagnostic Accuracy of Different Machine Learning Algorithms for Breast Cancer Risk Calculation: a Meta-Analysis,"Objective: The aim of this study was to determine the diagnostic accuracy of different machine learning algorithms for breast cancer risk calculation. Methods: A meta-analysis was conducted of published research articles on diagnostic test accuracy of different machine learning algorithms for breast cancer risk calculation published between January 2000 and May 2018 in the online article databases of PubMed, ProQuest and EBSCO. Paired forest plots were employed for the analysis. Numerical values for sensitivity and specificity were obtained from false negative (FN), false positive (FP), true negative (TN) and true positive (TP) rates, presented alongside graphical representations with boxes marking the values and horizontal lines showing the confidence intervals (CIs). Summary receiver operating characteristic (SROC) curves were applied to assess the performance of diagnostic tests. Data were processed using Review Manager 5.3 (RevMan 5.3). Results: A total of 1,879 articles were reviewed, of which 11 were selected for systematic review and meta-analysis. Fve algorithms for machine learning able to predict breast cancer risk were identified: Super Vector Machine (SVM); Artificial Neural Networks (ANN); Decision Tree (DT); Naive Bayes (NB); and K-Nearest Neighbor (KNN). With the SVM, the Area Under Curve (AUC) from the SROC was > 90%, therefore classified into the excellent category. Conclusion: The meta-analysis confirmed that the SVM algorithm is able to calculate breast cancer risk with better accuracy value than other machine learning algorithms.",2018-07-01,12,1575,112,1042
152,30090035,What's app? Electronic health technology in inflammatory bowel disease,"Electronic health (eHealth) data collection is increasingly used in many chronic illnesses, to track pattern of disease. eHealth systems have the potential to revolutionize care. Inflammatory bowel disease (IBD) is a paradigm for such an approach: this is a chronic disease that usually affects young and technologically literate patient population, who are motivated to be involved in their own care. A range of eHealth technologies are available for IBD. This review considers the strengths and weaknesses of 7 platforms that focus on patient-provider interaction. These have been developed in Denmark, United States, the Netherlands, and the United Kingdom, demonstrating an international interest in this form of technology and interaction. Not only do these technologies aim to improve care but they also have the potential to collect large amounts of information. Information includes demographics and patient reported outcomes (symptoms, quality of life), quality of care (steroid use, among other metrics) and outcomes such as hospitalization. These data could inform quality improvement programmes to improve their focus. eHealth technology is also open to machine learning to analyze large data sets, through which personalized algorithms may be developed.",2018-07-01,4,1266,70,1042
376,28436590,Identification of small molecules using accurate mass MS/MS search,"Tandem mass spectral library search (MS/MS) is the fastest way to correctly annotate MS/MS spectra from screening small molecules in fields such as environmental analysis, drug screening, lipid analysis, and metabolomics. The confidence in MS/MS-based annotation of chemical structures is impacted by instrumental settings and requirements, data acquisition modes including data-dependent and data-independent methods, library scoring algorithms, as well as post-curation steps. We critically discuss parameters that influence search results, such as mass accuracy, precursor ion isolation width, intensity thresholds, centroiding algorithms, and acquisition speed. A range of publicly and commercially available MS/MS databases such as NIST, MassBank, MoNA, LipidBlast, Wiley MSforID, and METLIN are surveyed. In addition, software tools including NIST MS Search, MS-DIAL, Mass Frontier, SmileMS, Mass++, and XCMS2 to perform fast MS/MS search are discussed. MS/MS scoring algorithms and challenges during compound annotation are reviewed. Advanced methods such as the in silico generation of tandem mass spectra using quantum chemistry and machine learning methods are covered. Community efforts for curation and sharing of tandem mass spectra that will allow for faster distribution of scientific discoveries are discussed.",2018-07-01,61,1326,66,1042
177,30050439,The Current State and Future of CRISPR-Cas9 gRNA Design Tools,"Recent years have seen the development of computational tools to assist researchers in performing CRISPR-Cas9 experiment optimally. More specifically, these tools aim to maximize on-target activity (guide efficiency) while also minimizing potential off-target effects (guide specificity) by analyzing the features of the target site. Nonetheless, currently available tools cannot robustly predict experimental success as prediction accuracy depends on the approximations of the underlying model and how closely the experimental setup matches the data the model was trained on. Here, we present an overview of the available computational tools, their current limitations and future considerations. We discuss new trends around personalized health by taking genomic variants into account when predicting target sites as well as discussing other governing factors that can improve prediction accuracy.",2018-07-01,21,898,61,1042
194,29996520,A Meta-Prediction of Methylenetetrahydrofolate-Reductase Polymorphisms and Air Pollution Increased the Risk of Ischemic Heart Diseases Worldwide,"Ischemic heart disease (IHD) is among the leading causes of death worldwide. Methylenetetrahydrofolate reductase (MTHFR) polymorphisms have been associated with IHD risk, but the findings presented with heterogeneity. The purpose of the present meta-analysis was to provide an updated evaluation by integrating machine-learning based analytics to examine the potential source of heterogeneity on the associations between MTHFR polymorphisms and the risk of various subtypes of IHD, as well as the possible impact of air pollution on MTHFR polymorphisms and IHD risks. A comprehensive search of various databases was conducted to locate 123 studies (29,697 cases and 31,028 controls) for MTHFR C677T, and 18 studies (7158 cases and 5482 controls) for MTHFR A1298C. Overall, MTHFR 677 polymorphisms were risks for IHD (TT: Risk ratio (RR) = 1.23, p < 0.0001; CT: RR = 1.04, p = 0.0028, and TT plus CT: RR = 1.09, p < 0.0001). In contrast, MTHFR 677 CC wildtype was protective against IHD (RR = 0.91, p < 0.00001) for overall populations. Three countries with elevated IHD risks from MTHFR C677T polymorphism with RR >2 included India, Turkey, and Tunisia. Meta-predictive analysis revealed that increased air pollution was associated with increased MTHFR 677 TT and CT polymorphisms in both the case and control group (p < 0.05), with the trend of increased IHD risk resulting from increased air pollution. These results associate the potential inflammatory pathway with air pollution and the folate pathway with MTHFR polymorphism. Future intervention studies can be designed to mitigate MTHFR enzyme deficiencies resulting from gene polymorphisms to prevent IHDs for at-risk populations.",2018-07-01,2,1687,144,1042
605,29305324,Real-world outcomes in patients with neovascular age-related macular degeneration treated with intravitreal vascular endothelial growth factor inhibitors,"Clinical trials identified intravitreal vascular endothelial growth factor inhibitors (anti-VEGF agents) have the potential to stabilise or even improve visual acuity outcomes in neovascular age-related macular degeneration (AMD), a sight-threatening disease. Real-world evidence allows us to assess whether results from randomised controlled trials can be applied to the general population. We describe the development of global registries, in particular the Fight Retinal Blindness! registry that originated in Australia, the United Kingdom AMD Electronic Medical Records User Group and the IRIS registry in the USA. Real-world observations relating to efficacy, safety and resource utilisation of intravitreal anti-VEGF therapy for neovascular AMD are then summarised. Novel observations that would have been challenging to identify in a clinical trial setting are then highlighted, including the risk of late disease reactivation, outcomes in second versus first treated eyes, and the increased risk of posterior capsular rupture during cataract surgery in patients who have received intravitreal anti-VEGF therapy. We conclude by exploring future directions in the field. This includes the development of a global consensus on real-world outcome measures to allow greater comparison of results. Real-world neovascular AMD outcome registries can be linked with other databases to determine systemic safety or genetic predictors of treatment efficacy. Machine learning offers opportunities to extract useful insights from ""Big Data"" often collected in these registries. Real-world registries could be used by drug regulatory authorities and industry as an alternative to more costly and time-consuming phase 4 clinical trials, potentially allowing medication costs to be based on outcomes achieved.",2018-07-01,39,1801,153,1042
2305,29629796,Advances in Pancreatic CT Imaging,"Objective:                    The purpose of this article is to discuss the advances in CT acquisition and image postprocessing as they apply to imaging the pancreas and to conceptualize the role of radiogenomics and machine learning in pancreatic imaging.              Conclusion:                    CT is the preferred imaging modality for assessment of pancreatic diseases. Recent advances in CT (dual-energy CT, CT perfusion, CT volumetry, and radiogenomics) and emerging computational algorithms (machine learning) have the potential to further increase the value of CT in pancreatic imaging.",2018-07-01,7,597,33,1042
624,29185073,Computational systems biology approaches for Parkinson's disease,"Parkinson's disease (PD) is a prime example of a complex and heterogeneous disorder, characterized by multifaceted and varied motor- and non-motor symptoms and different possible interplays of genetic and environmental risk factors. While investigations of individual PD-causing mutations and risk factors in isolation are providing important insights to improve our understanding of the molecular mechanisms behind PD, there is a growing consensus that a more complete understanding of these mechanisms will require an integrative modeling of multifactorial disease-associated perturbations in molecular networks. Identifying and interpreting the combinatorial effects of multiple PD-associated molecular changes may pave the way towards an earlier and reliable diagnosis and more effective therapeutic interventions. This review provides an overview of computational systems biology approaches developed in recent years to study multifactorial molecular alterations in complex disorders, with a focus on PD research applications. Strengths and weaknesses of different cellular pathway and network analyses, and multivariate machine learning techniques for investigating PD-related omics data are discussed, and strategies proposed to exploit the synergies of multiple biological knowledge and data sources. A final outlook provides an overview of specific challenges and possible next steps for translating systems biology findings in PD to new omics-based diagnostic tools and targeted, drug-based therapeutic approaches.",2018-07-01,3,1524,64,1042
2889,29986160,Computational Principles of Supervised Learning in the Cerebellum,"Supervised learning plays a key role in the operation of many biological and artificial neural networks. Analysis of the computations underlying supervised learning is facilitated by the relatively simple and uniform architecture of the cerebellum, a brain area that supports numerous motor, sensory, and cognitive functions. We highlight recent discoveries indicating that the cerebellum implements supervised learning using the following organizational principles: ( a) extensive preprocessing of input representations (i.e., feature engineering), ( b) massively recurrent circuit architecture, ( c) linear input-output computations, ( d) sophisticated instructive signals that can be regulated and are predictive, ( e) adaptive mechanisms of plasticity with multiple timescales, and ( f) task-specific hardware specializations. The principles emerging from studies of the cerebellum have striking parallels with those in other brain areas and in artificial neural networks, as well as some notable differences, which can inform future research on supervised learning and inspire next-generation machine-based algorithms.",2018-07-01,30,1123,65,1042
191,30011882,Computational Methodologies in the Exploration of Marine Natural Product Leads,"Computational methodologies are assisting the exploration of marine natural products (MNPs) to make the discovery of new leads more efficient, to repurpose known MNPs, to target new metabolites on the basis of genome analysis, to reveal mechanisms of action, and to optimize leads. In silico efforts in drug discovery of NPs have mainly focused on two tasks: dereplication and prediction of bioactivities. The exploration of new chemical spaces and the application of predicted spectral data must be included in new approaches to select species, extracts, and growth conditions with maximum probabilities of medicinal chemistry novelty. In this review, the most relevant current computational dereplication methodologies are highlighted. Structure-based (SB) and ligand-based (LB) chemoinformatics approaches have become essential tools for the virtual screening of NPs either in small datasets of isolated compounds or in large-scale databases. The most common LB techniques include Quantitative StructureActivity Relationships (QSAR), estimation of drug likeness, prediction of adsorption, distribution, metabolism, excretion, and toxicity (ADMET) properties, similarity searching, and pharmacophore identification. Analogously, molecular dynamics, docking and binding cavity analysis have been used in SB approaches. Their significance and achievements are the main focus of this review.",2018-07-01,20,1391,78,1042
189,30013400,Automated data-adaptive analytics for electronic healthcare data to study causal treatment effects,"Background:                    Decision makers in health care increasingly rely on nonrandomized database analyses to assess the effectiveness, safety, and value of medical products. Health care data scientists use data-adaptive approaches that automatically optimize confounding control to study causal treatment effects. This article summarizes relevant experiences and extensions.              Methods:                    The literature was reviewed on the uses of high-dimensional propensity score (HDPS) and related approaches for health care database analyses, including methodological articles on their performance and improvement. Articles were grouped into applications, comparative performance studies, and statistical simulation experiments.              Results:                    The HDPS algorithm has been referenced frequently with a variety of clinical applications and data sources from around the world. The appeal of HDPS for database research rests in 1) its superior performance in situations of unobserved confounding through proxy adjustment, 2) its predictable efficiency in extracting confounding information from a given data source, 3) its ability to automate estimation of causal treatment effects to the extent achievable in a given data source, and 4) its independence of data source and coding system. Extensions of the HDPS approach have focused on improving variable selection when exposure is sparse, using free text information and time-varying confounding adjustment.              Conclusion:                    Semiautomated and optimized confounding adjustment in health care database analyses has proven successful across a wide range of settings. Machine-learning extensions further automate its use in estimating causal treatment effects across a range of data scenarios.",2018-07-01,7,1814,98,1042
487,30240646,Data and Power Efficient Intelligence with Neuromorphic Learning Machines,"The success of deep networks and recent industry involvement in brain-inspired computing is igniting a widespread interest in neuromorphic hardware that emulates the biological processes of the brain on an electronic substrate. This review explores interdisciplinary approaches anchored in machine learning theory that enable the applicability of neuromorphic technologies to real-world, human-centric tasks. We find that (1) recent work in binary deep networks and approximate gradient descent learning are strikingly compatible with a neuromorphic substrate; (2) where real-time adaptability and autonomy are necessary, neuromorphic technologies can achieve significant advantages over main-stream ones; and (3) challenges in memory technologies, compounded by a tradition of bottom-up approaches in the field, block the road to major breakthroughs. We suggest that a neuromorphic learning framework, tuned specifically for the spatial and temporal constraints of the neuromorphic substrate, will help guiding hardware algorithm co-design and deploying neuromorphic hardware for proactive learning of real-world data.",2018-07-01,8,1119,73,1042
144,30101124,The Role of Machine Learning in Knowledge-Based Response-Adapted Radiotherapy,"With the continuous increase in radiotherapy patient-specific data from multimodality imaging and biotechnology molecular sources, knowledge-based response-adapted radiotherapy (KBR-ART) is emerging as a vital area for radiation oncology personalized treatment. In KBR-ART, planned dose distributions can be modified based on observed cues in patients' clinical, geometric, and physiological parameters. In this paper, we present current developments in the field of adaptive radiotherapy (ART), the progression toward KBR-ART, and examine several applications of static and dynamic machine learning approaches for realizing the KBR-ART framework potentials in maximizing tumor control and minimizing side effects with respect to individual radiotherapy patients. Specifically, three questions required for the realization of KBR-ART are addressed: (1) what knowledge is needed; (2) how to estimate RT outcomes accurately; and (3) how to adapt optimally. Different machine learning algorithms for KBR-ART application shall be discussed and contrasted. Representative examples of different KBR-ART stages are also visited.",2018-07-01,6,1121,77,1042
659,29049075,Anesthesia Information Management Systems,"Anesthesia information management systems (AIMS) have evolved from simple, automated intraoperative record keepers in a select few institutions to widely adopted, sophisticated hardware and software solutions that are integrated into a hospital's electronic health record system and used to manage and document a patient's entire perioperative experience. AIMS implementations have resulted in numerous billing, research, and clinical benefits, yet there remain challenges and areas of potential improvement to AIMS utilization. This article provides an overview of the history of AIMS, the components and features of AIMS, and the benefits and challenges associated with implementing and using AIMS. As AIMS continue to proliferate and data are increasingly shared across multi-institutional collaborations, visual analytics and advanced analytics techniques such as machine learning may be applied to AIMS data to reap even more benefits.",2018-07-01,2,940,41,1042
2901,31975919,Digital Technologies in Psychiatry: Present and Future,"The digital revolution has reached the world of mental health. Prominent examples include the rapidly growing use of mobile health apps, the integration of sophisticated machine learning or artificial intelligence for clinical decision support and automated therapy, and the incorporation of virtual reality-based treatments. These diverse technologies hold the promise of addressing several important problems in mental health care, including lack of measurement, uneven access to clinicians, delay in receiving care, fragmentation of care, and negative attitudes toward psychiatry. Here, the authors summarize the current and swiftly changing state of digital mental health. Specifically, they highlight the current unmet needs that emerging technologies may be able to address; summarize what digital health can offer for assessment, treatment, and care integration; and describe some of the challenges and some new directions for innovations in this field. The review concludes with guidance for clinicians to integrate digital technologies into their work and to provide responsible and useful advice to their patients.",2018-07-01,5,1124,54,1042
109,30190664,Unraveling the bioactivity of anticancer peptides as deduced from machine learning,"Cancer imposes a global health burden as it represents one of the leading causes of morbidity and mortality while also giving rise to significant economic burden owing to the associated expenditures for its monitoring and treatment. In spite of advancements in cancer therapy, the low success rate and recurrence of tumor has necessitated the ongoing search for new therapeutic agents. Aside from drugs based on small molecules and protein-based biopharmaceuticals, there has been an intense effort geared towards the development of peptide-based therapeutics owing to its favorable and intrinsic properties of being relatively small, highly selective, potent, safe and low in production costs. In spite of these advantages, there are several inherent weaknesses that are in need of attention in the design and development of therapeutic peptides. An abundance of data on bioactive and therapeutic peptides have been accumulated over the years and the burgeoning area of artificial intelligence has set the stage for the lucrative utilization of machine learning to make sense of these large and high-dimensional data. This review summarizes the current state-of-the-art on the application of machine learning for studying the bioactivity of anticancer peptides along with future outlook of the field. Data and R codes used in the analysis herein are available on GitHub at https://github.com/Shoombuatong2527/anticancer-peptides-review.",2018-07-01,20,1437,82,1042
161,30082061,Using Technology to Inform and Deliver Precise Personalized Care to Patients With End-Stage Kidney Disease,"Consistent with the increase of precision medicine, the care of patients with end-stage kidney disease (ESKD) requiring maintenance dialysis therapy should evolve to become more personalized. Precise and personalized care is nuanced and informed by a number of factors including an individual's needs and preferences, disease progression, and response to and tolerance of treatments. Technology can support the delivery of more precise and personalized care through multiple mechanisms, including more accurate and real-time assessments of key care elements, enhanced treatment monitoring, and remote monitoring of home dialysis therapies. Data from health care and non-health care sources and advanced analytical methods such as machine learning can be used to create novel insights, and large volumes of data can be integrated to support clinical decisions. Health care models continue to evolve and the opportunities and need for novel care approaches supported by technology and health informatics continue to expand as the delivery and organization of health care changes. Ultimately, precise personalized care for ESKD, including dialysis therapy, will become more feasible as the biological, social, and environmental determinants of health are more broadly understood and as advances in science, engineering, and information management create the means to provide truly precise care for ESKD.",2018-07-01,5,1400,106,1042
181,30048614,"Representation, Pattern Information, and Brain Signatures: From Neurons to Neuroimaging","Human neuroimaging research has transitioned from mapping local effects to developing predictive models of mental events that integrate information distributed across multiple brain systems. Here we review work demonstrating how multivariate predictive models have been utilized to provide quantitative, falsifiable predictions; establish mappings between brain and mind with larger effects than traditional approaches; and help explain how the brain represents mental constructs and processes. Although there is increasing progress toward the first two of these goals, models are only beginning to address the latter objective. By explicitly identifying gaps in knowledge, research programs can move deliberately and programmatically toward the goal of identifying brain representations underlying mental states and processes.",2018-07-01,25,827,87,1042
2907,29953247,The current limits in virtual screening and property prediction,"Beyond finding inhibitors that show high binding affinity to the respective target, there is the challenge of optimizing their properties with respect to metabolic and toxicological issues, as well as further off-target effects. To reduce the experimental effort of synthesizing and testing actual substances in corresponding assays, virtual screening has become an indispensable toolbox in preclinical development. The scope of application covers the prediction of molecular properties including solubility, metabolic liability and binding to antitargets, such as the hERG channel. Furthermore, prediction of binding sites and drugable targets are emerging aspects of virtual screening. Issues involved with the currently applied computational models including machine learning algorithms are outlined, such as limitations to the accuracy of prediction and overfitting.",2018-07-01,7,870,63,1042
178,30049875,Inverse molecular design using machine learning: Generative models for matter engineering,"The discovery of new materials can bring enormous societal and technological progress. In this context, exploring completely the large space of potential materials is computationally intractable. Here, we review methods for achieving inverse design, which aims to discover tailored materials from the starting point of a particular desired functionality. Recent advances from the rapidly growing field of artificial intelligence, mostly from the subfield of machine learning, have resulted in a fertile exchange of ideas, where approaches to inverse molecular design are being proposed and employed at a rapid pace. Among these, deep generative models have been applied to numerous classes of materials: rational design of prospective drugs, synthetic routes to organic compounds, and optimization of photovoltaics and redox flow batteries, as well as a variety of other solid-state materials.",2018-07-01,74,893,89,1042
2893,29980865,Future Direction for Using Artificial Intelligence to Predict and Manage Hypertension,"Purpose of review:                    Evidence that artificial intelligence (AI) is useful for predicting risk factors for hypertension and its management is emerging. However, we are far from harnessing the innovative AI tools to predict these risk factors for hypertension and applying them to personalized management. This review summarizes recent advances in the computer science and medical field, illustrating the innovative AI approach for potential prediction of early stages of hypertension. Additionally, we review ongoing research and future implications of AI in hypertension management and clinical trials, with an eye towards personalized medicine.              Recent findings:                    Although recent studies demonstrate that AI in hypertension research is feasible and possibly useful, AI-informed care has yet to transform blood pressure (BP) control. This is due, in part, to lack of data on AI's consistency, accuracy, and reliability in the BP sphere. However, many factors contribute to poorly controlled BP, including biological, environmental, and lifestyle issues. AI allows insight into extrapolating data analytics to inform prescribers and patients about specific factors that may impact their BP control. To date, AI has been mainly used to investigate risk factors for hypertension, but has not yet been utilized for hypertension management due to the limitations of study design and of physician's engagement in computer science literature. The future of AI with more robust architecture using multi-omics approaches and wearable technology will likely be an important tool allowing to incorporate biological, lifestyle, and environmental factors into decision-making of appropriate drug use for BP control.",2018-07-01,11,1749,85,1042
1043,31496926,Inter-Species/Host-Parasite Protein Interaction Predictions Reviewed,"Background:                    Host-parasite protein interactions (HPPI) are those interactions occurring between a parasite and its host. Host-parasite protein interaction enhances the understanding of how parasite can infect its host. The interaction plays an important role in initiating infections, although it is not all host-parasite interactions that result in infection. Identifying the protein-protein interactions (PPIs) that allow a parasite to infect its host has a lot do in discovering possible drug targets. Such PPIs, when altered, would prevent the host from being infected by the parasite and in some cases, result in the parasite inability to complete specific stages of its life cycle and invariably lead to the death of such parasite. It therefore becomes important to understand the workings of host-parasite interactions which are the major causes of most infectious diseases.              Objective:                    Many studies have been conducted in literature to predict HPPI, mostly using computational methods with few experimental methods. Computational method has proved to be faster and more efficient in manipulating and analyzing real life data. This study looks at various computational methods used in literature for host-parasite/inter-species protein-protein interaction predictions with the hope of getting a better insight into computational methods used and identify whether machine learning approaches have been extensively used for the same purpose.              Methods:                    The various methods involved in host-parasite protein interactions were reviewed with their individual strengths. Tabulations of studies that carried out host-parasite/inter-species protein interaction predictions were performed, analyzing their predictive methods, filters used, potential protein-protein interactions discovered in those studies and various validation measurements used as the case may be. The commonly used measurement indexes for such studies were highlighted displaying the various formulas. Finally, future prospects of studies specific to human-plasmodium falciparum PPI predictions were proposed.              Result:                    We discovered that quite a few studies reviewed implemented machine learning approach for HPPI predictions when compared with methods such as sequence homology search and protein structure and domain-motif. The key challenge well noted in HPPI predictions is getting relevant information.              Conclusion:                    This review presents useful knowledge and future directions on the subject matter.",2018-08-01,3,2613,68,1011
179,30049358,The Emerging Role of Wearable Technologies in Detection of Arrhythmia,"Over the past decade, there has been an explosion of consumer devices for the purposes of health and fitness tracking. The wearable technology market, composed of devices that monitor physiological parameters, such as heart rate and sleep pattern, is anticipated to grow to 929 million connected devices in 2021. These devices encompass wristbands, glasses, in-ear monitors, or electronic shirts, with varying capacity to monitor heart rate, heart rhythm, blood pressure, physical activity, respiratory rate, blood glucose, and sleep patterns. For heart-rate monitoring, most wearable devices use photoplethysmography (PPG) technology, meaning they are inherently less accurate than conventional electrocardiography monitoring techniques (reference standard). However, a growing body of evidence suggests that these technologies can be harnessed to facilitate arrhythmia detection in the appropriate context. Studies evaluating PPG-based wearables in conjunction with machine-learning algorithms have shown promise in detection of such arrhythmias, as atrial fibrillation. Limitations of wearable technologies include their accuracy and accessibility and the clinical implications of wearable-detected arrhythmias. Despite this, wearable technologies represent an important frontier in health evaluation. Future wearables will benefit from improved reliability and accuracy, collect additional health and fitness parameters, support management of chronic disease, and provide real-time connectivity and feedback that may supplant conventional medical monitoring. Wearables have the potential to become truly disruptive in our health care sector, with large segments of the population soon to have readily available health data that the physician must interpret.",2018-08-01,12,1761,69,1011
187,30017512,A review of statistical and machine learning methods for modeling cancer risk using structured clinical data,"Advancements are constantly being made in oncology, improving prevention and treatment of cancers. To help reduce the impact and deadliness of cancers, they must be detected early. Additionally, there is a risk of cancers recurring after potentially curative treatments are performed. Predictive models can be built using historical patient data to model the characteristics of patients that developed cancer or relapsed. These models can then be deployed into clinical settings to determine if new patients are at high risk for cancer development or recurrence. For large-scale predictive models to be built, structured data must be captured for a wide range of diverse patients. This paper explores current methods for building cancer risk models using structured clinical patient data. Trends in statistical and machine learning techniques are explored, and gaps are identified for future research. The field of cancer risk prediction is a high-impact one, and research must continue for these models to be embraced for clinical decision support of both practitioners and patients.",2018-08-01,15,1084,108,1011
2968,29750902,Machine learning in chemoinformatics and drug discovery,"Chemoinformatics is an established discipline focusing on extracting, processing and extrapolating meaningful data from chemical structures. With the rapid explosion of chemical 'big' data from HTS and combinatorial synthesis, machine learning has become an indispensable tool for drug designers to mine chemical information from large compound databases to design drugs with important biological properties. To process the chemical data, we first reviewed multiple processing layers in the chemoinformatics pipeline followed by the introduction of commonly used machine learning models in drug discovery and QSAR analysis. Here, we present basic principles and recent case studies to demonstrate the utility of machine learning techniques in chemoinformatics analyses; and we discuss limitations and future directions to guide further development in this evolving field.",2018-08-01,77,871,55,1011
2959,29782369,Neuroimaging in epilepsy,"Purpose of review:                    Epilepsy neuroimaging is important for detecting the seizure onset zone, predicting and preventing deficits from surgery and illuminating mechanisms of epileptogenesis. An aspiration is to integrate imaging and genetic biomarkers to enable personalized epilepsy treatments.              Recent findings:                    The ability to detect lesions, particularly focal cortical dysplasia and hippocampal sclerosis, is increased using ultra high-field imaging and postprocessing techniques such as automated volumetry, T2 relaxometry, voxel-based morphometry and surface-based techniques. Statistical analysis of PET and single photon emission computer tomography (STATISCOM) are superior to qualitative analysis alone in identifying focal abnormalities in MRI-negative patients. These methods have also been used to study mechanisms of epileptogenesis and pharmacoresistance.Recent language fMRI studies aim to localize, and also lateralize language functions. Memory fMRI has been recommended to lateralize mnemonic function and predict outcome after surgery in temporal lobe epilepsy.              Summary:                    Combinations of structural, functional and post-processing methods have been used in multimodal and machine learning models to improve the identification of the seizure onset zone and increase understanding of mechanisms underlying structural and functional aberrations in epilepsy.",2018-08-01,7,1452,24,1011
2350,29450843,Detection of Lung Contour with Closed Principal Curve and Machine Learning,"Radiation therapy plays an essential role in the treatment of cancer. In radiation therapy, the ideal radiation doses are delivered to the observed tumor while not affecting neighboring normal tissues. In three-dimensional computed tomography (3D-CT) scans, the contours of tumors and organs-at-risk (OARs) are often manually delineated by radiologists. The task is complicated and time-consuming, and the manually delineated results will be variable from different radiologists. We propose a semi-supervised contour detection algorithm, which firstly uses a few points of region of interest (ROI) as an approximate initialization. Then the data sequences are achieved by the closed polygonal line (CPL) algorithm, where the data sequences consist of the ordered projection indexes and the corresponding initial points. Finally, the smooth lung contour can be obtained, when the data sequences are trained by the backpropagation neural network model (BNNM). We use the private clinical dataset and the public Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) dataset to measure the accuracy of the presented method, respectively. To the private dataset, experimental results on the initial points which are as low as 15% of the manually delineated points show that the Dice coefficient reaches up to 0.95 and the global error is as low as 1.47  10-2. The performance of the proposed algorithm is also better than the cubic spline interpolation (CSI) algorithm. While on the public LIDC-IDRI dataset, our method achieves superior segmentation performance with average Dice of 0.83.",2018-08-01,0,1613,74,1011
491,30233373,Future Information Technology Tools for Fighting Substandard and Falsified Medicines in Low- and Middle-Income Countries,"Substandard and falsified (SF) medicines have emerged as a global public health issue within the last two decades especially in low- and middle-income countries (LMICs). Serious consequences of this problem include a loss of trust and increased financial costs due to less disease control and more frequent complications during therapy. Of note, antimicrobial resistance is an additional long-term implication of poor-quality antimicrobials. This review covers information technology tools including medicines authentication tools (MAT) as mobile apps and messaging service, 2D barcoding approaches with drug safety alert systems, web based drug safety alerts, radiofrequency identification tags, databases to support visual inspection, digital aids to enhance the performance of quality evaluation kits, reference libraries for identification of falsified and substandard medicines, and quality evaluation kits based on machine learning for field testing. While being easy to access and simple to use, these initiatives are gaining acceptance in LMICs. Implementing 2D barcoding based on end-to-end verification and ""Track and Trace"" systems has emerged as a step toward global security in the supply chain. A breakthrough in web-based drug safety alert systems and data bases was the establishment of the Global Surveillance and Monitoring System by the World Health Organization in 2013. Future applications include concepts including ""lab on a chip"" and ""paper analytical devices"" and are claimed to be convenient and simple to use as well as affordable. The principles discussed herein are making profound impact in the fight against substandard and falsified medicines, offering cheap and accessible solutions.",2018-08-01,4,1716,120,1011
2969,29750730,The changing landscape of motor neuron disease imaging: the transition from descriptive studies to precision clinical tools,"Purpose of review:                    Neuroimaging in motor neuron disease (MND) has traditionally been seen as an academic tool with limited direct relevance to individualized patient care. This has changed radically in recent years as computational imaging has emerged as a viable clinical tool with true biomarker potential. This transition is not only fuelled by technological advances but also by important conceptual developments.              Recent findings:                    The natural history of MND is now evaluated by presymptomatic, postmortem and multi-timepoint longitudinal imaging studies. The anatomical spectrum of MND imaging has also been expanded from an overwhelmingly cerebral focus to innovative spinal and muscle applications. In contrast to the group-comparisons of previous studies, machine-learning and deep-learning approaches are increasingly utilized to model real-life diagnostic dilemmas and aid prognostic classification. The focus from evaluating focal structural changes has shifted to the appraisal of network integrity by connectivity-based approaches. The armamentarium of MND imaging has also been complemented by novel PET-ligands, spinal toolboxes and the availability of magnetoencephalography and high-field magnetic resonance (MR) imaging platforms.              Summary:                    In addition to the technological and conceptual advances, collaborative multicentre research efforts have also gained considerable momentum. This opinion-piece reviews emerging trends in MND imaging and their implications to clinical care and drug development.",2018-08-01,29,1600,123,1011
2890,29982543,A Bibliometric Analysis of the Landscape of Cancer Rehabilitation Research (1992-2016),"Cancer rehabilitation research has accelerated as great attention has focused on improving survivorship care. Recent expert consensus has attempted to prioritize research needs and suggests greater focus on studying physical functioning of survivors. However, no analysis of the publication landscape has substantiated these proposed needs. This manuscript provides an analysis of PubMed indexed articles related to cancer rehabilitation published between 1992 and 2017. A total of 22 171 publications were analyzed using machine learning and text analysis to assess publication metrics, topic areas of emphasis, and their interrelationships through topic similarity networks. Publications have increased at a rate of 136 articles per year. Approximately 10% of publications were funded by the National Institutes of Health institutes and centers, with the National Cancer Institute being the most prominent funder. The greatest volume and rate of publication increase were in the topics of Cognitive and Behavioral Therapies and Psychological Interventions, followed by Depression and Exercise Therapy. Four research topic similarity networks were identified and provide insight on areas of robust publication and notable deficits. Findings suggest that publication emphasis has strongly supported cognitive, behavioral, and psychological therapies; however, studies of functional morbidity and physical rehabilitation research are lacking. Three areas of publication deficits are noted: research on populations outside of breast, prostate, and lung cancers; methods for integrating physical rehabilitation services with cancer care, specifically regarding functional screening and assessment; and physical rehabilitation interventions. These deficits align with the needs identified by expert consensus and support the supposition that future research should emphasize a focus on physical rehabilitation.",2018-08-01,14,1906,86,1011
2974,29729378,Leveraging knowledge engineering and machine learning for microbial bio-manufacturing,"Genome scale modeling (GSM) predicts the performance of microbial workhorses and helps identify beneficial gene targets. GSM integrated with intracellular flux dynamics, omics, and thermodynamics have shown remarkable progress in both elucidating complex cellular phenomena and computational strain design (CSD). Nonetheless, these models still show high uncertainty due to a poor understanding of innate pathway regulations, metabolic burdens, and other factors (such as stress tolerance and metabolite channeling). Besides, the engineered hosts may have genetic mutations or non-genetic variations in bioreactor conditions and thus CSD rarely foresees fermentation rate and titer. Metabolic models play important role in design-build-test-learn cycles for strain improvement, and machine learning (ML) may provide a viable complementary approach for driving strain design and deciphering cellular processes. In order to develop quality ML models, knowledge engineering leverages and standardizes the wealth of information in literature (e.g., genomic/phenomic data, synthetic biology strategies, and bioprocess variables). Data driven frameworks can offer new constraints for mechanistic models to describe cellular regulations, to design pathways, to search gene targets, and to estimate fermentation titer/rate/yield under specified growth conditions (e.g., mixing, nutrients, and O2). This review highlights the scope of information collections, database constructions, and machine learning techniques (such as deep learning and transfer learning), which may facilitate ""Learn and Design"" for strain development.",2018-08-01,7,1617,85,1011
631,29174666,"Combining ecological momentary assessment with objective, ambulatory measures of behavior and physiology in substance-use research","Whereas substance-use researchers have long combined self-report with objective measures of behavior and physiology inside the laboratory, developments in mobile/wearable electronic technology are increasingly allowing for the collection of both subjective and objective information in participants' daily lives. For self-report, ecological momentary assessment (EMA), as implemented on contemporary smartphones or personal digital assistants, can provide researchers with near-real-time information on participants' behavior and mood in their natural environments. Data from portable/wearable electronic sensors measuring participants' internal and external environments can be combined with EMA (e.g., by timestamps recorded on questionnaires) to provide objective information useful in determining the momentary context of behavior and mood and/or validating participants' self-reports. Here, we review three objective ambulatory monitoring techniques that have been combined with EMA, with a focus on detecting drug use and/or measuring the behavioral or physiological correlates of mental events (i.e., emotions, cognitions): (1) collection and processing of biological samples in the field to measure drug use or participants' physiological activity (e.g., hypothalamic-pituitary-adrenal axis activity); (2) global positioning system (GPS) location information to link environmental characteristics (disorder/disadvantage, retail drug outlets) to drug use and affect; (3) ambulatory electronic physiological monitoring (e.g., electrocardiography) to detect drug use and mental events, as advances in machine learning algorithms make it possible to distinguish target changes from confounds (e.g., physical activity). Finally, we consider several other mobile/wearable technologies that hold promise to be combined with EMA, as well as potential challenges faced by researchers working with multiple mobile/wearable technologies simultaneously in the field.",2018-08-01,18,1962,130,1011
2897,29972344,Oximetry use in obstructive sleep apnea,"Overnight oximetry has been proposed as an accessible, simple, and reliable technique for obstructive sleep apnea syndrome (OSAS) diagnosis. From visual inspection to advanced signal processing, several studies have demonstrated the usefulness of oximetry as a screening tool. However, there is still controversy regarding the general application of oximetry as a single screening methodology for OSAS. Areas covered: Currently, high-resolution portable devices combined with pattern recognition-based applications are able to achieve high performance in the detection of this disease. In this review, recent studies involving automated analysis of oximetry by means of advanced signal processing and machine learning algorithms are analyzed. Advantages and limitations are highlighted and novel research lines aimed at improving the screening ability of oximetry are proposed. Expert commentary: Oximetry is a cost-effective tool for OSAS screening in patients showing high pretest probability for the disease. Nevertheless, exhaustive analyses are still needed to further assess unattended oximetry monitoring as a single diagnostic test for sleep apnea, particularly in the pediatric population and in populations with significant comorbidities. In the following years, communication technologies and big data analyses will overcome current limitations of simplified sleep testing approaches, changing the detection and management of OSAS.",2018-08-01,5,1442,39,1011
661,29047033,Characterization of Pulmonary Nodules Based on Features of Margin Sharpness and Texture,"Lung cancer is the leading cause of cancer-related deaths in the world, and one of its manifestations occurs with the appearance of pulmonary nodules. The classification of pulmonary nodules may be a complex task to specialists due to temporal, subjective, and qualitative aspects. Therefore, it is important to integrate computational tools to the early pulmonary nodule classification process, since they have the potential to characterize objectively and quantitatively the lesions. In this context, the goal of this work is to perform the classification of pulmonary nodules based on image features of texture and margin sharpness. Computed tomography scans were obtained from a publicly available image database. Texture attributes were extracted from a co-occurrence matrix obtained from the nodule volume. Margin sharpness attributes were extracted from perpendicular lines drawn over the borders on all nodule slices. Feature selection was performed by different algorithms. Classification was performed by several machine learning classifiers and assessed by the area under the receiver operating characteristic curve, sensitivity, specificity, and accuracy. Highest classification performance was obtained by a random forest algorithm with all 48 extracted features. However, a decision tree using only two selected features obtained statistically equivalent performance on sensitivity and specificity.",2018-08-01,7,1412,87,1011
662,29047032,Rethinking Skin Lesion Segmentation in a Convolutional Classifier,"Melanoma is a fatal form of skin cancer when left undiagnosed. Computer-aided diagnosis systems powered by convolutional neural networks (CNNs) can improve diagnostic accuracy and save lives. CNNs have been successfully used in both skin lesion segmentation and classification. For reasons heretofore unclear, previous works have found image segmentation to be, conflictingly, both detrimental and beneficial to skin lesion classification. We investigate the effect of expanding the segmentation border to include pixels surrounding the target lesion. Ostensibly, segmenting a target skin lesion will remove inessential information, non-lesion skin, and artifacts to aid in classification. Our results indicate that segmentation border enlargement produces, to a certain degree, better results across all metrics of interest when using a convolutional based classifier built using the transfer learning paradigm. Consequently, preprocessing methods which produce borders larger than the actual lesion can potentially improve classifier performance, more than both perfect segmentation, using dermatologist created ground truth masks, and no segmentation altogether.",2018-08-01,6,1165,65,1011
2908,29952835,MRI-based neuroimaging: atypical parkinsonisms and other movement disorders,"Purpose of review:                    MRI has become a well established technical tool for parkinsonism both in the diagnostic work-up to differentiate between causes and to serve as a neurobiological marker. This review summarizes current developments in the advanced MRI-based assessment of brain structure and function in atypical parkinsonian syndromes and explores their potential in a clinical and neuroscientific setting.              Recent findings:                    Computer-based unbiased quantitative MRI analyses were demonstrated to guide in the discrimination of parkinsonian syndromes at single-patient level, with major contributions when combined with machine-learning techniques/support vector machine classification. These techniques have shown their potential in tracking the disease progression, perhaps also as a read-out in clinical trials. The characterization of different brain compartments at various levels of structural and functional alterations can be provided by multiparametric MRI, including a growing variety of diffusion-weighted imaging approaches and potentially iron-sensitive and functional MRI.              Summary:                    In case that the recent advances in the MRI-based assessment of atypical parkinsonism will lead to standardized protocols for image acquisition and analysis after the confirmation in large-scale multicenter studies, these approaches may constitute a great achievement in the (operator-independent) detection, discrimination and characterization of degenerative parkinsonian disorders at an individual basis.",2018-08-01,6,1587,75,1011
2909,29944078,Current Applications and Future Impact of Machine Learning in Radiology,"Recent advances and future perspectives of machine learning techniques offer promising applications in medical imaging. Machine learning has the potential to improve different steps of the radiology workflow including order scheduling and triage, clinical decision support systems, detection and interpretation of findings, postprocessing and dose estimation, examination quality control, and radiology reporting. In this article, the authors review examples of current applications of machine learning and artificial intelligence techniques in diagnostic radiology. In addition, the future impact and natural extension of these techniques in radiology practice are discussed.",2018-08-01,92,676,71,1011
2911,29934920,Convolutional neural networks: an overview and application in radiology,"Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care. KEY POINTS:  Convolutional neural network is a class of deep learning methods which has become dominant in various computer vision tasks and is attracting interest across a variety of domains, including radiology.  Convolutional neural network is composed of multiple building blocks, such as convolution layers, pooling layers, and fully connected layers, and is designed to automatically and adaptively learn spatial hierarchies of features through a backpropagation algorithm.  Familiarity with the concepts and advantages, as well as limitations, of convolutional neural network is essential to leverage its potential to improve radiologist performance and, eventually, patient care.",2018-08-01,85,1707,71,1011
2954,29790107,The Role of Pharmacogenomics in Bipolar Disorder: Moving Towards Precision Medicine,"Bipolar disorder (BD) is a common and disabling psychiatric condition with a severe socioeconomic impact. BD is treated with mood stabilizers, among which lithium represents the first-line treatment. Lithium alone or in combination is effective in 60% of chronically treated patients, but response remains heterogenous and a large number of patients require a change in therapy after several weeks or months. Many studies have so far tried to identify molecular and genetic markers that could help us to predict response to mood stabilizers or the risk for adverse drug reactions. Pharmacogenetic studies in BD have been for the most part focused on lithium, but the complexity and variability of the response phenotype, together with the unclear mechanism of action of lithium, limited the power of these studies to identify robust biomarkers. Recent pharmacogenomic studies on lithium response have provided promising findings, suggesting that the integration of genome-wide investigations with deep phenotyping, in silico analyses and machine learning could lead us closer to personalized treatments for BD. Nevertheless, to date none of the genes suggested by pharmacogenetic studies on mood stabilizers have been included in any of the genetic tests approved by the Food and Drug Administration (FDA) for drug efficacy. On the other hand, genetic information has been included in drug labels to test for the safety of carbamazepine and valproate. In this review, we will outline available studies investigating the pharmacogenetics and pharmacogenomics of lithium and other mood stabilizers, with a specific focus on the limitations of these studies and potential strategies to overcome them. We will also discuss FDA-approved pharmacogenetic tests for treatments commonly used in the management of BD.",2018-08-01,7,1807,83,1011
168,31723874,Deep Learning in the Medical Domain: Predicting Cardiac Arrest Using Deep Learning,"With the wider adoption of electronic health records, the rapid response team initially believed that mortalities could be significantly reduced but due to low accuracy and false alarms, the healthcare system is currently fraught with many challenges. Rule-based methods (e.g., Modified Early Warning Score) and machine learning (e.g., random forest) were proposed as a solution but not effective. In this article, we introduce the DeepEWS (Deep learning based Early Warning Score), which is based on a novel deep learning algorithm. Relative to the standard of care and current solutions in the marketplace, there is high accuracy, and in the clinical setting even when we consider the number of alarms, the accuracy levels are superior.",2018-08-01,2,738,82,1011
99,30211115,The Network of Non-coding RNAs in Cancer Drug Resistance,"Non-coding RNAs (ncRNAs) have been implicated in most cellular functions. The disruption of their function through somatic mutations, genomic imprinting, transcriptional and post-transcriptional regulation, plays an ever-increasing role in cancer development. ncRNAs, including notorious microRNAs, have been thus proposed to function as tumor suppressors or oncogenes, often in a context-dependent fashion. In parallel, ncRNAs with altered expression in cancer have been reported to exert a key role in determining drug sensitivity or restoring drug responsiveness in resistant cells. Acquisition of resistance to anti-cancer drugs is a major hindrance to effective chemotherapy and is one of the most important causes of relapse and mortality in cancer patients. For these reasons, non-coding RNAs have become recent focuses as prognostic agents and modifiers of chemo-sensitivity. This review starts with a brief outline of the role of most studied non-coding RNAs in cancer and then highlights the modulation of cancer drug resistance via known ncRNAs based mechanisms. We identified from literature 388 ncRNA-drugs interactions and analyzed them using an unsupervised approach. Essentially, we performed a network analysis of the non-coding RNAs with direct relations with cancer drugs. Within such a machine-learning framework we detected the most representative ncRNAs-drug associations and groups. We finally discussed the higher integration of the drug-ncRNA clusters with the goal of disentangling effectors from downstream effects and further clarify the involvement of ncRNAs in the cellular mechanisms underlying resistance to cancer treatments.",2018-08-01,28,1658,56,1011
136,30116102,A Novel Approach for Identifying Relevant Genes for Breast Cancer Survivability on Specific Therapies,"Analyzing the genetic activity of breast cancer survival for a specific type of therapy provides a better understanding of the body response to the treatment and helps select the best course of action and while leading to the design of drugs based on gene activity. In this work, we use supervised and nonsupervised machine learning methods to deal with a multiclass classification problem in which we label the samples based on the combination of the 5-year survivability and treatment; we focus on hormone therapy, radiotherapy, and surgery. The proposed nonsupervised hierarchical models are created to find the highest separability between combinations of the classes. The supervised model consists of a combination of feature selection techniques and efficient classifiers used to find a potential set of biomarker genes specific to response to therapy. The results show that different models achieve different performance scores with accuracies ranging from 80.9% to 100%. We have investigated the roles of many biomarkers through the literature and found that some of the discriminative genes in the computational model such as ZC3H11A, VAX2, MAF1, and ZFP91 are related to breast cancer and other types of cancer.",2018-08-01,3,1221,101,1011
2929,29880463,Artificial Intelligence Approaches in Hematopoietic Cell Transplantation: A Review of the Current Status and Future Directions,"The evidence-based literature on healthcare is currently expanding exponentially. The opportunities provided by the advancement in artificial intelligence (AI) tools such as machine learning are appealing in tackling many of the current healthcare challenges. Thus, AI integration is expanding in most fields of healthcare, including the field of hematology. This study aims to review the current applications of AI in the field of hematopoietic cell transplantation (HCT). A literature search was done involving the following databases: Ovid MEDLINE, including In-Process and other non-indexed citations, and Google Scholar. The abstracts of the following professional societies were also screened: American Society of Hematology, American Society for Blood and Marrow Transplantation, and European Society for Blood and Marrow Transplantation. The literature review showed that the integration of AI in the field of HCT has grown remarkably in the last decade and offers promising avenues in diagnosis and prognosis in HCT populations targeting both pre- and post-transplant challenges. Studies of AI integration in HCT have many limitations that include poorly tested algorithms, lack of generalizability, and limited use of different AI tools. Machine learning techniques in HCT are an intense area of research that needs much development and extensive support from hematology and HCT societies and organizations globally as we believe that this will be the future practice paradigm.",2018-08-01,0,1487,126,1011
167,31763498,How to find the right drug for each patient? Advances and challenges in pharmacogenomics,"Cancer is a highly heterogeneous disease with complex underlying biology. For these reasons, effective cancer treatment is still a challenge. Nowadays, it is clear that a cancer therapy that fits all the cases cannot be found, and as a result the design of therapies tailored to the patient's molecular characteristics is needed. Pharmacogenomics aims to study the relationship between an individual's genotype and drug response. Scientists use different biological models, ranging from cell lines to mouse models, as proxies for patients for preclinical and translational studies. The rapid development of ""-omics"" technologies is increasing the amount of features that can be measured in these models, expanding the possibilities of finding predictive biomarkers of drug response. Finding these relationships requires diverse computational approaches ranging from machine learning to dynamic modeling. Despite major advances, we are still far from being able to precisely predict drug efficacy in cancer models, let alone directly on patients. We believe that the new experimental techniques and computational approaches covered in this review will bring us closer to this goal.",2018-08-01,4,1180,88,1011
125,30149790,Hyperspectral Sensors and Imaging Technologies in Phytopathology: State of the Art,"Plant disease detection represents a tremendous challenge for research and practical applications. Visual assessment by human raters is time-consuming, expensive, and error prone. Disease rating and plant protection need new and innovative techniques to address forthcoming challenges and trends in agricultural production that require more precision than ever before. Within this context, hyperspectral sensors and imaging techniques-intrinsically tied to efficient data analysis approaches-have shown an enormous potential to provide new insights into plant-pathogen interactions and for the detection of plant diseases. This article provides an overview of hyperspectral sensors and imaging technologies for assessing compatible and incompatible plant-pathogen interactions. Within the progress of digital technologies, the vision, which is increasingly discussed in the society and industry, includes smart and intuitive solutions for assessing plant features in plant phenotyping or for making decisions on plant protection measures in the context of precision agriculture.",2018-08-01,9,1078,82,1011
137,30110960,Machine Learning in Agriculture: A Review,"Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.",2018-08-01,46,1014,41,1011
2927,29884988,Natural Language Processing and Its Implications for the Future of Medication Safety: A Narrative Review of Recent Advances and Challenges,"The safety of medication use has been a priority in the United States since the late 1930s. Recently, it has gained prominence due to the increasing amount of data suggesting that a large amount of patient harm is preventable and can be mitigated with effective risk strategies that have not been sufficiently adopted. Adverse events from medications are part of clinical practice, but the ability to identify a patient's risk and to minimize that risk must be a priority. The ability to identify adverse events has been a challenge due to limitations of available data sources, which are often free text. The use of natural language processing (NLP) may help to address these limitations. NLP is the artificial intelligence domain of computer science that uses computers to manipulate unstructured data (i.e., narrative text or speech data) in the context of a specific task. In this narrative review, we illustrate the fundamentals of NLP and discuss NLP's application to medication safety in four data sources: electronic health records, Internet-based data, published literature, and reporting systems. Given the magnitude of available data from these sources, a growing area is the use of computer algorithms to help automatically detect associations between medications and adverse effects. The main benefit of NLP is in the time savings associated with automation of various medication safety tasks such as the medication reconciliation process facilitated by computers, as well as the potential for near-real-time identification of adverse events for postmarketing surveillance such as those posted on social media that would otherwise go unanalyzed. NLP is limited by a lack of data sharing between health care organizations due to insufficient interoperability capabilities, inhibiting large-scale adverse event monitoring across populations. We anticipate that future work in this area will focus on the integration of data sources from different domains to improve the ability to identify potential adverse events more quickly and to improve clinical decision support with regard to a patient's estimated risk for specific adverse events at the time of medication prescription or review.",2018-08-01,11,2199,138,1011
139,30105183,Futuristic biosensors for cardiac health care: an artificial intelligence approach,"Biosensor-based devices are pioneering in the modern biomedical applications and will be the future of cardiac health care. The coupling of artificial intelligence (AI) for cardiac monitoring-based biosensors for the point of care (POC) diagnostics is prominently reviewed here. This review deciphers the most significant machine-learning algorithms for the futuristic biosensors along with the internet of things, computational techniques and microchip-based essential cardiac biomarkers for real-time health monitoring and improving patient compliance. The present review also discusses the recently developed cardiac biosensors along with technical strategies involved in their mechanism of working and their applications in healthcare. Additionally, it provides a key for the ontogeny of an effective and supportive hierarchical protocol for clinical decision-making about personalized medicine through combinatory information analysis, and integrated multidisciplinary AI approaches.",2018-08-01,5,988,82,1011
141,30103448,Machine Learning Based Toxicity Prediction: From Chemical Structural Description to Transcriptome Analysis,"Toxicity prediction is very important to public health. Among its many applications, toxicity prediction is essential to reduce the cost and labor of a drug's preclinical and clinical trials, because a lot of drug evaluations (cellular, animal, and clinical) can be spared due to the predicted toxicity. In the era of Big Data and artificial intelligence, toxicity prediction can benefit from machine learning, which has been widely used in many fields such as natural language processing, speech recognition, image recognition, computational chemistry, and bioinformatics, with excellent performance. In this article, we review machine learning methods that have been applied to toxicity prediction, including deep learning, random forests, k-nearest neighbors, and support vector machines. We also discuss the input parameter to the machine learning algorithm, especially its shift from chemical structural description only to that combined with human transcriptome data analysis, which can greatly enhance prediction accuracy.",2018-08-01,10,1029,106,1011
121,30157513,"Sensor, Signal, and Imaging Informatics in 2017","Objective:                     To summarize significant contributions to sensor, signal, and imaging informatics literature published in 2017.              Methods:                     PubMed and Web of Science were searched to identify the scientific publications published in 2017 that addressed sensors, signals, and imaging in medical informatics. Fifteen papers were selected by consensus as candidate best papers. Each candidate article was reviewed by section editors and at least two other external reviewers. The final selection of the four best papers was conducted by the editorial board of the International Medical Informatics Association (IMIA) Yearbook.              Results:                     The selected papers of 2017 demonstrate the important scientific advances in management and analysis of sensor, signal, and imaging information.              Conclusion:                    The growth of signal and imaging data and the increasing power of machine learning techniques have engendered new opportunities for research in medical informatics. This synopsis highlights cutting-edge contributions to the science of Sensor, Signal, and Imaging Informatics.",2018-08-01,1,1177,47,1011
2317,29570167,Advanced Morphologic Analysis for Diagnosing Allograft Rejection: The Case of Cardiac Transplant Rejection,"Allograft rejection remains a significant concern after all solid organ transplants. Although qualitative morphologic analysis with histologic grading of biopsy samples is the main tool employed for diagnosing allograft rejection, this standard has significant limitations in precision and accuracy that affect patient care. The use of endomyocardial biopsy to diagnose cardiac allograft rejection illustrates the significant shortcomings of current approaches for diagnosing allograft rejection. Despite disappointing interobserver variability, concerns about discordance with clinical trajectories, attempts at revising the histologic criteria and efforts to establish new diagnostic tools with imaging and gene expression profiling, no method has yet supplanted endomyocardial biopsy as the diagnostic gold standard. In this context, automated approaches to complex data analysis problems-often referred to as ""machine learning""-represent promising strategies to improve overall diagnostic accuracy. By focusing on cardiac allograft rejection, where tissue sampling is relatively frequent, this review highlights the limitations of the current approach to diagnosing allograft rejection, introduces the basic methodology behind machine learning and automated image feature detection, and highlights the initial successes of these approaches within cardiovascular medicine.",2018-08-01,3,1375,106,1011
2926,29885309,Micro RNA as a potential blood-based epigenetic biomarker for Alzheimer's disease,"As the prevalence of Alzheimer's disease (AD) increases, the search for a definitive, easy to access diagnostic biomarker has become increasingly important. Micro RNA (miRNA), involved in the epigenetic regulation of protein synthesis, is a biological mark which varies in association with a number of disease states, possibly including AD. Here we comprehensively review methods and findings from 26 studies comparing the measurement of miRNA in blood between AD cases and controls. Thirteen of these studies used receiver operator characteristic (ROC) analysis to determine the diagnostic accuracy of identified miRNA to predict AD, and three studies did this with a machine learning approach. Of 8098 individually measured miRNAs, 23 that were differentially expressed between AD cases and controls were found to be significant in two or more studies. Only six of these were consistent in their direction of expression between studies (miR-107, miR-125b, miR-146a, miR-181c, miR-29b, and miR-342), and they were all shown to be down regulated in individuals with AD compared to controls. Of these directionally concordant miRNAs, the strongest evidence was for miR-107 which has also been shown in previous studies to be involved in the dysregulation of proteins involved in aspects of AD pathology, as well as being consistently downregulated in studies of AD brains. We conclude that imperative to the discovery of reliable and replicable miRNA biomarkers of AD, standardised methods of measurements, appropriate statistical analysis, utilization of large datasets with machine learning approaches, and comprehensive reporting of findings is urgently needed.",2018-08-01,20,1663,81,1011
143,30101283,Bioinformatics applications on Apache Spark,"With the rapid development of next-generation sequencing technology, ever-increasing quantities of genomic data pose a tremendous challenge to data processing. Therefore, there is an urgent need for highly scalable and powerful computational systems. Among the state-of-the-art parallel computing platforms, Apache Spark is a fast, general-purpose, in-memory, iterative computing framework for large-scale data processing that ensures high fault tolerance and high scalability by introducing the resilient distributed dataset abstraction. In terms of performance, Spark can be up to 100 times faster in terms of memory access and 10 times faster in terms of disk access than Hadoop. Moreover, it provides advanced application programming interfaces in Java, Scala, Python, and R. It also supports some advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. We surveyed Spark-based applications used in next-generation sequencing and other biological domains, such as epigenetics, phylogeny, and drug discovery. The results of this survey are used to provide a comprehensive guideline allowing bioinformatics researchers to apply Spark in their own fields.",2018-08-01,17,1272,43,1011
131,33141723,"Robots for the people, by the people: Personalizing human-machine interaction","Multimodal, interactive, and multitask machine learning can be applied to personalize human-robot and human-machine interactions for the broad diversity of individuals and their unique needs.",2018-08-01,1,191,77,1011
156,30082644,Evolution of In Silico Strategies for Protein-Protein Interaction Drug Discovery,"The advent of advanced molecular modeling software, big data analytics, and high-speed processing units has led to the exponential evolution of modern drug discovery and better insights into complex biological processes and disease networks. This has progressively steered current research interests to understanding protein-protein interaction (PPI) systems that are related to a number of relevant diseases, such as cancer, neurological illnesses, metabolic disorders, etc. However, targeting PPIs are challenging due to their ""undruggable"" binding interfaces. In this review, we focus on the current obstacles that impede PPI drug discovery, and how recent discoveries and advances in in silico approaches can alleviate these barriers to expedite the search for potential leads, as shown in several exemplary studies. We will also discuss about currently available information on PPI compounds and systems, along with their usefulness in molecular modeling. Finally, we conclude by presenting the limits of in silico application in drug discovery and offer a perspective in the field of computer-aided PPI drug discovery.",2018-08-01,14,1124,80,1011
166,30063164,Are there advances in pharmacotherapy for panic disorder? A systematic review of the past five years,"Introduction:                    Several effective medications are available for treating panic disorder (PD). However, outcomes are unsatisfactory in a number of patients, suggesting the usefulness of expanding the array of antipanic drugs and improving the quality of response to current recommended treatments.              Areas covered:                    The authors have performed an updated systematic review of pharmacological studies (phase III onwards) to examine whether advances have been made in the last five years. Only four studies were included. D-cycloserine no longer seemed promising as a cognitive-behavioral therapy (CBT) enhancer. Some preliminary findings concerning the optimization of recommended medications deserved consideration, including: the possibility that SSRIs are more effective than CBT alone in treating panic attacks, combined therapy is preferable when agoraphobia is present, and clonazepam is more potent than paroxetine in decreasing panic relapse.              Expert opinion:                    Given the lack of novel treatments, expanding a personalized approach to the existing medications seems to be the most feasible strategy to improve pharmacotherapy outcomes regarding PD. Recent technological progress, including wearable devices collecting real-time data, 'big data' platforms, and application of machine learning techniques might help make outcome prediction more reliable. Further research on previously promising novel treatments is also recommended.",2018-08-01,6,1511,100,1011
119,30159406,Machine learning and image-based profiling in drug discovery,"The increase in imaging throughput, new analytical frameworks and high-performance computational resources open new avenues for data-rich phenotypic profiling of small molecules in drug discovery. Image-based profiling assays assessing single-cell phenotypes have been used to explore mechanisms of action, target efficacy and toxicity of small molecules. Technological advances to generate large data sets together with new machine learning approaches for the analysis of high-dimensional profiling data create opportunities to improve many steps in drug discovery. In this review, we will discuss how recent studies applied machine learning approaches in functional profiling workflows with a focus on chemical genetics. While their utility in image-based screening and profiling is predictably evident, examples of novel insights beyond the status quo based on the applications of machine learning approaches are just beginning to emerge. To enable discoveries, future studies also need to develop methodologies that lower the entry barriers to high-throughput profiling experiments by streamlining image-based profiling assays and providing applications for advanced learning technologies such as easy to deploy deep neural networks.",2018-08-01,24,1237,60,1011
118,30167371,Structural neuroimaging as clinical predictor: A review of machine learning applications,"In this paper, we provide an extensive overview of machine learning techniques applied to structural magnetic resonance imaging (MRI) data to obtain clinical classifiers. We specifically address practical problems commonly encountered in the literature, with the aim of helping researchers improve the application of these techniques in future works. Additionally, we survey how these algorithms are applied to a wide range of diseases and disorders (e.g. Alzheimer's disease (AD), Parkinson's disease (PD), autism, multiple sclerosis, traumatic brain injury, etc.) in order to provide a comprehensive view of the state of the art in different fields.",2018-08-01,21,651,88,1011
164,30068955,Machine learning at the energy and intensity frontiers of particle physics,"Our knowledge of the fundamental particles of nature and their interactions is summarized by the standard model of particle physics. Advancing our understanding in this field has required experiments that operate at ever higher energies and intensities, which produce extremely large and information-rich data samples. The use of machine-learning techniques is revolutionizing how we interpret these data samples, greatly increasing the discovery potential of present and future experiments. Here we summarize the challenges and opportunities that come with the use of machine learning at the frontiers of particle physics.",2018-08-01,7,623,74,1011
108,30190674,A Comprehensive Review of Magnetoencephalography (MEG) Studies for Brain Functionality in Healthy Aging and Alzheimer's Disease (AD),"Neural oscillations were established with their association with neurophysiological activities and the altered rhythmic patterns are believed to be linked directly to the progression of cognitive decline. Magnetoencephalography (MEG) is a non-invasive technique to record such neuronal activity due to excellent temporal and fair amount of spatial resolution. Single channel, connectivity as well as brain network analysis using MEG data in resting state and task-based experiments were analyzed from existing literature. Single channel analysis studies reported a less complex, more regular and predictable oscillations in Alzheimer's disease (AD) primarily in the left parietal, temporal and occipital regions. Investigations on both functional connectivity (FC) and effective (EC) connectivity analysis demonstrated a loss of connectivity in AD compared to healthy control (HC) subjects found in higher frequency bands. It has been reported from multiplex network of MEG study in AD in the affected regions of hippocampus, posterior default mode network (DMN) and occipital areas, however, conclusions cannot be drawn due to limited availability of clinical literature. Potential utilization of high spatial resolution in MEG likely to provide information related to in-depth brain functioning and underlying factors responsible for changes in neuronal waves in AD. This review is a comprehensive report to investigate diagnostic biomarkers for AD may be identified by from MEG data. It is also important to note that MEG data can also be utilized for the same pursuit in combination with other imaging modalities.",2018-08-01,8,1617,132,1011
2918,29908150,Microscopy in Infectious Disease Research-Imaging Across Scales,"A comprehensive understanding of host-pathogen interactions requires quantitative assessment of molecular events across a wide range of spatiotemporal scales and organizational complexities. Due to recent technical developments, this is currently only achievable with microscopy. This article is providing a general perspective on the importance of microscopy in infectious disease research, with a focus on new imaging modalities that promise to have a major impact in biomedical research in the years to come. Every major technological breakthrough in light microscopy depends on, and is supported by, advancements in computing and information technologies. Bioimage acquisition and analysis based on machine learning will pave the way toward more robust, automated and objective implementation of new imaging modalities and in biomedical research in general. The combination of novel imaging technologies with machine learning and near-physiological model systems promises to accelerate discoveries and breakthroughs in our understanding of infectious diseases, from basic research all the way to clinical applications.",2018-08-01,3,1122,63,1011
162,30081607,Indirect Measurement of Ground Reaction Forces and Moments by Means of Wearable Inertial Sensors: A Systematic Review,"In the last few years, estimating ground reaction forces by means of wearable sensors has come to be a challenging research topic paving the way to kinetic analysis and sport performance testing outside of labs. One possible approach involves estimating the ground reaction forces from kinematic data obtained by inertial measurement units (IMUs) worn by the subject. As estimating kinetic quantities from kinematic data is not an easy task, several models and protocols have been developed over the years. Non-wearable sensors, such as optoelectronic systems along with force platforms, remain the most accurate systems to record motion. In this review, we identified, selected and categorized the methodologies for estimating the ground reaction forces from IMUs as proposed across the years. Scopus, Google Scholar, IEEE Xplore, and PubMed databases were interrogated on the topic of Ground Reaction Forces estimation based on kinematic data obtained by IMUs. The identified papers were classified according to the methodology proposed: (i) methods based on direct modelling; (ii) methods based on machine learning. The methods based on direct modelling were further classified according to the task studied (walking, running, jumping, etc.). Finally, we comparatively examined the methods in order to identify the most reliable approaches for the implementation of a ground reaction force estimator based on IMU data.",2018-08-01,27,1421,117,1011
120,30158822,Natural Language Processing of Social Media as Screening for Suicide Risk,"Suicide is among the 10 most common causes of death, as assessed by the World Health Organization. For every death by suicide, an estimated 138 people's lives are meaningfully affected, and almost any other statistic around suicide deaths is equally alarming. The pervasiveness of social media-and the near-ubiquity of mobile devices used to access social media networks-offers new types of data for understanding the behavior of those who (attempt to) take their own lives and suggests new possibilities for preventive intervention. We demonstrate the feasibility of using social media data to detect those at risk for suicide. Specifically, we use natural language processing and machine learning (specifically deep learning) techniques to detect quantifiable signals around suicide attempts, and describe designs for an automated system for estimating suicide risk, usable by those without specialized mental health training (eg, a primary care doctor). We also discuss the ethical use of such technology and examine privacy implications. Currently, this technology is only used for intervention for individuals who have ""opted in"" for the analysis and intervention, but the technology enables scalable screening for suicide risk, potentially identifying many people who are at risk preventively and prior to any engagement with a health care system. This raises a significant cultural question about the trade-off between privacy and prevention-we have potentially life-saving technology that is currently reaching only a fraction of the possible people at risk because of respect for their privacy. Is the current trade-off between privacy and prevention the right one?",2018-08-01,27,1674,73,1011
105,30200333,Machine Learning for Drug-Target Interaction Prediction,"Identifying drug-target interactions will greatly narrow down the scope of search of candidate medications, and thus can serve as the vital first step in drug discovery. Considering that in vitro experiments are extremely costly and time-consuming, high efficiency computational prediction methods could serve as promising strategies for drug-target interaction (DTI) prediction. In this review, our goal is to focus on machine learning approaches and provide a comprehensive overview. First, we summarize a brief list of databases frequently used in drug discovery. Next, we adopt a hierarchical classification scheme and introduce several representative methods of each category, especially the recent state-of-the-art methods. In addition, we compare the advantages and limitations of methods in each category. Lastly, we discuss the remaining challenges and future outlook of machine learning in DTI prediction. This article may provide a reference and tutorial insights on machine learning-based DTI prediction for future researchers.",2018-08-01,20,1039,55,1011
103,30201875,Prediction Methods of Herbal Compounds in Chinese Medicinal Herbs,"Chinese herbal medicine has recently gained worldwide attention. The curative mechanism of Chinese herbal medicine is compared with that of western medicine at the molecular level. The treatment mechanism of most Chinese herbal medicines is still not clear. How do we integrate Chinese herbal medicine compounds with modern medicine? Chinese herbal medicine drug-like prediction method is particularly important. A growing number of Chinese herbal source compounds are now widely used as drug-like compound candidates. An important way for pharmaceutical companies to develop drugs is to discover potentially active compounds from related herbs in Chinese herbs. The methods for predicting the drug-like properties of Chinese herbal compounds include the virtual screening method, pharmacophore model method and machine learning method. In this paper, we focus on the prediction methods for the medicinal properties of Chinese herbal medicines. We analyze the advantages and disadvantages of the above three methods, and then introduce the specific steps of the virtual screening method. Finally, we present the prospect of the joint application of various methods.",2018-09-01,2,1165,65,980
882,31304333,Machine learning and medical education,Artificial intelligence (AI) driven by machine learning (ML) algorithms is a branch in computer science that is rapidly gaining popularity within the healthcare sector. Recent regulatory approvals of AI-driven companion diagnostics and other products are glimmers of a future in which these tools could play a key role by defining the way medicine will be practiced. Educating the next generation of medical professionals with the right ML techniques will enable them to become part of this emerging data science revolution.,2018-09-01,15,524,38,980
2309,29614377,Neural representations of time-linked memory,"Many cognitive processes, such as episodic memory and decision making, rely on the ability to form associations between two events that occur separately in time. The formation of such temporal associations depends on neural representations of three types of information: what has been presented (trace holding), what will follow (temporal expectation), and when the following event will occur (explicit timing). The present review seeks to link these representations with firing patterns of single neurons recorded while rodents and non-human primates associate stimuli, outcomes, and motor responses over time intervals. Across these studies, two distinct firing patterns were observed in the hippocampus, neocortex, and striatum: some neurons change firing rates during or shortly after the stimulus presentation and sustain the firing rate stably or sidlingly during the subsequent intervals (tonic firings). Other neurons transiently change firing rates during a specific moment within the time intervals (phasic firings), and as a group, they form a sequential firing pattern that covers the entire interval. Clever task designs used in some of these studies collectively provide evidence that both tonic and phasic firing responses represent trace holding, temporal expectation, and explicit timing. Subsequently, we applied machine-learning based classification approaches to the two firing patterns within the same dataset collected from rat medial prefrontal cortex during trace eyeblink conditioning. This quantitative analysis revealed that phasic-firing patterns showed greater selectivity for stimulus identity and temporal position than tonic-firing patterns. Our summary illuminates distributed neural representations of temporal association in the forebrain and generates several ideas for future investigations.",2018-09-01,1,1828,44,980
2523,30581287,Forty years of structural brain imaging in mental disorders: is it clinically useful or not?,"Structural brain imaging was introduced into routine clinical practice more than 40 years ago with the hope that it would support the diagnosis and treatment of mental disorders. It is now widely used to exclude organic brain disease (eg, brain tumors, cardiovascular, and inflammatory processes) in mental disorders. However, questions have been raised about whether structural brain imaging is still needed today and whether it could also be clinically useful to apply new biostatistical methods, such as machine learning. Therefore, the current paper not only reviews structural findings in Alzheimer disease, depression, bipolar disorder, and schizophrenia but also discusses the role of structural imaging in supporting diagnostic, prognostic, and therapeutic processes in mental disorders. Thus, it attempts to answer the questions whether, after four decades of use, structural brain imaging is clinically useful in mental disorders or whether it will become so in the future.",2018-09-01,3,983,92,980
581,30906397,Artificial Neural Network: Understanding the Basic Concepts without Mathematics,"Machine learning is where a machine (i.e., computer) determines for itself how input data is processed and predicts outcomes when provided with new data. An artificial neural network is a machine learning algorithm based on the concept of a human neuron. The purpose of this review is to explain the fundamental concepts of artificial neural networks.",2018-09-01,2,351,79,980
2888,29989977,Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis,"The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHRs). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and deidentification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research.",2018-09-01,99,1062,109,980
2896,29974498,"Automation, machine learning, and artificial intelligence in echocardiography: A brave new world","Automation, machine learning, and artificial intelligence (AI) are changing the landscape of echocardiography providing complimentary tools to physicians to enhance patient care. Multiple vendor software programs have incorporated automation to improve accuracy and efficiency of manual tracings. Automation with longitudinal strain and 3D echocardiography has shown great accuracy and reproducibility allowing the incorporation of these techniques into daily workflow. This will give further experience to nonexpert readers and allow the integration of these essential tools into more echocardiography laboratories. The potential for machine learning in cardiovascular imaging is still being discovered as algorithms are being created, with training on large data sets beyond what traditional statistical reasoning can handle. Deep learning when applied to large image repositories will recognize complex relationships and patterns integrating all properties of the image, which will unlock further connections about the natural history and prognosis of cardiac disease states. The purpose of this review article was to describe the role and current use of automation, machine learning, and AI in echocardiography and discuss potential limitations and challenges of in the future.",2018-09-01,14,1281,96,980
2942,29858745,"Machine Learning to Predict, Detect, and Intervene Older Adults Vulnerable for Adverse Drug Events in the Emergency Department","Adverse drug events (ADEs) are common and have serious consequences in older adults. ED visits are opportunities to identify and alter the course of such vulnerable patients. Current practice, however, is limited by inaccurate reporting of medication list, time-consuming medication reconciliation, and poor ADE assessment. This manuscript describes a novel approach to predict, detect, and intervene vulnerable older adults at risk of ADE using machine learning. Toxicologists' expertise in ADE is essential to creating the machine learning algorithm. Leveraging the existing electronic health records to better capture older adults at risk of ADE in the ED may improve their care.",2018-09-01,3,682,126,980
495,30221328,Posttraumatic Stress Disorder and Death From Suicide,"Purpose of review:                    This review summarizes the increasing public health concern about PTSD and suicide, and the population-based studies that have examined this association. Further, we discuss methodological issues that provide important context for the examination of this association.              Recent findings:                    The majority of epidemiologic studies have shown that PTSD is associated with an increased risk of suicide; however, a notable minority of studies have documented a decreased risk of suicide among persons with PTSD. Methodological (e.g., sample size and misclassification) and etiologic issues (e.g., complicated psychiatric comorbidity) may explain the conflicting evidence. PTSD may be associated with an increased risk of suicide, but further research is needed. Increasing the use of appropriate methods (e.g., marginal structural models that can evaluate both confounding and effect modification, machine learning methods, quantification of systematic error) will strengthen the evidence base and advance our understanding.",2018-09-01,5,1083,52,980
2939,29860027,Patient Similarity Networks for Precision Medicine,"Clinical research and practice in the 21st century is poised to be transformed by analysis of computable electronic medical records and population-level genome-scale patient profiles. Genomic data capture genetic and environmental state, providing information on heterogeneity in disease and treatment outcome, but genomic-based clinical risk scores are limited. Achieving the goal of routine precision medicine that takes advantage of these rich genomics data will require computational methods that support heterogeneous data, have excellent predictive performance, and ideally, provide biologically interpretable results. Traditional machine-learning approaches excel at performance, but often have limited interpretability. Patient similarity networks are an emerging paradigm for precision medicine, in which patients are clustered or classified based on their similarities in various features, including genomic profiles. This strategy is analogous to standard medical diagnosis, has excellent performance, is interpretable, and can preserve patient privacy. We review new methods based on patient similarity networks, including Similarity Network Fusion for patient clustering and netDx for patient classification. While these methods are already useful, much work is required to improve their scalability for contemporary genetic cohorts, optimize parameters, and incorporate a wide range of genomics and clinical data. The coming 5 years will provide an opportunity to assess the utility of network-based algorithms for precision medicine.",2018-09-01,11,1548,50,980
160,30082298,Machine learning and genomics: precision medicine versus patient privacy,"Machine learning can have a major societal impact in computational biology applications. In particular, it plays a central role in the development of precision medicine, whereby treatment is tailored to the clinical or genetic features of the patient. However, these advances require collecting and sharing among researchers large amounts of genomic data, which generates much concern about privacy. Researchers, study participants and governing bodies should be aware of the ways in which the privacy of participants might be compromised, as well as of the large body of research on technical solutions to these issues. We review how breaches in patient privacy can occur, present recent developments in computational data protection and discuss how they can be combined with legal and ethical perspectives to provide secure frameworks for genomic data sharing.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'.",2018-09-01,6,1000,72,980
159,30082299,Algorithms: transparency and accountability,"This opinion piece explores the issues of accountability and transparency in relation to the growing use of machine learning algorithms. Citing the recent work of the Royal Society and the British Academy, it looks at the legal protections for individuals afforded by the EU General Data Protection Regulation and asks whether the legal system will be able to adapt to rapid technological change. It concludes by calling for continuing debate that is itself accountable, transparent and public.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'.",2018-09-01,1,632,43,980
158,30082306,How should we regulate artificial intelligence?,"Using artificial intelligence (AI) technology to replace human decision-making will inevitably create new risks whose consequences are unforeseeable. This naturally leads to calls for regulation, but I argue that it is too early to attempt a general system of AI regulation. Instead, we should work incrementally within the existing legal and regulatory schemes which allocate responsibility, and therefore liability, to persons. Where AI clearly creates risks which current law and regulation cannot deal with adequately, then new regulation will be needed. But in most cases, the current system can work effectively if the producers of AI technology can provide sufficient transparency in explaining how AI decisions are made. Transparency ex post can often be achieved through retrospective analysis of the technology's operations, and will be sufficient if the main goal is to compensate victims of incorrect decisions. Ex ante transparency is more challenging, and can limit the use of some AI technologies such as neural networks. It should only be demanded by regulation where the AI presents risks to fundamental rights, or where society needs reassuring that the technology can safely be used. Masterly inactivity in regulation is likely to achieve a better long-term solution than a rush to regulate in ignorance.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'.",2018-09-01,4,1461,47,980
157,30082307,Algorithmic accountability,"There is enormous opportunity for positive social impact from the rise of algorithms and machine learning. But this requires a licence to operate from the public, based on trustworthiness. There are a range of concerns relating to how algorithms might be held to account in areas affecting the public sphere. This paper outlines a number of approaches including greater transparency, monitoring of outcomes and improved governance. It makes a case that public sector bodies that hold datasets should be more confident in negotiating terms with the private sector. It also argues that all regulators (not just data regulators) need to wake up to the challenges posed by changing technology. Other improvements include diversity of the workforce, ethics training, codes of conduct for data scientists, and new deliberative bodies. Even if these narrower issues are solved, the paper poses some wider concerns including data monopolies, the challenge to democracy, public participation and maintaining the public interest.This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'.",2018-09-01,2,1157,26,980
135,30122222,Recent applications of machine learning in medicinal chemistry,"In recent decades, artificial intelligence and machine learning have played a significant role in increasing the efficiency of processes across a wide spectrum of industries. When it comes to the pharmaceutical and biotechnology sectors, numerous tools enabled by advancement of computer science have been developed and are now routinely utilized. However, there are many aspects of the drug discovery process, which can further benefit from refinement of computational methods and tools, as well as improvement of accessibility of these new technologies. In this review, examples of recent developments in machine learning application are described, which have the potential to impact different parts of the drug discovery and development flow scheme. Notably, new deep learning-based approaches across compound design and synthesis, prediction of binding, activity and ADMET properties, as well as applications of genetic algorithms are highlighted.",2018-09-01,8,951,62,980
2956,29789268,Making Individual Prognoses in Psychiatry Using Neuroimaging and Machine Learning,"Psychiatric prognosis is a difficult problem. Making a prognosis requires looking far into the future, as opposed to making a diagnosis, which is concerned with the current state. During the follow-up period, many factors will influence the course of the disease. Combined with the usually scarcer longitudinal data and the variability in the definition of outcomes/transition, this makes prognostic predictions a challenging endeavor. Employing neuroimaging data in this endeavor introduces the additional hurdle of high dimensionality. Machine learning techniques are especially suited to tackle this challenging problem. This review starts with a brief introduction to machine learning in the context of its application to clinical neuroimaging data. We highlight a few issues that are especially relevant for prediction of outcome and transition using neuroimaging. We then review the literature that discusses the application of machine learning for this purpose. Critical examination of the studies and their results with respect to the relevant issues revealed the following: 1) there is growing evidence for the prognostic capability of machine learning-based models using neuroimaging; and 2) reported accuracies may be too optimistic owing to small sample sizes and the lack of independent test samples. Finally, we discuss options to improve the reliability of (prognostic) prediction models. These include new methodologies and multimodal modeling. Paramount, however, is our conclusion that future work will need to provide properly (cross-)validated accuracy estimates of models trained on sufficiently large datasets. Nevertheless, with the technological advances enabling acquisition of large databases of patients and healthy subjects, machine learning represents a powerful tool in the search for psychiatric biomarkers.",2018-09-01,23,1838,81,980
2916,29923622,Advances in the computational and molecular understanding of the prostate cancer cell nucleus,"Nuclear alterations are a hallmark of many types of cancers, including prostate cancer (PCa). Recent evidence shows that subvisual changes, ones that may not be visually perceptible to a pathologist, to the nucleus and its ultrastructural components can precede visual histopathological recognition of cancer. Alterations to nuclear features, such as nuclear size and shape, texture, and spatial architecture, reflect the complex molecular-level changes that occur during oncogenesis. Quantitative nuclear morphometry, a field that uses computational approaches to identify and quantify malignancy-induced nuclear changes, can enable a detailed and objective analysis of the PCa cell nucleus. Recent advances in machine learning-based approaches can now automatically mine data related to these changes to aid in the diagnosis, decision making, and prediction of PCa prognoses. In this review, we use PCa as a case study to connect the molecular-level mechanisms that underlie these nuclear changes to the machine learning computational approaches, bridging the gap between the clinical and computational understanding of PCa. First, we will discuss recent developments to our understanding of the molecular events that drive nuclear alterations in the context of PCa: the role of the nuclear matrix and lamina in size and shape changes, the role of 3-dimensional chromatin organization and epigenetic modifications in textural changes, and the role of the tumor microenvironment in altering nuclear spatial topology. We will then discuss the advances in the applications of machine learning algorithms to automatically segment nuclei in prostate histopathological images, extract nuclear features to aid in diagnostic decision making, and predict potential outcomes, such as biochemical recurrence and survival. Finally, we will discuss the challenges and opportunities associated with translation of the quantitative nuclear morphometry methodology into the clinical space. Ultimately, accurate identification and quantification of nuclear alterations can contribute to the field of nucleomics and has applications for computationally driven precision oncologic patient care.",2018-09-01,3,2177,93,980
466,30298124,Classification of Pediatric Asthma: From Phenotype Discovery to Clinical Practice,"Advances in big data analytics have created an opportunity for a step change in unraveling mechanisms underlying the development of complex diseases such as asthma, providing valuable insights that drive better diagnostic decision-making in clinical practice, and opening up paths to individualized treatment plans. However, translating findings from data-driven analyses into meaningful insights and actionable solutions requires approaches and tools which move beyond mining and patterning longitudinal data. The purpose of this review is to summarize recent advances in phenotyping of asthma, to discuss key hurdles currently hampering the translation of phenotypic variation into mechanistic insights and clinical setting, and to suggest potential solutions that may address these limitations and accelerate moving discoveries into practice. In order to advance the field of phenotypic discovery, greater focus should be placed on investigating the extent of within-phenotype variation. We advocate a more cautious modeling approach by ""supervising"" the findings to delineate more precisely the characteristics of the individual trajectories assigned to each phenotype. Furthermore, it is important to employ different methods within a study to compare the stability of derived phenotypes, and to assess the immutability of individual assignments to phenotypes. If we are to make a step change toward precision (stratified or personalized) medicine and capitalize on the available big data assets, we have to develop genuine cross-disciplinary collaborations, wherein data scientists who turn data into information using algorithms and machine learning, team up with medical professionals who provide deep insights on specific subjects from a clinical perspective.",2018-09-01,5,1768,81,980
481,30254580,Optimizing Clinical Assessments in Parkinson's Disease Through the Use of Wearable Sensors and Data Driven Modeling,"The emergence of motion sensors as a tool that provides objective motor performance data on individuals afflicted with Parkinson's disease offers an opportunity to expand the horizon of clinical care for this neurodegenerative condition. Subjective clinical scales and patient based motor diaries have limited clinometric properties and produce a glimpse rather than continuous real time perspective into motor disability. Furthermore, the expansion of machine learn algorithms is yielding novel classification and probabilistic clinical models that stand to change existing treatment paradigms, refine the application of advance therapeutics, and may facilitate the development and testing of disease modifying agents for this disease. We review the use of inertial sensors and machine learning algorithms in Parkinson's disease.",2018-09-01,10,830,115,980
488,30240512,Applications of mechanistic modelling to clinical and experimental immunology: an emerging technology to accelerate immunotherapeutic discovery and development,"The application of in-silico modelling is beginning to emerge as a key methodology to advance our understanding of mechanisms of disease pathophysiology and related drug action, and in the design of experimental medicine and clinical studies. From this perspective, we will present a non-technical discussion of a small number of recent and historical applications of mathematical, statistical and computational modelling to clinical and experimental immunology. We focus specifically upon mechanistic questions relating to human viral infection, tumour growth and metastasis and T cell activation. These exemplar applications highlight the potential of this approach to impact upon human immunology informed by ever-expanding experimental, clinical and 'omics' data. Despite the capacity of mechanistic modelling to accelerate therapeutic discovery and development and to de-risk clinical trial design, it is not utilized widely across the field. We outline ongoing challenges facing the integration of mechanistic modelling with experimental and clinical immunology, and suggest how these may be overcome. Advances in key technologies, including multi-scale modelling, machine learning and the wealth of 'omics' data sets, coupled with advancements in computational capacity, are providing the basis for mechanistic modelling to impact on immunotherapeutic discovery and development during the next decade.",2018-09-01,3,1408,159,980
2961,29774657,"Cheminformatics in Drug Discovery, an Industrial Perspective","Cheminformatics has established itself as a core discipline within large scale drug discovery operations. It would be impossible to handle the amount of data generated today in a small molecule drug discovery project without persons skilled in cheminformatics. In addition, due to increased emphasis on ""Big Data"", machine learning and artificial intelligence, not only in the society in general, but also in drug discovery, it is expected that the cheminformatics field will be even more important in the future. Traditional areas like virtual screening, library design and high-throughput screening analysis are highlighted in this review. Applying machine learning in drug discovery is an area that has become very important. Applications of machine learning in early drug discovery has been extended from predicting ADME properties and target activity to tasks like de novo molecular design and prediction of chemical reactions.",2018-09-01,6,932,60,980
489,30238167,"Dermatoscopy of Neoplastic Skin Lesions: Recent Advances, Updates, and Revisions","Dermatoscopy (dermoscopy) improves the diagnosis of benign and malignant cutaneous neoplasms in comparison with examination with the unaided eye and should be used routinely for all pigmented and non-pigmented cutaneous neoplasms. It is especially useful for the early stage of melanoma when melanoma-specific criteria are invisible to the unaided eye. Preselection by the unaided eye is therefore not recommended. The increased availability of polarized dermatoscopes, and the extended use of dermatoscopy in non-pigmented lesions led to the discovery of new criteria, and we recommend that lesions should be examined with polarized and non-polarized dermatoscopy. The ""chaos and clues algorithm"" is a good starting point for beginners because it is easy to use, accurate, and it works for all types of pigmented lesions not only for those melanocytic. Physicians, who use dermatoscopy routinely, should be aware of new clues for acral melanomas, nail matrix melanomas, melanoma in situ, and nodular melanoma. Dermatoscopy should also be used to distinguish between different subtypes of basal cell carcinoma and to discriminate highly from poorly differentiated squamous cell carcinomas to optimize therapy and management of non-melanoma skin cancer. One of the most exciting areas of research is the use of dermatoscopic images for machine learning and automated diagnosis. Convolutional neural networks trained with dermatoscopic images are able to diagnose pigmented lesions with the same accuracy as human experts. We humans should not be afraid of this new and exciting development because it will most likely lead to a peaceful and fruitful coexistence of human experts and decision support systems.",2018-09-01,5,1707,80,980
490,30237869,POLLAR: Impact of air POLLution on Asthma and Rhinitis; a European Institute of Innovation and Technology Health (EIT Health) project,"Allergic rhinitis (AR) is impacted by allergens and air pollution but interactions between air pollution, sleep and allergic diseases are insufficiently understood. POLLAR (Impact of air POLLution on sleep, Asthma and Rhinitis) is a project of the European Institute of Innovation and Technology (EIT Health). It will use a freely-existing application for AR monitoring that has been tested in 23 countries (the Allergy Diary, iOS and Android, 17,000 users, TLR8). The Allergy Diary will be combined with a new tool allowing queries on allergen, pollen (TLR2), sleep quality and disorders (TRL2) as well as existing longitudinal and geolocalized pollution data. Machine learning will be used to assess the relationship between air pollution, sleep and AR comparing polluted and non-polluted areas in 6 EU countries. Data generated in 2018 will be confirmed in 2019 and extended by the individual prospective assessment of pollution (portable sensor, TLR7) in AR. Sleep apnea patients will be used as a demonstrator of sleep disorder that can be modulated in terms of symptoms and severity by air pollution and AR. The geographic information system GIS will map the results. Consequences on quality of life (EQ-5D), asthma, school, work and sleep will be monitored and disseminated towards the population. The impacts of POLLAR will be (1) to propose novel care pathways integrating pollution, sleep and patients' literacy, (2) to study sleep consequences of pollution and its impact on frequent chronic diseases, (3) to improve work productivity, (4) to propose the basis for a sentinel network at the EU level for pollution and allergy, (5) to assess the societal implications of the interaction. MASK paper N32.",2018-09-01,15,1714,133,980
493,30225234,Application of artificial intelligence in ophthalmology,"Artificial intelligence is a general term that means to accomplish a task mainly by a computer, with the least human beings participation, and it is widely accepted as the invention of robots. With the development of this new technology, artificial intelligence has been one of the most influential information technology revolutions. We searched these English-language studies relative to ophthalmology published on PubMed and Springer databases. The application of artificial intelligence in ophthalmology mainly concentrates on the diseases with a high incidence, such as diabetic retinopathy, age-related macular degeneration, glaucoma, retinopathy of prematurity, age-related or congenital cataract and few with retinal vein occlusion. According to the above studies, we conclude that the sensitivity of detection and accuracy for proliferative diabetic retinopathy ranged from 75% to 91.7%, for non-proliferative diabetic retinopathy ranged from 75% to 94.7%, for age-related macular degeneration it ranged from 75% to 100%, for retinopathy of prematurity ranged over 95%, for retinal vein occlusion just one study reported ranged over 97%, for glaucoma ranged 63.7% to 93.1%, and for cataract it achieved a more than 70% similarity against clinical grading.",2018-09-01,13,1264,55,980
449,30319422,"Empirical Scoring Functions for Structure-Based Virtual Screening: Applications, Critical Aspects, and Challenges","Structure-based virtual screening (VS) is a widely used approach that employs the knowledge of the three-dimensional structure of the target of interest in the design of new lead compounds from large-scale molecular docking experiments. Through the prediction of the binding mode and affinity of a small molecule within the binding site of the target of interest, it is possible to understand important properties related to the binding process. Empirical scoring functions are widely used for pose and affinity prediction. Although pose prediction is performed with satisfactory accuracy, the correct prediction of binding affinity is still a challenging task and crucial for the success of structure-based VS experiments. There are several efforts in distinct fronts to develop even more sophisticated and accurate models for filtering and ranking large libraries of compounds. This paper will cover some recent successful applications and methodological advances, including strategies to explore the ligand entropy and solvent effects, training with sophisticated machine-learning techniques, and the use of quantum mechanics. Particular emphasis will be given to the discussion of critical aspects and further directions for the development of more accurate empirical scoring functions.",2018-09-01,27,1290,113,980
672,29025029,CT coronary imaging-a fast evolving world,"Computed tomography (CT) has become an important modality in the evaluation of coronary artery disease (CAD). The tremendous technological advances in CT in the last two decades has made it possible to obtain high quality images of coronary arteries with high spatial and temporal resolutions. Multiple trials have confirmed the accuracy of CT compared to invasive catheter angiography. CT is also able to evaluate beyond the lumen in characterizing and quantifying atherosclerotic plaques, including evaluation of high risk features. Although CTA has low specificity in identification of lesion-specific ischemia, functional techniques are now possible such as CT myocardial perfusion and CT-fractional flow reserve (FFR) which evaluate the hemodynamic significance of stenosis and help with revascularization strategies. Multi-energy CT provides additional information beyond what is possible with a conventional CT and is useful in variety of clinical applications, including myocardial perfusion imaging, lesion characterization and low contrast studies. Large trials have confirmed the ability of CT to predict major adverse cardiovascular events and recent trials have even demonstrated improved clinical outcomes by using CT for the evaluation of CAD. CT is also useful in structural heart disease and 3 D printing is now increasingly used for surgical/interventional planning. Machine learning is evolving rapidly and is likely to impact diagnosis and management.",2018-09-01,0,1471,41,980
2590,30443413,Digital Epidemiology: Use of Digital Data Collected for Non-epidemiological Purposes in Epidemiological Studies,"Objectives:                    We reviewed digital epidemiological studies to characterize how researchers are using digital data by topic domain, study purpose, data source, and analytic method.              Methods:                    We reviewed research articles published within the last decade that used digital data to answer epidemiological research questions. Data were abstracted from these articles using a data collection tool that we developed. Finally, we summarized the characteristics of the digital epidemiological studies.              Results:                    We identified six main topic domains: infectious diseases (58.7%), non-communicable diseases (29.4%), mental health and substance use (8.3%), general population behavior (4.6%), environmental, dietary, and lifestyle (4.6%), and vital status (0.9%). We identified four categories for the study purpose: description (22.9%), exploration (34.9%), explanation (27.5%), and prediction and control (14.7%). We identified eight categories for the data sources: web search query (52.3%), social media posts (31.2%), web portal posts (11.9%), webpage access logs (7.3%), images (7.3%), mobile phone network data (1.8%), global positioning system data (1.8%), and others (2.8%). Of these, 50.5% used correlation analyses, 41.3% regression analyses, 25.6% machine learning, and 19.3% descriptive analyses.              Conclusions:                    Digital data collected for non-epidemiological purposes are being used to study health phenomena in a variety of topic domains. Digital epidemiology requires access to large datasets and advanced analytics. Ensuring open access is clearly at odds with the desire to have as little personal data as possible in these large datasets to protect privacy. Establishment of data cooperatives with restricted access may be a solution to this dilemma.",2018-10-01,9,1865,111,950
1068,31458044,Artificial Intelligence: The Future for Organic Chemistry?,"On the basis of a recent article ""Predicting reaction performance in C-N cross-coupling using machine learning"" that appeared in Science, we had decided to highlight the way forward for artificial intelligence in chemistry. Synthesis of molecules remains one of the most important challenges in organic chemistry, and the standard approach involved by a chemist to solve a problem is based on experience and constitutes a repetitive, time-consuming task, often resulting in nonoptimized solutions. Thus, considering the recent phenomenal progresses that have been made in machine learning, there is little doubt that these systems, once fully operational in organic chemistry, will dramatically speed up development of new drugs and will constitute the future of chemistry.",2018-10-01,3,773,58,950
2575,30488745,Computational approaches for skin sensitization prediction,"Drugs, cosmetics, preservatives, fragrances, pesticides, metals, and other chemicals can cause skin sensitization. The ability to predict the skin sensitization potential and potency of substances is therefore of enormous importance to a host of different industries, to customers' and workers' safety. Animal experiments have been the preferred testing method for most risk assessment and regulatory purposes but considerable efforts to replace them with non-animal models and in silico models are ongoing. This review provides a comprehensive overview of the computational approaches and models that have been developed for skin sensitization prediction over the last 10 years. The scope and limitations of rule-based approaches, read-across, linear and nonlinear (quantitative) structure-activity relationship ((Q)SAR) modeling, hybrid or combined approaches, and models integrating computational methods with experimental results are discussed followed by examples of relevant models. Emphasis is placed on models that are accessible to the scientific community, and on model validation. A dedicated section reports on comparative performance assessments of various approaches and models. The review also provides a concise overview of relevant data sources on skin sensitization.",2018-10-01,3,1284,58,950
2553,30532667,A Review of Denoising Medical Images Using Machine Learning Approaches,"Background:                    This paper attempts to identify suitable Machine Learning (ML) approach for image denoising of radiology based medical application. The Identification of ML approach is based on (i) Review of ML approach for denoising (ii) Review of suitable Medical Denoising approach.              Discussion:                    The review focuses on six application of radiology: Medical Ultrasound (US) for fetus development, US Computer Aided Diagnosis (CAD) and detection for breast, skin lesions, brain tumor MRI diagnosis, X-Ray for chest analysis, Breast cancer using MRI imaging. This survey identifies the ML approach with better accuracy for medical diagnosis by radiologists. The image denoising approaches further includes basic filtering techniques, wavelet medical denoising, curvelet and optimization techniques. In most of the applications, the machine learning performance is better than the conventional image denoising techniques. For fast and computational results the radiologists are using the machine learning methods on MRI, US, X-Ray and Skin lesion images. The characteristics and contributions of different ML approaches are considered in this paper.              Conclusion:                    The problem faced by the researchers during image denoising techniques and machine learning applications for clinical settings have also been discussed.",2018-10-01,4,1390,70,950
2887,29989994,Deep Learning in Microscopy Image Analysis: A Survey,"Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning techniques have powered many aspects of medical investigation and clinical practice. Recently, deep learning is emerging as a leading machine learning tool in computer vision and has attracted considerable attention in biomedical image analysis. In this paper, we provide a snapshot of this fast-growing field, specifically for microscopy image analysis. We briefly introduce the popular deep neural networks and summarize current deep learning achievements in various tasks, such as detection, segmentation, and classification in microscopy image analysis. In particular, we explain the architectures and the principles of convolutional neural networks, fully convolutional networks, recurrent neural networks, stacked autoencoders, and deep belief networks, and interpret their formulations or modelings for specific tasks on various microscopy images. In addition, we discuss the open challenges and the potential trends of future research in microscopy image analysis using deep learning.",2018-10-01,18,1115,52,950
428,30368611,"Machine Learning for Predicting Cognitive Diseases: Methods, Data Sources and Risk Factors","Machine learning and data mining approaches are being successfully applied to different fields of life sciences for the past 20 years. Medicine is one of the most suitable application domains for these techniques since they help model diagnostic information based on causal and/or statistical data and therefore reveal hidden dependencies between symptoms and illnesses. In this paper we give a detailed overview of the recent machine learning research and its applications for predicting cognitive diseases, especially the Alzheimer's disease, mild cognitive impairment and the Parkinson's disease. We survey different state-of-the-art methodological approaches, data sources and public data, and provide their comparative analysis. We conclude by identifying the open problems within the field that include an early detection of the cognitive diseases and inclusion of machine learning tools into diagnostic practice and therapy planning.",2018-10-01,11,940,90,950
471,30287797,Machine Learning Approaches for ProteinProtein Interaction Hot Spot Prediction: Progress and Comparative Assessment,"Hot spots are the subset of interface residues that account for most of the binding free energy, and they play essential roles in the stability of protein binding. Effectively identifying which specific interface residues of proteinprotein complexes form the hot spots is critical for understanding the principles of protein interactions, and it has broad application prospects in protein design and drug development. Experimental methods like alanine scanning mutagenesis are labor-intensive and time-consuming. At present, the experimentally measured hot spots are very limited. Hence, the use of computational approaches to predicting hot spots is becoming increasingly important. Here, we describe the basic concepts and recent advances of machine learning applications in inferring the proteinprotein interaction hot spots, and assess the performance of widely used features, machine learning algorithms, and existing state-of-the-art approaches. We also discuss the challenges and future directions in the prediction of hot spots.",2018-10-01,11,1038,116,950
171,30054833,Artificial intelligence in drug design,"Thanks to the fast improvement of the computing power and the rapid development of the computational chemistry and biology, the computer-aided drug design techniques have been successfully applied in almost every stage of the drug discovery and development pipeline to speed up the process of research and reduce the cost and risk related to preclinical and clinical trials. Owing to the development of machine learning theory and the accumulation of pharmacological data, the artificial intelligence (AI) technology, as a powerful data mining tool, has cut a figure in various fields of the drug design, such as virtual screening, activity scoring, quantitative structure-activity relationship (QSAR) analysis, de novo drug design, and in silico evaluation of absorption, distribution, metabolism, excretion and toxicity (ADME/T) properties. Although it is still challenging to provide a physical explanation of the AI-based models, it indeed has been acting as a great power to help manipulating the drug discovery through the versatile frameworks. Recently, due to the strong generalization ability and powerful feature extraction capability, deep learning methods have been employed in predicting the molecular properties as well as generating the desired molecules, which will further promote the application of AI technologies in the field of drug design.",2018-10-01,7,1361,38,950
183,30044996,Machine learning detects EEG microstate alterations in patients living with temporal lobe epilepsy,"Purpose:                    Quasi-stable electrical distribution in EEG called microstates could carry useful information on the dynamics of large scale brain networks. Using machine learning techniques we explored if abnormalities in microstates can identify patients with Temporal Lobe Epilepsy (TLE) in the absence of an interictal discharge (IED).              Method:                    4 Classes of microstates were computed from 2 min artefact free EEG epochs in 42 subjects (21 TLE and 21 controls). The percentage of time coverage, frequency of occurrence and duration for each of these microstates were computed and redundancy reduced using feature selection methods. Subsequently, Fishers Linear Discriminant Analysis (FLDA) and logistic regression were used for classification.              Result:                    FLDA distinguished TLE with 76.1% accuracy (85.0% sensitivity, 66.6% specificity) considering frequency of occurrence and percentage of time coverage of microstate C as features.              Conclusion:                    Microstate alterations are present in patients with TLE. This feature might be useful in the diagnosis of epilepsy even in the absence of an IED.",2018-10-01,2,1198,98,950
185,30041284,Understanding Neurogastroenterology From Neuroimaging Perspective: A Comprehensive Review of Functional and Structural Brain Imaging in Functional Gastrointestinal Disorders,"This review provides a comprehensive overview of brain imaging studies of the brain-gut interaction in functional gastrointestinal disorders (FGIDs). Functional neuroimaging studies during gut stimulation have shown enhanced brain responses in regions related to sensory processing of the homeostatic condition of the gut (homeostatic afferent) and responses to salience stimuli (salience network), as well as increased and decreased brain activity in the emotional response areas and reduced activation in areas associated with the top-down modulation of visceral afferent signals. Altered central regulation of the endocrine and autonomic nervous responses, the key mediators of the brain-gut axis, has been demonstrated. Studies using resting-state functional magnetic resonance imaging reported abnormal local and global connectivity in the areas related to pain processing and the default mode network (a physiological baseline of brain activity at rest associated with self-awareness and memory) in FGIDs. Structural imaging with brain morphometry and diffusion imaging demonstrated altered gray- and white-matter structures in areas that also showed changes in functional imaging studies, although this requires replication. Molecular imaging by magnetic resonance spectroscopy and positron emission tomography in FGIDs remains relatively sparse. Progress using analytical methods such as machine learning algorithms may shift neuroimaging studies from brain mapping to predicting clinical outcomes. Because several factors contribute to the pathophysiology of FGIDs and because its population is quite heterogeneous, a new model is needed in future studies to assess the importance of the factors and brain functions that are responsible for an optimal homeostatic state.",2018-10-01,13,1779,173,950
405,30410432,Deep Learning With Spiking Neurons: Opportunities and Challenges,"Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.",2018-10-01,20,1988,64,950
414,30381431,Latent Factors and Dynamics in Motor Cortex and Their Application to Brain-Machine Interfaces,"In the 1960s, Evarts first recorded the activity of single neurons in motor cortex of behaving monkeys (Evarts, 1968). In the 50 years since, great effort has been devoted to understanding how single neuron activity relates to movement. Yet these single neurons exist within a vast network, the nature of which has been largely inaccessible. With advances in recording technologies, algorithms, and computational power, the ability to study these networks is increasing exponentially. Recent experimental results suggest that the dynamical properties of these networks are critical to movement planning and execution. Here we discuss this dynamical systems perspective and how it is reshaping our understanding of the motor cortices. Following an overview of key studies in motor cortex, we discuss techniques to uncover the ""latent factors"" underlying observed neural population activity. Finally, we discuss efforts to use these factors to improve the performance of brain-machine interfaces, promising to make these findings broadly relevant to neuroengineering as well as systems neuroscience.",2018-10-01,10,1097,93,950
444,30334108,Machine Meets Biology: a Primer on Artificial Intelligence in Cardiology and Cardiac Imaging,"Purpose of review:                    An understanding of the basics concepts of deep learning can be helpful in not only understanding the potential applications of this technique but also in critically reviewing literature in which neural networks are utilized for analysis and modeling.              Recent findings:                    The term ""deep learning"" has been applied to a subset of machine learning that utilizes a ""neural network"" and is often used interchangeably with ""artificial intelligence."" It has been increasingly utilized in healthcare for computational ""learning"", especially for pattern recognition for diagnostic imaging. Another promising application is the potential for these neural networks to improve the accuracy in the identification of patients who are at risk for cardiovascular events and could benefit most from preventive treatment in comparison with more conventional statistical techniques. The importance of such tailored cardiovascular risk assessment and disease management in individual patients is far reaching given that cardiovascular disease is the leading cause of morbidity and mortality in the world. Nearly half of myocardial infarctions and strokes occur in patients who are not predicted to be at risk for cardiovascular events by current guideline-based approaches. Equally important are individuals who are not at risk for cardiovascular events and yet are given expensive and unnecessary preventive treatment with potential untoward side effects. The application of powerful artificial intelligence/deep learning tools in medicine is likely to result in more effective and efficient health care delivery with the potential for significant cost savings by shifting preventative treatment from inappropriate to appropriate patient subgroups.",2018-10-01,3,1797,92,950
423,30374293,Analysis Tools for Large Connectomes,"New reconstruction techniques are generating connectomes of unprecedented size. These must be analyzed to generate human comprehensible results. The analyses being used fall into three general categories. The first is interactive tools used during reconstruction, to help guide the effort, look for possible errors, identify potential cell classes, and answer other preliminary questions. The second type of analysis is support for formal documents such as papers and theses. Scientific norms here require that the data be archived and accessible, and the analysis reproducible. In contrast to some other ""omic"" fields such as genomics, where a few specific analyses dominate usage, connectomics is rapidly evolving and the analyses used are often specific to the connectome being analyzed. These analyses are typically performed in a variety of conventional programming language, such as Matlab, R, Python, or C++, and read the connectomic data either from a file or through database queries, neither of which are standardized. In the short term we see no alternative to the use of specific analyses, so the best that can be done is to publish the analysis code, and the interface by which it reads connectomic data. A similar situation exists for archiving connectome data. Each group independently makes their data available, but there is no standardized format and long-term accessibility is neither enforced nor funded. In the long term, as connectomics becomes more common, a natural evolution would be a central facility for storing and querying connectomic data, playing a role similar to the National Center for Biotechnology Information for genomes. The final form of analysis is the import of connectome data into downstream tools such as neural simulation or machine learning. In this process, there are two main problems that need to be addressed. First, the reconstructed circuits contain huge amounts of detail, which must be intelligently reduced to a form the downstream tools can use. Second, much of the data needed for these downstream operations must be obtained by other methods (such as genetic or optical) and must be merged with the extracted connectome.",2018-10-01,2,2179,36,950
431,30364792,Artificial intelligence in gastrointestinal endoscopy: The future is almost here,"Artificial intelligence (AI) enables machines to provide unparalleled value in a myriad of industries and applications. In recent years, researchers have harnessed artificial intelligence to analyze large-volume, unstructured medical data and perform clinical tasks, such as the identification of diabetic retinopathy or the diagnosis of cutaneous malignancies. Applications of artificial intelligence techniques, specifically machine learning and more recently deep learning, are beginning to emerge in gastrointestinal endoscopy. The most promising of these efforts have been in computer-aided detection and computer-aided diagnosis of colorectal polyps, with recent systems demonstrating high sensitivity and accuracy even when compared to expert human endoscopists. AI has also been utilized to identify gastrointestinal bleeding, to detect areas of inflammation, and even to diagnose certain gastrointestinal infections. Future work in the field should concentrate on creating seamless integration of AI systems with current endoscopy platforms and electronic medical records, developing training modules to teach clinicians how to use AI tools, and determining the best means for regulation and approval of new AI technology.",2018-10-01,28,1231,80,950
432,30361937,Big Data in Head and Neck Cancer,"Head and neck cancers can be used as a paradigm for exploring ""big data"" applications in oncology. Computational strategies derived from big data science hold the promise of shedding new light on the molecular mechanisms driving head and neck cancer pathogenesis, identifying new prognostic and predictive factors, and discovering potential therapeutics against this highly complex disease. Big data strategies integrate robust data input, from radiomics, genomics, and clinical-epidemiological data to deeply describe head and neck cancer characteristics. Thus, big data may advance research generating new knowledge and improve head and neck cancer prognosis supporting clinical decision-making and development of treatment recommendations.",2018-10-01,5,742,32,950
435,30356770,Challenges and Future Perspectives on Electroencephalogram-Based Biometrics in Person Recognition,"The emergence of the digital world has greatly increased the number of accounts and passwords that users must remember. It has also increased the need for secure access to personal information in the cloud. Biometrics is one approach to person recognition, which can be used in identification as well as authentication. Among the various modalities that have been developed, electroencephalography (EEG)-based biometrics features unparalleled universality, distinctiveness and collectability, while minimizing the risk of circumvention. However, commercializing EEG-based person recognition poses a number of challenges. This article reviews the various systems proposed over the past few years with a focus on the shortcomings that have prevented wide-scale implementation, including issues pertaining to temporal stability, psychological and physiological changes, protocol design, equipment and performance evaluation. We also examine several directions for the further development of usable EEG-based recognition systems as well as the niche markets to which they could be applied. It is expected that rapid advancements in EEG instrumentation, on-device processing and machine learning techniques will lead to the emergence of commercialized person recognition systems in the near future.",2018-10-01,4,1293,97,950
436,30356768,Network-Based Methods for Prediction of Drug-Target Interactions,"Drug-target interaction (DTI) is the basis of drug discovery. However, it is time-consuming and costly to determine DTIs experimentally. Over the past decade, various computational methods were proposed to predict potential DTIs with high efficiency and low costs. These methods can be roughly divided into several categories, such as molecular docking-based, pharmacophore-based, similarity-based, machine learning-based, and network-based methods. Among them, network-based methods, which do not rely on three-dimensional structures of targets and negative samples, have shown great advantages over the others. In this article, we focused on network-based methods for DTI prediction, in particular our network-based inference (NBI) methods that were derived from recommendation algorithms. We first introduced the methodologies and evaluation of network-based methods, and then the emphasis was put on their applications in a wide range of fields, including target prediction and elucidation of molecular mechanisms of therapeutic effects or safety problems. Finally, limitations and perspectives of network-based methods were discussed. In a word, network-based methods provide alternative tools for studies in drug repurposing, new drug discovery, systems pharmacology and systems toxicology.",2018-10-01,16,1296,64,950
437,30353365,Artificial intelligence in medical imaging: threat or opportunity? Radiologists again at the forefront of innovation in medicine,"One of the most promising areas of health innovation is the application of artificial intelligence (AI), primarily in medical imaging. This article provides basic definitions of terms such as ""machine/deep learning"" and analyses the integration of AI into radiology. Publications on AI have drastically increased from about 100-150 per year in 2007-2008 to 700-800 per year in 2016-2017. Magnetic resonance imaging and computed tomography collectively account for more than 50% of current articles. Neuroradiology appears in about one-third of the papers, followed by musculoskeletal, cardiovascular, breast, urogenital, lung/thorax, and abdomen, each representing 6-9% of articles. With an irreversible increase in the amount of data and the possibility to use AI to identify findings either detectable or not by the human eye, radiology is now moving from a subjective perceptual skill to a more objective science. Radiologists, who were on the forefront of the digital era in medicine, can guide the introduction of AI into healthcare. Yet, they will not be replaced because radiology includes communication of diagnosis, consideration of patient's values and preferences, medical judgment, quality assurance, education, policy-making, and interventional procedures. The higher efficiency provided by AI will allow radiologists to perform more value-added tasks, becoming more visible to patients and playing a vital role in multidisciplinary clinical teams.",2018-10-01,64,1461,128,950
443,30337064,Computer-aided diagnosis of glaucoma using fundus images: A review,"Background and objectives:                    Glaucoma is an eye condition which leads to permanent blindness when the disease progresses to an advanced stage. It occurs due to inappropriate intraocular pressure within the eye, resulting in damage to the optic nerve. Glaucoma does not exhibit any symptoms in its nascent stage and thus, it is important to diagnose early to prevent blindness. Fundus photography is widely used by ophthalmologists to assist in diagnosis of glaucoma and is cost-effective.              Methods:                    The morphological features of the disc that is characteristic of glaucoma are clearly seen in the fundus images. However, manual inspection of the acquired fundus images may be prone to inter-observer variation. Therefore, a computer-aided detection (CAD) system is proposed to make an accurate, reliable and fast diagnosis of glaucoma based on the optic nerve features of fundus imaging. In this paper, we reviewed existing techniques to automatically diagnose glaucoma.              Results:                    The use of CAD is very effective in the diagnosis of glaucoma and can assist the clinicians to alleviate their workload significantly. We have also discussed the advantages of employing state-of-art techniques, including deep learning (DL), when developing the automated system. The DL methods are effective in glaucoma diagnosis.              Conclusions:                    Novel DL algorithms with big data availability are required to develop a reliable CAD system. Such techniques can be employed to diagnose other eye diseases accurately.",2018-10-01,4,1604,66,950
465,30298337,Medical Image Analysis using Convolutional Neural Networks: A Review,"The science of solving clinical problems by analyzing images generated in clinical practice is known as medical image analysis. The aim is to extract information in an affective and efficient manner for improved clinical diagnosis. The recent advances in the field of biomedical engineering have made medical image analysis one of the top research and development area. One of the reasons for this advancement is the application of machine learning techniques for the analysis of medical images. Deep learning is successfully used as a tool for machine learning, where a neural network is capable of automatically learning features. This is in contrast to those methods where traditionally hand crafted features are used. The selection and calculation of these features is a challenging task. Among deep learning techniques, deep convolutional networks are actively used for the purpose of medical image analysis. This includes application areas such as segmentation, abnormality detection, disease classification, computer aided diagnosis and retrieval. In this study, a comprehensive review of the current state-of-the-art in medical image analysis using deep convolutional networks is presented. The challenges and potential of these techniques are also highlighted.",2018-10-01,44,1269,68,950
133,30124358,"Rise of Deep Learning for Genomic, Proteomic, and Metabolomic Data Integration in Precision Medicine","Machine learning (ML) is being ubiquitously incorporated into everyday products such as Internet search, email spam filters, product recommendations, image classification, and speech recognition. New approaches for highly integrated manufacturing and automation such as the Industry 4.0 and the Internet of things are also converging with ML methodologies. Many approaches incorporate complex artificial neural network architectures and are collectively referred to as deep learning (DL) applications. These methods have been shown capable of representing and learning predictable relationships in many diverse forms of data and hold promise for transforming the future of omics research and applications in precision medicine. Omics and electronic health record data pose considerable challenges for DL. This is due to many factors such as low signal to noise, analytical variance, and complex data integration requirements. However, DL models have already been shown capable of both improving the ease of data encoding and predictive model performance over alternative approaches. It may not be surprising that concepts encountered in DL share similarities with those observed in biological message relay systems such as gene, protein, and metabolite networks. This expert review examines the challenges and opportunities for DL at a systems and biological scale for a precision medicine readership.",2018-10-01,37,1401,100,950
140,30104148,Deep Learning for Plant Stress Phenotyping: Trends and Future Perspectives,"Deep learning (DL), a subset of machine learning approaches, has emerged as a versatile tool to assimilate large amounts of heterogeneous data and provide reliable predictions of complex and uncertain phenomena. These tools are increasingly being used by the plant science community to make sense of the large datasets now regularly collected via high-throughput phenotyping and genotyping. We review recent work where DL principles have been utilized for digital image-based plant stress phenotyping. We provide a comparative assessment of DL tools against other existing techniques, with respect to decision accuracy, data size requirement, and applicability in various scenarios. Finally, we outline several avenues of research leveraging current and future DL tools in plant science.",2018-10-01,43,787,74,950
2975,29729293,Principles of Temporal Processing Across the Cortical Hierarchy,"The world is richly structured on multiple spatiotemporal scales. In order to represent spatial structure, many machine-learning models repeat a set of basic operations at each layer of a hierarchical architecture. These iterated spatial operations - including pooling, normalization and pattern completion - enable these systems to recognize and predict spatial structure, while robust to changes in the spatial scale, contrast and noisiness of the input signal. Because our brains also process temporal information that is rich and occurs across multiple time scales, might the brain employ an analogous set of operations for temporal information processing? Here we define a candidate set of temporal operations, and we review evidence that they are implemented in the mammalian cerebral cortex in a hierarchical manner. We conclude that multiple consecutive stages of cortical processing can be understood to perform temporal pooling, temporal normalization and temporal pattern completion.",2018-10-01,11,994,63,950
2886,29990622,Scoring of tumor-infiltrating lymphocytes: From visual estimation to machine learning,"The extent of tumor-infiltrating lymphocytes (TILs), along with immunomodulatory ligands, tumor-mutational burden and other biomarkers, has been demonstrated to be a marker of response to immune-checkpoint therapy in several cancers. Pathologists have therefore started to devise standardized visual approaches to quantify TILs for therapy prediction. However, despite successful standardization efforts visual TIL estimation is slow, with limited precision and lacks the ability to evaluate more complex properties such as TIL distribution patterns. Therefore, computational image analysis approaches are needed to provide standardized and efficient TIL quantification. Here, we discuss different automated TIL scoring approaches ranging from classical image segmentation, where cell boundaries are identified and the resulting objects classified according to shape properties, to machine learning-based approaches that directly classify cells without segmentation but rely on large amounts of training data. In contrast to conventional machine learning (ML) approaches that are often criticized for their ""black-box"" characteristics, we also discuss explainable machine learning. Such approaches render ML results interpretable and explain the computational decision-making process through high-resolution heatmaps that highlight TILs and cancer cells and therefore allow for quantification and plausibility checks in biomedical research and diagnostics.",2018-10-01,21,1456,85,950
2892,29981923,In-silico approach for drug induced liver injury prediction: Recent advances,"Drug induced liver injury (DILI) is the prime cause of liver disfunction which may lead to mild non-specific symptoms to more severe signs like hepatitis, cholestasis, cirrhosis and jaundice. Not only the prescription medications, but the consumption of herbs and health supplements have also been reported to cause these adverse reactions resulting into high mortality rates and post marketing withdrawal of drugs. Due to the continuously increasing DILI incidences in recent years, robust prediction methods with high accuracy, specificity and sensitivity are of priority. Bioinformatics is the emerging field of science that has been used in the past few years to explore the mechanisms of DILI. The major emphasis of this review is the recent advances of in silico tools for the diagnostic and therapeutic interventions of DILI. These tools have been developed and widely used in the past few years for the prediction of pathways induced from both hepatotoxic as well as hepatoprotective Chinese drugs and for the identification of DILI specific biomarkers for prognostic purpose. In addition to this, advanced machine learning models have been developed for the classification of drugs into DILI causing and non-DILI causing. Moreover, development of 3 class models over 2 class offers better understanding of multi-class DILI risks and at the same time providing authentic prediction of toxicity during drug designing before clinical trials.",2018-10-01,5,1447,76,950
2902,29957849,Silicon Oxide (SiO x ): A Promising Material for Resistance Switching?,"Interest in resistance switching is currently growing apace. The promise of novel high-density, low-power, high-speed nonvolatile memory devices is appealing enough, but beyond that there are exciting future possibilities for applications in hardware acceleration for machine learning and artificial intelligence, and for neuromorphic computing. A very wide range of material systems exhibit resistance switching, a number of which-primarily transition metal oxides-are currently being investigated as complementary metal-oxide-semiconductor (CMOS)-compatible technologies. Here, the case is made for silicon oxide, perhaps the most CMOS-compatible dielectric, yet one that has had comparatively little attention as a resistance-switching material. Herein, a taxonomy of switching mechanisms in silicon oxide is presented, and the current state of the art in modeling, understanding fundamental switching mechanisms, and exciting device applications is summarized. In conclusion, silicon oxide is an excellent choice for resistance-switching technologies, offering a number of compelling advantages over competing material systems.",2018-10-01,13,1131,70,950
608,29278737,Network science in clinical trials: A patient-centered approach,"There has been a paradigm shift in translational oncology with the advent of novel molecular diagnostic tools in the clinic. However, several challenges are associated with the integration of these sophisticated tools into clinical oncology and daily practice. High-throughput profiling at the DNA, RNA and protein levels (omics) generate a massive amount of data. The analysis and interpretation of these is non-trivial but will allow a more thorough understanding of cancer. Linear modelling of the data as it is often used today is likely to limit our understanding of cancer as a complex disease, and at times under-performs to capture a phenotype of interest. Network science and systems biology-based approaches, using machine learning and network science principles, that integrate multiple data sources, can uncover complex changes in a biological system. This approach will integrate a large number of potential biomarkers in preclinical studies to better inform therapeutic decisions and ultimately make substantial progress towards precision medicine. It will however require development of a new generation of clinical trials. Beyond discussing the challenges of high-throughput technologies, this review will develop a framework on how to implement a network science approach in new clinical trial designs in order to advance cancer care.",2018-10-01,3,1351,63,950
478,30267935,Structural biology meets data science: does anything change?,"Data science has emerged from the proliferation of digital data, coupled with advances in algorithms, software and hardware (e.g., GPU computing). Innovations in structural biology have been driven by similar factors, spurring us to ask: can these two fields impact one another in deep and hitherto unforeseen ways? We posit that the answer is yes. New biological knowledge lies in the relationships between sequence, structure, function and disease, all of which play out on the stage of evolution, and data science enables us to elucidate these relationships at scale. Here, we consider the above question from the five key pillars of data science: acquisition, engineering, analytics, visualization and policy, with an emphasis on machine learning as the premier analytics approach.",2018-10-01,3,785,60,950
458,30308967,Chemical Diversity of Metal Sulfide Minerals and Its Implications for the Origin of Life,"Prebiotic organic synthesis catalyzed by Earth-abundant metal sulfides is a key process for understanding the evolution of biochemistry from inorganic molecules, yet the catalytic functions of sulfides have remained poorly explored in the context of the origin of life. Past studies on prebiotic chemistry have mostly focused on a few types of metal sulfide catalysts, such as FeS or NiS, which form limited types of products with inferior activity and selectivity. To explore the potential of metal sulfides on catalyzing prebiotic chemical reactions, here, the chemical diversity (variations in chemical composition and phase structure) of 304 natural metal sulfide minerals in a mineralogy database was surveyed. Approaches to rationally predict the catalytic functions of metal sulfides are discussed based on advanced theories and analytical tools of electrocatalysis such as proton-coupled electron transfer, structural comparisons between enzymes and minerals, and in situ spectroscopy. To this end, we introduce a model of geoelectrochemistry driven prebiotic synthesis for chemical evolution, as it helps us to predict kinetics and selectivity of targeted prebiotic chemistry under ""chemically messy conditions"". We expect that combining the data-mining of mineral databases with experimental methods, theories, and machine-learning approaches developed in the field of electrocatalysis will facilitate the prediction and verification of catalytic performance under a wide range of pH and Eh conditions, and will aid in the rational screening of mineral catalysts involved in the origin of life.",2018-10-01,7,1604,88,950
107,30195423,A review of image analysis and machine learning techniques for automated cervical cancer screening from pap-smear images,"Background and objective:                    Early diagnosis and classification of a cancer type can help facilitate the subsequent clinical management of the patient. Cervical cancer ranks as the fourth most prevalent cancer affecting women worldwide and its early detection provides the opportunity to help save life. To that end, automated diagnosis and classification of cervical cancer from pap-smear images has become a necessity as it enables accurate, reliable and timely analysis of the condition's progress. This paper presents an overview of the state of the art as articulated in prominent recent publications focusing on automated detection of cervical cancer from pap-smear images.              Methods:                    The survey reviews publications on applications of image analysis and machine learning in automated diagnosis and classification of cervical cancer from pap-smear images spanning 15 years. The survey reviews 30 journal papers obtained electronically through four scientific databases (Google Scholar, Scopus, IEEE and Science Direct) searched using three sets of keywords: (1) segmentation, classification, cervical cancer; (2) medical imaging, machine learning, pap-smear; (3) automated system, classification, pap-smear.              Results:                    Most of the existing algorithms facilitate an accuracy of nearly 93.78% on an open pap-smear data set, segmented using CHAMP digital image software. K-nearest-neighbors and support vector machines algorithms have been reported to be excellent classifiers for cervical images with accuracies of over 99.27% and 98.5% respectively when applied to a 2-class classification problem (normal or abnormal).              Conclusion:                    The reviewed papers indicate that there are still weaknesses in the available techniques that result in low accuracy of classification in some classes of cells. Moreover, most of the existing algorithms work either on single or on multiple cervical smear images. This accuracy can be increased by varying various parameters such as the features to be extracted, improvement in noise removal, using hybrid segmentation and classification techniques such of multi-level classifiers. Combining K-nearest-neighbors algorithm with other algorithm(s) such as support vector machines, pixel level classifications and including statistical shape models can also improve performance. Further, most of the developed classifiers are tested on accurately segmented images using commercially available software such as CHAMP software. There is thus a deficit of evidence that these algorithms will work in clinical settings found in developing countries (where 85% of cervical cancer incidences occur) that lack sufficient trained cytologists and the funds to buy the commercial segmentation software.",2018-10-01,5,2833,120,950
117,30168572,Perspectives and applications of machine learning for evolutionary developmental biology,"Evolutionary Developmental Biology (Evo-Devo) is an ever-expanding field that aims to understand how development was modulated by the evolutionary process. In this sense, ""omic"" studies emerged as a powerful ally to unravel the molecular mechanisms underlying development. In this scenario, bioinformatics tools become necessary to analyze the growing amount of information. Among computational approaches, machine learning stands out as a promising field to generate knowledge and trace new research perspectives for bioinformatics. In this review, we aim to expose the current advances of machine learning applied to evolution and development. We draw clear perspectives and argue how evolution impacted machine learning techniques.",2018-10-01,2,734,88,950
127,30139641,Complex-Trait Prediction in the Era of Big Data,"Accurate prediction of complex traits requires using a large number of DNA variants. Advances in statistical and machine learning methodology enable the identification of complex patterns in high-dimensional settings. However, training these highly parameterized methods requires very large data sets. Until recently, such data sets were not available. But the situation is changing rapidly as very large biomedical data sets comprising individual genotype-phenotype data for hundreds of thousands of individuals become available in public and private domains. We argue that the convergence of advances in methodology and the advent of Big Genomic Data will enable unprecedented improvements in complex-trait prediction; we review theory and evidence supporting our claim and discuss challenges and opportunities that Big Data will bring to complex-trait prediction.",2018-10-01,14,866,47,950
2365,29419402,Deep Learning in Neuroradiology,"Deep learning is a form of machine learning using a convolutional neural network architecture that shows tremendous promise for imaging applications. It is increasingly being adapted from its original demonstration in computer vision applications to medical imaging. Because of the high volume and wealth of multimodal imaging information acquired in typical studies, neuroradiology is poised to be an early adopter of deep learning. Compelling deep learning research applications have been demonstrated, and their use is likely to grow rapidly. This review article describes the reasons, outlines the basic methods used to train and test deep learning models, and presents a brief overview of current and potential clinical applications with an emphasis on how they are likely to change future neuroradiology practice. Facility with these methods among neuroimaging researchers and clinicians will be important to channel and harness the vast potential of this new method.",2018-10-01,42,973,31,950
468,30295871,Multi-omic and multi-view clustering algorithms: review and cancer benchmark,"Recent high throughput experimental methods have been used to collect large biomedical omics datasets. Clustering of single omic datasets has proven invaluable for biological and medical research. The decreasing cost and development of additional high throughput methods now enable measurement of multi-omic data. Clustering multi-omic data has the potential to reveal further systems-level insights, but raises computational and biological challenges. Here, we review algorithms for multi-omics clustering, and discuss key issues in applying these algorithms. Our review covers methods developed specifically for omic data as well as generic multi-view methods developed in the machine learning community for joint clustering of multiple data types. In addition, using cancer data from TCGA, we perform an extensive benchmark spanning ten different cancer types, providing the first systematic comparison of leading multi-omics and multi-view clustering algorithms. The results highlight key issues regarding the use of single- versus multi-omics, the choice of clustering strategy, the power of generic multi-view methods and the use of approximated p-values for gauging solution quality. Due to the growing use of multi-omics data, we expect these issues to be important for future progress in the field.",2018-11-01,48,1307,76,919
494,30222245,Synchrotron Big Data Science,"The rapid development of synchrotrons has massively increased the speed at which experiments can be performed, while new techniques have increased the amount of raw data collected during each experiment. While this has created enormous new opportunities, it has also created tremendous challenges for national facilities and users. With the huge increase in data volume, the manual analysis of data is no longer possible. As a result, only a fraction of the data collected during the time- and money-expensive synchrotron beam-time is analyzed and used to deliver new science. Additionally, the lack of an appropriate data analysis environment limits the realization of experiments that generate a large amount of data in a very short period of time. The current lack of automated data analysis pipelines prevents the fine-tuning of beam-time experiments, further reducing their potential usage. These effects, collectively known as the ""data deluge,"" affect synchrotrons in several different ways including fast data collection, available local storage, data management systems, and curation of the data. This review highlights the Big Data strategies adopted nowadays at synchrotrons, documenting this novel and promising hybridization between science and technology, which promise a dramatic increase in the number of scientific discoveries.",2018-11-01,2,1344,28,919
2900,29959818,Materials Nanoarchitectonics for Mechanical Tools in Chemical and Biological Sensing,"In this Focus Review, nanoarchitectonic approaches for mechanical-action-based chemical and biological sensors are briefly discussed. In particular, recent examples of piezoelectric devices, such as quartz crystal microbalances (QCM and QCM-D) and a membrane-type surface stress sensor (MSS), are introduced. Sensors need well-designed nanostructured sensing materials for the sensitive and selective detection of specific targets. Nanoarchitectonic approaches for sensing materials, such as mesoporous materials, 2D materials, fullerene assemblies, supported lipid bilayers, and layer-by-layer assemblies, are highlighted. Based on these sensing approaches, examples of bioanalytical applications are presented for toxic gas detection, cell membrane interactions, label-free biomolecular assays, anticancer drug evaluation, complement activation-related multiprotein membrane attack complexes, and daily biodiagnosis, which are partially supported by data analysis, such as machine learning and principal component analysis.",2018-11-01,9,1025,84,919
2967,29752973,Big Data and Data Science in Critical Care,"The digitalization of the health-care system has resulted in a deluge of clinical big data and has prompted the rapid growth of data science in medicine. Data science, which is the field of study dedicated to the principled extraction of knowledge from complex data, is particularly relevant in the critical care setting. The availability of large amounts of data in the ICU, the need for better evidence-based care, and the complexity of critical illness makes the use of data science techniques and data-driven research particularly appealing to intensivists. Despite the increasing number of studies and publications in the field, thus far there have been few examples of data science projects that have resulted in successful implementations of data-driven systems in the ICU. However, given the expected growth in the field, intensivists should be familiar with the opportunities and challenges of big data and data science. The present article reviews the definitions, types of algorithms, applications, challenges, and future of big data and data science in critical care.",2018-11-01,31,1079,42,919
473,30279002,"Machine learning in human movement biomechanics: Best practices, common pitfalls, and new opportunities","Traditional laboratory experiments, rehabilitation clinics, and wearable sensors offer biomechanists a wealth of data on healthy and pathological movement. To harness the power of these data and make research more efficient, modern machine learning techniques are starting to complement traditional statistical tools. This survey summarizes the current usage of machine learning methods in human movement biomechanics and highlights best practices that will enable critical evaluation of the literature. We carried out a PubMed/Medline database search for original research articles that used machine learning to study movement biomechanics in patients with musculoskeletal and neuromuscular diseases. Most studies that met our inclusion criteria focused on classifying pathological movement, predicting risk of developing a disease, estimating the effect of an intervention, or automatically recognizing activities to facilitate out-of-clinic patient monitoring. We found that research studies build and evaluate models inconsistently, which motivated our discussion of best practices. We provide recommendations for training and evaluating machine learning models and discuss the potential of several underutilized approaches, such as deep learning, to generate new knowledge about human movement. We believe that cross-training biomechanists in data science and a cultural shift toward sharing of data and tools are essential to maximize the impact of biomechanics research.",2018-11-01,28,1477,103,919
482,30253869,"Parkinson's disease: Cause factors, measurable indicators, and early diagnosis","Parkinson's disease (PD) is a neurodegenerative disease of the central nervous system caused due to the loss of dopaminergic neurons. It is classified under movement disorder as patients with PD present with tremor, rigidity, postural changes, and a decrease in spontaneous movements. Comorbidities including anxiety, depression, fatigue, and sleep disorders are observed prior to the diagnosis of PD. Gene mutations, exposure to toxic substances, and aging are considered as the causative factors of PD even though its genesis is unknown. This paper reviews PD etiologies, progression, and in particular measurable indicators of PD such as neuroimaging and electrophysiology modalities. In addition to gene therapy, neuroprotective, pharmacological, and neural transplantation treatments, researchers are actively aiming at identifying biological markers of PD with the goal of early diagnosis. Neuroimaging modalities used together with advanced machine learning techniques offer a promising path for the early detection and intervention in PD patients.",2018-11-01,6,1055,78,919
2537,30555503,Statistical and Machine Learning Approaches to Predict Gene Regulatory Networks From Transcriptome Datasets,"Statistical and machine learning (ML)-based methods have recently advanced in construction of gene regulatory network (GRNs) based on high-throughput biological datasets. GRNs underlie almost all cellular phenomena; hence, comprehensive GRN maps are essential tools to elucidate gene function, thereby facilitating the identification and prioritization of candidate genes for functional analysis. High-throughput gene expression datasets have yielded various statistical and ML-based algorithms to infer causal relationship between genes and decipher GRNs. This review summarizes the recent advancements in the computational inference of GRNs, based on large-scale transcriptome sequencing datasets of model plants and crops. We highlight strategies to select contextual genes for GRN inference, and statistical and ML-based methods for inferring GRNs based on transcriptome datasets from plants. Furthermore, we discuss the challenges and opportunities for the elucidation of GRNs based on large-scale datasets obtained from emerging transcriptomic applications, such as from population-scale, single-cell level, and life-course transcriptome analyses.",2018-11-01,5,1153,107,919
448,30322481,Novel Quantitative PET Techniques for Clinical Decision Support in Oncology,"Quantitative image analysis has deep roots in the usage of positron emission tomography (PET) in clinical and research settings to address a wide variety of diseases. It has been extensively employed to assess molecular and physiological biomarkers in vivo in healthy and disease states, in oncology, cardiology, neurology, and psychiatry. Quantitative PET allows relating the time-varying activity concentration in tissues/organs of interest and the basic functional parameters governing the biological processes being studied. Yet, quantitative PET is challenged by a number of degrading physical factors related to the physics of PET imaging, the limitations of the instrumentation used, and the physiological status of the patient. Moreover, there is no consensus on the most reliable and robust image-derived PET metric(s) that can be used with confidence in clinical oncology owing to the discrepancies between the conclusions reported in the literature. There is also increasing interest in the use of artificial intelligence based techniques, particularly machine learning and deep learning techniques in a variety of applications to extract quantitative features (radiomics) from PET including image segmentation and outcome prediction in clinical oncology. These novel techniques are revolutionizing clinical practice and are now offering unique capabilities to the clinical molecular imaging community and biomedical researchers at large. In this report, we summarize recent developments and future tendencies in quantitative PET imaging and present example applications in clinical decision support to illustrate its potential in the context of clinical oncology.",2018-11-01,5,1675,75,919
2562,30505265,"A New Frontier: The Convergence of Nanotechnology, Brain Machine Interfaces, and Artificial Intelligence","A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable ""smart"" nanoengineered brain machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to changing functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as non-invasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved in order to achieve this.",2018-11-01,3,1051,104,919
2569,30510440,"Precision pharmacotherapy: psychiatry's future direction in preventing, diagnosing, and treating mental disorders","Mental disorders account for around one-third of disability worldwide and cause enormous personal and societal burden. Current pharmacotherapies and nonpharmacotherapies do help many patients, but there are still high rates of partial or no response, delayed effect, and unfavorable adverse effects. The current diagnostic taxonomy of mental disorders by the Diagnostic and Statistical Manual of Mental Disorders and the International Classification of Diseases relies on presenting signs and symptoms, but does not reflect evidence from neurobiological and behavioral systems. However, in the last decades, the understanding of biological mechanisms underlying mental disorders has grown and can be used for the development of precision medicine, that is, to deliver a patient-tailored individual treatment. Precision medicine may incorporate genetic variants contributing to the mental disorder and the response to pharmacotherapies, but also consider gene  environment interactions, blood-based markers, neuropsychological tests, data from electronic health records, early life adversity, stressful life events, and very proximal factors such as lifestyle, nutrition, and sport. Methods such as artificial intelligence and the underlying machine learning and deep learning approaches provide the framework to stratify patients, initiate specific tailored treatments and thus increase response rates, reduce adverse effects and medical errors. In conclusion, precision medicine uses measurable health parameters to identify individuals at risk of a mental disorder, to improve the diagnostic process and to deliver a patient-tailored treatment.",2018-11-01,11,1647,113,919
2572,30499008,Mental state and emotion detection from musically stimulated EEG,"This literature survey attempts to clarify different approaches considered to study the impact of the musical stimulus on the human brain using EEG Modality. Glancing at the field through various aspects of such studies specifically an experimental protocol, the EEG machine, number of channels investigated, feature extracted, categories of emotions, the brain area, the brainwaves, statistical tests, machine learning algorithms used for classification and validation of the developed model. This article comments on how these different approaches have particular weaknesses and strengths. Ultimately, this review concludes a suitable method to study the impact of the musical stimulus on brain and implications of such kind of studies.",2018-11-01,0,738,64,919
2577,30483163,Is It Possible to Predict the Future in First-Episode Psychosis?,"The outcome of first-episode psychosis (FEP) is highly variable, ranging from early sustained recovery to antipsychotic treatment resistance from the onset of illness. For clinicians, a possibility to predict patient outcomes would be highly valuable for the selection of antipsychotic treatment and in tailoring psychosocial treatments and psychoeducation. This selective review summarizes current knowledge of prognostic markers in FEP. We sought potential outcome predictors from clinical and sociodemographic factors, cognition, brain imaging, genetics, and blood-based biomarkers, and we considered different outcomes, like remission, recovery, physical comorbidities, and suicide risk. Based on the review, it is currently possible to predict the future for FEP patients to some extent. Some clinical features-like the longer duration of untreated psychosis (DUP), poor premorbid adjustment, the insidious mode of onset, the greater severity of negative symptoms, comorbid substance use disorders (SUDs), a history of suicide attempts and suicidal ideation and having non-affective psychosis-are associated with a worse outcome. Of the social and demographic factors, male gender, social disadvantage, neighborhood deprivation, dysfunctional family environment, and ethnicity may be relevant. Treatment non-adherence is a substantial risk factor for relapse, but a small minority of patients with acute onset of FEP and early remission may benefit from antipsychotic discontinuation. Cognitive functioning is associated with functional outcomes. Brain imaging currently has limited utility as an outcome predictor, but this may change with methodological advancements. Polygenic risk scores (PRSs) might be useful as one component of a predictive tool, and pharmacogenetic testing is already available and valuable for patients who have problems in treatment response or with side effects. Most blood-based biomarkers need further validation. None of the currently available predictive markers has adequate sensitivity or specificity used alone. However, personalized treatment of FEP will need predictive tools. We discuss some methodologies, such as machine learning (ML), and tools that could lead to the improved prediction and clinical utility of different prognostic markers in FEP. Combination of different markers in ML models with a user friendly interface, or novel findings from e.g., molecular genetics or neuroimaging, may result in computer-assisted clinical applications in the near future.",2018-11-01,12,2511,64,919
2583,30467491,Machine Learning in Acute Ischemic Stroke Neuroimaging,"Machine Learning (ML) through pattern recognition algorithms is currently becoming an essential aid for the diagnosis, treatment, and prediction of complications and patient outcomes in a number of neurological diseases. The evaluation and treatment of Acute Ischemic Stroke (AIS) have experienced a significant advancement over the past few years, increasingly requiring the use of neuroimaging for decision-making. In this review, we offer an insight into the recent developments and applications of ML in neuroimaging focusing on acute ischemic stroke.",2018-11-01,10,555,54,919
2585,30458727,Prognostic models for intracerebral hemorrhage: systematic review and meta-analysis,"Background:                    Prognostic tools for intracerebral hemorrhage (ICH) patients are potentially useful for ascertaining prognosis and recommended in guidelines to facilitate streamline assessment and communication between providers. In this systematic review with meta-analysis we identified and characterized all existing prognostic tools for this population, performed a methodological evaluation of the conducting and reporting of such studies and compared different methods of prognostic tool derivation in terms of discrimination for mortality and functional outcome prediction.              Methods:                    PubMed, ISI, Scopus and CENTRAL were searched up to 15th September 2016, with additional studies identified using reference check. Two reviewers independently extracted data regarding the population studied, process of tool derivation, included predictors and discrimination (c statistic) using a predesignated spreadsheet based in the CHARMS checklist. Disagreements were solved by consensus. C statistics were pooled using robust variance estimation and meta-regression was applied for group comparisons using random effect models.              Results:                    Fifty nine studies were retrieved, including 48,133 patients and reporting on the derivation of 72 prognostic tools. Data on discrimination (c statistic) was available for 53 tools, 38 focusing on mortality and 15 focusing on functional outcome. Discrimination was high for both outcomes, with a pooled c statistic of 0.88 for mortality and 0.87 for functional outcome. Forty three tools were regression based and nine tools were derived using machine learning algorithms, with no differences found between the two methods in terms of discrimination (p = 0.490). Several methodological issues however were identified, relating to handling of missing data, low number of events per variable, insufficient length of follow-up, absence of blinding, infrequent use of internal validation, and underreporting of important model performance measures.              Conclusions:                    Prognostic tools for ICH discriminated well for mortality and functional outcome in derivation studies but methodological issues require confirmation of these findings in validation studies. Logistic regression based risk scores are particularly promising given their good performance and ease of application.",2018-11-01,7,2411,83,919
2587,30455216,The use and misuse of herbarium specimens in evaluating plant extinction risks,"Herbarium specimens provide verifiable and citable evidence of the occurrence of particular plants at particular points in space and time, and are vital resources for assessing extinction risk in the tropics, where plant diversity and threats to plants are greatest. We reviewed approaches to assessing extinction risk in response to the Convention on Biological Diversity's Global Strategy for Plant Conservation Target 2: an assessment of the conservation status of all known plant species by 2020. We tested five alternative approaches, using herbarium-derived data for trees, shrubs and herbs in five different plant groups from temperate and tropical regions. All species were previously fully assessed for the IUCN Red List. We found significant variation in the accuracy with which different approaches classified species as threatened or not threatened. Accuracy was highest for the machine learning model (90%) but the least data-intensive approach also performed well (82%). Despite concerns about spatial, temporal and taxonomic biases and uncertainties in herbarium data, when specimens represent the best available evidence for particular species, their use as a basis for extinction risk assessment is appropriate, necessary and urgent. Resourcing herbaria to maintain, increase and disseminate their specimen data is essential to guide and focus conservation action.This article is part of the theme issue 'Biological collections for understanding biodiversity in the Anthropocene'.",2018-11-01,7,1497,78,919
451,30317059,Automated seizure prediction,"In the past two decades, significant advances have been made on automated electroencephalogram (EEG)-based diagnosis of epilepsy and seizure detection. A number of innovative algorithms have been introduced that can aid in epilepsy diagnosis with a high degree of accuracy. In recent years, the frontiers of computational epilepsy research have moved to seizure prediction, a more challenging problem. While antiepileptic medication can result in complete seizure freedom in many patients with epilepsy, up to one-third of patients living with epilepsy will have medically intractable epilepsy, where medications reduce seizure frequency but do not completely control seizures. If a seizure can be predicted prior to its clinical manifestation, then there is potential for abortive treatment to be given, either self-administered or via an implanted device administering medication or electrical stimulation. This will have a far-reaching impact on the treatment of epilepsy and patient's quality of life. This paper presents a state-of-the-art review of recent efforts and journal articles on seizure prediction. The technologies developed for epilepsy diagnosis and seizure detection are being adapted and extended for seizure prediction. The paper ends with some novel ideas for seizure prediction using the increasingly ubiquitous machine learning technology, particularly deep neural network machine learning.",2018-11-01,3,1414,28,919
2588,30453459,Machine learning applications for the differentiation of primary central nervous system lymphoma from glioblastoma on imaging: a systematic review and meta-analysis,"OBJECTIVEGlioblastoma (GBM) and primary central nervous system lymphoma (PCNSL) are common intracranial pathologies encountered by neurosurgeons. They often may have similar radiological findings, making diagnosis difficult without surgical biopsy; however, management is quite different between these two entities. Recently, predictive analytics, including machine learning (ML), have garnered attention for their potential to aid in the diagnostic assessment of a variety of pathologies. Several ML algorithms have recently been designed to differentiate GBM from PCNSL radiologically with a high sensitivity and specificity. The objective of this systematic review and meta-analysis was to evaluate the implementation of ML algorithms in differentiating GBM and PCNSL.METHODSThe authors performed a systematic review of the literature using PubMed in accordance with PRISMA guidelines to select and evaluate studies that included themes of ML and brain tumors. These studies were further narrowed down to focus on works published between January 2008 and May 2018 addressing the use of ML in training models to distinguish between GBM and PCNSL on radiological imaging. Outcomes assessed were test characteristics such as accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC).RESULTSEight studies were identified addressing use of ML in training classifiers to distinguish between GBM and PCNSL on radiological imaging. ML performed well with the lowest reported AUC being 0.878. In studies in which ML was directly compared with radiologists, ML performed better than or as well as the radiologists. However, when ML was applied to an external data set, it performed more poorly.CONCLUSIONSFew studies have applied ML to solve the problem of differentiating GBM from PCNSL using imaging alone. Of the currently published studies, ML algorithms have demonstrated promising results and certainly have the potential to aid radiologists with difficult cases, which could expedite the neurosurgical decision-making process. It is likely that ML algorithms will help to optimize neurosurgical patient outcomes as well as the cost-effectiveness of neurosurgical care if the problem of overfitting can be overcome.",2018-11-01,2,2257,164,919
115,30170101,A Deep Look Into the Future of Quantitative Imaging in Oncology: A Statement of Working Principles and Proposal for Change,"The adoption of enterprise digital imaging, along with the development of quantitative imaging methods and the re-emergence of statistical learning, has opened the opportunity for more personalized cancer treatments through transformative data science research. In the last 5 years, accumulating evidence has indicated that noninvasive advanced imaging analytics (i.e., radiomics) can reveal key components of tumor phenotype for multiple lesions at multiple time points over the course of treatment. Many groups using homegrown software have extracted engineered and deep quantitative features on 3-dimensional medical images for better spatial and longitudinal understanding of tumor biology and for the prediction of diverse outcomes. These developments could augment patient stratification and prognostication, buttressing emerging targeted therapeutic approaches. Unfortunately, the rapid growth in popularity of this immature scientific discipline has resulted in many early publications that miss key information or use underpowered patient data sets, without production of generalizable results. Quantitative imaging research is complex, and key principles should be followed to realize its full potential. The fields of quantitative imaging and radiomics in particular require a renewed focus on optimal study design and reporting practices, standardization, interpretability, data sharing, and clinical trials. Standardization of image acquisition, feature calculation, and statistical analysis (i.e., machine learning) are required for the field to move forward. A new data-sharing paradigm enacted among open and diverse participants (medical institutions, vendors and associations) should be embraced for faster development and comprehensive clinical validation of imaging biomarkers. In this review and critique of the field, we propose working principles and fundamental changes to the current scientific approach, with the goal of high-impact research and development of actionable prediction models that will yield more meaningful applications of precision cancer medicine.",2018-11-01,19,2090,122,919
129,30136381,Machine learning in major depression: From classification to treatment outcome prediction,"Aims:                    Major depression disorder (MDD) is the single greatest cause of disability and morbidity, and affects about 10% of the population worldwide. Currently, there are no clinically useful diagnostic biomarkers that are able to confirm a diagnosis of MDD from bipolar disorder (BD) in the early depressive episode. Therefore, exploring translational biomarkers of mood disorders based on machine learning is in pressing need, though it is challenging, but with great potential to improve our understanding of these disorders.              Discussions:                    In this study, we review popular machine-learning methods used for brain imaging classification and predictions, and provide an overview of studies, specifically for MDD, that have used magnetic resonance imaging data to either (a) classify MDDs from controls or other mood disorders or (b) investigate treatment outcome predictors for individual patients. Finally, challenges, future directions, and potential limitations related to MDD biomarker identification are also discussed, with a goal of offering a comprehensive overview that may help readers to better understand the applications of neuroimaging data mining in depression.              Conclusions:                    We hope such efforts may highlight the need for an urgently needed paradigm shift in treatment, to guide personalized optimal clinical care.",2018-11-01,29,1410,89,919
132,30124495,Novel imaging techniques in pulmonary hypertension,"Purpose of review:                    Pulmonary hypertension is a life-shortening condition, which may be idiopathic but is more frequently seen in association with other conditions. Current guidelines recommend cardiac catheterization to confirm the diagnosis of pulmonary hypertension. Evidence suggests an increasing role for noninvasive imaging modalities in the initial diagnostic and prognostic assessment and evaluation of treatment response.              Recent findings:                    In this review we examine the evidence for current noninvasive imaging methodologies: echocardiography computed tomography and MRI in the diagnostic and prognostic assessment of suspected pulmonary hypertension and explore the potential utility of modeling and machine-learning approaches.              Summary:                    Noninvasive imaging allows a comprehensive assessment of patients with suspected pulmonary hypertension. It plays a key part in the initial diagnostic and prognostic assessment and machine-learning approaches show promise in the diagnosis of pulmonary hypertension.",2018-11-01,3,1095,50,919
148,30097305,Priors in Animal and Artificial Intelligence: Where Does Learning Begin?,"A major goal for the next generation of artificial intelligence (AI) is to build machines that are able to reason and cope with novel tasks, environments, and situations in a manner that approaches the abilities of animals. Evidence from precocial species suggests that driving learning through suitable priors can help to successfully face this challenge.",2018-11-01,9,356,72,919
163,30076935,Artificial intelligence in retina,"Major advances in diagnostic technologies are offering unprecedented insight into the condition of the retina and beyond ocular disease. Digital images providing millions of morphological datasets can fast and non-invasively be analyzed in a comprehensive manner using artificial intelligence (AI). Methods based on machine learning (ML) and particularly deep learning (DL) are able to identify, localize and quantify pathological features in almost every macular and retinal disease. Convolutional neural networks thereby mimic the path of the human brain for object recognition through learning of pathological features from training sets, supervised ML, or even extrapolation from patterns recognized independently, unsupervised ML. The methods of AI-based retinal analyses are diverse and differ widely in their applicability, interpretability and reliability in different datasets and diseases. Fully automated AI-based systems have recently been approved for screening of diabetic retinopathy (DR). The overall potential of ML/DL includes screening, diagnostic grading as well as guidance of therapy with automated detection of disease activity, recurrences, quantification of therapeutic effects and identification of relevant targets for novel therapeutic approaches. Prediction and prognostic conclusions further expand the potential benefit of AI in retina which will enable personalized health care as well as large scale management and will empower the ophthalmologist to provide high quality diagnosis/therapy and successfully deal with the complexity of 21st century ophthalmology.",2018-11-01,67,1595,33,919
416,33500999,Survey of Image Processing Techniques for Brain Pathology Diagnosis: Challenges and Opportunities,"In recent years, a number of new products introduced to the global market combine intelligent robotics, artificial intelligence and smart interfaces to provide powerful tools to support professional decision making. However, while brain disease diagnosis from the brain scan images is supported by imaging robotics, the data analysis to form a medical diagnosis is performed solely by highly trained medical professionals. Recent advances in medical imaging techniques, artificial intelligence, machine learning and computer vision present new opportunities to build intelligent decision support tools to aid the diagnostic process, increase the disease detection accuracy, reduce error, automate the monitoring of patient's recovery, and discover new knowledge about the disease cause and its treatment. This article introduces the topic of medical diagnosis of brain diseases from the MRI based images. We describe existing, multi-modal imaging techniques of the brain's soft tissue and describe in detail how are the resulting images are analyzed by a radiologist to form a diagnosis. Several comparisons between the best results of classifying natural scenes and medical image analysis illustrate the challenges of applying existing image processing techniques to the medical image analysis domain. The survey of medical image processing methods also identified several knowledge gaps, the need for automation of image processing analysis, and the identification of the brain structures in the medical images that differentiate healthy tissue from a pathology. This survey is grounded in the cases of brain tumor analysis and the traumatic brain injury diagnoses, as these two case studies illustrate the vastly different approaches needed to define, extract, and synthesize meaningful information from multiple MRI image sets for a diagnosis. Finally, the article summarizes artificial intelligence frameworks that are built as multi-stage, hybrid, hierarchical information processing work-flows and the benefits of applying these models for medical diagnosis to build intelligent physician's aids with knowledge transparency, expert knowledge embedding, and increased analytical quality.",2018-11-01,0,2193,97,919
881,31304340,Reflecting health: smart mirrors for personalized medicine,"Inexpensive embedded computing and the related Internet of Things technologies enable the recent development of smart products that can respond to human needs and improve everyday tasks in an attempt to make traditional environments more ""intelligent"". Several projects have augmented mirrors for a range of smarter applications in automobiles and homes. The opportunity to apply smart mirror technology to healthcare to predict and to monitor aspects of health and disease is a natural but mostly underdeveloped idea. We envision that smart mirrors comprising a combination of intelligent hardware and software could identify subtle, yet clinically relevant changes in physique and appearance. Similarly, a smart mirror could record and evaluate body position and motion to identify posture and movement issues, as well as offer feedback for corrective actions. Successful development and implementation of smart mirrors for healthcare applications will require overcoming new challenges in engineering, machine learning, computer vision, and biomedical research. This paper examines the potential uses of smart mirrors in healthcare and explores how this technology might benefit users in various medical environments. We also provide a brief description of the state-of-the-art, including a functional prototype concept developed by our group, and highlight the directions to make this device more mainstream in health-related applications.",2018-11-01,5,1443,58,919
101,30204184,Towards operando computational modeling in heterogeneous catalysis,"An increased synergy between experimental and theoretical investigations in heterogeneous catalysis has become apparent during the last decade. Experimental work has extended from ultra-high vacuum and low temperature towards operando conditions. These developments have motivated the computational community to move from standard descriptive computational models, based on inspection of the potential energy surface at 0 K and low reactant concentrations (0 K/UHV model), to more realistic conditions. The transition from 0 K/UHV to operando models has been backed by significant developments in computer hardware and software over the past few decades. New methodological developments, designed to overcome part of the gap between 0 K/UHV and operando conditions, include (i) global optimization techniques, (ii) ab initio constrained thermodynamics, (iii) biased molecular dynamics, (iv) microkinetic models of reaction networks and (v) machine learning approaches. The importance of the transition is highlighted by discussing how the molecular level picture of catalytic sites and the associated reaction mechanisms changes when the chemical environment, pressure and temperature effects are correctly accounted for in molecular simulations. It is the purpose of this review to discuss each method on an equal footing, and to draw connections between methods, particularly where they may be applied in combination.",2018-11-01,13,1419,66,919
2310,29606338,Deep Learning in Radiology,"As radiology is inherently a data-driven specialty, it is especially conducive to utilizing data processing techniques. One such technique, deep learning (DL), has become a remarkably powerful tool for image processing in recent years. In this work, the Association of University Radiologists Radiology Research Alliance Task Force on Deep Learning provides an overview of DL for the radiologist. This article aims to present an overview of DL in a manner that is understandable to radiologists; to examine past, present, and future applications; as well as to evaluate how radiologists may benefit from this remarkable new tool. We describe several areas within radiology in which DL techniques are having the most significant impact: lesion or disease detection, classification, quantification, and segmentation. The legal and ethical hurdles to implementation are also discussed. By taking advantage of this powerful tool, radiologists can become increasingly more accurate in their interpretations with fewer errors and spend more time to focus on patient care.",2018-11-01,43,1065,26,919
2552,30534555,Intelligence Algorithms for Protein Classification by Mass Spectrometry,"Mass spectrometry (MS) is an important technique in protein research. Effective classification methods by MS data could contribute to early and less-invasive diagnosis and also facilitate developments in the bioinformatics field. As MS data is featured by high dimension, appropriate methods which can effectively deal with the large amount of MS data have been widely studied. In this paper, the applications of methods based on intelligence algorithms have been investigated. Firstly, classification and biomarker analysis methods using typical machine learning approaches have been discussed. Then those are followed by the Ensemble strategy algorithms. Clearly, simple and basic machine learning algorithms hardly addressed the various needs of protein MS classification. Preprocessing algorithms have been also studied, as these methods are useful for feature selection or feature extraction to improve classification performance. Protein MS data growing with data volume becomes complicated and large; improvements in classification methods in terms of classifier selection and combinations of different algorithms and preprocessing algorithms are more emphasized in further work.",2018-11-01,1,1186,71,919
2375,29395627,Challenges and Promises of PET Radiomics,"Purpose:                    Radiomics describes the extraction of multiple, otherwise invisible, features from medical images that, with bioinformatic approaches, can be used to provide additional information that can predict underlying tumor biology and behavior.              Methods and materials:                    Radiomic signatures can be used alone or with other patient-specific data to improve tumor phenotyping, treatment response prediction, and prognosis, noninvasively. The data describing 18F-fluorodeoxyglucose positron emission tomography radiomics, often using texture or heterogeneity parameters, are increasing rapidly.              Results:                    In relation to radiation therapy practice, early data have reported the use of radiomic approaches to better define tumor volumes and predict radiation toxicity and treatment response.              Conclusions:                    Although at an early stage of development, with many technical challenges remaining and a need for standardization, promise nevertheless exists that PET radiomics will contribute to personalized medicine, especially with the availability of increased computing power and the development of machine-learning approaches for imaging.",2018-11-01,25,1242,40,919
2512,30613284,"The theranostic promise for Neuroendocrine Tumors in the late 2010s - Where do we stand, where do we go?","More than 25 years after the first peptide receptor radionuclide therapy (PRRT), the concept of somatostatin receptor (SSTR)-directed imaging and therapy for neuroendocrine tumors (NET) is seeing rapidly increasing use. To maximize the full potential of its theranostic promise, efforts in recent years have expanded recommendations in current guidelines and included the evaluation of novel theranostic radiotracers for imaging and treatment of NET. Moreover, the introduction of standardized reporting framework systems may harmonize PET reading, address pitfalls in interpreting SSTR-PET/CT scans and guide the treating physician in selecting PRRT candidates. Notably, the concept of PRRT has also been applied beyond oncology, e.g. for treatment of inflammatory conditions like sarcoidosis. Future perspectives may include the efficacy evaluation of PRRT compared to other common treatment options for NET, novel strategies for closer monitoring of potential side effects, the introduction of novel radiotracers with beneficial pharmacodynamic and kinetic properties or the use of supervised machine learning approaches for outcome prediction. This article reviews how the SSTR-directed theranostic concept is currently applied and also reflects on recent developments that hold promise for the future of theranostics in this context.",2018-11-01,14,1338,104,919
2507,30619452,Recent Advances on the Machine Learning Methods in Identifying DNA Replication Origins in Eukaryotic Genomics,"The initiate site of DNA replication is called origins of replication (ORI) which is regulated by a set of regulatory proteins and plays important roles in the basic biochemical process during cell growth and division in all living organisms. Therefore, the study of ORIs is essential for understanding the cell-division cycle and gene expression regulation so that scholars can develop a new strategy against genetic diseases by using the knowledge of DNA replication. Thus, the accurate identification of ORIs will provide key clues for DNA replication research and clinical medicine. Although, the conventional experiments could provide accurate results, they are time-consuming and cost ineffective. On the contrary, bioinformatics-based methods can overcome these shortcomings. Especially, with the emergence of DNA sequences in the post-genomic era, it is highly expected to develop high throughput tools to identify ORIs based on sequence information. In this review, we will summarize the current progress in computational prediction of eukaryotic ORIs including the collection of benchmark dataset, the application of machine learning-based techniques, the results obtained by these methods, and the construction of web servers. Finally, we gave the future perspectives on ORIs prediction. The review provided readers with a whole background of ORIs prediction based on machine learning methods, which will be helpful for researchers to study DNA replication in-depth and drug therapy of genetic defect.",2018-12-01,2,1512,109,889
2548,30541791,Big data and black-box medical algorithms,"New machine-learning techniques entering medicine present challenges in validation, regulation, and integration into practice.",2018-12-01,10,126,41,889
2547,30544420,Machine learning methods for automatic pain assessment using facial expression information: Protocol for a systematic review and meta-analysis,"Introduction:                    Prediction of pain using machine learning algorithms is an emerging field in both computer science and clinical medicine. Several machine algorithms were developed and validated in recent years. However, the majority of studies in this topic was published on bioinformatics or computer science journals instead of medical journals. This tendency and preference led to a gap of knowledge and acknowledgment between computer scientists who invent the algorithm and medical researchers who may use the algorithms in practice. As a consequence, some of these prediction papers did not discuss the clinical utility aspects and were causally reported without following related professional guidelines (e.g., TRIPOD statement). The aim of this protocol is to systematically summarize the current evidences about performance and utility of different machine learning methods used for automatic pain assessments based on human facial expression. In addition, this study is aimed to demonstrate and fill the knowledge gap to promote interdisciplinary collaboration.              Methods and analysis:                    We will search all English language literature in the following electronic databases: PubMed, Web of Science and IEEE Xplore. A systematic review and meta-analysis summarizing the accuracy, interpretability, generalizability, and computational efficiency of machine learning methods will be conducted. Subgroup analyses by machine learning method types will be conducted.              Timeline:                    The formal meta-analysis will start on Jan 15, 2019 and expected to finish by April 15, 2019.              Ethics and dissemination:                    Ethical approval will be exempted or will not be required because the data collected and analyzed in this meta-analysis will not be on an individual level. The results will be disseminated in the form of an official publication in a peer-reviewed journal and/or presentation at relevant conferences.              Registration:                    PROSPERO CRD42018103059.",2018-12-01,1,2079,142,889
438,30347013,The Science of Prognosis in Psychiatry: A Review,"Importance:                    Prognosis is a venerable component of medical knowledge introduced by Hippocrates (460-377 BC). This educational review presents a contemporary evidence-based approach for how to incorporate clinical risk prediction models in modern psychiatry. The article is organized around key methodological themes most relevant for the science of prognosis in psychiatry. Within each theme, the article highlights key challenges and makes pragmatic recommendations to improve scientific understanding of prognosis in psychiatry.              Observations:                    The initial step to building clinical risk prediction models that can affect psychiatric care involves designing the model: preparation of the protocol and definition of the outcomes and of the statistical methods (theme 1). Further initial steps involve carefully selecting the predictors, preparing the data, and developing the model in these data. A subsequent step is the validation of the model to accurately test its generalizability (theme 2). The next consideration is that the accuracy of the clinical prediction model is affected by the incidence of the psychiatric condition under investigation (theme 3). Eventually, clinical prediction models need to be implemented in real-world clinical routine, and this is usually the most challenging step (theme 4). Advanced methods such as machine learning approaches can overcome some problems that undermine the previous steps (theme 5). The relevance of each of these themes to current clinical risk prediction modeling in psychiatry is discussed and recommendations are given.              Conclusions and relevance:                    Together, these perspectives intend to contribute to an integrative, evidence-based science of prognosis in psychiatry. By focusing on the outcome of the individuals, rather than on the disease, clinical risk prediction modeling can become the cornerstone for a scientific and personalized psychiatry.",2018-12-01,32,1989,48,889
2919,29907338,Artificial intelligence in radiation oncology: A specialty-wide disruptive transformation?,"Artificial intelligence (AI) is emerging as a technology with the power to transform established industries, and with applications from automated manufacturing to advertising and facial recognition to fully autonomous transportation. Advances in each of these domains have led some to call AI the ""fourth"" industrial revolution [1]. In healthcare, AI is emerging as both a productive and disruptive force across many disciplines. This is perhaps most evident in Diagnostic Radiology and Pathology, specialties largely built around the processing and complex interpretation of medical images, where the role of AI is increasingly seen as both a boon and a threat. In Radiation Oncology as well, AI seems poised to reshape the specialty in significant ways, though the impact of AI has been relatively limited at present, and may rightly seem more distant to many, given the predominantly interpersonal and complex interventional nature of the specialty. In this overview, we will explore the current state and anticipated future impact of AI on Radiation Oncology, in detail, focusing on key topics from multiple stakeholder perspectives, as well as the role our specialty may play in helping to shape the future of AI within the larger spectrum of medicine.",2018-12-01,28,1257,90,889
434,30357911,An update on adaptive deep brain stimulation in Parkinson's disease,"Advancing conventional open-loop DBS as a therapy for PD is crucial for overcoming important issues such as the delicate balance between beneficial and adverse effects and limited battery longevity that are currently associated with treatment. Closed-loop or adaptive DBS aims to overcome these limitations by real-time adjustment of stimulation parameters based on continuous feedback input signals that are representative of the patient's clinical state. The focus of this update is to discuss the most recent developments regarding potential input signals and possible stimulation parameter modulation for adaptive DBS in PD. Potential input signals for adaptive DBS include basal ganglia local field potentials, cortical recordings (electrocorticography), wearable sensors, and eHealth and mHealth devices. Furthermore, adaptive DBS can be applied with different approaches of stimulation parameter modulation, the feasibility of which can be adapted depending on specific PD phenotypes. Implementation of technological developments like machine learning show potential in the design of such approaches; however, energy consumption deserves further attention. Furthermore, we discuss future considerations regarding the clinical implementation of adaptive DBS in PD.  2018 The Authors. Movement Disorders published by Wiley Periodicals, Inc. on behalf of International Parkinson and Movement Disorder Society.",2018-12-01,8,1414,67,889
427,30369809,History and application of artificial neural networks in dentistry,"Artificial intelligence (AI) is a commonly used term in daily life, and there are now two subconcepts that divide the entire range of meanings currently encompassed by the term. The coexistence of the concepts of strong and weak AI can be seen as a result of the recognition of the limits of mathematical and engineering concepts that have dominated the definition. This presentation reviewed the concept, history, and the current application of AI in daily life. Applications of AI are becoming a reality that is commonplace in all areas of modern human life. Efforts to develop robots controlled by AI have been continuously carried out to maximize human convenience. AI has also been applied in the medical decision-making process, and these AI systems can help nonspecialists to obtain expert-level information. Artificial neural networks are highly interconnected networks of computer processors inspired by biological nervous systems. These systems may help connect dental professionals all over the world. Currently, the use of AI is rapidly advancing beyond text-based, image-based dental practice. This presentation reviewed the history of artificial neural networks in the medical and dental fields, as well as current application in dentistry. As the use of AI in the entire medical field increases, the role of AI in dentistry will be greatly expanded. Currently, the use of AI is rapidly advancing beyond text-based, image-based dental practice. In addition to diagnosis of visually confirmed dental caries and impacted teeth, studies applying machine learning based on artificial neural networks to dental treatment through analysis of dental magnetic resonance imaging, computed tomography, and cephalometric radiography are actively underway, and some visible results are emerging at a rapid pace for commercialization.",2018-12-01,6,1835,66,889
114,30172554,Outcome Tracking in Facial Palsy,"Outcome tracking in facial palsy is multimodal, consisting of patient-reported outcome measures, clinician-graded scoring systems, objective assessment tools, and novel tools for layperson and spontaneity assessment. Patient-reported outcome measures are critical to understanding burden of disease in facial palsy and effects of interventions from the patient perspective. Clinician-graded scoring systems are inherently subjective and no 1 single system satisfies all needs. Objective assessment tools quantify facial movements but can be laborious. Recent advances in facial recognition technology have enabled automated facial measurements. Novel assessment tools analyze attributes such as spontaneous smile, emotional expressivity, disfigurement, and attractiveness as determined by laypersons.",2018-12-01,2,800,32,889
2522,30586882,Hand Gesture Recognition in Automotive HumanMachine Interaction Using Depth Cameras,"In this review, we describe current Machine Learning approaches to hand gesture recognition with depth data from time-of-flight sensors. In particular, we summarise the achievements on a line of research at the Computational Neuroscience laboratory at the Ruhr West University of Applied Sciences. Relating our results to the work of others in this field, we confirm that Convolutional Neural Networks and Long Short-Term Memory yield most reliable results. We investigated several sensor data fusion techniques in a deep learning framework and performed user studies to evaluate our system in practice. During our course of research, we gathered and published our data in a novel benchmark dataset (REHAP), containing over a million unique three-dimensional hand posture samples.",2018-12-01,5,780,84,889
116,30169341,What we can learn from Big Data about factors influencing perioperative outcome,"Purpose of review:                    This narrative review will discuss what value Big Data has to offer anesthesiology and aims to highlight recently published articles of large databases exploring factors influencing perioperative outcome. Additionally, the future perspectives of Big Data and its major pitfalls will be discussed.              Recent findings:                    The potential of Big Data has given an incentive to create nationwide and anesthesia-initiated registries like the MPOG and NACOR. These large databases have contributed in elucidating some of the rare perioperative complications, such as declined cognition after exposure to general anesthesia and epidural hematomas in parturients. Additionally, they are useful in finding patterns such as similar outcome in subtypes of beta-blockers and lower incidence of pneumonia in preoperative influenza vaccinations in the elderly.              Summary:                    Big Data is becoming increasingly popular with the collaborative collection of registries offering anesthesia a way to explore rare perioperative complications and outcome to encourage further hypotheses testing. Although Big Data has its flaws in security, lack of expertise and methodological concerns, the future potential of analytics combined with genomics, machine learning and real-time decision support looks promising.",2018-12-01,2,1377,79,889
417,30379703,Neurotrauma as a big-data problem,"Purpose of review:                    The field of neurotrauma research faces a reproducibility crisis. In response, research leaders in traumatic brain injury (TBI) and spinal cord injury (SCI) are leveraging data curation and analytics methods to encourage transparency, and improve the rigor and reproducibility. Here we review the current challenges and opportunities that come from efforts to transform neurotrauma's big data to knowledge.              Recent findings:                    Three parallel movements are driving data-driven-discovery in neurotrauma. First, large multicenter consortia are collecting large quantities of neurotrauma data, refining common data elements (CDEs) that can be used across studies. Investigators are now testing the validity of CDEs in diverse research settings. Second, data sharing initiatives are working to make neurotrauma data findable, accessible, interoperable, and reusable (FAIR). These efforts are reflected by recent open data repository projects for preclinical and clinical neurotrauma. Third, machine learning analytics are allowing researchers to uncover novel data-driven-hypotheses and test new therapeutics in multidimensional outcome space.              Summary:                    We are on the threshold of a new era in data collection, curation, and analysis. The next phase of big data in neurotrauma research will require responsible data stewardship, a culture of data-sharing, and the illumination of 'dark data'.",2018-12-01,5,1485,33,889
124,30153635,Applications of machine learning algorithms to predict therapeutic outcomes in depression: A meta-analysis and systematic review,"Background:                    No previous study has comprehensively reviewed the application of machine learning algorithms in mood disorders populations. Herein, we qualitatively and quantitatively evaluate previous studies of machine learning-devised models that predict therapeutic outcomes in mood disorders populations.              Methods:                    We searched Ovid MEDLINE/PubMed from inception to February 8, 2018 for relevant studies that included adults with bipolar or unipolar depression; assessed therapeutic outcomes with a pharmacological, neuromodulatory, or manual-based psychotherapeutic intervention for depression; applied a machine learning algorithm; and reported predictors of therapeutic response. A random-effects meta-analysis of proportions and meta-regression analyses were conducted.              Results:                    We identified 639 records: 75 full-text publications were assessed for eligibility; 26 studies (n=17,499) and 20 studies (n=6325) were included in qualitative and quantitative review, respectively. Classification algorithms were able to predict therapeutic outcomes with an overall accuracy of 0.82 (95% confidence interval [CI] of [0.77, 0.87]). Pooled estimates of classification accuracy were significantly greater (p < 0.01) in models informed by multiple data types (e.g., composite of phenomenological patient features and neuroimaging or peripheral gene expression data; pooled proportion [95% CI] = 0.93[0.86, 0.97]) when compared to models with lower-dimension data types (pooledproportion=0.68[0.62,0.74]to0.85[0.81,0.88]).              Limitations:                    Most studies were retrospective; differences in machine learning algorithms and their implementation (e.g., cross-validation, hyperparameter tuning); cannot infer importance of individual variables fed into learning algorithm.              Conclusions:                    Machine learning algorithms provide a powerful conceptual and analytic framework capable of integrating multiple data types and sources. An integrative approach may more effectively model neurobiological components as functional modules of pathophysiology embedded within the complex, social dynamics that influence the phenomenology of mental disorders.",2018-12-01,24,2271,128,889
2525,30578332,Role of imaging in progressive-fibrosing interstitial lung diseases,"Imaging techniques are an essential component of the diagnostic process for interstitial lung diseases (ILDs). Chest radiography is frequently the initial indicator of an ILD, and comparison of radiographs taken at different time points can show the rate of disease progression. However, radiography provides only limited specificity and sensitivity and is primarily used to rule out other diseases, such as left heart failure. High-resolution computed tomography (HRCT) is a more sensitive method and is considered central in the diagnosis of ILDs. Abnormalities observed on HRCT can help identify specific ILDs. HRCT also can be used to evaluate the patient's prognosis, while disease progression can be assessed through serial imaging. Other imaging techniques such as positron emission tomography-computed tomography and magnetic resonance imaging have been investigated, but they are not commonly used to assess patients with ILDs. Disease severity may potentially be estimated using quantitative methods, as well as visual analysis of images. For example, comprehensive assessment of disease staging and progression in patients with ILDs requires visual analysis of pulmonary features that can be performed in parallel with quantitative analysis of the extent of fibrosis. New approaches to image analysis, including the application of machine learning, are being developed.",2018-12-01,6,1380,67,889
413,30382605,A Machine Learning Approach to Predicting Need for Hospitalization for Pediatric Asthma Exacerbation at the Time of Emergency Department Triage,"Objectives:                    Pediatric asthma is a leading cause of emergency department (ED) utilization and hospitalization. Earlier identification of need for hospital-level care could triage patients more efficiently to high- or low-resource ED tracks. Existing tools to predict disposition for pediatric asthma use only clinical data, perform best several hours into the ED stay, and are static or score-based. Machine learning offers a population-specific, dynamic option that allows real-time integration of available nonclinical data at triage. Our objective was to compare the performance of four common machine learning approaches, incorporating clinical data available at the time of triage with information about weather, neighborhood characteristics, and community viral load for early prediction of the need for hospital-level care in pediatric asthma.              Methods:                    Retrospective analysis of patients ages 2 to 18 years seen at two urban pediatric EDs with asthma exacerbation over 4 years. Asthma exacerbation was defined as receiving both albuterol and systemic corticosteroids. We included patient features, measures of illness severity available in triage, weather features, and Centers for Disease Control and Prevention influenza patterns. We tested four models: decision trees, LASSO logistic regression, random forests, and gradient boosting machines. For each model, 80% of the data set was used for training and 20% was used to validate the models. The area under the receiver operating characteristic (AUC) curve was calculated for each model.              Results:                    There were 29,392 patients included in the analyses: mean (SD) age of 7.0 (4.2) years, 42% female, 77% non-Hispanic black, and 76% public insurance. The AUCs for each model were: decision tree 0.72 (95% confidence interval [CI] = 0.66-0.77), logistic regression 0.83 (95% CI = 0.82-0.83), random forests 0.82 (95% CI = 0.81-0.83), and gradient boosting machines 0.84 (95% CI = 0.83-0.85). In the lowest decile of risk, only 3% of patients required hospitalization; in the highest decile this rate was 100%. After patient vital signs and acuity, age and weight, followed by socioeconomic status (SES) and weather-related features, were the most important for predicting hospitalization.              Conclusions:                    Three of the four machine learning models performed well with decision trees preforming the worst. The gradient boosting machines model demonstrated a slight advantage over other approaches at predicting need for hospital-level care at the time of triage in pediatric patients presenting with asthma exacerbation. The addition of weight, SES, and weather data improved the performance of this model.",2018-12-01,7,2772,143,889
2965,29766512,Structured radiology reporting on an institutional level-benefit or new administrative burden?,"Significant technical advances have been made in radiology since the first discovery of X-rays. Diagnostic techniques have become more and more complex, workflows have been digitized, and data production has increased exponentially. However, the radiology report as the main method for communicating examination results has largely remained unchanged. Growing evidence supports that more structured radiology reports offer various benefits over conventional narrative reports. Various efforts have been made to further develop and promote structured reporting. However, regardless of the potential benefits, structured reporting has still not seen widespread implementation into the clinical routine. With recent technical advances, especially new research topics such as big data and machine learning, structured reporting could prove essential for the future of radiology. New interoperable solutions are needed to facilitate the implementation of template-based structured reporting into the clinical routine.",2018-12-01,0,1012,94,889
2546,30544901,Assessing the Role of Artificial Intelligence (AI) in Clinical Oncology: Utility of Machine Learning in Radiotherapy Target Volume Delineation,"The fields of radiotherapy and clinical oncology have been rapidly changed by the advances of technology. Improvement in computer processing power and imaging quality heralded precision radiotherapy allowing radiotherapy to be delivered efficiently, safely and effectively for patient benefit. Artificial intelligence (AI) is an emerging field of computer science which uses computer models and algorithms to replicate human-like intelligence and perform specific tasks which offers a huge potential to healthcare. We reviewed and presented the history, evolution and advancement in the fields of radiotherapy, clinical oncology and machine learning. Radiotherapy target delineation is a complex task of outlining tumour and organ at risks volumes to allow accurate delivery of radiotherapy. We discussed the radiotherapy planning, treatment delivery and reviewed how technology can help with this challenging process. We explored the evidence and clinical application of machine learning to radiotherapy. We concluded on the challenges, possible future directions and potential collaborations to achieve better outcome for cancer patients.",2018-12-01,9,1140,142,889
130,30126707,Pharmacovigilance: An Overview,"Purpose:                    Pharmacovigilance (PV) is a relatively new discipline in the pharmaceutical industry. Having undergone rapid growth over the past 2 decades, PV now touches many other disciplines in the research and development enterprise. With its growth has come a heightened awareness and interest in the medical community about the roles that PV plays. This article provides insights into the background and inner workings of PV.              Methods:                    This narrative review covers the core PV activities and other major areas of the pharmaceutical enterprise in which PV makes significant contributions.              Findings:                    Drug safety monitoring activities were organized by the US Food and Drug Administration and academic medical centers in the early 1950s in response to growing concern over the occurrence of aplastic anemia and other blood dyscrasias associated with the use of chloramphenicol. This experience was codified in the 1962 Kefauver-Harris Amendments to the Federal Food, Drug and Cosmetic Act as adverse event evaluation and reporting requirements. The ensuing decades have seen the development of core PV functions for pharmaceutical companies: case management, signal management, and benefit-risk management. A broader scope of PV has developed to include the following major activities: support of patient safety during the conduct of clinical trials through assuring proper use of informed consent and institutional review boards (ethics committees); selection of the first safe dose for use in humans, based on pharmacologic data obtained in animal studies; development of the safety profile for proper use of a new molecular entity and appropriate communication of that information to the range of relevant stakeholders; attendance to surveillance activities through a set of signal management processes; monitoring the manufactured product itself through collaborative activities with manufacturing professionals; management of benefit-risk to assure appropriate use in medical care after marketing; and maintenance of inspection readiness as a corporate cultural process.              Implications:                    The extent and pace of change promise to accelerate with the integration of biomedical informatics, analytics, artificial intelligence, and machine learning. This progress has implications for the development of the next generation of PV professionals who will need to be trained in entirely new skill sets to lead continued improvements in the safe use of pharmaceuticals.",2018-12-01,3,2574,30,889
412,30383900,"Quantitative imaging of cancer in the postgenomic era: Radio(geno)mics, deep learning, and habitats","Although cancer often is referred to as ""a disease of the genes,"" it is indisputable that the (epi)genetic properties of individual cancer cells are highly variable, even within the same tumor. Hence, preexisting resistant clones will emerge and proliferate after therapeutic selection that targets sensitive clones. Herein, the authors propose that quantitative image analytics, known as ""radiomics,"" can be used to quantify and characterize this heterogeneity. Virtually every patient with cancer is imaged radiologically. Radiomics is predicated on the beliefs that these images reflect underlying pathophysiologies, and that they can be converted into mineable data for improved diagnosis, prognosis, prediction, and therapy monitoring. In the last decade, the radiomics of cancer has grown from a few laboratories to a worldwide enterprise. During this growth, radiomics has established a convention, wherein a large set of annotated image features (1-2000 features) are extracted from segmented regions of interest and used to build classifier models to separate individual patients into their appropriate class (eg, indolent vs aggressive disease). An extension of this conventional radiomics is the application of ""deep learning,"" wherein convolutional neural networks can be used to detect the most informative regions and features without human intervention. A further extension of radiomics involves automatically segmenting informative subregions (""habitats"") within tumors, which can be linked to underlying tumor pathophysiology. The goal of the radiomics enterprise is to provide informed decision support for the practice of precision oncology.",2018-12-01,21,1660,99,889
410,30394238,Introduction to Machine Learning in Digital Healthcare Epidemiology,"To exploit the full potential of big routine data in healthcare and to efficiently communicate and collaborate with information technology specialists and data analysts, healthcare epidemiologists should have some knowledge of large-scale analysis techniques, particularly about machine learning. This review focuses on the broad area of machine learning and its first applications in the emerging field of digital healthcare epidemiology.",2018-12-01,4,439,67,889
142,30102808,eDoctor: machine learning and the future of medicine,"Machine learning (ML) is a burgeoning field of medicine with huge resources being applied to fuse computer science and statistics to medical problems. Proponents of ML extol its ability to deal with large, complex and disparate data, often found within medicine and feel that ML is the future for biomedical research, personalized medicine, computer-aided diagnosis to significantly advance global health care. However, the concepts of ML are unfamiliar to many medical professionals and there is untapped potential in the use of ML as a research tool. In this article, we provide an overview of the theory behind ML, explore the common ML algorithms used in medicine including their pitfalls and discuss the potential future of ML in medicine.",2018-12-01,25,744,52,889
146,30099083,Speech analysis for health: Current state-of-the-art and the increasing impact of deep learning,"Due to the complex and intricate nature associated with their production, the acoustic-prosodic properties of a speech signal are modulated with a range of health related effects. There is an active and growing area of machine learning research in this speech and health domain, focusing on developing paradigms to objectively extract and measure such effects. Concurrently, deep learning is transforming intelligent signal analysis, such that machines are now reaching near human capabilities in a range of recognition and analysis tasks. Herein, we review current state-of-the-art approaches with speech-based health detection, placing a particular focus on the impact of deep learning within this domain. Based on this overview, it is evident while that deep learning based solutions be become more present in the literature, it has not had the same overall dominating effect seen in other related fields. In this regard, we suggest some possible research directions aimed at fully leveraging the advantages that deep learning can offer speech-based health detection.",2018-12-01,7,1070,95,889
408,30400053,Artificial intelligence and echocardiography,"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome.",2018-12-01,18,1553,44,889
2910,29935311,Molecular pathway activation - New type of biomarkers for tumor morphology and personalized selection of target drugs,"Anticancer target drugs (ATDs) specifically bind and inhibit molecular targets that play important roles in cancer development and progression, being deeply implicated in intracellular signaling pathways. To date, hundreds of different ATDs were approved for clinical use in the different countries. Compared to previous chemotherapy treatments, ATDs often demonstrate reduced side effects and increased efficiency, but also have higher costs. However, the efficiency of ATDs for the advanced stage tumors is still insufficient. Different ATDs have different mechanisms of action and are effective in different cohorts of patients. Personalized approaches are therefore needed to select the best ATD candidates for the individual patients. In this review, we focus on a new generation of biomarkers - molecular pathway activation - and on their applications for predicting individual tumor response to ATDs. The success in high throughput gene expression profiling and emergence of novel bioinformatic tools reinforced quick development of pathway related field of molecular biomedicine. The ability to quantitatively measure degree of a pathway activation using gene expression data has revolutionized this field and made the corresponding analysis quick, robust and inexpensive. This success was further enhanced by using machine learning algorithms for selection of the best biomarkers. We review here the current progress in translating these studies to clinical oncology and patient-oriented adjustment of cancer therapy.",2018-12-01,29,1526,117,889
397,30430428,Radiomics and liquid biopsy in oncology: the holons of systems medicine,"Radiomics is a process of extraction and analysis of quantitative features from diagnostic images. Liquid biopsy is a test done on a sample of blood to look for cancer cells or for pieces of tumourigenic DNA circulating in the blood. Radiomics and liquid biopsy have great potential in oncology, since both are minimally invasive, easy to perform, and can be repeated in patient follow-up visits, enabling the extraction of valuable information regarding tumour type, aggressiveness, progression, and response to treatment. Both methods are in their infancy, with major evidence of application in lung and gastrointestinal cancer, while still undergoing evaluation in other cancer types. In this paper, the main oncologic applications of radiomics and liquid biopsy are reviewed, and a synergistic approach incorporating both tests for cancer diagnosis and follow-up is discussed within the context of systems medicine. TEACHING POINTS:  Radiomics is a process of extraction and analysis of quantitative features from diagnostic images.  Most clinical applications of radiomics are in the field of oncologic imaging.  Radiomics applies to all imaging modalities.  A cluster of radiomic features is a ""radiomic signature"".  Machine learning may improve the efficacy of radiomics analysis.",2018-12-01,17,1292,71,889
399,30420264,Prognostic models in primary biliary cholangitis,"Risk prediction modelling is important to better understand the determinants of the course and outcome of PBC and to inform the risk across the disease continuum in PBC enabling risk-stratified follow-up care and personalised therapy. Current prognostic models in PBC are based on treatment response to ursodeoxycholic acid because of the well-established relationship between alkaline phosphatase on treatment and long-term outcome. In addition, serum alkaline phosphatase correlates with ductular reaction and biliary metaplasia, which are hallmark of biliary injury. Considering the waiting time for treatment failure in high-risk patients is not inconsequential, efforts are focused on bringing forward risk stratification at diagnosis by predicting treatment response at onset. There is a need for better prognostic variables that are central to the disease process. We should take an integrative approach that incorporates multiple layers of information including genetic and environmental influences, host characteristics, clinical data, and molecular alterations for risk assessments. Biomarker discovery has an accelerated pace taking advantage of the emergence of large-scale omics platforms (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and others) and whole-genome sequencing. In the digital era, applications of artificial intelligence, such as machine learning, can support the computing power required to analyse the vast amount of data produced by omics. The information is then used for the development of personalised risk prediction models that through clinical trials and hopefully industry partnerships can guide risk management strategies. We are facing an unprecedented opportunity for the integration of molecular diagnostics into the clinic, which promotes progress toward the personalised management of patients with PBC.",2018-12-01,3,1868,48,889
2508,30619024,Monitoring Motor Symptoms During Activities of Daily Living in Individuals With Parkinson's Disease,"This literature review addressed wearable sensor systems to monitor motor symptoms in individuals with Parkinson's disease (PD) during activities of daily living (ADLs). Specifically, progress in monitoring tremor, freezing of gait, dyskinesia, bradykinesia, and hypokinesia was reviewed. Twenty-seven studies were found that met the criteria of measuring symptoms in a home or home-like setting, with some studies examining multiple motor disorders. Accelerometers, gyroscopes, and electromyography sensors were included, with some studies using more than one type of sensor. Five studies measured tremor, five studies examined bradykinesia or hypokinesia, thirteen studies included devices to measure dyskinesia or motor fluctuations, and ten studies measured akinesia or freezing of gait. Current sensor technology can detect the presence and severity of each of these symptoms; however, most systems require sensors on multiple body parts, which is challenging for remote or ecologically valid observation. Different symptoms are detected by different sensor placement, suggesting that the goal of detecting all symptoms with a reduced set of sensors may not be achievable. For the goal of monitoring motor symptoms during ADLs in a home setting, the measurement system should be simple to use, unobtrusive to the wearer and easy for an individual with PD to put on and take off. Machine learning algorithms such as neural networks appear to be the most promising way to detect symptoms using a small number of sensors. More work should be done validating the systems during unscripted and unconstrained ADLs rather than in scripted motions.",2018-12-01,15,1645,99,889
2499,30631320,"Leveraging Multilayered ""Omics"" Data for Atopic Dermatitis: A Road Map to Precision Medicine","Atopic dermatitis (AD) is a complex multifactorial inflammatory skin disease that affects ~280 million people worldwide. About 85% of AD cases begin in childhood, a significant portion of which can persist into adulthood. Moreover, a typical progression of children with AD to food allergy, asthma or allergic rhinitis has been reported (""allergic march"" or ""atopic march""). AD comprises highly heterogeneous sub-phenotypes/endotypes resulting from complex interplay between intrinsic and extrinsic factors, such as environmental stimuli, and genetic factors regulating cutaneous functions (impaired barrier function, epidermal lipid, and protease abnormalities), immune functions and the microbiome. Though the roles of high-throughput ""omics"" integrations in defining endotypes are recognized, current analyses are primarily based on individual omics data and using binary clinical outcomes. Although individual omics analysis, such as genome-wide association studies (GWAS), can effectively map variants correlated with AD, the majority of the heritability and the functional relevance of discovered variants are not explained or known by the identified variants. The limited success of singular approaches underscores the need for holistic and integrated approaches to investigate complex phenotypes using trans-omics data integration strategies. Integrating omics layers (e.g., genome, epigenome, transcriptome, proteome, metabolome, lipidome, exposome, microbiome), which often have complementary and synergistic effects, might provide the opportunity to capture the flow of information underlying AD disease manifestation. Overlapping genes/candidates derived from multiple omics types include FLG, SPINK5, S100A8, and SERPINB3 in AD pathogenesis. Overlapping pathways include macrophage, endothelial cell and fibroblast activation pathways, in addition to well-known Th1/Th2 and NFkB activation pathways. Interestingly, there was more multi-omics overlap at the pathway level than gene level. Further analysis of multi-omics overlap at the tissue level showed that among 30 tissue types from the GTEx database, skin and esophagus were significantly enriched, indicating the biological interconnection between AD and food allergy. The present work explores multi-omics integration and provides new biological insights to better define the biological basis of AD etiology and confirm previously reported AD genes/pathways. In this context, we also discuss opportunities and challenges introduced by ""big omics data"" and their integration.",2018-12-01,13,2544,92,889
2534,30560386,High Throughput and Computational Repurposing for Neglected Diseases,"Purpose:                    Neglected tropical diseases (NTDs) represent are a heterogeneous group of communicable diseases that are found within the poorest populations of the world. There are 23 NTDs that have been prioritized by the World Health Organization, which are endemic in 149 countries and affect more than 1.4 billion people, costing these developing economies billions of dollars annually. The NTDs result from four different causative pathogens: protozoa, bacteria, helminth and virus. The majority of the diseases lack effective treatments. Therefore, new therapeutics for NTDs are desperately needed.              Methods:                    We describe various high throughput screening and computational approaches that have been performed in recent years. We have collated the molecules identified in these studies and calculated molecular properties.              Results:                    Numerous global repurposing efforts have yielded some promising compounds for various neglected tropical diseases. These compounds when analyzed as one would expect appear drug-like. Several large datasets are also now in the public domain and this enables machine learning models to be constructed that then facilitate the discovery of new molecules for these pathogens.              Conclusions:                    In the space of a few years many groups have either performed experimental or computational repurposing high throughput screens against neglected diseases. These have identified compounds which in many cases are already approved drugs. Such approaches perhaps offer a more efficient way to develop treatments which are generally not a focus for global pharmaceutical companies because of the economics or the lack of a viable market. Other diseases could perhaps benefit from these repurposing approaches.",2018-12-01,11,1835,68,889
2448,30759006,Improving Detection of Early Chronic Obstructive Pulmonary Disease,"Despite being a major cause of morbidity and mortality, chronic obstructive pulmonary disease (COPD) is frequently undiagnosed. Yet the burden of disease among the undiagnosed is significant, as these individuals experience symptoms, exacerbations, and excess mortality compared to those without COPD. The U.S. Preventive Services Task Force recommends against routine screening of asymptomatic individuals with spirometry. Hence, case-finding approaches are needed. A recently developed instrument, the five-item COPD Assessment in Primary Care to Identify Undiagnosed Respiratory Disease and Exacerbation Risk questionnaire plus peak expiratory flow, demonstrates good sensitivity and specificity for distinguishing cases from control subjects and is being studied prospectively in primary care settings to determine its impact on patient outcomes. However, finding the undiagnosed is only half the battle. Mounting evidence suggests significant COPD-like respiratory burden among individuals without airflow obstruction. Many experience dyspnea, mucus production, and exacerbation events and have emphysema and airway abnormalities on computed tomographic (CT) imaging of the chest. However, it is still unclear how to best treat these individuals and which individuals go on to develop spirometric obstruction. These challenges underline the importance of defining what constitutes ""early disease."" A recently proposed definition characterizes early COPD as either: 1) airflow limitation, 2) compatible CT imaging abnormalities, or 3) accelerated forced expiratory volume in 1 second decline in persons younger than 50 years and with greater than a 10 pack-year smoking history. Although it is recognized that this definition does not encompass all individuals who will develop COPD, it is an attempt to identify a group of individuals with most rapid decline to better understand mechanisms of disease development and where disease-modifying interventions are most likely to be successful. Ultimately, leveraging tools such as chest CT imaging, the electronic medical record, and machine learning algorithms may aid in the identification of such individuals.",2018-12-01,8,2163,66,889
459,30308542,Who is a high-risk surgical patient?,"Purpose of review:                    Timely identification of high-risk surgical candidates facilitate surgical decision-making and allows appropriate tailoring of perioperative management strategies. This review aims to summarize the recent advances in perioperative risk stratification.              Recent findings:                    Use of indices which include various combinations of preoperative and postoperative variables remain the most commonly used risk-stratification strategy. Incorporation of biomarkers (troponin and natriuretic peptides), comprehensive objective assessment of functional capacity, and frailty into the current framework enhance perioperative risk estimation. Intraoperative hemodynamic parameters can provide further signals towards identifying patients at risk of adverse postoperative outcomes. Implementation of machine-learning algorithms is showing promising results in real-time forecasting of perioperative outcomes.              Summary:                    Perioperative risk estimation is multidimensional including validated indices, biomarkers, functional capacity estimation, and intraoperative hemodynamics. Identification and implementation of targeted strategies which mitigate predicted risk remains a greater challenge.",2018-12-01,2,1272,36,889
2568,30515717,"Big data, artificial intelligence, and structured reporting","The past few years have seen a considerable rise in interest towards artificial intelligence and machine learning applications in radiology. However, in order for such systems to perform adequately, large amounts of training data are required. These data should ideally be standardised and of adequate quality to allow for further usage in training of artificial intelligence algorithms. Unfortunately, in many current clinical and radiological information technology ecosystems, access to relevant pieces of information is difficult. This is mostly because a significant portion of information is handled as a collection of narrative texts and interoperability is still lacking. This review aims at giving a brief overview on how structured reporting can help to facilitate research in artificial intelligence and the context of big data.",2018-12-01,11,839,59,889
467,30297207,'Nonlinear' Biochemistry of Nucleosome Detergents,"The transcriptional activation domains (TADs) are critical for life, yet intrinsically disordered polypeptides with no specific consensus sequence, interacting with multiple targets via low-specificity fuzzy contacts. The recent integration of machine learning approaches in biochemistry allows analysis of large experimental datasets of functional TADs as a whole and clear observation of TAD features. The emerging picture describes TADs as sequences without consensus but with a variety of detergent-like mini-motifs enriched in negatively charged and aromatic amino acids. Comparison of the canonical direct coactivator recruitment model and a new model describing TADs as nucleosome detergents that trigger chromatin remodeling during gene activation helps solve a fundamental enigma of molecular biology spanning 30 years.",2018-12-01,3,828,49,889
463,30303758,Hippocampal replays under the scrutiny of reinforcement learning models,"Multiple in vivo studies have shown that place cells from the hippocampus replay previously experienced trajectories. These replays are commonly considered to mainly reflect memory consolidation processes. Some data, however, have highlighted a functional link between replays and reinforcement learning (RL). This theory, extensively used in machine learning, has introduced efficient algorithms and can explain various behavioral and physiological measures from different brain regions. RL algorithms could constitute a mechanistic description of replays and explain how replays can reduce the number of iterations required to explore the environment during learning. We review the main findings concerning the different hippocampal replay types and the possible associated RL models (either model-based, model-free, or hybrid model types). We conclude by tying these frameworks together. We illustrate the link between data and RL through a series of model simulations. This review, at the frontier between informatics and biology, paves the way for future work on replays.",2018-12-01,5,1076,71,889
2563,30504368,Application of Artificial Intelligence-based Technology in Cancer Management: A Commentary on the Deployment of Artificial Neural Networks,"Artificial intelligence was recognised many years ago as a potential and powerful tool to predict disease outcome in many clinical situations. The conventional approaches using statistical methods have provided much information, but are subject to limitations imposed by the complexity of medical data. The structures of the important variants of the machine learning system artificial neural networks (ANN) are discussed and emphasis is given to the powerful analytical support that could be provided by ANN for the prediction of cancer progression and prognosis. The predictive ability of the cellular markers, DNA ploidy and cell-cycle profiles, and molecular markers, such as tumour promoter and suppressor gene, and growth factor and steroid hormone receptors in breast cancer management were also analysed. ANN systems have been successfully deployed to evaluate microRNA profiles of tumours which saliently sway cancer progression and prognosis of the disease, thus counteracting the negative implications of their numerical abundance. Finally, in this setting, the prospective technical improvements in artificial neural networks, as hybrid systems in combination with fuzzy logic and artificial immune networks were also addressed.",2018-12-01,3,1240,138,889
480,30255463,Deep learning with convolutional neural network for objective skill evaluation in robot-assisted surgery,"Purpose:                    With the advent of robot-assisted surgery, the role of data-driven approaches to integrate statistics and machine learning is growing rapidly with prominent interests in objective surgical skill assessment. However, most existing work requires translating robot motion kinematics into intermediate features or gesture segments that are expensive to extract, lack efficiency, and require significant domain-specific knowledge.              Methods:                    We propose an analytical deep learning framework for skill assessment in surgical training. A deep convolutional neural network is implemented to map multivariate time series data of the motion kinematics to individual skill levels.              Results:                    We perform experiments on the public minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our proposed learning model achieved competitive accuracies of 92.5%, 95.4%, and 91.3%, in the standard training tasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need of engineered features or carefully tuned gesture segmentation, our model can successfully decode skill information from raw motion profiles via end-to-end learning. Meanwhile, the proposed model is able to reliably interpret skills within a 1-3 second window, without needing an observation of entire training trial.              Conclusion:                    This study highlights the potential of deep architectures for efficient online skill assessment in modern surgical training.",2018-12-01,17,1590,104,889
2559,30527225,The application of artificial intelligence in the IMRT planning process for head and neck cancer,"Artificial intelligence (AI) is beginning to transform IMRT treatment planning for head and neck patients. However, the complexity and novelty of AI algorithms make them susceptible to misuse by researchers and clinicians. Understanding nuances of new technologies could serve to mitigate potential clinical implementation pitfalls. This article is intended to facilitate integration of AI into the radiotherapy clinic by providing an overview of AI algorithms, including support vector machines (SVMs), random forests (RF), gradient boosting (GB), and several variations of deep learning. This document describes current AI algorithms that have been applied to head and neck IMRT planning and identifies rapidly growing branches of AI in industry that have potential applications to head and neck cancer patients receiving IMRT. AI algorithms have great clinical potential if used correctly but can also cause harm if misused, so it is important to raise the level of AI competence within radiation oncology so that the benefits can be realized in a controlled and safe manner.",2018-12-01,6,1078,96,889
474,30277175,"Review: Precision nutrition of ruminants: approaches, challenges and potential gains","A plethora of sensors and information technologies with applications to the precision nutrition of herbivores have been developed and continue to be developed. The nutritional processes start outside of the animal body with the available feed (quantity and quality) and continue inside it once the feed is consumed, degraded in the gastrointestinal tract and metabolised by organs and tissues. Finally, some nutrients are wasted via urination, defecation and gaseous emissions through breathing and belching whereas remaining nutrients ensure maintenance and production. Nowadays, several processes can be monitored in real-time using new technologies, but although these provide valuable data 'as is', further gains could be obtained using this information as inputs to nutrition simulation models to predict unmeasurable variables in real-time and to forecast outcomes of interest. Data provided by sensors can create synergies with simulation models and this approach has the potential to expand current applications. In addition, data provided by sensors could be used with advanced analytical techniques such as data fusion, optimisation techniques and machine learning to improve their value for applications in precision animal nutrition. The present paper reviews technologies that can monitor different nutritional processes relevant to animal production, profitability, environmental management and welfare. We discussed the model-data fusion approach in which data provided by sensor technologies can be used as input of nutrition simulation models in near-real time to produce more accurate, certain and timely predictions. We also discuss some examples that have taken this model-data fusion approach to complement the capabilities of both models and sensor data, and provided examples such as predicting feed intake and methane emissions. Challenges with automatising the nutritional management of individual animals include monitoring and predicting of the flow of nutrients including nutrient intake, quantity and composition of body growth and milk production, gestation, maintenance and physical activities at the individual animal level. We concluded that the livestock industries are already seeing benefits from the development of sensor and information technologies, and this benefit is expected to grow exponentially soon with the integration of nutrition simulation models and techniques for big data analysis. However, this approach may need re-evaluating or performing new empirical research in both fields of animal nutrition and simulation modelling to accommodate a new type of data provided by the sensor technologies.",2018-12-01,5,2648,84,889
407,30403523,Machine Learning in Neurooncology Imaging: From Study Request to Diagnosis and Treatment,"Objective:                    Machine learning has potential to play a key role across a variety of medical imaging applications. This review seeks to elucidate the ways in which machine learning can aid and enhance diagnosis, treatment, and follow-up in neurooncology.              Conclusion:                    Given the rapid pace of development in machine learning over the past several years, a basic proficiency of the key tenets and use cases in the field is critical to assessing potential opportunities and challenges of this exciting new technology.",2019-01-01,0,560,88,858
2494,30650551,Optimized Clustering Algorithms for Large Wireless Sensor Networks: A Review,"During the past few years, Wireless Sensor Networks (WSNs) have become widely used due to their large amount of applications. The use of WSNs is an imperative necessity for future revolutionary areas like ecological fields or smart cities in which more than hundreds or thousands of sensor nodes are deployed. In those large scale WSNs, hierarchical approaches improve the performance of the network and increase its lifetime. Hierarchy inside a WSN consists in cutting the whole network into sub-networks called clusters which are led by Cluster Heads. In spite of the advantages of the clustering on large WSNs, it remains a non-deterministic polynomial hard problem which is not solved efficiently by traditional clustering. The recent researches conducted on Machine Learning, Computational Intelligence, and WSNs bring out the optimized clustering algorithms for WSNs. These kinds of clustering are based on environmental behaviors and outperform the traditional clustering algorithms. However, due to the diversity of WSN applications, the choice of an appropriate paradigm for a clustering solution remains a problem. In this paper, we conduct a wide review of proposed optimized clustering solutions nowadays. In order to evaluate them, we consider 10 parameters. Based on these parameters, we propose a comparison of these optimized clustering approaches. From the analysis, we observe that centralized clustering solutions based on the Swarm Intelligence paradigm are more adapted for applications with low energy consumption, high data delivery rate, or high scalability than algorithms based on the other presented paradigms. Moreover, when an application does not need a large amount of nodes within a field, the Fuzzy Logic based solution are suitable.",2019-01-01,6,1766,76,858
2555,30531846,Stimulus- and goal-oriented frameworks for understanding natural vision,"Our knowledge of sensory processing has advanced dramatically in the last few decades, but this understanding remains far from complete, especially for stimuli with the large dynamic range and strong temporal and spatial correlations characteristic of natural visual inputs. Here we describe some of the issues that make understanding the encoding of natural images a challenge. We highlight two broad strategies for approaching this problem: a stimulus-oriented framework and a goal-oriented one. Different contexts can call for one framework or the other. Looking forward, recent advances, particularly those based in machine learning, show promise in borrowing key strengths of both frameworks and by doing so illuminating a path to a more comprehensive understanding of the encoding of natural stimuli.",2019-01-01,4,806,71,858
2556,30531069,Opportunities and challenges for developing closed-loop bioelectronic medicines,"The peripheral nervous system plays a major role in the maintenance of our physiology. Several peripheral nerves intimately regulate the state of the brain, spinal cord, and visceral systems. A new class of therapeutics, called bioelectronic medicines, are being developed to precisely regulate physiology and treat dysfunction using peripheral nerve stimulation. In this review, we first discuss new work using closed-loop bioelectronic medicine to treat upper limb paralysis. In contrast to open-loop bioelectronic medicines, closed-loop approaches trigger 'on demand' peripheral nerve stimulation due to a change in function (e.g., during an upper limb movement or a change in cardiopulmonary state). We also outline our perspective on timing rules for closed-loop bioelectronic stimulation, interface features for non-invasively stimulating peripheral nerves, and machine learning algorithms to recognize disease events for closed-loop stimulation control. Although there will be several challenges for this emerging field, we look forward to future bioelectronic medicines that can autonomously sense changes in the body, to provide closed-loop peripheral nerve stimulation and treat disease.",2019-01-01,1,1197,79,858
426,30370798,Post-viral atopic airway disease: pathogenesis and potential avenues for intervention,"Introduction: In early childhood, wheezing due to lower respiratory tract illness is often associated with infection by commonly known respiratory viruses such as respiratory syncytial virus (RSV) and human rhinovirus (RV). How respiratory viral infections lead to wheeze and/or asthma is an area of active research. Areas covered: This review provides an updated summary of the published information on the development of post-viral induced atopy and asthma and the mechanisms involved. We focus on the contribution of animal models in identifying pathways that may contribute to atopy and asthma following respiratory virus infection, different polymorphisms that have been associated with asthma development, and current options for disease management and potential future interventions. Expert commentary: Currently there are no prophylactic therapies that prevent infants infected with respiratory viruses from developing asthma or atopy. Neither are there curative therapies for patients with asthma. Therefore, a better understanding of genetic factors and other associated biomarkers in respiratory viral induced pathogenesis is important for developing effective personalized therapies.",2019-01-01,3,1195,85,858
401,30418085,Systems Biology and Machine Learning in Plant-Pathogen Interactions,"Systems biology is an inclusive approach to study the static and dynamic emergent properties on a global scale by integrating multiomics datasets to establish qualitative and quantitative associations among multiple biological components. With an abundance of improved high throughput -omics datasets, network-based analyses and machine learning technologies are playing a pivotal role in comprehensive understanding of biological systems. Network topological features reveal most important nodes within a network as well as prioritize significant molecular components for diverse biological networks, including coexpression, protein-protein interaction, and gene regulatory networks. Machine learning techniques provide enormous predictive power through specific feature extraction from biological data. Deep learning, a subtype of machine learning, has plausible future applications because a domain expert for feature extraction is not needed in this algorithm. Inspired by diverse domains of biology, we here review classic systems biology techniques applied in plant immunity thus far. We also discuss additional advanced approaches in both graph theory and machine learning, which may provide new insights for understanding plant-microbe interactions. Finally, we propose a hybrid approach in plant immune systems that harnesses the power of both network biology and machine learning, with a potential to be applicable to both model systems and agronomically important crop plants.",2019-01-01,8,1487,67,858
2493,30650562,Predicting VTE in Cancer Patients: Candidate Biomarkers and Risk Assessment Models,"Risk prediction of chemotherapy-associated venous thromboembolism (VTE) is a compelling challenge in contemporary oncology, as VTE may result in treatment delays, impaired quality of life, and increased mortality. Current guidelines do not recommend thromboprophylaxis for primary prevention, but assessment of the patient's individual risk of VTE prior to chemotherapy is generally advocated. In recent years, efforts have been devoted to building accurate predictive tools for VTE risk assessment in cancer patients. This review focuses on candidate biomarkers and prediction models currently under investigation, considering their advantages and disadvantages, and discussing their diagnostic performance and potential pitfalls.",2019-01-01,7,731,82,858
424,30371751,Machine Learning in Human Olfactory Research,"The complexity of the human sense of smell is increasingly reflected in complex and high-dimensional data, which opens opportunities for data-driven approaches that complement hypothesis-driven research. Contemporary developments in computational and data science, with its currently most popular implementation as machine learning, facilitate complex data-driven research approaches. The use of machine learning in human olfactory research included major approaches comprising 1) the study of the physiology of pattern-based odor detection and recognition processes, 2) pattern recognition in olfactory phenotypes, 3) the development of complex disease biomarkers including olfactory features, 4) odor prediction from physico-chemical properties of volatile molecules, and 5) knowledge discovery in publicly available big databases. A limited set of unsupervised and supervised machine-learned methods has been used in these projects, however, the increasing use of contemporary methods of computational science is reflected in a growing number of reports employing machine learning for human olfactory research. This review provides key concepts of machine learning and summarizes current applications on human olfactory data.",2019-01-01,4,1228,44,858
2501,30628494,Deep learning for image analysis: Personalizing medicine closer to the point of care,"The precision-based revolution in medicine continues to demand stratification of patients into smaller and more personalized subgroups. While genomic technologies have largely led this movement, diagnostic results can take days to weeks to generate. Management at, or closer to, the point of care still heavily relies on the subjective qualitative interpretation of clinical and diagnostic imaging findings. New and emerging technological advances in artificial intelligence (AI) now appear poised to help bring objectivity and precision to these traditionally qualitative analytic tools. In particular, one specific form of AI, known as deep learning, is achieving expert-level disease classifications in many areas of diagnostic medicine dependent on visual and image-based findings. Here, we briefly review concepts of deep learning, and more specifically recent developments in convolutional neural networks (CNNs), to highlight their transformative potential in personalized medicine and, in particular, diagnostic histopathology. Understanding the opportunities and challenges of these quantitative machine-based decision support tools is critical to their widespread introduction into routine diagnostics.",2019-01-01,12,1212,84,858
1045,31495281,Artificial Intelligence in Radiotherapy Treatment Planning: Present and Future,"Treatment planning is an essential step of the radiotherapy workflow. It has become more sophisticated over the past couple of decades with the help of computer science, enabling planners to design highly complex radiotherapy plans to minimize the normal tissue damage while persevering sufficient tumor control. As a result, treatment planning has become more labor intensive, requiring hours or even days of planner effort to optimize an individual patient case in a trial-and-error fashion. More recently, artificial intelligence has been utilized to automate and improve various aspects of medical science. For radiotherapy treatment planning, many algorithms have been developed to better support planners. These algorithms focus on automating the planning process and/or optimizing dosimetric trade-offs, and they have already made great impact on improving treatment planning efficiency and plan quality consistency. In this review, the smart planning tools in current clinical use are summarized in 3 main categories: automated rule implementation and reasoning, modeling of prior knowledge in clinical practice, and multicriteria optimization. Novel artificial intelligence-based treatment planning applications, such as deep learning-based algorithms and emerging research directions, are also reviewed. Finally, the challenges of artificial intelligence-based treatment planning are discussed for future works.",2019-01-01,8,1421,78,858
184,30044963,Machine learning approaches infer vitamin D signaling: Critical impact of vitamin D receptor binding within topologically associated domains,"The vitamin D-modulated transcriptome of highly responsive human cells, such as THP-1 monocytes, comprises more than 500 genes, half of which are primary targets. Recently, we proposed a chromatin model of vitamin D signaling demonstrating that nearly all vitamin D target genes are located within vitamin D-modulated topologically associated domains (TADs). This model is based on genome-wide binding patterns of the vitamin D receptor (VDR), the pioneer transcription factor PU.1, the chromatin organizer CTCF and histone markers of active promoter regions (H3K4me3) and active chromatin (H3K27ac). In addition, time-dependent data on accessible chromatin and mRNA expression are implemented. For the interrogation and in deep inspection of these high-dimensional datasets unsupervised and supervised machine learning algorithms were applied. Unsupervised methods, such as the vector quantization tool K-means and the dimensionality reduction algorithm self-organizing map, generated descriptions of how attributes, such as VDR binding and chromatin accessibility, affect each other as a function of time and/or co-localization within the same genomic region. Supervised algorithms, such as random forests, allowed the data to be classified into pre-existing categories like persistent (i.e. constant) and time-dependent (i.e. transient) VDR binding sites. The relative amounts of these VDR categories in TADs showed to be the main discriminator for sorting the latter into five classes carrying vitamin D target genes involved in distinct biological processes. In conclusion, via the application of machine learning methods we identified the spatio-temporal VDR binding pattern in TADs as the most critical attribute for specific regulation of vitamin D target genes and the segregation of vitamin D's physiologic function.",2019-01-01,4,1826,140,858
2915,29925268,Multimodal Neuroimaging: Basic Concepts and Classification of Neuropsychiatric Diseases,"Neuroimaging techniques are widely used in neuroscience to visualize neural activity, to improve our understanding of brain mechanisms, and to identify biomarkers-especially for psychiatric diseases; however, each neuroimaging technique has several limitations. These limitations led to the development of multimodal neuroimaging (MN), which combines data obtained from multiple neuroimaging techniques, such as electroencephalography, functional magnetic resonance imaging, and yields more detailed information about brain dynamics. There are several types of MN, including visual inspection, data integration, and data fusion. This literature review aimed to provide a brief summary and basic information about MN techniques (data fusion approaches in particular) and classification approaches. Data fusion approaches are generally categorized as asymmetric and symmetric. The present review focused exclusively on studies based on symmetric data fusion methods (data-driven methods), such as independent component analysis and principal component analysis. Machine learning techniques have recently been introduced for use in identifying diseases and biomarkers of disease. The machine learning technique most widely used by neuroscientists is classification-especially support vector machine classification. Several studies differentiated patients with psychiatric diseases and healthy controls with using combined datasets. The common conclusion among these studies is that the prediction of diseases increases when combining data via MN techniques; however, there remain a few challenges associated with MN, such as sample size. Perhaps in the future N-way fusion can be used to combine multiple neuroimaging techniques or nonimaging predictors (eg, cognitive ability) to overcome the limitations of MN.",2019-01-01,4,1809,87,858
2489,30657889,A comprehensive review and comparison of existing computational methods for intrinsically disordered protein and region prediction,"Intrinsically disordered proteins and regions are widely distributed in proteins, which are associated with many biological processes and diseases. Accurate prediction of intrinsically disordered proteins and regions is critical for both basic research (such as protein structure and function prediction) and practical applications (such as drug development). During the past decades, many computational approaches have been proposed, which have greatly facilitated the development of this important field. Therefore, a comprehensive and updated review is highly required. In this regard, we give a review on the computational methods for intrinsically disordered protein and region prediction, especially focusing on the recent development in this field. These computational approaches are divided into four categories based on their methodologies, including physicochemical-based method, machine-learning-based method, template-based method and meta method. Furthermore, their advantages and disadvantages are also discussed. The performance of 40 state-of-the-art predictors is directly compared on the target proteins in the task of disordered region prediction in the 10th Critical Assessment of protein Structure Prediction. A more comprehensive performance comparison of 45 different predictors is conducted based on seven widely used benchmark data sets. Finally, some open problems and perspectives are discussed.",2019-01-01,21,1422,130,858
123,30155978,Current state and future prospects of artificial intelligence in ophthalmology: a review,"Artificial intelligence (AI) has emerged as a major frontier in computer science research. Although AI has broad application across many medical fields, it will have particular utility in ophthalmology and will dramatically change the diagnostic and treatment pathways for many eye conditions such as corneal ectasias, glaucoma, age-related macular degeneration and diabetic retinopathy. However, given that AI has primarily been driven as a computer science, its concepts and terminology are unfamiliar to many medical professionals. Important key terms such as machine learning and deep learning are often misunderstood and incorrectly used interchangeably. This article presents an overview of AI and new developments relevant to ophthalmology.",2019-01-01,12,747,88,858
2516,30599936,Interplay between food and gut microbiota in health and disease,"Numerous microorganisms colonize the human gastrointestinal tract playing pivotal roles in relation to digestion and absorption of dietary components. They biotransform food components and produce metabolites, which in combination with food components shape and modulate the host immune system and metabolic responses. Reciprocally, the diet modulates the composition and functional capacity of the gut microbiota, which subsequently influence host biochemical processes establishing a system of mutual interaction and inter-dependency. Macronutrients, fibers, as well as polyphenols and prebiotics are strong drivers shaping the composition of the gut microbiota. Especially, short-chain fatty acids produced from ingested fibers and tryptophan metabolites are key in modulating host immune responses. Since reciprocal interactions between diet, host, and microbiota are personal, understanding this complex network of interactions calls for novel use of large datasets and the implementation of machine learning algorithms and artificial intelligence. In this review, we aim to provide a base for future investigations of how interactions between food components and gut microbiota may influence or even determine human health and disease.",2019-01-01,15,1241,63,858
469,30293864,A review on microelectrode recording selection of features for machine learning in deep brain stimulation surgery for Parkinson's disease,"Objective:                    This study seeks to systematically review the selection of features and algorithms for machine learning and automation in deep brain stimulation surgery (DBS) for Parkinson's disease. This will assist in consolidating current knowledge and accuracy levels to allow greater understanding and research to be performed in automating this process, which could lead to improved clinical outcomes.              Methods:                    A systematic literature review search was conducted for all studies that utilized machine learning and DBS in Parkinson's disease.              Results:                    Ten studies were identified from 2006 utilizing machine learning in DBS surgery for Parkinson's disease. Different combinations of both spike independent and spike dependent features have been utilized with different machine learning algorithms to attempt to delineate the subthalamic nucleus (STN) and its surrounding structures.              Conclusion:                    The state-of-the-art algorithms achieve good accuracy and error rates with relatively short computing time, however, the currently achievable accuracy is not sufficiently robust enough for clinical practice. Moreover, further research is required for identifying subterritories of the STN.              Significance:                    This is a comprehensive summary of current machine learning algorithms that discriminate the STN and its adjacent structures for DBS surgery in Parkinson's disease.",2019-01-01,2,1510,137,858
472,30286415,A systematic meta-review of predictors of antidepressant treatment outcome in major depressive disorder,"Introduction:                    The heterogeneity of symptoms and complex etiology of depression pose a significant challenge to the personalization of treatment. Meanwhile, the current application of generic treatment approaches to patients with vastly differing biological and clinical profiles is far from optimal. Here, we conduct a meta-review to identify predictors of response to antidepressant therapy in order to select robust input features for machine learning models of treatment response. These machine learning models will allow us to learn associations between patient features and treatment response which have predictive value at the individual patient level; this learning can be optimized by selecting high-quality input features for the model. While current research is difficult to directly apply to the clinic, machine learning models built using knowledge gleaned from current research may become useful clinical tools.              Methods:                    The EMBASE and MEDLINE/PubMed online databases were searched from January 1996 to August 2017, using a combination of MeSH terms and keywords to identify relevant literature reviews. We identified a total of 1909 articles, wherein 199 articles met our inclusion criteria.              Results:                    An array of genetic, immune, endocrine, neuroimaging, sociodemographic, and symptom-based predictors of treatment response were extracted, varying widely in clinical utility.              Limitations:                    Due to heterogeneous sample sizes, effect sizes, publication biases, and methodological disparities across reviews, we could not accurately assess the strength and directionality of every predictor.              Conclusion:                    Notwithstanding our cautious interpretation of the results, we have identified a multitude of predictors that can be used to formulate a priori hypotheses regarding the input features for a computational model. We highlight the importance of large-scale research initiatives and clinically accessible biomarkers, as well as the need for replication studies of current findings. In addition, we provide recommendations for future improvement and standardization of research efforts in this field.",2019-01-01,15,2256,103,858
2203,31213198,Analysis of Collagen Spatial Structure Using Multiphoton Microscopy and Machine Learning Methods,"Pathogenesis of many diseases is associated with changes in the collagen spatial structure. Traditionally, the 3D structure of collagen in biological tissues is analyzed using histochemistry, immunohistochemistry, magnetic resonance imaging, and X-radiography. At present, multiphoton microscopy (MPM) is commonly used to study the structure of biological tissues. MPM has a high spatial resolution comparable to histological analysis and can be used for direct visualization of collagen spatial structure. Because of a large volume of data accumulated due to the high spatial resolution of MPM, special analytical methods should be used for identification of informative features in the images and quantitative evaluation of relationship between these features and pathological processes resulting in the destruction of collagen structure. Here, we describe current approaches and achievements in the identification of informative features in the MPM images of collagen in biological tissues, as well as the development on this basis of algorithms for computer-aided classification of collagen structures using machine learning as a type of artificial intelligence methods.",2019-01-01,2,1174,96,858
2560,30520975,Computer vision-based phenotyping for improvement of plant productivity: a machine learning perspective,"Employing computer vision to extract useful information from images and videos is becoming a key technique for identifying phenotypic changes in plants. Here, we review the emerging aspects of computer vision for automated plant phenotyping. Recent advances in image analysis empowered by machine learning-based techniques, including convolutional neural network-based modeling, have expanded their application to assist high-throughput plant phenotyping. Combinatorial use of multiple sensors to acquire various spectra has allowed us to noninvasively obtain a series of datasets, including those related to the development and physiological responses of plants throughout their life. Automated phenotyping platforms accelerate the elucidation of gene functions associated with traits in model plants under controlled conditions. Remote sensing techniques with image collection platforms, such as unmanned vehicles and tractors, are also emerging for large-scale field phenotyping for crop breeding and precision agriculture. Computer vision-based phenotyping will play significant roles in both the nowcasting and forecasting of plant traits through modeling of genotype/phenotype relationships.",2019-01-01,13,1197,103,858
404,30414395,Human and computational models of atopic dermatitis: A review and perspectives by an expert panel of the International Eczema Council,"Atopic dermatitis (AD) is a prevalent disease worldwide and is associated with systemic comorbidities representing a significant burden on patients, their families, and society. Therapeutic options for AD remain limited, in part because of a lack of well-characterized animal models. There has been increasing interest in developing experimental approaches to study the pathogenesis of human AD in vivo, in vitro, and in silico to better define pathophysiologic mechanisms and identify novel therapeutic targets and biomarkers that predict therapeutic response. This review critically appraises a range of models, including genetic mutations relevant to AD, experimental challenge of human skin in vivo, tissue culture models, integration of ""omics"" data sets, and development of predictive computational models. Although no one individual model recapitulates the complex AD pathophysiology, our review highlights insights gained into key elements of cutaneous biology, molecular pathways, and therapeutic target identification through each approach. Recent developments in computational analysis, including application of machine learning and a systems approach to data integration and predictive modeling, highlight the applicability of these methods to AD subclassification (endotyping), therapy development, and precision medicine. Such predictive modeling will highlight knowledge gaps, further inform refinement of biological models, and support new experimental and systems approaches to AD.",2019-01-01,5,1498,133,858
2505,30621101,An Appraisal of Lung Nodules Automatic Classification Algorithms for CT Images,"Lung cancer is one of the most deadly diseases around the world representing about 26% of all cancers in 2017. The five-year cure rate is only 18% despite great progress in recent diagnosis and treatment. Before diagnosis, lung nodule classification is a key step, especially since automatic classification can help clinicians by providing a valuable opinion. Modern computer vision and machine learning technologies allow very fast and reliable CT image classification. This research area has become very hot for its high efficiency and labor saving. The paper aims to draw a systematic review of the state of the art of automatic classification of lung nodules. This research paper covers published works selected from the Web of Science, IEEEXplore, and DBLP databases up to June 2018. Each paper is critically reviewed based on objective, methodology, research dataset, and performance evaluation. Mainstream algorithms are conveyed and generic structures are summarized. Our work reveals that lung nodule classification based on deep learning becomes dominant for its excellent performance. It is concluded that the consistency of the research objective and integration of data deserves more attention. Moreover, collaborative works among developers, clinicians, and other parties should be strengthened.",2019-01-01,6,1309,78,858
2186,31673712,Advances in Data-Driven Responses to Preventing Spread of Antibiotic Resistance Across Health-Care Settings,"Among the most urgent and serious threats to public health are 7 antibiotic-resistant bacterial infections predominately acquired during health-care delivery. There is an emerging field of health-care epidemiology that is focused on preventing health care-associated infections with antibiotic-resistant bacteria and incorporates data from patient transfers or patient movements within and between facilities. This analytic field is being used to help public health professionals identify best opportunities for prevention. Different analytic approaches that draw on uses of big data are being explored to help target the use of limited public health resources, leverage expertise, and enact effective policy to maximize an impact on population-level health. Here, the following recent advances in data-driven responses to preventing spread of antibiotic resistance across health-care settings are summarized: leveraging big data for machine learning, integration or advances in tracking patient movement, and highlighting the value of coordinating response across institutions within a region.",2019-01-01,0,1094,107,858
484,30247662,Radiomics with artificial intelligence for precision medicine in radiation therapy,"Recently, the concept of radiomics has emerged from radiation oncology. It is a novel approach for solving the issues of precision medicine and how it can be performed, based on multimodality medical images that are non-invasive, fast and low in cost. Radiomics is the comprehensive analysis of massive numbers of medical images in order to extract a large number of phenotypic features (radiomic biomarkers) reflecting cancer traits, and it explores the associations between the features and patients' prognoses in order to improve decision-making in precision medicine. Individual patients can be stratified into subtypes based on radiomic biomarkers that contain information about cancer traits that determine the patient's prognosis. Machine-learning algorithms of AI are boosting the powers of radiomics for prediction of prognoses or factors associated with treatment strategies, such as survival time, recurrence, adverse events, and subtypes. Therefore, radiomic approaches, in combination with AI, may potentially enable practical use of precision medicine in radiation therapy by predicting outcomes and toxicity for individual patients.",2019-01-01,14,1147,82,858
486,30240882,A review study: Computational techniques for expecting the impact of non-synonymous single nucleotide variants in human diseases,"Non-Synonymous Single-Nucleotide Variants (nsSNVs) and mutations can create a diversity effect on proteins as changing genotype and phenotype, which interrupts its stability. The alterations in the protein stability may cause diseases like cancer. Discovering of nsSNVs and mutations can be a useful tool for diagnosing the disease at a beginning stage. Many studies introduced the various predicting singular and consensus tools that based on different Machine Learning Techniques (MLTs) using diverse datasets. Therefore, we introduce the current comprehensive review of the most popular and recent unique tools that predict pathogenic variations and Meta-tool that merge some of them for enhancing their predictive power. Also, we scanned the several types computational techniques in the state-of-the-art and methods for predicting the effect both of coding and noncoding variants. We then displayed, the protein stability predictors. We offer the details of the most common benchmark database for variations including the main predictive features used by the different methods. Finally, we address the most common fundamental criteria for performance assessment of predictive tools. This review is targeted at bioinformaticians attentive in the characterization of regulatory variants, geneticists, molecular biologists attentive in understanding more about the nature and effective role of such variants from a functional point of views, and clinicians who may hope to learn about variants in human associated with a specific disease and find out what to do next to uncover how they impact on the underlying mechanisms.",2019-01-01,6,1625,128,858
2514,30609719,"A Comprehensive Survey on Spectrum Sensing in Cognitive Radio Networks: Recent Advances, New Challenges, and Future Research Directions","Cognitive radio technology has the potential to address the shortage of available radio spectrum by enabling dynamic spectrum access. Since its introduction, researchers have been working on enabling this innovative technology in managing the radio spectrum. As a result, this research field has been progressing at a rapid pace and significant advances have been made. To help researchers stay abreast of these advances, surveys and tutorial papers are strongly needed. Therefore, in this paper, we aimed to provide an in-depth survey on the most recent advances in spectrum sensing, covering its development from its inception to its current state and beyond. In addition, we highlight the efficiency and limitations of both narrowband and wideband spectrum sensing techniques as well as the challenges involved in their implementation. TV white spaces are also discussed in this paper as the first real application of cognitive radio. Last but by no means least, we discuss future research directions. This survey paper was designed in a way to help new researchers in the field to become familiar with the concepts of spectrum sensing, compressive sensing, and machine learning, all of which are the enabling technologies of the future networks, yet to help researchers further improve the efficiently of spectrum sensing.",2019-01-01,13,1326,135,858
593,30881899,Personalising medicine in inflammatory bowel disease-current and future perspectives,"Up to 25% of inflammatory bowel disease (IBD) presents during childhood, often with severe and extensive disease, leading to significant morbidity including delayed growth and nutritional impairment. The classical approach to management has centred on differentiation into Crohn's disease (CD) or ulcerative colitis (UC), with subsequent treatment based on symptoms, results and complications. However, IBD is a heterogeneous condition with substantial variation in phenotype, disease course and outcome, so whilst effective treatment exists one size does not fit all. The ability to predict disease course at diagnosis, alongside tailoring medications based on response gives the potential for a more 'personalised approach'. The move to a pre-emptive strategy to prevent IBD-related complications, whilst simultaneously minimising side effects and long-term toxicity from therapy, particularly in those with relatively indolent disease, has the potential to revolutionise care. In very early-onset IBD, personalised approaches to diagnosis and management have become the standard of treatment enabling clinicians to significantly alter the outcomes of the few children with monogenic disease. However, the promise of discoveries in genomics, microbiome and transcriptomics in paediatric IBD has not yet translated to clinical application for the vast majority of patients. Despite this, the opportunity presents itself to apply data gathered at diagnosis and follow-up to predict which patients are likely to progress to complicated disease, which will respond well and which will require additional therapy. Using complex mathematics and innovative, cutting-edge machine learning (ML) techniques gives the potential to use this data to develop personalised clinical care algorithms to treat patients more effectively, reduce toxicity and improve outcome. In this review, we will consider current management of paediatric IBD, discuss how precision medicine is making inroads into clinical practice already, examine the contemporary studies applying data to stratify patients and explore how future management may be revolutionised by personalisation with clinical, genomic and other multi-omic data.",2019-01-01,3,2202,84,858
406,30407665,Applying the latest advances in genomics and phenomics for trait discovery in polyploid wheat,"Improving traits in wheat has historically been challenging due to its large and polyploid genome, limited genetic diversity and in-field phenotyping constraints. However, within recent years many of these barriers have been lowered. The availability of a chromosome-level assembly of the wheat genome now facilitates a step-change in wheat genetics and provides a common platform for resources, including variation data, gene expression data and genetic markers. The development of sequenced mutant populations and gene-editing techniques now enables the rapid assessment of gene function in wheat directly. The ability to alter gene function in a targeted manner will unmask the effects of homoeolog redundancy and allow the hidden potential of this polyploid genome to be discovered. New techniques to identify and exploit the genetic diversity within wheat wild relatives now enable wheat breeders to take advantage of these additional sources of variation to address challenges facing food production. Finally, advances in phenomics have unlocked rapid screening of populations for many traits of interest both in greenhouses and in the field. Looking forwards, integrating diverse data types, including genomic, epigenetic and phenomics data, will take advantage of big data approaches including machine learning to understand trait biology in wheat in unprecedented detail.",2019-01-01,17,1380,93,858
429,30367497,Deep learning in medical imaging and radiation therapy,"The goals of this review paper on deep learning (DL) in medical imaging and radiation therapy are to (a) summarize what has been achieved to date; (b) identify common and unique challenges, and strategies that researchers have taken to address these challenges; and (c) identify some of the promising avenues for the future both in terms of applications as well as technical innovations. We introduce the general principles of DL and convolutional neural networks, survey five major areas of application of DL in medical imaging and radiation therapy, identify common themes, discuss methods for dataset expansion, and conclude by summarizing lessons learned, remaining challenges, and future directions.",2019-01-01,58,704,54,858
2457,30728827,"Genomic Selection in Aquaculture: Application, Limitations and Opportunities With Special Reference to Marine Shrimp and Pearl Oysters","Within aquaculture industries, selection based on genomic information (genomic selection) has the profound potential to change genetic improvement programs and production systems. Genomic selection exploits the use of realized genomic relationships among individuals and information from genome-wide markers in close linkage disequilibrium with genes of biological and economic importance. We discuss the technical advances, practical requirements, and commercial applications that have made genomic selection feasible in a range of aquaculture industries, with a particular focus on molluscs (pearl oysters, Pinctada maxima) and marine shrimp (Litopenaeus vannamei and Penaeus monodon). The use of low-cost genome sequencing has enabled cost-effective genotyping on a large scale and is of particular value for species without a reference genome or access to commercial genotyping arrays. We highlight the pitfalls and offer the solutions to the genotyping by sequencing approach and the building of appropriate genetic resources to undertake genomic selection from first-hand experience. We describe the potential to capture large-scale commercial phenotypes based on image analysis and artificial intelligence through machine learning, as inputs for calculation of genomic breeding values. The application of genomic selection over traditional aquatic breeding programs offers significant advantages through being able to accurately predict complex polygenic traits including disease resistance; increasing rates of genetic gain; minimizing inbreeding; and negating potential limiting effects of genotype by environment interactions. Further practical advantages of genomic selection through the use of large-scale communal mating and rearing systems are highlighted, as well as presenting rate-limiting steps that impact on attaining maximum benefits from adopting genomic selection. Genomic selection is now at the tipping point where commercial applications can be readily adopted and offer significant short- and long-term solutions to sustainable and profitable aquaculture industries.",2019-01-01,20,2093,134,858
2342,29481793,Patterns of epileptic seizure occurrence,"Background:                    The occurrence of epileptic seizures in seemingly random patterns takes a great toll on persons with epilepsy and their families. Seizure prediction may markedly improve epilepsy management and, therefore, the quality of life of persons with epilepsy.              Methods:                    Literature review.              Results:                    Seizures tend to occur following complex non-random patterns. Circadian oscillators may contribute to the rhythmic patterns of seizure occurrence. Complex mathematical models based on chaos theory try to explain and even predict seizure occurrence. There are several patterns of epileptic seizure occurrence based on seizure location, seizure semiology, and hormonal factors, among others. These patterns are most frequently described for large populations. Inter-individual variability and complex interactions between the rhythmic generators continue to make it more difficult to predict seizures in any individual person. The increasing use of large databases and machine learning techniques may help better define patterns of seizure occurrence in individual patients. Improvements in seizure detection -such as wearable seizure detectors- and in seizure prediction -such as machine learning techniques and artificial as well as neuronal networks- promise to provide further progress in the field of epilepsy and are being applied to closed-loop systems for the treatment of epilepsy.              Conclusions:                    Seizures tend to occur following complex and patient-specific patterns despite their apparently random occurrence. A better understanding of these patterns and current technological advances may allow the implementation of closed-loop detection, prediction, and treatment systems in routine clinical practice.",2019-01-01,3,1827,40,858
452,30317006,Characterization of expressed human meibum using hyperspectral stimulated Raman scattering microscopy,"Purpose:                    This study examined whether hyperspectral stimulated Raman scattering (hsSRS) microscopy can detect differences in meibum lipid to protein composition of normal and evaporative dry eye subjects with meibomian gland dysfunction.              Methods:                    Subjects were evaluated for tear breakup time (TBUT), staining, meibum expression and gland dropout. Expressed meibum was analyzed using SRS vibrational signatures in the CH stretching region (2800-3050 cm-1). Vertex component analysis and K-means clustering were used to group the spectral signatures into four fractions containing high lipid (G1) to high protein (G4).              Results:                    Thirty-three subjects could be statistically analyzed using pooled meibum (13 with stable tear films (TBUTs > 10 s) and 20 with unstable tear films (TBUTs  10 s). Significant differences in meibum from subjects with unstable vs. stable TBUTs were found for the G1 fraction (medians 0.164 and 0.020, respectively; p = 0.012) and the G2 fraction (medians 0.244 and 0.272, respectively; p = 0.045). No differences were observed for the G3 and G4 fractions. Single orifice samples were not significantly different vs. pooled samples from the fellow eye, and eyelid sector samples (nasal, central and temporal) G2:G3 fractional components were not significantly different (p = 0.449). Spearman analysis suggested a significant inverse correlation between G1 fraction and TBUT (R = -0.351; p = 0.045).              Conclusions:                    hsSRS microscopy allows compositional analysis of expressed meibum from humans which correlated to changes in TBUT. These findings support the hypothesis that hsSRS may be useful in classifying meibum quality and evaluating the effects of therapy.",2019-01-01,1,1798,101,858
2470,30696086,Machine Learning and Integrative Analysis of Biomedical Big Data,"Recent developments in high-throughput technologies have accelerated the accumulation of massive amounts of omics data from multiple sources: genome, epigenome, transcriptome, proteome, metabolome, etc. Traditionally, data from each source (e.g., genome) is analyzed in isolation using statistical and machine learning (ML) methods. Integrative analysis of multi-omics and clinical data is key to new biomedical discoveries and advancements in precision medicine. However, data integration poses new computational challenges as well as exacerbates the ones associated with single-omics studies. Specialized computational approaches are required to effectively and efficiently perform integrative analysis of biomedical data acquired from diverse modalities. In this review, we discuss state-of-the-art ML-based approaches for tackling five specific computational challenges associated with integrative analysis: curse of dimensionality, data heterogeneity, missing data, class imbalance and scalability issues.",2019-01-01,29,1010,64,858
2509,30617331,Privacy in the age of medical big data,"Big data has become the ubiquitous watch word of medical innovation. The rapid development of machine-learning techniques and artificial intelligence in particular has promised to revolutionize medical practice from the allocation of resources to the diagnosis of complex diseases. But with big data comes big risks and challenges, among them significant questions about patient privacy. Here, we outline the legal and ethical challenges big data brings to patient privacy. We discuss, among other topics, how best to conceive of health privacy; the importance of equity, consent, and patient governance in data collection; discrimination in data uses; and how to handle data breaches. We close by sketching possible ways forward for the regulatory system.",2019-01-01,41,756,38,858
446,30332290,Peering Into the Black Box of Artificial Intelligence: Evaluation Metrics of Machine Learning Methods,"Objective:                    Machine learning (ML) and artificial intelligence (AI) are rapidly becoming the most talked about and controversial topics in radiology and medicine. Over the past few years, the numbers of ML- or AI-focused studies in the literature have increased almost exponentially, and ML has become a hot topic at academic and industry conferences. However, despite the increased awareness of ML as a tool, many medical professionals have a poor understanding of how ML works and how to critically appraise studies and tools that are presented to us. Thus, we present a brief overview of ML, explain the metrics used in ML and how to interpret them, and explain some of the technical jargon associated with the field so that readers with a medical background and basic knowledge of statistics can feel more comfortable when examining ML applications.              Conclusion:                    Attention to sample size, overfitting, underfitting, cross validation, as well as a broad knowledge of the metrics of machine learning, can help those with little or no technical knowledge begin to assess machine learning studies. However, transparency in methods and sharing of algorithms is vital to allow clinicians to assess these tools themselves.",2019-01-01,19,1267,101,858
2582,30468663,State-of-the-art review on deep learning in medical imaging,"Deep learning (DL) is affecting each and every sphere of public and private lives and becoming a tool for daily use. The power of DL lies in the fact that it tries to imitate the activities of neurons in the neocortex of human brain where the thought process takes place. Therefore, like the brain, it tries to learn and recognize patterns in the form of digital images. This power is built on the depth of many layers of computing neurons backed by high power processors and graphics processing units (GPUs) easily available today. In the current scenario, we have provided detailed survey of various types of DL systems available today, and specifically, we have concentrated our efforts on current applications of DL in medical imaging. We have also focused our efforts on explaining the readers the rapid transition of technology from machine learning to DL and have tried our best in reasoning this paradigm shift. Further, a detailed analysis of complexities involved in this shift and possible benefits accrued by the users and developers.",2019-01-01,17,1046,59,858
2484,30669406,A Review on a Deep Learning Perspective in Brain Cancer Classification,"A World Health Organization (WHO) Feb 2018 report has recently shown that mortality rate due to brain or central nervous system (CNS) cancer is the highest in the Asian continent. It is of critical importance that cancer be detected earlier so that many of these lives can be saved. Cancer grading is an important aspect for targeted therapy. As cancer diagnosis is highly invasive, time consuming and expensive, there is an immediate requirement to develop a non-invasive, cost-effective and efficient tools for brain cancer characterization and grade estimation. Brain scans using magnetic resonance imaging (MRI), computed tomography (CT), as well as other imaging modalities, are fast and safer methods for tumor detection. In this paper, we tried to summarize the pathophysiology of brain cancer, imaging modalities of brain cancer and automatic computer assisted methods for brain cancer characterization in a machine and deep learning paradigm. Another objective of this paper is to find the current issues in existing engineering methods and also project a future paradigm. Further, we have highlighted the relationship between brain cancer and other brain disorders like stroke, Alzheimer's, Parkinson's, and Wilson's disease, leukoriaosis, and other neurological disorders in the context of machine learning and the deep learning paradigm.",2019-01-01,17,1349,70,858
2586,30455357,Computational approaches for the analysis of RNA-protein interactions: A primer for biologists,"RNA-binding proteins (RBPs) play important roles in the control of gene expression and the coordination of different layers of post-transcriptional regulation. Interactions between certain RBPs and mRNA transcripts are notoriously difficult to predict, as any given protein-RNA interaction may rely not only on RNA sequence, but also on three-dimensional RNA structures, competitive inhibition from other RBPs, and input from cellular signaling pathways. Advanced and high-throughput technologies for the identification of RNA-protein interactions have come to the rescue, but the identification of binding sites and downstream functional effects of RBPs from the resulting data can be challenging. In this review, we discuss statistical inference and machine-learning approaches and tools relevant for the study of RBPs and the analysis of large-scale RNA-protein interaction datasets. This primer is intended for life scientists who are interested in incorporating these tools into their own research. We begin with the demystification of regression models, as used in the analysis of next-generation sequencing data, and progress to a discussion of Hidden Markov Models, which are of particular value in analyzing cross-linking followed by immunoprecipitation data. We then continue with examples of machine learning techniques, such as support vector machines and gradient tree boosting. We close with a brief discussion of current trends in the field, including deep learning architectures.",2019-01-01,2,1495,94,858
2591,30442428,Rethinking multiscale cardiac electrophysiology with machine learning and predictive modelling,"We review some of the latest approaches to analysing cardiac electrophysiology data using machine learning and predictive modelling. Cardiac arrhythmias, particularly atrial fibrillation, are a major global healthcare challenge. Treatment is often through catheter ablation, which involves the targeted localised destruction of regions of the myocardium responsible for initiating or perpetuating the arrhythmia. Ablation targets are either anatomically defined, or identified based on their functional properties as determined through the analysis of contact intracardiac electrograms acquired with increasing spatial density by modern electroanatomic mapping systems. While numerous quantitative approaches have been investigated over the past decades for identifying these critical curative sites, few have provided a reliable and reproducible advance in success rates. Machine learning techniques, including recent deep-learning approaches, offer a potential route to gaining new insight from this wealth of highly complex spatio-temporal information that existing methods struggle to analyse. Coupled with predictive modelling, these techniques offer exciting opportunities to advance the field and produce more accurate diagnoses and robust personalised treatment. We outline some of these methods and illustrate their use in making predictions from the contact electrogram and augmenting predictive modelling tools, both by more rapidly predicting future states of the system and by inferring the parameters of these models from experimental observations.",2019-01-01,13,1562,94,858
445,30332296,State of the Art: Machine Learning Applications in Glioma Imaging,"Objective:                    Machine learning has recently gained considerable attention because of promising results for a wide range of radiology applications. Here we review recent work using machine learning in brain tumor imaging, specifically segmentation and MRI radiomics of gliomas.              Conclusion:                    We discuss available resources, state-of-the-art segmentation methods, and machine learning radiomics for glioma. We highlight the challenges of these techniques as well as the future potential in clinical diagnostics, prognostics, and decision making.",2019-01-01,15,589,65,858
2551,30522862,A Primer on Data Analytics in Functional Genomics: How to Move from Data to Insight?,"High-throughput methodologies and machine learning have been central in developing systems-level perspectives in molecular biology. Unfortunately, performing such integrative analyses has traditionally been reserved for bioinformaticians. This is now changing with the appearance of resources to help bench-side biologists become skilled at computational data analysis and handling large omics data sets. Here, we show an entry route into the field of omics data analytics. We provide information about easily accessible data sources and suggest some first steps for aspiring computational data analysts. Moreover, we highlight how machine learning is transforming the field and how it can help make sense of biological data. Finally, we suggest good starting points for self-learning and hope to convince readers that computational data analysis and programming are not intimidating.",2019-01-01,2,884,84,858
2469,30696115,Large-Scale Assessment of Bioinformatics Tools for Lysine Succinylation Sites,"Lysine succinylation is a form of posttranslational modification of the proteins that play an essential functional role in every aspect of cell metabolism in both prokaryotes and eukaryotes. Aside from experimental identification of succinylation sites, there has been an intense effort geared towards the development of sequence-based prediction through machine learning, due to its promising and essential properties of being highly accurate, robust and cost-effective. In spite of these advantages, there are several problems that are in need of attention in the design and development of succinylation site predictors. Notwithstanding of many studies on the employment of machine learning approaches, few articles have examined this bioinformatics field in a systematic manner. Thus, we review the advancements regarding the current state-of-the-art prediction models, datasets, and online resources and illustrate the challenges and limitations to present a useful guideline for developing powerful succinylation site prediction tools.",2019-01-01,13,1040,77,858
2476,30684706,"Machine Learning in Neuro-Oncology: Can Data Analysis from 5,346 Patients Change Decision Making Paradigms?","Background:                    Machine learning (ML) is an application of artificial intelligence (AI) giving computer systems the ability to learn data, without being explicitly programmed. ML is currently successfully used for optical character recognition, spam filtering, and face recognition. The aim of this study is to review its current application in the field of neuro-oncology.              Methods:                    We conducted a systematic literature review on PubMed and Cochrane Database using a keyword search for the period January 30, 2000-March 31, 2018. Data were clustered for neuro-oncology scope of ML into three categories: patient outcome predictors, imaging analysis, and gene expression.              Results:                    Data from 5,346 patients in 29 studies has been used to develop ML based algorithms (MLBA) in neuro-oncology. MLBA were used to predict outcome in 2,483 patients with a sensitivity range of 78-98% and specificity range of 76-95%. In all studies, MLBA had higher accuracy than conventional ones. MLBA for image analysis showed accuracy diagnosing low grade versus high grade gliomas (HGG) ranging from 80 to 93% and 90% diagnosing HGG versus lymphoma. Seven studies used MLBA to analyze gene expression in neuro-oncology.              Conclusions:                    MLBA in neuro-oncology have shown to predict patients' outcome more accurately than conventional parameters in retrospective analysis. If their high diagnostic accuracy in imaging analysis and detection of somatic mutations is corroborated in prospective studies, tissue diagnosis or liquid biopsy might curtail. Finally, MLBA are promising to help guide targeted therapy, lead to personalized medicine, and open areas of study in the cancer cellular signaling system, not otherwise known.",2019-01-01,6,1814,107,858
2459,30723470,"The Immune System Computes the State of the Body: Crowd Wisdom, Machine Learning, and Immune Cell Reference Repertoires Help Manage Inflammation","Here, we outline an overview of the mammalian immune system that updates and extends the classical clonal selection paradigm. Rather than focusing on strict self-not-self discrimination, we propose that the system orchestrates variable inflammatory responses that maintain the body and its symbiosis with the microbiome while eliminating the threat from pathogenic infectious agents and from tumors. The paper makes four points: The immune system classifies healthy and pathologic states of the body-including both self and foreign elements-by deploying individual lymphocytes as cellular computing machines; immune cells transform input signals from the body into an output of specific immune reactions.Rather than independent clonal responses, groups of individually activated immune-system cells co-react in lymphoid organs to make collective decisions through a type of self-organizing swarm intelligence or crowd wisdom.Collective choices by swarms of immune cells, like those of schools of fish, are modified by relatively small numbers of individual regulators responding to shifting conditions-such collective inflammatory responses are dynamically responsive.Self-reactive autoantibody and T-cell receptor (TCR) repertoires shared by healthy individuals function in a biological version of experience-based supervised machine learning. Immune system decisions are primed by formative experience with training sets of self-antigens encountered during lymphocyte development; these initially trained T cell and B cell repertoires form a Wellness Profile that then guides immune responses to test sets of antigens encountered later. This experience-based machine learning strategy is analogous to that deployed by supervised machine-learning algorithms. We propose experiments to test these ideas. This overview of the immune system bears clinical implications for monitoring wellness and for treating autoimmune disease, cancer, and allograft reactions.",2019-01-01,22,1960,144,858
2550,30523334,Machine-learning-based patient-specific prediction models for knee osteoarthritis,"Osteoarthritis (OA) is an extremely common musculoskeletal disease. However, current guidelines are not well suited for diagnosing patients in the early stages of disease and do not discriminate patients for whom the disease might progress rapidly. The most important hurdle in OA management is identifying and classifying patients who will benefit most from treatment. Further efforts are needed in patient subgrouping and developing prediction models. Conventional statistical modelling approaches exist; however, these models are limited in the amount of information they can adequately process. Comprehensive patient-specific prediction models need to be developed. Approaches such as data mining and machine learning should aid in the development of such models. Although a challenging task, technology is now available that should enable subgrouping of patients with OA and lead to improved clinical decision-making and precision medicine.",2019-01-01,20,945,81,858
2495,30645625,The PLOS ONE collection on machine learning in health and biomedicine: Towards open code and open data,"Recent years have seen a surge of studies in machine learning in health and biomedicine, driven by digitalization of healthcare environments and increasingly accessible computer systems for conducting analyses. Many of us believe that these developments will lead to significant improvements in patient care. Like many academic disciplines, however, progress is hampered by lack of code and data sharing. In bringing together this PLOS ONE collection on machine learning in health and biomedicine, we sought to focus on the importance of reproducibility, making it a requirement, as far as possible, for authors to share data and code alongside their papers.",2019-01-01,3,658,102,858
2450,30745864,Probabilistic Encoding Models for Multivariate Neural Data,"A key problem in systems neuroscience is to characterize how populations of neurons encode information in their patterns of activity. An understanding of the encoding process is essential both for gaining insight into the origins of perception and for the development of brain-computer interfaces. However, this characterization is complicated by the highly variable nature of neural responses, and thus usually requires probabilistic methods for analysis. Drawing on techniques from statistical modeling and machine learning, we review recent methods for extracting important variables that quantitatively describe how sensory information is encoded in neural activity. In particular, we discuss methods for estimating receptive fields, modeling neural population dynamics, and inferring low dimensional latent structure from a population of neurons, in the context of both electrophysiology and calcium imaging data.",2019-01-01,2,918,58,858
102,30201975,Paving the way for precision medicine v2.0 in intensive care by profiling necroinflammation in biofluids,"Current clinical diagnosis is typically based on a combination of approaches including clinical examination of the patient, clinical experience, physiologic and/or genetic parameters, high-tech diagnostic medical imaging, and an extended list of laboratory values mostly determined in biofluids such as blood and urine. One could consider this as precision medicine v1.0. However, recent advances in technology and better understanding of molecular mechanisms underlying disease will allow us to better characterize patients in the future. These improvements will enable us to distinguish patients who have similar clinical presentations but different cellular and molecular responses. Treatments will be able to be chosen more ""precisely"", resulting in more appropriate therapy, precision medicine v2.0. In this review, we will reflect on the potential added value of recent advances in technology and a better molecular understanding of necrosis and inflammation for improving diagnosis and treatment of critically ill patients. We give a brief overview on the mutual interplay between necrosis and inflammation, which are two crucial detrimental factors in organ and/or systemic dysfunction. One of the challenges for the future will thus be the cellular and molecular profiling of necroinflammation in biofluids. The huge amount of data generated by profiling biomolecules and single cells through, for example, different omic-approaches is needed for data mining methods to allow patient-clustering and identify novel biomarkers. The real-time monitoring of biomarkers will allow continuous (re)evaluation of treatment strategies using machine learning models. Ultimately, we may be able to offer precision therapies specifically designed to target the molecular set-up of an individual patient, as has begun to be done in cancer therapeutics.",2019-01-01,5,1848,104,858
2570,30500991,Computational aspects underlying genome to phenome analysis in plants,"Recent advances in genomics technologies have greatly accelerated the progress in both fundamental plant science and applied breeding research. Concurrently, high-throughput plant phenotyping is becoming widely adopted in the plant community, promising to alleviate the phenotypic bottleneck. While these technological breakthroughs are significantly accelerating quantitative trait locus (QTL) and causal gene identification, challenges to enable even more sophisticated analyses remain. In particular, care needs to be taken to standardize, describe and conduct experiments robustly while relying on plant physiology expertise. In this article, we review the state of the art regarding genome assembly and the future potential of pangenomics in plant research. We also describe the necessity of standardizing and describing phenotypic studies using the Minimum Information About a Plant Phenotyping Experiment (MIAPPE) standard to enable the reuse and integration of phenotypic data. In addition, we show how deep phenotypic data might yield novel trait-trait correlations and review how to link phenotypic data to genomic data. Finally, we provide perspectives on the golden future of machine learning and their potential in linking phenotypes to genomic features.",2019-01-01,13,1267,69,858
2458,30728787,Quantitative Electroencephalography in Guiding Treatment of Major Depression,"This paper reviews significant contributions to the evidence for the use of quantitative electroencephalography features as biomarkers of depression treatment and examines the potential of such technology to guide pharmacotherapy. Frequency band abnormalities such as alpha and theta band abnormalities have shown promise as have combinatorial measures such as cordance (a measure combining alpha and theta power) and the Antidepressant Treatment Response Index in predicting medication treatment response. Nevertheless, studies have been hampered by methodological problems and inconsistencies, and these approaches have ultimately failed to elicit any significant interest in actual clinical practice. More recent machine learning approaches such as the Psychiatric Encephalography Evaluation Registry (PEER) technology and other efforts analyze large datasets to develop variables that may best predict response rather than test a priori hypotheses. PEER is a technology that may go beyond predicting response to a particular antidepressant and help to guide pharmacotherapy.",2019-01-01,3,1078,76,858
2477,30684090,Rheumatoid Arthritis: Atherosclerosis Imaging and Cardiovascular Risk Assessment Using Machine and Deep Learning-Based Tissue Characterization,"Purpose of the review:                    Rheumatoid arthritis (RA) is a chronic, autoimmune disease which may result in a higher risk of cardiovascular (CV) events and stroke. Tissue characterization and risk stratification of patients with rheumatoid arthritis are a challenging problem. Risk stratification of RA patients using traditional risk factor-based calculators either underestimates or overestimates the CV risk. Advancements in medical imaging have facilitated early and accurate CV risk stratification compared to conventional cardiovascular risk calculators.              Recent finding:                    In recent years, a link between carotid atherosclerosis and rheumatoid arthritis has been widely discussed by multiple studies. Imaging the carotid artery using 2-D ultrasound is a noninvasive, economic, and efficient imaging approach that provides an atherosclerotic plaque tissue-specific image. Such images can help to morphologically characterize the plaque type and accurately measure vital phenotypes such as media wall thickness and wall variability. Intelligence-based paradigms such as machine learning- and deep learning-based techniques not only automate the risk characterization process but also provide an accurate CV risk stratification for better management of RA patients. This review provides a brief understanding of the pathogenesis of RA and its association with carotid atherosclerosis imaged using the B-mode ultrasound technique. Lacunas in traditional risk scores and the role of machine learning-based tissue characterization algorithms are discussed and could facilitate cardiovascular risk assessment in RA patients. The key takeaway points from this review are the following: (i) inflammation is a common link between RA and atherosclerotic plaque buildup, (ii) carotid ultrasound is a better choice to characterize the atherosclerotic plaque tissues in RA patients, and (iii) intelligence-based paradigms are useful for accurate tissue characterization and risk stratification of RA patients.",2019-01-01,12,2044,142,858
2578,30478442,A primer on deep learning in genomics,"Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial.",2019-01-01,95,530,37,858
2573,30498877,Machine learning studies on major brain diseases: 5-year trends of 2014-2018,"In the recent 5 years (2014-2018), there has been growing interest in the use of machine learning (ML) techniques to explore image diagnosis and prognosis of therapeutic lesion changes within the area of neuroradiology. However, to date, the majority of research trend and current status have not been clearly illuminated in the neuroradiology field. More than 1000 papers have been published during the past 5 years on subject classification and prediction focused on multiple brain disorders. We provide a survey of 209 papers in this field with a focus on top ten active areas of research; i.e., Alzheimer's disease/mild cognitive impairment, brain tumor; schizophrenia, depressive disorders, Parkinson's disease, attention-deficit hyperactivity disorder, autism spectrum disease, epilepsy, multiple sclerosis, stroke, and traumatic brain injury. Detailed information of these studies, such as ML methods, sample size, type of inputted features and reported accuracy, are summarized. This paper reviews the evidences, current limitations and status of studies using ML to assess brain disorders in neuroimaging data. The main bottleneck of this research field is still the limited sample size, which could be potentially addressed by modern data sharing models, such as ADNI.",2019-01-01,25,1278,76,858
2538,30555113,On medical application of neural networks trained with various types of data,"Neural networks have garnered attention over the past few years. A neural network is a typical model of machine learning that is used to identify visual patterns. Neural networks are used to solve a wide variety of problems, including image recognition problems and time series prediction problems. In addition, neural networks have been applied to medicine over the past few years. This paper classifies the ways in which neural networks have been applied to medicine based on the type of data used to train those networks. Applications of neural networks to medicine can be categorized two types: automated diagnosis and physician aids. Considering the number of patients per physician, neural networks could be used to diagnose diseases related to the vascular system, heart, brain, spinal column, head, neck, and tumors/cancer in three fields: vascular and interventional radiology, interventional cardiology, and neuroradiology. Lastly, this paper also considers areas of medicine where neural networks can be effectively applied in the future.",2019-01-01,1,1049,76,858
2504,30621954,New Concepts in Sudden Cardiac Arrest to Address an Intractable Epidemic: JACC State-of-the-Art Review,"Sudden cardiac arrest (SCA) is one of the largest causes of mortality globally, with an out-of-hospital survival below 10% despite intense research. This document outlines challenges in addressing the epidemic of SCA, along the framework of respond, understand and predict, and prevent. Response could be improved by technology-assisted orchestration of community responder systems, access to automated external defibrillators, and innovations to match resuscitation resources to victims in place and time. Efforts to understand and predict SCA may be enhanced by refining taxonomy along phenotypical and pathophysiological ""axes of risk,"" extending beyond cardiovascular pathology to identify less heterogeneous cohorts, facilitated by open-data platforms and analytics including machine learning to integrate discoveries across disciplines. Prevention of SCA must integrate these concepts, recognizing that all members of society are stakeholders. Ultimately, solutions to the public health challenge of SCA will require greater awareness, societal debate and focused public policy.",2019-01-01,5,1084,102,858
2576,30488731,Advances with support vector machines for novel drug discovery,"Novel drug discovery remains an enormous challenge, with various computer-aided drug design (CADD) approaches having been widely employed for this purpose. CADD, specifically the commonly used support vector machines (SVMs), can employ machine learning techniques. SVMs and their variations offer numerous drug discovery applications, which range from the classification of substances (as active or inactive) to the construction of regression models and the ranking/virtual screening of databased compounds. Areas covered: Herein, the authors consider some of the applications of SVMs in medicinal chemistry, illustrating their main advantages and disadvantages, as well as trends in their utilization, via the available published literature. The aim of this review is to provide an up-to-date review of the recent applications of SVMs in drug discovery as described by the literature, thereby highlighting their strengths, weaknesses, and future challenges. Expert opinion: Techniques based on SVMs are considered as powerful approaches in early drug discovery. The ability of SVMs to classify active or inactive compounds has enabled the prioritization of substances for virtual screening. Indeed, one of the main advantages of SVMs is related to their potential in the analysis of nonlinear problems. However, despite successes in employing SVMs, the challenges of improving accuracy remain.",2019-01-01,4,1394,62,858
2266,31099675,"The Impact of Big Data Research on Practice, Policy, and Cancer Care","The concept of ""big data"" research-the aggregation and analysis of biologic, clinical, administrative, and other data sources to drive new advances in biomedical knowledge-has been embraced by the cancer research enterprise. Although much of the conversation has concentrated on the amalgamation of basic biologic data (e.g., genomics, metabolomics, tumor tissue), new opportunities to extend potential contributions of big data to clinical practice and policy abound. This article examines these opportunities through discussion of three major data sources: aggregated clinical trial data, administrative data (including insurance claims data), and data from electronic health records. We will discuss the benefits of data use to answer key oncology practice and policy research questions, along with limitations inherent in these complex data sources. Finally, the article will discuss overarching themes across data types and offer next steps for the research, practice, and policy communities. The use of multiple sources of big data has the promise of improving knowledge and providing more accurate data for clinicians and policy decision makers. In the future, optimization of machine learning may allow for current limitations of big data analyses to be attenuated, thereby resulting in improved patient care and outcomes.",2019-01-01,4,1330,68,858
439,30342246,Artificial intelligence and its potential in oncology,"The two main branches associated with Artificial Intelligence (AI) in medicine are virtual and physical. The virtual component includes machine learning (ML) and algorithms, whereas physical AI includes medical devices and robots for delivering care. AI is used successfully in tumour segmentation, histopathological diagnosis, tracking tumour development, and prognosis prediction. CURATE.AI, developed at the National University of Singapore, is a platform that automatically decides the optimum dose of drugs for a durable response, allowing the patient to resume a completely normal life. With the involvement of technology multinationals, such as Google and Microsoft, in AI and healthcare in association with leading healthcare companies, the future of AI in healthcare looks very promising.",2019-01-01,6,797,53,858
2485,30667309,New Frontiers: An Update on Computer-Aided Diagnosis for Breast Imaging in the Age of Artificial Intelligence,"Objective:                    The purpose of this article is to compare traditional versus machine learning-based computer-aided detection (CAD) platforms in breast imaging with a focus on mammography, to underscore limitations of traditional CAD, and to highlight potential solutions in new CAD systems under development for the future.              Conclusion:                    CAD development for breast imaging is undergoing a paradigm shift based on vast improvement of computing power and rapid emergence of advanced deep learning algorithms, heralding new systems that may hold real potential to improve clinical care.",2019-02-01,8,627,109,827
2428,30815458,Progress in the Development and Challenges for the Use of Artificial Kidneys and Wearable Dialysis Devices,"Background:                    Renal transplantation is the treatment of choice for chronic kidney disease (CKD) patients, but the shortage of kidneys and the disabling medical conditions these patients suffer from make dialysis essential for most of them. Since dialysis drastically affects the patients' lifestyle, there are great expectations for the development of wearable artificial kidneys, although their use is currently impeded by major concerns about safety. On the other hand, dialysis patients with hemodynamic instability do not usually tolerate intermittent dialysis therapy because of their inability to adapt to a changing scenario of unforeseen events. Thus, the development of novel wearable dialysis devices and the improvement of clinical tolerance will need contributions from new branches of engineering such as artificial intelligence (AI) and machine learning (ML) for the real-time analysis of equipment alarms, dialysis parameters, and patient-related data with a real-time feedback response. These technologies are endowed with abilities normally associated with human intelligence such as learning, problem solving, human speech understanding, or planning and decision-making. Examples of common applications of AI are visual perception (computer vision), speech recognition, and language translation. In this review, we discuss recent progresses in the area of dialysis and challenges for the use of AI in the development of artificial kidneys.              Summary and key messages:                    Emerging technologies derived from AI, ML, electronics, and robotics will offer great opportunities for dialysis therapy, but much innovation is needed before we achieve a smart dialysis machine able to analyze and understand changes in patient homeostasis and to respond appropriately in real time. Great efforts are being made in the fields of tissue engineering and regenerative medicine to provide alternative cell-based approaches for the treatment of renal failure, including bioartificial renal systems and the implantation of bioengineered kidney constructs.",2019-02-01,5,2099,106,827
454,30315314,A machine learning approach relating 3D body scans to body composition in humans,"A long-standing question in nutrition and obesity research involves quantifying the relationship between body fat and anthropometry. To date, the mathematical formulation of these relationships has relied on pairing easily obtained anthropometric measurements such as the body mass index (BMI), waist circumference, or hip circumference to body fat. Recent advances in 3D body shape imaging technology provides a new opportunity for quickly and accurately obtaining hundreds of anthropometric measurements within seconds, however, there does not yet exist a large diverse database that pairs these measurements to body fat. Herein, we leverage 3D scanned anthropometry obtained from a population of United States Army basic training recruits to derive four subpopulations of homogenous body shape archetypes using a combined principal components and cluster analysis. While the Army database was large and diverse, it did not have body composition measurements. Therefore, these body shape archetypes were paired to an alternate smaller sample of participants from the Pennington Biomedical Research Center in Baton Rouge, LA that were not only similarly imaged by the same 3D scanning machine, but also had concomitant measures of body composition by dual-energy X-ray absorptiometry body composition. With this enhanced ability to obtain anthropometry through 3D scanning quickly of large populations, our machine learning approach for pairing body shapes from large datasets to smaller datasets that also contain state-of-the-art body composition measurements can be extended to pair other health outcomes to 3D body shape anthropometry.",2019-02-01,5,1640,80,827
2903,29956866,"Utilizing state-of-the-art ""omics"" technology and bioinformatics to identify new biological mechanisms and biomarkers for coronary artery disease","Identification of the four standard modifiable cardiovascular risk factors (SMuRFs)-diabetes mellitus, hyperlipidaemia, hypertension, and cigarette smoking-has allowed the development of risk scores. These have been used in conjunction with primary and secondary prevention strategies targeting SMuRFs to reduce the burden of CAD. Recent studies show that up to 25% of ACS patients do not have any SMuRFs. Thus, SMuRFs do not explain the entire burden of CAD. There appears to be variation at the individual level rendering some individuals relatively susceptible or resilient to developing atherosclerosis. Important disease pathways remain to be discovered, and there is renewed enthusiasm to discover novel biomarkers, biological mechanisms, and therapeutic targets for atherosclerosis. Two broad approaches are being taken: traditional approaches investigating known candidate pathways and unbiased omics approaches. We review recent progress in the field and discuss opportunities made possible by technological and data science advances. Developments in network analytics and machine learning algorithms used in conjunction with large-scale multi-omic platforms have the potential to uncover biological networks that may not have been identifiable using traditional approaches. These approaches are useful for both biomedical research and precision medicine strategies.",2019-02-01,5,1375,145,827
2402,30858931,Artificial intelligence in breast ultrasound,"Artificial intelligence (AI) is gaining extensive attention for its excellent performance in image-recognition tasks and increasingly applied in breast ultrasound. AI can conduct a quantitative assessment by recognizing imaging information automatically and make more accurate and reproductive imaging diagnosis. Breast cancer is the most commonly diagnosed cancer in women, severely threatening women's health, the early screening of which is closely related to the prognosis of patients. Therefore, utilization of AI in breast cancer screening and detection is of great significance, which can not only save time for radiologists, but also make up for experience and skill deficiency on some beginners. This article illustrates the basic technical knowledge regarding AI in breast ultrasound, including early machine learning algorithms and deep learning algorithms, and their application in the differential diagnosis of benign and malignant masses. At last, we talk about the future perspectives of AI in breast ultrasound.",2019-02-01,8,1027,44,827
2409,30837982,Comparison of Open-Source Reverse Vaccinology Programs for Bacterial Vaccine Antigen Discovery,"Reverse Vaccinology (RV) is a widely used approach to identify potential vaccine candidates (PVCs) by screening the proteome of a pathogen through computational analyses. Since its first application in Group B meningococcus (MenB) vaccine in early 1990's, several software programs have been developed implementing different flavors of the first RV protocol. However, there has been no comprehensive review to date on these different RV tools. We have compared six of these applications designed for bacterial vaccines (NERVE, Vaxign, VaxiJen, Jenner-predict, Bowman-Heinson, and VacSol) against a set of 11 pathogens for which a curated list of known bacterial protective antigens (BPAs) was available. We present results on: (1) the comparison of criteria and programs used for the selection of PVCs (2) computational runtime and (3) performances in terms of fraction of proteome identified as PVC, fraction and enrichment of BPA identified in the set of PVCs. This review demonstrates that none of the programs was able to recall 100% of the tested set of BPAs and that the output lists of proteins are in poor agreement suggesting in the process of prioritize vaccine candidates not to rely on a single RV tool response. Singularly the best balance in terms of fraction of a proteome predicted as good candidate and recall of BPAs has been observed by the machine-learning approach proposed by Bowman (1) and enhanced by Heinson (2). Even though more performing than the other approaches it shows the disadvantage of limited accessibility to non-experts users and strong dependence between results and a-priori training dataset composition. In conclusion we believe that to significantly enhance the performances of next RV methods further studies should focus on the enhancement of accuracy of the existing protein annotation tools and should leverage on the assets of machine-learning techniques applied to biological datasets expanded also through the incorporation and curation of bacterial proteins characterized by negative experimental results.",2019-02-01,13,2055,94,827
409,30397993,"Legal, regulatory, and ethical frameworks for development of standards in artificial intelligence (AI) and autonomous robotic surgery","Background:                    This paper aims to move the debate forward regarding the potential for artificial intelligence (AI) and autonomous robotic surgery with a particular focus on ethics, regulation and legal aspects (such as civil law, international law, tort law, liability, medical malpractice, privacy and product/device legislation, among other aspects).              Methods:                    We conducted an intensive literature search on current or emerging AI and autonomous technologies (eg, vehicles), military and medical technologies (eg, surgical robots), relevant frameworks and standards, cyber security/safety- and legal-systems worldwide. We provide a discussion on unique challenges for robotic surgery faced by proposals made for AI more generally (eg, Explainable AI) and machine learning more specifically (eg, black box), as well as recommendations for developing and improving relevant frameworks or standards.              Conclusion:                    We classify responsibility into the following: (1) Accountability; (2) Liability; and (3) Culpability. All three aspects were addressed when discussing responsibility for AI and autonomous surgical robots, be these civil or military patients (however, these aspects may require revision in cases where robots become citizens). The component which produces the least clarity is Culpability, since it is unthinkable in the current state of technology. We envision that in the near future a surgical robot can learn and perform routine operative tasks that can then be supervised by a human surgeon. This represents a surgical parallel to autonomously driven vehicles. Here a human remains in the 'driving seat' as a 'doctor-in-the-loop' thereby safeguarding patients undergoing operations that are supported by surgical machines with autonomous capabilities.",2019-02-01,12,1846,133,827
2417,30828380,Current trends in biomarker discovery and analysis tools for traumatic brain injury,"Traumatic brain injury (TBI) affects 1.7 million people in the United States each year, causing lifelong functional deficits in cognition and behavior. The complex pathophysiology of neural injury is a primary barrier to developing sensitive and specific diagnostic tools, which consequentially has a detrimental effect on treatment regimens. Biomarkers of other diseases (e.g. cancer) have provided critical insight into disease emergence and progression that lend to developing powerful clinical tools for intervention. Therefore, the biomarker discovery field has recently focused on TBI and made substantial advancements to characterize markers with promise of transforming TBI patient diagnostics and care. This review focuses on these key advances in neural injury biomarkers discovery, including novel approaches spanning from omics-based approaches to imaging and machine learning as well as the evolution of established techniques.",2019-02-01,7,940,83,827
2545,30545729,"Esophageal atresia, Europe, and the future: BAPS Journal of Pediatric Surgery Lecture","Europe has changed remarkably over the past decades and so have concepts and outcomes of esophageal atresia repair. In this article, both the efforts to create a united Europe and the achievements in dealing with esophageal atresia from the 1950s on are outlined. Furthermore, this paper deals with the future of pediatric surgery and is focused on two aspects: the ""Fourth Industrial Revolution"" which builds on the digital revolution, artificial intelligence and robotics, and its potential impact on pediatric surgery and the life of patients. I suggest that pediatric surgeons should participate and lead in the development of machine learning, data control, assuring appropriate use of machines, control misuse, and in particular ensure appropriate maintenance of ethical standards. Changes in health care structures within Europe, in particular the effect of centralization, will affect the concept of treatment for patients with rare diseases.",2019-02-01,1,950,85,827
479,30265280,Deep learning in omics: a survey and guideline,"Omics, such as genomics, transcriptome and proteomics, has been affected by the era of big data. A huge amount of high dimensional and complex structured data has made it no longer applicable for conventional machine learning algorithms. Fortunately, deep learning technology can contribute toward resolving these challenges. There is evidence that deep learning can handle omics data well and resolve omics problems. This survey aims to provide an entry-level guideline for researchers, to understand and use deep learning in order to solve omics problems. We first introduce several deep learning models and then discuss several research areas which have combined omics and deep learning in recent years. In addition, we summarize the general steps involved in using deep learning which have not yet been systematically discussed in the existent literature on this topic. Finally, we compare the features and performance of current mainstream open source deep learning frameworks and present the opportunities and challenges involved in deep learning. This survey will be a good starting point and guideline for omics researchers to understand deep learning.",2019-02-01,22,1160,46,827
2416,30828395,Radiomics in Oncological PET/CT: a Methodological Overview,"Radiomics is a medical imaging analysis approach based on computer-vision. Metabolic radiomics in particular analyses the spatial distribution patterns of molecular metabolism on PET images. Measuring intratumoral heterogeneity via image is one of the main targets of radiomics research, and it aims to build a image-based model for better patient management. The workflow of radiomics using texture analysis follows these steps: 1) imaging (image acquisition and reconstruction); 2) preprocessing (segmentation & quantization); 3) quantification (texture matrix design & texture feature extraction); and 4) analysis (statistics and/or machine learning). The parameters or conditions at each of these steps are effect on the results. In statistical testing or modeling, problems such as multiple comparisons, dependence on other variables, and high dimensionality of small sample size data should be considered. Standardization of methodology and harmonization of image quality are one of the most important challenges with radiomics methodology. Even though there are current issues in radiomics methodology, it is expected that radiomics will be clinically useful in personalized medicine for oncology.",2019-02-01,8,1204,58,827
415,30381421,Genome-Based Prediction of Bacterial Antibiotic Resistance,"Clinical microbiology has long relied on growing bacteria in culture to determine antimicrobial susceptibility profiles, but the use of whole-genome sequencing for antibiotic susceptibility testing (WGS-AST) is now a powerful alternative. This review discusses the technologies that made this possible and presents results from recent studies to predict resistance based on genome sequences. We examine differences between calling antibiotic resistance profiles by the simple presence or absence of previously known genes and single-nucleotide polymorphisms (SNPs) against approaches that deploy machine learning and statistical models. Often, the limitations to genome-based prediction arise from limitations of accuracy of culture-based AST in addition to an incomplete knowledge of the genetic basis of resistance. However, we need to maintain phenotypic testing even as genome-based prediction becomes more widespread to ensure that the results do not diverge over time. We argue that standardization of WGS-AST by challenge with consistently phenotyped strain sets of defined genetic diversity is necessary to compare the efficacy of methods of prediction of antibiotic resistance based on genome sequences.",2019-02-01,41,1212,58,827
2535,30557052,Risk Stratification for Screening Mammography: Benefits and Harms,"Objective:                    The purpose of this article is to compare commonly used breast cancer risk assessment models, describe the machine learning approach and big data in risk prediction, and summarize the potential benefits and harms of restrictive risk-based screening.              Conclusion:                    The commonly used risk assessment models for breast cancer can be complex and cumbersome to use. Each model incorporates different sets of risk factors, which are weighted differently and can produce different results for the same patient. No model is appropriate for all subgroups of the general population and only one model incorporates mammographic breast density. Future development of risk prediction tools that are generalizable and simpler to use are needed in guiding clinical decisions.",2019-02-01,0,820,65,827
425,30370979,"Membrane Filtration with Liquids: A Global Approach with Prior Successes, New Developments and Unresolved Challenges","After 70 years, modern pressure-driven polymer membrane processes with liquids are mature and accepted in many industries due to their good performance, ease of scale-up, low energy consumption, modular compact construction, and low operating costs compared with thermal systems. Successful isothermal operation of synthetic membranes with liquids requires consideration of three critical aspects or ""legs"" in order of relevance: selectivity, capacity (i.e. permeation flow rate per unit area) and transport of mass and momentum comprising concentration polarization (CP) and fouling (F). Major challenges remain with respect to increasing selectivity and controlling mass transport in, to and away from membranes. Thus, prediction and control of membrane morphology and a deep understanding of the mechanism of dissolved and suspended solute transport near and in the membrane (i.e. diffusional and convective mass transport) is essential. Here, we focus on materials development to address the relatively poor selectivity of liquid membrane filtration with polymers and discuss the critical aspects of transport limitations. Machine learning could help optimize membrane structure design and transport conditions for improved membrane filtration performance.",2019-02-01,1,1260,116,827
2410,30837884,Making Sense of the Epigenome Using Data Integration Approaches,"Epigenetic research involves examining the mitotically heritable processes that regulate gene expression, independent of changes in the DNA sequence. Recent technical advances such as whole-genome bisulfite sequencing and affordable epigenomic array-based technologies, allow researchers to measure epigenetic profiles of large cohorts at a genome-wide level, generating comprehensive high-dimensional datasets that may contain important information for disease development and treatment opportunities. The epigenomic profile for a certain disease is often a result of the complex interplay between multiple genetic and environmental factors, which poses an enormous challenge to visualize and interpret these data. Furthermore, due to the dynamic nature of the epigenome, it is critical to determine causal relationships from the many correlated associations. In this review we provide an overview of recent data analysis approaches to integrate various omics layers to understand epigenetic mechanisms of complex diseases, such as obesity and cancer. We discuss the following topics: (i) advantages and limitations of major epigenetic profiling techniques, (ii) resources for standardization, annotation and harmonization of epigenetic data, and (iii) statistical methods and machine learning methods for establishing data-driven hypotheses of key regulatory mechanisms. Finally, we discuss the future directions for data integration that shall facilitate the discovery of epigenetic-based biomarkers and therapies.",2019-02-01,7,1517,63,827
2899,29970286,Non-invasive imaging techniques and assessment of carotid vasa vasorum neovascularization: Promises and pitfalls,"Carotid adventitia vasa vasorum neovascularization (VVn) is associated with the initial stages of arteriosclerosis and with the formation of unstable plaque. However, techniques to accurately quantify that neovascularization in a standard, fast, non-invasive, and efficient way are still lacking. The development of such techniques holds the promise of enabling wide, inexpensive, and safe screening programs that could stratify patients and help in personalized preventive cardiovascular medicine. In this paper, we review the recent scientific literature pertaining to imaging techniques that could set the stage for the development of standard methods for quantitative assessment of atherosclerotic plaque and carotid VVn. We present and discuss the alternative imaging techniques being used in clinical practice and we review the computational developments that are contributing to speed up image analysis and interpretation. We conclude that one of the greatest upcoming challenges will be the use of machine learning techniques to develop automated methods that assist in the interpretation of images to stratify patients according to their risk.",2019-02-01,0,1152,112,827
2529,30571146,Childhood Asthma: Advances Using Machine Learning and Mechanistic Studies,"A paradigm shift brought by the recognition that childhood asthma is an aggregated diagnosis that comprises several different endotypes underpinned by different pathophysiology, coupled with advances in understanding potentially important causal mechanisms, offers a real opportunity for a step change to reduce the burden of the disease on individual children, families, and society. Data-driven methodologies facilitate the discovery of ""hidden"" structures within ""big healthcare data"" to help generate new hypotheses. These findings can be translated into clinical practice by linking discovered ""phenotypes"" to specific mechanisms and clinical presentations. Epidemiological studies have provided important clues about mechanistic avenues that should be pursued to identify interventions to prevent the development or alter the natural history of asthma-related diseases. Findings from cohort studies followed by mechanistic studies in humans and in neonatal mouse models provided evidence that environments such as traditional farming may offer protection by modulating innate immune responses and that impaired innate immunity may increase susceptibility. The key question of which component of these exposures can be translated into interventions requires confirmation. Increasing mechanistic evidence is demonstrating that shaping the microbiome in early life may modulate immune function to confer protection. Iterative dialogue and continuous interaction between experts with different but complementary skill sets, including data scientists who generate information about the hidden structures within ""big data"" assets, and medical professionals, epidemiologists, basic scientists, and geneticists who provide critical clinical and mechanistic insights about the mechanisms underpinning the architecture of the heterogeneity, are keys to delivering mechanism-based stratified treatments and prevention.",2019-02-01,9,1913,73,827
2520,30591420,Connected orthopedics and trauma surgery: New perspectives,"Information is everywhere in the surgeon's life. It can improve medical practice and allow for personalized care. To answer the question, ""How should the surgeon be connected?"" we must assess the role and limitations of digital information in daily practice, particularly through mobile applications or mHealth. These tools and their scope must be defined in order to measure their impact on our clinical practice. New regulations on medical data have been introduced imposing that privacy be maintained. Connected applications can assist the surgeon in making the diagnosis and deciding on the treatment. These tools are already being used widely. Decision algorithms based on machine learning are also a promising way to optimize patient care. Connected applications make the clinical follow-up easier by allowing more reliable, relevant and frequent data transmission. They also provide access to information and training, either early academic learning or continuing medical education. We must adapt to these new modes of learning. Thus, smartphones, tablets and digital applications now have a central role in modern orthopedic surgery. Surgeons have information, technical resources and storage for research data at their disposal, while patients can establish a link with their doctor (current or future) and find lay information about their condition.",2019-02-01,2,1359,58,827
113,30182201,Machine learning: applications of artificial intelligence to imaging and diagnosis,"Machine learning (ML) is a form of artificial intelligence which is placed to transform the twenty-first century. Rapid, recent progress in its underlying architecture and algorithms and growth in the size of datasets have led to increasing computer competence across a range of fields. These include driving a vehicle, language translation, chatbots and beyond human performance at complex board games such as Go. Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimisation. We then summarise the applications of ML to medicine. In particular, we showcase recent diagnostic performances, and caveats, in the fields of dermatology, radiology, pathology and general microscopy.",2019-02-01,17,751,82,827
2482,30671672,"Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges and future perspectives","In the field of cancer genomics, the broad availability of genetic information offered by next-generation sequencing technologies and rapid growth in biomedical publication has led to the advent of the big-data era. Integration of artificial intelligence (AI) approaches such as machine learning, deep learning, and natural language processing (NLP) to tackle the challenges of scalability and high dimensionality of data and to transform big data into clinically actionable knowledge is expanding and becoming the foundation of precision medicine. In this paper, we review the current status and future directions of AI application in cancer genomics within the context of workflows to integrate genomic analysis for precision cancer care. The existing solutions of AI and their limitations in cancer genetic testing and diagnostics such as variant calling and interpretation are critically analyzed. Publicly available tools or algorithms for key NLP technologies in the literature mining for evidence-based clinical recommendations are reviewed and compared. In addition, the present paper highlights the challenges to AI adoption in digital healthcare with regard to data requirements, algorithmic transparency, reproducibility, and real-world assessment, and discusses the importance of preparing patients and physicians for modern digitized healthcare. We believe that AI will remain the main driver to healthcare transformation toward precision medicine, yet the unprecedented challenges posed should be addressed to ensure safety and beneficial impact to healthcare.",2019-02-01,12,1574,130,827
2436,30783371,Artificial intelligence in medical imaging of the liver,"Artificial intelligence (AI), particularly deep learning algorithms, is gaining extensive attention for its excellent performance in image-recognition tasks. They can automatically make a quantitative assessment of complex medical image characteristics and achieve an increased accuracy for diagnosis with higher efficiency. AI is widely used and getting increasingly popular in the medical imaging of the liver, including radiology, ultrasound, and nuclear medicine. AI can assist physicians to make more accurate and reproductive imaging diagnosis and also reduce the physicians' workload. This article illustrates basic technical knowledge about AI, including traditional machine learning and deep learning algorithms, especially convolutional neural networks, and their clinical application in the medical imaging of liver diseases, such as detecting and evaluating focal liver lesions, facilitating treatment, and predicting liver treatment response. We conclude that machine-assisted medical services will be a promising solution for future liver medical care. Lastly, we discuss the challenges and future directions of clinical application of deep learning techniques.",2019-02-01,5,1175,55,827
2549,30523919,A comprehensive review of EEG-based brain-computer interface paradigms,"Advances in brain science and computer technology in the past decade have led to exciting developments in brain-computer interface (BCI), thereby making BCI a top research area in applied science. The renaissance of BCI opens new methods of neurorehabilitation for physically disabled people (e.g. paralyzed patients and amputees) and patients with brain injuries (e.g. stroke patients). Recent technological advances such as wireless recording, machine learning analysis, and real-time temporal resolution have increased interest in electroencephalographic (EEG) based BCI approaches. Many BCI studies have focused on decoding EEG signals associated with whole-body kinematics/kinetics, motor imagery, and various senses. Thus, there is a need to understand the various experimental paradigms used in EEG-based BCI systems. Moreover, given that there are many available options, it is essential to choose the most appropriate BCI application to properly manipulate a neuroprosthetic or neurorehabilitation device. The current review evaluates EEG-based BCI paradigms regarding their advantages and disadvantages from a variety of perspectives. For each paradigm, various EEG decoding algorithms and classification methods are evaluated. The applications of these paradigms with targeted patients are summarized. Finally, potential problems with EEG-based BCI systems are discussed, and possible solutions are proposed.",2019-02-01,23,1419,70,827
2925,29885419,Secondary brain injury: Predicting and preventing insults,"Mortality or severe disability affects the majority of patients after severe traumatic brain injury (TBI). Adherence to the brain trauma foundation guidelines has overall improved outcomes; however, traditional as well as novel interventions towards intracranial hypertension and secondary brain injury have come under scrutiny after series of negative randomized controlled trials. In fact, it would not be unfair to say there has been no single major breakthrough in the management of severe TBI in the last two decades. One plausible hypothesis for the aforementioned failures is that by the time treatment is initiated for neuroprotection, or physiologic optimization, irreversible brain injury has already set in. We, and others, have recently developed predictive models based on machine learning from continuous time series of intracranial pressure and partial brain tissue oxygenation. These models provide accurate predictions of physiologic crises events in a timely fashion, offering the opportunity for an earlier application of targeted interventions. In this article, we review the rationale for prediction, discuss available predictive models with examples, and offer suggestions for their future prospective testing in conjunction with preventive clinical algorithms. This article is part of the Special Issue entitled ""Novel Treatments for Traumatic Brain Injury"".",2019-02-01,11,1381,57,827
2211,31198073,A review on computer-aided recent developments for automatic detection of diabetic retinopathy,"Diabetic retinopathy is a serious microvascular disorder that might result in loss of vision and blindness. It seriously damages the retinal blood vessels and reduces the light-sensitive inner layer of the eye. Due to the manual inspection of retinal fundus images on diabetic retinopathy to detect the morphological abnormalities in Microaneurysms (MAs), Exudates (EXs), Haemorrhages (HMs), and Inter retinal microvascular abnormalities (IRMA) is very difficult and time consuming process. In order to avoid this, the regular follow-up screening process, and early automatic Diabetic Retinopathy detection are necessary. This paper discusses various methods of analysing automatic retinopathy detection and classification of different grading based on the severity levels. In addition, retinal blood vessel detection techniques are also discussed for the ultimate detection and diagnostic procedure of proliferative diabetic retinopathy. Furthermore, the paper elaborately discussed the systematic review accessed by authors on various publicly available databases collected from different medical sources. In the survey, meta-analysis of several methods for diabetic feature extraction, segmentation and various types of classifiers have been used to evaluate the system performance metrics for the diagnosis of DR. This survey will be helpful for the technical persons and researchers who want to focus on enhancing the diagnosis of a system that would be more powerful in real life.",2019-02-01,0,1486,94,827
2472,30689359,Meta-Analysis of Nanoparticle Cytotoxicity via Data-Mining the Literature,"Developing predictive modeling frameworks of potential cytotoxicity of engineered nanoparticles is critical for environmental and health risk analysis. The complexity and the heterogeneity of available data on potential risks of nanoparticles, in addition to interdependency of relevant influential attributes, makes it challenging to develop a generalization of nanoparticle toxicity behavior. Lack of systematic approaches to investigate these risks further adds uncertainties and variability to the body of literature and limits generalizability of existing studies. Here, we developed a rigorous approach for assembling published evidence on cytotoxicity of several organic and inorganic nanoparticles and unraveled hidden relationships that were not targeted in the original publications. We used a machine learning approach that employs decision trees together with feature selection algorithms ( e.g., Gain ratio) to analyze a set of published nanoparticle cytotoxicity sample data (2896 samples). The specific studies were selected because they specified nanoparticle-, cell-, and screening method-related attributes. The resultant decision-tree classifiers are sufficiently simple, accurate, and with high prediction power and should be widely applicable to a spectrum of nanoparticle cytotoxicity settings. Among several influential attributes, we show that the cytotoxicity of nanoparticles is primarily predicted from the nanoparticle material chemistry, followed by nanoparticle concentration and size, cell type, and cytotoxicity screening indicator. Overall, our study indicates that following rigorous and transparent methodological experimental approaches, in parallel to continuous addition to this data set developed using our approach, will offer higher predictive power and accuracy and uncover hidden relationships. Results obtained in this study help focus future studies to develop nanoparticles that are safe by design.",2019-02-01,9,1944,73,827
2426,30815461,Blood Pressure Assessment with Differential Pulse Transit Time and Deep Learning: A Proof of Concept,"Background:                    Modern clinical environments are laden with technology devices continuously gathering physiological data from patients. This is especially true in critical care environments, where life-saving decisions may have to be made on the basis of signals from monitoring devices. Hemodynamic monitoring is essential in dialysis, surgery, and in critically ill patients. For the most severe patients, blood pressure is normally assessed through a catheter, which is an invasive procedure that may result in adverse effects. Blood pressure can also be monitored noninvasively through different methods and these data can be used for the continuous assessment of pressure using machine learning methods. Previous studies have found pulse transit time to be related to blood pressure. In this short paper, we propose to study the feasibility of implementing a data-driven model based on restricted Boltzmann machine artificial neural networks, delivering a first proof of concept for the validity and viability of a method for blood pressure prediction based on these models.              Summary and key messages:                    For the most severe patients (e.g., dialysis, surgery, and the critically ill), blood pressure is normally assessed through invasive catheters. Alternatively, noninvasive methods have also been developed for its monitorization. Data obtained from noninvasive measurements can be used for the continuous assessment of pressure using machine learning methods. In this study, a restricted Boltzmann machine artificial neural network is used to present a first proof of concept for the validity and viability of a method for blood pressure prediction.",2019-02-01,1,1700,100,827
2519,30594306,Toxicogenomics: A 2020 Vision,"Toxicogenomics (TGx) has contributed significantly to toxicology and now has great potential to support moves towards animal-free approaches in regulatory decision making. Here, we discuss in vitro TGx systems and their potential impact on risk assessment. We raise awareness of the rapid advancement of genomics technologies, which generates novel genomics features essential for enhanced risk assessment. We specifically emphasize the importance of reproducibility in utilizing TGx in the regulatory setting. We also highlight the role of machine learning (particularly deep learning) in developing TGx-based predictive models. Lastly, we touch on the topics of how TGx approaches could facilitate adverse outcome pathways (AOP) development and enhance read-across strategies to further regulatory application. Finally, we summarize current efforts to develop TGx for risk assessment and set out remaining challenges.",2019-02-01,16,919,29,827
2460,30720708,Skin Sensitization Testing-What's Next?,"There is an increasing demand for alternative in vitro methods to replace animal testing, and, to succeed, new methods are required to be at least as accurate as existing in vivo tests. However, skin sensitization is a complex process requiring coordinated and tightly regulated interactions between a variety of cells and molecules. Consequently, there is considerable difficulty in reproducing this level of biological complexity in vitro, and as a result the development of non-animal methods has posed a major challenge. However, with the use of a relevant biological system, the high information content of whole genome expression, and comprehensive bioinformatics, assays for most complex biological processes can be achieved. We propose that the Genomic Allergen Rapid Detection (GARD) assay, developed to create a holistic data-driven in vitro model with high informational content, could be such an example. Based on the genomic expression of a mature human dendritic cell line and state-of-the-art machine learning techniques, GARD can today accurately predict skin sensitizers and correctly categorize skin sensitizing potency. Consequently, by utilizing advanced processing tools in combination with high information genomic or proteomic data, we can take the next step toward alternative methods with the same predictive accuracy as today's in vivo methods-and beyond.",2019-02-01,0,1383,39,827
2454,30736374,The Role of Movement Analysis in Diagnosing and Monitoring Neurodegenerative Conditions: Insights from Gait and Postural Control,"Quantifying gait and postural control adds valuable information that aids in understanding neurological conditions where motor symptoms predominate and cause considerable functional impairment. Disease-specific clinical scales exist; however, they are often susceptible to subjectivity, and can lack sensitivity when identifying subtle gait and postural impairments in prodromal cohorts and longitudinally to document disease progression. Numerous devices are available to objectively quantify a range of measurement outcomes pertaining to gait and postural control; however, efforts are required to standardise and harmonise approaches that are specific to the neurological condition and clinical assessment. Tools are urgently needed that address a number of unmet needs in neurological practice. Namely, these include timely and accurate diagnosis; disease stratification; risk prediction; tracking disease progression; and decision making for intervention optimisation and maximising therapeutic response (such as medication selection, disease staging, and targeted support). Using some recent examples of research across a range of relevant neurological conditions-including Parkinson's disease, ataxia, and dementia-we will illustrate evidence that supports progress against these unmet clinical needs. We summarise the novel 'big data' approaches that utilise data mining and machine learning techniques to improve disease classification and risk prediction, and conclude with recommendations for future direction.",2019-02-01,19,1521,128,827
2565,30502096,Artificial intelligence and machine learning | applications in musculoskeletal physiotherapy,"Introduction:                    Artificial intelligence (AI) is a field of mathematical engineering which has potential to enhance healthcare through new care delivery strategies, informed decision making and facilitation of patient engagement. Machine learning (ML) is a form of narrow artificial intelligence which can be used to automate decision making and make predictions based upon patient data.              Purpose:                    This review outlines key applications of supervised and unsupervised machine learning in musculoskeletal medicine; such as diagnostic imaging, patient measurement data, and clinical decision support. The current literature base is examined to identify areas where ML performs equal to or more accurately than human levels.              Implications:                    Potential is apparent for intelligent machines to enhance various areas of physiotherapy practice through automization of tasks which involve data analysis, classification and prediction. Changes to service provision through applications of ML, should encourage physiotherapists to increase their awareness of and experiences with emerging technologies. Data literacy should be a component of professional development plans to assist physiotherapists in the application of ML and the preparation of information technology systems to use these techniques.",2019-02-01,4,1368,92,827
2445,30760912,Deep learning and process understanding for data-driven Earth system science,"Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning.",2019-02-01,44,807,76,827
2427,30815459,Societal Issues Concerning the Application of Artificial Intelligence in Medicine,"Background:                    Medicine is becoming an increasingly data-centred discipline and, beyond classical statistical approaches, artificial intelligence (AI) and, in particular, machine learning (ML) are attracting much interest for the analysis of medical data. It has been argued that AI is experiencing a fast process of commodification. This characterization correctly reflects the current process of industrialization of AI and its reach into society. Therefore, societal issues related to the use of AI and ML should not be ignored any longer and certainly not in the medical domain. These societal issues may take many forms, but they all entail the design of models from a human-centred perspective, incorporating human-relevant requirements and constraints. In this brief paper, we discuss a number of specific issues affecting the use of AI and ML in medicine, such as fairness, privacy and anonymity, explainability and interpretability, but also some broader societal issues, such as ethics and legislation. We reckon that all of these are relevant aspects to consider in order to achieve the objective of fostering acceptance of AI- and ML-based technologies, as well as to comply with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. Our specific goal here is to reflect on how all these topics affect medical applications of AI and ML. This paper includes some of the contents of the ""2nd Meeting of Science and Dialysis: Artificial Intelligence,"" organized in the Bellvitge University Hospital, Barcelona, Spain.              Summary and key messages:                    AI and ML are attracting much interest from the medical community as key approaches to knowledge extraction from data. These approaches are increasingly colonizing ambits of social impact, such as medicine and healthcare. Issues of social relevance with an impact on medicine and healthcare include (although they are not limited to) fairness, explainability, privacy, ethics and legislation.",2019-02-01,7,2053,81,827
2439,30774271,Functional gastrointestinal disorders and gut-brain axis: What does the future hold?,"Despite their high prevalence, lack of understanding of the exact pathophysiology of the functional gastrointestinal disorders has restricted us to symptomatic diagnostic tools and therapies. Complex mechanisms underlying the disturbances in the bidirectional communication between the gastrointestinal tract and the brain have a vital role in the pathogenesis and are key to our understanding of the disease phenomenon. Although we have come a long way in our understanding of these complex disorders with the help of studies on animals especially rodents, there need to be more studies in humans, especially to identify the therapeutic targets. This review study looks at the anatomical features of the gut-brain axis in order to discuss the different factors and underlying molecular mechanisms that may have a role in the pathogenesis of functional gastrointestinal disorders. These molecules and their receptors can be targeted in future for further studies and possible therapeutic interventions. The article also discusses the potential role of artificial intelligence and machine learning and its possible role in our understanding of these scientifically challenging disorders.",2019-02-01,12,1186,84,827
2438,30779785,Applications of artificial neural networks in health care organizational decision-making: A scoping review,"Health care organizations are leveraging machine-learning techniques, such as artificial neural networks (ANN), to improve delivery of care at a reduced cost. Applications of ANN to diagnosis are well-known; however, ANN are increasingly used to inform health care management decisions. We provide a seminal review of the applications of ANN to health care organizational decision-making. We screened 3,397 articles from six databases with coverage of Health Administration, Computer Science and Business Administration. We extracted study characteristics, aim, methodology and context (including level of analysis) from 80 articles meeting inclusion criteria. Articles were published from 1997-2018 and originated from 24 countries, with a plurality of papers (26 articles) published by authors from the United States. Types of ANN used included ANN (36 articles), feed-forward networks (25 articles), or hybrid models (23 articles); reported accuracy varied from 50% to 100%. The majority of ANN informed decision-making at the micro level (61 articles), between patients and health care providers. Fewer ANN were deployed for intra-organizational (meso- level, 29 articles) and system, policy or inter-organizational (macro- level, 10 articles) decision-making. Our review identifies key characteristics and drivers for market uptake of ANN for health care organizational decision-making to guide further adoption of this technique.",2019-02-01,14,1435,106,827
2461,30718453,Neurodevelopmental heterogeneity and computational approaches for understanding autism,"In recent years, the emerging field of computational psychiatry has impelled the use of machine learning models as a means to further understand the pathogenesis of multiple clinical disorders. In this paper, we discuss how autism spectrum disorder (ASD) was and continues to be diagnosed in the context of its complex neurodevelopmental heterogeneity. We review machine learning approaches to streamline ASD's diagnostic methods, to discern similarities and differences from comorbid diagnoses, and to follow developmentally variable outcomes. Both supervised machine learning models for classification outcome and unsupervised approaches to identify new dimensions and subgroups are discussed. We provide an illustrative example of how computational analytic methods and a longitudinal design can improve our inferential ability to detect early dysfunctional behaviors that may or may not reach threshold levels for formal diagnoses. Specifically, an unsupervised machine learning approach of anomaly detection is used to illustrate how community samples may be utilized to investigate early autism risk, multidimensional features, and outcome variables. Because ASD symptoms and challenges are not static within individuals across development, computational approaches present a promising method to elucidate subgroups of etiological contributions to phenotype, alternative developmental courses, interactions with biomedical comorbidities, and to predict potential responses to therapeutic interventions.",2019-02-01,10,1508,86,827
2511,30614150,Automated techniques for blood vessels segmentation through fundus retinal images: A review,"Retina is the interior part of human's eye, has a vital role in vision. The digital image captured by fundus camera is very useful to analyze the abnormalities in retina especially in retinal blood vessels. To get information of blood vessels through fundus retinal image, a precise and accurate vessels segmentation image is required. This segmented blood vessel image is most beneficial to detect retinal diseases. Many automated techniques are widely used for retinal vessels segmentation which is a primary element of computerized diagnostic systems for retinal diseases. The automatic vessels segmentation may lead to more challenging task in the presence of lesions and abnormalities. This paper briefly describes the various publicly available retinal image databases and various machine learning techniques. State of the art exhibited that researchers have proposed several vessel segmentation methods based on supervised and supervised techniques and evaluated their results mostly on publicly datasets such as digital retinal images for vessel extraction and structured analysis of the retina. A comprehensive review of existing supervised and unsupervised vessel segmentation techniques or algorithms is presented which describes the philosophy of each algorithm. This review will be useful for readers in their future research.",2019-02-01,1,1339,91,827
2517,30599797,Cortico-limbic connectivity as a possible biomarker for bipolar disorder: where are we now?,"The fronto-limbic network has been suggested as a key circuitry in the pathophysiology and maintenance of bipolar disorder. In the past decade, a disrupted connectivity within prefrontal-limbic structures was identified as a promising candidate biomarker for the disorder. Areas Covered: In this review, the authors examine current literature in terms of the structural, functional and effective connectivity in bipolar disorder, integrating recent findings of imaging genetics and machine learning. This paper profiles the current knowledge and identifies future perspectives to provide reliable and usable neuroimaging biomarkers for bipolar psychopathology in clinical practice. Expert Opinion: The replication and the translation of acquired knowledge into useful and usable tools represents one of the current greatest challenges in biomarker research applied to psychiatry.",2019-02-01,2,879,91,827
196,29994486,Natural Language Processing for EHR-Based Computational Phenotyping,"This article reviews recent advances in applying natural language processing (NLP) to Electronic Health Records (EHRs) for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction (DDI), and adverse drug event (ADE) detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often, better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives.",2019-02-01,18,1581,67,827
2393,30872992,"Machine Learning in Amyotrophic Lateral Sclerosis: Achievements, Pitfalls, and Future Directions","Background: Amyotrophic Lateral Sclerosis (ALS) is a relentlessly progressive neurodegenerative condition with limited therapeutic options at present. Survival from symptom onset ranges from 3 to 5 years depending on genetic, demographic, and phenotypic factors. Despite tireless research efforts, the core etiology of the disease remains elusive and drug development efforts are confounded by the lack of accurate monitoring markers. Disease heterogeneity, late-stage recruitment into pharmaceutical trials, and inclusion of phenotypically admixed patient cohorts are some of the key barriers to successful clinical trials. Machine Learning (ML) models and large international data sets offer unprecedented opportunities to appraise candidate diagnostic, monitoring, and prognostic markers. Accurate patient stratification into well-defined prognostic categories is another aspiration of emerging classification and staging systems. Methods: The objective of this paper is the comprehensive, systematic, and critical review of ML initiatives in ALS to date and their potential in research, clinical, and pharmacological applications. The focus of this review is to provide a dual, clinical-mathematical perspective on recent advances and future directions of the field. Another objective of the paper is the frank discussion of the pitfalls and drawbacks of specific models, highlighting the shortcomings of existing studies and to provide methodological recommendations for future study designs. Results: Despite considerable sample size limitations, ML techniques have already been successfully applied to ALS data sets and a number of promising diagnosis models have been proposed. Prognostic models have been tested using core clinical variables, biological, and neuroimaging data. These models also offer patient stratification opportunities for future clinical trials. Despite the enormous potential of ML in ALS research, statistical assumptions are often violated, the choice of specific statistical models is seldom justified, and the constraints of ML models are rarely enunciated. Conclusions: From a mathematical perspective, the main barrier to the development of validated diagnostic, prognostic, and monitoring indicators stem from limited sample sizes. The combination of multiple clinical, biofluid, and imaging biomarkers is likely to increase the accuracy of mathematical modeling and contribute to optimized clinical trial designs.",2019-02-01,16,2452,96,827
2513,30610579,Precision medicine review: rare driver mutations and their biophysical classification,"How can biophysical principles help precision medicine identify rare driver mutations? A major tenet of pragmatic approaches to precision oncology and pharmacology is that driver mutations are very frequent. However, frequency is a statistical attribute, not a mechanistic one. Rare mutations can also act through the same mechanism, and as we discuss below, ""latent driver"" mutations may also follow the same route, with ""helper"" mutations. Here, we review how biophysics provides mechanistic guidelines that extend precision medicine. We outline principles and strategies, especially focusing on mutations that drive cancer. Biophysics has contributed profoundly to deciphering biological processes. However, driven by data science, precision medicine has skirted some of its major tenets. Data science embodies genomics, tissue- and cell-specific expression levels, making it capable of defining genome- and systems-wide molecular disease signatures. It classifies cancer driver genes/mutations and affected pathways, and its associated protein structural data guide drug discovery. Biophysics complements data science. It considers structures and their heterogeneous ensembles, explains how mutational variants can signal through distinct pathways, and how allo-network drugs can be harnessed. Biophysics clarifies how one mutation-frequent or rare-can affect multiple phenotypic traits by populating conformations that favor interactions with other network modules. It also suggests how to identify such mutations and their signaling consequences. Biophysics offers principles and strategies that can help precision medicine push the boundaries to transform our insight into biological processes and the practice of personalized medicine. By contrast, ""phenotypic drug discovery,"" which capitalizes on physiological cellular conditions and first-in-class drug discovery, may not capture the proper molecular variant. This is because variants of the same protein can express more than one phenotype, and a phenotype can be encoded by several variants.",2019-02-01,13,2055,85,827
2424,30820462,Artificial intelligence for precision oncology: beyond patient stratification,"The data-driven identification of disease states and treatment options is a crucial challenge for precision oncology. Artificial intelligence (AI) offers unique opportunities for enhancing such predictive capabilities in the lab and the clinic. AI, including its best-known branch of research, machine learning, has significant potential to enable precision oncology well beyond relatively well-known pattern recognition applications, such as the supervised classification of single-source omics or imaging datasets. This perspective highlights key advances and challenges in that direction. Furthermore, it argues that AI's scope and depth of research need to be expanded to achieve ground-breaking progress in precision oncology.",2019-02-01,19,731,77,827
147,30097794,Machine learning and feature selection for drug response prediction in precision oncology applications,"In-depth modeling of the complex interplay among multiple omics data measured from cancer cell lines or patient tumors is providing new opportunities toward identification of tailored therapies for individual cancer patients. Supervised machine learning algorithms are increasingly being applied to the omics profiles as they enable integrative analyses among the high-dimensional data sets, as well as personalized predictions of therapy responses using multi-omics panels of response-predictive biomarkers identified through feature selection and cross-validation. However, technical variability and frequent missingness in input ""big data"" require the application of dedicated data preprocessing pipelines that often lead to some loss of information and compressed view of the biological signal. We describe here the state-of-the-art machine learning methods for anti-cancer drug response modeling and prediction and give our perspective on further opportunities to make better use of high-dimensional multi-omics profiles along with knowledge about cancer pathways targeted by anti-cancer compounds when predicting their phenotypic responses.",2019-02-01,32,1146,102,827
584,30898263,Rise of the Machines: Advances in Deep Learning for Cancer Diagnosis,"Deep learning refers to a set of computer models that have recently been used to make unprecedented progress in the way computers extract information from images. These algorithms have been applied to tasks in numerous medical specialties, most extensively radiology and pathology, and in some cases have attained performance comparable to human experts. Furthermore, it is possible that deep learning could be used to extract data from medical images that would not be apparent by human analysis and could be used to inform on molecular status, prognosis, or treatment sensitivity. In this review, we outline the current developments and state-of-the-art in applying deep learning for cancer diagnosis, and discuss the challenges in adapting the technology for widespread clinical deployment.",2019-03-01,15,793,68,799
2574,30496527,Recurrent Neural Networks in Mobile Sampling and Intervention,"The rapid rise and now widespread distribution of handheld and wearable devices, such as smartphones, fitness trackers, or smartwatches, has opened a new universe of possibilities for monitoring emotion and cognition in everyday-life context, and for applying experience- and context-specific interventions in psychosis. These devices are equipped with multiple sensors, recording channels, and app-based opportunities for assessment using experience sampling methodology (ESM), which enables to collect vast amounts of temporally highly resolved and ecologically valid personal data from various domains in daily life. In psychosis, this allows to elucidate intermediate and clinical phenotypes, psychological processes and mechanisms, and their interplay with socioenvironmental factors, as well as to evaluate the effects of treatments for psychosis on important clinical and social outcomes. Although these data offer immense opportunities, they also pose tremendous challenges for data analysis. These challenges include the sheer amount of time series data generated and the many different data modalities and their specific properties and sampling rates. After a brief review of studies and approaches to ESM and ecological momentary interventions in psychosis, we will discuss recurrent neural networks (RNNs) as a powerful statistical machine learning approach for time series analysis and prediction in this context. RNNs can be trained on multiple data modalities simultaneously to learn a dynamical model that could be used to forecast individual trajectories and schedule online feedback and intervention accordingly. Future research using this approach is likely going to offer new avenues to further our understanding and treatments of psychosis.",2019-03-01,4,1761,61,799
2518,30594648,RNA splicing analysis in genomic medicine,"High-throughput next-generation sequencing technologies have led to a rapid increase in the number of sequence variants identified in clinical practice via diagnostic genetic tests. Current bioinformatic analysis pipelines fail to take adequate account of the possible splicing effects of such variants, particularly where variants fall outwith canonical splice site sequences, and consequently the pathogenicity of such variants may often be missed. The regulation of splicing is highly complex and as a result, in silico prediction tools lack sufficient sensitivity and specificity for reliable use. Variants of all kinds can be linked to aberrant splicing in disease and the need for correct identification and diagnosis grows ever more crucial as novel splice-switching antisense oligonucleotide therapies start to enter clinical usage. RT-PCR provides a useful targeted assay of the splicing effects of identified variants, while minigene assays, massive parallel reporter assays and animal models can also be used for more detailed study of a particular splicing system, given enough time and resources. However, RNA-sequencing (RNA-seq) has the potential to be used as a rapid diagnostic tool in genomic medicine. By utilising data science approaches and machine learning, it may prove possible to finally understand and interpret the 'splicing code' and apply this knowledge in human disease diagnostics.",2019-03-01,7,1412,41,799
2411,30834982,Spatio-temporal simulation and prediction of land-use change using conventional and machine learning models: a review,"Spatio-temporal land-use change modeling, simulation, and prediction have become one of the critical issues in the last three decades due to uncertainty, structure, flexibility, accuracy, the ability for improvement, and the capability for integration of available models. Therefore, many types of models such as dynamic, statistical, and machine learning (ML) models have been used in the geographic information system (GIS) environment to fulfill the high-performance requirements of land-use modeling. This paper provides a literature review on models for modeling, simulating, and predicting land-use change to determine the best approach that can realistically simulate land-use changes. Therefore, the general characteristics of conventional and ML models for land-use change are described, and the different techniques used in the design of these models are classified. The strengths and weaknesses of the various dynamic, statistical, and ML models are determined according to the analysis and discussion of the characteristics of these models. The results of the review confirm that ML models are the most powerful models for simulating land-use change because they can include all driving forces of land-use change in the simulation process and simulate linear and non-linear phenomena, which dynamic models and statistical models are unable to do. However, ML models also have limitations. For instance, some ML models are complex, the simulation rules cannot be changed, and it is difficult to understand how ML models work in a system. However, this can be solved via the use of programming languages such as Python, which in turn improve the simulation capabilities of the ML models.",2019-03-01,2,1697,117,799
543,30984469,Introduction to Digital Image Analysis in Whole-slide Imaging: A White Paper from the Digital Pathology Association,"The advent of whole-slide imaging in digital pathology has brought about the advancement of computer-aided examination of tissue via digital image analysis. Digitized slides can now be easily annotated and analyzed via a variety of algorithms. This study reviews the fundamentals of tissue image analysis and aims to provide pathologists with basic information regarding the features, applications, and general workflow of these new tools. The review gives an overview of the basic categories of software solutions available, potential analysis strategies, technical considerations, and general algorithm readouts. Advantages and limitations of tissue image analysis are discussed, and emerging concepts, such as artificial intelligence and machine learning, are introduced. Finally, examples of how digital image analysis tools are currently being used in diagnostic laboratories, translational research, and drug development are discussed.",2019-03-01,38,941,115,799
2251,31123497,The role of emotions in cancer patients' decision-making,"Introduction:                    Despite the attempt to make decisions based on evidence, doctors still have to consider patients' choices which often involve other factors. In particular, emotions seem to influence the way that options and the surrounding information are interpreted and used.              Objective:                    The objective of the present review is to provide a brief overview of research on decision making and cancer with a specific focus on the role of emotions.              Method:                    Thirty-nine studies were identified and analysed. Most of the studies investigated anxiety and fear. Worry was the other psychological factor that, together with anxiety, played a crucial role in cancer-related decision-making.              Results:                    The roles of fear, anxiety and worry were described for detection behaviour, diagnosis, choice about prevention and curative treatments and help-seeking behaviour. Results were inconsistent among the studies. Results stressed that cognitive appraisal and emotional arousal (emotion's intensity level) interact in shaping the decision. Moderate levels of anxiety and worry improved decision-making, while low and high levels tended to have no effect or a hindering effect on decision making. Moderating factors played an under-investigated role.              Conclusions:                    Decision making is a complex non-linear process that is affected by several factors, such as, for example, personal knowledge, past experiences, individual differences and certainly emotions. Research studies should investigate further potential moderators of the effect of emotions on cancer-related choice. Big data and machine learning could be a good opportunity to test the interaction between a large amount of factors that is not feasible in traditional research. New technologies such as eHealth and virtual reality can offer support for the regulation of emotions and decision making.",2019-03-01,12,1986,56,799
544,30983960,Childhood Trauma in Schizophrenia: Current Findings and Research Perspectives,"Schizophrenia is a severe neuropsychiatric disorder with persistence of symptoms throughout adult life in most of the affected patients. This unfavorable course is associated with multiple episodes and residual symptoms, mainly negative symptoms and cognitive deficits. The neural diathesis-stress model proposes that psychosocial stress acts on a pre-existing vulnerability and thus triggers the symptoms of schizophrenia. Childhood trauma is a severe form of stress that renders individuals more vulnerable to developing schizophrenia; neurobiological effects of such trauma on the endocrine system and epigenetic mechanisms are discussed. Childhood trauma is associated with impaired working memory, executive function, verbal learning, and attention in schizophrenia patients, including those at ultra-high risk to develop psychosis. In these patients, higher levels of childhood trauma were correlated with higher levels of attenuated positive symptoms, general symptoms, and depressive symptoms; lower levels of global functioning; and poorer cognitive performance in visual episodic memory end executive functions. In this review, we discuss effects of specific gene variants that interact with childhood trauma in patients with schizophrenia and describe new findings on the brain structural and functional level. Additive effects between childhood trauma and brain-derived neurotrophic factor methionine carriers on volume loss of the hippocampal subregions cornu ammonis (CA)4/dentate gyrus and CA2/3 have been reported in schizophrenia patients. A functional magnetic resonance imaging study showed that childhood trauma exposure resulted in aberrant function of parietal areas involved in working memory and of visual cortical areas involved in attention. In a theory of mind task reflecting social cognition, childhood trauma was associated with activation of the posterior cingulate gyrus, precuneus, and dorsomedial prefrontal cortex in patients with schizophrenia. In addition, decreased connectivity was shown between the posterior cingulate/precuneus region and the amygdala in patients with high levels of physical neglect and sexual abuse during childhood, suggesting that disturbances in specific brain networks underlie cognitive abilities. Finally, we discuss some of the questionnaires that are commonly used to assess childhood trauma and outline possibilities to use recent biostatistical methods, such as machine learning, to analyze the resulting datasets.",2019-03-01,7,2484,77,799
455,30315284,Artificial Intelligence-Assisted Gastroenterology- Promises and Pitfalls,"Technological advances in artificial intelligence (AI) represent an enticing opportunity to benefit gastroenterological practice. Moreover, AI, through machine or deep learning, permits the ability to develop predictive models from large datasets. Possibilities of predictive model development in machine learning are numerous dependent on the clinical question. For example, binary classifiers aim to stratify allocation to a categorical outcome, such as the presence or absence of a gastrointestinal disease. In addition, continuous variable fitting techniques can be used to predict quantity of a therapeutic response, thus offering a tool to predict which therapeutic intervention may be most beneficial to the given patient. Namely, this permits an important opportunity for personalization of medicine, including a movement from guideline-specific treatment algorithms to patient-specific ones, providing both clinician and patient the capacity for data-driven decision making. Furthermore, such analyses could predict the development of GI disease prior to the manifestation of symptoms, raising the possibility of prevention or pre-treatment. In addition, computer vision additionally provides an exciting opportunity in endoscopy to automatically detect lesions. In this review, we overview the recent developments in healthcare-based AI and machine learning and describe promises and pitfalls for its application to gastroenterology.",2019-03-01,14,1443,72,799
2463,30712598,Prediction of sepsis patients using machine learning approach: A meta-analysis,"Study objective:                    Sepsis is a common and major health crisis in hospitals globally. An innovative and feasible tool for predicting sepsis remains elusive. However, early and accurate prediction of sepsis could help physicians with proper treatments and minimize the diagnostic uncertainty. Machine learning models could help to identify potential clinical variables and provide higher performance than existing traditional low-performance models. We therefore performed a meta-analysis of observational studies to quantify the performance of a machine learning model to predict sepsis.              Methods:                    A comprehensive literature search was conducted through the electronic database (e.g. PubMed, Scopus, Google Scholar, EMBASE, etc.) between January 1, 2000, and March 1, 2018. All the studies published in English and reporting the sepsis prediction using machine learning algorithms were considered in this study. Two authors independently extracted valuable information from the included studies. Inclusion and exclusion of studies were based on the Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) guidelines.              Results:                    A total of 7 out of 135 studies met all of our inclusion criteria. For machine learning models, the pooled area under receiving operating curve (SAUROC) for predicting sepsis onset 3 to 4 h before, was 0.89 (95%CI: 0.86-0.92); sensitivity 0.81 (95%CI:0.80-0.81), and specificity 0.72 (95%CI:0.72-0.72) whereas the pooled SAUROC for SIRS, MEWS, and SOFA was 0.70, 0.50, and 0.78. Additionally, diagnostic odd ratio for machine learning, SIRS, MEWS, and SOFA was 15.17 (95%CI: 9.51-24.20), 3.23 (95%CI: 1.52-6.87), 31.99 (95% CI: 1.54-666.74), and 3.75(95%CI: 2.06-6.83).              Conclusion:                    Our study findings suggest that the machine learning approach had a better performance than the existing sepsis scoring systems in predicting sepsis.",2019-03-01,16,1988,78,799
2536,30557049,Artificial Intelligence for Medical Image Analysis: A Guide for Authors and Reviewers,"Objective:                    The purpose of this article is to highlight best practices for writing and reviewing articles on artificial intelligence for medical image analysis.              Conclusion:                    Artificial intelligence is in the early phases of application to medical imaging, and patient safety demands a commitment to sound methods and avoidance of rhetorical and overly optimistic claims. Adherence to best practices should elevate the quality of articles submitted to and published by clinical journals.",2019-03-01,15,535,85,799
2397,30867681,Encodings and models for antimicrobial peptide classification for multi-resistant pathogens,"Antimicrobial peptides (AMPs) are part of the inherent immune system. In fact, they occur in almost all organisms including, e.g., plants, animals, and humans. Remarkably, they show effectivity also against multi-resistant pathogens with a high selectivity. This is especially crucial in times, where society is faced with the major threat of an ever-increasing amount of antibiotic resistant microbes. In addition, AMPs can also exhibit antitumor and antiviral effects, thus a variety of scientific studies dealt with the prediction of active peptides in recent years. Due to their potential, even the pharmaceutical industry is keen on discovering and developing novel AMPs. However, AMPs are difficult to verify in vitro, hence researchers conduct sequence similarity experiments against known, active peptides. Unfortunately, this approach is very time-consuming and limits potential candidates to sequences with a high similarity to known AMPs. Machine learning methods offer the opportunity to explore the huge space of sequence variations in a timely manner. These algorithms have, in principal, paved the way for an automated discovery of AMPs. However, machine learning models require a numerical input, thus an informative encoding is very important. Unfortunately, developing an appropriate encoding is a major challenge, which has not been entirely solved so far. For this reason, the development of novel amino acid encodings is established as a stand-alone research branch. The present review introduces state-of-the-art encodings of amino acids as well as their properties in sequence and structure based aggregation. Moreover, albeit a well-chosen encoding is essential, performant classifiers are required, which is reflected by a tendency towards specifically designed models in the literature. Furthermore, we introduce these models with a particular focus on encodings derived from support vector machines and deep learning approaches. Albeit a strong focus has been set on AMP predictions, not all of the mentioned encodings have been elaborated as part of antimicrobial research studies, but rather as general protein or peptide representations.",2019-03-01,4,2167,91,799
464,30301571,Deep Learning with Microfluidics for Biotechnology,"Advances in high-throughput and multiplexed microfluidics have rewarded biotechnology researchers with vast amounts of data but not necessarily the ability to analyze complex data effectively. Over the past few years, deep artificial neural networks (ANNs) leveraging modern graphics processing units (GPUs) have enabled the rapid analysis of structured input data - sequences, images, videos - to predict complex outputs with unprecedented accuracy. While there have been early successes in flow cytometry, for example, the extensive potential of pairing microfluidics (to acquire data) and deep learning (to analyze data) to tackle biotechnology challenges remains largely untapped. Here we provide a roadmap to integrating deep learning and microfluidics in biotechnology laboratories that matches computational architectures to problem types, and provide an outlook on emerging opportunities.",2019-03-01,14,896,50,799
2413,30832275,Pharmacogenomic and Pharmacotranscriptomic Profiling of Childhood Acute Lymphoblastic Leukemia: Paving the Way to Personalized Treatment,"Personalized medicine is focused on research disciplines which contribute to the individualization of therapy, like pharmacogenomics and pharmacotranscriptomics. Acute lymphoblastic leukemia (ALL) is the most common malignancy of childhood. It is one of the pediatric malignancies with the highest cure rate, but still a lethal outcome due to therapy accounts for 1%3% of deaths. Further improvement of treatment protocols is needed through the implementation of pharmacogenomics and pharmacotranscriptomics. Emerging high-throughput technologies, including microarrays and next-generation sequencing, have provided an enormous amount of molecular data with the potential to be implemented in childhood ALL treatment protocols. In the current review, we summarized the contribution of these novel technologies to the pharmacogenomics and pharmacotranscriptomics of childhood ALL. We have presented data on molecular markers responsible for the efficacy, side effects, and toxicity of the drugs commonly used for childhood ALL treatment, i.e., glucocorticoids, vincristine, asparaginase, anthracyclines, thiopurines, and methotrexate. Big data was generated using high-throughput technologies, but their implementation in clinical practice is poor. Research efforts should be focused on data analysis and designing prediction models using machine learning algorithms. Bioinformatics tools and the implementation of artificial i Lack of association of the CEP72 rs924607 TT genotype with intelligence are expected to open the door wide for personalized medicine in the clinical practice of childhood ALL.",2019-03-01,7,1603,136,799
2544,30547185,Breaking the curse of dimensionality to identify causal variants in Breeding 4,"In the past, plant breeding has undergone three major transformations and is currently transitioning to a new technological phase, Breeding 4. This phase is characterized by the development of methods for biological design of plant varieties, including transformation and gene editing techniques directed toward causal loci. The application of such technologies will require to reliably estimate the effect of loci in plant genomes by avoiding the situation where the number of loci assayed (p) surpasses the number of plant genotypes (n). Here, we discuss approaches to avoid this curse of dimensionality (n  p), which will involve analyzing intermediate phenotypes such as molecular traits and component traits related to plant morphology or physiology. Because these approaches will rely on novel data types such as DNA sequences and high-throughput phenotyping images, Breeding 4 will call for analyses that are complementary to traditional quantitative genetic studies, being based on machine learning techniques which make efficient use of sequence and image data. In this article, we will present some of these techniques and their application for prioritizing causal loci and developing improved varieties in Breeding 4.",2019-03-01,10,1229,78,799
2395,30871264,Computational Approaches in Theranostics: Mining and Predicting Cancer Data,"The ability to understand the complexity of cancer-related data has been prompted by the applications of (1) computer and data sciences, including data mining, predictive analytics, machine learning, and artificial intelligence, and (2) advances in imaging technology and probe development. Computational modelling and simulation are systematic and cost-effective tools able to identify important temporal/spatial patterns (and relationships), characterize distinct molecular features of cancer states, and address other relevant aspects, including tumor detection and heterogeneity, progression and metastasis, and drug resistance. These approaches have provided invaluable insights for improving the experimental design of therapeutic delivery systems and for increasing the translational value of the results obtained from early and preclinical studies. The big question is: Could cancer theranostics be determined and controlled in silico? This review describes the recent progress in the development of computational models and methods used to facilitate research on the molecular basis of cancer and on the respective diagnosis and optimized treatment, with particular emphasis on the design and optimization of theranostic systems. The current role of computational approaches is providing innovative, incremental, and complementary data-driven solutions for the prediction, simplification, and characterization of cancer and intrinsic mechanisms, and to promote new data-intensive, accurate diagnostics and therapeutics.",2019-03-01,4,1528,75,799
2561,30507517,Into the Wild: The Challenges of Physiological Stress Detection in Laboratory and Ambulatory Settings,"Stress and mental health have become major concerns worldwide. Research has already extensively investigated physiological signals as quantitative and continuous markers of stress. In recent years, the focus of the field has shifted from the laboratory to the ambulatory environment. We provide an overview of physiological stress detection in laboratory settings with a focus on identifying physiological sensing priorities, including electrocardiogram, skin conductance, and electromyogram, and the most suitable machine learning techniques, of which the choice depends on the context of the application. Additionally, an overview is given of new challenges ahead to move toward the ambulant environment, including the influence of physical activity, lower signal quality due to motion artifacts, the lack of a stress reference, and the subject-dependent nature of the physiological stress response. Finally, several recommendations for future research are listed, focusing on large-scale, longitudinal trials across different population groups and just-in-time interventions to move toward disease prevention and interception.",2019-03-01,5,1129,101,799
2414,30832207,Snails In Silico: A Review of Computational Studies on the Conopeptides,"Marine cone snails are carnivorous gastropods that use peptide toxins called conopeptides both as a defense mechanism and as a means to immobilize and kill their prey. These peptide toxins exhibit a large chemical diversity that enables exquisite specificity and potency for target receptor proteins. This diversity arises in terms of variations both in amino acid sequence and length, and in posttranslational modifications, particularly the formation of multiple disulfide linkages. Most of the functionally characterized conopeptides target ion channels of animal nervous systems, which has led to research on their therapeutic applications. Many facets of the underlying molecular mechanisms responsible for the specificity and virulence of conopeptides, however, remain poorly understood. In this review, we will explore the chemical diversity of conopeptides from a computational perspective. First, we discuss current approaches used for classifying conopeptides. Next, we review different computational strategies that have been applied to understanding and predicting their structure and function, from machine learning techniques for predictive classification to docking studies and molecular dynamics simulations for molecular-level understanding. We then review recent novel computational approaches for rapid high-throughput screening and chemical design of conopeptides for particular applications. We close with an assessment of the state of the field, emphasizing important questions for future lines of inquiry.",2019-03-01,7,1528,71,799
2432,30802231,Deep Learning Applications in Chest Radiography and Computed Tomography: Current State of the Art,"Deep learning is a genre of machine learning that allows computational models to learn representations of data with multiple levels of abstraction using numerous processing layers. A distinctive feature of deep learning, compared with conventional machine learning methods, is that it can generate appropriate models for tasks directly from the raw data, removing the need for human-led feature extraction. Medical images are particularly suited for deep learning applications. Deep learning techniques have already demonstrated high performance in the detection of diabetic retinopathy on fundoscopic images and metastatic breast cancer cells on pathologic images. In radiology, deep learning has the opportunity to provide improved accuracy of image interpretation and diagnosis. Many groups are exploring the possibility of using deep learning-based applications to solve unmet clinical needs. In chest imaging, there has been a large effort to develop and apply computer-aided detection systems for the detection of lung nodules on chest radiographs and chest computed tomography. The essential limitation to computer-aided detection is an inability to learn from new information. To overcome these deficiencies, many groups have turned to deep learning approaches with promising results. In addition to nodule detection, interstitial lung disease recognition, lesion segmentation, diagnosis and patient outcomes have been addressed by deep learning approaches. The purpose of this review article was to cover the current state of the art for deep learning approaches and its limitations, and some of the potential impact on the field of radiology, with specific reference to chest imaging.",2019-03-01,10,1694,97,799
1067,31463458,Artificial intelligence and machine learning in spine research,"Artificial intelligence (AI) and machine learning (ML) techniques are revolutionizing several industrial and research fields like computer vision, autonomous driving, natural language processing, and speech recognition. These novel tools are already having a major impact in radiology, diagnostics, and many other fields in which the availability of automated solution may benefit the accuracy and repeatability of the execution of critical tasks. In this narrative review, we first present a brief description of the various techniques that are being developed nowadays, with special focus on those used in spine research. Then, we describe the applications of AI and ML to problems related to the spine which have been published so far, including the localization of vertebrae and discs in radiological images, image segmentation, computer-aided diagnosis, prediction of clinical outcomes and complications, decision support systems, content-based image retrieval, biomechanics, and motion analysis. Finally, we briefly discuss major ethical issues related to the use of AI in healthcare, namely, accountability, risk of biased decisions as well as data privacy and security, which are nowadays being debated in the scientific community and by regulatory agencies.",2019-03-01,10,1266,62,799
534,31007871,"Computational Prediction of MoRFs, Short Disorder-to-order Transitioning Protein Binding Regions","Molecular recognition features (MoRFs) are short protein-binding regions that undergo disorder-to-order transitions (induced folding) upon binding protein partners. These regions are abundant in nature and can be predicted from protein sequences based on their distinctive sequence signatures. This first-of-its-kind survey covers 14 MoRF predictors and six related methods for the prediction of short protein-binding linear motifs, disordered protein-binding regions and semi-disordered regions. We show that the development of MoRF predictors has accelerated in the recent years. These predictors depend on machine learning-derived models that were generated using training datasets where MoRFs are annotated using putative disorder. Our analysis reveals that they generate accurate predictions. We identified eight methods that offer area under the ROC curve (AUC)  0.7 on experimentally-validated test datasets. We show that modern MoRF predictors accurately find experimentally annotated MoRFs even though they were trained using the putative disorder annotations. They are relatively highly-cited, particularly the methods available as webservers that on average secure three times more citations than methods without this option. MoRF predictions contribute to the experimental discovery of protein-protein interactions, annotation of protein functions and computational analysis of a variety of proteomes, protein families, and pathways. We outline future development and application directions for these tools, stressing the importance to develop novel tools that would target interactions of disordered regions with other types of partners.",2019-03-01,6,1651,96,799
477,30269984,Patient-Driven Diabetes Care of the Future in the Technology Era,"The growing burden of diabetes is fueled by obesity-inducing lifestyle behaviors including high-calorie diets and lack of physical activity. Challenges in access to diabetes specialists and educators, low adherence to medications, and inadequate motivational support for proper disease self-management contribute to poor glycemic control in patients with diabetes. Simultaneously, high patient volumes and low reimbursement rates limit physicians' time spent on lifestyle behavior counseling. These barriers to efficient diabetes care lead to high rates of diabetes-related complications, driving healthcare costs up and reducing the quality of patients' lives. Considering recent advancements in healthcare delivery technologies such as smartphone applications, telemedicine, m-health, device connectivity, machine-learning technology, and artificial intelligence, there is significant opportunity to achieve better efficiency in diabetes care and increase patient involvement in diabetes self-management, which ultimately may put an end to soaring diabetes-related healthcare expenditures. This review explores the patient-driven diabetes care of the future in the technology era.",2019-03-01,4,1182,64,799
2444,30762223,Multiparametric MRI and radiomics in prostate cancer: a review,"Multiparametric MRI (mpMRI) is an imaging modality that combines anatomical MR imaging with one or more functional MRI sequences. It has become a versatile tool for detecting and characterising prostate cancer (PCa). The traditional role of mpMRI was confined to PCa staging, but due to the advanced imaging techniques, its role has expanded to various stages in clinical practises including tumour detection, disease monitor during active surveillance and sequential imaging for patient follow-up. Meanwhile, with the growing speed of data generation and the increasing volume of imaging data, it is highly demanded to apply computerised methods to process mpMRI data and extract useful information. Hence quantitative analysis for imaging data using radiomics has become an emerging paradigm. The application of radiomics approaches in prostate cancer has not only enabled automatic localisation of the disease but also provided a non-invasive solution to assess tumour biology (e.g. aggressiveness and the presence of hypoxia). This article reviews mpMRI and its expanding role in PCa detection, staging and patient management. Following that, an overview of prostate radiomics will be provided, with a special focus on its current applications as well as its future directions.",2019-03-01,16,1281,62,799
2487,30664063,Measurement Variability in Treatment Response Determination for Non-Small Cell Lung Cancer: Improvements Using Radiomics,"Multimodality imaging measurements of treatment response are critical for clinical practice, oncology trials, and the evaluation of new treatment modalities. The current standard for determining treatment response in non-small cell lung cancer (NSCLC) is based on tumor size using the RECIST criteria. Molecular targeted agents and immunotherapies often cause morphological change without reduction of tumor size. Therefore, it is difficult to evaluate therapeutic response by conventional methods. Radiomics is the study of cancer imaging features that are extracted using machine learning and other semantic features. This method can provide comprehensive information on tumor phenotypes and can be used to assess therapeutic response in this new age of immunotherapy. Delta radiomics, which evaluates the longitudinal changes in radiomics features, shows potential in gauging treatment response in NSCLC. It is well known that quantitative measurement methods may be subject to substantial variability due to differences in technical factors and require standardization. In this review, we describe measurement variability in the evaluation of NSCLC and the emerging role of radiomics.",2019-03-01,5,1188,120,799
2453,30738026,Virtual Reality: Beyond Visualization,"Virtual reality (VR) has recently become an affordable technology. A wide range of options are available to access this unique visualization medium, from simple cardboard inserts for smartphones to truly advanced headsets tracked by external sensors. While it is now possible for any research team to gain access to VR, we can still question what it brings to scientific research. Visualization and the ability to navigate complex three-dimensional data are undoubtedly a gateway to many scientific applications; however, we are convinced that data treatment and numerical simulations, especially those mixing interactions with data, human cognition, and automated algorithms will be the future of VR in scientific research. Moreover, VR might soon merit the same level of attention to imaging data as machine learning currently has. In this short perspective, we discuss approaches that employ VR in scientific research based on some concrete examples.",2019-03-01,1,953,37,799
526,31019547,A Technical Review of Convolutional Neural Network-Based Mammographic Breast Cancer Diagnosis,"This study reviews the technique of convolutional neural network (CNN) applied in a specific field of mammographic breast cancer diagnosis (MBCD). It aims to provide several clues on how to use CNN for related tasks. MBCD is a long-standing problem, and massive computer-aided diagnosis models have been proposed. The models of CNN-based MBCD can be broadly categorized into three groups. One is to design shallow or to modify existing models to decrease the time cost as well as the number of instances for training; another is to make the best use of a pretrained CNN by transfer learning and fine-tuning; the third is to take advantage of CNN models for feature extraction, and the differentiation of malignant lesions from benign ones is fulfilled by using machine learning classifiers. This study enrolls peer-reviewed journal publications and presents technical details and pros and cons of each model. Furthermore, the findings, challenges and limitations are summarized and some clues on the future work are also given. Conclusively, CNN-based MBCD is at its early stage, and there is still a long way ahead in achieving the ultimate goal of using deep learning tools to facilitate clinical practice. This review benefits scientific researchers, industrial engineers, and those who are devoted to intelligent cancer diagnosis.",2019-03-01,9,1334,93,799
447,30325042,Prediction of clinically relevant drug-induced liver injury from structure using machine learning,"Drug-induced liver injury (DILI) is the most common cause of acute liver failure and often responsible for drug withdrawals from the market. Clinical manifestations vary, and toxicity may or may not appear dose-dependent. We present several machine-learning models (decision tree induction, k-nearest neighbor, support vector machines, artificial neural networks) for the prediction of clinically relevant DILI based solely on drug structure, with data taken from published DILI cases. Our models achieved corrected classification rates of up to 89%. We also studied the association of a drug's interaction with carriers, enzymes and transporters, and the relationship of defined daily doses with hepatotoxicity. The results presented here are useful as a screening tool both in a clinical setting in the assessment of DILI as well as in the early stages of drug development to rule out potentially hepatotoxic candidates.",2019-03-01,5,922,97,799
594,30881539,Machine Learning in Relation to Emergency Medicine Clinical and Operational Scenarios: An Overview,"Health informatics is a vital technology that holds great promise in the healthcare setting. We describe two prominent health informatics tools relevant to emergency care, as well as the historical background and the current state of informatics. We also identify recent research findings and practice changes. The recent advances in machine learning and natural language processing (NLP) are a prominent development in health informatics overall and relevant in emergency medicine (EM). A basic comprehension of machine-learning algorithms is the key to understand the recent usage of artificial intelligence in healthcare. We are using NLP more in clinical use for documentation. NLP has started to be used in research to identify clinically important diseases and conditions. Health informatics has the potential to benefit both healthcare providers and patients. We cover two powerful tools from health informatics for EM clinicians and researchers by describing the previous successes and challenges and conclude with their implications to emergency care.",2019-03-01,4,1060,98,799
2297,29672663,Analysis of long noncoding RNAs highlights region-specific altered expression patterns and diagnostic roles in Alzheimer's disease,"Increasing evidence has revealed the multiple roles of long noncoding RNAs (lncRNAs) in neurodevelopment, brain function and aging, and their dysregulation was implicated in many types of neurological diseases. However, expression pattern and diagnostic role of lncRNAs in Alzheimer's disease (AD) remain largely unknown and has gained significant attention. In this study, we performed a comparative analysis for lncRNA expression profiles in four brain regions in brain aging and AD. Our analysis revealed age- and disease-dependent region-specific lncRNA expression patterns in aging and AD. Moreover, we identified a panel of nine lncRNAs (termed LncSigAD9) in a discovery cohort of 114 samples using supervised machine learning and stepwise selection method. The LncSigAD9 was able to differentiate between AD and healthy controls with high diagnostic sensitivity and specificity both in the discovery cohort (86.3 and 89.5%) and the additional independent AD cohort (90.8 and 83.8%). The receiver operating characteristic curves for the LncSigAD9 were 0.863 and 0.939 for discovery and independent cohorts, respectively. Furthermore, the LncSigAD9 demonstrated higher diagnostic performance than nine-minus-one lncRNA signature and mRNA-based signature with a similar number of genes. In silico functional analysis indicated the involvement of lncRNA expression variation in brain development- and metabolism-related biological processes. Taken together, our study highlights the importance of lncRNAs in brain aging and AD, and demonstrated the utility of lncRNAs as a promising biomarker for early AD diagnosis and treatment.",2019-03-01,31,1633,130,799
582,30898903,Machine learning for data-driven discovery in solid Earth geoscience,"Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth's behavior and by the inaccessibility of nearly all of Earth's subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.",2019-03-01,11,734,68,799
2399,30866425,Automatic Pulmonary Nodule Detection Applying Deep Learning or Machine Learning Algorithms to the LIDC-IDRI Database: A Systematic Review,"The aim of this study was to provide an overview of the literature available on machine learning (ML) algorithms applied to the Lung Image Database Consortium Image Collection (LIDC-IDRI) database as a tool for the optimization of detecting lung nodules in thoracic CT scans. This systematic review was compiled according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Only original research articles concerning algorithms applied to the LIDC-IDRI database were included. The initial search yielded 1972 publications after removing duplicates, and 41 of these articles were included in this study. The articles were divided into two subcategories describing their overall architecture. The majority of feature-based algorithms achieved an accuracy >90% compared to the deep learning (DL) algorithms that achieved an accuracy in the range of 82.2%97.6%. In conclusion, ML and DL algorithms are able to detect lung nodules with a high level of accuracy, sensitivity, and specificity using ML, when applied to an annotated archive of CT scans of the lung. However, there is no consensus on the method applied to determine the efficiency of ML algorithms.",2019-03-01,13,1196,137,799
2300,29659698,Molecular subtyping of cancer: current status and moving toward clinical applications,"Cancer is a collection of genetic diseases, with large phenotypic differences and genetic heterogeneity between different types of cancers and even within the same cancer type. Recent advances in genome-wide profiling provide an opportunity to investigate global molecular changes during the development and progression of cancer. Meanwhile, numerous statistical and machine learning algorithms have been designed for the processing and interpretation of high-throughput molecular data. Molecular subtyping studies have allowed the allocation of cancer into homogeneous groups that are considered to harbor similar molecular and clinical characteristics. Furthermore, this has helped researchers to identify both actionable targets for drug design as well as biomarkers for response prediction. In this review, we introduce five frequently applied techniques for generating molecular data, which are microarray, RNA sequencing, quantitative polymerase chain reaction, NanoString and tissue microarray. Commonly used molecular data for cancer subtyping and clinical applications are discussed. Next, we summarize a workflow for molecular subtyping of cancer, including data preprocessing, cluster analysis, supervised classification and subtype characterizations. Finally, we identify and describe four major challenges in the molecular subtyping of cancer that may preclude clinical implementation. We suggest that standardized methods should be established to help identify intrinsic subgroup signatures and build robust classifiers that pave the way toward stratified treatment of cancer patients.",2019-03-01,10,1599,85,799
2524,30580109,Driving status of patients with generalized spike-wave on EEG but no clinical seizures,"Generalized spike-wave discharges (SWDs) are the hallmark of generalized epilepsy on the electroencephalogram (EEG). In clinically obvious cases, generalized SWDs produce myoclonic, atonic/tonic, or absence seizures with brief episodes of staring and behavioral unresponsiveness. However, some generalized SWDs have no obvious behavioral effects. A serious challenge arises when patients with no clinical seizures request driving privileges and licensure, yet their EEG shows generalized SWD. Specialized behavioral testing has demonstrated prolonged reaction times or missed responses during SWD, which may present a driving hazard even when patients or family members do not notice any deficits. On the other hand, some SWDs are truly asymptomatic in which case driving privileges should not be restricted. Clinicians often decide on driving privileges based on SWD duration or other EEG features. However, there are currently no empirically-validated guidelines for distinguishing generalized SWDs that are ""safe"" versus ""unsafe"" for driving. Here, we review the clinical presentation of generalized SWD and recent work investigating mechanisms of behavioral impairment during SWD with implications for driving safety. As a future approach, computational analysis of large sets of EEG data during simulated driving utilizing machine learning could lead to powerful methods to classify generalized SWD as safe vs. unsafe. This may ultimately provide more objective EEG criteria to guide decisions on driving safety in people with epilepsy.",2019-03-01,2,1541,86,799
422,30375164,Multiscale Modeling of Silk and Silk-Based Biomaterials-A Review,"Silk embodies outstanding material properties and biologically relevant functions achieved through a delicate hierarchical structure. It can be used to create high-performance, multifunctional, and biocompatible materials through mild processes and careful rational material designs. To achieve this goal, computational modeling has proven to be a powerful platform to unravel the causes of the excellent mechanical properties of silk, to predict the properties of the biomaterials derived thereof, and to assist in devising new manufacturing strategies. Fine-scale modeling has been done mainly through all-atom and coarse-grained molecular dynamics simulations, which offer a bottom-up description of silk. In this work, a selection of relevant contributions of computational modeling is reviewed to understand the properties of natural silk, and to the design of silk-based materials, especially combined with experimental methods. Future research directions are also pointed out, including approaches such as 3D printing and machine learning, that may enable a high throughput design and manufacturing of silk-based biomaterials.",2019-03-01,3,1133,64,799
2401,30861015,TECLA: A temperament and psychological type prediction framework from Twitter data,"Temperament and Psychological Types can be defined as innate psychological characteristics associated with how we relate with the world, and often influence our study and career choices. Furthermore, understanding these features help us manage conflicts, develop leadership, improve teaching and many other skills. Assigning temperament and psychological types is usually made by filling specific questionnaires. However, it is possible to identify temperamental characteristics from a linguistic and behavioral analysis of social media data from a user. Thus, machine-learning algorithms can be used to learn from a user's social media data and infer his/her behavioral type. This paper initially provides a brief historical review of theories on temperament and then brings a survey of research aimed at predicting temperament and psychological types from social media data. It follows with the proposal of a framework to predict temperament and psychological types from a linguistic and behavioral analysis of Twitter data. The proposed framework infers temperament types following the David Keirsey's model, and psychological types based on the MBTI model. Various data modelling and classifiers are used. The results showed that Random Forests with the LIWC technique can predict with 96.46% of accuracy the Artisan temperament, 92.19% the Guardian temperament, 78.68% the Idealist, and 83.82% the Rational temperament. The MBTI results also showed that Random Forests achieved a better performance with an accuracy of 82.05% for the E/I pair, 88.38% for the S/N pair, 80.57% for the T/F pair, and 78.26% for the J/P pair.",2019-03-01,0,1627,82,799
2491,30654138,An extensive experimental survey of regression methods,"Regression is a very relevant problem in machine learning, with many different available approaches. The current work presents a comparison of a large collection composed by 77 popular regression models which belong to 19 families: linear and generalized linear models, generalized additive models, least squares, projection methods, LASSO and ridge regression, Bayesian models, Gaussian processes, quantile regression, nearest neighbors, regression trees and rules, random forests, bagging and boosting, neural networks, deep learning and support vector regression. These methods are evaluated using all the regression datasets of the UCI machine learning repository (83 datasets), with some exceptions due to technical reasons. The experimental work identifies several outstanding regression models: the M5 rule-based model with corrections based on nearest neighbors (cubist), the gradient boosted machine (gbm), the boosting ensemble of regression trees (bstTree) and the M5 regression tree. Cubist achieves the best squared correlation ( R2) in 15.7% of datasets being very near to it, with difference below 0.2 for 89.1% of datasets, and the median of these differences over the dataset collection is very low (0.0192), compared e.g. to the classical linear regression (0.150). However, cubist is slow and fails in several large datasets, while other similar regression models as M5 never fail and its difference to the best R2 is below 0.2 for 92.8% of datasets. Other well-performing regression models are the committee of neural networks (avNNet), extremely randomized regression trees (extraTrees, which achieves the best R2 in 33.7% of datasets), random forest (rf) and -support vector regression (svr), but they are slower and fail in several datasets. The fastest regression model is least angle regression lars, which is 70 and 2,115 times faster than M5 and cubist, respectively. The model which requires least memory is non-negative least squares (nnls), about 2 GB, similarly to cubist, while M5 requires about 8 GB. For 97.6% of datasets there is a regression model among the 10 bests which is very near (difference below 0.1) to the best R2, which increases to 100% allowing differences of 0.2. Therefore, provided that our dataset and model collection are representative enough, the main conclusion of this study is that, for a new regression problem, some model in our top-10 should achieve R2 near to the best attainable for that problem.",2019-03-01,8,2461,54,799
586,30897793,Sketching the Power of Machine Learning to Decrypt a Neural Systems Model of Behavior,"Uncovering brain-behavior mechanisms is the ultimate goal of neuroscience. A formidable amount of discoveries has been made in the past 50 years, but the very essence of brain-behavior mechanisms still escapes us. The recent exploitation of machine learning (ML) tools in neuroscience opens new avenues for illuminating these mechanisms. A key advantage of ML is to enable the treatment of large data, combing highly complex processes. This essay provides a glimpse of how ML tools could test a heuristic neural systems model of motivated behavior, the triadic neural systems model, which was designed to understand behavioral transitions in adolescence. This essay previews analytic strategies, using fictitious examples, to demonstrate the potential power of ML to decrypt the neural networks of motivated behavior, generically and across development. Of note, our intent is not to provide a tutorial for these analyses nor a pipeline. The ultimate objective is to relate, as simply as possible, how complex neuroscience constructs can benefit from ML methods for validation and further discovery. By extension, the present work provides a guide that can serve to query the mechanisms underlying the contributions of prefrontal circuits to emotion regulation. The target audience concerns mainly clinical neuroscientists. As a caveat, this broad approach leaves gaps, for which references to comprehensive publications are provided.",2019-03-01,1,1434,85,799
573,30935123,Precision Livestock Farming in Swine Welfare: A Review for Swine Practitioners,"The burgeoning research and applications of technological advances are launching the development of precision livestock farming. Through sensors (cameras, microphones and accelerometers), images, sounds and movements are combined with algorithms to non-invasively monitor animals to detect their welfare and predict productivity. In turn, this remote monitoring of livestock can provide quantitative and early alerts to situations of poor welfare requiring the stockperson's attention. While swine practitioners' skills include translation of pig data entry into pig health and well-being indices, many do not yet have enough familiarity to advise their clients on the adoption of precision livestock farming practices. This review, intended for swine veterinarians and specialists, (1) includes an introduction to algorithms and machine learning, (2) summarizes current literature on relevant sensors and sensor network systems, and drawing from industry pig welfare audit criteria, (3) explains how these applications can be used to improve swine welfare and meet current pork production stakeholder expectations. Swine practitioners, by virtue of their animal and client advocacy roles, interpretation of benchmarking data, and stewardship in regulatory and traceability programs, can play a broader role as advisors in the transfer of precision livestock farming technology, and its implications to their clients.",2019-03-01,9,1417,78,799
570,30949431,Advances in automated tongue diagnosis techniques,"Tongue diagnosis can be an effective, noninvasive method to perform an auxiliary diagnosis any time anywhere, which can support the global need in the primary healthcare system. This work reviews the recent advances in tongue diagnosis, which is a significant constituent of traditional oriental medicinal technology, and explores the literature to evaluate the works done on the various aspects of computerized tongue diagnosis, namely preprocessing, tongue detection, segmentation, feature extraction, tongue analysis, especially in traditional Chinese medicine (TCM). In spite of huge volume of work done on automatic tongue diagnosis (ATD), there is a lack of adequate survey, especially to combine it with the current diagnosis trends. This paper studies the merits, capabilities, and associated research gaps in current works on ATD systems. After exploring the algorithms used in tongue diagnosis, the current trend and global requirements in health domain motivates us to propose a conceptual framework for the automated tongue diagnostic system on mobile enabled platform. This framework will be able to connect tongue diagnosis with the future point-of-care health system.",2019-03-01,1,1182,49,799
589,30890859,Visual Analytics of Genomic and Cancer Data: A Systematic Review,"Visual analytics and visualisation can leverage the human perceptual system to interpret and uncover hidden patterns in big data. The advent of next-generation sequencing technologies has allowed the rapid production of massive amounts of genomic data and created a corresponding need for new tools and methods for visualising and interpreting these data. Visualising genomic data requires not only simply plotting of data but should also offer a decision or a choice about what the message should be conveyed in the particular plot; which methodologies should be used to represent the results must provide an easy, clear, and accurate way to the clinicians, experts, or researchers to interact with the data. Genomic data visual analytics is rapidly evolving in parallel with advances in high-throughput technologies such as artificial intelligence (AI) and virtual reality (VR). Personalised medicine requires new genomic visualisation tools, which can efficiently extract knowledge from the genomic data and speed up expert decisions about the best treatment of individual patient's needs. However, meaningful visual analytics of such large genomic data remains a serious challenge. This article provides a comprehensive systematic review and discussion on the tools, methods, and trends for visual analytics of cancer-related genomic data. We reviewed methods for genomic data visualisation including traditional approaches such as scatter plots, heatmaps, coordinates, and networks, as well as emerging technologies using AI and VR. We also demonstrate the development of genomic data visualisation tools over time and analyse the evolution of visualising genomic data.",2019-03-01,1,1674,64,799
585,30898208,Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review,"Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with ""big data"" from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.",2019-03-01,40,897,79,799
568,30950746,Machine Learning Approaches to Analyze Speech-Evoked Neurophysiological Responses,"Purpose Speech-evoked neurophysiological responses are often collected to answer clinically and theoretically driven questions concerning speech and language processing. Here, we highlight the practical application of machine learning (ML)-based approaches to analyzing speech-evoked neurophysiological responses. Method Two categories of ML-based approaches are introduced: decoding models, which generate a speech stimulus output using the features from the neurophysiological responses, and encoding models, which use speech stimulus features to predict neurophysiological responses. In this review, we focus on (a) a decoding model classification approach, wherein speech-evoked neurophysiological responses are classified as belonging to 1 of a finite set of possible speech events (e.g., phonological categories), and (b) an encoding model temporal response function approach, which quantifies the transformation of a speech stimulus feature to continuous neural activity. Results We illustrate the utility of the classification approach to analyze early electroencephalographic (EEG) responses to Mandarin lexical tone categories from a traditional experimental design, and to classify EEG responses to English phonemes evoked by natural continuous speech (i.e., an audiobook) into phonological categories (plosive, fricative, nasal, and vowel). We also demonstrate the utility of temporal response function to predict EEG responses to natural continuous speech from acoustic features. Neural metrics from the 3 examples all exhibit statistically significant effects at the individual level. Conclusion We propose that ML-based approaches can complement traditional analysis approaches to analyze neurophysiological responses to speech signals and provide a deeper understanding of natural speech and language processing using ecologically valid paradigms in both typical and clinical populations.",2019-03-01,3,1904,81,799
2506,30620934,Electronic nose: a non-invasive technology for breath analysis of diabetes and lung cancer patients,"In human exhaled breath, more than 3000 volatile organic compounds (VOCs) are found, which are directly or indirectly related to internal biochemical processes in the body. Electronic noses (E-noses) could play a potential role in screening/analyzing various respiratory and systemic diseases by studying breath signatures. An E-nose integrates a sensor array and an artificial neural network that responds to specific patterns of VOCs, and thus can act as a non-invasive technology for disease monitoring. The gold standard blood glucose monitoring test for diabetes diagnostics is invasive and highly uncomfortable. This contributes to the massive need for technologies which are non-invasive and can be used as an alternative to blood measurements for glucose detection. While lung cancer is one of the deadliest cancers with the highest death rate and an extremely high yearly global burden, the conventional diagnosis means, such as sputum cytology, chest radiography, or computed tomography, do not support wide-range population screening. A few standard non-invasive techniques, such as mass spectrometry and gas chromatography, are expensive, non-portable, and require skilled personnel for operation and are again not suitable for large-scale screening. Breath contains markers for both diabetes and lung cancer along with markers for several diseases and thus, a non-invasive technique such as the E-nose would greatly improve analysis procedures over existing invasive methods. This review shows the state-of-the-art technologies for VOC detection and machine learning approaches for two clinical models: diabetes and lung cancer detection.",2019-03-01,15,1651,99,799
498,31051023,Digital imaging applications and informatics in dermatology,"In this chapter, we present the use of whole slide imaging (WSI) and dermoscopy in the field of dermatology. Image digitization has allowed for increasing computer-assisted clinical decision-making. An introduction to common digital imaging data sources such as WSI and dermoscopy is provided. We also review some commonly used image quantification methods and their potential applications in dermatology. Finally, we review how machine learning approaches utilize novel large dermatology image datasets.",2019-03-01,0,504,59,799
2418,30827579,Malformations of cortical development: The role of 7-Tesla magnetic resonance imaging in diagnosis,"Comparison studies between 7T and 1.5 or 3T magnetic resonance imaging (MRI) have demonstrated the added value of ultra-high field (UHF) MRI to better identify, delineate and characterize malformations of cortical development (MCD), and to disambiguate doubtful findings observed at lower field strengths. High resolution structural sequences such as magnetization prepared two rapid acquisition gradient echoes (MP2RAGE), fluid and white matter suppression MP2RAGE (FLAWS), and susceptibility-weighted imaging (SWI) appear to be key to the improvement of MCD diagnosis in clinical practice. 7T MRI offers not only images of high resolution and contrast but also provides many quantitative approaches capable of acting as more efficient probes of microstructure and ameliorating the categorization of MCDs. Post-processing of multiparametric ultra-high resolution and quantitative data may also be used to improve automated detection of MCD via machine learning. Therefore, 7T MRI can be considered as a useful tool in the presurgical evaluation of drug-resistant partial epilepsies, particularly, but not exclusively, in cases of normal appearing conventional MRI. It also opens many perspectives in the fields of in vivo histology and computational anatomy.",2019-03-01,3,1259,98,799
138,30105410,"Demystification of AI-driven medical image interpretation: past, present and future","The recent explosion of 'big data' has ushered in a new era of artificial intelligence (AI) algorithms in every sphere of technological activity, including medicine, and in particular radiology. However, the recent success of AI in certain flagship applications has, to some extent, masked decades-long advances in computational technology development for medical image analysis. In this article, we provide an overview of the history of AI methods for radiological image analysis in order to provide a context for the latest developments. We review the functioning, strengths and limitations of more classical methods as well as of the more recent deep learning techniques. We discuss the unique characteristics of medical data and medical science that set medicine apart from other technological domains in order to highlight not only the potential of AI in radiology but also the very real and often overlooked constraints that may limit the applicability of certain AI methods. Finally, we provide a comprehensive perspective on the potential impact of AI on radiology and on how to evaluate it not only from a technical point of view but also from a clinical one, so that patients can ultimately benefit from it. KEY POINTS:  Artificial intelligence (AI) research in medical imaging has a long history  The functioning, strengths and limitations of more classical AI methods is reviewed, together with that of more recent deep learning methods.  A perspective is provided on the potential impact of AI on radiology and on its evaluation from both technical and clinical points of view.",2019-03-01,21,1593,83,799
2922,29897410,iProt-Sub: a comprehensive package for accurately mapping and predicting protease-specific substrates and cleavage sites,"Regulation of proteolysis plays a critical role in a myriad of important cellular processes. The key to better understanding the mechanisms that control this process is to identify the specific substrates that each protease targets. To address this, we have developed iProt-Sub, a powerful bioinformatics tool for the accurate prediction of protease-specific substrates and their cleavage sites. Importantly, iProt-Sub represents a significantly advanced version of its successful predecessor, PROSPER. It provides optimized cleavage site prediction models with better prediction performance and coverage for more species-specific proteases (4 major protease families and 38 different proteases). iProt-Sub integrates heterogeneous sequence and structural features and uses a two-step feature selection procedure to further remove redundant and irrelevant features in an effort to improve the cleavage site prediction accuracy. Features used by iProt-Sub are encoded by 11 different sequence encoding schemes, including local amino acid sequence profile, secondary structure, solvent accessibility and native disorder, which will allow a more accurate representation of the protease specificity of approximately 38 proteases and training of the prediction models. Benchmarking experiments using cross-validation and independent tests showed that iProt-Sub is able to achieve a better performance than several existing generic tools. We anticipate that iProt-Sub will be a powerful tool for proteome-wide prediction of protease-specific substrates and their cleavage sites, and will facilitate hypothesis-driven functional interrogation of protease-specific substrate cleavage and proteolytic events.",2019-03-01,40,1699,120,799
499,31051022,"The role of public challenges and data sets towards algorithm development, trust, and use in clinical practice","In the past decade, machine learning and artificial intelligence have made significant advancements in pattern analysis, including speech and natural language processing, image recognition, object detection, facial recognition, and action categorization. Indeed, in many of these applications, accuracy has reached or exceeded human levels of performance. Subsequently, a multitude of studies have begun to examine the application of these technologies to health care, and in particular, medical image analysis. Perhaps the most difficult subdomain involves skin imaging because of the lack of standards around imaging hardware, technique, color, and lighting conditions. In addition, unlike radiological images, skin image appearance can be significantly affected by skin tone as well as the broad range of diseases. Furthermore, automated algorithm development relies on large high-quality annotated image data sets that incorporate the breadth of this circumstantial and diagnostic variety. These issues, in combination with unique complexities regarding integrating artificial intelligence systems into a clinical workflow, have led to difficulty in using these systems to improve sensitivity and specificity of skin diagnostics in health care networks around the world. In this article, we summarize recent advancements in machine learning, with a focused perspective on the role of public challenges and data sets on the progression of these technologies in skin imaging. In addition, we highlight the remaining hurdles toward effective implementation of technologies to the clinical workflow and discuss how public challenges and data sets can catalyze the development of solutions.",2019-03-01,1,1689,110,799
557,30972100,Recent Advances of Deep Learning in Bioinformatics and Computational Biology,"Extracting inherent valuable knowledge from omics big data remains as a daunting problem in bioinformatics and computational biology. Deep learning, as an emerging branch from machine learning, has exhibited unprecedented performance in quite a few applications from academia and industry. We highlight the difference and similarity in widely utilized models in deep learning studies, through discussing their basic structures, and reviewing diverse applications and disadvantages. We anticipate the work can serve as a meaningful perspective for further development of its theory, algorithm and application in bioinformatic and computational biology.",2019-03-01,19,651,76,799
556,30972108,Machine Learning SNP Based Prediction for Precision Medicine,"In the past decade, precision genomics based medicine has emerged to provide tailored and effective healthcare for patients depending upon their genetic features. Genome Wide Association Studies have also identified population based risk genetic variants for common and complex diseases. In order to meet the full promise of precision medicine, research is attempting to leverage our increasing genomic understanding and further develop personalized medical healthcare through ever more accurate disease risk prediction models. Polygenic risk scoring and machine learning are two primary approaches for disease risk prediction. Despite recent improvements, the results of polygenic risk scoring remain limited due to the approaches that are currently used. By contrast, machine learning algorithms have increased predictive abilities for complex disease risk. This increase in predictive abilities results from the ability of machine learning algorithms to handle multi-dimensional data. Here, we provide an overview of polygenic risk scoring and machine learning in complex disease risk prediction. We highlight recent machine learning application developments and describe how machine learning approaches can lead to improved complex disease prediction, which will help to incorporate genetic features into future personalized healthcare. Finally, we discuss how the future application of machine learning prediction models might help manage complex disease by providing tissue-specific targets for customized, preventive interventions.",2019-03-01,15,1538,60,799
555,30972291,Radiomics and Machine Learning for Radiotherapy in Head and Neck Cancers,"Introduction: An increasing number of parameters can be considered when making decisions in oncology. Tumor characteristics can also be extracted from imaging through the use of radiomics and add to this wealth of clinical data. Machine learning can encompass these parameters and thus enhance clinical decision as well as radiotherapy workflow. Methods: We performed a description of machine learning applications at each step of treatment by radiotherapy in head and neck cancers. We then performed a systematic review on radiomics and machine learning outcome prediction models in head and neck cancers. Results: Machine Learning has several promising applications in treatment planning with automatic organ at risk delineation improvements and adaptative radiotherapy workflow automation. It may also provide new approaches for Normal Tissue Complication Probability models. Radiomics may provide additional data on tumors for improved machine learning powered predictive models, not only on survival, but also on risk of distant metastasis, in field recurrence, HPV status and extra nodal spread. However, most studies provide preliminary data requiring further validation. Conclusion: Promising perspectives arise from machine learning applications and radiomics based models, yet further data are necessary for their implementation in daily care.",2019-03-01,12,1353,72,799
2422,30824244,Recent Developments in the Treatment of Depression,"The cognitive and behavioral interventions can be as efficacious as antidepressant medications and more enduring, but some patients will be more likely to respond to one than the other. Recent work has focused on developing sophisticated selection algorithms using machine-learning approaches that answer the question, ""What works best for whom?"" Moreover, the vast majority of people suffering from depression reside in low- and middle-income countries where access to either psychotherapy or medications is virtually nonexistent. Great strides have been made in training nonspecialist providers (known as task sharing) to overcome this gap. Finally, recent work growing out of evolutionary psychology suggests that antidepressant medications may suppress symptoms at the expense of prolonging the underlying episode so as to increase the risk of relapse whenever someone tries to stop. We address each of these developments and their cumulative implications.",2019-03-01,4,960,50,799
500,31051017,"Bioinformatic applications in psoriasis: genetics, transcriptomics, and microbiomics","Bioinformatics uses computationally intensive approaches to make sense of complex biological data sets. Here we review the role of bioinformatics in 3 areas of biology: genetics, transcriptomics, and microbiomics. Examples of bioinformatics in each area are given with respect to psoriasis and psoriatic arthritis, related inflammatory disorders at the forefront of bioinformatic research in dermatology. While bioinformatic technologies and analyses have traditionally been developed and deployed in siloes, the field of integrative omics is on the horizon. Powered by the advent of machine learning, bioinformatic integration of large data sets has the potential to dramatically revolutionize our knowledge of pathogenetic mechanisms and therapeutic targets.",2019-03-01,0,760,84,799
2984,29697740,Pattern recognition analysis on long noncoding RNAs: a tool for prediction in plants,"Motivation:                    Long noncoding RNAs (lncRNAs) correspond to a eukaryotic noncoding RNA class that gained great attention in the past years as a higher layer of regulation for gene expression in cells. There is, however, a lack of specific computational approaches to reliably predict lncRNA in plants, which contrast the variety of prediction tools available for mammalian lncRNAs. This distinction is not that obvious, given that biological features and mechanisms generating lncRNAs in the cell are likely different between animals and plants. Considering this, we present a machine learning analysis and a classifier approach called RNAplonc (https://github.com/TatianneNegri/RNAplonc/) to identify lncRNAs in plants.              Results:                    Our feature selection analysis considered 5468 features, and it used only 16 features to robustly identify lncRNA with the REPTree algorithm. That was the base to create the model and train it with lncRNA and mRNA data from five plant species (thale cress, cucumber, soybean, poplar and Asian rice). After an extensive comparison with other tools largely used in plants (CPC, CPC2, CPAT and PLncPRO), we found that RNAplonc produced more reliable lncRNA predictions from plant transcripts with 87.5% of the best result in eight tests in eight species from the GreeNC database and four independent studies in monocotyledonous (Brachypodium) and eudicotyledonous (Populus and Gossypium) species.",2019-03-01,10,1470,84,799
2478,30682710,Deep learning in spiking neural networks,"In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.",2019-03-01,20,1588,40,799
509,31044598,Localizing epileptogenic regions using high-frequency oscillations and machine learning,"Pathological high frequency oscillations (HFOs) are putative neurophysiological biomarkers of epileptogenic brain tissue. Utilizing HFOs for epilepsy surgery planning offers the promise of improved seizure outcomes for patients with medically refractory epilepsy. This review discusses possible machine learning strategies that can be applied to HFO biomarkers to better identify epileptogenic regions. We discuss the role of HFO rate, and utilizing features such as explicit HFO properties (spectral content, duration, and power) and phase-amplitude coupling for distinguishing pathological HFO (pHFO) events from physiological HFO events. In addition, the review highlights the importance of neuroanatomical localization in machine learning strategies.",2019-04-01,4,754,87,768
503,31049075,Tools to reverse-engineer multicellular systems: case studies using the fruit fly,"Reverse-engineering how complex multicellular systems develop and function is a grand challenge for systems bioengineers. This challenge has motivated the creation of a suite of bioengineering tools to develop increasingly quantitative descriptions of multicellular systems. Here, we survey a selection of these tools including microfluidic devices, imaging and computer vision techniques. We provide a selected overview of the emerging cross-talk between engineering methods and quantitative investigations within developmental biology. In particular, the review highlights selected recent examples from the Drosophila system, an excellent platform for understanding the interplay between genetics and biophysics. In sum, the integrative approaches that combine multiple advances in these fields are increasingly necessary to enable a deeper understanding of how to analyze both natural and synthetic multicellular systems.",2019-04-01,3,924,81,768
400,30420244,Computational methods for Gene Regulatory Networks reconstruction and analysis: A review,"In the recent years, the vast amount of genetic information generated by new-generation approaches, have led to the need of new data handling methods. The integrative analysis of diverse-nature gene information could provide a much-sought overview to study complex biological systems and processes. In this sense, Gene Regulatory Networks (GRN) arise as an increasingly-promising tool for the modelling and analysis of biological processes. This review is an attempt to summarize the state of the art in the field of GRNs. Essential points in the field are addressed, thereof: (a) the type of data used for network generation, (b) machine learning methods and tools used for network generation, (c) model optimization and (d) computational approaches used for network validation. This survey is intended to provide an overview of the subject for readers to improve their knowledge in the field of GRN for future research.",2019-04-01,12,921,88,768
483,30248307,The current state of artificial intelligence in ophthalmology,"Artificial intelligence (AI) is a branch of computer science that deals with the development of algorithms that seek to simulate human intelligence. We provide an overview of the basic principles in AI that are essential to the understanding of AI and its application in health care. We also present a descriptive analysis of the current state of AI in various fields of medicine, especially ophthalmology. Finally, we review the potential limitations and challenges that come along with the development and implementation of this new technology that will likely play a major role in clinical medicine in the near future.",2019-04-01,10,621,61,768
470,30290208,Translational machine learning for psychiatric neuroimaging,"Despite its initial promise, neuroimaging has not been widely translated into clinical psychiatry to assist in the prediction of diagnoses, prognoses, and optimal therapeutic strategies. Machine learning approaches may enhance the translational potential of neuroimaging because they specifically focus on overcoming biases by optimizing the generalizability of pipelines that measure complex brain patterns to predict targets at a single-subject level. This article introduces some fundamentals of a translational machine learning approach before selectively reviewing literature to-date. Promising initial results are then balanced by the description of limitations that should be considered in order to interpret existing research and maximize the possibility of future translation. Future directions are then presented in order to inspire further research and progress the field towards clinical translation.",2019-04-01,9,912,59,768
2580,30471192,Computational methods and tools to predict cytochrome P450 metabolism for drug discovery,"In this review, we present important, recent developments in the computational prediction of cytochrome P450 (CYP) metabolism in the context of drug discovery. We discuss in silico models for the various aspects of CYP metabolism prediction, including CYP substrate and inhibitor predictors, site of metabolism predictors (i.e., metabolically labile sites within potential substrates) and metabolite structure predictors. We summarize the different approaches taken by these models, such as rule-based methods, machine learning, data mining, quantum chemical methods, molecular interaction fields, and docking. We highlight the scope and limitations of each method and discuss future implications for the field of metabolism prediction in drug discovery.",2019-04-01,10,754,88,768
2581,30470933,Precision immunoprofiling by image analysis and artificial intelligence,"Clinical success of immunotherapy is driving the need for new prognostic and predictive assays to inform patient selection and stratification. This requirement can be met by a combination of computational pathology and artificial intelligence. Here, we critically assess computational approaches supporting the development of a standardized methodology in the assessment of immune-oncology biomarkers, such as PD-L1 and immune cell infiltrates. We examine immunoprofiling through spatial analysis of tumor-immune cell interactions and multiplexing technologies as a predictor of patient response to cancer treatment. Further, we discuss how integrated bioinformatics can enable the amalgamation of complex morphological phenotypes with the multiomics datasets that drive precision medicine. We provide an outline to machine learning (ML) and artificial intelligence tools and illustrate fields of application in immune-oncology, such as pattern-recognition in large and complex datasets and deep learning approaches for survival analysis. Synergies of surgical pathology and computational analyses are expected to improve patient stratification in immuno-oncology. We propose that future clinical demands will be best met by (1) dedicated research at the interface of pathology and bioinformatics, supported by professional societies, and (2) the integration of data sciences and digital image analysis in the professional education of pathologists.",2019-04-01,20,1449,71,768
104,30201325,A survey on computer-assisted Parkinson's Disease diagnosis,"Background and objective:                    In this work, we present a systematic review concerning the recent enabling technologies as a tool to the diagnosis, treatment and better quality of life of patients diagnosed with Parkinson's Disease (PD), as well as an analysis of future trends on new approaches to this end.              Methods:                    In this review, we compile a number of works published at some well-established databases, such as Science Direct, IEEEXplore, PubMed, Plos One, Multidisciplinary Digital Publishing Institute (MDPI), Association for Computing Machinery (ACM), Springer and Hindawi Publishing Corporation. Each selected work has been carefully analyzed in order to identify its objective, methodology and results.              Results:                    The review showed the majority of works make use of signal-based data, which are often acquired by means of sensors. Also, we have observed the increasing number of works that employ virtual reality and e-health monitoring systems to increase the life quality of PD patients. Despite the different approaches found in the literature, almost all of them make use of some sort of machine learning mechanism to aid the automatic PD diagnosis.              Conclusions:                    The main focus of this survey is to consider computer-assisted diagnosis, and how effective they can be when handling the problem of PD identification. Also, the main contribution of this review is to consider very recent works only, mainly from 2015 and 2016.",2019-04-01,6,1546,59,768
2405,30851654,Backpropagation through time and the brain,"It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike.",2019-04-01,7,1072,42,768
2312,29601896,Differentiating between bipolar and unipolar depression in functional and structural MRI studies,"Distinguishing depression in bipolar disorder (BD) from unipolar depression (UD) solely based on clinical clues is difficult, which has led to the exploration of promising neural markers in neuroimaging measures for discriminating between BD depression and UD. In this article, we review structural and functional magnetic resonance imaging (MRI) studies that directly compare UD and BD depression based on neuroimaging modalities including functional MRI studies on regional brain activation or functional connectivity, structural MRI on gray or white matter morphology, and pattern classification analyses using a machine learning approach. Numerous studies have reported distinct functional and structural alterations in emotion- or reward-processing neural circuits between BD depression and UD. Different activation patterns in neural networks including the amygdala, anterior cingulate cortex (ACC), prefrontal cortex (PFC), and striatum during emotion-, reward-, or cognition-related tasks have been reported between BD and UD. A stronger functional connectivity pattern in BD was pronounced in default mode and in frontoparietal networks and brain regions including the PFC, ACC, parietal and temporal regions, and thalamus compared to UD. Gray matter volume differences in the ACC, hippocampus, amygdala, and dorsolateral prefrontal cortex (DLPFC) have been reported between BD and UD, along with a thinner DLPFC in BD compared to UD. BD showed reduced integrity in the anterior part of the corpus callosum and posterior cingulum compared to UD. Several studies performed pattern classification analysis using structural and functional MRI data to distinguish between UD and BD depression using a supervised machine learning approach, which yielded a moderate level of accuracy in classification.",2019-04-01,21,1805,96,768
150,30092623,Prediction of Acquired Taxane Resistance Using a Personalized Pathway-Based Machine Learning Method,"Purpose:                    This study was conducted to develop and validate an individualized prediction model for automated detection of acquired taxane resistance (ATR).              Materials and methods:                    Penalized regression, combinedwith an individualized pathway score algorithm,was applied to construct a predictive model using publically available genomic cohorts of ATR and intrinsic taxane resistance (ITR). To develop a model with enhanced generalizability, we merged multiple ATR studies then updated the learning parameter via robust cross-study validation.              Results:                    For internal cross-study validation, the ATR model produced a perfect performance with an overall area under the receiver operating curve (AUROC) of 1.000 with an area under the precision-recall curve (AUPRC) of 1.000, a Brier score of 0.007, a sensitivity and a specificity of 100%. The model showed an excellent performance on two independent blind ATR cohorts (overall AUROC of 0.940, AUPRC of 0.940, a Brier score of 0.127). When we applied our algorithm to two large-scale pharmacogenomic resources for ITR, the Cancer Genome Project (CGP) and the Cancer Cell Line Encyclopedia (CCLE), an overall ITR cross-study AUROC was 0.70, which is a far better accuracy than an almost random level reported by previous studies. Furthermore, this model had a high transferability on blind ATR cohorts with an AUROC of 0.69, suggesting that general predictive features may be at work across both ITR and ATR.              Conclusion:                    We successfully constructed a multi-study-derived personalized prediction model for ATR with excellent accuracy, generalizability, and transferability.",2019-04-01,2,1729,99,768
2249,31131140,Analyzing and Visualizing Knowledge Structures of Health Informatics from 1974 to 2018: A Bibliometric and Social Network Analysis,"Objectives:                    This paper aims to provide a theoretical clarification of the health informatics field by conducting a quantitative review analysis of the health informatics literature. And this paper aims to map scientific networks; to uncover the explicit and hidden patterns, knowledge structures, and sub-structures in scientific networks; to track the flow and burst of scientific topics; and to discover what effects they have on the scientific growth of health informatics.              Methods:                    This study was a quantitative literature review of the health informatics field, employing text mining and bibliometric research methods. This paper reviews 30,115 articles with health informatics as their topic, which are indexed in the Web of Science Core Collection Database from 1974 to 2018. This study analyzed and mapped four networks: author co-citation network, co-occurring author keywords and keywords plus, co-occurring subject categories, and country co-citation network. We used CiteSpace 5.3 and VOSviewer to analyze data, and we used Gephi 0.9.2 and VOSviewer to visualize the networks.              Results:                    This study found that the three major themes of the literature from 1974 to 2018 were the utilization of computer science in healthcare, the impact of health informatics on patient safety and the quality of healthcare, and decision support systems. The study found that, since 2016, health informatics has entered a new era to provide predictive, preventative, personalized, and participatory healthcare systems.              Conclusions:                    This study found that the future strands of research may be patient-generated health data, deep learning algorithms, quantified self and self-tracking tools, and Internet of Things based decision support systems.",2019-04-01,8,1851,130,768
2419,30825704,An integrative computational architecture for object-driven cortex,"Computational architecture for object-driven cortex Objects in motion activate multiple cortical regions in every lobe of the human brain. Do these regions represent a collection of independent systems, or is there an overarching functional architecture spanning all of object-driven cortex? Inspired by recent work in artificial intelligence (AI), machine learning, and cognitive science, we consider the hypothesis that these regions can be understood as a coherent network implementing an integrative computational system that unifies the functions needed to perceive, predict, reason about, and plan with physical objects-as in the paradigmatic case of using or making tools. Our proposal draws on a modeling framework that combines multiple AI methods, including causal generative models, hybrid symbolic-continuous planning algorithms, and neural recognition networks, with object-centric, physics-based representations. We review evidence relating specific components of our proposal to the specific regions that comprise object-driven cortex, and lay out future research directions with the goal of building a complete functional and mechanistic account of this system.",2019-04-01,2,1177,66,768
2396,30870615,What does the mind learn? A comparison of human and machine learning representations,"We present a brief review of modern machine learning techniques and their use in models of human mental representations, detailing three notable branches: spatial methods, logical methods and artificial neural networks. Each of these branches contains an extensive set of systems, and demonstrate accurate emulations of human learning of categories, concepts and language, despite substantial differences in operation. We suggest that continued applications will allow cognitive researchers the ability to model the complex real-world problems where machine learning has recently been successful, providing more complete behavioural descriptions. This will, however, also require careful consideration of appropriate algorithmic constraints alongside these methods in order to find a combination which captures both the strengths and weaknesses of human cognition.",2019-04-01,0,864,84,768
2558,30529148,The algorithmic architecture of exploration in the human brain,"Balancing exploration and exploitation is one of the central problems in reinforcement learning. We review recent studies that have identified multiple algorithmic strategies underlying exploration. In particular, humans use a combination of random and uncertainty-directed exploration strategies, which rely on different brain systems, have different developmental trajectories, and are sensitive to different task manipulations. Humans are also able to exploit sophisticated structural knowledge to aid their exploration, such as information about correlations between options. New computational models, drawing inspiration from machine learning, have begun to formalize these ideas and offer new ways to understand the neural basis of reinforcement learning.",2019-04-01,15,761,62,768
513,31040236,Meta-analysis of Plasmodium falciparum var Signatures Contributing to Severe Malaria in African Children and Indian Adults,"The clinical presentation of severe Plasmodium falciparum malaria differs between children and adults, but the mechanistic basis for this remains unclear. Contributing factors to disease severity include total parasite biomass and the diverse cytoadhesive properties mediated by the polymorphic var gene parasite ligand family displayed on infected erythrocytes. To explore these factors, we performed a multicohort analysis of the contribution of var expression and parasite biomass to severe malaria in two previously published pediatric cohorts in Tanzania and Malawi and an adult cohort in India. Machine learning analysis revealed independent and complementary roles for var adhesion types and parasite biomass in adult and pediatric severe malaria and showed that similar var profiles, including upregulation of group A and DC8 var, predict severe malaria in adults and children. Among adults, patients with multiorgan complications presented infections with significantly higher parasite biomass without significant differences in var adhesion types. Conversely, pediatric patients with specific complications showed distinct var signatures. Cerebral malaria patients showed broadly increased expression of var genes, in particular group A and DC8 var, while children with severe malaria anemia were classified based on high transcription of DC8 var only. This study represents the first large multisite meta-analysis of var expression, and it demonstrates the presence of common var profiles in severe malaria patients of different ages across distant geographical sites, as well as syndrome-specific disease signatures. The complex associations between parasite biomass, var adhesion type, and clinical presentation revealed here represent the most comprehensive picture so far of the relationship between cytoadhesion, parasite load, and clinical syndrome.IMPORTANCE P. falciparum malaria can cause multiple disease complications that differ by patient age. Previous studies have attempted to address the roles of parasite adhesion and biomass in disease severity; however, these studies have been limited to single geographical sites, and there is limited understanding of how parasite adhesion and biomass interact to influence disease manifestations. In this meta-analysis, we compared parasite disease determinants in African children and Indian adults. This study demonstrates that parasite biomass and specific subsets of var genes are independently associated with detrimental outcomes in both childhood and adult malaria. We also explored how parasite var adhesion types and biomass play different roles in the development of specific severe malaria pathologies, including childhood cerebral malaria and multiorgan complications in adults. This work represents the largest study to date of the role of both var adhesion types and biomass in severe malaria.",2019-04-01,5,2874,122,768
2429,30810430,Introduction to artificial intelligence in medicine,"The term Artificial Intelligence (AI) was coined by John McCarthy in 1956 during a conference held on this subject. However, the possibility of machines being able to simulate human behavior and actually think was raised earlier by Alan Turing who developed the Turing test in order to differentiate humans from machines. Since then, computational power has grown to the point of instant calculations and the ability evaluate new data, according to previously assessed data, in real time. Today, AI is integrated into our daily lives in many forms, such as personal assistants (Siri, Alexa, Google assistant etc.), automated mass transportation, aviation and computer gaming. More recently, AI has also begun to be incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy, opening the path to providing better healthcare overall. Radiological images, pathology slides, and patients' electronic medical records (EMR) are being evaluated by machine learning, aiding in the process of diagnosis and treatment of patients and augmenting physicians' capabilities. Herein we describe the current status of AI in medicine, the way it is used in the different disciplines and future trends.",2019-04-01,11,1232,51,768
520,31031688,Spinal Cord Imaging in Amyotrophic Lateral Sclerosis: Historical Concepts-Novel Techniques,"Amyotrophic lateral sclerosis (ALS) is the most common adult onset motor neuron disease with no effective disease modifying therapies at present. Spinal cord degeneration is a hallmark feature of ALS, highlighted in the earliest descriptions of the disease by Lockhart Clarke and Jean-Martin Charcot. The anterior horns and corticospinal tracts are invariably affected in ALS, but up to recently it has been notoriously challenging to detect and characterize spinal pathology in vivo. With recent technological advances, spinal imaging now offers unique opportunities to appraise lower motor neuron degeneration, sensory involvement, metabolic alterations, and interneuron pathology in ALS. Quantitative spinal imaging in ALS has now been used in cross-sectional and longitudinal study designs, applied to presymptomatic mutation carriers, and utilized in machine learning applications. Despite its enormous clinical and academic potential, a number of physiological, technological, and methodological challenges limit the routine use of computational spinal imaging in ALS. In this review, we provide a comprehensive overview of emerging spinal cord imaging methods and discuss their advantages, drawbacks, and biomarker potential in clinical applications, clinical trial settings, monitoring, and prognostic roles.",2019-04-01,8,1316,90,768
2289,31057526,Application of Machine Learning in Microbiology,"Microorganisms are ubiquitous and closely related to people's daily lives. Since they were first discovered in the 19th century, researchers have shown great interest in microorganisms. People studied microorganisms through cultivation, but this method is expensive and time consuming. However, the cultivation method cannot keep a pace with the development of high-throughput sequencing technology. To deal with this problem, machine learning (ML) methods have been widely applied to the field of microbiology. Literature reviews have shown that ML can be used in many aspects of microbiology research, especially classification problems, and for exploring the interaction between microorganisms and the surrounding environment. In this study, we summarize the application of ML in microbiology.",2019-04-01,22,796,47,768
535,31005679,Applications of machine learning in GPCR bioactive ligand discovery,"GPCRs constitute the largest druggable family having targets for 475 Food and Drug Administration (FDA) approved drugs. As GPCRs are of great interest to pharmaceutical industry, enormous efforts are being expended to find relevant and potent GPCR ligands as lead compounds. There are tens of millions of compounds present in different chemical databases. In order to scan this immense chemical space, computational methods, especially machine learning (ML) methods, are essential components of GPCR drug discovery pipelines. ML approaches have applications in both ligand-based and structure-based virtual screening. We present here a cheminformatics overview of ML applications to different stages of GPCR drug discovery. Focusing on olfactory receptors, which are the largest family of GPCRs, a case study for predicting agonists for an ectopic olfactory receptor, OR1G1, compares four classical ML methods.",2019-04-01,2,910,67,768
565,30953518,Are innovation and new technologies in precision medicine paving a new era in patients centric care?,"Healthcare is undergoing a transformation, and it is imperative to leverage new technologies to generate new data and support the advent of precision medicine (PM). Recent scientific breakthroughs and technological advancements have improved our understanding of disease pathogenesis and changed the way we diagnose and treat disease leading to more precise, predictable and powerful health care that is customized for the individual patient. Genetic, genomics, and epigenetic alterations appear to be contributing to different diseases. Deep clinical phenotyping, combined with advanced molecular phenotypic profiling, enables the construction of causal network models in which a genomic region is proposed to influence the levels of transcripts, proteins, and metabolites. Phenotypic analysis bears great importance to elucidat the pathophysiology of networks at the molecular and cellular level. Digital biomarkers (BMs) can have several applications beyond clinical trials in diagnostics-to identify patients affected by a disease or to guide treatment. Digital BMs present a big opportunity to measure clinical endpoints in a remote, objective and unbiased manner. However, the use of ""omics"" technologies and large sample sizes have generated massive amounts of data sets, and their analyses have become a major bottleneck requiring sophisticated computational and statistical methods. With the wealth of information for different diseases and its link to intrinsic biology, the challenge is now to turn the multi-parametric taxonomic classification of a disease into better clinical decision-making by more precisely defining a disease. As a result, the big data revolution has provided an opportunity to apply artificial intelligence (AI) and machine learning algorithms to this vast data set. The advancements in digital health opportunities have also arisen numerous questions and concerns on the future of healthcare practices in particular with what regards the reliability of AI diagnostic tools, the impact on clinical practice and vulnerability of algorithms. AI, machine learning algorithms, computational biology, and digital BMs will offer an opportunity to translate new data into actionable information thus, allowing earlier diagnosis and precise treatment options. A better understanding and cohesiveness of the different components of the knowledge network is a must to fully exploit the potential of it.",2019-04-01,16,2427,100,768
2285,31065266,Gastroenterology Meets Machine Learning: Status Quo and Quo Vadis,"Machine learning has undergone a transition phase from being a pure statistical tool to being one of the main drivers of modern medicine. In gastroenterology, this technology is motivating a growing number of studies that rely on these innovative methods to deal with critical issues related to this practice. Hence, in the light of the burgeoning research on the use of machine learning in gastroenterology, a systematic review of the literature is timely. In this work, we present the results gleaned through a systematic review of prominent gastroenterology literature using machine learning techniques. Based on the analysis of 88 journal articles, we delimit the scope of application, we discuss current limitations including bias, lack of transparency, accountability, and data availability, and we put forward future avenues.",2019-04-01,2,832,65,768
537,31001196,Deep Brain Stimulation Programming 2.0: Future Perspectives for Target Identification and Adaptive Closed Loop Stimulation,"Deep brain stimulation has developed into an established treatment for movement disorders and is being actively investigated for numerous other neurological as well as psychiatric disorders. An accurate electrode placement in the target area and the effective programming of DBS devices are considered the most important factors for the individual outcome. Recent research in humans highlights the relevance of widespread networks connected to specific DBS targets. Improving the targeting of anatomical and functional networks involved in the generation of pathological neural activity will improve the clinical DBS effect and limit side-effects. Here, we offer a comprehensive overview over the latest research on target structures and targeting strategies in DBS. In addition, we provide a detailed synopsis of novel technologies that will support DBS programming and parameter selection in the future, with a particular focus on closed-loop stimulation and associated biofeedback signals.",2019-04-01,8,992,122,768
538,30999271,Machine learning and big data in psychiatry: toward clinical applications,"Psychiatry is a medical field concerned with the treatment of mental illness. Psychiatric disorders broadly relate to higher functions of the brain, and as such are richly intertwined with social, cultural, and experiential factors. This makes them exquisitely complex phenomena that depend on and interact with a large number of variables. Computational psychiatry provides two ways of approaching this complexity. Theory-driven computational approaches employ mechanistic models to make explicit hypotheses at multiple levels of analysis. Data-driven machine-learning approaches can make predictions from high-dimensional data and are generally agnostic as to the underlying mechanisms. Here, we review recent advances in the use of big data and machine-learning approaches toward the aim of alleviating the suffering that arises from psychiatric disorders.",2019-04-01,8,859,73,768
539,30995728,Sensors that Learn: The Evolution from Taste Fingerprints to Patterns of Early Disease Detection,"The McDevitt group has sustained efforts to develop a programmable sensing platform that offers advanced, multiplexed/multiclass chem-/bio-detection capabilities. This scalable chip-based platform has been optimized to service real-world biological specimens and validated for analytical performance. Fashioned as a sensor that learns, the platform can host new content for the application at hand. Identification of biomarker-based fingerprints from complex mixtures has a direct linkage to e-nose and e-tongue research. Recently, we have moved to the point of big data acquisition alongside the linkage to machine learning and artificial intelligence. Here, exciting opportunities are afforded by multiparameter sensing that mimics the sense of taste, overcoming the limitations of salty, sweet, sour, bitter, and glutamate sensing and moving into fingerprints of health and wellness. This article summarizes developments related to the electronic taste chip system evolving into a platform that digitizes biology and affords clinical decision support tools. A dynamic body of literature and key review articles that have contributed to the shaping of these activities are also highlighted. This fully integrated sensor promises more rapid transition of biomarker panels into wide-spread clinical practice yielding valuable new insights into health diagnostics, benefiting early disease detection.",2019-04-01,0,1399,96,768
2527,30575178,Deep learning in radiology: An overview of the concepts and a survey of the state of the art with focus on MRI,"Deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data in order to solve complex problems. Deep-learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. They have often matched or exceeded human performance. Since the medical field of radiology mainly relies on extracting useful information from images, it is a very natural application area for deep learning, and research in this area has rapidly grown in recent years. In this article, we discuss the general context of radiology and opportunities for application of deep-learning algorithms. We also introduce basic concepts of deep learning, including convolutional neural networks. Then, we present a survey of the research in deep learning applied to radiology. We organize the studies by the types of specific tasks that they attempt to solve and review a broad range of deep-learning algorithms being utilized. Finally, we briefly discuss opportunities and challenges for incorporating deep learning in the radiology practice of the future. Level of Evidence: 3 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2019;49:939-954.",2019-04-01,42,1247,110,768
2530,30566385,The Digitization of Patient Care: A Review of the Effects of Electronic Health Records on Health Care Quality and Utilization,"Electronic health records (EHRs) adoption has become nearly universal during the past decade. Academic research into the effects of EHRs has examined factors influencing adoption, clinical care benefits, financial and cost implications, and more. We provide an interdisciplinary overview and synthesis of this literature, drawing on work in public and population health, informatics, medicine, management information systems, and economics. We then chart paths forward for policy, practice, and research.",2019-04-01,8,504,125,768
2531,30565841,Deep Learning in Image Cytometry: A Review,"Artificial intelligence, deep convolutional neural networks, and deep learning are all niche terms that are increasingly appearing in scientific presentations as well as in the general media. In this review, we focus on deep learning and how it is applied to microscopy image data of cells and tissue samples. Starting with an analogy to neuroscience, we aim to give the reader an overview of the key concepts of neural networks, and an understanding of how deep learning differs from more classical approaches for extracting information from image data. We aim to increase the understanding of these methods, while highlighting considerations regarding input data requirements, computational resources, challenges, and limitations. We do not provide a full manual for applying these methods to your own data, but rather review previously published articles on deep learning in image cytometry, and guide the readers toward further reading on specific networks and methods, including new methods not yet applied to cytometry data.  2018 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry.",2019-04-01,24,1169,42,768
542,30987289,Computational Methods for the Discovery of Metabolic Markers of Complex Traits,"Metabolomics uses quantitative analyses of metabolites from tissues or bodily fluids to acquire a functional readout of the physiological state. Complex diseases arise from the influence of multiple factors, such as genetics, environment and lifestyle. Since genes, RNAs and proteins converge onto the terminal downstream metabolome, metabolomics datasets offer a rich source of information in a complex and convoluted presentation. Thus, powerful computational methods capable of deciphering the effects of many upstream influences have become increasingly necessary. In this review, the workflow of metabolic marker discovery is outlined from metabolite extraction to model interpretation and validation. Additionally, current metabolomics research in various complex disease areas is examined to identify gaps and trends in the use of several statistical and computational algorithms. Then, we highlight and discuss three advanced machine-learning algorithms, specifically ensemble learning, artificial neural networks, and genetic programming, that are currently less visible, but are budding with high potential for utility in metabolomics research. With an upward trend in the use of highly-accurate, multivariate models in the metabolomics literature, diagnostic biomarker panels of complex diseases are more recently achieving accuracies approaching or exceeding traditional diagnostic procedures. This review aims to provide an overview of computational methods in metabolomics and promote the use of up-to-date machine-learning and computational methods by metabolomics researchers.",2019-04-01,4,1592,78,768
2290,31055238,"Big data in nanoscale connectomics, and the greed for training labels","The neurosciences have developed methods that outpace most other biomedical fields in terms of acquired bytes. We review how the information content and analysis challenge of such data indicates that electron microscopy (EM)-based connectomics is an especially hard problem. Here, as in many other current machine learning applications, the need for excessive amounts of labelled data while utilizing only a small fraction of available raw image data for algorithm training illustrates the still fundamental gap between artificial and biological intelligence. Substantial improvements of label and energy efficiency in machine learning may be required to address the formidable challenge of acquiring the nanoscale connectome of a human brain.",2019-04-01,3,743,69,768
545,33501040,"The Right Direction Needed to Develop White-Box Deep Learning in Radiology, Pathology, and Ophthalmology: A Short Review","The popularity of deep learning (DL) in the machine learning community has been dramatically increasing since 2012. The theoretical foundations of DL are well-rooted in the classical neural network (NN). Rule extraction is not a new concept, but was originally devised for a shallow NN. For about the past 30 years, extensive efforts have been made by many researchers to resolve the ""black box"" problem of trained shallow NNs using rule extraction technology. A rule extraction technology that is well-balanced between accuracy and interpretability has recently been proposed for shallow NNs as a promising means to address this black box problem. Recently, we have been confronting a ""new black box"" problem caused by highly complex deep NNs (DNNs) generated by DL. In this paper, we first review four rule extraction approaches to resolve the black box problem of DNNs trained by DL in computer vision. Next, we discuss the fundamental limitations and criticisms of current DL approaches in radiology, pathology, and ophthalmology from the black box point of view. We also review the conversion methods from DNNs to decision trees and point out their limitations. Furthermore, we describe a transparent approach for resolving the black box problem of DNNs trained by a deep belief network. Finally, we provide a brief description to realize the transparency of DNNs generated by a convolutional NN and discuss a practical way to realize the transparency of DL in radiology, pathology, and ophthalmology.",2019-04-01,0,1506,120,768
2981,29705713,The neural markers of MRI to differentiate depression and panic disorder,"Depression and panic disorder (PD) share the common pathophysiology from the perspectives of neurotransmitters. The relatively high comorbidity between depression and PD contributes to the substantial obstacles to differentiate from depression and PD, especially for the brain pathophysiology. There are significant differences in the diagnostic criteria between depression and PD. However, the paradox of similar pathophysiology and different diagnostic criteria in these two disorders were still the issues needing to be addressed. Therefore the clarification of potential difference in the field of neuroscience and pathophysiology between depression and PD can help the clinicians and scientists to understand more comprehensively about significant differences between depression and PD. The researchers should be curious about the underlying difference of pathophysiology beneath the significant distinction of clinical symptoms. In this review article, I tried to find some evidences for the differences between depression and PD, especially for neural markers revealed by magnetic resonance imaging (MRI). The distinctions of structural and functional alterations in depression and PD are reviewed. From the structural perspectives, PD seems to have less severe gray matter alterations in frontal and temporal lobes than depression. The study of white matter microintegrity reveals more widespread alterations in fronto-limbic circuit of depression patients than PD patients, such as the uncinate fasciculus and anterior thalamic radiation. PD might have a more restrictive pattern of structural alterations when compared to depression. For the functional perspectives, the core site of depression pathophysiology is the anterior subnetwork of resting-state network, such as anterior cingulate cortex, which is not significantly altered in PD. A possibly emerging pattern of fronto-limbic distinction between depression and PD has been revealed by these explorative reports. The future trend for machine learning and pattern recognition might confirm the differentiation pattern between depression and PD based on the explorative results.",2019-04-01,2,2145,72,768
2421,30825037,Towards a Mechanistic-Driven Precision Medicine Approach for Tinnitus,"In this position review, we propose to establish a path for replacing the empirical classification of tinnitus with a taxonomy from precision medicine. The goal of a classification system is to understand the inherent heterogeneity of individuals experiencing and suffering from tinnitus and to identify what differentiates potential subgroups. Identification of different patient subgroups with distinct audiological, psychophysical, and neurophysiological characteristics will facilitate the management of patients with tinnitus as well as the design and execution of drug development and clinical trials, which, for the most part, have not yielded conclusive results. An alternative outcome of a precision medicine approach in tinnitus would be that additional mechanistic phenotyping might not lead to the identification of distinct drivers in each individual, but instead, it might reveal that each individual may display a quantitative blend of causal factors. Therefore, a precision medicine approach towards identifying these causal factors might not lead to subtyping these patients but may instead highlight causal pathways that can be manipulated for therapeutic gain. These two outcomes are not mutually exclusive, and no matter what the final outcome is, a mechanistic-driven precision medicine approach is a win-win approach for advancing tinnitus research and treatment. Although there are several controversies and inconsistencies in the tinnitus field, which will not be discussed here, we will give a few examples, as to how the field can move forward by exploring the major neurophysiological tinnitus models, mostly by taking advantage of the common features supported by all of the models. Our position stems from the central concept that, as a field, we can and must do more to bring studies of mechanisms into the realm of neuroscience.",2019-04-01,3,1859,69,768
2420,30825538,Stress detection in daily life scenarios using smart phones and wearable sensors: A survey,"Stress has become a significant cause for many diseases in the modern society. Recently, smartphones, smartwatches and smart wrist-bands have become an integral part of our lives and have reached a widespread usage. This raised the question of whether we can detect and prevent stress with smartphones and wearable sensors. In this survey, we will examine the recent works on stress detection in daily life which are using smartphones and wearable devices. Although there are a number of works related to stress detection in controlled laboratory conditions, the number of studies examining stress detection in daily life is limited. We will divide and investigate the works according to used physiological modality and their targeted environment such as office, campus, car and unrestricted daily life conditions. We will also discuss promising techniques, alleviation methods and research challenges.",2019-04-01,17,902,90,768
551,30975395,Bayesian Networks for Risk Prediction Using Real-World Data: A Tool for Precision Medicine,"Objective:                    The fields of medicine and public health are undergoing a data revolution. An increasing availability of data has brought about a growing interest in machine-learning algorithms. Our objective is to present the reader with an introduction to a knowledge representation and machine-learning tool for risk estimation in medical science known as Bayesian networks (BNs).              Study design:                    In this article we review how BNs are compact and intuitive graphical representations of joint probability distributions (JPDs) that can be used to conduct causal reasoning and risk estimation analysis and offer several advantages over regression-based methods. We discuss how BNs represent a different approach to risk estimation in that they are graphical representations of JPDs that take the form of a network representing model random variables and the influences between them, respectively.              Methods:                    We explore some of the challenges associated with traditional risk prediction methods and then describe BNs, their construction, application, and advantages in risk prediction based on examples in cancer and heart disease.              Results:                    Risk modeling with BNs has advantages over regression-based approaches, and in this article we focus on three that are relevant to health outcomes research: (1) the generation of network structures in which relationships between variables can be easily communicated; (2) their ability to apply Bayes's theorem to conduct individual-level risk estimation; and (3) their easy transformation into decision models.              Conclusions:                    Bayesian networks represent a powerful and flexible tool for the analysis of health economics and outcomes research data in the era of precision medicine.",2019-04-01,9,1856,90,768
552,30974797,"Novel Surface-Enhanced Raman Spectroscopy Techniques for DNA, Protein and Drug Detection","Surface-enhanced Raman spectroscopy (SERS) is a vibrational spectroscopic technique in which the Raman scattering signal strength of molecules, absorbed by rough metals or the surface of nanoparticles, experiences an exponential growth (10-10 times and even 1014-1015 times) because of electromagnetic or chemical enhancements. Nowadays, SERS has attracted tremendous attention in the field of analytical chemistry due to its specific advantages, including high selectivity, rich informative spectral properties, nondestructive testing, and the prominent multiplexing capabilities of Raman spectroscopy. In this review, we present the applications of state-of-the-art SERS for the detection of DNA, proteins and drugs. Moreover, we focus on highlighting the merits and mechanisms of achieving enhanced SERS signals for food safety and clinical treatment. The machine learning techniques, combined with SERS detection, are also indicated herein. This review concludes with recommendations for future studies on the development of SERS.",2019-04-01,3,1036,88,768
592,30883745,Automated sensing of daily activity: A new lens into development,"Rapidly maturing technologies for sensing and activity recognition can provide unprecedented access to the complex structure daily activity and interaction, promising new insight into the mechanisms by which experience shapes developmental outcomes. Motion data, autonomic activity, and ""snippets"" of audio and video recordings can be conveniently logged by wearable sensors (Lazer et al., 2009). Machine learning algorithms can process these signals into meaningful markers, from child and parent behavior to outcomes such as depression or teenage drinking. Theoretically motivated aspects of daily activity can be combined and synchronized to examine reciprocal effects between children's behaviors and their environments or internal processes. Captured over longitudinal time, such data provide a new opportunity to study the processes by which individual differences emerge and stabilize. This paper introduces the reader to developments in sensing and activity recognition with implications for developmental phenomena across the lifespan, sketching a framework for leveraging mobile sensors for transactional analyses that bridge micro- and longitudinal- timescales of development. It finishes by detailing resources and best practices to facilitate the next generation of developmentalists to contribute to this emerging area.",2019-04-01,8,1333,64,768
588,30892723,Machine Learning for Prediction of Posttraumatic Stress and Resilience Following Trauma: An Overview of Basic Concepts and Recent Advances,"Posttraumatic stress responses are characterized by a heterogeneity in clinical appearance and etiology. This heterogeneity impacts the field's ability to characterize, predict, and remediate maladaptive responses to trauma. Machine learning (ML) approaches are increasingly utilized to overcome this foundational problem in characterization, prediction, and treatment selection across branches of medicine that have struggled with similar clinical realities of heterogeneity in etiology and outcome, such as oncology. In this article, we review and evaluate ML approaches and applications utilized in the areas of posttraumatic stress, stress pathology, and resilience research, and present didactic information and examples to aid researchers interested in the relevance of ML to their own research. The examined studies exemplify the high potential of ML approaches to build accurate predictive and diagnostic models of posttraumatic stress and stress pathology risk based on diverse sources of available information. The use of ML approaches to integrate high-dimensional data demonstrates substantial gains in risk prediction even when the sources of data are the same as those used in traditional predictive models. This area of research will greatly benefit from collaboration and data sharing among researchers of posttraumatic stress disorder, stress pathology, and resilience.",2019-04-01,9,1386,138,768
580,30909105,Automated discovery of GPCR bioactive ligands,"While G-protein-coupled receptors (GPCRs) constitute the largest class of membrane proteins, structures and endogenous ligands of a large portion of GPCRs remain unknown. Because of the involvement of GPCRs in various signaling pathways and physiological roles, the identification of endogenous ligands as well as designing novel drugs is of high interest to the research and medical communities. Along with highlighting the recent advances in structure-based ligand discovery, including docking and molecular dynamics, this article focuses on the latest advances for automating the discovery of bioactive ligands using machine learning. Machine learning is centered around the development and applications of algorithms that can learn from data automatically. Such an approach offers immense opportunities for bioactivity prediction as well as quantitative structure-activity relationship studies. This review describes the most recent and successful applications of machine learning for bioactive ligand discovery, concluding with an outlook on deep learning methods that are capable of automatically extracting salient information from structural data as a promising future direction for rapid and efficient bioactive ligand discovery.",2019-04-01,2,1238,45,768
579,30909106,Computational design for thermostabilization of GPCRs,"GPCR superfamily is the largest clinically relevant family of targets in human genome; however, low thermostability and high conformational plasticity of these integral membrane proteins make them notoriously hard to handle in biochemical, biophysical, and structural experiments. Here, we describe the recent advances in computational approaches to design stabilizing mutations for GPCR that take advantage of the structural and sequence conservation properties of the receptors, and employ machine learning on accumulated mutation data for the superfamily. The fast and effective computational tools can provide a viable alternative to existing experimental mutation screening and are poised for further improvements with expansion of thermostability datasets for training the machine learning models. The rapidly growing practical applications of computational stability design streamline GPCR structure determination and may contribute to more efficient drug discovery.",2019-04-01,5,973,53,768
576,30923883,"Machine learning concepts, concerns and opportunities for a pediatric radiologist","Machine learning, a subfield of artificial intelligence, is a rapidly evolving technology that offers great potential for expanding the quality and value of pediatric radiology. We describe specific types of learning, including supervised, unsupervised and semisupervised. Subsequently, we illustrate two core concepts for the reader: data partitioning and under/overfitting. We also provide an expanded discussion of the challenges of implementing machine learning in children's imaging. These include the requirement for very large data sets, the need to accurately label these images with a relatively small number of pediatric imagers, technical and regulatory hurdles, as well as the opaque character of convolution neural networks. We review machine learning cases in radiology including detection, classification and segmentation. Last, three pediatric radiologists from the Society for Pediatric Radiology Quality and Safety Committee share perspectives for potential areas of development.",2019-04-01,7,997,81,768
563,30959828,Current State and Future Directions of Technology-Based Ecological Momentary Assessment and Intervention for Major Depressive Disorder: A Systematic Review,"Ecological momentary assessment (EMA) and ecological momentary intervention (EMI) are alternative approaches to retrospective self-reports and face-to-face treatments, and they make it possible to repeatedly assess patients in naturalistic settings and extend psychological support into real life. The increase in smartphone applications and the availability of low-cost wearable biosensors have further improved the potential of EMA and EMI, which, however, have not yet been applied in clinical practice. Here, we conducted a systematic review, using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, to explore the state of the art of technology-based EMA and EMI for major depressive disorder (MDD). A total of 33 articles were included (EMA = 26; EMI = 7). First, we provide a detailed analysis of the included studies from technical (sampling methods, duration, prompts), clinical (fields of application, adherence rates, dropouts, intervention effectiveness), and technological (adopted devices) perspectives. Then, we identify the advantages of using information and communications technologies (ICTs) to extend the potential of these approaches to the understanding, assessment, and intervention in depression. Furthermore, we point out the relevant issues that still need to be addressed within this field, and we discuss how EMA and EMI could benefit from the use of sensors and biosensors, along with recent advances in machine learning for affective modelling.",2019-04-01,15,1515,155,768
569,30950665,Enabling artificial intelligence in high acuity medical environments,"Acute patient treatment can heavily profit from AI-based assistive and decision support systems, in terms of improved patient outcome as well as increased efficiency. Yet, only very few applications have been reported because of the limited accessibility of device data due to the lack of adoption of open standards, and the complexity of regulatory/approval requirements for AI-based systems. The fragmentation of data, still being stored in isolated silos, results in limited accessibility for AI in healthcare and machine learning is complicated by the loss of semantics in data conversions. We outline a reference model that addresses the requirements of innovative AI-based research systems as well as the clinical reality. The integration of networked medical devices and Clinical Repositories based on open standards, such as IEEE 11073 SDC and HL7 FHIR, will foster novel assistance and decision support. The reference model will make point-of-care device data available for AI-based approaches. Semantic interoperability between Clinical and Research Repositories will allow correlating patient data, device data, and the patient outcome. Thus, complete workflows in high acuity environments can be analysed. Open semantic interoperability will enable the improvement of patient outcome and the increase of efficiency on a large scale and across clinical applications.",2019-04-01,1,1377,68,768
546,30982123,Treatment of Heart Failure With Preserved Ejection Fraction (HFpEF): the Phenotype-Guided Approach,"The syndrome of heart failure with preserved ejection (HFpEF) continues to rise in prevalence without persuasive evidence of current pharmacologic interventions that can reduce mortality. Clinical trials thus far have generally enrolled ""all-comers"" with the clinical syndrome of heart failure and objective evidence of a preserved ejection fraction. However, HFpEF is increasingly understood to be a heterogeneous syndrome likely borne from the interplay of genetic predisposition, lifestyle factors, and high burden of associated comorbidities with each contributing to a variety of incompletely understood pathophysiologic abnormalities. Complicating management further, such abnormalities appear to be present to varying degrees among individual patients. Ongoing studies, along with the use of computational statistics/machine learning, offer the hope of clarifying the pathophysiological substrates giving rise to the syndrome of HFpEF in different patient subsets. With better understanding of the syndrome's underpinnings, there will be the potential for development of truly targeted therapies. However, for now, there is substantial evidence for the use of currently available pharmacologic device and lifestyle therapy for the optimized management of patients. Such therapy can be tailored to presently identifiable patient clusters-called ""phenotypes""-distinguished by both the presence of predominant presenting symptoms and/or predominant comorbidity profiles. Examples of clinical presentation phenotypes include lung congestion, chronotropic incompetence, pulmonary hypertension, or skeletal muscle weakness as predominant features. Additionally, such patients may have underlying metabolic syndrome, systemic (arterial) hypertension, renal dysfunction, atrial fibrillation, and/or coronary artery disease as principal underlying comorbidities. Here, we review a ""phenotype-guided"" approach to the management of patients with HFpEF, based on a stepwise method of making the HFpEF diagnosis, identifying the prominent sources of organ dysfunction, and treating accordingly.",2019-04-01,6,2088,98,768
2955,29789997,Non-Gaussian Methods for Causal Structure Learning,"Causal structure learning is one of the most exciting new topics in the fields of machine learning and statistics. In many empirical sciences including prevention science, the causal mechanisms underlying various phenomena need to be studied. Nevertheless, in many cases, classical methods for causal structure learning are not capable of estimating the causal structure of variables. This is because it explicitly or implicitly assumes Gaussianity of data and typically utilizes only the covariance structure. In many applications, however, non-Gaussian data are often obtained, which means that more information may be contained in the data distribution than the covariance matrix is capable of containing. Thus, many new methods have recently been proposed for using the non-Gaussian structure of data and inferring the causal structure of variables. This paper introduces prevention scientists to such causal structure learning methods, particularly those based on the linear, non-Gaussian, acyclic model known as LiNGAM. These non-Gaussian data analysis tools can fully estimate the underlying causal structures of variables under assumptions even in the presence of unobserved common causes. This feature is in contrast to other approaches. A simulated example is also provided.",2019-04-01,2,1284,50,768
566,30950797,Applications of Machine Learning in Real-Life Digital Health Interventions: Review of the Literature,"Background:                    Machine learning has attracted considerable research interest toward developing smart digital health interventions. These interventions have the potential to revolutionize health care and lead to substantial outcomes for patients and medical professionals.              Objective:                    Our objective was to review the literature on applications of machine learning in real-life digital health interventions, aiming to improve the understanding of researchers, clinicians, engineers, and policy makers in developing robust and impactful data-driven interventions in the health care domain.              Methods:                    We searched the PubMed and Scopus bibliographic databases with terms related to machine learning, to identify real-life studies of digital health interventions incorporating machine learning algorithms. We grouped those interventions according to their target (ie, target condition), study design, number of enrolled participants, follow-up duration, primary outcome and whether this had been statistically significant, machine learning algorithms used in the intervention, and outcome of the algorithms (eg, prediction).              Results:                    Our literature search identified 8 interventions incorporating machine learning in a real-life research setting, of which 3 (37%) were evaluated in a randomized controlled trial and 5 (63%) in a pilot or experimental single-group study. The interventions targeted depression prediction and management, speech recognition for people with speech disabilities, self-efficacy for weight loss, detection of changes in biopsychosocial condition of patients with multiple morbidity, stress management, treatment of phantom limb pain, smoking cessation, and personalized nutrition based on glycemic response. The average number of enrolled participants in the studies was 71 (range 8-214), and the average follow-up study duration was 69 days (range 3-180). Of the 8 interventions, 6 (75%) showed statistical significance (at the P=.05 level) in health outcomes.              Conclusions:                    This review found that digital health interventions incorporating machine learning algorithms in real-life studies can be useful and effective. Given the low number of studies identified in this review and that they did not follow a rigorous machine learning evaluation methodology, we urge the research community to conduct further studies in intervention settings following evaluation principles and demonstrating the potential of machine learning in clinical practice.",2019-04-01,19,2609,100,768
2465,30709766,Thermozymes: Adaptive strategies and tools for their biotechnological applications,"In today's scenario of global climate change, there is a colossal demand for sustainable industrial processes and enzymes from thermophiles. Plausibly, thermozymes are an important toolkit, as they are known to be polyextremophilic in nature. Small genome size and diverse molecular conformational modifications have been implicated in devising adaptive strategies. Besides, the utilization of chemical technology and gene editing attributions according to mechanical necessities are the additional key factor for efficacious bioprocess development. Microbial thermozymes have been extensively used in waste management, biofuel, food, paper, detergent, medicinal and pharmaceutical industries. To understand the strength of enzymes at higher temperatures different models utilize X-ray structures of thermostable proteins, machine learning calculations, neural networks, but unified adaptive measures are yet to be totally comprehended. The present review provides a recent updates on thermozymes and various interdisciplinary applications including the aspects of thermophiles bioengineering utilizing synthetic biology and gene editing tools.",2019-04-01,8,1144,82,768
2455,30733322,Machine Learning in Nuclear Medicine: Part 1-Introduction,"This article, the first in a 2-part series, provides an introduction to machine learning (ML) in a nuclear medicine context. This part addresses the history of ML and describes common algorithms, with illustrations of when they can be helpful in nuclear medicine. Part 2 focuses on current contributions of ML to our field, addresses future expectations and limitations, and provides a critical appraisal of what ML can and cannot do.",2019-04-01,4,434,57,768
2462,30717624,Artificial Intelligence for the Otolaryngologist: A State of the Art Review,"Objective:                    To provide a state of the art review of artificial intelligence (AI), including its subfields of machine learning and natural language processing, as it applies to otolaryngology and to discuss current applications, future impact, and limitations of these technologies.              Data sources:                    PubMed and Medline search engines.              Review methods:                    A structured search of the current literature was performed (up to and including September 2018). Search terms related to topics of AI in otolaryngology were identified and queried to identify relevant articles.              Conclusions:                    AI is at the forefront of conversation in academic research and popular culture. In recent years, it has been touted for its potential to revolutionize health care delivery. Yet, to date, it has made few contributions to actual medical practice or patient care. Future adoption of AI technologies in otolaryngology practice may be hindered by misconceptions of what AI is and a fear that machine errors may compromise patient care. However, with potential clinical and economic benefits, it is vital for otolaryngologists to understand the principles and scope of AI.              Implications for practice:                    In the coming years, AI is likely to have a major impact on biomedical research and the practice of medicine. Otolaryngologists are key stakeholders in the development and clinical integration of meaningful AI technologies that will improve patient care. High-quality data collection is essential for the development of AI technologies, and otolaryngologists should seek opportunities to collaborate with data scientists to guide them toward the most impactful clinical questions.",2019-04-01,3,1793,75,768
2471,30690654,Artificial intelligence and machine learning for human reproduction and embryology presented at ASRM and ESHRE 2018,"Sixteen artificial intelligence (AI) and machine learning (ML) approaches were reported at the 2018 annual congresses of the American Society for Reproductive Biology (9) and European Society for Human Reproduction and Embryology (7). Nearly every aspect of patient care was investigated, including sperm morphology, sperm identification, identification of empty or oocyte containing follicles, predicting embryo cell stages, predicting blastocyst formation from oocytes, assessing human blastocyst quality, predicting live birth from blastocysts, improving embryo selection, and for developing optimal IVF stimulation protocols. This represents a substantial increase in reports over 2017, where just one abstract each was reported at ASRM (AI) and ESHRE (ML). Our analysis reveals wide variability in how AI and ML methods are described (from not at all or very generic to fully describing the architectural framework) and large variability on accepted dataset sizes (from just 3 patients with 16 follicles in the smallest dataset to 661,060 images of 11,898 human embryos in one of the largest). AI and ML are clearly burgeoning methodologies in human reproduction and embryology and would benefit from early application of reporting standards.",2019-04-01,10,1247,115,768
2473,30686616,Reprint of: Mapping human brain lesions and their functional consequences,"Neuroscience has a long history of inferring brain function by examining the relationship between brain injury and subsequent behavioral impairments. The primary advantage of this method over correlative methods is that it can tell us if a certain brain region is necessary for a given cognitive function. In addition, lesion-based analyses provide unique insights into clinical deficits. In the last decade, statistical voxel-based lesion behavior mapping (VLBM) emerged as a powerful method for understanding the architecture of the human brain. This review illustrates how VLBM improves our knowledge of functional brain architecture, as well as how it is inherently limited by its mass-univariate approach. A wide array of recently developed methods appear to supplement traditional VLBM. This paper provides an overview of these new methods, including the use of specialized imaging modalities, the combination of structural imaging with normative connectome data, as well as multivariate analyses of structural imaging data. We see these new methods as complementing rather than replacing traditional VLBM, providing synergistic tools to answer related questions. Finally, we discuss the potential for these methods to become established in cognitive neuroscience and in clinical applications.",2019-04-01,1,1299,73,768
523,31022958,"A Review of Remote Sensing Approaches for Monitoring Blue Carbon Ecosystems: Mangroves, Seagrassesand Salt Marshes during 20102018","Blue carbon (BC) ecosystems are an important coastal resource, as they provide a range of goods and services to the environment. They play a vital role in the global carbon cycle by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, there has been a large reduction in the global BC ecosystems due to their conversion to agriculture and aquaculture, overexploitation, and removal for human settlements. Effectively monitoring BC ecosystems at large scales remains a challenge owing to practical difficulties in monitoring and the time-consuming field measurement approaches used. As a result, sensible policies and actions for the sustainability and conservation of BC ecosystems can be hard to implement. In this context, remote sensing provides a useful tool for mapping and monitoring BC ecosystems faster and at larger scales. Numerous studies have been carried out on various sensors based on optical imagery, synthetic aperture radar (SAR), light detection and ranging (LiDAR), aerial photographs (APs), and multispectral data. Remote sensing-based approaches have been proven effective for mapping and monitoring BC ecosystems by a large number of studies. However, to the best of our knowledge, this is the first comprehensive review on the applications of remote sensing techniques for mapping and monitoring BC ecosystems. The main goal of this review is to provide an overview and summary of the key studies undertaken from 2010 onwards on remote sensing applications for mapping and monitoring BC ecosystems. Our review showed that optical imagery, such as multispectral and hyper-spectral data, is the most common for mapping BC ecosystems, while the Landsat time-series are the most widely-used data for monitoring their changes on larger scales. We investigate the limitations of current studies and suggest several key aspects for future applications of remote sensing combined with state-of-the-art machine learning techniques for mapping coastal vegetation and monitoring their extents and changes.",2019-04-01,1,2054,131,768
2452,30738835,The roles of supervised machine learning in systems neuroscience,"Over the last several years, the use of machine learning (ML) in neuroscience has been rapidly increasing. Here, we review ML's contributions, both realized and potential, across several areas of systems neuroscience. We describe four primary roles of ML within neuroscience: (1) creating solutions to engineering problems, (2) identifying predictive variables, (3) setting benchmarks for simple models of the brain, and (4) serving itself as a model for the brain. The breadth and ease of its applicability suggests that machine learning should be in the toolbox of most systems neuroscientists.",2019-04-01,9,596,64,768
1102,31413915,A fluorescent biosensor-based platform for the discovery of immunogenic cancer cell death inducers,"Systemic anticancer immunity can be reinstated via the induction of immunogenic cell death (ICD) in malignant cells. Thus, certain classes of cytotoxic compounds, for example, anthracyclines, oxaliplatin and taxanes are endowed with the capacity to act on cancer cells to ignite premortem stress pathways that lead to the surface exposure of calreticulin (CALR) and the cellular release of adenosine triphosphate, annexin A1, high mobility group B1 and type-1 interferons. Altogether, these alterations constitute the hallmarks of ICD. Here we report the design of a discovery pipeline for the identification of novel ICD inducers by means of a phenotypic screening platform. The use of fluorescent biosensors as proxies for the manifestation of ICD hallmarks has enabled the exploration of large collections of chemical compounds by automatized screening routines. Imaging-based assessment and phenotypic selection led to the identification of potential ICD inducers that could be validated further in vitro and in vivo, confirming that bona fide ICD inducers possess the capacity to induce immunological long-term memory and to confer resistance against rechallenge with syngeneic tumors. Machine learning algorithms analyzing the physicochemical properties of ICD inducers can assist in the preselection of compounds with potential ICD-stimulatory properties, further accelerating the screening efforts designed to develop new immunotherapeutic agents.",2019-04-01,3,1455,98,768
527,31019502,2D Visualization of the Psoriasis Transcriptome Fails to Support the Existence of Dual-Secreting IL-17A/IL-22 Th17 T Cells,"The present paradigm of psoriasis pathogenesis revolves around the IL-23/IL-17A axis. Dual-secreting Th17 T cells presumably are the predominant sources of the psoriasis phenotype-driving cytokines, IL-17A and IL-22. We thus conducted a meta-analysis of independently acquired RNA-seq psoriasis datasets to explore the relationship between the expression of IL17A and IL22. This analysis failed to support the existence of dual secreting IL-17A/IL-22 Th17 cells as a major source of these cytokines. However, variable relationships amongst the expression of psoriasis susceptibility genes and of IL17A, IL22, and IL23A were identified. Additionally, to shed light on gene expression relationships in psoriasis, we applied a machine learning nonlinear dimensionality reduction strategy (t-SNE) to display the entire psoriasis transcriptome as a 2-dimensonal image. This analysis revealed a variety of gene clusters, relevant to psoriasis pathophysiology but failed to support a relationship between IL17A and IL22. These results support existing theories on alternative sources of IL-17A and IL-22 in psoriasis such as a Th22 cells and non-T cell populations.",2019-04-01,2,1158,122,768
2475,30686485,Mapping the Delirium Literature Through Probabilistic Topic Modeling and Network Analysis: A Computational Scoping Review,"Background:                    Delirium is an acute confusional state, associated with morbidity and mortality in diverse medically-ill populations. Delirium is recognized, through both professional competencies and instructional materials, as a core topic in consultation psychiatry.              Objective:                    Conduct a computational scoping review of the delirium literature to identify the overall contours of this literature and evolution of the delirium literature over time.              Methods:                    Algorithmic analysis of all research articles on delirium indexed in MEDLINE between 1995 and 2015 using network analysis of citation Medical Subject Headings (MeSH) tags and probabilistic topic modeling of article abstracts.              Results:                    The delirium corpus included 3591 articles in 874 unique journals, of which 95 were primarily psychiatric. The annual delirium publication volume increased from 40 in 1995 to 420 in 2015 and grew as a proportion of total indexed publications from 8.9 to 38.6 per 100,000. The psychiatric journals published 720 of the delirium publications. Articles on treatment of delirium (806) outnumber articles on prevention of delirium (432). Abstract topic modeling and Medical Subject Headings graph community analysis identified similar genres in the delirium literature, including: delirium in geriatric, critically ill, palliative care, and postsurgical patients as well as diagnostic criteria or scales, and clinical risk factors. The genres identified by topic modeling and community analysis were distributed unevenly between psychiatric journals and nonpsychiatric journals.              Conclusion:                    The delirium literature is large and growing. Much of this growth is outside of psychiatric journals. Subtopics of the delirium literature can be algorithmically identified, and these subtopics are distributed unevenly across psychiatric journals.",2019-04-01,0,1971,121,768
2435,30785004,Analyzing biological and artificial neural networks: challenges with opportunities for synergy?,"Deep neural networks (DNNs) transform stimuli across multiple processing stages to produce representations that can be used to solve complex tasks, such as object recognition in images. However, a full understanding of how they achieve this remains elusive. The complexity of biological neural networks substantially exceeds the complexity of DNNs, making it even more challenging to understand the representations they learn. Thus, both machine learning and computational neuroscience are faced with a shared challenge: how can we analyze their representations in order to understand how they solve complex tasks? We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs, and in turn, how recently developed techniques for analysis of DNNs can be useful for understanding representations in biological neural networks. We explore opportunities for synergy between the two fields, such as the use of DNNs as in silico model systems for neuroscience, and how this synergy can lead to new hypotheses about the operating principles of biological neural networks.",2019-04-01,8,1149,95,768
517,31034926,"Drug repurposing in oncology: Compounds, pathways, phenotypes and computational approaches for colorectal cancer","The strategy of using existing drugs originally developed for one disease to treat other indications has found success across medical fields. Such drug repurposing promises faster access of drugs to patients while reducing costs in the long and difficult process of drug development. However, the number of existing drugs and diseases, together with the heterogeneity of patients and diseases, notably including cancers, can make repurposing time consuming and inefficient. The key question we address is how to efficiently repurpose an existing drug to treat a given indication. As drug efficacy remains the main bottleneck for overall success, we discuss the need for machine-learning computational methods in combination with specific phenotypic studies along with mechanistic studies, chemical genetics and omics assays to successfully predict disease-drug pairs. Such a pipeline could be particularly important to cancer patients who face heterogeneous, recurrent and metastatic disease and need fast and personalized treatments. Here we focus on drug repurposing for colorectal cancer and describe selected therapeutics already repositioned for its prevention and/or treatment as well as potential candidates. We consider this review as a selective compilation of approaches and methodologies, and argue how, taken together, they could bring drug repurposing to the next level.",2019-04-01,28,1383,112,768
529,31013673,Augmentative and Alternative Communication (AAC) Advances: A Review of Configurations for Individuals with a Speech Disability,"High-tech augmentative and alternative communication (AAC) methods are on a constant rise; however, the interaction between the user and the assistive technology is still challenged for an optimal user experience centered around the desired activity. This review presents a range of signal sensing and acquisition methods utilized in conjunction with the existing high-tech AAC platforms for individuals with a speech disability, including imaging methods, touch-enabled systems, mechanical and electro-mechanical access, breath-activated methods, and brain-computer interfaces (BCI). The listed AAC sensing modalities are compared in terms of ease of access, affordability, complexity, portability, and typical conversational speeds. A revelation of the associated AAC signal processing, encoding, and retrieval highlights the roles of machine learning (ML) and deep learning (DL) in the development of intelligent AAC solutions. The demands and the affordability of most systems hinder the scale of usage of high-tech AAC. Further research is indeed needed for the development of intelligent AAC applications reducing the associated costs and enhancing the portability of the solutions for a real user's environment. The consolidation of natural language processing with current solutions also needs to be further explored for the amelioration of the conversational speeds. The recommendations for prospective advances in coming high-tech AAC are addressed in terms of developments to support mobile health communicative applications.",2019-04-01,1,1536,126,768
2496,30641443,Harnessing networks and machine learning in neuropsychiatric care,"The development of next-generation therapies for neuropsychiatric illness will likely rely on a precise and accurate understanding of human brain dynamics. Toward this end, researchers have focused on collecting large quantities of neuroimaging data. For simplicity, we will refer to large cross-sectional neuroimaging studies as broad studies and to intensive longitudinal studies as deep studies. Recent progress in identifying illness subtypes and predicting treatment response in neuropsychiatry has been supported by these study designs, along with methods bridging machine learning and network science. Such methods combine analytic power, interpretability, and direct connection to underlying theory in cognitive neuroscience. Ultimately, we propose a general framework for the treatment of neuropsychiatric illness relying on the findings from broad and deep studies combined with basic cognitive and physiologic measurements.",2019-04-01,2,934,65,768
530,32477031,"An Individualized, Data-Driven Digital Approach for Precision Behavior Change","Chronic disease now affects approximately half of the US population, causes 7 in 10 deaths, and accounts for roughly 80% of US health care expenditure. Because the root causes of chronic diseases are largely behavioral, effective therapies require frequent, individualized interventions that extend beyond the hospital and clinic to reach patients in their day-to-day lives. However, a mismatch currently exists between what the health care system is equipped to provide and the interventions necessary to effectively address the chronic disease burden. To remedy this health crisis, we present an individualized, data-driven digital approach for chronic disease management and prevention through precision behavior change. The rapid growth of information, biological, and communication technologies makes this an opportune time to develop digital tools that deliver precision interventions for health behavior change to address the chronic disease crisis. Building on this rapid growth, we propose a framework that includes the precise targeting of risk-producing behaviors using real-time sensing technology, machine learning data analysis to identify the most effective intervention, and delivery of that intervention with health-reinforcing feedback to provide real-time, individualized support to empower sustainable health behavior change.",2019-04-01,1,1345,77,768
2449,30748019,Assessment of Alcohol Use in the Natural Environment,"The current article critically reviews 3 methodological options for assessing drinking episodes in the natural environment. Ecological momentary assessment (EMA) typically involves using mobile devices to collect self-report data from participants in daily life. This technique is now widely used in alcohol research, but investigators have implemented diverse assessment strategies. This article focuses on ""high-resolution"" EMA protocols that oversample experiences and behaviors within individual drinking episodes. A number of approaches have been used to accomplish this, including using signaled follow-ups tied to drinking initiation, asking participants to log entries before and after individual drinks or drinking episodes, and delivering frequent signaled assessments during periods of the day when alcohol use is most common. Transdermal alcohol sensors (TAS) are devices that are worn continuously and are capable of detecting alcohol eliminated through the skin. These methods are appealing because they do not rely upon drinkers' self-report. Studies using TAS have been appearing with greater frequency over the past several years. New methods are making the use of TAS more tractable by permitting back-translation of transdermal alcohol concentration data to more familiar estimates of blood alcohol concentration or breath alcohol concentration. However, the current generation of devices can have problems with missing data and tend to be relatively insensitive to low-level drinking. An emerging area of research investigates the possibility of using mobile device data and machine learning to passively detect the user's drinking, with promising early findings. EMA, TAS, and sensor-based approaches are all valid, and tend to produce convergent information when used in conjunction with one another. Each has a unique profile of advantages, disadvantages, and threats to validity. Therefore, the nature of the underlying research question must dictate the method(s) investigators select.",2019-04-01,9,2010,52,768
583,30898381,Artificial intelligence in breast imaging,"This article reviews current limitations and future opportunities for the application of computer-aided detection (CAD) systems and artificial intelligence in breast imaging. Traditional CAD systems in mammography screening have followed a rules-based approach, incorporating domain knowledge into hand-crafted features before using classical machine learning techniques as a classifier. The first commercial CAD system, ImageChecker M1000, relies on computer vision techniques for pattern recognition. Unfortunately, CAD systems have been shown to adversely affect some radiologists' performance and increase recall rates. The Digital Mammography DREAM Challenge was a multidisciplinary collaboration that provided 640,000 mammography images for teams to help decrease false-positive rates in breast cancer screening. Winning solutions leveraged deep learning's (DL) automatic hierarchical feature learning capabilities and used convolutional neural networks. Start-ups Therapixel and Kheiron Medical Technologies are using DL for breast cancer screening. With increasing use of digital breast tomosynthesis, specific artificial intelligence (AI)-CAD systems are emerging to include iCAD's PowerLook Tomo Detection and ScreenPoint Medical's Transpara. Other AI-CAD systems are focusing on breast diagnostic techniques such as ultrasound and magnetic resonance imaging (MRI). There is a gap in the market for contrast-enhanced spectral mammography AI-CAD tools. Clinical implementation of AI-CAD tools requires testing in scenarios mimicking real life to prove its usefulness in the clinical environment. This requires a large and representative dataset for testing and assessment of the reader's interaction with the tools. A cost-effectiveness assessment should be undertaken, with a large feasibility study carried out to ensure there are no unintended consequences. AI-CAD systems should incorporate explainable AI in accordance with the European Union General Data Protection Regulation (GDPR).",2019-05-01,23,1999,41,738
2515,30609102,Machine learning in suicide science: Applications and ethics,"For decades, our ability to predict suicide has remained at near-chance levels. Machine learning has recently emerged as a promising tool for advancing suicide science, particularly in the domain of suicide prediction. The present review provides an introduction to machine learning and its potential application to open questions in suicide research. Although only a few studies have implemented machine learning for suicide prediction, results to date indicate considerable improvement in accuracy and positive predictive value. Potential barriers to algorithm integration into clinical practice are discussed, as well as attendant ethical issues. Overall, machine learning approaches hold promise for accurate, scalable, and effective suicide risk detection; however, many critical questions and issues remain unexplored.",2019-05-01,8,824,60,738
2431,30803815,Machine learning in whole-body MRI: experiences and challenges from an applied study using multicentre data,"Machine learning is now being increasingly employed in radiology to assist with tasks such as automatic lesion detection, segmentation, and characterisation. We are currently involved in an National Institute of Health Research (NIHR)-funded project, which aims to develop machine learning methods to improve the diagnostic performance and reduce the radiology reading time of whole-body magnetic resonance imaging (MRI) scans, in patients being staged for cancer (MALIBO study). We describe here the main challenges we have encountered during the course of this project. Data quality and uniformity are the two most important data traits to be considered in clinical trials incorporating machine learning. Robust data pre-processing and machine learning pipelines have been employed in MALIBO, a task facilitated by the now freely available machine learning libraries and toolboxes. Another important consideration for achieving the desired clinical outcome in MALIBO, was to effectively host the resulting machine learning output, along with the clinical images, for reading in a clinical environment. Finally, a range of legal, ethical, and clinical acceptance issues should be considered when attempting to incorporate computer-assisting tools into clinical practice. The road from translating computational methods into potentially useful clinical tools involves an analytical, stepwise adaptation approach, as well as engagement of a multidisciplinary team.",2019-05-01,1,1463,107,738
2446,30760118,Preparing next-generation scientists for biomedical big data: artificial intelligence approaches,"Personalized medicine is being realized by our ability to measure biological and environmental information about patients. Much of these data are being stored in electronic health records yielding big data that presents challenges for its management and analysis. Here, we review several areas of knowledge that are necessary for next-generation scientists to fully realize the potential of biomedical big data. We begin with an overview of big data and its storage and management. We then review statistics and data science as foundational topics followed by a core curriculum of artificial intelligence, machine learning and natural language processing that are needed to develop predictive models for clinical decision making. We end with some specific training recommendations for preparing next-generation scientists for biomedical big data.",2019-05-01,3,846,96,738
2437,30780045,Continual lifelong learning with neural networks: A review,"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.",2019-05-01,32,1819,58,738
2272,33467340,The Development of a Personalised Training Framework: Implementation of Emerging Technologies for Performance,"Over the last decade, there has been considerable interest in the individualisation of athlete training, including the use of genetic information, alongside more advanced data capture and analysis techniques. Here, we explore the evidence for, and practical use of, a number of these emerging technologies, including the measurement and quantification of epigenetic changes, microbiome analysis and the use of cell-free DNA, along with data mining and machine learning. In doing so, we develop a theoretical model for the use of these technologies in an elite sport setting, allowing the coach to better answer six key questions: (1) To what training will my athlete best respond? (2) How well is my athlete adapting to training? (3) When should I change the training stimulus (i.e., has the athlete reached their adaptive ceiling for this training modality)? (4) How long will it take for a certain adaptation to occur? (5) How well is my athlete tolerating the current training load? (6) What load can my athlete handle today? Special consideration is given to whether such an individualised training framework will outperform current methods as well as the challenges in implementing this approach.",2019-05-01,0,1201,109,738
411,30391107,"Status epilepticus prevention, ambulatory monitoring, early seizure detection and prediction in at-risk patients","Purpose:                    Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.              Method:                    Narrative review.              Results:                    Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.              Conclusions:                    Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families.",2019-05-01,0,1933,112,738
590,30890362,ADMET modeling approaches in drug discovery,"In silico prediction of ADMET is an important component of pharmaceutical R&D. Last year, the FDA approved 59 new molecular entities, with small molecules comprising 64% of the therapies approved in 2018. Estimation of pharmacokinetic properties in the early phases of drug discovery has been central to guiding hit-to-lead and lead-optimization efforts. Given the outstanding complexity of the current R&D model, drug discovery players have intensely pursued molecular modeling strategies to identify patterns in ADMET data and convert them into knowledge. The field has advanced alongside the progress of chemoinformatics, which has evolved from traditional chemometrics to advanced machine learning methods.",2019-05-01,22,710,43,738
2270,31089906,Artificial Intelligence in Cardiovascular Medicine,"Purpose of review:                    The ripples of artificial intelligence are being felt in various sectors of human life. Machine learning, a subset of artificial intelligence, extracts information from large databases of information and is gaining traction in various fields of cardiology. In this review, we highlight noteworthy examples of machine learning utilization in echocardiography, nuclear cardiology, computed tomography, and magnetic resonance imaging over the past year.              Recent findings:                    In the past year, machine learning (ML) has expanded its boundaries in cardiology with several positive results. Some studies have integrated clinical and imaging information to further augment the accuracy of these ML algorithms. All the studies mentioned in this review have clearly demonstrated superior results of ML in relation to conventional approaches for identifying obstructions or predicting major adverse events in reference to conventional approaches. As the influx of data arriving from gradually evolving technologies in health care and wearable devices continues to be more complex, ML may serve as the bridge to transcend the gap between health care and patients in the future. In order to facilitate a seamless transition between both, a few issues must be resolved for a successful implementation of ML in health care.",2019-05-01,13,1375,50,738
591,30884989,A new wave of innovation in Semantic web tools for drug discovery,"The use of semantic web technologies to aid drug discovery has gained momentum over recent years. Researchers in this domain have realized that semantic web technologies are key to dealing with the high levels of data for drug discovery. These technologies enable us to represent the data in a formal, structured, interoperable and comparable way, and to tease out undiscovered links between drug data (be it identifying new drug-targets or relevant compounds, or links between specific drugs and diseases). Areas covered: This review focuses on explaining how semantic web technologies are being used to aid advances in drug discovery. The main types of semantic web technologies are explained, outlining how they work and how they can be used in the drug discovery process, with a consideration of how the use of these technologies has progressed from their initial usage. Expert opinion: The increased availability of shared semantic resources (tools, data and importantly the communities) have enabled the application of semantic web technologies to facilitate semantic (context dependent) search across multiple data sources, which can be used by machine learning to produce better predictions by exploiting the semantic links in knowledge graphs and linked datasets.",2019-05-01,1,1272,65,738
2554,30531869,Cardiovascular calcification: artificial intelligence and big data accelerate mechanistic discovery,"Cardiovascular calcification is a health disorder with increasing prevalence and high morbidity and mortality. The only available therapeutic options for calcific vascular and valvular heart disease are invasive transcatheter procedures or surgeries that do not fully address the wide spectrum of these conditions; therefore, an urgent need exists for medical options. Cardiovascular calcification is an active process, which provides a potential opportunity for effective therapeutic targeting. Numerous biological processes are involved in calcific disease, including matrix remodelling, transcriptional regulation, mitochondrial dysfunction, oxidative stress, calcium and phosphate signalling, endoplasmic reticulum stress, lipid and mineral metabolism, autophagy, inflammation, apoptosis, loss of mineralization inhibition, impaired mineral resorption, cellular senescence and extracellular vesicles that act as precursors of microcalcification. Advances in molecular imaging and big data technology, including in multiomics and network medicine, and the integration of these approaches are helping to provide a more comprehensive map of human disease. In this Review, we discuss ectopic calcification processes in the cardiovascular system, with an emphasis on emerging mechanistic knowledge obtained through patient data and advances in imaging methods, experimental models and multiomics-generated big data. We also highlight the potential and challenges of artificial intelligence, machine learning and deep learning to integrate imaging and mechanistic data for drug discovery.",2019-05-01,22,1586,99,738
2488,30659282,Spatial proteomics: a powerful discovery tool for cell biology,"Protein subcellular localization is tightly controlled and intimately linked to protein function in health and disease. Capturing the spatial proteome - that is, the localizations of proteins and their dynamics at the subcellular level - is therefore essential for a complete understanding of cell biology. Owing to substantial advances in microscopy, mass spectrometry and machine learning applications for data analysis, the field is now mature for proteome-wide investigations of spatial cellular regulation. Studies of the human proteome have begun to reveal a complex architecture, including single-cell variations, dynamic protein translocations, changing interaction networks and proteins localizing to multiple compartments. Furthermore, several studies have successfully harnessed the power of comparative spatial proteomics as a discovery tool to unravel disease mechanisms. We are at the beginning of an era in which spatial proteomics finally integrates with cell biology and medical research, thereby paving the way for unbiased systems-level insights into cellular processes. Here, we discuss current methods for spatial proteomics using imaging or mass spectrometry and specifically highlight global comparative applications. The aim of this Review is to survey the state of the field and also to encourage more cell biologists to apply spatial proteomics approaches.",2019-05-01,40,1382,62,738
621,29194464,"Computer-aided biomarker discovery for precision medicine: data resources, models and applications","Biomarkers are a class of measurable and evaluable indicators with the potential to predict disease initiation and progression. In contrast to disease-associated factors, biomarkers hold the promise to capture the changeable signatures of biological states. With methodological advances, computer-aided biomarker discovery has now become a burgeoning paradigm in the field of biomedical science. In recent years, the 'big data' term has accumulated for the systematical investigation of complex biological phenomena and promoted the flourishing of computational methods for systems-level biomarker screening. Compared with routine wet-lab experiments, bioinformatics approaches are more efficient to decode disease pathogenesis under a holistic framework, which is propitious to identify biomarkers ranging from single molecules to molecular networks for disease diagnosis, prognosis and therapy. In this review, the concept and characteristics of typical biomarker types, e.g. single molecular biomarkers, module/network biomarkers, cross-level biomarkers, etc., are explicated on the guidance of systems biology. Then, publicly available data resources together with some well-constructed biomarker databases and knowledge bases are introduced. Biomarker identification models using mathematical, network and machine learning theories are sequentially discussed. Based on network substructural and functional evidences, a novel bioinformatics model is particularly highlighted for microRNA biomarker discovery. This article aims to give deep insights into the advantages and challenges of current computational approaches for biomarker detection, and to light up the future wisdom toward precision medicine and nation-wide healthcare.",2019-05-01,16,1736,98,738
2480,30676989,Deep Learning: Current and Emerging Applications in Medicine and Technology,"Machine learning is enabling researchers to analyze and understand increasingly complex physical and biological phenomena in traditional fields such as biology, medicine, and engineering and emerging fields like synthetic biology, automated chemical synthesis, and biomanufacturing. These fields require new paradigms toward understanding increasingly complex data and converting such data into medical products and services for patients. The move toward deep learning and complex modeling is an attempt to bridge the gap between acquiring massive quantities of complex data, and converting such data into practical insights. Here, we provide an overview of the field of machine learning, its current applications and needs in traditional and emerging fields, and discuss an illustrative attempt at using deep learning to understand swarm behavior of molecular shuttles.",2019-05-01,7,870,75,738
2276,33178922,"Artificial intelligence in oncology, its scope and future prospects with specific reference to radiation oncology","Objective:                    Artificial intelligence (AI) seems to be bridging the gap between the acquisition of data and its meaningful interpretation. These approaches, have shown outstanding capabilities, outperforming most classification and regression methods to date and the ability to automatically learn the most suitable data representation for the task at hand and present it for better correlation. This article tries to sensitize the practising radiation oncologists to understand where the potential role of AI lies and what further can be achieved with it.              Methods and materials:                    Contemporary literature was searched and the available literature was sorted and an attempt at writing a comprehensive non-systematic review was made.              Results:                    The article addresses various areas in oncology, especially in the field of radiation oncology, where the work based on AI has been done. Whether it's the screening modalities, or diagnosis or the prognostic assays, AI has come with more accurately defining results and survival of patients. Various steps and protocols in radiation oncology are now using AI-based methods, like in the steps of planning, segmentation and delivery of radiation. Benefit of AI across all the platforms of health sector may lead to a more refined and personalized medicine in near future.              Conclusion:                    AI with the use of machine learning and artificial neural networks has come up with faster and more accurate solutions for the problems faced by oncologist. The uses of AI,are likely to get increased exponentially . However, concerns regarding demographic discrepancies in relation to patients, disease and their natural history and reports of manipulation of AI, the ultimate responsibility will rest on the treating physicians.",2019-05-01,0,1863,113,738
2474,30686613,A gentle introduction to deep learning in medical image processing,"This paper tries to give a gentle introduction to deep learning in medical image processing, proceeding from theoretical foundations to applications. We first discuss general reasons for the popularity of deep learning, including several major breakthroughs in computer science. Next, we start reviewing the fundamental basics of the perceptron and neural networks, along with some fundamental theory that is often omitted. Doing so allows us to understand the reasons for the rise of deep learning in many application domains. Obviously medical image processing is one of these areas which has been largely affected by this rapid progress, in particular in image detection and recognition, image segmentation, image registration, and computer-aided diagnosis. There are also recent trends in physical simulation, modeling, and reconstruction that have led to astonishing results. Yet, some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results. These apparent weaknesses highlight current limitations of deep ()learning. However, we also briefly discuss promising approaches that might be able to resolve these problems in the future.",2019-05-01,35,1182,66,738
2206,31205413,Machine Learning-Enhanced T Cell Neoepitope Discovery for Immunotherapy Design,"Immune responses mediated by T cells are aimed at specific peptides, designated T cell epitopes, that are recognized when bound to human leukocyte antigen (HLA) molecules. The HLA genes are remarkably polymorphic in the human population allowing a broad and fine-tuned capacity to bind a wide array of peptide sequences. Polymorphisms might generate neoepitopes by impacting the HLA-peptide interaction and potentially alter the level and type of generated T cell responses. Multiple algorithms and tools based on machine learning (ML) have been implemented and are able to predict HLA-peptide binding affinity with considerable accuracy. Challenges in this field include the availability of adequate epitope datasets for training and benchmarking and the development of fully integrated pipelines going from next-generation sequencing to neoepitope prediction and quality analysis metrics. Effectively predicting neoepitopes from in silico data is a demanding task that has been facilitated by ML and will be of great value for the future of personalized immunotherapies against cancer and other diseases.",2019-05-01,3,1106,78,738
2280,31074158,Orthodontics in the era of big data analytics,"The objective of this report was to provide an overview of the current landscape of big data analytics in the healthcare sector, introduce various approaches of machine learning and discuss potential implications in the field of orthodontics. With the increasing availability of data from various sources, the traditional analytical methods may not be conducive anymore for examining clinical outcomes. Machine-learning approaches, which are algorithms trained to identify patterns in large data sets, are ideally suited to facilitate data-driven decision making. The field of orthodontics is particularly ripe for embracing the big data analytics platform to improve decision making in clinical practice. The availability of omics data, state-of-the-art imaging and potential for establishing large clinical data repositories have favourably positioned the specialty of orthodontics to deliver personalized and precision orthodontic care. Specifically, we discuss about next-generation sequencing, radiomics in the context of CBCT imaging, and how centralized data repositories can enable real-time data pooling from multiple sources.",2019-05-01,0,1135,45,738
922,31239834,A Review of Characterization Approaches for Smallholder Farmers: Towards Predictive Farm Typologies,"Characterization of smallholder farmers has been conducted in various researches by using machine learning algorithms, participatory and expert-based methods. All approaches used end up with the development of some subgroups known as farm typologies. The main purpose of this paper is to highlight the main approaches used to characterize smallholder farmers, presenting the pros and cons of the approaches. By understanding the nature and key advantages of the reviewed approaches, the paper recommends a hybrid approach towards having predictive farm typologies. Search of relevant research articles published between 2007 and 2018 was done on ScienceDirect and Google Scholar. By using a generated search query, 20 research articles related to characterization of smallholder farmers were retained. Cluster-based algorithms appeared to be the mostly used in characterizing smallholder farmers. However, being highly unpredictable and inconsistent, use of clustering methods calls in for a discussion on how well the developed farm typologies can be used to predict future trends of the farmers. A thorough discussion is presented and recommends use of supervised models to validate unsupervised models. In order to achieve predictive farm typologies, three stages in characterization are recommended as tested in smallholder dairy farmers datasets: (a) develop farm types from a comparative analysis of more than two unsupervised learning algorithms by using training models, (b) assess the training models' robustness in predicting farm types for a testing dataset, and (c) assess the predictive power of the developed farm types from each algorithm by predicting the trend of several response variables.",2019-05-01,0,1708,99,738
2213,31191325,Treatment-Resistant Schizophrenia: Insights From Genetic Studies and Machine Learning Approaches,"Schizophrenia (SCZ) is a severe psychiatric disorder affecting approximately 23 million people worldwide. It is considered the eighth leading cause of disability according to the World Health Organization and is associated with a significant reduction in life expectancy. Antipsychotics represent the first-choice treatment in SCZ, but approximately 30% of patients fail to respond to acute treatment. These patients are generally defined as treatment-resistant and are eligible for clozapine treatment. Treatment-resistant patients show a more severe course of the disease, but it has been suggested that treatment-resistant schizophrenia (TRS) may constitute a distinct phenotype that is more than just a more severe form of SCZ. TRS is heritable, and genetics has been shown to play an important role in modulating response to antipsychotics. Important efforts have been put into place in order to better understand the genetic architecture of TRS, with the main goal of identifying reliable predictive markers that might improve the management and quality of life of TRS patients. However, the number of candidate gene and genome-wide association studies specifically focused on TRS is limited, and to date, findings do not allow the disentanglement of its polygenic nature. More recent studies implemented polygenic risk score, gene-based and machine learning methods to explore the genetics of TRS, reporting promising findings. In this review, we present an overview on the genetics of TRS, particularly focusing our discussion on studies implementing polygenic approaches.",2019-05-01,3,1580,96,738
2951,29792115,Using Machine Learning to Advance Personality Assessment and Theory,"Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality.",2019-05-01,6,905,67,738
2479,30677473,"Novel genetic and epigenetic factors of importance for inter-individual differences in drug disposition, response and toxicity","Individuals differ substantially in their response to pharmacological treatment. Personalized medicine aspires to embrace these inter-individual differences and customize therapy by taking a wealth of patient-specific data into account. Pharmacogenomic constitutes a cornerstone of personalized medicine that provides therapeutic guidance based on the genomic profile of a given patient. Pharmacogenomics already has applications in the clinics, particularly in oncology, whereas future development in this area is needed in order to establish pharmacogenomic biomarkers as useful clinical tools. In this review we present an updated overview of current and emerging pharmacogenomic biomarkers in different therapeutic areas and critically discuss their potential to transform clinical care. Furthermore, we discuss opportunities of technological, methodological and institutional advances to improve biomarker discovery. We also summarize recent progress in our understanding of epigenetic effects on drug disposition and response, including a discussion of the only few pharmacogenomic biomarkers implemented into routine care. We anticipate, in part due to exciting rapid developments in Next Generation Sequencing technologies, machine learning methods and national biobanks, that the field will make great advances in the upcoming years towards unlocking the full potential of genomic data.",2019-05-01,13,1395,126,738
2236,31151223,In-Vivo and Ex-Vivo Tissue Analysis through Hyperspectral Imaging Techniques: Revealing the Invisible Features of Cancer,"In contrast to conventional optical imaging modalities, hyperspectral imaging (HSI) is able to capture much more information from a certain scene, both within and beyond the visual spectral range (from 400 to 700 nm). This imaging modality is based on the principle that each material provides different responses to light reflection, absorption, and scattering across the electromagnetic spectrum. Due to these properties, it is possible to differentiate and identify the different materials/substances presented in a certain scene by their spectral signature. Over the last two decades, HSI has demonstrated potential to become a powerful tool to study and identify several diseases in the medical field, being a non-contact, non-ionizing, and a label-free imaging modality. In this review, the use of HSI as an imaging tool for the analysis and detection of cancer is presented. The basic concepts related to this technology are detailed. The most relevant, state-of-the-art studies that can be found in the literature using HSI for cancer analysis are presented and summarized, both in-vivo and ex-vivo. Lastly, we discuss the current limitations of this technology in the field of cancer detection, together with some insights into possible future steps in the improvement of this technology.",2019-05-01,17,1297,120,738
2237,31151154,Innovative MRI Techniques in Neuroimaging Approaches for Cerebrovascular Diseases and Vascular Cognitive Impairment,"Cognitive impairment and dementia are recognized as major threats to public health. Many studies have shown the important role played by challenges to the cerebral vasculature and the neurovascular unit. To investigate the structural and functional characteristics of the brain, MRI has proven an invaluable tool for visualizing the internal organs of patients and analyzing the parameters related to neuronal activation and blood flow in vivo. Different strategies of imaging can be combined to obtain various parameters: (i) measures of cortical and subcortical structures (cortical thickness, subcortical structures volume); (ii) evaluation of microstructural characteristics of the white matter (fractional anisotropy, mean diffusivity); (iii) neuronal activation and synchronicity to identify functional networks across different regions (functional connectivity between specific regions, graph measures of specific nodes); and (iv) structure of the cerebral vasculature and its efficacy in irrorating the brain (main vessel diameter, cerebral perfusion). The high amount of data obtainable from multi-modal sources calls for methods of advanced analysis, like machine-learning algorithms that allow the discrimination of the most informative features, to comprehensively characterize the cerebrovascular network into specific and sensitive biomarkers. By using the same techniques of human imaging in pre-clinical research, we can also investigate the mechanisms underlying the pathophysiological alterations identified in patients by imaging, with the chance of looking for molecular mechanisms to recover the pathology or hamper its progression.",2019-05-01,1,1653,115,738
2291,31054502,"Radiological images and machine learning: Trends, perspectives, and prospects","The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently.",2019-05-01,10,1310,77,738
2564,30503919,What does a pain 'biomarker' mean and can a machine be taught to measure pain?,"Artificial intelligence allows machines to predict human faculties such as image and voice recognition. Can machines be taught to measure pain? We argue that the two fundamental requirements for a device with 'pain biomarker' capabilities are hardware and software. We discuss the merits and limitations of electroencephalography (EEG) as the hardware component of a putative embodiment of the device, and advances in the application of machine learning approaches to EEG for predicting pain.",2019-05-01,3,492,78,738
2245,32903302,Robots in laparoscopic surgery: current and future status,"In this paper, we focus on robots used for laparoscopic surgery, which is one of the most active areas for research and development of surgical robots. We introduce research and development of laparoscope-holder robots, master-slave robots and hand-held robotic forceps. Then, we discuss future directions for surgical robots. For robot hardware, snake like flexible mechanisms for single-port access surgery (SPA) and NOTES (Natural Orifice Transluminal Endoscopic Surgery) and applications of soft robotics are actively used. On the software side, research such as automation of surgical procedures using machine learning is one of the hot topics.",2019-05-01,0,649,57,738
2262,33137726,Computer vision and machine learning in science fiction,Science fiction has a cautionary view of computer vision and machine learning.,2019-05-01,0,78,55,738
2257,31114635,"Innovative strategies for annotating the ""relationSNP"" between variants and molecular phenotypes","Characterizing how variation at the level of individual nucleotides contributes to traits and diseases has been an area of growing interest since the completion of sequencing the first human genome. Our understanding of how a single nucleotide polymorphism (SNP) leads to a pathogenic phenotype on a genome-wide scale is a fruitful endeavor for anyone interested in developing diagnostic tests, therapeutics, or simply wanting to understand the etiology of a disease or trait. To this end, many datasets and algorithms have been developed as resources/tools to annotate SNPs. One of the most common practices is to annotate coding SNPs that affect the protein sequence. Synonymous variants are often grouped as one type of variant, however there are in fact many tools available to dissect their effects on gene expression. More recently, large consortiums like ENCODE and GTEx have made it possible to annotate non-coding regions. Although annotating variants is a common technique among human geneticists, the constant advances in tools and biology surrounding SNPs requires an updated summary of what is known and the trajectory of the field. This review will discuss the history behind SNP annotation, commonly used tools, and newer strategies for SNP annotation. Additionally, we will comment on the caveats that distinguish approaches from one another, along with gaps in the current state of knowledge, and potential future directions. We do not intend for this to be a comprehensive review for any specific area of SNP annotation, but rather it will be an excellent resource for those unfamiliar with computational tools used to functionally characterize SNPs. In summary, this review will help illustrate how each SNP annotation method impacts the way in which the genetic and molecular etiology of a disease is explored in-silico.",2019-05-01,1,1840,96,738
497,31052580,Drug Combinations: Mathematical Modeling and Networking Methods,"Treatments consisting of mixtures of pharmacological agents have been shown to have superior effects to treatments involving single compounds. Given the vast amount of possible combinations involving multiple drugs and the restrictions in time and resources required to test all such combinations in vitro, mathematical methods are essential to model the interactive behavior of the drug mixture and the target, ultimately allowing one to better predict the outcome of the combination. In this review, we investigate various mathematical methods that model combination therapies. This survey includes the methods that focus on predicting the outcome of drug combinations with respect to synergism and antagonism, as well as the methods that explore the dynamics of combination therapy and its role in combating drug resistance. This comprehensive investigation of the mathematical methods includes models that employ pharmacodynamics equations, those that rely on signaling and how the underlying chemical networks are affected by the topological structure of the target proteins, and models that are based on stochastic models for evolutionary dynamics. Additionally, this article reviews computational methods including mathematical algorithms, machine learning, and search algorithms that can identify promising combinations of drug compounds. A description of existing data and software resources is provided that can support investigations in drug combination therapies. Finally, the article concludes with a summary of future directions for investigation by the research community.",2019-05-01,9,1587,63,738
2539,30554770,Embracing Environmental Genomics and Machine Learning for Routine Biomonitoring,"Genomics is fast becoming a routine tool in medical diagnostics and cutting-edge biotechnologies. Yet, its use for environmental biomonitoring is still considered a futuristic ideal. Until now, environmental genomics was mainly used as a replacement of the burdensome morphological identification, to screen known morphologically distinguishable bioindicator taxa. While prokaryotic and eukaryotic microbial diversity is of key importance in ecosystem functioning, its implementation in biomonitoring programs is still largely unappreciated, mainly because of difficulties in identifying microbes and limited knowledge of their ecological functions. Here, we argue that the combination of massive environmental genomics microbial data with machine learning algorithms can be extremely powerful for biomonitoring programs and pave the way to fill important gaps in our understanding of microbial ecology.",2019-05-01,5,903,79,738
2265,31104157,Connected Health Technology for Cardiovascular Disease Prevention and Management,"Purpose of the review:                    Advances in computing power and wireless technologies have reshaped our approach to patient monitoring. Medical grade sensors and apps that were once restricted to hospitals and specialized clinic are now widely available. Here, we review the current evidence supporting the use of connected health technologies for the prevention and management of cardiovascular disease in an effort to highlight gaps and future opportunities for innovation.              Recent findings:                    Initial studies in connected health for cardiovascular disease prevention and management focused primarily on activity tracking and blood pressure monitoring but have since expanded to include a full panoply of novel sensors and pioneering smartphone apps with targeted interventions in diet, lipid management and risk assessment, smoking cessation, cardiac rehabilitation, heart failure, and arrhythmias. While outfitting patients with sensors and devices alone is infrequently a lasting solution, monitoring programs that include personalized insights based on patient-level data are more likely to lead to improved outcomes. Advances in this space have been driven by patients and researchers while healthcare systems remain slow to fully integrate and adequately adapt these new technologies into their workflows. Cardiovascular disease prevention and management continue to be key focus areas for clinicians and researchers in the connected health space. Exciting progress has been made though studies continue to suffer from small sample size and limited follow-up. Efforts that combine home patient monitoring, engagement, and personalized feedback are the most promising. Ultimately, combining patient-level ambulatory sensor data, electronic health records, and genomics using machine learning analytics will bring precision medicine closer to reality.",2019-05-01,2,1896,80,738
2264,31107871,Animal models of chemotherapy-induced peripheral neuropathy: A machine-assisted systematic review and meta-analysis,"We report a systematic review and meta-analysis of research using animal models of chemotherapy-induced peripheral neuropathy (CIPN). We systematically searched 5 online databases in September 2012 and updated the search in November 2015 using machine learning and text mining to reduce the screening for inclusion workload and improve accuracy. For each comparison, we calculated a standardised mean difference (SMD) effect size, and then combined effects in a random-effects meta-analysis. We assessed the impact of study design factors and reporting of measures to reduce risks of bias. We present power analyses for the most frequently reported behavioural tests; 337 publications were included. Most studies (84%) used male animals only. The most frequently reported outcome measure was evoked limb withdrawal in response to mechanical monofilaments. There was modest reporting of measures to reduce risks of bias. The number of animals required to obtain 80% power with a significance level of 0.05 varied substantially across behavioural tests. In this comprehensive summary of the use of animal models of CIPN, we have identified areas in which the value of preclinical CIPN studies might be increased. Using both sexes of animals in the modelling of CIPN, ensuring that outcome measures align with those most relevant in the clinic, and the animal's pain contextualised ethology will likely improve external validity. Measures to reduce risk of bias should be employed to increase the internal validity of studies. Different outcome measures have different statistical power, and this can refine our approaches in the modelling of CIPN.",2019-05-01,6,1645,115,738
541,30988417,Computational advances in combating colloidal aggregation in drug discovery,"Small molecule effectors are essential for drug discovery. Specific molecular recognition, reversible binding and dose-dependency are usually key requirements to ensure utility of a novel chemical entity. However, artefactual frequent-hitter and assay interference compounds may divert lead optimization and screening programmes towards attrition-prone chemical matter. Colloidal aggregates are the prime source of false positive readouts, either through protein sequestration or protein-scaffold mimicry. Nevertheless, assessment of colloidal aggregation remains somewhat overlooked and under-appreciated. In this Review, we discuss the impact of aggregation in drug discovery by analysing select examples from the literature and publicly-available datasets. We also examine and comment on technologies used to experimentally identify these potentially problematic entities. We focus on evidence-based computational filters and machine learning algorithms that may be swiftly deployed to flag chemical matter and mitigate the impact of aggregates in discovery programmes. We highlight the tools that can be used to scrutinize libraries, and identify and eliminate these problematic compounds.",2019-05-01,6,1193,75,738
548,30981588,A Novel Framework for Unconscious Processing,"Understanding the distinction between conscious and unconscious cognition remains a priority in psychology and neuroscience. A comprehensive neurocognitive account of conscious awareness will not be possible without a sound framework to isolate and understand unconscious information processing. Here, we provide a brain-based framework that allows the identification of unconscious processes, even with null effects on behaviour.",2019-05-01,2,430,44,738
536,31005165,The present and future of deep learning in radiology,"The advent of Deep Learning (DL) is poised to dramatically change the delivery of healthcare in the near future. Not only has DL profoundly affected the healthcare industry it has also influenced global businesses. Within a span of very few years, advances such as self-driving cars, robots performing jobs that are hazardous to human, and chat bots talking with human operators have proved that DL has already made large impact on our lives. The open source nature of DL and decreasing prices of computer hardware will further propel such changes. In healthcare, the potential is immense due to the need to automate the processes and evolve error free paradigms. The sheer quantum of DL publications in healthcare has surpassed other domains growing at a very fast pace, particular in radiology. It is therefore imperative for the radiologists to learn about DL and how it differs from other approaches of Artificial Intelligence (AI). The next generation of radiology will see a significant role of DL and will likely serve as the base for augmented radiology (AR). Better clinical judgement by AR will help in improving the quality of life and help in life saving decisions, while lowering healthcare costs. A comprehensive review of DL as well as its implications upon the healthcare is presented in this review. We had analysed 150 articles of DL in healthcare domain from PubMed, Google Scholar, and IEEE EXPLORE focused in medical imagery only. We have further examined the ethic, moral and legal issues surrounding the use of DL in medical imaging.",2019-05-01,29,1556,52,738
2267,31095701,Challenges in IBD Research: Precision Medicine,"Precision medicine is part of five focus areas of the Challenges in IBD research document, which also includes preclinical human IBD mechanisms, environmental triggers, novel technologies, and pragmatic clinical research. The Challenges in IBD Research document provides a comprehensive overview of current gaps in inflammatory bowel diseases (IBD) research and delivers actionable approaches to address them. It is the result of a multidisciplinary input from scientists, clinicians, patients, and funders, and represents a valuable resource for patient centric research prioritization. In particular, the precision medicine section is focused on highlighting the main gap areas that must be addressed to get closer to treatments tailored to the biological and clinical characteristics of each patient, which is the aim of precision medicine. The main gaps were identified in: 1) understanding and predicting the natural history of IBD: disease susceptibility, activity, and behavior; 2) predicting disease course and treatment response; and 3) optimizing current and developing new molecular technologies. Suggested approaches to bridge these gaps include prospective longitudinal cohort studies to identify and validate precision biomarkers for prognostication of disease course, and prediction and monitoring of treatment response. To achieve this, harmonization across studies is key as well as development of standardized methods and infrastructure. The implementation of state-of-the-art molecular technologies, systems biology and machine learning approaches for multi-omics and clinical data integration and analysis will be also fundamental. Finally, randomized biomarker-stratified trials will be critical to evaluate the clinical utility of validated signatures and biomarkers in improving patient outcomes and cost-effective care.",2019-05-01,13,1843,46,738
508,31044723,Digital pathology and artificial intelligence,"In modern clinical practice, digital pathology has a crucial role and is increasingly a technological requirement in the scientific laboratory environment. The advent of whole-slide imaging, availability of faster networks, and cheaper storage solutions has made it easier for pathologists to manage digital slide images and share them for clinical use. In parallel, unprecedented advances in machine learning have enabled the synergy of artificial intelligence and digital pathology, which offers image-based diagnosis possibilities that were once limited only to radiology and cardiology. Integration of digital slides into the pathology workflow, advanced algorithms, and computer-aided diagnostic techniques extend the frontiers of the pathologist's view beyond a microscopic slide and enable true utilisation and integration of knowledge that is beyond human limits and boundaries, and we believe there is clear potential for artificial intelligence breakthroughs in the pathology setting. In this Review, we discuss advancements in digital slide-based image diagnosis for cancer along with some challenges and opportunities for artificial intelligence in digital pathology.",2019-05-01,49,1179,45,738
2533,30560558,An evidence-based approach to the routine use of optical coherence tomography,"Optical coherence tomography is an imaging technology that has revolutionised the detection, assessment and management of ocular disease. It is now a mainstream technology in clinical practice and is performed by non-specialised personnel in some settings. This article provides a clinical perspective on the implications of that movement and describes best practice using multimodal imaging and an evidence-based approach. Practical, illustrative guides on the interpretation of optical coherence tomography are provided for three major diseases of the ocular fundus, in which optical coherence tomography is often crucial to management: age-related macular degeneration, diabetic retinopathy and glaucoma. Topics discussed include: cross-sectional and longitudinal signs in ocular disease, so-called 'red-green' disease whereby clinicians rely on machine/statistical comparisons for diagnosis in managing treatment-nave patients, and the utility of optical coherence tomography angiography and machine learning.",2019-05-01,0,1014,77,738
442,30338736,Survey of Machine Learning Techniques for Prediction of the Isoform Specificity of Cytochrome P450 Substrates,"Background:                    Determination or prediction of the Absorption, Distribution, Metabolism, and Excretion (ADME) properties of drug candidates and drug-induced toxicity plays crucial roles in drug discovery and development. Metabolism is one of the most complicated pharmacokinetic properties to be understood and predicted. However, experimental determination of the substrate binding, selectivity, sites and rates of metabolism is time- and recourse- consuming. In the phase I metabolism of foreign compounds (i.e., most of drugs), cytochrome P450 enzymes play a key role. To help develop drugs with proper ADME properties, computational models are highly desired to predict the ADME properties of drug candidates, particularly for drugs binding to cytochrome P450.              Objective:                    This narrative review aims to briefly summarize machine learning techniques used in the prediction of the cytochrome P450 isoform specificity of drug candidates.              Results:                    Both single-label and multi-label classification methods have demonstrated good performance on modelling and prediction of the isoform specificity of substrates based on their quantitative descriptors.              Conclusion:                    This review provides a guide for researchers to develop machine learning-based methods to predict the cytochrome P450 isoform specificity of drug candidates.",2019-05-01,8,1429,109,738
2540,30553609,An overview of deep learning in medical imaging focusing on MRI,"What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of deep learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.",2019-05-01,86,1607,63,738
507,31044724,Big data and machine learning algorithms for health-care delivery,"Analysis of big data by machine learning offers considerable advantages for assimilation and evaluation of large amounts of complex health-care data. However, to effectively use machine learning tools in health care, several limitations must be addressed and key issues considered, such as its clinical implementation and ethics in health-care delivery. Advantages of machine learning include flexibility and scalability compared with traditional biostatistical methods, which makes it deployable for many tasks, such as risk stratification, diagnosis and classification, and survival predictions. Another advantage of machine learning algorithms is the ability to analyse diverse data types (eg, demographic data, laboratory findings, imaging data, and doctors' free-text notes) and incorporate them into predictions for disease risk, diagnosis, prognosis, and appropriate treatments. Despite these advantages, the application of machine learning in health-care delivery also presents unique challenges that require data pre-processing, model training, and refinement of the system with respect to the actual clinical problem. Also crucial are ethical considerations, which include medico-legal implications, doctors' understanding of machine learning tools, and data privacy and security. In this Review, we discuss some of the benefits and challenges of big data and machine learning in health care.",2019-05-01,52,1402,65,738
2253,31118821,Taiwan's National Health Insurance Research Database: past and future,"Taiwan's National Health Insurance Research Database (NHIRD) exemplifies a population-level data source for generating real-world evidence to support clinical decisions and health care policy-making. Like with all claims databases, there have been some validity concerns of studies using the NHIRD, such as the accuracy of diagnosis codes and issues around unmeasured confounders. Endeavors to validate diagnosed codes or to develop methodologic approaches to address unmeasured confounders have largely increased the reliability of NHIRD studies. Recently, Taiwan's Ministry of Health and Welfare (MOHW) established a Health and Welfare Data Center (HWDC), a data repository site that centralizes the NHIRD and about 70 other health-related databases for data management and analyses. To strengthen the protection of data privacy, investigators are required to conduct on-site analysis at an HWDC through remote connection to MOHW servers. Although the tight regulation of this on-site analysis has led to inconvenience for analysts and has increased time and costs required for research, the HWDC has created opportunities for enriched dimensions of study by linking across the NHIRD and other databases. In the near future, researchers will have greater opportunity to distill knowledge from the NHIRD linked to hospital-based electronic medical records databases containing unstructured patient-level information by using artificial intelligence techniques, including machine learning and natural language processes. We believe that NHIRD with multiple data sources could represent a powerful research engine with enriched dimensions and could serve as a guiding light for real-world evidence-based medicine in Taiwan.",2019-05-01,84,1722,69,738
564,30959445,"Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis","Machine learning (ML) algorithms have made a tremendous impact in the field of medical imaging. While medical imaging datasets have been growing in size, a challenge for supervised ML algorithms that is frequently mentioned is the lack of annotated data. As a result, various methods that can learn with less/other types of supervision, have been proposed. We give an overview of semi-supervised, multiple instance, and transfer learning in medical imaging, both in diagnosis or segmentation tasks. We also discuss connections between these learning scenarios, and opportunities for future research. A dataset with the details of the surveyed papers is available via https://figshare.com/articles/Database_of_surveyed_literature_in_Not-so-supervised_a_survey_of_semi-supervised_multi-instance_and_transfer_learning_in_medical_image_analysis_/7479416.",2019-05-01,33,850,111,738
2261,31109150,Gap Junction Channels of Innexins and Connexins: Relations and Computational Perspectives,"Gap junction (GJ) channels in invertebrates have been used to understand cell-to-cell communication in vertebrates. GJs are a common form of intercellular communication channels which connect the cytoplasm of adjacent cells. Dysregulation and structural alteration of the gap junction-mediated communication have been proven to be associated with a myriad of symptoms and tissue-specific pathologies. Animal models relying on the invertebrate nervous system have exposed a relationship between GJs and the formation of electrical synapses during embryogenesis and adulthood. The modulation of GJs as a therapeutic and clinical tool may eventually provide an alternative for treating tissue formation-related diseases and cell propagation. This review concerns the similarities between Hirudo medicinalis innexins and human connexins from nucleotide and protein sequence level perspectives. It also sets forth evidence of computational techniques applied to the study of proteins, sequences, and molecular dynamics. Furthermore, we propose machine learning techniques as a method that could be used to study protein structure, gap junction inhibition, metabolism, and drug development.",2019-05-01,4,1184,89,738
2406,30850092,Deep learning can see the unseeable: predicting molecular markers from MRI of brain gliomas,"This paper describes state-of-the-art methods for molecular biomarker prediction utilising magnetic resonance imaging. This review paper covers both classical machine learning approaches and deep learning approaches to identifying the predictive features and to perform the actual prediction. In particular, there have been substantial advances in recent years in predicting molecular markers for diffuse gliomas. There are few examples of molecular marker prediction for other brain tumours. Deep learning has contributed significantly to these advances, but suffers from challenges in identifying the features used to make predictions. Tools to better identify and understand those features represent an important area of active research.",2019-05-01,6,740,91,738
512,31041615,A Special Report on Changing Trends in Preventive Stroke/Cardiovascular Risk Assessment Via B-Mode Ultrasonography,"Purpose of review:                    Cardiovascular disease (CVD) and stroke risk assessment have been largely based on the success of traditional statistically derived risk calculators such as Pooled Cohort Risk Score or Framingham Risk Score. However, over the last decade, automated computational paradigms such as machine learning (ML) and deep learning (DL) techniques have penetrated into a variety of medical domains including CVD/stroke risk assessment. This review is mainly focused on the changing trends in CVD/stroke risk assessment and its stratification from statistical-based models to ML-based paradigms using non-invasive carotid ultrasonography.              Recent findings:                    In this review, ML-based strategies are categorized into two types: non-image (or conventional ML-based) and image-based (or integrated ML-based). The success of conventional (non-image-based) ML-based algorithms lies in the different data-driven patterns or features which are used to train the ML systems. Typically these features are the patients' demographics, serum biomarkers, and multiple clinical parameters. The integrated (image-based) ML-based algorithms integrate the features derived from the ultrasound scans of the arterial walls (such as morphological measurements) with conventional risk factors in ML frameworks. Even though the review covers ML-based system designs for carotid and coronary ultrasonography, the main focus of the review is on CVD/stroke risk scores based on carotid ultrasound. There are two key conclusions from this review: (i) fusion of image-based features with conventional cardiovascular risk factors can lead to more accurate CVD/stroke risk stratification; (ii) the ability to handle multiple sources of information in big data framework using artificial intelligence-based paradigms (such as ML and DL) is likely to be the future in preventive CVD/stroke risk assessment.",2019-05-01,9,1930,114,738
2541,30548534,Integrating molecular networks with genetic variant interpretation for precision medicine,"More reliable and cheaper sequencing technologies have revealed the vast mutational landscapes characteristic of many phenotypes. The analysis of such genetic variants has led to successful identification of altered proteins underlying many Mendelian disorders. Nevertheless the simple one-variant one-phenotype model valid for many monogenic diseases does not capture the complexity of polygenic traits and disorders. Although experimental and computational approaches have improved detection of functionally deleterious variants and important interactions between gene products, the development of comprehensive models relating genotype and phenotypes remains a challenge in the field of genomic medicine. In this context, a new view of the pathologic state as significant perturbation of the network of interactions between biomolecules is crucial for the identification of biochemical pathways associated with complex phenotypes. Seminal studies in systems biology combined the analysis of genetic variation with protein-protein interaction networks to demonstrate that even as biological systems evolve to be robust to genetic variation, their topologies create disease vulnerabilities. More recent analyses model the impact of genetic variants as changes to the ""wiring"" of the interactome to better capture heterogeneity in genotype-phenotype relationships. These studies lay the foundation for using networks to predict variant effects at scale using machine-learning or algorithmic approaches. A wealth of databases and resources for the annotation of genotype-phenotype relationships have been developed to support developments in this area. This overview describes how study of the molecular interactome has generated insights linking the organization of biological systems to disease mechanism, and how this information can enable precision medicine. This article is categorized under: Translational, Genomic, and Systems Medicine > Translational Medicine Biological Mechanisms > Cell Signaling Models of Systems Properties and Processes > Mechanistic Models Analytical and Computational Methods > Computational Methods.",2019-05-01,11,2132,89,738
496,31052598,Revealing Drug-Target Interactions with Computational Models and Algorithms,"Background:                    Identifying possible drug-target interactions (DTIs) has become an important task in drug research and development. Although high-throughput screening is becoming available, experimental methods narrow down the validation space because of extremely high cost, low success rate, and time consumption. Therefore, various computational models have been exploited to infer DTI candidates.              Methods:                    We introduced relevant databases and packages, mainly provided a comprehensive review of computational models for DTI identification, including network-based algorithms and machine learning-based methods. Specially, machine learning-based methods mainly include bipartite local model, matrix factorization, regularized least squares, and deep learning.              Results:                    Although computational methods have obtained significant improvement in the process of DTI prediction, these models have their limitations. We discussed potential avenues for boosting DTI prediction accuracy as well as further directions.",2019-05-01,9,1089,75,738
533,31009397,Machine Learning and Deep Neural Networks in Thoracic and Cardiovascular Imaging,"Advances in technology have always had the potential and opportunity to shape the practice of medicine, and in no medical specialty has technology been more rapidly embraced and adopted than radiology. Machine learning and deep neural networks promise to transform the practice of medicine, and, in particular, the practice of diagnostic radiology. These technologies are evolving at a rapid pace due to innovations in computational hardware and novel neural network architectures. Several cutting-edge postprocessing analysis applications are actively being developed in the fields of thoracic and cardiovascular imaging, including applications for lesion detection and characterization, lung parenchymal characterization, coronary artery assessment, cardiac volumetry and function, and anatomic localization. Cardiothoracic and cardiovascular imaging lies at the technological forefront of radiology due to a confluence of technical advances. Enhanced equipment has enabled computed tomography and magnetic resonance imaging scanners that can safely capture images that freeze the motion of the heart to exquisitely delineate fine anatomic structures. Computing hardware developments have enabled an explosion in computational capabilities and in data storage. Progress in software and fluid mechanical models is enabling complex 3D and 4D reconstructions to not only visualize and assess the dynamic motion of the heart, but also quantify its blood flow and hemodynamics. And now, innovations in machine learning, particularly in the form of deep neural networks, are enabling us to leverage the increasingly massive data repositories that are prevalent in the field. Here, we discuss developments in machine learning techniques and deep neural networks to highlight their likely role in future radiologic practice, both in and outside of image interpretation and analysis. We discuss the concepts of validation, generalizability, and clinical utility, as they pertain to this and other new technologies, and we reflect upon the opportunities and challenges of bringing these into daily use.",2019-05-01,9,2094,80,738
562,30962048,Canadian Association of Radiologists White Paper on Ethical and Legal Issues Related to Artificial Intelligence in Radiology,"Artificial intelligence (AI) software that analyzes medical images is becoming increasingly prevalent. Unlike earlier generations of AI software, which relied on expert knowledge to identify imaging features, machine learning approaches automatically learn to recognize these features. However, the promise of accurate personalized medicine can only be fulfilled with access to large quantities of medical data from patients. This data could be used for purposes such as predicting disease, diagnosis, treatment optimization, and prognostication. Radiology is positioned to lead development and implementation of AI algorithms and to manage the associated ethical and legal challenges. This white paper from the Canadian Association of Radiologists provides a framework for study of the legal and ethical issues related to AI in medical imaging, related to patient data (privacy, confidentiality, ownership, and sharing); algorithms (levels of autonomy, liability, and jurisprudence); practice (best practices and current legal framework); and finally, opportunities in AI from the perspective of a universal health care system.",2019-05-01,7,1128,124,738
2234,31153155,Lumbar spondylolisthesis: modern registries and the development of artificial intelligence,"OBJECTIVEThere are a wide variety of comparative treatment options in neurosurgery that do not lend themselves to traditional randomized controlled trials. The object of this article was to examine how clinical registries might be used to generate new evidence to support a particular treatment option when comparable options exist. Lumbar spondylolisthesis is used as an example.METHODSThe authors reviewed the literature examining the comparative effectiveness of decompression alone versus decompression with fusion for lumbar stenosis with degenerative spondylolisthesis. Modern data acquisition for the creation of registries was also reviewed with an eye toward how artificial intelligence for the treatment of lumbar spondylolisthesis might be explored.RESULTSCurrent randomized controlled trials differ on the importance of adding fusion when performing decompression for lumbar spondylolisthesis. Standardized approaches to extracting data from the electronic medical record as well as the ability to capture radiographic imaging and incorporate patient-reported outcomes (PROs) will ultimately lead to the development of modern, structured, data-filled registries that will lay the foundation for machine learning.CONCLUSIONSThere is a growing realization that patient experience, satisfaction, and outcomes are essential to improving the overall quality of spine care. There is a need to use practical, validated PRO tools in the quest to optimize outcomes within spine care. Registries will be designed to contain robust clinical data in which predictive analytics can be generated to develop and guide data-driven personalized spine care.",2019-06-01,3,1651,90,707
2589,30448825,"Cycles, Arrows and Turbulence: Time Patterns in Renal Disease, a Path from Epidemiology to Personalized Medicine?","Patients with end-stage renal disease (ESRD) experience unique patterns in their lifetime, such as the start of dialysis and renal transplantation. In addition, there is also an intricate link between ESRD and biological time patterns. In terms of cyclic patterns, the circadian blood pressure (BP) rhythm can be flattened, contributing to allostatic load, whereas the circadian temperature rhythm is related to the decline in BP during hemodialysis (HD). Seasonal variations in BP and interdialytic-weight gain have been observed in ESRD patients in addition to a profound relative increase in mortality during the winter period. Moreover, nonphysiological treatment patters are imposed in HD patients, leading to an excess mortality at the end of the long interdialytic interval. Recently, new evidence has emerged on the prognostic impact of trajectories of common clinical and laboratory parameters such as BP, body temperature, and serum albumin, in addition to single point in time measurements. Backward analysis of changes in cardiovascular, nutritional, and inflammatory parameters before the occurrence as hospitalization or death has shown that changes may already occur within months to even 1-2 years before the event, possibly providing a window of opportunity for earlier interventions. Disturbances in physiological variability, such as in heart rate, characterized by a loss of fractal patterns, are associated with increased mortality. In addition, an increase in random variability in different parameters such as BP and sodium is also associated with adverse outcomes. Novel techniques, based on time-dependent analysis of variability and trends and interactions of multiple physiological and laboratory parameters, for which machine-learning -approaches may be necessary, are likely of help to the clinician in the future. However, upcoming research should also evaluate whether dynamic patterns observed in large epidemiological studies have relevance for the individual risk profile of the patient.",2019-06-01,3,2021,113,707
2209,31200809,Association mapping in plants in the post-GWAS genomics era,"With the availability of DNA-based molecular markers during early 1980s and that of sophisticated statistical tools in late 1980s and later, it became possible to identify genomic regions that control a quantitative trait. The two methods used for this purpose included quantitative trait loci (QTL) interval mapping and genome-wide association mapping/studies (GWAS). Both these methods have their own merits and demerits, so that newer approaches were developed in order to deal with the demerits. We have now entered a post-GWAS era, where either the original data on individual genotypes are being used again keeping in view the results of GWAS or else summary statistics obtained through GWAS is subjected to further analysis. The first half of this review briefly deals with the approaches that were used for GWAS, the GWAS results obtained in some major crops (maize, wheat, rice, sorghum and soybean), their utilization for crop improvement and the improvements made to address the limitations of original GWA studies (computational demand, multiple testing and false discovery, rare marker alleles, etc.). These improvements included the development of multi-locus and multi-trait analysis, joint linkage association mapping, etc. Since originally GWA studies were used for mere identification of marker-trait association for marker-assisted selection, the second half of the review is devoted to activities in post-GWAS era, which include different methods that are being used for identification of causal variants and their prioritization (meta-analysis, pathway-based analysis, methylation QTL), functional characterization of candidate signals, gene- and gene-set based association mapping, GWAS using high dimensional data through machine learning, etc. The last section deals with popular resources available for GWAS in plants in the post-GWAS era and the implications of the results of post-GWAS for crop improvement.",2019-06-01,8,1934,59,707
2238,31149787,Promising Artificial Intelligence-Machine Learning-Deep Learning Algorithms in Ophthalmology,"The lifestyle of modern society has changed significantly with the emergence of artificial intelligence (AI), machine learning (ML), and deep learning (DL) technologies in recent years. Artificial intelligence is a multidimensional technology with various components such as advanced algorithms, ML and DL. Together, AI, ML, and DL are expected to provide automated devices to ophthalmologists for early diagnosis and timely treatment of ocular disorders in the near future. In fact, AI, ML, and DL have been used in ophthalmic setting to validate the diagnosis of diseases, read images, perform corneal topographic mapping and intraocular lens calculations. Diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are the 3 most common causes of irreversible blindness on a global scale. Ophthalmic imaging provides a way to diagnose and objectively detect the progression of a number of pathologies including DR, AMD, glaucoma, and other ophthalmic disorders. There are 2 methods of imaging used as diagnostic methods in ophthalmic practice: fundus digital photography and optical coherence tomography (OCT). Of note, OCT has become the most widely used imaging modality in ophthalmology settings in the developed world. Changes in population demographics and lifestyle, extension of average lifespan, and the changing pattern of chronic diseases such as obesity, diabetes, DR, AMD, and glaucoma create a rising demand for such images. Furthermore, the limitation of availability of retina specialists and trained human graders is a major problem in many countries. Consequently, given the current population growth trends, it is inevitable that analyzing such images is time-consuming, costly, and prone to human error. Therefore, the detection and treatment of DR, AMD, glaucoma, and other ophthalmic disorders through unmanned automated applications system in the near future will be inevitable. We provide an overview of the potential impact of the current AI, ML, and DL methods and their applications on the early detection and treatment of DR, AMD, glaucoma, and other ophthalmic diseases.",2019-06-01,12,2122,92,707
889,31281846,Human Systems Biology and Metabolic Modelling: A Review-From Disease Metabolism to Precision Medicine,"In cell and molecular biology, metabolism is the only system that can be fully simulated at genome scale. Metabolic systems biology offers powerful abstraction tools to simulate all known metabolic reactions in a cell, therefore providing a snapshot that is close to its observable phenotype. In this review, we cover the 15 years of human metabolic modelling. We show that, although the past five years have not experienced large improvements in the size of the gene and metabolite sets in human metabolic models, their accuracy is rapidly increasing. We also describe how condition-, tissue-, and patient-specific metabolic models shed light on cell-specific changes occurring in the metabolic network, therefore predicting biomarkers of disease metabolism. We finally discuss current challenges and future promising directions for this research field, including machine/deep learning and precision medicine. In the omics era, profiling patients and biological processes from a multiomic point of view is becoming more common and less expensive. Starting from multiomic data collected from patients and N-of-1 trials where individual patients constitute different case studies, methods for model-building and data integration are being used to generate patient-specific models. Coupled with state-of-the-art machine learning methods, this will allow characterizing each patient's disease phenotype and delivering precision medicine solutions, therefore leading to preventative medicine, reduced treatment, and in silico clinical trials.",2019-06-01,6,1538,101,707
1712,31622219,Identifying Cancer Targets Based on Machine Learning Methods via Chou's 5-steps Rule and General Pseudo Components,"In recent years, the successful implementation of human genome project has made people realize that genetic, environmental and lifestyle factors should be combined together to study cancer due to the complexity and various forms of the disease. The increasing availability and growth rate of 'big data' derived from various omics, opens a new window for study and therapy of cancer. In this paper, we will introduce the application of machine learning methods in handling cancer big data including the use of artificial neural networks, support vector machines, ensemble learning and nave Bayes classifiers.",2019-06-01,1,608,114,707
2212,31194543,Deep Learning in Chemistry,"Machine learning enables computers to address problems by learning from data. Deep learning is a type of machine learning that uses a hierarchical recombination of features to extract pertinent information and then learn the patterns represented in the data. Over the last eight years, its abilities have increasingly been applied to a wide variety of chemical challenges, from improving computational chemistry to drug and materials design and even synthesis planning. This review aims to explain the concepts of deep learning to chemists from any background and follows this with an overview of the diverse applications demonstrated in the literature. We hope that this will empower the broader chemical community to engage with this burgeoning field and foster the growing movement of deep learning accelerated chemistry.",2019-06-01,24,824,26,707
485,30246637,Towards Computational Models of Identifying Protein Ubiquitination Sites,"Ubiquitination is an important post-translational modification (PTM) process for the regulation of protein functions, which is associated with cancer, cardiovascular and other diseases. Recent initiatives have focused on the detection of potential ubiquitination sites with the aid of physicochemical test approaches in conjunction with the application of computational methods. The identification of ubiquitination sites using laboratory tests is especially susceptible to the temporality and reversibility of the ubiquitination processes, and is also costly and time-consuming. It has been demonstrated that computational methods are effective in extracting potential rules or inferences from biological sequence collections. Up to the present, the computational strategy has been one of the critical research approaches that have been applied for the identification of ubiquitination sites, and currently, there are numerous state-of-the-art computational methods that have been developed from machine learning and statistical analysis to undertake such work. In the present study, the construction of benchmark datasets is summarized, together with feature representation methods, feature selection approaches and the classifiers involved in several previous publications. In an attempt to explore pertinent development trends for the identification of ubiquitination sites, an independent test dataset was constructed and the predicting results obtained from five prediction tools are reported here, together with some related discussions.",2019-06-01,1,1544,72,707
1786,31525565,Addressing heterogeneity (and homogeneity) in treatment mechanisms in depression and the potential to develop diagnostic and predictive biomarkers,"It has been 10 years since machine learning was first applied to neuroimaging data in psychiatric disorders to identify diagnostic and prognostic markers at the level of the individual. Proof of concept findings in major depression have since been extended in international samples and are beginning to include hundreds of samples from multisite data. Neuroimaging provides the unique capability to detect an acute depressive state in major depression, while we would not expect perfect classification with current diagnostic criteria which are based solely on clinical features. We review developments and the potential impact of heterogeneity, as well as homogeneity, on classification for diagnosis and prediction of clinical outcome. It is likely that there are distinct biotypes which comprise the disorder and which predict clinical outcome. Neuroimaging-based biotypes could aid in identifying the illness in individuals who are unable to recognise their illness and perhaps to identify the treatment resistant form early in the course of the illness. We propose that heterogeneous symptom profiles can arise from a limited number of neural biotypes and that apparently heterogeneous clinical outcomes include a common baseline predictor and common mechanism of treatment. Baseline predictors of clinical outcome reflect factors which indicate the general likelihood of response as well as those which are selective for a particular form of treatment. Irrespective of the mechanism, the capacity for response will moderate the outcome, which includes inherent models of interpersonal relationships that could be associated with genetic risk load and represented by patterns of functional and structural neural correlates as a predictive biomarker. We propose that methods which directly address heterogeneity are essential and that a synergistic combination could bring together data-driven inductive and symptom-based deductive approaches. Through this iterative process, major depression can develop from being syndrome characterized by a collection of symptoms to a disease with an identifiable pathophysiology.",2019-06-01,0,2121,146,707
1776,31544714,Recent Advances and Computational Approaches in Peptide Drug Discovery,"Background:                    Drug design and development is a vast field that requires huge investment along with a long duration for providing approval to suitable drug candidates. With the advancement in the field of genomics, the information about druggable targets is being updated at a fast rate which is helpful in finding a cure for various diseases.              Methods:                    There are certain biochemicals as well as physiological advantages of using peptide-based therapeutics. Additionally, the limitations of peptide-based drugs can be overcome by modulating the properties of peptide molecules through various biomolecular engineering techniques. Recent advances in computational approaches have been helpful in studying the effect of peptide drugs on the biomolecular targets. Receptor - ligand-based molecular docking studies have made it easy to screen compatible inhibitors against a target.Furthermore, there are simulation tools available to evaluate stability of complexes at the molecular level. Machine learning methods have added a new edge by enabling accurate prediction of therapeutic peptides.              Results:                    Peptide-based drugs are expected to take over many popular drugs in the near future due to their biosafety, lower off-target binding chances and multifunctional properties.              Conclusion:                    This article summarises the latest developments in the field of peptide-based therapeutics related to their usage, tools, and databases.",2019-06-01,2,1532,70,707
2219,31174387,A Structure-Based Drug Discovery Paradigm,"Structure-based drug design is becoming an essential tool for faster and more cost-efficient lead discovery relative to the traditional method. Genomic, proteomic, and structural studies have provided hundreds of new targets and opportunities for future drug discovery. This situation poses a major problem: the necessity to handle the ""big data"" generated by combinatorial chemistry. Artificial intelligence (AI) and deep learning play a pivotal role in the analysis and systemization of larger data sets by statistical machine learning methods. Advanced AI-based sophisticated machine learning tools have a significant impact on the drug discovery process including medicinal chemistry. In this review, we focus on the currently available methods and algorithms for structure-based drug design including virtual screening and de novo drug design, with a special emphasis on AI- and deep-learning-based methods used for drug discovery.",2019-06-01,22,936,41,707
532,31009920,The flux and impact of wastewater infrastructure microorganisms on human and ecosystem health,"Wastewater infrastructure is designed, in part, to remove microorganisms. However, many microorganisms are able to colonize infrastructure and resist treatment, resulting in an enormous flux of microorganisms to urban adjacent waters. These urban-associated microorganisms are discharged through three primary routes 1) failing infrastructure, 2) stormwater, and 3) treated wastewater effluent. Bacterial load estimates indicate failing infrastructure should be considered an equivalent source of microbial pollution as the other routes, but overall discharges are not well parameterized. More sophisticated methods, such as machine learning algorithms and microbiome characterization, are now used to track urban-derived microorganisms, including targets beyond fecal indicators, but development of methods to quantify the impact of these microbes/genes on human and ecosystem health is needed.",2019-06-01,2,895,93,707
531,31012166,Toward Design of Novel Materials for Organic Electronics,"Materials for organic electronics are presently used in prominent applications, such as displays in mobile devices, while being intensely researched for other purposes, such as organic photovoltaics, large-area devices, and thin-film transistors. Many of the challenges to improve and optimize these applications are material related and there is a nearly infinite chemical space that needs to be explored to identify the most suitable material candidates. Established experimental approaches struggle with the size and complexity of this chemical space. Herein, the development of simulation methods is addressed, with a particular emphasis on predictive multiscale protocols, to complement experimental research in the identification of novel materials and illustrate the potential of these methods with a few prominent recent applications. Finally, the potential of machine learning and methods based on artificial intelligence is discussed to further accelerate the search for new materials.",2019-06-01,4,995,56,707
1296,32008510,A Survey on Medical Image Analysis in Capsule Endoscopy,"Background and objective:                    Capsule Endoscopy (CE) is a non-invasive, patient-friendly alternative to conventional endoscopy procedure. However, CE produces 6 to 8 hrs long video posing a tedious challenge to a gastroenterologist for abnormality detection. Major challenges to an expert are lengthy videos, need of constant concentration and subjectivity of the abnormality. To address these challenges along with high diagnostic accuracy, design and development of automated abnormality detection system is a must. Machine learning and computer vision techniques are devised to develop such automated systems.              Methods:                    Study presents a review of quality research papers published in IEEE, Scopus, and Science Direct database with search criteria as capsule endoscopy, engineering, and journal papers. The initial search retrieved 144 publications. After evaluating all articles, 62 publications pertaining to image analysis are selected.              Results:                    This paper presents a rigorous review comprising all the aspects of medical image analysis concerning capsule endoscopy namely video summarization and redundant image elimination, Image enhancement and interpretation, segmentation and region identification, Computer-aided abnormality detection in capsule endoscopy, Image and video compression. The study provides a comparative analysis of various approaches, experimental setup, performance, strengths, and limitations of the aspects stated above.              Conclusions:                    The analyzed image analysis techniques for capsule endoscopy have not yet overcome all current challenges mainly due to lack of dataset and complex nature of the gastrointestinal tract.",2019-06-01,1,1759,55,707
2225,31163504,"Artificial Intelligence in Musculoskeletal Imaging: Review of Current Literature, Challenges, and Trends","Artificial intelligence (AI) has gained major attention with a rapid increase in the number of published articles, mostly recently. This review provides a general understanding of how AI can or will be useful to the musculoskeletal radiologist. After a brief technical background on AI, machine learning, and deep learning, we illustrate, through examples from the musculoskeletal literature, potential AI applications in the various steps of the radiologist's workflow, from managing the request to communication of results. The implementation of AI solutions does not go without challenges and limitations. These are also discussed, as well as the trends and perspectives.",2019-06-01,5,674,104,707
2498,30635892,Workflow Development for the Functional Characterization of ncRNAs,"During the last decade, ncRNAs have been investigated intensively and revealed their regulatory role in various biological processes. Worldwide research efforts have identified numerous ncRNAs and multiple RNA subtypes, which are attributed to diverse functionalities known to interact with different functional layers, from DNA and RNA to proteins. This makes the prediction of functions for newly identified ncRNAs challenging. Current bioinformatics and systems biology approaches show promising results to facilitate an identification of these diverse ncRNA functionalities. Here, we review (a) current experimental protocols, i.e., for Next Generation Sequencing, for a successful identification of ncRNAs; (b) sequencing data analysis workflows as well as available computational environments; and (c) state-of-the-art approaches to functionally characterize ncRNAs, e.g., by means of transcriptome-wide association studies, molecular network analyses, or artificial intelligence guided prediction. In addition, we present a strategy to cover the identification and functional characterization of unknown transcripts by using connective workflows.",2019-06-01,2,1153,66,707
1761,31555764,Applications of Machine Learning Approaches in Emergency Medicine; a Review Article,"Using artificial intelligence and machine learning techniques in different medical fields, especially emergency medicine is rapidly growing. In this paper, studies conducted in the recent years on using artificial intelligence in emergency medicine have been collected and assessed. These studies belonged to three categories: prediction and detection of disease; prediction of need for admission, discharge and also mortality; and machine learning based triage systems. In each of these categories, the most important studies have been chosen and accuracy and results of the algorithms have been briefly evaluated by mentioning machine learning techniques and used datasets.",2019-06-01,5,675,83,707
2497,30635896,Computational Resources for Prediction and Analysis of Functional miRNA and Their Targetome,"microRNAs are evolutionarily conserved, endogenously produced, noncoding RNAs (ncRNAs) of approximately 19-24 nucleotides (nts) in length known to exhibit gene silencing of complementary target sequence. Their deregulated expression is reported in various disease conditions and thus has therapeutic implications. In the last decade, various computational resources are published in this field. In this chapter, we have reviewed bioinformatics resources, i.e., miRNA-centered databases, algorithms, and tools to predict miRNA targets. First section has enlisted more than 75 databases, which mainly covers information regarding miRNA registries, targets, disease associations, differential expression, interactions with other noncoding RNAs, and all-in-one resources. In the algorithms section, we have compiled about 140 algorithms from eight subcategories, viz. for the prediction of precursor (pre-) and mature miRNAs. These algorithms are developed on various sequence, structure, and thermodynamic based features incorporated into different machine learning techniques (MLTs). In addition, computational identification of miRNAs from high-throughput next generation sequencing (NGS) data and their variants, viz. isomiRs, differential expression, miR-SNPs, and functional annotation, are discussed. Prediction and analysis of miRNAs and their associated targets are also evaluated under miR-targets section providing knowledge regarding novel miRNA targets and complex host-pathogen interactions. In conclusion, we have provided comprehensive review of in silico resources published in miRNA research to help scientific community be updated and choose the appropriate tool according to their needs.",2019-06-01,4,1703,91,707
2233,31153403,Computer-aided diagnosis of congestive heart failure using ECG signals - A review,"The heart muscle pumps blood to vital organs, which is indispensable for human life. Congestive heart failure (CHF) is characterized by the inability of the heart to pump blood adequately throughout the body without an increase in intracardiac pressure. The symptoms include lung and peripheral congestion, leading to breathing difficulty and swollen limbs, dizziness from reduced delivery of blood to the brain, as well as arrhythmia. Coronary artery disease, myocardial infarction, and medical co-morbidities such as kidney disease, diabetes, and high blood pressure all take a toll on the heart and can impair myocardial function. CHF prevalence is growing worldwide. It afflicts millions of people globally, and is a leading cause of death. Hence, proper diagnosis, monitoring and management are imperative. The importance of an objective CHF diagnostic tool cannot be overemphasized. Standard diagnostic tests for CHF include chest X-ray, magnetic resonance imaging (MRI), nuclear imaging, echocardiography, and invasive angiography. However, these methods are costly, time-consuming, and they can be operator-dependent. Electrocardiography (ECG) is inexpensive and widely accessible, but ECG changes are typically not specific for CHF diagnosis. A properly designed computer-aided detection (CAD) system for CHF, based on the ECG, would potentially reduce subjectivity and provide quantitative assessment for informed decision-making. Herein, we review existing CAD for automatic CHF diagnosis, and highlight the development of an ECG-based CAD diagnostic system that employs deep learning algorithms to automatically detect CHF.",2019-06-01,2,1635,81,707
2223,31171259,The Future of Cardiovascular Computed Tomography: Advanced Analytics and Clinical Insights,"Cardiovascular computed tomography (CCT) has undergone rapid maturation over the last decade and is now of proven clinical utility in the diagnosis and management of coronary artery disease, in guiding structural heart disease intervention, and in the diagnosis and treatment of congenital heart disease. The next decade will undoubtedly witness further advances in hardware and advanced analytics that will potentially see an increasingly core role for CCT at the center of clinical cardiovascular practice. In coronary artery disease assessment this may be via improved hemodynamic adjudication, and shear stress analysis using computational flow dynamics, more accurate and robust plaque characterization with spectral or photon-counting CT, or advanced quantification of CT data via artificial intelligence, machine learning, and radiomics. In structural heart disease, CCT is already pivotal to procedural planning with adjudication of gradients before and following intervention, whereas in congenital heart disease CCT is already used to support clinical decision making from neonates to adults, often with minimal radiation dose. In both these areas the role of computational flow dynamics, advanced tissue printing, and image modelling has the potential to revolutionize the way these complex conditions are managed, and CCT is likely to become an increasingly critical enabler across the whole advancing field of cardiovascular medicine.",2019-06-01,12,1447,90,707
2269,31090273,Genomic data mining for functional annotation of human long noncoding RNAs,"Life may have begun in an RNA world, which is supported by increasing evidence of the vital role that RNAs perform in biological systems. In the human genome, most genes actually do not encode proteins; they are noncoding RNA genes. The largest class of noncoding genes is known as long noncoding RNAs (lncRNAs), which are transcripts greater in length than 200 nucleotides, but with no protein-coding capacity. While some lncRNAs have been demonstrated to be key regulators of gene expression and 3D genome organization, most lncRNAs are still uncharacterized. We thus propose several data mining and machine learning approaches for the functional annotation of human lncRNAs by leveraging the vast amount of data from genetic and genomic studies. Recent results from our studies and those of other groups indicate that genomic data mining can give insights into lncRNA functions and provide valuable information for experimental studies of candidate lncRNAs associated with human disease.",2019-06-01,7,990,74,707
1260,32055582,A review of computational drug repurposing,"Although sciences and technology have progressed rapidly, de novo drug development has been a costly and time-consuming process over the past decades. In view of these circumstances, 'drug repurposing' (or 'drug repositioning') has appeared as an alternative tool to accelerate drug development process by seeking new indications for already approved drugs rather than discovering de novo drug compounds, nowadays accounting for 30% of newly marked drugs in the U.S. In the meantime, the explosive and large-scale growth of molecular, genomic and phenotypic data of pharmacological compounds is enabling the development of new area of drug repurposing called computational drug repurposing. This review provides an overview of recent progress in the area of computational drug repurposing. First, it summarizes available repositioning strategies, followed by computational methods commonly used. Then, it describes validation techniques for repurposing studies. Finally, it concludes by discussing the remaining challenges in computational repurposing.",2019-06-01,14,1052,42,707
2430,30808014,Deep learning for electroencephalogram (EEG) classification tasks: a review,"Objective:                    Electroencephalography (EEG) analysis has been an important tool in neuroscience with applications in neuroscience, neural engineering (e.g. Brain-computer interfaces, BCI's), and even commercial applications. Many of the analytical tools used in EEG studies have used machine learning to uncover relevant information for neural classification and neuroimaging. Recently, the availability of large EEG data sets and advances in machine learning have both led to the deployment of deep learning architectures, especially in the analysis of EEG signals and in understanding the information it may contain for brain functionality. The robust automatic classification of these signals is an important step towards making the use of EEG more practical in many applications and less reliant on trained professionals. Towards this goal, a systematic review of the literature on deep learning applications to EEG classification was performed to address the following critical questions: (1) Which EEG classification tasks have been explored with deep learning? (2) What input formulations have been used for training the deep networks? (3) Are there specific deep learning network structures suitable for specific types of tasks?              Approach:                    A systematic literature review of EEG classification using deep learning was performed on Web of Science and PubMed databases, resulting in 90 identified studies. Those studies were analyzed based on type of task, EEG preprocessing methods, input type, and deep learning architecture.              Main results:                    For EEG classification tasks, convolutional neural networks, recurrent neural networks, deep belief networks outperform stacked auto-encoders and multi-layer perceptron neural networks in classification accuracy. The tasks that used deep learning fell into five general groups: emotion recognition, motor imagery, mental workload, seizure detection, event related potential detection, and sleep scoring. For each type of task, we describe the specific input formulation, major characteristics, and end classifier recommendations found through this review.              Significance:                    This review summarizes the current practices and performance outcomes in the use of deep learning for EEG classification. Practical suggestions on the selection of many hyperparameters are provided in the hope that they will promote or guide the deployment of deep learning to EEG datasets in future research.",2019-06-01,28,2536,75,707
2201,31215361,Recent Advances in Machine Learning Based Prediction of RNA-protein Interactions,"The interactions between RNAs and proteins play critical roles in many biological processes. Therefore, characterizing these interactions becomes critical for mechanistic, biomedical, and clinical studies. Many experimental methods can be used to determine RNA-protein interactions in multiple aspects. However, due to the facts that RNA-protein interactions are tissuespecific and condition-specific, as well as these interactions are weak and frequently compete with each other, those experimental techniques can not be made full use of to discover the complete spectrum of RNA-protein interactions. To moderate these issues, continuous efforts have been devoted to developing high quality computational techniques to study the interactions between RNAs and proteins. Many important progresses have been achieved with the application of novel techniques and strategies, such as machine learning techniques. Especially, with the development and application of CLIP techniques, more and more experimental data on RNA-protein interaction under specific biological conditions are available. These CLIP data altogether provide a rich source for developing advanced machine learning predictors. In this review, recent progresses on computational predictors for RNA-protein interaction were summarized in the following aspects: dataset, prediction strategies, and input features. Possible future developments were also discussed at the end of the review.",2019-06-01,1,1449,80,707
893,31279913,Artificial Intelligence in Aortic Surgery: The Rise of the Machine,"The first concept of Artificial Intelligence (AI) came into attention during 1920s and currently it is rapidly being integrated in our daily clinical practice. The use of AI has evolved from basic image-based analysis into complex decisions related to different surgical procedure. AI has been very widely used in the cardiology field, however the use of such machine-led decisions has been limited and explored at slower pace in surgical practice. The use of AI in cardiac surgery is still at its infancy but growing dramatically to reflect the changes in the clinical decision making process for better patient outcomes. The machine-led but human controlled algorithms will soon be taking over most of the decision making processes in cardiac surgery. This review article focuses on the practice of AI in aortic surgery and the future of such technology-led decision making pathways on patient outcomes, surgeon's learning skills and adaptability.",2019-06-01,1,949,66,707
890,31281326,Systems and Synthetic Biology of Forest Trees: A Bioengineering Paradigm for Woody Biomass Feedstocks,"Fast-growing forest plantations are sustainable feedstocks of plant biomass that can serve as alternatives to fossil carbon resources for materials, chemicals, and energy. Their ability to efficiently harvest light energy and carbon from the atmosphere and sequester this into metabolic precursors for lignocellulosic biopolymers and a wide range of plant specialized metabolites make them excellent biochemical production platforms and living biorefineries. Their large sizes have facilitated multi-omics analyses and systems modeling of key biological processes such as lignin biosynthesis in trees. High-throughput 'omics' approaches have also been applied in segregating tree populations where genetic variation creates abundant genetic perturbations of system components allowing construction of systems genetics models linking genes and pathways to complex trait variation. With this information in hand, it is now possible to start using synthetic biology and genome editing techniques in a bioengineering approach based on a deeper understanding and rational design of biological parts, devices, and integrated systems. However, the complexity of the biology and interacting components will require investment in big data informatics, machine learning, and intuitive visualization to fully explore multi-dimensional patterns and identify emergent properties of biological systems. Predictive systems models could be tested rapidly through high-throughput synthetic biology approaches and multigene editing. Such a bioengineering paradigm, together with accelerated genomic breeding, will be crucial for the development of a new generation of woody biorefinery crops.",2019-06-01,3,1674,101,707
2202,31214847,Structural Imaging in Parkinson's Disease: New Developments,"Purpose of review:                    To review the advances in structural imaging for the diagnosis, prognosis, and treatment of Parkinson's disease (PD) during the last 5 years.              Recent findings:                    Structural imaging using high-field MRI ( 3 T) and new MR sequences sensitive to iron and nigral pigments have achieved to assess in vivo pathological surrogates useful for PD diagnosis (notably decreased nigral neuromelanin and loss of dorsal nigral hyperintensity, increased nigral iron content, diffusivity, and free-water), prodromal diagnosis (decreased neuromelanin signal in the locus coeruleus), and PD progression (with increasing nigral iron content (increasing R2* rate) and nigral damage (increasing free-water)). Additionally, evaluation of atrophy in small monoaminergic nuclei is useful for prognosis, including cholinergic basal forebrain nuclei atrophy for cognitive impairment. New advances in multimodal structural imaging improve diagnosis, prognosis, and prediction of invasive treatment outcome in PD, and may further benefit from machine learning and large scale longitudinal studies to better identify prognostic subtypes.",2019-06-01,6,1176,59,707
2157,31743905,Artificial Intelligence Techniques for Automated Diagnosis of Neurological Disorders,"Background:                    Authors have been advocating the research ideology that a computer-aided diagnosis (CAD) system trained using lots of patient data and physiological signals and images based on adroit integration of advanced signal processing and artificial intelligence (AI)/machine learning techniques in an automated fashion can assist neurologists, neurosurgeons, radiologists, and other medical providers to make better clinical decisions.              Summary:                    This paper presents a state-of-the-art review of research on automated diagnosis of 5 neurological disorders in the past 2 decades using AI techniques: epilepsy, Parkinson's disease, Alzheimer's disease, multiple sclerosis, and ischemic brain stroke using physiological signals and images. Recent research articles on different feature extraction methods, dimensionality reduction techniques, feature selection, and classification techniques are reviewed. Key Message: CAD systems using AI and advanced signal processing techniques can assist clinicians in analyzing and interpreting physiological signals and images more effectively.",2019-06-01,3,1134,84,707
2567,30516106,Understanding Membrane Protein Drug Targets in Computational Perspective,"Membrane proteins play crucial physiological roles in vivo and are the major category of drug targets for pharmaceuticals. The research on membrane protein is a significant part in the drug discovery. The biological process is a cycled network, and the membrane protein is a vital hub in the network since most drugs achieve the therapeutic effect via interacting with the membrane protein. In this review, typical membrane protein targets are described, including GPCRs, transporters and ion channels. Also, we conclude network servers and databases that are referring to the drug, drug-target information and their relevant data. Furthermore, we chiefly introduce the development and practice of modern medicines, particularly demonstrating a series of state-of-the-art computational models for the prediction of drug-target interaction containing network-based approach and machine-learningbased approach as well as showing current achievements. Finally, we discuss the prospective orientation of drug repurposing and drug discovery as well as propose some improved framework in bioactivity data, created or improved predicted approaches, alternative understanding approaches of drugs bioactivity and their biological processes.",2019-06-01,3,1231,72,707
1077,31446573,Eye Movements in Neuropsychological Tasks,"This chapter reviews how recording and analysis of eye movements have been applied to understanding cognitive functioning in patients with neurological disease. Measures derived from the performance of instructed eye movement tests such as the anti-saccade and memory-guided saccade tasks have been shown to be associated with cognitive test performance and the early stages of neurodegenerative disorders including Alzheimer's and Parkinson's disease. Other researchers have taken an ecological approach and recorded the uninstructed pattern of saccades made by patients during performance of established neuropsychological tasks. Studies that have analysed the eye movement strategies used in a number of widely used tests are reviewed, including the Corsi blocks, Tower of London, 'CANTAB' Spatial Working Memory and Brixton Spatial Anticipation test. The findings illustrate that eye movements are not purely in the service of vision, but support visuospatial working memory and forward action planning. Eye movement tests and measures also have potential for application in the assessment and diagnosis of neurological disease and cognitive impairment. Establishing large-scale normative data sets in healthy older adults and use of machine learning multivariate classifier algorithms may be key to further developing eye tracking applications in neuropsychological assessment.",2019-06-01,1,1382,41,707
973,31895018,A review of traditional and machine learning methods applied to animal breeding,"The current livestock management landscape is transitioning to a high-throughput digital era where large amounts of information captured by systems of electro-optical, acoustical, mechanical, and biosensors is stored and analyzed on a daily and hourly basis, and actionable decisions are made based on quantitative and qualitative analytic results. While traditional animal breeding prediction methods have been used with great success until recently, the deluge of information starts to create a computational and storage bottleneck that could lead to negative long-term impacts on herd management strategies if not handled properly. A plethora of machine learning approaches, successfully used in various industrial and scientific applications, made their way in the mainstream approaches for livestock breeding techniques, and current results show that such methods have the potential to match or surpass the traditional approaches, while most of the time they are more scalable from a computational and storage perspective. This article provides a succinct view on what traditional and novel prediction methods are currently used in the livestock breeding field, how successful they are, and how the future of the field looks in the new digital agriculture era.",2019-06-01,2,1265,79,707
2451,30741241,Cancer Phenotype Development: A Literature Review,"EHR-based, computable phenotypes can be leveraged by healthcare organizations and researchers to improve the cohort identification process. The ability to identify patient cohorts using aspects of care and outcomes based on clinical characteristics or diagnostic conditions and/or risk factors presents opportunities to researchers targeting specific populations for drug development and disease interventions. The objective of this review was to summarize the literature describing the development and use of phenotypes for cohort identification of cancer patients. A survey of the literature indexed in PubMed was performed to identify studies using EHR-based phenotypes for use in cancer studies. Specific search criteria were formulated by leveraging a phenotype identification guideline developed by the Phenotypes, Data Standards, and Data Quality Core of the NIH Health Care Systems Research Collaboratory. The final set of articles was examined further to identify 1) the cancer of interest and 2) the different approaches used for phenotype development, validation and implementation. The articles reviewed were specific to breast cancer, colorectal cancer, ovarian cancer, and lung cancer. The approaches taken for phenotype development and validation varied slightly among the relevant publications. Four studies relied on chart review, three utilized machine learning techniques, one took an ontological approach, and one utilized natural language processing (NLP).",2019-06-01,2,1477,49,707
2571,30500302,Introduction to Machine Learning for Ophthalmologists,"New diagnostic and imaging techniques generate such an incredible amount of data that it is often a challenge to extract all information that could be possibly useful in clinical practice. Machine Learning techniques emerged as an objective tool to assist practitioners to diagnose certain conditions and take clinical decisions. In particular, Machine Learning techniques have repeatedly shown their usefulness for ophthalmologists. The possible applications of this technology go much further than been used as diagnostic tool, as it may also be used to grade the severity of a pathology, perform early disease detection, or predict the evolution of a condition. This work reviews not only the latest achievements of Machine Learning in ocular sciences, but also aims to be a comprehensive and concise overview of all steps of the process, with clear and easy explanation for each technical term, focusing on the basic knowledge required to understand Machine Learning.",2019-06-01,3,971,53,707
972,31895021,A review of knowledge discovery process in control and mitigation of avian influenza,"In the last several decades, avian influenza virus has caused numerous outbreaks around the world. These outbreaks pose a significant threat to the poultry industry and also to public health. When an avian influenza (AI) outbreak occurs, it is critical to make informed decisions about the potential risks, impact, and control measures. To this end, many modeling approaches have been proposed to acquire knowledge from different sources of data and perspectives to enhance decision making. Although some of these approaches have shown to be effective, they do not follow the process of knowledge discovery in databases (KDD). KDD is an iterative process, consisting of five steps, that aims at extracting unknown and useful information from the data. The present review attempts to survey AI modeling methods in the context of KDD process. We first divide the modeling techniques used in AI into two main categories: data-intensive modeling and small-data modeling. We then investigate the existing gaps in the literature and suggest several potential directions and techniques for future studies. Overall, this review provides insights into the control of AI in terms of the risk of introduction and spread of the virus.",2019-06-01,0,1222,84,707
2542,30547445,Using Drug Expression Profiles and Machine Learning Approach for Drug Repurposing,"The cost of new drug development has been increasing, and repurposing known medications for new indications serves as an important way to hasten drug discovery. One promising approach to drug repositioning is to take advantage of machine learning (ML) algorithms to learn patterns in biological data related to drugs and then link them up to the potential of treating specific diseases. Here we give an overview of the general principles and different types of ML algorithms, as well as common approaches to evaluating predictive performances, with reference to the application of ML algorithms to predict repurposing opportunities using drug expression data as features. We will highlight common issues and caveats when applying such models to repositioning. We also introduce resources of drug expression data and highlight recent studies employing such an approach to repositioning.",2019-06-01,5,885,81,707
2543,30547396,Overview and Evaluation of Recent Methods for Statistical Inference of Gene Regulatory Networks from Time Series Data,"A challenging problem in systems biology is the reconstruction of gene regulatory networks from postgenomic data. A variety of reverse engineering methods from machine learning and computational statistics have been proposed in the literature. However, deciding on the best method to adopt for a particular application or data set might be a confusing task. The present chapter provides a broad overview of state-of-the-art methods with an emphasis on conceptual understanding rather than a deluge of mathematical details, and the pros and cons of the various approaches are discussed. Guidance on practical applications with pointers to publicly available software implementations are included. The chapter concludes with a comprehensive comparative benchmark study on simulated data and a real-work application taken from the current plant systems biology.",2019-06-01,1,858,117,707
2205,31207930,Optimizing Neuro-Oncology Imaging: A Review of Deep Learning Approaches for Glioma Imaging,"Radiographic assessment with magnetic resonance imaging (MRI) is widely used to characterize gliomas, which represent 80% of all primary malignant brain tumors. Unfortunately, glioma biology is marked by heterogeneous angiogenesis, cellular proliferation, cellular invasion, and apoptosis. This translates into varying degrees of enhancement, edema, and necrosis, making reliable imaging assessment challenging. Deep learning, a subset of machine learning artificial intelligence, has gained traction as a method, which has seen effective employment in solving image-based problems, including those in medical imaging. This review seeks to summarize current deep learning applications used in the field of glioma detection and outcome prediction and will focus on (1) pre- and post-operative tumor segmentation, (2) genetic characterization of tissue, and (3) prognostication. We demonstrate that deep learning methods of segmenting, characterizing, grading, and predicting survival in gliomas are promising opportunities that may enhance both research and clinical activities.",2019-06-01,11,1077,90,707
1086,31433750,Blood Brain Barrier Permeability Prediction Using Machine Learning Techniques: An Update,"Blood Brain Barrier (BBB) is the collection of vessels of blood with special properties of permeability that allow a limited range of drug and compounds to pass through it. The BBB plays a vital role in maintaining balance between intracellular and extracellular environment for brain. Brain Capillary Endothelial Cells (BECs) act as vehicle for transport and the transport mechanisms across BBB involve active and passive diffusion of compounds. Efficient prediction models of BBB permeability can be vital at the preliminary stages of drug development. There have been persistent efforts in identifying the prediction of BBB permeability of compounds employing multiple machine learning methods in an attempt to minimize the attrition rate of drug candidates taking up preclinical and clinical trials. However, there is an urgent need to review the progress of such machine learning derived prediction models in the prediction of BBB permeability. In the current article, we have analyzed the recently developed prediction model for BBB permeability using machine learning.",2019-06-01,2,1075,88,707
920,31241024,Fuzzy Classification Methods Based Diagnosis of Parkinson's disease from Speech Test Cases,"Background:                    Together with the Alzheimer's disease, Parkinson's disease is considered as one of the two serious known neurodegenerative diseases. Physicians find it hard to predict whether a given patient has already developed or is expected to develop the Parkinson's disease in the future. To overcome this difficulty, it is possible to develop a computing model, which analyzes the data related to a given patient and predicts with acceptable accuracy when he/she is anticipated to develop the Parkinson's disease.              Objectives:                    This paper contributes an attractive prediction framework based on some machine learning approaches for distinguishing people with Parkinsonism from healthy individuals.              Methods:                    Several fuzzy classifiers such as Inductive Fuzzy Classifier, Fuzzy Rough Classifier and two types of neuro-fuzzy classifiers have been employed.              Results:                    The fuzzy classifiers utilized in this study have been tested using the ""Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set"" of 40 subjects available on the UCI repository.              Conclusion:                    The results achieved show that FURIA, MLP- Bagging - SGD, genfis2 and scg1 performed the best among the fuzzy rough, WEKA, adaptive neuro-fuzzy and neuro-fuzzy classifiers, respectively. The worst performance belongs to nearest neighborhood, IBK, genfis3 and scg3 among the formerly mentioned classifiers. The results reported in this paper are better in comparison to the results reported in Sakar et al., where the same dataset was used, with utilization of different classifiers. This demonstrates the applicability and effectiveness of the fuzzy classifiers used in this study as compared to the non-fuzzy classifiers used by Sakar et al.",2019-06-01,0,1860,90,707
2466,30706823,Computational Approaches as Rational Decision Support Systems for Discovering Next-Generation Antitubercular Agents: Mini-Review,"Tuberculosis, malaria, dengue, chikungunya, leishmaniasis etc. are a large group of neglected tropical diseases that prevail in tropical and subtropical countries, affecting one billion people every year. Minimal funding and grants for research on these scientific problems challenge many researchers to find a different way to reduce the extensive time and cost involved in the drug discovery cycle of these problems. Computer-aided drug design techniques have already been proved successful in the discovery of new molecules rationally by reducing the time and cost involved in the development of drugs. In the current minireview, we are highlighting on the molecular modeling studies published during 2010-2018 for target specific antitubercular agents. This review includes the studies of Structure-Based (SB) and Ligand-Based (LB) modeling and those involving Machine Learning (ML) techniques against different antitubercular targets such as dihydrofolate reductase (DHFR), enoyl Acyl Carrier Protein (ACP) reductase (InhA), catalase-peroxidase (KatG), enzyme antigen 85C, protein tyrosine phosphatases (PtpA and PtpB), dUTPase, thioredoxin reductase (MtTrxR), etc. The information presented in this review will help the researchers to get acquainted with the recent progress in the modeling studies of antitubercular agents.",2019-06-01,0,1330,128,707
502,31049622,Accelerating the implementation of biocatalysis in industry,"Despite enormous progress in protein engineering, complemented by bioprocess engineering, the revolution awaiting the application of biocatalysis in the fine chemical industry has still not been fully realized. In order to achieve that, further research is required on several topics, including (1) rapid methods for protein engineering using machine learning, (2) mathematical modelling of multi-enzyme cascade processes, (3) process standardization, (4) continuous process technology, (5) methods to identify improvements required to achieve industrial implementation, (6) downstream processing, (7) enzyme stability modelling and prediction, as well as (8) new reactor technology. In this brief mini-review, the status of each of these topics will be briefly discussed.",2019-06-01,9,772,59,707
2256,31115885,A Review of Microarray Datasets: Where to Find Them and Specific Characteristics,"The advent of DNA microarray datasets has stimulated a new line of research both in bioinformatics and in machine learning. This type of data is used to collect information from tissue and cell samples regarding gene expression differences that could be useful for disease diagnosis or for distinguishing specific types of tumor. Microarray data classification is a difficult challenge for machine learning researchers due to its high number of features and the small sample sizes. This chapter is devoted to reviewing the microarray databases most frequently used in the literature. We also make the interested reader aware of the problematic of data characteristics in this domain, such as the imbalance of the data, their complexity, and the so-called dataset shift.",2019-06-01,0,769,80,707
2327,29521204,In Silico Chemogenomics Drug Repositioning Strategies for Neglected Tropical Diseases,"Only ~1% of all drug candidates against Neglected Tropical Diseases (NTDs) have reached clinical trials in the last decades, underscoring the need for new, safe and effective treatments. In such context, drug repositioning, which allows finding novel indications for approved drugs whose pharmacokinetic and safety profiles are already known, emerging as a promising strategy for tackling NTDs. Chemogenomics is a direct descendent of the typical drug discovery process that involves the systematic screening of chemical compounds against drug targets in high-throughput screening (HTS) efforts, for the identification of lead compounds. However, different to the one-drug-one-target paradigm, chemogenomics attempts to identify all potential ligands for all possible targets and diseases. In this review, we summarize current methodological development efforts in drug repositioning that use state-of-the-art computational ligand- and structure-based chemogenomics approaches. Furthermore, we highlighted the recent progress in computational drug repositioning for some NTDs, based on curation and modeling of genomic, biological, and chemical data. Additionally, we also present in-house and other successful examples and suggest possible solutions to existing pitfalls.",2019-06-01,5,1272,85,707
1747,31582098,Predicting hypotension in perioperative and intensive care medicine,"Blood pressure is the main determinant of organ perfusion. Hypotension is common in patients having surgery and in critically ill patients. The severity and duration of hypotension are associated with hypoperfusion and organ dysfunction. Hypotension is mostly treated reactively after low blood pressure values have already occurred. However, prediction of hypotension before it becomes clinically apparent would allow the clinician to treat hypotension pre-emptively, thereby reducing the severity and duration of hypotension. Hypotension can now be predicted minutes before it actually occurs from the blood pressure waveform using machine-learning algorithms that can be trained to detect subtle changes in cardiovascular dynamics preceding clinically apparent hypotension. However, analyzing the complex cardiovascular system is a challenge because cardiovascular physiology is highly interdependent, works within complicated networks, and is influenced by compensatory mechanisms. Improved hemodynamic data collection and integration will be a key to improve current models and develop new hypotension prediction models.",2019-06-01,2,1125,67,707
2243,31140082,Applications of deep learning for the analysis of medical data,"Over the past decade, deep learning has demonstrated superior performances in solving many problems in various fields of medicine compared with other machine learning methods. To understand how deep learning has surpassed traditional machine learning techniques, in this review, we briefly explore the basic learning algorithms underlying deep learning. In addition, the procedures for building deep learning-based classifiers for seizure electroencephalograms and gastric tissue slides are described as examples to demonstrate the simplicity and effectiveness of deep learning applications. Finally, we review the clinical applications of deep learning in radiology, pathology, and drug discovery, where deep learning has been actively adopted. Considering the great advantages of deep learning techniques, deep learning will be increasingly and widely utilized in a wide variety of different areas in medicine in the coming decades.",2019-06-01,8,934,62,707
1746,31582102,Postoperative ward monitoring - Why and what now?,"The postoperative ward is considered an ideal nursing environment for stable patients transitioning out of the hospital. However, approximately half of all in-hospital cardiorespiratory arrests occur here and are associated with poor outcomes. Current monitoring practices on the hospital ward mandate intermittent vital sign checks. Subtle changes in vital signs often occur at least 8-12 h before an acute event, and continuous monitoring of vital signs would allow for effective therapeutic interventions and potentially avoid an imminent cardiorespiratory arrest event. It seems tempting to apply continuous monitoring to every patient on the ward, but inherent challenges such as artifacts and alarm fatigue need to be considered. This review looks to the future where a continuous, smarter, and portable platform for monitoring of vital signs on the hospital ward will be accompanied with a central monitoring platform and machine learning-based pattern detection solutions to improve safety for hospitalized patients.",2019-06-01,1,1024,49,707
165,30068270,Recent Advances in Computational Methods for Identifying Anticancer Peptides,"Anticancer peptide (ACP) is a kind of small peptides that can kill cancer cells without damaging normal cells. In recent years, ACP has been pre-clinically used for cancer treatment. Therefore, accurate identification of ACPs will promote their clinical applications. In contrast to labor-intensive experimental techniques, a series of computational methods have been proposed for identifying ACPs. In this review, we briefly summarized the current progress in computational identification of ACPs. The challenges and future perspectives in developing reliable methods for identification of ACPs were also discussed. We anticipate that this review could provide novel insights into future researches on anticancer peptides.",2019-06-01,2,723,76,707
169,30060039,Clinical applications of machine learning in cardiovascular disease and its relevance to cardiac imaging,"Artificial intelligence (AI) has transformed key aspects of human life. Machine learning (ML), which is a subset of AI wherein machines autonomously acquire information by extracting patterns from large databases, has been increasingly used within the medical community, and specifically within the domain of cardiovascular diseases. In this review, we present a brief overview of ML methodologies that are used for the construction of inferential and predictive data-driven models. We highlight several domains of ML application such as echocardiography, electrocardiography, and recently developed non-invasive imaging modalities such as coronary artery calcium scoring and coronary computed tomography angiography. We conclude by reviewing the limitations associated with contemporary application of ML algorithms within the cardiovascular disease field.",2019-06-01,42,857,104,707
170,30058484,"Aqueous Drug Solubility: What Do We Measure, Calculate and QSPR Predict?","Detailed critical analysis of publications devoted to QSPR of aqueous solubility is presented in the review with discussion of four types of aqueous solubility (three different thermodynamic solubilities with unknown solute structure, intrinsic solubility, solubility in physiological media at pH=7.4 and kinetic solubility), variety of molecular descriptors (from topological to quantum chemical), traditional statistical and machine learning methods as well as original QSPR models.",2019-06-01,1,484,72,707
1310,31989876,"MRI Imaging, Comparison of MRI with other Modalities, Noise in MRI Images and Machine Learning Techniques for Noise Removal: A Review","Background:                    Medical imaging is to assume greater and greater significance in an efficient and precise diagnosis process.              Discussion:                    It is a set of various methodologies which are used to capture internal or external images of the human body and organs for clinical and diagnosis needs to examine human form for various kind of ailments. Computationally intelligent machine learning techniques and their application in medical imaging can play a significant role in expediting the diagnosis process and making it more precise.              Conclusion:                    This review presents an up-to-date coverage about research topics which include recent literature in the areas of MRI imaging, comparison with other modalities, noise in MRI and machine learning techniques to remove the noise.",2019-06-01,0,848,133,707
2282,31069725,"Complex Mixtures, Complex Analyses: an Emphasis on Interpretable Results","Purpose of review:                    The purpose of this review is to outline the main questions in environmental mixtures research and provide a non-technical explanation of novel or advanced methods to answer these questions.              Recent findings:                    Machine learning techniques are now being incorporated into environmental mixture research to overcome issues with traditional methods. Though some methods perform well on specific tasks, no method consistently outperforms all others in complex mixture analyses, largely because different methods were developed to answer different research questions. We discuss four main questions in environmental mixtures research: (1) Are there specific exposure patterns in the study population? (2) Which are the toxic agents in the mixture? (3) Are mixture members acting synergistically? And, (4) what is the overall effect of the mixture? We emphasize the importance of robust methods and interpretable results over predictive accuracy. We encourage collaboration with computer scientists, data scientists, and biostatisticians in future mixture method development.",2019-06-01,10,1136,72,707
2200,31217702,"Critical Care, Critical Data","As big data, machine learning, and artificial intelligence continue to penetrate into and transform many facets of our lives, we are witnessing the emergence of these powerful technologies within health care. The use and growth of these technologies has been contingent on the availability of reliable and usable data, a particularly robust resource in critical care medicine where continuous monitoring forms a key component of the infrastructure of care. The response to this opportunity has included the development of open databases for research and other purposes; the development of a collaborative form of clinical data science intended to fully leverage these data resources, and the creation of data-driven applications for purposes such as clinical decision support. Most recently, data levels have reached the thresholds required for the development of robust artificial intelligence features for clinical purposes. The systematic capture and analysis of clinical data in both individuals and populations allows us to begin to move toward precision medicine in the intensive care unit (ICU). In this perspective review, we examine the fundamental role of data as we present the current progress that has been made toward an artificial intelligence (AI)-supported, data-driven precision critical care medicine.",2019-06-01,6,1320,28,707
460,30307875,Advances in Acoustic Signal Processing Techniques for Enhanced Bowel Sound Analysis,"With the invention of the electronic stethoscope and other similar recording and data logging devices, acoustic signal processing concepts and methods can now be applied to bowel sounds. In this paper, the literature pertaining to acoustic signal processing for bowel sound analysis is reviewed and discussed. The paper outlines some of the fundamental approaches and machine learning principles that may be used in bowel sound analysis. The advances in signal processing techniques that have allowed useful information to be obtained from bowel sounds from a historical perspective are provided. The document specifically address the progress in bowel sound analysis, such as improved noise reduction, segmentation, signal enhancement, feature extraction, localization of sounds, and machine learning techniques. We have found that advanced acoustic signal processing incorporating novel machine learning methods and artificial intelligence can lead to better interpretation of acoustic information emanating from the bowel.",2019-06-01,3,1025,83,707
516,31036284,Machine Learning and Other Emerging Decision Support Tools,"Emerging applications of machine learning and artificial intelligence offer the opportunity to discover new clinical knowledge through secondary exploration of existing patient medical records. This new knowledge may in turn offer a foundation to build new types of clinical decision support (CDS) that provide patient-specific insights and guidance across a wide range of clinical questions and settings. This article will provide an overview of these emerging approaches to CDS, discussing both existing technologies as well as challenges that health systems and informaticists will need to address to allow these emerging approaches to reach their full potential.",2019-06-01,2,666,58,707
2254,31118321,Evaluation of Chatbot Prototypes for Taking the Virtual Patient's History,"In medical education Virtual Patients (VP) are often applied to train students in different scenarios such as recording the patient's medical history or deciding a treatment option. Usually, such interactions are predefined by software logic and databases following strict rules. At this point, Natural Language Processing/Machine Learning (NLP/ML) algorithms could help to increase the overall flexibility, since most of the rules can derive directly from training data. This would allow a more sophisticated and individual conversation between student and VP. One type of technology that is heavily based on such algorithmic advances are chatbots or conversational agents. Therefore, a literature review is carried out to give insight into existing educational ideas with such agents. Besides, different prototypes are implemented for the scenario of taking the patient's medical history, responding with the classified intent of a generic anamnestic question. Although the small number of questions (n=109) leads to a high SD during evaluation, all scores (recall, precision, f1) reach already a level above 80% (micro-averaged). This shows a first promising step to use these prototypes for taking the medical history of a VP.",2019-06-01,0,1230,73,707
2194,31226833,Is It Possible to Predict the Odor of a Molecule on the Basis of its Structure?,"The olfactory sense is the dominant sensory perception for many animals. When Richard Axel and Linda B. Buck received the Nobel Prize in 2004 for discovering the G protein-coupled receptors' role in olfactory cells, they highlighted the importance of olfaction to the scientific community. Several theories have tried to explain how cells are able to distinguish such a wide variety of odorant molecules in a complex context in which enantiomers can result in completely different perceptions and structurally different molecules. Moreover, sex, age, cultural origin, and individual differences contribute to odor perception variations that complicate the picture. In this article, recent advances in olfaction theory are presented, and future trends in human olfaction such as structure-based odor prediction and artificial sniffing are discussed at the frontiers of chemistry, physiology, neurobiology, and machine learning.",2019-06-01,2,926,79,707
2199,31219658,Current status of artificial intelligence applications in urology and their potential to influence clinical practice,"Objective:                    To investigate the applications of artificial intelligence (AI) in diagnosis, treatment and outcome predictionin urologic diseases and evaluate its advantages over traditional models and methods.              Materials and methods:                    A literature search was performed after PROSPERO registration (CRD42018103701) and in compliance with Preferred Reported Items for Systematic Reviews and Meta-Analyses (PRISMA) methods. Articles between 1994 and 2018 using the search terms ""urology"", ""artificial intelligence"", ""machine learning"" were included and categorized by the application of AI in urology. Review articles, editorial comments, articles with no full-text access, and nonurologic studies were excluded.              Results:                    Initial search yielded 231 articles, but after excluding duplicates and following full-text review and examination of article references, only 111 articles were included in the final analysis. AI applications in urology include: utilizing radiomic imaging or ultrasonic echo data to improve or automate cancer detection or outcome prediction, utilizing digitized tissue specimen images to automate detection of cancer on pathology slides, and combining patient clinical data, biomarkers, or gene expression to assist disease diagnosis or outcome prediction. Some studies employed AI to plan brachytherapy and radiation treatments while others used video based or robotic automated performance metrics to objectively evaluate surgical skill. Compared to conventional statistical analysis, 71.8% of studies concluded that AI is superior in diagnosis and outcome prediction.              Conclusion:                    AI has been widely adopted in urology. Compared to conventional statistics AI approaches are more accurate in prediction and more explorative for analyzing large data cohorts. With an increasing library of patient data accessible to clinicians, AI may help facilitate evidence-based and individualized patient care.",2019-06-01,8,2028,116,707
2252,31119599,A Brief History of Protein Sorting Prediction,"Ever since the signal hypothesis was proposed in 1971, the exact nature of signal peptides has been a focus point of research. The prediction of signal peptides and protein subcellular location from amino acid sequences has been an important problem in bioinformatics since the dawn of this research field, involving many statistical and machine learning technologies. In this review, we provide a historical account of how position-weight matrices, artificial neural networks, hidden Markov models, support vector machines and, lately, deep learning techniques have been used in the attempts to predict where proteins go. Because the secretory pathway was the first one to be studied both experimentally and through bioinformatics, our main focus is on the historical development of prediction methods for signal peptides that target proteins for secretion; prediction methods to identify targeting signals for other cellular compartments are treated in less detail.",2019-06-01,18,967,45,707
2948,29804538,The Application of Machine Learning Techniques in Clinical Drug Therapy,"Introduction:                    The development of a novel drug is an extremely complicated process that includes the target identification, design and manufacture, and proper therapy of the novel drug, as well as drug dose selection, drug efficacy evaluation, and adverse drug reaction control. Due to the limited resources, high costs, long duration, and low hit-to-lead ratio in the development of pharmacogenetics and computer technology, machine learning techniques have assisted novel drug development and have gradually received more attention by researchers.              Methods:                    According to current research, machine learning techniques are widely applied in the process of the discovery of new drugs and novel drug targets, the decision surrounding proper therapy and drug dose, and the prediction of drug efficacy and adverse drug reactions.              Results and conclusion:                    In this article, we discussed the history, workflow, and advantages and disadvantages of machine learning techniques in the processes mentioned above. Although the advantages of machine learning techniques are fairly obvious, the application of machine learning techniques is currently limited. With further research, the application of machine techniques in drug development could be much more widespread and could potentially be one of the major methods used in drug development.",2019-06-01,3,1412,71,707
450,30317992,The Development of Machine Learning Methods in Cell-Penetrating Peptides Identification: A Brief Review,"Background:                    Cell-penetrating Peptides (CPPs) are important short peptides that facilitate cellular intake or uptake of various molecules. CPPs can transport drug molecules through the plasma membrane and send these molecules to different cellular organelles. Thus, CPP identification and related mechanisms have been extensively explored. In order to reveal the penetration mechanisms of a large number of CPPs, it is necessary to develop convenient and fast methods for CPPs identification.              Methods:                    Biochemical experiments can provide precise details for accurately identifying CPP, but these methods are expensive and laborious. To overcome these disadvantages, several computational methods have been developed to identify CPPs. We have performed review on the development of machine learning methods in CPP identification. This review provides an insight into CPP identification.              Results:                    We summarized the machine learning-based CPP identification methods and compared the construction strategies of 11 different computational methods. Furthermore, we pointed out the limitations and difficulties in predicting CPPs.              Conclusion:                    In this review, the last studies on CPP identification using machine learning method were reported. We also discussed the future development direction of CPP recognition with computational methods.",2019-06-01,1,1447,103,707
550,30976107,Applications of machine learning in drug discovery and development,"Drug discovery and development pipelines are long, complex and depend on numerous factors. Machine learning (ML) approaches provide a set of tools that can improve discovery and decision making for well-specified questions with abundant, high-quality data. Opportunities to apply ML occur in all stages of drug discovery. Examples include target validation, identification of prognostic biomarkers and analysis of digital pathology data in clinical trials. Applications have ranged in context and methodology, with some approaches yielding accurate predictions and insights. The challenges of applying ML lie primarily with the lack of interpretability and repeatability of ML-generated results, which may limit their application. In all areas, systematic and comprehensive high-dimensional data still need to be generated. With ongoing efforts to tackle these issues, as well as increasing awareness of the factors needed to validate ML approaches, the application of ML can promote data-driven decision making and has the potential to speed up the process and reduce failure rates in drug discovery and development.",2019-06-01,92,1117,66,707
595,30878282,Medical ethics considerations on artificial intelligence,"Artificial intelligence (AI) is currently one of the mostly controversial matters of the world. This article discusses AI in terms of the medical ethics issues involved, both existing and potential. Once artificial intelligence is fully developed within electronic systems, it will afford many useful applications in many sectors ranging from banking, agriculture, medical procedures to military operations, especially by decreasing the involvement of humans in critically dangerous activities. Robots as well as computers themselves are embodiments of values inasmuch as they entail actions and choices, but their practical applications are modelled or programmed by the engineers building the systems. AI will need algorithmic procedures to ensure safety in the implementation of such systems. The AI algorithms written could naturally contain errors that may result in unforeseen consequences and unfair outcomes along economic and racial class lines. It is crucial that measures be taken to monitor technological developments ensuring preventative and precautionary safeguards are in place to safeguard the rights of those involved against direct or indirect coercion. While it is the responsibility of AI researchers to ensure that the future impact is more positive than negative, ethicists and philosophers need to be deeply involved in the development of such technologies from the beginning.",2019-06-01,3,1400,56,707
2408,30844756,"Application of mobile health, telemedicine and artificial intelligence to echocardiography","The intersection of global broadband technology and miniaturized high-capability computing devices has led to a revolution in the delivery of healthcare and the birth of telemedicine and mobile health (mHealth). Rapid advances in handheld imaging devices with other mHealth devices such as smartphone apps and wearable devices are making great strides in the field of cardiovascular imaging like never before. Although these technologies offer a bright promise in cardiovascular imaging, it is far from straightforward. The massive data influx from telemedicine and mHealth including cardiovascular imaging supersedes the existing capabilities of current healthcare system and statistical software. Artificial intelligence with machine learning is the one and only way to navigate through this complex maze of the data influx through various approaches. Deep learning techniques are further expanding their role by image recognition and automated measurements. Artificial intelligence provides limitless opportunity to rigorously analyze data. As we move forward, the futures of mHealth, telemedicine and artificial intelligence are increasingly becoming intertwined to give rise to precision medicine.",2019-06-01,12,1202,90,707
441,30338743,Virtual Screening Meets Deep Learning,"Background:                    Automated compound testing is currently the de facto standard method for drug screening, but it has not brought the great increase in the number of new drugs that was expected. Computer- aided compounds search, known as Virtual Screening, has shown the benefits to this field as a complement or even alternative to the robotic drug discovery. There are different methods and approaches to address this problem and most of them are often included in one of the main screening strategies. Machine learning, however, has established itself as a virtual screening methodology in its own right and it may grow in popularity with the new trends on artificial intelligence.              Objective:                    This paper will attempt to provide a comprehensive and structured review that collects the most important proposals made so far in this area of research. Particular attention is given to some recent developments carried out in the machine learning field: the deep learning approach, which is pointed out as a future key player in the virtual screening landscape.",2019-06-01,3,1103,37,707
587,30893115,Prediction of postoperative pulmonary complications,"Purpose of review:                    Prediction of postoperative pulmonary complications (PPCs) enables individually applied preventive measures and maybe even early treatment if a PPC eventually starts to develop. The purpose of this review is to describe crucial steps in the development and validation of prediction models, examine these steps in the current literature and describe what the future holds for PPC prediction.              Recent findings:                    A systematic search of the medical literature identified 21 articles reporting on prediction models for PPCs. The studies were heterogeneous with regard to design, derivation cohort and whether or not a validation cohort was used. Furthermore, as definitions for PPCs varied substantially, PPC rates were quite different. One-third of the studies had a sufficient sample size for building a prediction model. In most articles, an internal validation step was reported, suggesting a good fit. In the four articles that reported an externally validation step, in three the prognostic model performed less well in external validation. The ARISCAT risk score was the only score that kept sufficient predictive power in external validation, albeit that the sample sizes of the cohorts used may have been too small. Analysis by machine learning could help building new prediction models, as unbiased cluster analyses could uncover clusters of patients with specific underlying pathophysiological mechanisms. Adding biomarkers to the model could optimize identification of biological phenotypes of risk groups.              Summary:                    Many predictive models for PPCs have been reported on. Development of more robust PPC prediction models could be supported by machine learning.",2019-06-01,5,1766,51,707
1029,31521231,Intrinsically disordered proteins in various hypotheses on the pathogenesis of Alzheimer's and Parkinson's diseases,"Amyloid- (A) and -synuclein (S) are two intrinsically disordered proteins (IDPs) at the centers of the pathogenesis of Alzheimer's and Parkinson's diseases, respectively. Different hypotheses have been proposed for explanation of the molecular mechanisms of the pathogenesis of these two diseases, with these two IDPs being involved in many of these hypotheses. Currently, we do not know, which of these hypothesis is more accurate. Experiments face challenges due to the rapid conformational changes, fast aggregation processes, solvent and paramagnetic effects in studying these two IDPs in detail. Furthermore, pathological modifications impact their structures and energetics. Theoretical studies using computational chemistry and computational biology have been utilized to understand the structures and energetics of A and S. In this chapter, we introduce A and S in light of various hypotheses, and discuss different experimental and theoretical techniques that are used to study these two proteins along with their weaknesses and strengths. We suggest that a promising solution for studying A and S at the center of varying hypotheses could be provided by developing new techniques that link quantum mechanics, statistical mechanics, thermodynamics, bioinformatics to machine learning. Such new developments could also lead to development in experimental techniques.",2019-06-01,3,1384,115,707
418,30378494,Recent Advances in Machine Learning Methods for Predicting Heat Shock Proteins,"Background:                    As molecular chaperones, Heat Shock Proteins (HSPs) not only play key roles in protein folding and maintaining protein stabilities, but are also linked with multiple kinds of diseases. Therefore, HSPs have been regarded as the focus of drug design. Since HSPs from different families play distinct functions, accurately classifying the families of HSPs is the key step to clearly understand their biological functions. In contrast to laborintensive and cost-ineffective experimental methods, computational classification of HSP families has emerged to be an alternative approach.              Methods:                    We reviewed the paper that described the existing datasets of HSPs and the representative computational approaches developed for the identification and classification of HSPs.              Results:                    The two benchmark datasets of HSPs, namely HSPIR and sHSPdb were introduced, which provided invaluable resources for computationally identifying HSPs. The gold standard dataset and sequence encoding schemes for building computational methods of classifying HSPs were also introduced. The three representative web-servers for identifying HSPs and their families were described.              Conclusion:                    The existing machine learning methods for identifying the different families of HSPs indeed yielded quite encouraging results and did play a role in promoting the research on HSPs. However, the number of HSPs with known structures is very limited. Therefore, determining the structure of the HSPs is also urgent, which will be helpful in revealing their functions.",2019-06-01,28,1654,78,707
578,30912017,In Silico Drug-Target Profiling,"Pharmacological science is trying to establish the link between chemicals, targets, and disease-related phenotypes. A plethora of chemical proteomics and structural data have been generated, thanks to the target-based approach that has dominated drug discovery at the turn of the century. There is an invaluable source of information for in silico target profiling. Prediction is based on the principle of chemical similarity (similar drugs bind similar targets) or on first principles from the biophysics of molecular interactions. In the first case, compound comparison is made through ligand-based chemical similarity search or through classifier-based machine learning approach. The 3D techniques are based on 3D structural descriptors or energy-based scoring scheme to infer a binding affinity of a compound with its putative target. More recently, a new approach based on compound set metric has been proposed in which a query compound is compared with a whole of compounds associated with a target or a family of targets. This chapter reviews the different techniques of in silico target profiling and their main applications such as inference of unwanted targets, drug repurposing, or compound prioritization after phenotypic-based screening campaigns.",2019-06-01,2,1260,31,707
420,30378077,Computational Methods for Subtyping of Tumors and Their Applications for Deciphering Tumor Heterogeneity,"With the rapid development of deep sequencing technologies, many programs are generating multi-platform genomic profiles (e.g., somatic mutation, DNA methylation, and gene expression) for a large number of tumors. This activity has provided unique opportunities and challenges to stratify tumors and decipher tumor heterogeneity. In this chapter, we summarize several computational methods to address the challenge of tumor stratification with different types of genomic data. We further introduce their applications in emerging large-scale genomic data to show their effectiveness in deciphering tumor heterogeneity and clinical relevance.",2019-06-01,0,640,104,707
1028,31521235,Computational prediction of functions of intrinsically disordered regions,"Intrinsically disorder regions (IDRs) are abundant in nature, particularly among Eukaryotes. While they facilitate a wide spectrum of cellular functions including signaling, molecular assembly and recognition, translation, transcription and regulation, only several hundred IDRs are annotated functionally. This annotation gap motivates the development of fast and accurate computational methods that predict IDR functions directly from protein sequences. We introduce and describe a comprehensive collection of 25 methods that provide accurate predictions of IDRs that interact with proteins and nucleic acids, that function as flexible linkers and that moonlight multiple functions. Virtually all of these predictors can be accessed online and many were developed in the last few years. They utilize a wide range of predictive architectures and take advantage of modern machine learning algorithms. Our empirical analysis shows that predictors that are available as webservers enjoy high rates of citations, attesting to their practical value and popularity. The most cited methods include DISOPRED3, ANCHOR, alpha-MoRFpred, MoRFpred, fMoRFpred and MoRFCHiBi. We present two case studies to demonstrate that predictions produced by these computational tools are relatively easy to interpret and that they deliver valuable functional clues. However, the current computational tools cover a relatively narrow range of disorder functions. Further development efforts that would cover a broader range of functions should be pursued. We demonstrate that a sufficient amount of functionally annotated IDRs that are associated with several other disorder functions is already available and can be used to design and validate novel predictors.",2019-06-01,3,1737,73,707
2197,31221831,Trends and challenges in robot manipulation,"Dexterous manipulation is one of the primary goals in robotics. Robots with this capability could sort and package objects, chop vegetables, and fold clothes. As robots come to work side by side with humans, they must also become human-aware. Over the past decade, research has made strides toward these goals. Progress has come from advances in visual and haptic perception and in mechanics in the form of soft actuators that offer a natural compliance. Most notably, immense progress in machine learning has been leveraged to encapsulate models of uncertainty and to support improvements in adaptive and robust control. Open questions remain in terms of how to enable robots to deal with the most unpredictable agent of all, the human.",2019-06-01,9,737,43,707
2271,31083923,Use of Magnetic Resonance Imaging and Artificial Intelligence in Studies of Diagnosis of Parkinson's Disease,"Parkinson's disease (PD) is a common neurodegenerative disorder. It has a delitescent onset and a slow progress. The clinical manifestations of PD in patients are highly heterogeneous. Thus, PD diagnosis process is complex and mainly depends on the professional knowledge and experience of the physician. Magnetic resonance imaging (MRI) could detect the small changes in the brain of PD patients, and quantitative analysis of brain MRI may improve the clinical diagnosis efficiency. However, due to the complexity of clinical courses in PD and the high dimensionality in multimodal MRI data, traditional mathematical analysis could not effectively extract the huge information in them. Up to now, the accuracy of PD diagnosis in large sample size is still unsatisfying. As artificial intelligence (AI) is becoming more mature, varieties of statistical models and machine learning (ML) algorithms have been used for quantitative imaging data analysis to explore a diagnostic result. This review aims to state an overview of existing research recently that used statistical ML/AI methods to perform quantitative analysis of MR image data for the study of PD diagnosis. First we review the recent research in three subareas: diagnosis, differential diagnosis, and subtyping of PD. Then we described the overall workflow from MR image to classification result. Finally, we summarized a critical assessment of the current research and provide some recommendations for likely future research developments and trends.",2019-06-01,2,1511,108,707
2392,30877639,An Overview of Scoring Functions Used for Protein-Ligand Interactions in Molecular Docking,"Currently, molecular docking is becoming a key tool in drug discovery and molecular modeling applications. The reliability of molecular docking depends on the accuracy of the adopted scoring function, which can guide and determine the ligand poses when thousands of possible poses of ligand are generated. The scoring function can be used to determine the binding mode and site of a ligand, predict binding affinity and identify the potential drug leads for a given protein target. Despite intensive research over the years, accurate and rapid prediction of protein-ligand interactions is still a challenge in molecular docking. For this reason, this study reviews four basic types of scoring functions, physics-based, empirical, knowledge-based, and machine learning-based scoring functions, based on an up-to-date classification scheme. We not only discuss the foundations of the four types scoring functions, suitable application areas and shortcomings, but also discuss challenges and potential future study directions.",2019-06-01,20,1023,90,707
1118,31388566,The medical AI insurgency: what physicians must know about data to practice with intelligent machines,"Machine learning (ML) and its parent technology trend, artificial intelligence (AI), are deriving novel insights from ever larger and more complex datasets. Efficient and accurate AI analytics require fastidious data science-the careful curating of knowledge representations in databases, decomposition of data matrices to reduce dimensionality, and preprocessing of datasets to mitigate the confounding effects of messy (i.e., missing, redundant, and outlier) data. Messier, bigger and more dynamic medical datasets create the potential for ML computing systems querying databases to draw erroneous data inferences, portending real-world human health consequences. High-dimensional medical datasets can be static or dynamic. For example, principal component analysis (PCA) used within R computing packages can speed & scale disease association analytics for deriving polygenic risk scores from static gene-expression microarrays. Robust PCA of k-dimensional subspace data accelerates image acquisition and reconstruction of dynamic 4-D magnetic resonance imaging studies, enhancing tracking of organ physiology, tissue relaxation parameters, and contrast agent effects. Unlike other data-dense business and scientific sectors, medical AI users must be aware that input data quality limitations can have health implications, potentially reducing analytic model accuracy for predicting clinical disease risks and patient outcomes. As AI technologies find more health applications, physicians should contribute their health domain expertize to rules-/ML-based computer system development, inform input data provenance and recognize the importance of data preprocessing quality assurance before interpreting the clinical implications of intelligent machine outputs to patients.",2019-06-01,1,1774,101,707
2486,30664391,Post-LASIK Ectasia: Twenty Years of a Conundrum,"Corneal ectasia has emerged as a serious complication of laser vision correction (LVC) procedures since the first report by Seiler in 1998. Thereby, its prevention has become a major concern for refractive surgeons. Ectasia occurs due to biomechanical decompensation of the stroma, which may be related to a severe impact on corneal structure (i.e., attempted treatment for high myopia) or the altered biomechanical properties preoperatively. The current understanding is that a combination from those factors determines stability or ectasia progression after LVC. Abnormal corneal topography has been the most important surrogate for lower biomechanical properties, but novel imaging technologies such as tomography and biomechanical assessment have proven to enhance the ability for detecting mild ectatic disease, such as in the eyes with normal topography from patients with clinical ectasia in the fellow eye. Bohac and associates in a retrospective case series analyzed data from 30,167 eyes from 16,732 documented ten eyes (0.033%) of seven patients that developed post-LASIK ectasia. This data supports the concept that the actual incidence of ectasia has decreased from 0.66% reported by Pallikaris in 2001. This has been the result of major development related to the advanced screening strategies. Nevertheless, mysterious cases of ectasia still challenge the field and stimulated research in this field. Ocular allergy and eye rubbing may be a factor that triggered ectasia in such series. Artificial intelligence (AI) and machine-learning algorithms may play a definitive role for further enhancing ectasia risk assessment. Reporting ectasia after LVC is needed.",2019-06-01,8,1675,47,707
151,30091413,Application of Machine Learning Approaches for the Design and Study of Anticancer Drugs,"Background:                    Globally the number of cancer patients and deaths are continuing to increase yearly, and cancer has, therefore, become one of the world's highest causes of morbidity and mortality. In recent years, the study of anticancer drugs has become one of the most popular medical topics.              Objective:                    In this review, in order to study the application of machine learning in predicting anticancer drugs activity, some machine learning approaches such as Linear Discriminant Analysis (LDA), Principal components analysis (PCA), Support Vector Machine (SVM), Random forest (RF), k-Nearest Neighbor (kNN), and Nave Bayes (NB) were selected, and the examples of their applications in anticancer drugs design are listed.              Results:                    Machine learning contributes a lot to anticancer drugs design and helps researchers by saving time and is cost effective. However, it can only be an assisting tool for drug design.              Conclusion:                    This paper introduces the application of machine learning approaches in anticancer drug design. Many examples of success in identification and prediction in the area of anticancer drugs activity prediction are discussed, and the anticancer drugs research is still in active progress. Moreover, the merits of some web servers related to anticancer drugs are mentioned.",2019-06-01,3,1401,87,707
2481,30674262,Recent Progress in Machine Learning-based Prediction of Peptide Activity for Drug Discovery,"Over the past decades, peptide as a therapeutic candidate has received increasing attention in drug discovery, especially for antimicrobial peptides (AMPs), anticancer peptides (ACPs) and antiinflammatory peptides (AIPs). It is considered that the peptides can regulate various complex diseases which are previously untouchable. In recent years, the critical problem of antimicrobial resistance drives the pharmaceutical industry to look for new therapeutic agents. Compared to organic small drugs, peptide- based therapy exhibits high specificity and minimal toxicity. Thus, peptides are widely recruited in the design and discovery of new potent drugs. Currently, large-scale screening of peptide activity with traditional approaches is costly, time-consuming and labor-intensive. Hence, in silico methods, mainly machine learning approaches, for their accuracy and effectiveness, have been introduced to predict the peptide activity. In this review, we document the recent progress in machine learning-based prediction of peptides which will be of great benefit to the discovery of potential active AMPs, ACPs and AIPs.",2019-06-01,3,1122,91,707
2241,31144539,Quantitative assessment of the activity of antituberculosis drugs and regimens,"Introduction: Identification of optimal drug doses and drug combinations is crucial for optimized treatment of tuberculosis. Areas covered: An unprecedented level of research activity involving multiple approaches is seeking to improve tuberculosis treatment. This report is a review of the quantitative methods currently used on clinical data sets to identify drug exposure targets and optimal drug combinations for tuberculosis treatment. A high-level summary of the methods, including the strengths and weaknesses of each method and potential methodological improvements is presented. Methods incorporating data generated from multiple sources such as in vitro and clinical studies, and their potential to provide better estimates of pharmacokinetic/pharmacodynamic (PK/PD) targets, are discussed. PK/PD relationships identified are compared between different studies and data analysis methods. Expert opinion: The relationships between drug exposures and tuberculosis treatment outcomes are complex and require analytical methods capable of handling the multidimensional nature of the relationships. The choice of a method is guided by its complexity, interpretability of results, and type of data available.",2019-06-01,0,1212,78,707
2403,30856578,Evaluating and comparing remote sensing terrestrial GPP models for their response to climate variability and CO 2 trends,"Remote sensing (RS)-based models play an important role in estimating and monitoring terrestrial ecosystem gross primary productivity (GPP). Several RS-based GPP models have been developed using different criteria, yet the sensitivities to environmental factors vary among models; thus, a comparison of model sensitivity is necessary for analyzing and interpreting results and for choosing suitable models. In this study, we globally evaluated and compared the sensitivities of 14 RS-based models (2 process-, 4 vegetation-index-, 5 light-use-efficiency, and 3 machine-learning-based models) and benchmarked them against GPP responses to climatic factors measured at flux sites and to elevated CO2 concentrations measured at free-air CO2 enrichment experiment sites. The results demonstrated that the models with relatively high sensitivity to increasing atmospheric CO2 concentrations showed a higher increasing GPP trend. The fundamental difference in the CO2 effect in the models' algorithm either considers the effect of CO2 through changes in greenness indices (nine models) or introduces the influences on photosynthesis (three models). The overall effects of temperature and radiation, in terms of both magnitude and sign, vary among the models, while the models respond relatively consistently to variations in precipitation. Spatially, larger differences among model sensitivity to climatic factors occur in the tropics; at high latitudes, models have a consistent and obvious positive response to variations in temperature and radiation, and precipitation significantly enhances the GPP in mid-latitudes. Compared with the results calculated by flux-site measurements, the model performance differed substantially among different sites. However, the sensitivities of most models are basically within the confidence interval of the flux-site results. In general, the comparison revealed that models differed substantially in the effect of environmental regulations, particularly CO2 fertilization and water stress, on GPP, and none of the models performed consistently better across the different ecosystems and under the various external conditions.",2019-06-01,1,2159,120,707
886,31293616,A Review and Tutorial of Machine Learning Methods for Microbiome Host Trait Prediction,"With the growing importance of microbiome research, there is increasing evidence that host variation in microbial communities is associated with overall host health. Advancement in genetic sequencing methods for microbiomes has coincided with improvements in machine learning, with important implications for disease risk prediction in humans. One aspect specific to microbiome prediction is the use of taxonomy-informed feature selection. In this review for non-experts, we explore the most commonly used machine learning methods, and evaluate their prediction accuracy as applied to microbiome host trait prediction. Methods are described at an introductory level, and R/Python code for the analyses is provided.",2019-06-01,26,714,86,707
883,31297148,Cardiac tissue engineering: state-of-the-art methods and outlook,"The purpose of this review is to assess the state-of-the-art fabrication methods, advances in genome editing, and the use of machine learning to shape the prospective growth in cardiac tissue engineering. Those interdisciplinary emerging innovations would move forward basic research in this field and their clinical applications. The long-entrenched challenges in this field could be addressed by novel 3-dimensional (3D) scaffold substrates for cardiomyocyte (CM) growth and maturation. Stem cell-based therapy through genome editing techniques can repair gene mutation, control better maturation of CMs or even reveal its molecular clock. Finally, machine learning and precision control for improvements of the construct fabrication process and optimization in tissue-specific clonal selections with an outlook of cardiac tissue engineering are also presented.",2019-06-01,14,863,64,707
880,31304857,A Review of Machine Learning Techniques for Keratoconus Detection and Refractive Surgery Screening,"Various machine learning techniques have been developed for keratoconus detection and refractive surgery screening. These techniques utilize inputs from a range of corneal imaging devices and are built with automated decision trees, support vector machines, and various types of neural networks. In general, these techniques demonstrate very good differentiation of normal and keratoconic eyes, as well as good differentiation of normal and form fruste keratoconus. However, it is difficult to directly compare these studies, as keratoconus represents a wide spectrum of disease. More importantly, no public dataset exists for research purposes. Despite these challenges, machine learning in keratoconus detection and refractive surgery screening is a burgeoning field of study, with significant potential for continued advancement as imaging devices and techniques become more sophisticated.",2019-06-01,3,892,98,707
2490,30657038,Recent Advances on Prediction of Human Papillomaviruses Risk Types,"Background:                    Some studies have shown that Human Papillomavirus (HPV) is strongly associated with cervical cancer. As we all know, cervical cancer still remains the fourth most common cancer, affecting women worldwide. Thus, it is both challenging and essential to detect risk types of human papillomaviruses.              Methods:                    In order to discriminate whether HPV type is highly risky or not, many epidemiological and experimental methods have been proposed recently. For HPV risk type prediction, there also have been a few computational studies which are all based on Machine Learning (ML) techniques, but adopt different feature extraction methods. Therefore, we conclude and discuss several classical approaches which have got a better result for the risk type prediction of HPV.              Results:                    This review summarizes the common methods to detect human papillomavirus. The main methods are sequence- derived features, text-based classification, gap-kernel method, ensemble SVM, Word statistical model, position- specific statistical model and mismatch kernel method (SVM). Among these methods, position-specific statistical model get a relatively high accuracy rate (accuracy=97.18%). Word statistical model is also a novel approach, which extracted the information of HPV from the protein ""sequence space"" with word statistical model to predict high-risk types of HPVs (accuracy=95.59%). These methods could potentially be used to improve prediction of highrisk types of HPVs.              Conclusion:                    From the prediction accuracy, we get that the classification results are more accurate by establishing mathematical models. Thus, adopting mathematical methods to predict risk type of HPV will be the main goal of research in the future.",2019-06-01,0,1829,66,707
915,31249591,Inferring Interaction Networks From Multi-Omics Data,"A major goal in systems biology is a comprehensive description of the entirety of all complex interactions between different types of biomolecules-also referred to as the interactome-and how these interactions give rise to higher, cellular and organism level functions or diseases. Numerous efforts have been undertaken to define such interactomes experimentally, for example yeast-two-hybrid based protein-protein interaction networks or ChIP-seq based protein-DNA interactions for individual proteins. To complement these direct measurements, genome-scale quantitative multi-omics data (transcriptomics, proteomics, metabolomics, etc.) enable researchers to predict novel functional interactions between molecular species. Moreover, these data allow to distinguish relevant functional from non-functional interactions in specific biological contexts. However, integration of multi-omics data is not straight forward due to their heterogeneity. Numerous methods for the inference of interaction networks from homogeneous functional data exist, but with the advent of large-scale paired multi-omics data a new class of methods for inferring comprehensive networks across different molecular species began to emerge. Here we review state-of-the-art techniques for inferring the topology of interaction networks from functional multi-omics data, encompassing graphical models with multiple node types and quantitative-trait-loci (QTL) based approaches. In addition, we will discuss Bayesian aspects of network inference, which allow for leveraging already established biological information such as known protein-protein or protein-DNA interactions, to guide the inference process.",2019-06-01,14,1679,52,707
122,30156155,Targeting Virus-host Protein Interactions: Feature Extraction and Machine Learning Approaches,"Background:                    Targeting critical viral-host Protein-Protein Interactions (PPIs) has enormous application prospects for therapeutics. Using experimental methods to evaluate all possible virus-host PPIs is labor-intensive and time-consuming. Recent growth in computational identification of virus-host PPIs provides new opportunities for gaining biological insights, including applications in disease control. We provide an overview of recent computational approaches for studying virus-host PPI interactions.              Methods:                    In this review, a variety of computational methods for virus-host PPIs prediction have been surveyed. These methods are categorized based on the features they utilize and different machine learning algorithms including classical and novel methods.              Results:                    We describe the pivotal and representative features extracted from relevant sources of biological data, mainly include sequence signatures, known domain interactions, protein motifs and protein structure information. We focus on state-of-the-art machine learning algorithms that are used to build binary prediction models for the classification of virus-host protein pairs and discuss their abilities, weakness and future directions.              Conclusion:                    The findings of this review confirm the importance of computational methods for finding the potential protein-protein interactions between virus and host. Although there has been significant progress in the prediction of virus-host PPIs in recent years, there is a lot of room for improvement in virus-host PPI prediction.",2019-06-01,9,1655,93,707
867,31324321,Diagnosis of seizures and encephalopathy using conventional EEG and amplitude integrated EEG,"Seizures are more common in the neonatal period than at any other time of life, partly due to the relative hyperexcitability of the neonatal brain. Brain monitoring of sick neonates in the NICU using either conventional electroencephalography or amplitude integrated EEG is essential to accurately detect seizures. Treatment of seizures is important, as evidence increasingly indicates that seizures damage the brain in addition to that caused by the underlying etiology. Prompt treatment has been shown to reduce seizure burden with the potential to ameliorate seizure-mediated damage. Neonatal encephalopathy most commonly caused by a hypoxia-ischemia results in an alteration of mental status and problems such as seizures, hypotonia, apnea, and feeding difficulties. Confirmation of encephalopathy with EEG monitoring can act as an important adjunct to other investigations and the clinical examination, particularly when considering treatment strategies such as therapeutic hypothermia. Brain monitoring also provides useful early prognostic indicators to clinicians. Recent use of machine learning in algorithms to continuously monitor the neonatal EEG, detect seizures, and grade encephalopathy offers the exciting prospect of real-time decision support in the NICU in the very near future.",2019-06-01,0,1297,92,707
1714,31621015,A Strength-Weaknesses-Opportunities-Threats (SWOT) Analysis of Cheminformatics in Natural Product Research,"Cheminformatics-based techniques, such as molecular modeling, docking, virtual screening, and machine learning, are well accepted for their usefulness in drug discovery and development of therapeutically relevant small molecules. Although delayed by several decades, their application in natural product research has led to outstanding findings. Combining information obtained from different sources, i.e., virtual predictions, traditional medicine, structural, biochemical, and biological data, and handling big data effectively will open up new possibilities, but also challenges in the future. Strategies and examples will be presented on how to integrate cheminformatics in pharmacognostic workflows to benefit from these two highly complementary disciplines toward streamlining experimental efforts. While considering their limits and pitfalls and by exploiting their potential, computer-aided strategies should successfully guide future studies and thereby augment our knowledge of bioactive natural lead structures.",2019-06-01,1,1022,106,707
860,31341467,Speech Technology Progress Based on New Machine Learning Paradigm,"Speech technologies have been developed for decades as a typical signal processing area, while the last decade has brought a huge progress based on new machine learning paradigms. Owing not only to their intrinsic complexity but also to their relation with cognitive sciences, speech technologies are now viewed as a prime example of interdisciplinary knowledge area. This review article on speech signal analysis and processing, corresponding machine learning algorithms, and applied computational intelligence aims to give an insight into several fields, covering speech production and auditory perception, cognitive aspects of speech communication and language understanding, both speech recognition and text-to-speech synthesis in more details, and consequently the main directions in development of spoken dialogue systems. Additionally, the article discusses the concepts and recent advances in speech signal compression, coding, and transmission, including cognitive speech coding. To conclude, the main intention of this article is to highlight recent achievements and challenges based on new machine learning paradigms that, over the last decade, had an immense impact in the field of speech signal processing.",2019-06-01,0,1219,65,707
908,31257740,Digital Medicine in Thyroidology: A New Era of Managing Thyroid Disease,"Digital medicine has the capacity to affect all aspects of medicine, including disease prediction, prevention, diagnosis, treatment, and post-treatment management. In the field of thyroidology, researchers are also investigating potential applications of digital technology for the thyroid disease. Recent studies using artificial intelligence (AI)/machine learning (ML) have reported reasonable performance for the classification of thyroid nodules based on ultrasonographic (US) images. AI/ML-based methods have also shown good diagnostic accuracy for distinguishing between benign and malignant thyroid lesions based on cytopathologic findings. Assistance from AI/ML methods could overcome the limitations of conventional thyroid US and fine-needle aspiration cytology. A web-based database has been developed for thyroid cancer care. In addition to its role as a nationwide registry of thyroid cancer, it is expected to serve as a clinical platform to facilitate better thyroid cancer care and as a research platform providing comprehensive disease-specific big data. Evidence has been found that biosignal monitoring with wearable devices may predict thyroid dysfunction. This real-world thyroid function monitoring could aid in the management and early detection of thyroid dysfunction. In the thyroidology field, research involving the range of digital medicine technologies and their clinical applications is expected to be even more active in the future.",2019-06-01,0,1463,71,707
2171,31705495,Development of Neuroimaging-Based Biomarkers in Psychiatry,"This chapter presents an overview of accumulating neuroimaging data with emphasis on translational potential. The subject will be described in the context of three disease states, i.e., schizophrenia, bipolar disorder, and major depressive disorder, and for three clinical goals, i.e., disease risk assessment, subtyping, and treatment decision.",2019-06-01,1,345,58,707
907,31258772,"Tracking knowledge evolution, hotspots and future directions of emerging technologies in cancers research: a bibliometrics review","Due to various environmental pollution issues, cancers have become the ""first killer"" of human beings in the 21st century and their control has become a global strategy of human health. The increasing development of emerging information technologies has provided opportunities for prevention, early detection, diagnosis, intervention, prognosis, nursing, and rehabilitation of cancers. In recent years, the literature associated with emerging technologies in cancer has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of this field. To explore the dynamic knowledge evolution of emerging information technologies in cancer literature, we comprehensively analyzed the development status and research hotspots in this field from bibliometrics perspective. We collected 7,136 articles (2000-2017) from the Web of Science database and visually displayed the dynamic knowledge evolution process via the analysis on time-sequence changes, spatial distribution, knowledge base, and hotspots. Much institutional cooperation occurs in this field, and research groups are relatively concentrated. BMC Bioinformatics, PLOS One, Journal of Urology, Scientific Reports, and Bioinformatics are the top five journals in this field. Research hotspots are mainly concentrated in two dimensions: the disease dimension (e.g., cancer, breast cancer, and prostate cancer), and the technical dimension (e.g., robotics, machine learning, data mining, and etc.). The emerging technologies in cancer research is fast ascending and promising. This study also provides researchers with panoramic knowledge of this field, as well as research hotspots and future directions.",2019-06-01,6,1726,129,707
1715,31621014,Open-Access Activity Prediction Tools for Natural Products. Case Study: hERG Blockers,"Interference with the hERG potassium ion channel may cause cardiac arrhythmia and can even lead to death. Over the last few decades, several drugs, already on the market, and many more investigational drugs in various development stages, have had to be discontinued because of their hERG-associated toxicity. To recognize potential hERG activity in the early stages of drug development, a wide array of computational tools, based on different principles, such as 3D QSAR, 2D and 3D similarity, and machine learning, have been developed and are reviewed in this chapter. The various available prediction tools Similarity Ensemble Approach, SuperPred, SwissTargetPrediction, HitPick, admetSAR, PASSonline, Pred-hERG, and VirtualToxLab were used to screen a dataset of known hERG synthetic and natural product actives and inactives to quantify and compare their predictive power. This contribution will allow the reader to evaluate the suitability of these computational methods for their own related projects. There is an unmet need for natural product-specific prediction tools in this field.",2019-06-01,1,1092,85,707
2174,31696804,Recent Development of Computational Predicting Bioluminescent Proteins,"Bioluminescent Proteins (BLPs) are widely distributed in many living organisms that act as a key role of light emission in bioluminescence. Bioluminescence serves various functions in finding food and protecting the organisms from predators. With the routine biotechnological application of bioluminescence, it is recognized to be essential for many medical, commercial and other general technological advances. Therefore, the prediction and characterization of BLPs are significant and can help to explore more secrets about bioluminescence and promote the development of application of bioluminescence. Since the experimental methods are money and time-consuming for BLPs identification, bioinformatics tools have played important role in fast and accurate prediction of BLPs by combining their sequences information with machine learning methods. In this review, we summarized and compared the application of machine learning methods in the prediction of BLPs from different aspects. We wish that this review will provide insights and inspirations for researches on BLPs.",2019-06-01,1,1074,70,707
847,31354276,Machine learning techniques in a structural and functional MRI diagnostic approach in schizophrenia: a systematic review,"Background:                    Diagnosis of schizophrenia (SCZ) is made exclusively clinically, since specific biomarkers that can predict the disease accurately remain unknown. Machine learning (ML) represents a promising approach that could support clinicians in the diagnosis of mental disorders.              Objectives:                    A systematic review, according to the PRISMA statement, was conducted to evaluate its accuracy to distinguish SCZ patients from healthy controls.              Methods:                    We systematically searched PubMed, Embase, MEDLINE, PsychINFO and the Cochrane Library through December 2018 using generic terms for ML techniques and SCZ without language or time restriction. Thirty-five studies were included in this review: eight of them used structural neuroimaging, twenty-six used functional neuroimaging and one both, with a minimum accuracy >60% (most of them 75-90%). Sensitivity, Specificity and accuracy were extracted from each publication or obtained directly from authors.              Results:                    Support vector machine, the most frequent technique, if associated with other ML techniques achieved accuracy close to 100%. The prefrontal and temporal cortices appeared to be the most useful brain regions for the diagnosis of SCZ. ML analysis can efficiently detect significantly altered brain connectivity in patients with SCZ (eg, default mode network, visual network, sensorimotor network, frontoparietal network and salience network).              Conclusion:                    The greater accuracy demonstrated by these predictive models and the new models resulting from the integration of multiple ML techniques will be increasingly decisive for early diagnosis and evaluation of the treatment response and to establish the prognosis of patients with SCZ. To achieve a real benefit for patients, the future challenge is to reach an accurate diagnosis not only through clinical evaluation but also with the aid of ML algorithms.",2019-06-01,5,2012,120,707
134,30124147,Survey of Machine Learning Techniques in Drug Discovery,"Background:                    Drug discovery, which is the process of discovering new candidate medications, is very important for pharmaceutical industries. At its current stage, discovering new drugs is still a very expensive and time-consuming process, requiring Phases I, II and III for clinical trials. Recently, machine learning techniques in Artificial Intelligence (AI), especially the deep learning techniques which allow a computational model to generate multiple layers, have been widely applied and achieved state-of-the-art performance in different fields, such as speech recognition, image classification, bioinformatics, etc. One very important application of these AI techniques is in the field of drug discovery.              Methods:                    We did a large-scale literature search on existing scientific websites (e.g, ScienceDirect, Arxiv) and startup companies to understand current status of machine learning techniques in drug discovery.              Results:                    Our experiments demonstrated that there are different patterns in machine learning fields and drug discovery fields. For example, keywords like prediction, brain, discovery, and treatment are usually in drug discovery fields. Also, the total number of papers published in drug discovery fields with machine learning techniques is increasing every year.              Conclusion:                    The main focus of this survey is to understand the current status of machine learning techniques in the drug discovery field within both academic and industrial settings, and discuss its potential future applications. Several interesting patterns for machine learning techniques in drug discovery fields are discussed in this survey.",2019-06-01,23,1743,55,707
475,30277150,A Survey for Predicting Enzyme Family Classes Using Machine Learning Methods,"Enzymes are proteins that act as biological catalysts to speed up cellular biochemical processes. According to their main Enzyme Commission (EC) numbers, enzymes are divided into six categories: EC-1: oxidoreductase; EC-2: transferase; EC-3: hydrolase; EC-4: lyase; EC-5: isomerase and EC-6: synthetase. Different enzymes have different biological functions and acting objects. Therefore, knowing which family an enzyme belongs to can help infer its catalytic mechanism and provide information about the relevant biological function. With the large amount of protein sequences influxing into databanks in the post-genomics age, the annotation of the family for an enzyme is very important. Since the experimental methods are cost ineffective, bioinformatics tool will be a great help for accurately classifying the family of the enzymes. In this review, we summarized the application of machine learning methods in the prediction of enzyme family from different aspects. We hope that this review will provide insights and inspirations for the researches on enzyme family classification.",2019-06-01,5,1086,76,707
1716,31621013,Cheminformatic Analysis of Natural Product Fragments,"Fragment-like natural products play a pivotal role in natural product research given their improved synthetic and computational tractability as well as commercial availability compared to more complex natural product structures. A multitude of computational tools have been developed to support the generation, analysis, and application of natural fragments for drug discovery and chemical biology research. In this contribution, the challenges and opportunities in such workflows are discussed and contextualized. Multiple successful applications and validations discussed herein attest to the relevance of natural fragments for drug discovery and the utility of machine learning and data science to support such endeavors.",2019-06-01,0,724,52,707
2415,30831310,Ten simple rules for predictive modeling of individual differences in neuroimaging,"Establishing brain-behavior associations that map brain organization to phenotypic measures and generalize to novel individuals remains a challenge in neuroimaging. Predictive modeling approaches that define and validate models with independent datasets offer a solution to this problem. While these methods can detect novel and generalizable brain-behavior associations, they can be daunting, which has limited their use by the wider connectivity community. Here, we offer practical advice and examples based on functional magnetic resonance imaging (fMRI) functional connectivity data for implementing these approaches. We hope these ten rules will increase the use of predictive models with neuroimaging data.",2019-06-01,49,712,82,707
1717,31621011,A Toolbox for the Identification of Modes of Action of Natural Products,"Natural products have long played a leading role as direct source of drugs or as a means to inspire informed molecular design. Indeed, natural products have been biologically prevalidated as protein-binding motifs by millions of years of evolutionary pressure. Despite the tailored architectures, and the ever-growing chemistry toolbox to aid access such privileged structures, identifying the modes of action by which these molecules can be harnessed as therapeutics remains a major bottleneck in discovery chemistry. Herein, an overview of cheminformatics methods applied to the identification of modes of action of natural products is given, and a discussion of successful case studies is provided. A special focus is given to machine learning methods that may help to streamline the development of natural products into drug leads.",2019-06-01,0,835,71,707
2287,31062835,Advancing Alzheimer's Disease Treatment: Lessons from CTAD 2018,"The 2018 Clinical Trials on Alzheimer's Disease (CTAD) conference showcased recent successes and failures in trials of Alzheimer's disease treatments. More importantly, the conference provided opportunities for investigators to share what they have learned from those studies with the goal of designing future trials with a greater likelihood of success. Data from studies of novel and non-amyloid treatment approaches were also shared, including neuroprotective and regenerative strategies and those that target neuroinflammation and synaptic function. New tools to improve the efficiency and productivity of clinical trials were described, including biomarkers and machine learning algorithms for predictive modeling.",2019-06-01,2,719,63,707
2286,31063735,Assessing exposure to outdoor air pollution for epidemiological studies: Model-based and personal sampling strategies,"Epidemiologic studies have found air pollution to be causally linked to respiratory health including the exacerbation and development of childhood asthma. Accurately characterizing exposure is paramount in these studies to ensure valid estimates of health effects. Here, we provide a brief overview of the evolution of air pollution exposure assessment ranging from the use of ground-based, single-site air monitoring stations for population-level estimates to recent advances in spatiotemporal models, which use advanced machine learning algorithms and satellite-based data to accurately estimate individual-level daily exposures at high spatial resolutions. In addition, we review recent advances in sensor technology that enable the use of personal monitoring in epidemiologic studies, long-considered the ""holy grail"" of air pollution exposure assessment. Finally, we highlight key advantages and uses of each approach including the generalizability and public health relevance of air pollution models and the accuracy of personal monitors that are useful to guide personalized prevention strategies. Investigators and clinicians interested in the effects of air pollution on allergic disease and asthma should carefully consider the pros and cons of each approach to guide their application in research and practice.",2019-06-01,8,1321,117,707
1718,31621009,Cheminformatics Explorations of Natural Products,"The chemistry of natural products is fascinating and has continuously attracted the attention of the scientific community for many reasons including, but not limited to, biosynthesis pathways, chemical diversity, the source of bioactive compounds and their marked impact on drug discovery. There is a broad range of experimental and computational techniques (molecular modeling and cheminformatics) that have evolved over the years and have assisted the investigation of natural products. Herein, we discuss cheminformatics strategies to explore the chemistry and applications of natural products. Since the potential synergisms between cheminformatics and natural products are vast, we will focus on three major aspects: (1) exploration of the chemical space of natural products to identify bioactive compounds, with emphasis on drug discovery; (2) assessment of the toxicity profile of natural products; and (3) diversity analysis of natural product collections and the design of chemical collections inspired by natural sources.",2019-06-01,2,1031,48,707
2247,31132292,Machine Learning in the Detection of the Glaucomatous Disc and Visual Field,"Glaucoma is the leading cause of irreversible blindness worldwide. Early detection is of utmost importance as there is abundant evidence that early treatment prevents disease progression, preserves vision, and improves patients' long-term quality of life. The structure and function thresholds that alert to the diagnosis of glaucoma can be obtained entirely via digital means, and as such, screening is well suited to benefit from artificial intelligence and specifically machine learning. This paper reviews the concepts and current literature on the use of machine learning for detection of the glaucomatous disc and visual field.",2019-06-01,0,633,75,707
2532,30561351,Neuroimaging and Machine Learning for Dementia Diagnosis: Recent Advancements and Future Prospects,"Dementia, a chronic and progressive cognitive declination of brain function caused by disease or impairment, is becoming more prevalent due to the aging population. A major challenge in dementia is achieving accurate and timely diagnosis. In recent years, neuroimaging with computer-aided algorithms have made remarkable advances in addressing this challenge. The success of these approaches is mostly attributed to the application of machine learning techniques for neuroimaging. In this review paper, we present a comprehensive survey of automated diagnostic approaches for dementia using medical image analysis and machine learning algorithms published in the recent years. Based on the rigorous review of the existing works, we have found that, while most of the studies focused on Alzheimer's disease, recent research has demonstrated reasonable performance in the identification of other types of dementia remains a major challenge. Multimodal imaging analysis deep learning approaches have shown promising results in the diagnosis of these other types of dementia. The main contributions of this review paper are as follows. 1) Based on the detailed analysis of the existing literature, this paper discusses neuroimaging procedures for dementia diagnosis. 2) It systematically explains the most recent machine learning techniques and, in particular, deep learning approaches for early detection of dementia.",2019-06-01,9,1414,98,707
2510,30616329,Application of machine learning in rheumatic disease research,"Over the past decade, there has been a paradigm shift in how clinical data are collected, processed and utilized. Machine learning and artificial intelligence, fueled by breakthroughs in high-performance computing, data availability and algorithmic innovations, are paving the way to effective analyses of large, multi-dimensional collections of patient histories, laboratory results, treatments, and outcomes. In the new era of machine learning and predictive analytics, the impact on clinical decision-making in all clinical areas, including rheumatology, will be unprecedented. Here we provide a critical review of the machine-learning methods currently used in the analysis of clinical data, the advantages and limitations of these methods, and how they can be leveraged within the field of rheumatology.",2019-07-01,5,808,61,677
2355,29438494,Artificial intelligence in drug combination therapy,"Currently, the development of medicines for complex diseases requires the development of combination drug therapies. It is necessary because in many cases, one drug cannot target all necessary points of intervention. For example, in cancer therapy, a physician often meets a patient having a genomic profile including more than five molecular aberrations. Drug combination therapy has been an area of interest for a while, for example the classical work of Loewe devoted to the synergism of drugs was published in 1928-and it is still used in calculations for optimal drug combinations. More recently, over the past several years, there has been an explosion in the available information related to the properties of drugs and the biomedical parameters of patients. For the drugs, hundreds of 2D and 3D molecular descriptors for medicines are now available, while for patients, large data sets related to genetic/proteomic and metabolomics profiles of the patients are now available, as well as the more traditional data relating to the histology, history of treatments, pretreatment state of the organism, etc. Moreover, during disease progression, the genetic profile can change. Thus, the ability to optimize drug combinations for each patient is rapidly moving beyond the comprehension and capabilities of an individual physician. This is the reason, that biomedical informatics methods have been developed and one of the more promising directions in this field is the application of artificial intelligence (AI). In this review, we discuss several AI methods that have been successfully implemented in several instances of combination drug therapy from HIV, hypertension, infectious diseases to cancer. The data clearly show that the combination of rule-based expert systems with machine learning algorithms may be promising direction in this field.",2019-07-01,6,1854,51,677
2217,31177872,An update on advanced dual-energy CT for head and neck cancer imaging,"Introduction: Dual-energy-computed tomography (DECT) is an advanced form of computed tomography (CT) that enables spectral tissue characterization beyond what is possible with conventional CT scans. DECT can improve non-invasive diagnostic evaluation of the neck, especially for the evaluation of head and neck cancer. Areas covered: This article is a review of current applications of DECT for the evaluation of head and neck cancer, focusing largely on squamous cell carcinoma (HNSCC). The article will begin with a brief overview of principles and different approaches for DECT scanning. This will be followed by a review of different DECT applications in diagnostic imaging and radiation oncology, practical and workflow considerations, and various emerging advanced applications for tumor analysis, including the use of DECT datasets for radiomics and machine learning applications. Expert opinion: Using a multi-parametric approach, different DECT reconstructions can be used to improve diagnostic evaluation and surveillance of head and neck cancer, including improving visibility of HNSCC, determination of tumor boundaries and extent, and invasion of critical organs such as the thyroid cartilage. In the future, the large amount of quantitative information on DECT scans may be leveraged for improving radiomic and machine learning models for tumor characterization.",2019-07-01,0,1376,69,677
855,31344945,Small Genomes and Big Data: Adaptation of Plastid Genomics to the High-Throughput Era,"Plastid genome sequences are becoming more readily available with the increase in high-throughput sequencing, and whole-organelle genetic data is available for algae and plants from across the diversity of photosynthetic eukaryotes. This has provided incredible opportunities for studying species which may not be amenable to in vivo study or genetic manipulation or may not yet have been cultured. Research into plastid genomes has pushed the limits of what can be deduced from genomic information, and in particular genomic information obtained from public databases. In this Review, we discuss how research into plastid genomes has benefitted enormously from the explosion of publicly available genome sequence. We describe two case studies in how using publicly available gene data has supported previously held hypotheses about plastid traits from lineage-restricted experiments across algal and plant diversity. We propose how this approach could be used across disciplines for inferring functional and biological characteristics from genomic approaches, including integration of new computational and bioinformatic approaches such as machine learning. We argue that the techniques developed to gain the maximum possible insight from plastid genomes can be applied across the eukaryotic tree of life.",2019-07-01,0,1306,85,677
861,31340696,Results of the European Group for the Study of Resistant Depression (GSRD) - basis for further research and clinical practice,"Objectives: The overview outlines two decades of research from the European Group for the Study of Resistant Depression (GSRD) that fundamentally impacted evidence-based algorithms for diagnostics and psychopharmacotherapy of treatment-resistant depression (TRD). Methods: The GSRD staging model characterising response, non-response and resistance to antidepressant (AD) treatment was applied to 2762 patients in eight European countries. Results: In case of non-response, dose escalation and switching between different AD classes did not show superiority over continuation of original AD treatment. Predictors for TRD were symptom severity, duration of the current major depressive episode (MDE), suicidality, psychotic and melancholic features, comorbid anxiety and personality disorders, add-on treatment, non-response to the first AD, adverse effects, high occupational level, recurrent disease course, previous hospitalisations, positive family history of MDD, early age of onset and novel associations of single nucleoid polymorphisms (SNPs) within the PPP3CC, ST8SIA2, CHL1, GAP43 and ITGB3 genes and gene pathways associated with neuroplasticity, intracellular signalling and chromatin silencing. A prediction model reaching accuracy of above 0.7 highlighted symptom severity, suicidality, comorbid anxiety and lifetime MDEs as the most informative predictors for TRD. Applying machine-learning algorithms, a signature of three SNPs of the BDNF, PPP3CC and HTR2A genes and lacking melancholia predicted treatment response. Conclusions: The GSRD findings offer a unique and balanced perspective on TRD representing foundation for further research elaborating on specific clinical and genetic hypotheses and treatment strategies within appropriate study-designs, especially interaction-based models and randomized controlled trials.",2019-07-01,10,1840,125,677
862,31330861,A Guide for Using Deep Learning for Complex Trait Genomic Prediction,"Deep learning (DL) has emerged as a powerful tool to make accurate predictions from complex data such as image, text, or video. However, its ability to predict phenotypic values from molecular data is less well studied. Here, we describe the theoretical foundations of DL and provide a generic code that can be easily modified to suit specific needs. DL comprises a wide variety of algorithms which depend on numerous hyperparameters. Careful optimization of hyperparameter values is critical to avoid overfitting. Among the DL architectures currently tested in genomic prediction, convolutional neural networks (CNNs) seem more promising than multilayer perceptrons (MLPs). A limitation of DL is in interpreting the results. This may not be relevant for genomic prediction in plant or animal breeding but can be critical when deciding the genetic risk to a disease. Although DL technologies are not ""plug-and-play"", they are easily implemented using Keras and TensorFlow public software. To illustrate the principles described here, we implemented a Keras-based code in GitHub.",2019-07-01,16,1078,68,677
868,31322128,Computer-Aided Detection for Breast Cancer Screening in Clinical Settings: Scoping Review,"Background:                    With the growth of machine learning applications, the practice of medicine is evolving. Computer-aided detection (CAD) is a software technology that has become widespread in radiology practices, particularly in breast cancer screening for improving detection rates at earlier stages. Many studies have investigated the diagnostic accuracy of CAD, but its implementation in clinical settings has been largely overlooked.              Objective:                    The aim of this scoping review was to summarize recent literature on the adoption and implementation of CAD during breast cancer screening by radiologists and to describe barriers and facilitators for CAD use.              Methods:                    The MEDLINE database was searched for English, peer-reviewed articles that described CAD implementation, including barriers or facilitators, in breast cancer screening and were published between January 2010 and March 2018. Articles describing the diagnostic accuracy of CAD for breast cancer detection were excluded. The search returned 526 citations, which were reviewed in duplicate through abstract and full-text screening. Reference lists and cited references in the included studies were reviewed.              Results:                    A total of nine articles met the inclusion criteria. The included articles showed that there is a tradeoff between the facilitators and barriers for CAD use. Facilitators for CAD use were improved breast cancer detection rates, increased profitability of breast imaging, and time saved by replacing double reading. Identified barriers were less favorable perceptions of CAD compared to double reading by radiologists, an increase in recall rates of patients for further testing, increased costs, and unclear effect on patient outcomes.              Conclusions:                    There is a gap in the literature between CAD's well-established diagnostic accuracy and its implementation and use by radiologists. Generally, the perceptions of radiologists have not been considered and details of implementation approaches for adoption of CAD have not been reported. The cost-effectiveness of CAD has not been well established for breast cancer screening in various populations. Further research is needed on how to best facilitate CAD in radiology practices in order to optimize patient outcomes, and the views of radiologists need to be better considered when advancing CAD use.",2019-07-01,1,2469,89,677
869,31320024,Impact of Artificial Intelligence on Interventional Cardiology: From Decision-Making Aid to Advanced Interventional Procedure Assistance,"Access to big data analyzed by supercomputers using advanced mathematical algorithms (i.e., deep machine learning) has allowed for enhancement of cognitive output (i.e., visual imaging interpretation) to previously unseen levels and promises to fundamentally change the practice of medicine. This field, known as ""artificial intelligence"" (AI), is making significant progress in areas such as automated clinical decision making, medical imaging analysis, and interventional procedures, and has the potential to dramatically influence the practice of interventional cardiology. The unique nature of interventional cardiology makes it an ideal target for the development of AI-based technologies designed to improve real-time clinical decision making, streamline workflow in the catheterization laboratory, and standardize catheter-based procedures through advanced robotics. This review provides an introduction to AI by highlighting its scope, potential applications, and limitations in interventional cardiology.",2019-07-01,3,1013,136,677
873,31313637,Mapping the Passions: Toward a High-Dimensional Taxonomy of Emotional Experience and Expression,"What would a comprehensive atlas of human emotions include? For 50 years, scientists have sought to map emotion-related experience, expression, physiology, and recognition in terms of the ""basic six""-anger, disgust, fear, happiness, sadness, and surprise. Claims about the relationships between these six emotions and prototypical facial configurations have provided the basis for a long-standing debate over the diagnostic value of expression (for review and latest installment in this debate, see Barrett et al., p. 1). Building on recent empirical findings and methodologies, we offer an alternative conceptual and methodological approach that reveals a richer taxonomy of emotion. Dozens of distinct varieties of emotion are reliably distinguished by language, evoked in distinct circumstances, and perceived in distinct expressions of the face, body, and voice. Traditional models-both the basic six and affective-circumplex model (valence and arousal)-capture a fraction of the systematic variability in emotional response. In contrast, emotion-related responses (e.g., the smile of embarrassment, triumphant postures, sympathetic vocalizations, blends of distinct expressions) can be explained by richer models of emotion. Given these developments, we discuss why tests of a basic-six model of emotion are not tests of the diagnostic value of facial expression more generally. Determining the full extent of what facial expressions can tell us, marginally and in conjunction with other behavioral and contextual cues, will require mapping the high-dimensional, continuous space of facial, bodily, and vocal signals onto richly multifaceted experiences using large-scale statistical modeling and machine-learning methods.",2019-07-01,6,1727,95,677
875,31312621,The Future of Robotic Surgery in Pediatric Urology: Upcoming Technology and Evolution Within the Field,"Since the introduction of the Da Vinci Surgical System (Intuitive Surgical, Inc., Sunnyvale, CA) in 1999, the market for robot assisted laparoscopic surgery has grown with urology. The initial surgical advantage seen in adults was for robotic prostatectomy, and over time this expanded to the pediatric population with robotic pyeloplasty. The introduction of three-dimensional visualization, tremor elimination, a 4th arm, and 7-degree range of motion allowed a significant operator advantage over laparoscopy, especially for anastomotic suturing. After starting with pyeloplasty, the use of robotic technology with pediatric urology has expanded to include ureteral reimplantation and even more complex reconstructive procedures, such as enterocystoplasty, appendicovesicostomy, and bladder neck reconstruction. However, limitations of the Da Vinci Surgical Systems still exist despite its continued technological advances over multiple generations in the past 20 years. Due to the smaller pediatric market, less focus appears to have been placed on the development of the smaller 5 mm instruments. As pediatric urology continues to utilize robotic technology for minimally invasive surgery, there is hope that additional pediatric-friendly instruments and components will be developed, either by Intuitive Surgical or one of the new robotic platforms in development that are working to address many of the shortcomings of current systems. These new robotic platforms include improved haptic feedback systems, flexible scopes, easier maneuverability, and even adaptive machine learning concepts to bring robotic assisted laparoscopic surgery to the next level. In this report, we review the present and upcoming technological advances of the current Da Vinci surgical systems as well as various new robotic platforms, each offering a unique set of technological advantages. As technology progresses, the understanding of and access to these new robotic platforms will help guide pediatric urologists into the next forefront of minimally invasive surgery.",2019-07-01,0,2056,102,677
876,31312276,The application of convolutional neural network to stem cell biology,"Induced pluripotent stem cells (iPSC) are one the most prominent innovations of medical research in the last few decades. iPSCs can be easily generated from human somatic cells and have several potential uses in regenerative medicine, disease modeling, drug screening, and precision medicine. However, further innovation is still required to realize their full potential. Machine learning is an algorithm that learns from large datasets for pattern formation and classification. Deep learning, a form of machine learning, uses a multilayered neural network that mimics human neural circuit structure. Deep neural networks can automatically extract features from an image, although classical machine learning methods still require feature extraction by a human expert. Deep learning technology has developed recently; in particular, the accuracy of an image classification task by using a convolutional neural network (CNN) has exceeded that of humans since 2015. CNN is now used to address several tasks including medical issues. We believe that CNN would also have a great impact on the research of stem cell biology. iPSCs are utilized after their differentiation to specific cells, which are characterized by molecular techniques such as immunostaining or lineage tracing. Each cell shows a characteristic morphology; thus, a morphology-based identification system of cell type by CNN would be an alternative technique. The development of CNN enables the automation of identifying cell types from phase contrast microscope images without molecular labeling, which will be applied to several researches and medical science. Image classification is a strong field among deep learning tasks, and several medical tasks will be solved by deep learning-based programs in the future.",2019-07-01,8,1779,68,677
877,31311655,Integrative Approaches to Cancer Immunotherapy,"Cancer immunotherapy aims to arm patients with cancer-fighting immunity. Many new cancer-specific immunotherapeutic drugs have gained approval in the past several years, demonstrating immunotherapy's efficacy and promise as an anticancer modality. Despite these successes, several outstanding questions remain for cancer immunotherapy, including how to make immunotherapy more efficacious in a broader range of cancer types and patients, and how to predict which patients will respond or not respond to therapy. We present a case for integrative systems approaches that will answer these questions. This involves applying mechanistic and statistical modeling, establishing consistent and widely adopted experimental tools to generate systems-level data, and creating sustained mechanisms of support. If implemented, these approaches will lead to major advances in cancer treatment.",2019-07-01,8,881,46,677
878,31311203,A Survey on Recent Trends and Open Issues in Energy Efficiency of 5G,"The rapidly increasing interest from various verticals for the upcoming 5th generation (5G) networks expect the network to support higher data rates and have an improved quality of service. This demand has been met so far by employing sophisticated transmission techniques including massive Multiple Input Multiple Output (MIMO), millimeter wave (mmWave) bands as well as bringing the computational power closer to the users via advanced baseband processing units at the base stations. Future evolution of the networks has also been assumed to open many new business horizons for the operators and the need of not only a resource efficient but also an energy efficient ecosystem has greatly been felt. The deployment of small cells has been envisioned as a promising answer for handling the massive heterogeneous traffic, but the adverse economic and environmental impacts cannot be neglected. Given that 10% of the world's energy consumption is due to the Information and Communications Technology (ICT) industry, energy-efficiency has thus become one of the key performance indicators (KPI). Various avenues of optimization, game theory and machine learning have been investigated for enhancing power allocation for downlink and uplink channels, as well as other energy consumption/saving approaches. This paper surveys the recent works that address energy efficiency of the radio access as well as the core of wireless networks, and outlines related challenges and open issues.",2019-07-01,4,1480,68,677
884,31295267,Machine and deep learning meet genome-scale metabolic modeling,"Omic data analysis is steadily growing as a driver of basic and applied molecular biology research. Core to the interpretation of complex and heterogeneous biological phenotypes are computational approaches in the fields of statistics and machine learning. In parallel, constraint-based metabolic modeling has established itself as the main tool to investigate large-scale relationships between genotype, phenotype, and environment. The development and application of these methodological frameworks have occurred independently for the most part, whereas the potential of their integration for biological, biomedical, and biotechnological research is less known. Here, we describe how machine learning and constraint-based modeling can be combined, reviewing recent works at the intersection of both domains and discussing the mathematical and practical aspects involved. We overlap systematic classifications from both frameworks, making them accessible to nonexperts. Finally, we delineate potential future scenarios, propose new joint theoretical frameworks, and suggest concrete points of investigation for this joint subfield. A multiview approach merging experimental and knowledge-driven omic data through machine learning methods can incorporate key mechanistic information in an otherwise biologically-agnostic learning process.",2019-07-01,16,1337,62,677
1119,31388413,Radiomics and Artificial Intelligence for Biomarker and Prediction Model Development in Oncology,Unlabelled Image.,2019-07-01,18,17,96,677
1113,31396245,Hydroponic Solutions for Soilless Production Systems: Issues and Opportunities in a Smart Agriculture Perspective,"Soilless cultivation represent a valid opportunity for the agricultural production sector, especially in areas characterized by severe soil degradation and limited water availability. Furthermore, this agronomic practice embodies a favorable response toward an environment-friendly agriculture and a promising tool in the vision of a general challenge in terms of food security. This review aims therefore at unraveling limitations and opportunities of hydroponic solutions used in soilless cropping systems focusing on the plant mineral nutrition process. In particular, this review provides information (1) on the processes and mechanisms occurring in the hydroponic solutions that ensure an adequate nutrient concentration and thus an optimal nutrient acquisition without leading to nutritional disorders influencing ultimately also crop quality (e.g., solubilization/precipitation of nutrients/elements in the hydroponic solution, substrate specificity in the nutrient uptake process, nutrient competition/antagonism and interactions among nutrients); (2) on new emerging technologies that might improve the management of soilless cropping systems such as the use of nanoparticles and beneficial microorganism like plant growth-promoting rhizobacteria (PGPRs); (3) on tools (multi-element sensors and interpretation algorithms based on machine learning logics to analyze such data) that might be exploited in a smart agriculture approach to monitor the availability of nutrients/elements in the hydroponic solution and to modify its composition in realtime. These aspects are discussed considering what has been recently demonstrated at the scientific level and applied in the industrial context.",2019-07-01,5,1700,113,677
1775,31545114,A survey of breast cancer screening techniques: thermography and electrical impedance tomography,"Breast cancer is a disease that threat many women's life, thus, the early and accurate detection play a key role in reducing the mortality rate. Mammography stands as the reference technique for breast cancer screening; nevertheless, many countries still lack access to mammograms due to economic, social and cultural issues. Last advances in computational tools, infra-red cameras and devices for bio-impedance quantification allowed the development of parallel techniques like, thermography, infra-red imaging and electrical impedance tomography, these being faster, reliable and cheaper. In the last decades, these have been considered as complement procedures for breast cancer diagnosis, where many studies concluded that false positive and false negative rates are greatly reduced. This work aims to review the last breakthroughs about the three above-mentioned techniques describing the benefits of mixing several computational skills to obtain a better global performance. In addition, we provide a comparison between several machine learning techniques applied to breast cancer diagnosis going from logistic regression, decision trees and random forest to artificial, deep and convolutional neural networks. Finally, it is mentioned several recommendations for 3D breast simulations, pre-processing techniques, biomedical devices in the research field, prediction of tumour location and size.",2019-07-01,1,1401,96,677
852,31349617,Computer Methods for Automatic Locomotion and Gesture Tracking in Mice and Small Animals for Neuroscience Applications: A Survey,"Neuroscience has traditionally relied on manually observing laboratory animals in controlled environments. Researchers usually record animals behaving freely or in a restrained manner and then annotate the data manually. The manual annotation is not desirable for three reasons; (i) it is time-consuming, (ii) it is prone to human errors, and (iii) no two human annotators will 100% agree on annotation, therefore, it is not reproducible. Consequently, automated annotation for such data has gained traction because it is efficient and replicable. Usually, the automatic annotation of neuroscience data relies on computer vision and machine learning techniques. In this article, we have covered most of the approaches taken by researchers for locomotion and gesture tracking of specific laboratory animals, i.e. rodents. We have divided these papers into categories based upon the hardware they use and the software approach they take. We have also summarized their strengths and weaknesses.",2019-07-01,3,991,128,677
2464,30710543,Artificial Intelligence Transforms the Future of Health Care,"Life sciences researchers using artificial intelligence (AI) are under pressure to innovate faster than ever. Large, multilevel, and integrated data sets offer the promise of unlocking novel insights and accelerating breakthroughs. Although more data are available than ever, only a fraction is being curated, integrated, understood, and analyzed. AI focuses on how computers learn from data and mimic human thought processes. AI increases learning capacity and provides decision support system at scales that are transforming the future of health care. This article is a review of applications for machine learning in health care with a focus on clinical, translational, and public health applications with an overview of the important role of privacy, data sharing, and genetic information.",2019-07-01,20,792,60,677
2425,30815669,Deep learning for cardiovascular medicine: a practical primer,"Deep learning (DL) is a branch of machine learning (ML) showing increasing promise in medicine, to assist in data classification, novel disease phenotyping and complex decision making. Deep learning is a form of ML typically implemented via multi-layered neural networks. Deep learning has accelerated by recent advances in computer hardware and algorithms and is increasingly applied in e-commerce, finance, and voice and image recognition to learn and classify complex datasets. The current medical literature shows both strengths and limitations of DL. Strengths of DL include its ability to automate medical image interpretation, enhance clinical decision-making, identify novel phenotypes, and select better treatment pathways in complex diseases. Deep learning may be well-suited to cardiovascular medicine in which haemodynamic and electrophysiological indices are increasingly captured on a continuous basis by wearable devices as well as image segmentation in cardiac imaging. However, DL also has significant weaknesses including difficulties in interpreting its models (the 'black-box' criticism), its need for extensive adjudicated ('labelled') data in training, lack of standardization in design, lack of data-efficiency in training, limited applicability to clinical trials, and other factors. Thus, the optimal clinical application of DL requires careful formulation of solvable problems, selection of most appropriate DL algorithms and data, and balanced interpretation of results. This review synthesizes the current state of DL for cardiovascular clinicians and investigators, and provides technical context to appreciate the promise, pitfalls, near-term challenges, and opportunities for this exciting new area.",2019-07-01,18,1730,61,677
895,31277828,Machine Learning for Health Services Researchers,"Background:                    Machine learning is increasingly used to predict healthcare outcomes, including cost, utilization, and quality.              Objective:                    We provide a high-level overview of machine learning for healthcare outcomes researchers and decision makers.              Methods:                    We introduce key concepts for understanding the application of machine learning methods to healthcare outcomes research. We first describe current standards to rigorously learn an estimator, which is an algorithm developed through machine learning to predict a particular outcome. We include steps for data preparation, estimator family selection, parameter learning, regularization, and evaluation. We then compare 3 of the most common machine learning methods: (1) decision tree methods that can be useful for identifying how different subpopulations experience different risks for an outcome; (2) deep learning methods that can identify complex nonlinear patterns or interactions between variables predictive of an outcome; and (3) ensemble methods that can improve predictive performance by combining multiple machine learning methods.              Results:                    We demonstrate the application of common machine methods to a simulated insurance claims dataset. We specifically include statistical code in R and Python for the development and evaluation of estimators for predicting which patients are at heightened risk for hospitalization from ambulatory care-sensitive conditions.              Conclusions:                    Outcomes researchers should be aware of key standards for rigorously evaluating an estimator developed through machine learning approaches. Although multiple methods use machine learning concepts, different approaches are best suited for different research problems.",2019-07-01,14,1849,48,677
899,31270976,Reproducibility and Generalizability in Radiomics Modeling: Possible Strategies in Radiologic and Statistical Perspectives,"Radiomics, which involves the use of high-dimensional quantitative imaging features for predictive purposes, is a powerful tool for developing and testing medical hypotheses. Radiologic and statistical challenges in radiomics include those related to the reproducibility of imaging data, control of overfitting due to high dimensionality, and the generalizability of modeling. The aims of this review article are to clarify the distinctions between radiomics features and other omics and imaging data, to describe the challenges and potential strategies in reproducibility and feature selection, and to reveal the epidemiological background of modeling, thereby facilitating and promoting more reproducible and generalizable radiomics research.",2019-07-01,36,744,122,677
1036,31510695,scOrange-a tool for hands-on training of concepts from single-cell data analytics,"Motivation:                    Single-cell RNA sequencing allows us to simultaneously profile the transcriptomes of thousands of cells and to indulge in exploring cell diversity, development and discovery of new molecular mechanisms. Analysis of scRNA data involves a combination of non-trivial steps from statistics, data visualization, bioinformatics and machine learning. Training molecular biologists in single-cell data analysis and empowering them to review and analyze their data can be challenging, both because of the complexity of the methods and the steep learning curve.              Results:                    We propose a workshop-style training in single-cell data analytics that relies on an explorative data analysis toolbox and a hands-on teaching style. The training relies on scOrange, a newly developed extension of a data mining framework that features workflow design through visual programming and interactive visualizations. Workshops with scOrange can proceed much faster than similar training methods that rely on computer programming and analysis through scripting in R or Python, allowing the trainer to cover more ground in the same time-frame. We here review the design principles of the scOrange toolbox that support such workshops and propose a syllabus for the course. We also provide examples of data analysis workflows that instructors can use during the training.              Availability and implementation:                    scOrange is an open-source software. The software, documentation and an emerging set of educational videos are available at http://singlecell.biolab.si.",2019-07-01,2,1619,81,677
2407,30846346,Multimodal wrist-worn devices for seizure detection and advancing research: Focus on the Empatica wristbands,"Wearable automated seizure detection devices offer a high potential to improve seizure management, through continuous ambulatory monitoring, accurate seizure counts, and real-time alerts for prompt intervention. More importantly, these devices can be a life-saving help for people with a higher risk of sudden unexpected death in epilepsy (SUDEP), especially in case of generalized tonic-clonic seizures (GTCS). The Embrace and E4 wristbands (Empatica) are the first commercially available multimodal wristbands that were designed to sense the physiological hallmarks of ongoing GTCS: while Embrace only embeds a machine learning-based detection algorithm, both E4 and Embrace devices are equipped with motion (accelerometers, ACC) and electrodermal activity (EDA) sensors and both the devices received medical clearance (E4 from EU CE, Embrace from EU CE and US FDA). The aim of this contribution is to provide updated evidence of the effectiveness of GTCS detection and monitoring relying on the combination of ACM and EDA sensors. A machine learning algorithm able to recognize ACC and EDA signatures of GTCS-like events has been developed on E4 data, labeled using gold-standard video-EEG examined by epileptologists in clinical centers, and has undergone continuous improvement. While keeping an elevated sensitivity to GTCS (92-100%), algorithm improvements and growing data availability led to lower false alarm rate (FAR) from the initial 2 down to 0.2-1 false alarms per day, as showed by retrospective and prospective analyses in inpatient settings. Algorithm adjustment to better discriminate real-life physical activities from GTCS, has brought the initial FAR of 6 on outpatient real life settings, down to values comparable to best-case clinical settings (FAR < 0.5), with comparable sensitivity. Moreover, using multimodal sensing, it has been possible not only to detect GTCS but also to quantify seizure-induced autonomic dysfunction, based on automatic features of abnormal motion and EDA. The latter biosignal correlates with the duration of post-ictal generalized EEG suppression, a biomarker observed in 100% of monitored SUDEP cases.",2019-07-01,10,2157,108,677
2398,30866728,Review of Medical Decision Support and Machine-Learning Methods,"Machine-learning methods can assist with the medical decision-making processes at the both the clinical and diagnostic levels. In this article, we first review historical milestones and specific applications of computer-based medical decision support tools in both veterinary and human medicine. Next, we take a mechanistic look at 3 archetypal learning algorithms-naive Bayes, decision trees, and neural network-commonly used to power these medical decision support tools. Last, we focus our discussion on the data sets used to train these algorithms and examine methods for validation, data representation, transformation, and feature selection. From this review, the reader should gain some appreciation for how these decision support tools have and can be used in medicine along with insight on their inner workings.",2019-07-01,4,820,63,677
900,31270636,Current Approaches to the Use of Artificial Intelligence for Injury Risk Assessment and Performance Prediction in Team Sports: a Systematic Review,"Background:                    The application of artificial intelligence (AI) opens an interesting perspective for predicting injury risk and performance in team sports. A better understanding of the techniques of AI employed and of the sports that are using AI is clearly warranted. The purpose of this study is to identify which AI approaches have been applied to investigate sport performance and injury risk and to find out which AI techniques each sport has been using.              Methods:                    Systematic searches through the PubMed, Scopus, and Web of Science online databases were conducted for articles reporting AI techniques or methods applied to team sports athletes.              Results:                    Fifty-eight studies were included in the review with 11 AI techniques or methods being applied in 12 team sports. Pooled sample consisted of 6456 participants (97% male, 25  8 years old; 3% female, 21  10 years old) with 76% of them being professional athletes. The AI techniques or methods most frequently used were artificial neural networks, decision tree classifier, support vector machine, and Markov process with good performance metrics for all of them. Soccer, basketball, handball, and volleyball were the team sports with more applications of AI.              Conclusions:                    The results of this review suggest a prevalent application of AI methods in team sports based on the number of published studies. The current state of development in the area proposes a promising future with regard to AI use in team sports. Further evaluation research based on prospective methods is warranted to establish the predictive performance of specific AI techniques and methods.",2019-07-01,10,1731,146,677
903,33178948,Balancing accuracy and interpretability of machine learning approaches for radiation treatment outcomes modeling,"Radiation outcomes prediction (ROP) plays an important role in personalized prescription and adaptive radiotherapy. A clinical decision may not only depend on an accurate radiation outcomes' prediction, but also needs to be made based on an informed understanding of the relationship among patients' characteristics, radiation response and treatment plans. As more patients' biophysical information become available, machine learning (ML) techniques will have a great potential for improving ROP. Creating explainable ML methods is an ultimate task for clinical practice but remains a challenging one. Towards complete explainability, the interpretability of ML approaches needs to be first explored. Hence, this review focuses on the application of ML techniques for clinical adoption in radiation oncology by balancing accuracy with interpretability of the predictive model of interest. An ML algorithm can be generally classified into an interpretable (IP) or non-interpretable (NIP) (""black box"") technique. While the former may provide a clearer explanation to aid clinical decision-making, its prediction performance is generally outperformed by the latter. Therefore, great efforts and resources have been dedicated towards balancing the accuracy and the interpretability of ML approaches in ROP, but more still needs to be done. In this review, current progress to increase the accuracy for IP ML approaches is introduced, and major trends to improve the interpretability and alleviate the ""black box"" stigma of ML in radiation outcomes modeling are summarized. Efforts to integrate IP and NIP ML approaches to produce predictive models with higher accuracy and interpretability for ROP are also discussed.",2019-07-01,2,1714,112,677
904,31262850,To Combine or Not Combine: Drug Interactions and Tools for Their Analysis. Reflections from the EORTC-PAMM Course on Preclinical and Early-phase Clinical Pharmacology,"Combination therapies are used in the clinic to achieve cure, better efficacy and to circumvent resistant disease in patients. Initial assessment of the effect of such combinations, usually of two agents, is frequently performed using in vitro assays. In this review, we give a short summary of the types of analyses that were presented during the Preclinical and Early-phase Clinical Pharmacology Course of the Pharmacology and Molecular Mechanisms Group, European Organization for Research and Treatment on Cancer, that can be used to determine the efficacy of drug combinations. The effect of a combination treatment can be calculated using mathematical equations based on either the Loewe additivity or Bliss independence model, or a combination of both, such as Chou and Talalay's median-drug effect model. Interactions can be additive, synergistic (more than additive), or antagonistic (less than additive). Software packages CalcuSyn (also available as CompuSyn) and Combenefit are designed to calculate the extent of the combined effects. Interestingly, the application of machine-learning methods in the prediction of combination treatments, which can include pharmacogenomic, genetic, metabolomic and proteomic profiles, might contribute to further refinement of combination regimens. However, more research is needed to apply appropriate rules of machine learning methods to ensure correct predictive models.",2019-07-01,1,1419,166,677
909,31257314,Utilization of Artificial Intelligence in Echocardiography,"Echocardiography has a central role in the diagnosis and management of cardiovascular disease. Precise and reliable echocardiographic assessment is required for clinical decision-making. Even if the development of new technologies (3-dimentional echocardiography, speckle-tracking, semi-automated analysis, etc.), the final decision on analysis is strongly dependent on operator experience. Diagnostic errors are a major unresolved problem. Moreover, not only can cardiologists differ from one another in image interpretation, but also the same observer may come to different findings when a reading is repeated. Daily high workloads in clinical practice may lead to this error, and all cardiologists require precise perception in this field. Artificial intelligence (AI) has the potential to improve analysis and interpretation of medical images to a new stage compared with previous algorithms. From our comprehensive review, we believe AI has the potential to improve accuracy of diagnosis, clinical management, and patient care. Although there are several concerns about the required large dataset and ""black box"" algorithm, AI can provide satisfactory results in this field. In the future, it will be necessary for cardiologists to adapt their daily practice to incorporate AI in this new stage of echocardiography.",2019-07-01,4,1320,58,677
917,31246909,A review of early warning systems for prompt detection of patients at risk for clinical decline,"Early Warning Scores (EWS) are a composite evaluation of a patient's basic physiology, changes of which are the first indicators of clinical decline and are used to prompt further patient assessment and when indicated intervention. These are sometimes referred to as ""track and triggers systems"" with tracking meant to denote periodic observation of physiology and trigger being a predetermined response criteria. This review article examines the most widely used EWS, with special attention paid to those used in military and trauma populations.The earliest EWS is the Modified Early Earning Score (MEWS). In MEWS, points are allocated to vital signs based on their degree of abnormality, and summed to yield an aggregate score. A score above a threshold would elicit a clinical response such as a rapid response team. Modified Early Earning Score was subsequently followed up with the United Kingdom's National Early Warning Score, the electronic cardiac arrest triage score, and the 10 Signs of Vitality score, among others.Severity of illness indicators have been in military and civilian trauma populations, such as the Revised Trauma Score, Injury Severity Score, and Trauma and Injury Severity. The sequential organ failure assessment score and its attenuated version quick sequential organ failure assessment were developed to aggressively identify patients near septic shock.Effective EWS have certain characteristics. First, they should accurately capture vital signs information. Second, almost all data should be derived electronically rather than manually. Third, the measurements should take into consideration multiple organ systems. Finally, information that goes into an EWS must be captured in a timely manner. Future trends include the use of machine learning to detect subtle changes in physiology and the inclusion of data from biomarkers. As EWS improve, they will be more broadly used in both military and civilian environments. LEVEL OF EVIDENCE: Review article, level I.",2019-07-01,5,1995,95,677
919,31243141,From observing to predicting single-cell structure and function with high-throughput/high-content microscopy,"In the past 15 years, cell-based microscopy has evolved its focus from observing cell function to aiming to predict it. In particular-powered by breakthroughs in computer vision, large-scale image analysis and machine learning-high-throughput and high-content microscopy imaging have enabled to uniquely harness single-cell information to systematically discover and annotate genes and regulatory pathways, uncover systems-level interactions and causal links between cellular processes, and begin to clarify and predict causal cellular behaviour and decision making. Here we review these developments, discuss emerging trends in the field, and describe how single-cell 'omics and single-cell microscopy are imminently in an intersecting trajectory. The marriage of these two fields will make possible an unprecedented understanding of cell and tissue behaviour and function.",2019-07-01,5,874,108,677
947,31945959,Outlier Detection in Health Record Free-Text using Deep Learning,"In recent years, machine learning approaches have been successfully applied to analysis of patient symptom data in the context of disease diagnosis, at least where such data is well codified. However, much of the data present in Electronic Health Records (EHR) is unlikely to prove suitable for classic machine learning approaches. In particular, the use of free (or unstructured) text for clinical notes presents significant analytical opportunities, but also unique difficulties. Furthermore, the wide dispersal of health data relating to individuals necessitates the development of decentralized solutions. We provide, in this paper, an overview of our approach to develop a neural network framework for patient classification in the environment of EHRs where data may be heterogeneous, incomplete (containing missing values), and noisy. In this paper we describe our system which provides prediction of outlier cases which are likely to relate to frequent attender patients, which acheives an Area-Under-the-Curve score of up to 0.92.",2019-07-01,0,1038,64,677
2378,29377981,Computational prediction of drug-target interactions using chemogenomic approaches: an empirical survey,"Computational prediction of drug-target interactions (DTIs) has become an essential task in the drug discovery process. It narrows down the search space for interactions by suggesting potential interaction candidates for validation via wet-lab experiments that are well known to be expensive and time-consuming. In this article, we aim to provide a comprehensive overview and empirical evaluation on the computational DTI prediction techniques, to act as a guide and reference for our fellow researchers. Specifically, we first describe the data used in such computational DTI prediction efforts. We then categorize and elaborate the state-of-the-art methods for predicting DTIs. Next, an empirical comparison is performed to demonstrate the prediction performance of some representative methods under different scenarios. We also present interesting findings from our evaluation study, discussing the advantages and disadvantages of each method. Finally, we highlight potential avenues for further enhancement of DTI prediction performance as well as related research directions.",2019-07-01,28,1080,103,677
2447,30759376,Potential biomarkers for persistent and neuropathic pain therapy,"Persistent, in particular neuropathic pain affects millions of people worldwide. However, the response rate of patients to existing analgesic drugs is less than 50%. There are several possibilities to increase this response rate, such as optimization of the pharmacokinetic and pharmacodynamic properties of analgesics. Another promising approach is to use prognostic biomarkers in patients to determine the optimal pharmacological therapy for each individual. Here, we discuss recent efforts to identify plasma and CSF biomarkers, as well as genetic biomarkers and sensory testing, and how these readouts could be exploited for the prediction of a suitable pharmacological treatment. Collectively, the information on single biomarkers may be stored in knowledge bases and processed by machine-learning and related artificial intelligence techniques, resulting in the optimal pharmacological treatment for individual pain patients. We highlight the potential for biomarker-based individualized pain therapies and discuss biomarker reliability and their utility in clinical practice, as well as limitations of this approach.",2019-07-01,3,1123,64,677
848,31353422,Artificial Intelligence in Reproductive Urology,"Purpose of review:                    The promise of artificial intelligence (AI) in medicine has been widely theorized over the past couple of decades. It has only been with technological advances over the past few years that physicians and computer scientists have started discovering its true clinical potential. Reproductive urology is a sub-discipline that AI could be of great contribution, as current predictive models and subjectivity within the field have several limitations. We review the literature to summarize recent AI applications in reproductive urology.              Recent findings:                    Early AI applications in reproductive urology focused on predicting semen parameters based on questionnaires that identify potential environmental factors and/or lifestyle habits impacting male fertility. AI has shown success in predicting the patient subpopulation most likely to need a genetic workup for azoospermia. With recent advances in image processing, automated sperm detection is a reality. Semen analyses, once a laboratory-only diagnostic test, have moved into health consumer homes with the advent of AI. AI's prospects in medicine are considerable and there is strong potential for AI within reproductive urology. Research in identifying the factors that can affect reproductive success either naturally or with assisted reproduction is of paramount importance to move the field forward.",2019-07-01,0,1423,47,677
858,31344143,Bone age assessment with various machine learning techniques: A systematic literature review and meta-analysis,"Background:                    The assessment of bone age and skeletal maturity and its comparison to chronological age is an important task in the medical environment for the diagnosis of pediatric endocrinology, orthodontics and orthopedic disorders, and legal environment in what concerns if an individual is a minor or not when there is a lack of documents. Being a time-consuming activity that can be prone to inter- and intra-rater variability, the use of methods which can automate it, like Machine Learning techniques, is of value.              Objective:                    The goal of this paper is to present the state of the art evidence, trends and gaps in the research related to bone age assessment studies that make use of Machine Learning techniques.              Method:                    A systematic literature review was carried out, starting with the writing of the protocol, followed by searches on three databases: Pubmed, Scopus and Web of Science to identify the relevant evidence related to bone age assessment using Machine Learning techniques. One round of backward snowballing was performed to find additional studies. A quality assessment was performed on the selected studies to check for bias and low quality studies, which were removed. Data was extracted from the included studies to build summary tables. Lastly, a meta-analysis was performed on the performances of the selected studies.              Results:                    26 studies constituted the final set of included studies. Most of them proposed automatic systems for bone age assessment and investigated methods for bone age assessment based on hand and wrist radiographs. The samples used in the studies were mostly comprehensive or bordered the age of 18, and the data origin was in most of cases from United States and West Europe. Few studies explored ethnic differences.              Conclusions:                    There is a clear focus of the research on bone age assessment methods based on radiographs whilst other types of medical imaging without radiation exposure (e.g. magnetic resonance imaging) are not much explored in the literature. Also, socioeconomic and other aspects that could influence in bone age were not addressed in the literature. Finally, studies that make use of more than one region of interest for bone age assessment are scarce.",2019-07-01,6,2364,110,677
841,31366110,Antibiotic-Resistant Septicemia in Pediatric Oncology Patients Associated with Post-Therapeutic Neutropenic Fever,"Death in cancer patients can be caused by the progression of tumors, their malignity, or other associated conditions such as sepsis, which is a multiphasic host response to a pathogen that can be significantly amplified by endogenous factors. Its incidence is continuously rising, which reflects the increasing number of sick patients at a higher risk of infection, especially those that are elderly, pediatric, or immunosuppressed. Sepsis appears to be directly associated with oncological treatment and fatal septic shock. Patients with a cancer diagnosis face a much higher risk of infections after being immunosuppressed by chemotherapy, radiotherapy, or anti-inflammatory therapy, especially caused by non-pathogenic, Gram-negative, and multidrug-resistant pathogens. There is a notorious difference between the incidence and mortality rates related to sepsis in pediatric oncologic patients between developed and developing countries: they are much higher in developing countries, where investment for diagnosis and treatment resources, infrastructure, medical specialists, cancer-related control programs, and post-therapeutic care is insufficient. This situation not only limits but also reduces the life expectancy of treated pediatric oncologic patients, and demands higher costs from the healthcare systems. Therefore, efforts must aim to limit the progression of sepsis conditions, applying the most recommended therapeutic regimens as soon as the initial risk factors are clinically evident-or even before they are, as when taking advantage of machine learning prediction systems to analyze data.",2019-07-01,0,1609,113,677
2227,31159603,Towards Patient-centered Diagnosis of Pediatric Obstructive Sleep Apnea-A Review of Biomedical Engineering Strategies,"Introduction:                    Obstructive sleep apnea in children has a prevalence of 5%. Polysomnography is considered to be the gold standard for diagnosis and stratification of the condition. However, it is resource-intensive, expensive and uncomfortable for children and their families.              Areas covered:                    We focus this review on technical developments in sensor technology, materials and predictive analytics for translation to (i) patient comfort and compliance in the laboratory and (ii) validation of home sleep apnea testing in children. Key developments in adult polysomnography that could be considered for adoption in children are also highlighted. This review is organized by Sleep, Cardiovascular, Oximetry, Position, Effort, and Respiratory (SCOPER) parameters of interest.              Expert opinion:                    In the past decade, improvements in respiratory sensors and signal processing strategies have transitioned sleep apnea testing in adults from the laboratory to home, thus reducing costs and improving access. Unfortunately, such benefits have not been observed for children principally due to the lack of high-quality studies. The increasing cost of diagnosis of sleep apnea in children needs urgent attention. Recent technical developments as described in this review have the potential to support further evaluation of home sleep apnea testing while improving the current circumstances of in-lab polysomnography for children.",2019-07-01,1,1494,117,677
2231,31153774,The Heterogeneity Problem: Approaches to Identify Psychiatric Subtypes,"The imprecise nature of psychiatric nosology restricts progress towards characterizing and treating mental health disorders. One issue is the 'heterogeneity problem': different causal mechanisms may relate to the same disorder, and multiple outcomes of interest can occur within one individual. Our review tackles this heterogeneity problem, providing considerations, concepts, and approaches for investigators examining human cognition and mental health. We highlight the difficulty of pure dimensional approaches due to 'the curse of dimensionality'. Computationally, we consider supervised and unsupervised statistical approaches to identify putative subtypes within a population. However, we emphasize that subtype identification should be linked to a particular outcome or question. We conclude with novel hybrid approaches that can identify subtypes tied to outcomes, and may help advance precision diagnostic and treatment tools.",2019-07-01,30,936,70,677
2232,31153580,Diagnostic performance of fractional flow reserve derived from coronary CT angiography for detection of lesion-specific ischemia: A multi-center study and meta-analysis,"Purpose:                    To evaluate the diagnostic performance of coronary computed tomography angiography derived fractional flow reserve (CT-FFR) with invasive fractional flow reserve (FFR) in patients with coronary artery disease"" before ""with invasive fractional flow reserve serving as the reference standard.              Materials and methods:                    CT-FFR values based on a machine learning algorithm (cFFRML) in 183 vessels of 136 patients from four centers were measured with invasive FFR as reference standard. The diagnostic performance from our multicenter study was combined into a meta-analysis following a literature search in Web of Science, PubMed, Cochrane library to identify studies comparing diagnostic performance of coronary computed tomography angiography (CCTA) and CT-FFR. Sensitivity, specificity, accuracy were analyzed on both per-vessel and per-patient basis for intermediate lesions and by algorithm.              Results:                    Our multicenter study demonstrated sensitivities, specificities, and accuracies of cFFRML and CCTA of 0.85, 0.94, 0.90, and 0.95, 0.28, 0.55 on a per-vessel basis, respectively. For our meta-analysis, pooled sensitivities, specificities, and accuracies of CT-FFR and CCTA were 0.85, 0.82, 0.82, and 0.85, 0.57, 0.65 with AUC of 0.86 (95%CI: 0.830.89) and 0.83 (95%CI: 0.790.86) on a per-vessel basis, respectively. The sensitivity, specificity and accuracy for intermediate lesions using cFFRML were 0.84, 0.92, and 0.89. No significant difference was found among different algorithms of CT-FFR (P < 0.001).              Conslusion:                    This multicenter study with meta-analysis showed that CT-FFR had a high diagnostic accuracy in determining ischemia-specific lesions and intermediate lesions. There was no significant difference when comparing the combined diagnostic performance of different algorithms of CT-FFR with invasive FFR as the reference standard.",2019-07-01,2,1969,168,677
430,30366739,The unreasonable effectiveness of small neural ensembles in high-dimensional brain,"Complexity is an indisputable, well-known, and broadly accepted feature of the brain. Despite the apparently obvious and widely-spread consensus on the brain complexity, sprouts of the single neuron revolution emerged in neuroscience in the 1970s. They brought many unexpected discoveries, including grandmother or concept cells and sparse coding of information in the brain. In machine learning for a long time, the famous curse of dimensionality seemed to be an unsolvable problem. Nevertheless, the idea of the blessing of dimensionality becomes gradually more and more popular. Ensembles of non-interacting or weakly interacting simple units prove to be an effective tool for solving essentially multidimensional and apparently incomprehensible problems. This approach is especially useful for one-shot (non-iterative) correction of errors in large legacy artificial intelligence systems and when the complete re-training is impossible or too expensive. These simplicity revolutions in the era of complexity have deep fundamental reasons grounded in geometry of multidimensional data spaces. To explore and understand these reasons we revisit the background ideas of statistical physics. In the course of the 20th century they were developed into the concentration of measure theory. The Gibbs equivalence of ensembles with further generalizations shows that the data in high-dimensional spaces are concentrated near shells of smaller dimension. New stochastic separation theorems reveal the fine structure of the data clouds. We review and analyse biological, physical, and mathematical problems at the core of the fundamental question: how can high-dimensional brain organise reliable and fast learning in high-dimensional world of data by simple tools? To meet this challenge, we outline and setup a framework based on statistical physics of data. Two critical applications are reviewed to exemplify the approach: one-shot correction of errors in intellectual systems and emergence of static and associative memories in ensembles of single neurons. Error correctors should be simple; not damage the existing skills of the system; allow fast non-iterative learning and correction of new mistakes without destroying the previous fixes. All these demands can be satisfied by new tools based on the concentration of measure phenomena and stochastic separation theory. We show how a simple enough functional neuronal model is capable of explaining: i) the extreme selectivity of single neurons to the information content of high-dimensional data, ii) simultaneous separation of several uncorrelated informational items from a large set of stimuli, and iii) dynamic learning of new items by associating them with already ""known"" ones. These results constitute a basis for organisation of complex memories in ensembles of single neurons.",2019-07-01,8,2837,82,677
2255,31116112,The promise and perils of 'Big Data': focus on spondyloarthritis,"Purpose of review:                    This review will describe the available large-scale data sources to study spondyloarthritis (SpA), enumerate approaches to identify SpA and its disease-related manifestations and outcomes, and will outline existing and future methods to collect novel data types [e.g. patient-reported outcomes (PRO), passive data from wearables and biosensors].              Recent findings:                    In addition to traditional clinic visit-based SpA registries, newer data sources, such as health plan claims data, single and multispecialty electronic health record (EHR) based registries, patient registries and linkages between data sources, have catalyzed the breadth and depth of SpA research. Health activity tracker devices and PRO collected via PROMIS instruments have been shown to have good validity when assessed in SpA patients as compared to legacy disease-specific instruments. In certain cases, machine learning outperforms traditional methods to identify SpA and its associated manifestations in EHR and claims data, and may predict disease flare.              Summary:                    Although caution remains in the application of newer data sources and methods including the important need for replication, the availability of new data sources, health tracker devices and analytic methods holds great promise to catalyze SpA research.",2019-07-01,0,1388,64,677
505,31047892,The behavioral phenotype of early life adversity: A 3-level meta-analysis of rodent studies,"Altered cognitive performance is considered an intermediate phenotype mediating early life adversity (ELA) effects on later-life development of mental disorders, e.g. depression. Whereas most human studies are limited to correlational conclusions, rodent studies can prospectively investigate how ELA alters cognitive performance in several domains. Despite the volume of reports, there is no consensus on i) the behavioral domains being affected by ELA and ii) the extent of these effects. To test how ELA (here: aberrant maternal care) affects specific behavioral domains, we used a 3-level mixed-effect meta-analysis, and thoroughly explored heterogeneity with MetaForest, a novel machine-learning approach. Our results are based on >400 independent experiments, involving 8600 animals. Especially in males, ELA promotes memory formation during stressful learning but impairs non-stressful learning. Furthermore, ELA increases anxiety-like and decreases social behavior. The ELA phenotype was strongest when i) combined with other negative experiences (""hits""); ii) in rats; iii) in ELA models of 10days duration. All data is easily accessible with MaBapp (https://osf.io/ra947/), allowing researchers to run tailor-made meta-analyses, thereby revealing the optimal choice of experimental protocols and study power.",2019-07-01,14,1320,91,677
506,31045948,"Artificial intelligence, osteoporosis and fragility fractures","Purpose of review:                    Artificial intelligence tools have found new applications in medical diagnosis. These tools have the potential to capture underlying trends and patterns, otherwise impossible with previous modeling capabilities. Machine learning and deep learning models have found a role in osteoporosis, both to model the risk of fragility fracture, and to help with the identification and segmentation of images.              Recent findings:                    Here we survey the latest research in the artificial intelligence application to the prediction of osteoporosis that has been published between January 2017 and March 2019. Around half of the articles that are covered here predict (by classification or regression) an indicator of osteoporosis, such as bone mass or fragility fractures; the other half of studies use tools for automatic segmentation of the images of patients with or at risk of osteoporosis. The data for these studies include diverse signal sources: acoustics, MRI, CT, and of course, X-rays.              Summary:                    New methods for automatic image segmentation, and prediction of fracture risk show promising clinical value. Though these recent developments have had a successful initial application to osteoporosis research, their development is still under improvement, such as accounting for positive/negative class bias. We urge care when reporting accuracy metrics, and when comparing such metrics between different studies.",2019-07-01,6,1501,61,677
518,31033729,Systems serology for decoding infection and vaccine-induced antibody responses to HIV-1,"Purpose of review:                    Experimental and analytical advances have enabled systematic, high-resolution studies of humoral immune responses, and are beginning to define mechanisms of immunity to HIV.              Recent findings:                    High-throughput, information-rich experimental and analytical methods, whether genomic, proteomic, or transcriptomic, have firmly established their value across a diversity of fields. Consideration of these tools as trawlers in 'fishing expeditions' has faded as 'data-driven discovery' has come to be valued as an irreplaceable means to develop fundamental understanding of biological systems. Collectively, studies of HIV-1 infection and vaccination including functional, biophysical, and biochemical humoral profiling approaches have provided insights into the phenotypic characteristics of individual and pools of antibodies. Relating these measures to clinical status, protection/efficacy outcomes, and cellular profiling data using machine learning has offered the possibility of identifying unanticipated mechanisms of action and gaining insights into fundamental immunological processes that might otherwise be difficult to decipher.              Summary:                    Recent evidence establishes that systematic data collection and application of machine learning approaches can identify humoral immune correlates that are generalizable across distinct HIV-1 immunogens and vaccine regimens and translatable between model organisms and the clinic. These outcomes provide a strong rationale supporting the utility and further expansion of these approaches both in support of vaccine development and more broadly in defining mechanisms of immunity.",2019-07-01,2,1722,87,677
519,31033569,Application of machine learning in the diagnosis of axial spondyloarthritis,"Purpose of review:                    In this review article, we describe the development and application of machine-learning models in the field of rheumatology to improve the detection and diagnosis rates of underdiagnosed rheumatologic conditions, such as ankylosing spondylitis and axial spondyloarthritis (axSpA).              Recent findings:                    In an attempt to aid in the earlier diagnosis of axSpA, we developed machine-learning models to predict a diagnosis of ankylosing spondylitis and axSpA using administrative claims and electronic medical record data. Machine-learning algorithms based on medical claims data predicted the diagnosis of ankylosing spondylitis better than a model developed based on clinical characteristics of ankylosing spondylitis. With additional clinical data, machine-learning algorithms developed using electronic medical records identified patients with axSpA with 82.6-91.8% accuracy. These two algorithms have helped us understand potential opportunities and challenges associated with each data set and with different analytic approaches. Efforts to refine and validate these machine-learning models are ongoing.              Summary:                    We discuss the challenges and benefits of machine-learning models in healthcare, along with potential opportunities for its application in the field of rheumatology, particularly in the early diagnosis of axSpA and ankylosing spondylitis.",2019-07-01,2,1450,75,677
524,31022391,"Data science, artificial intelligence, and machine learning: Opportunities for laboratory medicine and the value of positive regulation","Artificial intelligence (AI) and data science are rapidly developing in healthcare, as is their translation into laboratory medicine. Our review article presents an overview of the data science domain while discussing the reasons for its emergence. We also present several perspectives of its applications in clinical laboratories, along with potential ethical challenges related to AI and data science.",2019-07-01,11,403,135,677
547,30982079,Computational algorithms for in silico profiling of activating mutations in cancer,"Methods to catalog and computationally assess the mutational landscape of proteins in human cancers are desirable. One approach is to adapt evolutionary or data-driven methods developed for predicting whether a single-nucleotide polymorphism (SNP) is deleterious to protein structure and function. In cases where understanding the mechanism of protein activation and regulation is desired, an alternative approach is to employ structure-based computational approaches to predict the effects of point mutations. Through a case study of mutations in kinase domains of three proteins, namely, the anaplastic lymphoma kinase (ALK) in pediatric neuroblastoma patients, serine/threonine-protein kinase B-Raf (BRAF) in melanoma patients, and erythroblastic oncogene B 2 (ErbB2 or HER2) in breast cancer patients, we compare the two approaches above. We find that the structure-based method is most appropriate for developing a binary classification of several different mutations, especially infrequently occurring ones, concerning the activation status of the given target protein. This approach is especially useful if the effects of mutations on the interactions of inhibitors with the target proteins are being sought. However, many patients will present with mutations spread across different target proteins, making structure-based models computationally demanding to implement and execute. In this situation, data-driven methods-including those based on machine learning techniques and evolutionary methods-are most appropriate for recognizing and illuminate mutational patterns. We show, however, that, in the present status of the field, the two methods have very different accuracies and confidence values, and hence, the optimal choice of their deployment is context-dependent.",2019-07-01,2,1781,82,677
549,30981085,Recent advances in physical reservoir computing: A review,"Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.",2019-07-01,25,1302,57,677
554,30972433,From biophysics to 'omics and systems biology,"Recent decades brought a revolution to biology, driven mainly by exponentially increasing amounts of data coming from ""'omics"" sciences. To handle these data, bioinformatics often has to combine biologically heterogeneous signals, for which methods from statistics and engineering (e.g. machine learning) are often used. While such an approach is sometimes necessary, it effectively treats the underlying biological processes as a black box. Similarly, systems biology deals with inherently complex systems, characterized by a large number of degrees of freedom, and interactions that are highly non-linear. To deal with this complexity, the underlying physical interactions are often (over)simplified, such as in Boolean modelling of network dynamics. In this review, we argue for the utility of applying a biophysical approach in bioinformatics and systems biology, including discussion of two examples from our research which address sequence analysis and understanding intracellular gene expression dynamics.",2019-07-01,2,1012,45,677
846,31354718,Computational Methodologies for the in vitro and in situ Quantification of Neutrophil Extracellular Traps,"Neutrophil extracellular traps (NETs) are a neutrophil defensive mechanism where chromatin is expelled together with antimicrobial proteins in response to a number of stimuli. Even though beneficial in many cases, their dysfunction has been implicated in many diseases, such as rheumatoid arthritis and cancer. Accurate quantification of NETs is of utmost importance for correctly studying their role in various diseases, especially when considering them as therapeutic targets. Unfortunately, NET quantification has a number of limitations. However, recent developments in computational methodologies for quantifying NETs have vastly improved the ability to study NETs. Methods range from using ImageJ to user friendly applications and to more sophisticated machine-learning approaches. These various methods are reviewed and discussed in this review.",2019-07-01,4,852,105,677
2268,31092914,A new era: artificial intelligence and machine learning in prostate cancer,"Artificial intelligence (AI) - the ability of a machine to perform cognitive tasks to achieve a particular goal based on provided data - is revolutionizing and reshaping our health-care systems. The current availability of ever-increasing computational power, highly developed pattern recognition algorithms and advanced image processing software working at very high speeds has led to the emergence of computer-based systems that are trained to perform complex tasks in bioinformatics, medical imaging and medical robotics. Accessibility to 'big data' enables the 'cognitive' computer to scan billions of bits of unstructured information, extract the relevant information and recognize complex patterns with increasing confidence. Computer-based decision-support systems based on machine learning (ML) have the potential to revolutionize medicine by performing complex tasks that are currently assigned to specialists to improve diagnostic accuracy, increase efficiency of throughputs, improve clinical workflow, decrease human resource costs and improve treatment choices. These characteristics could be especially helpful in the management of prostate cancer, with growing applications in diagnostic imaging, surgical interventions, skills training and assessment, digital pathology and genomics. Medicine must adapt to this changing world, and urologists, oncologists, radiologists and pathologists, as high-volume users of imaging and pathology, need to understand this burgeoning science and acknowledge that the development of highly accurate AI-based decision-support applications of ML will require collaboration between data scientists, computer researchers and engineers.",2019-07-01,31,1682,74,677
558,30971806,Deep learning: new computational modelling techniques for genomics,"As a data-driven science, genomics largely utilizes machine learning to capture dependencies in data and derive novel biological hypotheses. However, the ability to extract new insights from the exponentially increasing volume of genomics data requires more expressive machine learning models. By effectively leveraging large data sets, deep learning has transformed fields such as computer vision and natural language processing. Now, it is becoming the method of choice for many genomics modelling tasks, including predicting the impact of genetic variation on gene regulatory mechanisms such as DNA accessibility and splicing.",2019-07-01,96,629,66,677
2277,31076049,Intensive Care Unit Telemedicine: Innovations and Limitations,"Intensive care unit (ICU) telemedicine is an established entity that has the ability to not only improve the effectiveness, efficiency, and safety of critical care, but to also serve as a tool to combat staffing shortages and resource-limited environments. Several areas for future innovation exist within the field, including the use of advanced practice providers, robust inclusion in medical education, and concurrent application of advanced machine learning. The globalization of critical care services will also likely be predominantly delivered by ICU telemedicine. Limitations faced by the field include technical issues, financial concerns, and organizational elements.",2019-07-01,3,677,61,677
567,30950770,Identification of novel analgesics through a drug repurposing strategy,"The identification of new indications for approved or failed drugs is a process called drug repositioning or drug repurposing. The motivation includes overcoming the productivity gap that exists in drug development, which is a high-cost-high-risk process. Repositioning also includes rescuing drugs that have safely entered the market but have failed to demonstrate sufficient efficiency for the initial clinical indication. Considering the high prevalence of chronic pain, the lack of sufficient efficacy and the safety issues of current analgesics, repositioning seems to be an attractive approach. This review presents example of drugs that already have been repositioned and highlights new technologies that are available for the identification of additional compounds to stimulate the curiosity of readers for further exploration.",2019-07-01,0,835,70,677
575,30926431,Quality assurance of computer-aided detection and diagnosis in colonoscopy,"Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field ""deep learning,"" have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice-polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing.",2019-07-01,11,1589,74,677
838,31371969,Toward a personalized therapy for panic disorder: preliminary considerations from a work in progress,"Although several treatment options for panic disorder (PD) are available, the best intervention for each individual patient remains uncertain and the use of a more personalized therapeutic approach in PD is required. In clinical practice, clinicians combine general scientific information and personal experience in the decision-making process to choose a tailored treatment for each patient. In this sense, clinicians already use a somehow personalized medicine strategy. However, the influence of their interpretative personal models may lead to bias related to personal convictions, not sufficiently grounded on scientific evidence. Hence, an effort to give some advice based on the science of personalized medicine could have positive effects on clinicians' decisions. Based on a narrative review of meta-analyses, systematic reviews, and experimental studies, we proposed a first-step attempt of evidence-based personalized therapy for PD. We focused on some phenomenological profiles, encompassing symptoms during/outside panic attacks, related patterns of physiological functions, and some aspects of physical health, which might be worth considering when developing treatment plans for patients with PD. We considered respiratory, cardiac, vestibular, and derealization/depersonalization profiles, with related implications for treatment. Given the extensiveness of the topic, we considered only medications and some somatic interventions. Our proposal should be considered neither exhaustive nor conclusive, as it is meant as a very preliminary step toward a future, robust evidence-based personalized therapy for PD. Clearly much more work is needed to achieve this goal, and recent technological advances, such as wearable devices, big data platforms, and the application of machine learning techniques, may help obtain reliable findings. We believe that combining the efforts of different research groups in this work in progress can lead to largely shared conclusions in the near future.",2019-07-01,3,2000,100,677
2275,31078239,Artificial Intelligence for the Treatment of Lumbar Spondylolisthesis,Multiple registries are currently collecting patient-specific data on lumbar spondylolisthesis including outcomes data. The collection of imaging diagnostics data along with comparative outcomes data following decompression versus decompression and fusion treatments for degenerative spondylolisthesis represents an enormous opportunity for modern machine-learning analytics research.,2019-07-01,1,384,69,677
837,31372505,Artificial intelligence and machine learning in clinical development: a translational perspective,"Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, computing power to identify clinically meaningful patterns in the data using efficient artificial intelligence and machine-learning algorithms, and regulators embracing this change through new collaborations. This perspective summarizes insights, recent developments, and recommendations for infusing actionable computational evidence into clinical development and health care from academy, biotechnology industry, nonprofit foundations, regulators, and technology corporations. Analysis and learning from publically available biomedical and clinical trial data sets, real-world evidence from sensors, and health records by machine-learning architectures are discussed. Strategies for modernizing the clinical development process by integration of AI- and ML-based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration are outlined. We conclude by discussing applications and impact of digital algorithmic evidence to improve medical care for patients.",2019-07-01,28,1172,97,677
2279,31074639,"Social Profiling: A Review, Taxonomy, and Challenges","Social media has taken an important place in the routine life of people. Every single second, users from all over the world are sharing interests, emotions, and other useful information that leads to the generation of huge volumes of user-generated data. Profiling users by extracting attribute information from social media data has been gaining importance with the increasing user-generated content over social media platforms. Meeting the user's satisfaction level for information collection is becoming more challenging and difficult. This is because of too much noise generated, which affects the process of information collection due to explosively increasing online data. Social profiling is an emerging approach to overcome the challenges faced in meeting user's demands by introducing the concept of personalized search while keeping in consideration user profiles generated using social network data. This study reviews and classifies research inferring users social profile attributes from social media data as individual and group profiling. The existing techniques along with utilized data sources, the limitations, and challenges are highlighted. The prominent approaches adopted include Machine Learning, Ontology, and Fuzzy logic. Social media data from Twitter and Facebook have been used by most of the studies to infer the social attributes of users. The studies show that user social attributes, including age, gender, home location, wellness, emotion, opinion, relation, influence, and so on, still need to be explored. This review gives researchers insights of the current state of literature and challenges for inferring user profile attributes using social media data.",2019-07-01,0,1692,52,677
2278,31076048,"Intensive Care Unit Telemedicine in the Era of Big Data, Artificial Intelligence, and Computer Clinical Decision Support Systems","This article examines the history of the telemedicine intensive care unit (tele-ICU), the current state of clinical decision support systems (CDSS) in the tele-ICU, applications of machine learning (ML) algorithms to critical care, and opportunities to integrate ML with tele-ICU CDSS. The enormous quantities of data generated by tele-ICU systems is a major driver in the development of the large, comprehensive, heterogeneous, and granular data sets necessary to develop generalizable ML CDSS algorithms, and deidentification of these data sets expands opportunities for ML CDSS research.",2019-07-01,3,590,128,677
2104,31810541,Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases,"Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.",2019-08-01,0,1059,92,646
2215,31181343,Computational anatomy for multi-organ analysis in medical imaging: A review,"The medical image analysis field has traditionally been focused on the development of organ-, and disease-specific methods. Recently, the interest in the development of more comprehensive computational anatomical models has grown, leading to the creation of multi-organ models. Multi-organ approaches, unlike traditional organ-specific strategies, incorporate inter-organ relations into the model, thus leading to a more accurate representation of the complex human anatomy. Inter-organ relations are not only spatial, but also functional and physiological. Over the years, the strategies proposed to efficiently model multi-organ structures have evolved from the simple global modeling, to more sophisticated approaches such as sequential, hierarchical, or machine learning-based models. In this paper, we present a review of the state of the art on multi-organ analysis and associated computation anatomy methodology. The manuscript follows a methodology-based classification of the different techniques available for the analysis of multi-organs and multi-anatomical structures, from techniques using point distribution models to the most recent deep learning-based approaches. With more than 300 papers included in this review, we reflect on the trends and challenges of the field of computational anatomy, the particularities of each anatomical region, and the potential of multi-organ analysis to increase the impact of medical imaging applications on the future of healthcare.",2019-08-01,3,1483,75,646
2244,31138507,Identification of senescent cells in multipotent mesenchymal stromal cell cultures: Current methods and future directions,"Regardless of their tissue of origin, multipotent mesenchymal stromal cells (MSCs) are commonly expanded in vitro for several population doublings to achieve a sufficient number of cells for therapy. Prolonged MSC expansion has been shown to result in phenotypical, morphological and gene expression changes in MSCs, which ultimately lead to the state of senescence. The presence of senescent cells in therapeutic MSC batches is undesirable because it reduces their viability, differentiation potential and trophic capabilities. Additionally, senescent cells acquire senescence-activated secretory phenotype, which may not only induce apoptosis in the neighboring host cells following MSC transplantation, but also trigger local inflammatory reactions. This review outlines the current and promising new methodologies for the identification of senescent cells in MSC cultures, with a particular emphasis on non-destructive and label-free methodologies. Technologies allowing identification of individual senescent cells, based on new surface markers, offer potential advantage for targeted senescent cell removal using new-generation senolytic agents, and subsequent production of therapeutic MSC batches fully devoid of senescent cells. Methods or a combination of methods that are non-destructive and label-free, for example, involving cell size and spectroscopic measurements, could be the best way forward because they do not modify the cells of interest, thus maximizing the final output of therapeutic-grade MSC cultures. The further incorporation of machine learning methods has also recently shown promise in facilitating, automating and enhancing the analysis of these measured data.",2019-08-01,8,1692,121,646
2105,31809864,Functional Neuroimaging in the New Era of Big Data,"The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.",2019-08-01,2,1208,50,646
2193,31229078,Alternative data mining/machine learning methods for the analytical evaluation of food quality and authenticity - A review,"In recent years, the variety and volume of data acquired by modern analytical instruments in order to conduct a better authentication of food has dramatically increased. Several pattern recognition tools have been developed to deal with the large volume and complexity of available trial data. The most widely used methods are principal component analysis (PCA), partial least squares-discriminant analysis (PLS-DA), soft independent modelling by class analogy (SIMCA), k-nearest neighbours (kNN), parallel factor analysis (PARAFAC), and multivariate curve resolution-alternating least squares (MCR-ALS). Nevertheless, there are alternative data treatment methods, such as support vector machine (SVM), classification and regression tree (CART) and random forest (RF), that show a great potential and more advantages compared to conventional ones. In this paper, we explain the background of these methods and review and discuss the reported studies in which these three methods have been applied in the area of food quality and authenticity. In addition, we clarify the technical terminology used in this particular area of research.",2019-08-01,6,1134,122,646
1751,31579847,Neuroimaging-based biomarkers for pain: state of the field and current directions,"Chronic pain is an endemic problem involving both peripheral and brain pathophysiology. Although biomarkers have revolutionized many areas of medicine, biomarkers for pain have remained controversial and relatively underdeveloped. With the realization that biomarkers can reveal pain-causing mechanisms of disease in brain circuits and in the periphery, this situation is poised to change. In particular, brain pathophysiology may be diagnosable with human brain imaging, particularly when imaging is combined with machine learning techniques designed to identify predictive measures embedded in complex data sets. In this review, we explicate the need for brain-based biomarkers for pain, some of their potential uses, and some of the most popular machine learning approaches that have been brought to bear. Then, we evaluate the current state of pain biomarkers developed with several commonly used methods, including structural magnetic resonance imaging, functional magnetic resonance imaging and electroencephalography. The field is in the early stages of biomarker development, but these complementary methodologies have already produced some encouraging predictive models that must be tested more extensively across laboratories and clinical populations.",2019-08-01,8,1261,81,646
1728,31606116,Deep learning based computer-aided diagnosis systems for diabetic retinopathy: A survey,"Diabetic retinopathy (DR) results in vision loss if not treated early. A computer-aided diagnosis (CAD) system based on retinal fundus images is an efficient and effective method for early DR diagnosis and assisting experts. A computer-aided diagnosis (CAD) system involves various stages like detection, segmentation and classification of lesions in fundus images. Many traditional machine-learning (ML) techniques based on hand-engineered features have been introduced. The recent emergence of deep learning (DL) and its decisive victory over traditional ML methods for various applications motivated the researchers to employ it for DR diagnosis, and many deep-learning-based methods have been introduced. In this paper, we review these methods, highlighting their pros and cons. In addition, we point out the challenges to be addressed in designing and learning about efficient, effective and robust deep-learning algorithms for various problems in DR diagnosis and draw attention to directions for future research.",2019-08-01,5,1019,87,646
1729,31606109,Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry,"Introduction:                    Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.              Methods:                    Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.              Results:                    Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.              Conclusions:                    Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.",2019-08-01,7,1775,104,646
2434,30790780,Early seizure detection for closed loop direct neurostimulation devices in epilepsy,"Current treatment concepts for epilepsy are based on continuous drug delivery or electrical stimulation to prevent the occurrence of seizures, exposing the brain and body to a mostly unneeded risk of adverse effects. To address the infrequent occurrence and short duration of epileptic seizures, intelligent implantable closed-loop devices are needed which are based on a refined analysis of ongoing brain activity with highly specific and fast detection algorithms to allow for timely, ictal interventions. Since the development and FDA approval of a first closed loop neurostimulation device relying on simple threshold-based approaches, machine learning approaches became widely available, probably outperformed in the near future by deep convolutional neural networks, which already showed to be extremely successful in pattern recognition in images and partly in signal analysis. Handcrafted features or rules defined by experts become replaced by systematic feature selection procedures and systematic hyperparameter search approaches. Training of these classifiers augments the need of large databases with intracranial EEG recordings, which is partly given by existing databases but potentially can be replaced by continuously transferring data from implanted devices and their publication for research purposes. Already in early design states, the final target hardware must be taken into account for algorithm development. Size, power consumption and, as a consequence, limited computational resources given by low power microcontrollers, FPGAs and ASICS limit the complexity of feature computation, classifier complexity, and the numbers and complexity of layers of deep neuronal networks. Novel approaches for early seizure detection will be a key module for new generations of closed-loop devices together with improved low power implant hardware and will provide together with more efficient intervention paradigms new treatment options for patients with difficult to treat epilepsy.",2019-08-01,2,1997,83,646
2196,31222375,Machine learning approaches for pathologic diagnosis,"Machine learning techniques, especially deep learning techniques such as convolutional neural networks, have been successfully applied to general image recognitions since their overwhelming performance at the 2012 ImageNet Large Scale Visual Recognition Challenge. Recently, such techniques have also been applied to various medical, including histopathological, images to assist the process of medical diagnosis. In some cases, deep learning-based algorithms have already outperformed experienced pathologists for recognition of histopathological images. However, pathological images differ from general images in some aspects, and thus, machine learning of histopathological images requires specialized learning methods. Moreover, many pathologists are skeptical about the ability of deep learning technology to accurately recognize histopathological images because what the learned neural network recognizes is often indecipherable to humans. In this review, we first introduce various applications incorporating machine learning developed to assist the process of pathologic diagnosis, and then describe machine learning problems related to histopathological image analysis, and review potential ways to solve these problems.",2019-08-01,6,1229,52,646
2456,30730601,Automated De Novo Drug Design: Are We Nearly There Yet?,"Medicinal chemistry and, in particular, drug design have often been perceived as more of an art than a science. The many unknowns of human disease and the sheer complexity of chemical space render decision making in medicinal chemistry exceptionally demanding. Computational models can assist the medicinal chemist in this endeavour. Provided here is an overview of recent examples of automated de novo molecular design, a discussion of the concepts and computational approaches involved, and the daring prediction of some of the possibilities and limitations of drug design using machine intelligence.",2019-08-01,11,602,55,646
2274,31078331,Uncovering Ecological Patterns with Convolutional Neural Networks,"Using remotely sensed imagery to identify biophysical components across landscapes is an important avenue of investigation for ecologists studying ecosystem dynamics. With high-resolution remotely sensed imagery, algorithmic utilization of image context is crucial for accurate identification of biophysical components at large scales. In recent years, convolutional neural networks (CNNs) have become ubiquitous in image processing, and are rapidly becoming more common in ecology. Because the quantity of high-resolution remotely sensed imagery continues to rise, CNNs are increasingly essential tools for large-scale ecosystem analysis. We discuss here the conceptual advantages of CNNs, demonstrate how they can be used by ecologists through distinct examples of their application, and provide a walkthrough of how to use them for ecological applications.",2019-08-01,4,859,65,646
1762,31555327,Current applications of big data and machine learning in cardiology,"Machine learning (ML) is a software solution with the ability of making predictions without prior explicit programming, aiding in the analysis of large amounts of data. These algorithms can be trained through supervised or unsupervised learning. Cardiology is one of the fields of medicine with the highest interest in its applications. They can facilitate every step of patient care, reducing the margin of error and contributing to precision medicine. In particular, ML has been proposed for cardiac imaging applications such as automated computation of scores, differentiation of prognostic phenotypes, quantification of heart function and segmentation of the heart. These tools have also demonstrated the capability of performing early and accurate detection of anomalies in electrocardiographic exams. ML algorithms can also contribute to cardiovascular risk assessment in different settings and perform predictions of cardiovascular events. Another interesting research avenue in this field is represented by genomic assessment of cardiovascular diseases. Therefore, ML could aid in making earlier diagnosis of disease, develop patient-tailored therapies and identify predictive characteristics in different pathologic conditions, leading to precision cardiology.",2019-08-01,4,1269,67,646
1750,31579859,Delineating conditions and subtypes in chronic pain using neuroimaging,"Differentiating subtypes of chronic pain still remains a challenge-both from a subjective and objective point of view. Personalized medicine is the current goal of modern medical care and is limited by the subjective nature of patient self-reporting of symptoms and behavioral evaluation. Physiology-focused techniques such as genome and epigenetic analyses inform the delineation of pain groups; however, except under rare circumstances, they have diluted effects that again, share a common reliance on behavioral evaluation. The application of structural neuroimaging towards distinguishing pain subtypes is a growing field and may inform pain-group classification through the analysis of brain regions showing hypertrophic and atrophic changes in the presence of pain. Analytical techniques such as machine-learning classifiers have the capacity to process large volumes of data and delineate diagnostically relevant information from neuroimaging analysis. The issue of defining a ""brain type"" is an emerging field aimed at interpreting observed brain changes and delineating their clinical identity/significance. In this review, 2 chronic pain conditions (migraine and irritable bowel syndrome) with similar clinical phenotypes are compared in terms of their structural neuroimaging findings. Independent investigations are compared with findings from application of machine-learning algorithms. Findings are discussed in terms of differentiating patient subgroups using neuroimaging data in patients with chronic pain and how they may be applied towards defining a personalized pain signature that helps segregate patient subgroups (eg, migraine with and without aura, with or without nausea; irritable bowel syndrome vs other functional gastrointestinal disorders).",2019-08-01,0,1771,70,646
2258,31112393,Applications and limitations of machine learning in radiation oncology,"Machine learning approaches to problem-solving are growing rapidly within healthcare, and radiation oncology is no exception. With the burgeoning interest in machine learning comes the significant risk of misaligned expectations as to what it can and cannot accomplish. This paper evaluates the role of machine learning and the problems it solves within the context of current clinical challenges in radiation oncology. The role of learning algorithms within the workflow for external beam radiation therapy are surveyed, considering simulation imaging, multimodal fusion, image segmentation, treatment planning, quality assurance, and treatment delivery and adaptation. For each aspect, the clinical challenges faced, the learning algorithms proposed, and the successes and limitations of various approaches are analyzed. It is observed that machine learning has largely thrived on reproducibly mimicking conventional human-driven solutions with more efficiency and consistency. On the other hand, since algorithms are generally trained using expert opinion as ground truth, machine learning is of limited utility where problems or ground truths are not well-defined, or if suitable measures of correctness are not available. As a result, machines may excel at replicating, automating and standardizing human behaviour on manual chores, meanwhile the conceptual clinical challenges relating to definition, evaluation, and judgement remain in the realm of human intelligence and insight.",2019-08-01,9,1487,70,646
1080,31437875,Graft Rejection Prediction Following Kidney Transplantation Using Machine Learning Techniques: A Systematic Review and Meta-Analysis,"Kidney transplantation is recommended for patients with End-Stage Renal Disease (ESRD). However, complications, such as graft rejection are hard to predict due to donor and recipient variability. This study discusses the role of machine learning (ML) in predicting graft rejection following kidney transplantation, by reviewing the available related literature. PubMed, DBLP, and Scopus databases were searched to identify studies that utilized ML methods, in predicting outcome following kidney transplants. Fourteen studies were included. This study reviewed the deployment of ML in 109,317 kidney transplant patients from 14 studies. We extracted five different ML algorithms from reviewed studies. Decision Tree (DT) algorithms revealed slightly higher performance with overall mean Area Under the Curve (AUC) for DT (79.5%  0.06) was higher than Artificial Neural Network (ANN) (78.2%  0.08). For predicting graft rejection, ANN and DT were at the top among ML models that had higher accuracy and AUC.",2019-08-01,0,1008,132,646
864,31326236,Insights into Computational Drug Repurposing for Neurodegenerative Disease,"Computational drug repurposing has the ability to remarkably reduce drug development time and cost in an era where these factors are prohibitively high. Several examples of successful repurposed drugs exist in fields such as oncology, diabetes, leprosy, inflammatory bowel disease, among others, however computational drug repurposing in neurodegenerative disease has presented several unique challenges stemming from the lack of validation methods and difficulty in studying heterogenous diseases of aging. Here, we examine existing approaches to computational drug repurposing, including molecular, clinical, and biophysical methods, and propose data sources and methods to advance computational drug repurposing in neurodegenerative disease using Alzheimer's disease as an example.",2019-08-01,9,784,74,646
840,31366471,Artificial intelligence for microscopy: what you should know,"Artificial Intelligence based on Deep Learning (DL) is opening new horizons in biomedical research and promises to revolutionize the microscopy field. It is now transitioning from the hands of experts in computer sciences to biomedical researchers. Here, we introduce recent developments in DL applied to microscopy, in a manner accessible to non-experts. We give an overview of its concepts, capabilities and limitations, presenting applications in image segmentation, classification and restoration. We discuss how DL shows an outstanding potential to push the limits of microscopy, enhancing resolution, signal and information content in acquired data. Its pitfalls are discussed, along with the future directions expected in this field.",2019-08-01,11,740,60,646
1117,31390781,"Retrotransposons in Plant Genomes: Structure, Identification, and Classification through Bioinformatics and Machine Learning","Transposable elements (TEs) are genomic units able to move within the genome of virtually all organisms. Due to their natural repetitive numbers and their high structural diversity, the identification and classification of TEs remain a challenge in sequenced genomes. Although TEs were initially regarded as ""junk DNA"", it has been demonstrated that they play key roles in chromosome structures, gene expression, and regulation, as well as adaptation and evolution. A highly reliable annotation of these elements is, therefore, crucial to better understand genome functions and their evolution. To date, much bioinformatics software has been developed to address TE detection and classification processes, but many problematic aspects remain, such as the reliability, precision, and speed of the analyses. Machine learning and deep learning are algorithms that can make automatic predictions and decisions in a wide variety of scientific applications. They have been tested in bioinformatics and, more specifically for TEs, classification with encouraging results. In this review, we will discuss important aspects of TEs, such as their structure, importance in the evolution and architecture of the host, and their current classifications and nomenclatures. We will also address current methods and their limitations in identifying and classifying TEs.",2019-08-01,7,1353,124,646
887,31288140,Machine learning-based coronary artery disease diagnosis: A comprehensive review,"Coronary artery disease (CAD) is the most common cardiovascular disease (CVD) and often leads to a heart attack. It annually causes millions of deaths and billions of dollars in financial losses worldwide. Angiography, which is invasive and risky, is the standard procedure for diagnosing CAD. Alternatively, machine learning (ML) techniques have been widely used in the literature as fast, affordable, and noninvasive approaches for CAD detection. The results that have been published on ML-based CAD diagnosis differ substantially in terms of the analyzed datasets, sample sizes, features, location of data collection, performance metrics, and applied ML techniques. Due to these fundamental differences, achievements in the literature cannot be generalized. This paper conducts a comprehensive and multifaceted review of all relevant studies that were published between 1992 and 2019 for ML-based CAD diagnosis. The impacts of various factors, such as dataset characteristics (geographical location, sample size, features, and the stenosis of each coronary artery) and applied ML techniques (feature selection, performance metrics, and method) are investigated in detail. Finally, the important challenges and shortcomings of ML-based CAD diagnosis are discussed.",2019-08-01,4,1266,80,646
577,30917031,Stem cells and extracellular vesicles: biological regulators of physiology and disease,"Many different subpopulations of subcellular extracellular vesicles (EVs) have been described. EVs are released from all cell types and have been shown to regulate normal physiological homeostasis, as well as pathological states by influencing cell proliferation, differentiation, organ homing, injury and recovery, as well as disease progression. In this review, we focus on the bidirectional actions of vesicles from normal and diseased cells on normal or leukemic target cells; and on the leukemic microenvironment as a whole. EVs from human bone marrow mesenchymal stem cells (MSC) can have a healing effect, reversing the malignant phenotype in prostate and colorectal cancer, as well as mitigating radiation damage to marrow. The role of EVs in leukemia and their bimodal cross talk with the encompassing microenvironment remains to be fully characterized. This may provide insight for clinical advances via the application of EVs as potential therapy and the employment of statistical and machine learning models to capture the pleiotropic effects EVs endow to a dynamic microenvironment, possibly allowing for precise therapeutic intervention.",2019-08-01,11,1151,86,646
540,30993807,Measurement of physical activity in clinical practice using accelerometers,"Accelerometers are commonly used in clinical and epidemiological research for more detailed measures of physical activity and to target the limitations of self-report methods. Sensors are attached at the hip, wrist and thigh, and the acceleration data are processed and calibrated in different ways to determine activity intensity, body position and/or activity type. Simple linear modelling can be used to assess activity intensity from hip and thigh data, whilst more advanced machine-learning modelling is to prefer for the wrist. The thigh position is most optimal to assess body position and activity type using machine-learning modelling. Frequency filtering and measurement resolution needs to be considered for correct assessment of activity intensity. Simple physical activity measures and statistical methods are mostly used to investigate relationship with health, but do not take advantage of all information provided by accelerometers and do not consider all components of the physical activity behaviour and their interrelationships. More advanced statistical methods are suggested that analyse patterns of multiple measures of physical activity to demonstrate stronger and more specific relationships with health. However, evaluations of accelerometer methods show considerable measurement errors, especially at individual level, which interferes with their use in clinical research and practice. Therefore, better objective methods are needed with improved data processing and calibration techniques, exploring both simple linear and machine-learning alternatives. Development and implementation of accelerometer methods into clinical research and practice requires interdisciplinary collaboration to cover all aspects contributing to useful and accurate measures of physical activity behaviours related to health.",2019-08-01,9,1830,74,646
1112,31396720,Advanced Imaging Modalities to Monitor for Cardiotoxicity,"Early detection and treatment of cardiotoxicity from cancer therapies is key to preventing a rise in adverse cardiovascular outcomes in cancer patients. Over-diagnosis of cardiotoxicity in this context is however equally hazardous, leading to patients receiving suboptimal cancer treatment, thereby impacting cancer outcomes. Accurate screening therefore depends on the widespread availability of sensitive and reproducible biomarkers of cardiotoxicity, which can clearly discriminate early disease. Blood biomarkers are limited in cardiovascular disease and clinicians generally still use generic screening with ejection fraction, based on historical local expertise and resources. Recently, however, there has been growing recognition that simple measurement of left ventricular ejection fraction using 2D echocardiography may not be optimal for screening: diagnostic accuracy, reproducibility and feasibility are limited. Modern cancer therapies affect many myocardial pathways: inflammatory, fibrotic, metabolic, vascular and myocyte function, meaning that multiple biomarkers may be needed to track myocardial cardiotoxicity. Advanced imaging modalities including cardiovascular magnetic resonance (CMR), computed tomography (CT) and positron emission tomography (PET) add improved sensitivity and insights into the underlying pathophysiology, as well as the ability to screen for other cardiotoxicities including coronary artery, valve and pericardial diseases resulting from cancer treatment. Delivering screening for cardiotoxicity using advanced imaging modalities will however require a significant change in current clinical pathways, with incorporation of machine learning algorithms into imaging analysis fundamental to improving efficiency and precision. In the future, we should aspire to personalized rather than generic screening, based on a patient's individual risk factors and the pathophysiological mechanisms of the cancer treatment they are receiving. We should aspire that progress in cardiooncology is able to track progress in oncology, and to ensure that the current 'one size fits all' approach to screening be obsolete in the very near future.",2019-08-01,4,2172,57,646
879,31308553,Machine-learning-guided directed evolution for protein engineering,"Protein engineering through machine-learning-guided directed evolution enables the optimization of protein functions. Machine-learning approaches predict how sequence maps to function in a data-driven manner without requiring a detailed model of the underlying physics or biological pathways. Such methods accelerate directed evolution by learning from the properties of characterized variants and using that information to select sequences that are likely to exhibit improved properties. Here we introduce the steps required to build machine-learning sequence-function models and to use those models to guide engineering, making recommendations at each stage. This review covers basic concepts relevant to the use of machine learning for protein engineering, as well as the current literature and applications of this engineering paradigm. We illustrate the process with two case studies. Finally, we look to future opportunities for machine learning to enable the discovery of unknown protein functions and uncover the relationship between protein sequence and function.",2019-08-01,42,1072,66,646
916,31248976,How accurate are suicide risk prediction models? Asking the right questions for clinical practice,"Prediction models assist in stratifying and quantifying an individual's risk of developing a particular adverse outcome, and are widely used in cardiovascular and cancer medicine. Whether these approaches are accurate in predicting self-harm and suicide has been questioned. We searched for systematic reviews in the suicide risk assessment field, and identified three recent reviews that have examined current tools and models derived using machine learning approaches. In this clinical review, we present a critical appraisal of these reviews, and highlight three major limitations that are shared between them. First, structured tools are not compared with unstructured assessments routine in clinical practice. Second, they do not sufficiently consider a range of performance measures, including negative predictive value and calibration. Third, the potential role of these models as clinical adjuncts is not taken into consideration. We conclude by presenting the view that the current role of prediction models for self-harm and suicide is currently not known, and discuss some methodological issues and implications of some machine learning and other analytic techniques for clinical utility.",2019-08-01,6,1199,97,646
515,31038007,Predictive analytics and machine learning in stroke and neurovascular medicine,"Advances in predictive analytics and machine learning supported by an ever-increasing wealth of data and processing power are transforming almost every industry. Accuracy and precision of predictive analytics have significantly increased over the past few years and are evolving at an exponential pace. There have been significant breakthroughs in using Predictive Analytics in healthcare where it is held as the foundation of precision medicine. Yet, although the research in the field is expanding with the profuse volume of papers applying machine learning algorithms to medical data, very few have contributed meaningfully to clinical care. This lack of impact stands in stark contrast to the enormous relevance of machine learning to many other industries. Regardless of the status of its current contribution, the field of predictive analytics is expected to fundamentally change the way we diagnose and treat diseases, as well as the conduct of biomedical science research. In this review, we describe the main tools and techniques in predictive analytics and will analyze the trends in application of these techniques over the recent years. We will also provide examples of its application in medicine and more specifically in stroke and neurovascular research and outline current limitations.",2019-08-01,3,1301,78,646
924,31232610,Clinical pharmacology of old age,"Introduction: With the majority of elderly persons consuming multiple drugs, inappropriate drug use is a major issue in geriatric medicine. Areas covered: We reviewed PubMed, Embase, and Cochrane from inception to 1 May 2019 for potentially inappropriate use of medications, polypharmacy, and age-dependent changes in pharmacokinetics and pharmacodynamics. We selected to highlight new aspects that have emerged in recent years: appropriate monitoring of drug adherence and the introduction of Big Data analysis in advancing geriatric pharmacology. Expert opinion: There are major gaps in the pharmacological treatment of the elderly. Most drugs were designed and tested in adults, with no pharmacokinetic and pharmacodynamic data on changes in old age. This void must be corrected through systematic and well-designed research programs. Potentially inappropriate use of medications (PIM) in the elderly is a serious issue in advanced age. Analysis of PIM shows relatively low predictive value in real life medicine. Most physicians continue to prescribe to the elderly medicines which should not be given at all, or not combined. Polypharmacy is a complex issue in old age, and in many cases treating physicians are not conducting critical assessment of the need for numerous medications.",2019-08-01,2,1289,32,646
923,31235303,Artificial Intelligence: Can Information be Transformed into Intelligence in Surgical Education?,"Artificial intelligence (AI) is being rapidly integrated into various medical applications. Although early application of AI has been achieved in image-based, as well as statistical computational models, translation into procedure-based specialties such as surgery may take longer to achieve. A potential application of AI in surgical education is as a teaching coach or mentor that interacts with the used via virtual and/or augmented reality. The question arises as to whether machines will achieve the wisdom and intelligence of human educators.",2019-08-01,2,548,96,646
1071,31453373,Putting the data before the algorithm in big data addressing personalized healthcare,"Technologies leveraging big data, including predictive algorithms and machine learning, are playing an increasingly important role in the delivery of healthcare. However, evidence indicates that such algorithms have the potential to worsen disparities currently intrinsic to the contemporary healthcare system, including racial biases. Blame for these deficiencies has often been placed on the algorithm-but the underlying training data bears greater responsibility for these errors, as biased outputs are inexorably produced by biased inputs. The utility, equity, and generalizability of predictive models depend on population-representative training data with robust feature sets. So while the conventional paradigm of big data is deductive in nature-clinical decision support-a future model harnesses the potential of big data for inductive reasoning. This may be conceptualized as clinical decision questioning, intended to liberate the human predictive process from preconceived lenses in data solicitation and/or interpretation. Efficacy, representativeness and generalizability are all heightened in this schema. Thus, the possible risks of biased big data arising from the inputs themselves must be acknowledged and addressed. Awareness of data deficiencies, structures for data inclusiveness, strategies for data sanitation, and mechanisms for data correction can help realize the potential of big data for a personalized medicine era. Applied deliberately, these considerations could help mitigate risks of perpetuation of health inequity amidst widespread adoption of novel applications of big data.",2019-08-01,12,1610,84,646
440,30339893,Big data and targeted machine learning in action to assist medical decision in the ICU,"Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.",2019-08-01,6,765,86,646
1058,31480359,Review on Smart Gas Sensing Technology,"With the development of the Internet-of-Things (IoT) technology, the applications of gas sensors in the fields of smart homes, wearable devices, and smart mobile terminals have developed by leaps and bounds. In such complex sensing scenarios, the gas sensor shows the defects of cross sensitivity and low selectivity. Therefore, smart gas sensing methods have been proposed to address these issues by adding sensor arrays, signal processing, and machine learning techniques to traditional gas sensing technologies. This review introduces the reader to the overall framework of smart gas sensing technology, including three key points; gas sensor arrays made of different materials, signal processing for drift compensation and feature extraction, and gas pattern recognition including Support Vector Machine (SVM), Artificial Neural Network (ANN), and other techniques. The implementation, evaluation, and comparison of the proposed solutions in each step have been summarized covering most of the relevant recently published studies. This review also highlights the challenges facing smart gas sensing technology represented by repeatability and reusability, circuit integration and miniaturization, and real-time sensing. Besides, the proposed solutions, which show the future directions of smart gas sensing, are explored. Finally, the recommendations for smart gas sensing based on brain-like sensing are provided in this paper.",2019-08-01,21,1432,38,646
1041,31498057,Twin Registries Moving Forward and Meeting the Future: A Review,"Twin registries have developed as a valuable resource for the study of many aspects of disease and society over the years in many different countries. A number of these registries include large numbers of twins with data collected at varying information levels for twin cohorts over the past several decades. More recent expansion of twin datasets has allowed for the collection of genetic data, together with many other levels of 'omic' information along with multiple demographic, physiological, health outcomes and other measures typically used in epidemiologic research. Other twin data sources outside these registries reflect research interests in particular aspects of disease or specific phenotypic assessment. Twin registries have the potential to play a key role in many aspects of the artificial intelligence/machine learning-driven projects of the future and will continue to keep adapting to the changing research landscape.",2019-08-01,1,937,63,646
1116,31392526,Machine learning applications in prostate cancer magnetic resonance imaging,"With this review, we aimed to provide a synopsis of recently proposed applications of machine learning (ML) in radiology focusing on prostate magnetic resonance imaging (MRI). After defining the difference between ML and classical rule-based algorithms and the distinction among supervised, unsupervised and reinforcement learning, we explain the characteristic of deep learning (DL), a particular new type of ML, including its structure mimicking human neural networks and its 'black box' nature. Differences in the pipeline for applying ML and DL to prostate MRI are highlighted. The following potential clinical applications in different settings are outlined, many of them based only on MRI-unenhanced sequences: gland segmentation; assessment of lesion aggressiveness to distinguish between clinically significant and indolent cancers, allowing for active surveillance; cancer detection/diagnosis and localisation (transition versus peripheral zone, use of prostate imaging reporting and data system (PI-RADS) version 2), reading reproducibility, differentiation of cancers from prostatitis benign hyperplasia; local staging and pre-treatment assessment (detection of extraprostatic disease extension, planning of radiation therapy); and prediction of biochemical recurrence. Results are promising, but clinical applicability still requires more robust validation across scanner vendors, field strengths and institutions.",2019-08-01,12,1426,75,646
1106,33324889,Computer-aided imaging analysis in acute ischemic stroke - background and clinical applications,"Tools for medical image analysis have been developed to reduce the time needed to detect abnormalities and to provide more accurate results. Particularly, tools based on artificial intelligence and machine learning techniques have led to significant improvements in medical imaging interpretation in the last decade. Automatic evaluation of acute ischemic stroke in medical imaging is one of the fields that witnessed a major development. Commercially available products so far aim to identify (and quantify) the ischemic core, the ischemic penumbra, the site of arterial occlusion and the collateral flow but they are not (yet) intended as standalone diagnostic tools. Their use can be complementary; they are intended to support physicians' interpretation of medical images and hence standardise selection of patients for acute treatment. This review provides an introduction into the field of computer-aided diagnosis and focuses on the automatic analysis of non-contrast-enhanced computed tomography, computed tomography angiography and perfusion imaging. Future studies are necessary that allow the evaluation and comparison of different imaging strategies and post-processing algorithms during the diagnosis process in patients with suspected acute ischemic stroke; which may further facilitate the standardisation of treatment and stroke management.",2019-08-01,4,1356,95,646
1105,31410728,The Future of Digital Psychiatry,"Purpose of review:                    Treatments in psychiatry have been rapidly changing over the last century, following the development of psychopharmacology and new research achievements. However, with advances in technology, the practice of psychiatry in the future will likely be influenced by new trends based on computerized approaches and digital communication. We examined four major areas that will probably impact on the clinical practice in the next few years: telepsychiatry; social media; mobile applications and internet of things; artificial intelligence; and machine learning.              Recent findings:                    Developments in these four areas will benefit patients throughout the journey of the illness, encompassing early diagnosis, even before the patients present to a clinician; personalized treatment on demand at anytime and anywhere; better prediction on patient outcomes; and even how mental illnesses are diagnosed in the future. Though the evidence for many technology-based interventions or mobile applications is still insufficient, it is likely that such advances in technology will play a larger role in the way that patient receives mental health interventions in the future, leading to easier access to them and improved outcomes.",2019-08-01,3,1280,32,646
1075,31447800,Machine Learning Approaches for Epidemiological Investigations of Food-Borne Disease Outbreaks,"Foodborne diseases (FBDs) are infections of the gastrointestinal tract caused by foodborne pathogens (FBPs) such as bacteria [Salmonella, Listeria monocytogenes and Shiga toxin-producing E. coli (STEC)] and several viruses, but also parasites and some fungi. Artificial intelligence (AI) and its sub-discipline machine learning (ML) are re-emerging and gaining an ever increasing popularity in the scientific community and industry, and could lead to actionable knowledge in diverse ranges of sectors including epidemiological investigations of FBD outbreaks and antimicrobial resistance (AMR). As genotyping using whole-genome sequencing (WGS) is becoming more accessible and affordable, it is increasingly used as a routine tool for the detection of pathogens, and has the potential to differentiate between outbreak strains that are closely related, identify virulence/resistance genes and provide improved understanding of transmission events within hours to days. In most cases, the computational pipeline of WGS data analysis can be divided into four (though, not necessarily consecutive) major steps: de novo genome assembly, genome characterization, comparative genomics, and inference of phylogeny or phylogenomics. In each step, ML could be used to increase the speed and potentially the accuracy (provided increasing amounts of high-quality input data) of identification of the source of ongoing outbreaks, leading to more efficient treatment and prevention of additional cases. In this review, we explore whether ML or any other form of AI algorithms have already been proposed for the respective tasks and compare those with mechanistic model-based approaches.",2019-08-01,3,1673,94,646
1235,32089788,Causability and explainability of artificial intelligence in medicine,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge > Human Centricity and User Interaction.",2019-08-01,32,1367,69,646
1074,31450799,Cancer Diagnosis Using Deep Learning: A Bibliographic Review,"In this paper, we first describe the basics of the field of cancer diagnosis, which includes steps of cancer diagnosis followed by the typical classification methods used by doctors, providing a historical idea of cancer classification techniques to the readers. These methods include Asymmetry, Border, Color and Diameter (ABCD) method, seven-point detection method, Menzies method, and pattern analysis. They are used regularly by doctors for cancer diagnosis, although they are not considered very efficient for obtaining better performance. Moreover, considering all types of audience, the basic evaluation criteria are also discussed. The criteria include the receiver operating characteristic curve (ROC curve), Area under the ROC curve (AUC), F1 score, accuracy, specificity, sensitivity, precision, dice-coefficient, average accuracy, and Jaccard index. Previously used methods are considered inefficient, asking for better and smarter methods for cancer diagnosis. Artificial intelligence and cancer diagnosis are gaining attention as a way to define better diagnostic tools. In particular, deep neural networks can be successfully used for intelligent image analysis. The basic framework of how this machine learning works on medical imaging is provided in this study, i.e., pre-processing, image segmentation and post-processing. The second part of this manuscript describes the different deep learning techniques, such as convolutional neural networks (CNNs), generative adversarial models (GANs), deep autoencoders (DANs), restricted Boltzmann's machine (RBM), stacked autoencoders (SAE), convolutional autoencoders (CAE), recurrent neural networks (RNNs), long short-term memory (LTSM), multi-scale convolutional neural network (M-CNN), multi-instance learning convolutional neural network (MIL-CNN). For each technique, we provide Python codes, to allow interested readers to experiment with the cited algorithms on their own diagnostic problems. The third part of this manuscript compiles the successfully applied deep learning models for different types of cancers. Considering the length of the manuscript, we restrict ourselves to the discussion of breast cancer, lung cancer, brain cancer, and skin cancer. The purpose of this bibliographic review is to provide researchers opting to work in implementing deep learning and artificial neural networks for cancer diagnosis a knowledge from scratch of the state-of-the-art achievements.",2019-08-01,16,2453,60,646
1096,31419815,"Artificial Intelligence in Health in 2018: New Opportunities, Challenges, and Practical Implications","Objective:                    To summarize significant research contributions to the field of artificial intelligence (AI) in health in 2018.              Methods:                    Ovid MEDLINE and Web of Science databases were searched to identify original research articles that were published in the English language during 2018 and presented advances in the science of AI applied in health. Queries employed Medical Subject Heading (MeSH) terms and keywords representing AI methodologies and limited results to health applications. Section editors selected 15 best paper candidates that underwent peer review by internationally renowned domain experts. Final best papers were selected by the editorial board of the 2018 International Medical Informatics Association (IMIA) Yearbook.              Results:                    Database searches returned 1,480 unique publications. Best papers employed innovative AI techniques that incorporated domain knowledge or explored approaches to support distributed or federated learning. All top-ranked papers incorporated novel approaches to advance the science of AI in health and included rigorous evaluations of their methodologies.              Conclusions:                    Performance of state-of-the-art AI machine learning algorithms can be enhanced by approaches that employ a multidisciplinary biomedical informatics pipeline to incorporate domain knowledge and can overcome challenges such as sparse, missing, or inconsistent data. Innovative training heuristics and encryption techniques may support distributed learning with preservation of privacy.",2019-08-01,0,1614,100,646
1094,31419823,"Advancing Artificial Intelligence in Sensors, Signals, and Imaging Informatics","Objective:                    To identify research works that exemplify recent developments in the field of sensors, signals, and imaging informatics.              Method:                    A broad literature search was conducted using PubMed and Web of Science, supplemented with individual papers that were nominated by section editors. A predefined query made from a combination of Medical Subject Heading (MeSH) terms and keywords were used to search both sources. Section editors then filtered the entire set of retrieved papers with each paper having been reviewed by two section editors. Papers were assessed on a three-point Likert scale by two section editors, rated from 0 (do not include) to 2 (should be included). Only papers with a combined score of 2 or above were considered.              Results:                    A search for papers was executed at the start of January 2019, resulting in a combined set of 1,459 records published in 2018 in 119 unique journals. Section editors jointly filtered the list of candidates down to 14 nominations. The 14 candidate best papers were then ranked by a group of eight external reviewers. Four papers, representing different international groups and journals, were selected as the best papers by consensus of the International Medical Informatics Association (IMIA) Yearbook editorial board.              Conclusions:                    The fields of sensors, signals, and imaging informatics have rapidly evolved with the application of novel artificial intelligence/machine learning techniques. Studies have been able to discover hidden patterns and integrate different types of data towards improving diagnostic accuracy and patient outcomes. However, the quality of papers varied widely without clear reporting standards for these types of models. Nevertheless, a number of papers have demonstrated useful techniques to improve the generalizability, interpretability, and reproducibility of increasingly sophisticated models.",2019-08-01,0,1990,78,646
1101,31414666,"Privacy-Preserving Methods for Feature Engineering Using Blockchain: Review, Evaluation, and Proof of Concept","Background:                    The protection of private data is a key responsibility for research studies that collect identifiable information from study participants. Limiting the scope of data collection and preventing secondary use of the data are effective strategies for managing these risks. An ideal framework for data collection would incorporate feature engineering, a process where secondary features are derived from sensitive raw data in a secure environment without a trusted third party.              Objective:                    This study aimed to compare current approaches based on how they maintain data privacy and the practicality of their implementations. These approaches include traditional approaches that rely on trusted third parties, and cryptographic, secure hardware, and blockchain-based techniques.              Methods:                    A set of properties were defined for evaluating each approach. A qualitative comparison was presented based on these properties. The evaluation of each approach was framed with a use case of sharing geolocation data for biomedical research.              Results:                    We found that approaches that rely on a trusted third party for preserving participant privacy do not provide sufficiently strong guarantees that sensitive data will not be exposed in modern data ecosystems. Cryptographic techniques incorporate strong privacy-preserving paradigms but are appropriate only for select use cases or are currently limited because of computational complexity. Blockchain smart contracts alone are insufficient to provide data privacy because transactional data are public. Trusted execution environments (TEEs) may have hardware vulnerabilities and lack visibility into how data are processed. Hybrid approaches combining blockchain and cryptographic techniques or blockchain and TEEs provide promising frameworks for privacy preservation. For reference, we provide a software implementation where users can privately share features of their geolocation data using the hybrid approach combining blockchain with TEEs as a supplement.              Conclusions:                    Blockchain technology and smart contracts enable the development of new privacy-preserving feature engineering methods by obviating dependence on trusted parties and providing immutable, auditable data processing workflows. The overlap between blockchain and cryptographic techniques or blockchain and secure hardware technologies are promising fields for addressing important data privacy needs. Hybrid blockchain and TEE frameworks currently provide practical tools for implementing experimental privacy-preserving applications.",2019-08-01,4,2694,109,646
1093,31419831,Contributions from the 2018 Literature on Bioinformatics and Translational Informatics,"Objectives:                    To summarize recent research and select the best papers published in 2018 in the field of Bioinformatics and Translational Informatics (BTI) for the corresponding section of the International Medical Informatics Association (IMIA) Yearbook.              Methods:                    A literature review was performed for retrieving from PubMed papers indexed with keywords and free terms related to BTI. Independent review allowed the two section editors to select a list of 14 candidate best papers which were subsequently peer-reviewed. A final consensus meeting gathering the whole IMIA Yearbook editorial committee was organized to finally decide on the selection of the best papers.              Results:                    Among the 636 retrieved papers published in 2018 in the various subareas of BTI, the review process selected four best papers. The first paper presents a computational method to identify molecular markers for targeted treatment of acute myeloid leukemia using multi-omics data (genome-wide gene expression profiles) and in vitro sensitivity to 160 chemotherapy drugs. The second paper describes a deep neural network approach to predict the survival of patients suffering from glioma on the basis of digitalised pathology images and genomics biomarkers. The authors of the third paper adopt a pan-cancer approach to take benefit of multi-omics data for drug repurposing. The fourth paper presents a graph-based semi-supervised method to accurate phenotype classification applied to ovarian cancer.              Conclusions:                    Thanks to the normalization of open data and open science practices, research in BTI continues to develop and mature. Noteworthy achievements are sophisticated applications of leading edge machine-learning methods dedicated to personalized medicine.",2019-08-01,1,1851,86,646
897,31272605,Artificial intelligence in cytopathology: a review of the literature and overview of commercial landscape,"Artificial intelligence (AI) has made impressive strides recently in interpreting complex images, thanks to improvements in deep learning techniques and increasing computational power. Researchers have started applying these advanced techniques to pathology images, although most efforts have been focused on histopathology. Cytopathology, however, remains the original field of pathology for which AI models for clinical use were successfully commercialized, to assist with automating Papanicolaou test screening. Recent AI efforts have focused on whole slide images of both gynecologic and non-gynecologic cytopathology. This review summarizes the literature and commercial landscape of AI as applied to cytopathology.",2019-08-01,5,720,105,646
1079,31440497,Augmenting Basin-Hopping With Techniques From Unsupervised Machine Learning: Applications in Spectroscopy and Ion Mobility,"Evolutionary algorithms such as the basin-hopping (BH) algorithm have proven to be useful for difficult non-linear optimization problems with multiple modalities and variables. Applications of these algorithms range from characterization of molecular states in statistical physics and molecular biology to geometric packing problems. A key feature of BH is the fact that one can generate a coarse-grained mapping of a potential energy surface (PES) in terms of local minima. These results can then be utilized to gain insights into molecular dynamics and thermodynamic properties. Here we describe how one can employ concepts from unsupervised machine learning to augment BH PES searches to more efficiently identify local minima and the transition states connecting them. Specifically, we introduce the concepts of similarity indices, hierarchical clustering, and multidimensional scaling to the BH methodology. These same machine learning techniques can be used as tools for interpreting and rationalizing experimental results from spectroscopic and ion mobility investigations (e.g., spectral assignment, dynamic collision cross sections). We exemplify this in two case studies: (1) assigning the infrared multiple photon dissociation spectrum of the protonated serine dimer and (2) determining the temperature-dependent collision cross-section of protonated alanine tripeptide.",2019-08-01,0,1381,122,646
865,31326235,Artificial Intelligence for Clinical Trial Design,"Clinical trials consume the latter half of the 10 to 15 year, 1.5-2.0 billion USD, development cycle for bringing a single new drug to market. Hence, a failed trial sinks not only the investment into the trial itself but also the preclinical development costs, rendering the loss per failed clinical trial at 800 million to 1.4 billion USD. Suboptimal patient cohort selection and recruiting techniques, paired with the inability to monitor patients effectively during trials, are two of the main causes for high trial failure rates: only one of 10 compounds entering a clinical trial reaches the market. We explain how recent advances in artificial intelligence (AI) can be used to reshape key steps of clinical trial design towards increasing trial success rates.",2019-08-01,19,765,49,646
1078,31442779,Protein secondary structure prediction using neural networks and deep learning: A review,"Literature contains over fifty years of accumulated methods proposed by researchers for predicting the secondary structures of proteins in silico. A large part of this collection is comprised of artificial neural network-based approaches, a field of artificial intelligence and machine learning that is gaining increasing popularity in various application areas. The primary objective of this paper is to put together the summary of works that are important but sparse in time, to help new researchers have a clear view of the domain in a single place. An informative introduction to protein secondary structure and artificial neural networks is also included for context. This review will be valuable in designing future methods to improve protein secondary structure prediction accuracy. The various neural network methods found in this problem domain employ varying architectures and feature spaces, and a handful stand out due to significant improvements in prediction. Neural networks with larger feature scope and higher architecture complexity have been found to produce better protein secondary structure prediction. The current prediction accuracy lies around the 84% marks, leaving much room for further improvement in the prediction of secondary structures in silico. It was found that the estimated limit of 88% prediction accuracy has not been reached yet, hence further research is a timely demand.",2019-08-01,6,1412,88,646
1076,31447631,Computational Methods for Resting-State EEG of Patients With Disorders of Consciousness,"Patients who survive brain injuries may develop Disorders of Consciousness (DOC) such as Coma, Vegetative State (VS) or Minimally Conscious State (MCS). Unfortunately, the rate of misdiagnosis between VS and MCS due to clinical judgment is high. Therefore, diagnostic decision support systems aiming to correct any differentiation between VS and MCS are essential for the characterization of an adequate treatment and an effective prognosis. In recent decades, there has been a growing interest in the new EEG computational techniques. We have reviewed how resting-state EEG is computationally analyzed to support differential diagnosis between VS and MCS in view of applicability of these methods in clinical practice. The studies available so far have used different techniques and analyses; it is therefore hard to draw general conclusions. Studies using a discriminant analysis with a combination of various factors and reporting a cut-off are among the most interesting ones for a future clinical application.",2019-08-01,2,1014,87,646
1084,31434208,Application of Artificial Intelligence in the Detection and Differentiation of Colon Polyps: A Technical Review for Physicians,"Research in computer-aided diagnosis (CAD) and the application of artificial intelligence (AI) in the endoscopic evaluation of the gastrointestinal tract is novel. Since colonoscopy and detection of polyps can decrease the risk of colon cancer, it is recommended by multiple national and international societies. However, the procedure of colonoscopy is performed by humans where there are significant interoperator and interpatient variations, and hence, the risk of missing detection of adenomatous polyps. Early studies involving CAD and AI for the detection and differentiation of polyps show great promise. In this appraisal, we review existing scientific aspects of AI in CAD of colon polyps and discuss the pitfalls and future directions for advancing the science. This review addresses the technical intricacies in a manner that physicians can comprehend to promote a better understanding of this novel application.",2019-08-01,5,923,126,646
866,31324413,Meta-Analysis Reveals Reproducible Gut Microbiome Alterations in Response to a High-Fat Diet,"Multiple research groups have shown that diet impacts the gut microbiome; however, variability in experimental design and quantitative assessment have made it challenging to assess the degree to which similar diets have reproducible effects across studies. Through an unbiased subject-level meta-analysis framework, we re-analyzed 27 dietary studies including 1,101 samples from rodents and humans. We demonstrate that a high-fat diet (HFD) reproducibly changes gut microbial community structure. Finer taxonomic analysis revealed that the most reproducible signals of a HFD are Lactococcus species, which we experimentally demonstrate to be common dietary contaminants. Additionally, a machine-learning approach defined a signature that predicts the dietary intake of mice and demonstrated that phylogenetic and gene-centric transformations of this model can be translated to humans. Together, these results demonstrate the utility of microbiome meta-analyses in identifying robust and reproducible features for mechanistic studies in preclinical models.",2019-08-01,22,1055,92,646
2226,31162525,Roles of membrane transporters: connecting the dots from sequence to phenotype,"Background:                    Plant membrane transporters are involved in diverse cellular processes underpinning plant physiology, such as nutrient acquisition, hormone movement, resource allocation, exclusion or sequestration of various solutes from cells and tissues, and environmental and developmental signalling. A comprehensive characterization of transporter function is therefore key to understanding and improving plant performance.              Scope and conclusions:                    In this review, we focus on the complexities involved in characterizing transporter function and the impact that this has on current genomic annotations. Specific examples are provided that demonstrate why sequence homology alone cannot be relied upon to annotate and classify transporter function, and to show how even single amino acid residue variations can influence transporter activity and specificity. Misleading nomenclature of transporters is often a source of confusion in transporter characterization, especially for people new to or outside the field. Here, to aid researchers dealing with interpretation of large data sets that include transporter proteins, we provide examples of transporters that have been assigned names that misrepresent their cellular functions. Finally, we discuss the challenges in connecting transporter function at the molecular level with physiological data, and propose a solution through the creation of new databases. Further fundamental in-depth research on specific transport (and other) proteins is still required; without it, significant deficiencies in large-scale data sets and systems biology approaches will persist. Reliable characterization of transporter function requires integration of data at multiple levels, from amino acid residue sequence annotation to more in-depth biochemical, structural and physiological studies.",2019-09-01,1,1877,78,615
2224,31166761,Artificial Intelligence in Musculoskeletal Imaging: Current Status and Future Directions,"OBJECTIVE. The objective of this article is to show how artificial intelligence (AI) has impacted different components of the imaging value chain thus far as well as to describe its potential future uses. CONCLUSION. The use of AI has the potential to greatly enhance every component of the imaging value chain. From assessing the appropriateness of imaging orders to helping predict patients at risk for fracture, AI can increase the value that musculoskeletal imagers provide to their patients and to referring clinicians by improving image quality, patient centricity, imaging efficiency, and diagnostic accuracy.",2019-09-01,8,616,88,615
1710,31624739,Predictive modeling in spine surgery,"As the cost of healthcare in the United States increases at an unsustainable rate, health-policy leaders are looking towards innovative ways to maximize value in delivery of care. Incorporating technology, such as artificial intelligence/machine-learning, to assist physicians in decision-making and predicting outcomes, on a real-time basis, is a major topic of discussion. While machine learning is gradually pulling traction in the medical community, it still remains a nascent field in the realm of spine surgery. The current review aims to gather current literature discussing the validity and applicability of machine-learning models in spine surgery.",2019-09-01,2,657,36,615
1039,31500324,Independent Component Analysis for Unraveling the Complexity of Cancer Omics Datasets,"Independent component analysis (ICA) is a matrix factorization approach where the signals captured by each individual matrix factors are optimized to become as mutually independent as possible. Initially suggested for solving source blind separation problems in various fields, ICA was shown to be successful in analyzing functional magnetic resonance imaging (fMRI) and other types of biomedical data. In the last twenty years, ICA became a part of the standard machine learning toolbox, together with other matrix factorization methods such as principal component analysis (PCA) and non-negative matrix factorization (NMF). Here, we review a number of recent works where ICA was shown to be a useful tool for unraveling the complexity of cancer biology from the analysis of different types of omics data, mainly collected for tumoral samples. Such works highlight the use of ICA in dimensionality reduction, deconvolution, data pre-processing, meta-analysis, and others applied to different data types (transcriptome, methylome, proteome, single-cell data). We particularly focus on the technical aspects of ICA application in omics studies such as using different protocols, determining the optimal number of components, assessing and improving reproducibility of the ICA results, and comparison with other popular matrix factorization techniques. We discuss the emerging ICA applications to the integrative analysis of multi-level omics datasets and introduce a conceptual view on ICA as a tool for defining functional subsystems of a complex biological system and their interactions under various conditions. Our review is accompanied by a Jupyter notebook which illustrates the discussed concepts and provides a practical tool for applying ICA to the analysis of cancer omics datasets.",2019-09-01,7,1791,85,615
154,30084866,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases","The identification of interactions between drugs/compounds and their targets is crucial for the development of new drugs. In vitro screening experiments (i.e. bioassays) are frequently used for this purpose; however, experimental approaches are insufficient to explore novel drug-target interactions, mainly because of feasibility problems, as they are labour intensive, costly and time consuming. A computational field known as 'virtual screening' (VS) has emerged in the past decades to aid experimental drug discovery studies by statistically estimating unknown bio-interactions between compounds and biological targets. These methods use the physico-chemical and structural properties of compounds and/or target proteins along with the experimentally verified bio-interaction information to generate predictive models. Lately, sophisticated machine learning techniques are applied in VS to elevate the predictive performance. The objective of this study is to examine and discuss the recent applications of machine learning techniques in VS, including deep learning, which became highly popular after giving rise to epochal developments in the fields of computer vision and natural language processing. The past 3 years have witnessed an unprecedented amount of research studies considering the application of deep learning in biomedicine, including computational drug discovery. In this review, we first describe the main instruments of VS methods, including compound and protein features (i.e. representations and descriptors), frequently used libraries and toolkits for VS, bioactivity databases and gold-standard data sets for system training and benchmarking. We subsequently review recent VS studies with a strong emphasis on deep learning applications. Finally, we discuss the present state of the field, including the current challenges and suggest future directions. We believe that this survey will provide insight to the researchers working in the field of computational drug discovery in terms of comprehending and developing novel bio-prediction methods.",2019-09-01,31,2071,119,615
2248,31131957,Meta-analysis of massively parallel reporter assays enables prediction of regulatory function across cell types,"Deciphering the potential of noncoding loci to influence gene regulation has been the subject of intense research, with important implications in understanding genetic underpinnings of human diseases. Massively parallel reporter assays (MPRAs) can measure regulatory activity of thousands of DNA sequences and their variants in a single experiment. With increasing number of publically available MPRA data sets, one can now develop data-driven models which, given a DNA sequence, predict its regulatory activity. Here, we performed a comprehensive meta-analysis of several MPRA data sets in a variety of cellular contexts. We first applied an ensemble of methods to predict MPRA output in each context and observed that the most predictive features are consistent across data sets. We then demonstrate that predictive models trained in one cellular context can be used to predict MPRA output in another, with loss of accuracy attributed to cell-type-specific features. Finally, we show that our approach achieves top performance in the Fifth Critical Assessment of Genome Interpretation ""Regulation Saturation"" Challenge for predicting effects of single-nucleotide variants. Overall, our analysis provides insights into how MPRA data can be leveraged to highlight functional regulatory regions throughout the genome and can guide effective design of future experiments by better prioritizing regions of interest.",2019-09-01,3,1412,111,615
885,31294972,Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery,"Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.",2019-09-01,36,1055,72,615
1037,31509997,Integrations between Autonomous Systems and Modern Computing Techniques: A Mini Review,"The emulation of human behavior for autonomous problem solving has been an interdisciplinary field of research. Generally, classical control systems are used for static environments, where external disturbances and changes in internal parameters can be fully modulated before or neglected during operation. However, classical control systems are inadequate at addressing environmental uncertainty. By contrast, autonomous systems, which were first studied in the field of control systems, can be applied in an unknown environment. This paper summarizes the state of the art autonomous systems by first discussing the definition, modeling, and system structure of autonomous systems and then providing a perspective on how autonomous systems can be integrated with advanced resources (e.g., the Internet of Things, big data, Over-the-Air, and federated learning). Finally, what comes after reaching full autonomy is briefly discussed.",2019-09-01,2,933,86,615
1059,31479500,Evaluation of parameters affecting performance and reliability of machine learning-based antibiotic susceptibility testing from whole genome sequencing data,"Prediction of antibiotic resistance phenotypes from whole genome sequencing data by machine learning methods has been proposed as a promising platform for the development of sequence-based diagnostics. However, there has been no systematic evaluation of factors that may influence performance of such models, how they might apply to and vary across clinical populations, and what the implications might be in the clinical setting. Here, we performed a meta-analysis of seven large Neisseria gonorrhoeae datasets, as well as Klebsiella pneumoniae and Acinetobacter baumannii datasets, with whole genome sequence data and antibiotic susceptibility phenotypes using set covering machine classification, random forest classification, and random forest regression models to predict resistance phenotypes from genotype. We demonstrate how model performance varies by drug, dataset, resistance metric, and species, reflecting the complexities of generating clinically relevant conclusions from machine learning-derived models. Our findings underscore the importance of incorporating relevant biological and epidemiological knowledge into model design and assessment and suggest that doing so can inform tailored modeling for individual drugs, pathogens, and clinical populations. We further suggest that continued comprehensive sampling and incorporation of up-to-date whole genome sequence data, resistance phenotypes, and treatment outcome data into model training will be crucial to the clinical utility and sustainability of machine learning-based molecular diagnostics.",2019-09-01,15,1567,156,615
894,31278512,Practical issues in assessing nailfold capillaroscopic images: a summary,"Nailfold capillaroscopy (NC) is a highly sensitive, safe, and non-invasive technique to assess involvement rate of microvascularity in dermatomyositis and systemic sclerosis. A large number of studies have focused on NC pattern description, classification, and scoring system validation, but minimal information has been published on the accuracy and precision of the measurement. The objective of this review article is to identify different factors affecting the reliability and validity of the assessment in NC. Several factors can affect the reliability of the examination, e.g., physiological artifacts, the nailfold imaging instrument, human factors, and the assessment rules and standards. It is impossible to avoid all artifacts, e.g., skin transparency, physically injured fingers, and skin pigmentation. However, minimization of the impact of some of these artifacts by considering some protocols before the examination and by using specialized tools, training, guidelines, and software can help to reduce errors in the measurement and assessment of NC images. Establishing guidelines and instructions for automatic characterization and measurement based on machine learning techniques also may reduce ambiguities and the assessment time.",2019-09-01,1,1248,72,615
2923,29893792,Regulatory variants: from detection to predicting impact,"Variants within non-coding genomic regions can greatly affect disease. In recent years, increasing focus has been given to these variants, and how they can alter regulatory elements, such as enhancers, transcription factor binding sites and DNA methylation regions. Such variants can be considered regulatory variants. Concurrently, much effort has been put into establishing international consortia to undertake large projects aimed at discovering regulatory elements in different tissues, cell lines and organisms, and probing the effects of genetic variants on regulation by measuring gene expression. Here, we describe methods and techniques for discovering disease-associated non-coding variants using sequencing technologies. We then explain the computational procedures that can be used for annotating these variants using the information from the aforementioned projects, and prediction of their putative effects, including potential pathogenicity, based on rule-based and machine learning approaches. We provide the details of techniques to validate these predictions, by mapping chromatin-chromatin and chromatin-protein interactions, and introduce Clustered Regularly Interspaced Short Palindromic Repeats-Associated Protein 9 (CRISPR-Cas9) technology, which has already been used in this field and is likely to have a big impact on its future evolution. We also give examples of regulatory variants associated with multiple complex diseases. This review is aimed at bioinformaticians interested in the characterization of regulatory variants, molecular biologists and geneticists interested in understanding more about the nature and potential role of such variants from a functional point of views, and clinicians who may wish to learn about variants in non-coding genomic regions associated with a given disease and find out what to do next to uncover how they impact on the underlying mechanisms.",2019-09-01,17,1911,56,615
2229,31158511,Unbiased data analytic strategies to improve biomarker discovery in precision medicine,"Omics technologies promised improved biomarker discovery for precision medicine. The foremost problem of discovered biomarkers is irreproducibility between patient cohorts. From a data analytics perspective, the main reason for these failures is bias in statistical approaches and overfitting resulting from batch effects and confounding factors. The keys to reproducible biomarker discovery are: proper study design, unbiased data preprocessing and quality control analyses, and a knowledgeable application of statistics and machine learning algorithms. In this review, we discuss study design and analysis considerations and suggest standards from an expert point-of-view to promote unbiased decision-making in biomarker discovery in precision medicine.",2019-09-01,4,755,86,615
2235,31151893,A Road Map for Translational Research on Artificial Intelligence in Medical Imaging: From the 2018 National Institutes of Health/RSNA/ACR/The Academy Workshop,"Advances in machine learning in medical imaging are occurring at a rapid pace in research laboratories both at academic institutions and in industry. Important artificial intelligence (AI) tools for diagnostic imaging include algorithms for disease detection and classification, image optimization, radiation reduction, and workflow enhancement. Although advances in foundational research are occurring rapidly, translation to routine clinical practice has been slower. In August 2018, the National Institutes of Health assembled multiple relevant stakeholders at a public meeting to discuss the current state of knowledge, infrastructure gaps, and challenges to wider implementation. The conclusions of that meeting are summarized in two publications that identify and prioritize initiatives to accelerate foundational and translational research in AI for medical imaging. This publication summarizes key priorities for translational research developed at the workshop including: (1) creating structured AI use cases, defining and highlighting clinical challenges potentially solvable by AI; (2) establishing methods to encourage data sharing for training and testing AI algorithms to promote generalizability to widespread clinical practice and mitigate unintended bias; (3) establishing tools for validation and performance monitoring of AI algorithms to facilitate regulatory approval; and (4) developing standards and common data elements for seamless integration of AI tools into existing clinical workflows. An important goal of the resulting road map is to grow an ecosystem, facilitated by professional societies, industry, and government agencies, that will allow robust collaborations between practicing clinicians and AI researchers to advance foundational and translational research relevant to medical imaging.",2019-09-01,9,1824,158,615
1044,31496867,Toward a grey box approach for cardiovascular physiome,"The physiomic approach is now widely used in the diagnosis of cardiovascular diseases. There are two possible methods for cardiovascular physiome: the traditional mathematical model and the machine learning (ML) algorithm. ML is used in almost every area of society for various tasks formerly performed by humans. Specifically, various ML techniques in cardiovascular medicine are being developed and improved at unprecedented speed. The benefits of using ML for various tasks is that the inner working mechanism of the system does not need to be known, which can prove convenient in situations where determining the inner workings of the system can be difficult. The computation speed is also often higher than that of the traditional mathematical models. The limitations with ML are that it inherently leads to an approximation, and special care must be taken in cases where a high accuracy is required. Traditional mathematical models are, however, constructed based on underlying laws either proven or assumed. The results from the mathematical models are accurate as long as the model is. Combining the advantages of both the mathematical models and ML would increase both the accuracy and efficiency of the simulation for many problems. In this review, examples of cardiovascular physiome where approaches of mathematical modeling and ML can be combined are introduced.",2019-09-01,1,1375,54,615
2260,31111458,A primer in artificial intelligence in cardiovascular medicine,"Driven by recent developments in computational power, algorithms and web-based storage resources, machine learning (ML)-based artificial intelligence (AI) has quickly gained ground as the solution for many technological and societal challenges. AI education has become very popular and is oversubscribed at Dutch universities. Major investments were made in 2018 to develop and build the first AI-driven hospitals to improve patient care and reduce healthcare costs. AI has the potential to greatly enhance traditional statistical analyses in many domains and has been demonstrated to allow the discovery of 'hidden' information in highly complex datasets. As such, AI can also be of significant value in the diagnosis and treatment of cardiovascular disease, and the first applications of AI in the cardiovascular field are promising. However, many professionals in the cardiovascular field involved in patient care, education or science are unaware of the basics behind AI and the existing and expected applications in their field. In this review, we aim to introduce the broad cardiovascular community to the basics of modern ML-based AI and explain several of the commonly used algorithms. We also summarise their initial and future applications relevant to the cardiovascular field.",2019-09-01,7,1287,62,615
1053,31491852,Cardioprotective Melatonin: Translating from Proof-of-Concept Studies to Therapeutic Use,"In this review we summarized the actual clinical data for a cardioprotective therapeutic role of melatonin, listed melatonin and its agonists in different stages of development, and evaluated the melatonin cardiovascular target tractability and prediction using machine learning on ChEMBL. To date, most clinical trials investigating a cardioprotective therapeutic role of melatonin are in phase 2a. Selective melatonin receptor agonists Tasimelteon, Ramelteon, and combined melatonergic-serotonin Agomelatine, and other agonists with registered structures in CHEMBL were not yet investigated as cardioprotective or cardiovascular drugs. As drug-able for these therapeutic targets, melatonin receptor agonists have the benefit over melatonin of well-characterized pharmacologic profiles and extensive safety data. Recent reports of the X-ray crystal structures of MT1 and MT2 receptors shall lead to the development of highly selective melatonin receptor agonists. Predictive models using machine learning could help to identify cardiovascular targets for melatonin. Selecting ChEMBL scores > 4.5 in cardiovascular assays, and melatonin scores > 4, we obtained 284 records from 162 cardiovascular assays carried out with 80 molecules with predicted or measured melatonin activity. Melatonin activities (agonistic or antagonistic) found in these experimental cardiovascular assays and models include arrhythmias, coronary and large vessel contractility, and hypertension. Preclinical proof-of-concept and early clinical studies (phase 2a) suggest a cardioprotective benefit from melatonin in various heart diseases. However, larger phase 3 randomized interventional studies are necessary to establish melatonin and its agonists' actions as cardioprotective therapeutic agents.",2019-09-01,5,1775,88,615
850,31351213,Artificial intelligence for assisting diagnostics and assessment of Parkinson's disease-A review,"Artificial intelligence, specifically machine learning, has found numerous applications in computer-aided diagnostics, monitoring and management of neurodegenerative movement disorders of parkinsonian type. These tasks are not trivial due to high inter-subject variability and similarity of clinical presentations of different neurodegenerative disorders in the early stages. This paper aims to give a comprehensive, high-level overview of applications of artificial intelligence through machine learning algorithms in kinematic analysis of movement disorders, specifically Parkinson's disease (PD). We surveyed papers published between January 2007 and January 2019, within online databases, including PubMed and Science Direct, with a focus on the most recently published studies. The search encompassed papers dealing with the implementation of machine learning algorithms for diagnosis and assessment of PD using data describing motion of upper and lower extremities. This systematic review presents an overview of 48 relevant studies published in the abovementioned period, which investigate the use of artificial intelligence for diagnostics, therapy assessment and progress prediction in PD based on body kinematics. Different machine learning algorithms showed promising results, particularly for early PD diagnostics. The investigated publications demonstrated the potentials of collecting data from affordable and globally available devices. However, to fully exploit artificial intelligence technologies in the future, more widespread collaboration is advised among medical institutions, clinicians and researchers, to facilitate aligning of data collection protocols, sharing and merging of data sets.",2019-09-01,5,1713,96,615
1052,31492401,"Strengths, Weaknesses, Opportunities, and Threats Analysis of Artificial Intelligence and Machine Learning Applications in Radiology","Currently, the use of artificial intelligence (AI) in radiology, particularly machine learning (ML), has become a reality in clinical practice. Since the end of the last century, several ML algorithms have been introduced for a wide range of common imaging tasks, not only for diagnostic purposes but also for image acquisition and postprocessing. AI is now recognized to be a driving initiative in every aspect of radiology. There is growing evidence of the advantages of AI in radiology creating seamless imaging workflows for radiologists or even replacing radiologists. Most of the current AI methods have some internal and external disadvantages that are impeding their ultimate implementation in the clinical arena. As such, AI can be considered a portion of a business trying to be introduced in the health care market. For this reason, this review analyzes the current status of AI, and specifically ML, applied to radiology from the scope of strengths, weaknesses, opportunities, and threats (SWOT) analysis.",2019-09-01,10,1017,132,615
1051,31492403,The Application of Machine Learning to Quality Improvement Through the Lens of the Radiology Value Network,"Recent advances in machine learning and artificial intelligence offer promising applications to radiology quality improvement initiatives as they relate to the radiology value network. Coordination within the interlocking web of systems, events, and stakeholders in the radiology value network may be mitigated though standardization, automation, and a focus on workflow efficiency. In this article the authors present applications of these various strategies via use cases for quality improvement projects at different points in the radiology value network. In addition, the authors discuss opportunities for machine-learning applications in data aggregation as opposed to traditional applications in data extraction.",2019-09-01,0,718,106,615
1050,31492404,Using Artificial Intelligence to Improve the Quality and Safety of Radiation Therapy,"Within artificial intelligence, machine learning (ML) efforts in radiation oncology have augmented the transition from generalized to personalized treatment delivery. Although their impact on quality and safety of radiation therapy has been limited, they are increasingly being used throughout radiation therapy workflows. Various data-driven approaches have been used for outcome prediction, CT simulation, clinical decision support, knowledge-based planning, adaptive radiation therapy, plan validation, machine quality assurance, and process quality assurance; however, there are many challenges that need to be addressed with the creation and usage of ML algorithms as well as the interpretation and dissemination of findings. In this review, the authors present current applications of ML in radiation oncology quality and safety initiatives, discuss challenges faced by the radiation oncology community, and suggest future directions.",2019-09-01,4,940,84,615
1089,31427808,Do no harm: a roadmap for responsible machine learning for health care,"Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).",2019-09-01,43,453,70,615
1049,31492410,A Survey of Deep-Learning Applications in Ultrasound: Artificial Intelligence-Powered Ultrasound for Improving Clinical Workflow,"Ultrasound is the most commonly used imaging modality in clinical practice because it is a nonionizing, low-cost, and portable point-of-care imaging tool that provides real-time images. Artificial intelligence (AI)-powered ultrasound is becoming more mature and getting closer to routine clinical applications in recent times because of an increased need for efficient and objective acquisition and evaluation of ultrasound images. Because ultrasound images involve operator-, patient-, and scanner-dependent variations, the adaptation of classical machine learning methods to clinical applications becomes challenging. With their self-learning ability, deep-learning (DL) methods are able to harness exponentially growing graphics processing unit computing power to identify abstract and complex imaging features. This has given rise to tremendous opportunities such as providing robust and generalizable AI models for improving image acquisition, real-time assessment of image quality, objective diagnosis and detection of diseases, and optimizing ultrasound clinical workflow. In this report, the authors review current DL approaches and research directions in rapidly advancing ultrasound technology and present their outlook on future directions and trends for DL techniques to further improve diagnosis, reduce health care cost, and optimize ultrasound clinical workflow.",2019-09-01,10,1377,128,615
1056,31481588,Radiomics: Data Are Also Images,"The aim of this review is to provide readers with an update on the state of the art, pitfalls, solutions for those pitfalls, future perspectives, and challenges in the quickly evolving field of radiomics in nuclear medicine imaging and associated oncology applications. The main pitfalls were identified in study design, data acquisition, segmentation, feature calculation, and modeling; however, in most cases, potential solutions are available and existing recommendations should be followed to improve the overall quality and reproducibility of published radiomics studies. The techniques from the field of deep learning have some potential to provide solutions, especially in terms of automation. Some important challenges remain to be addressed but, overall, striking advances have been made in the field in the last 5 y.",2019-09-01,6,826,31,615
504,31048019,Deep learning in ophthalmology: The technical and clinical considerations,"The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.",2019-09-01,36,2116,73,615
1048,31492411,The Potential Role of Radiomics and Radiogenomics in Patient Stratification by Tumor Hypoxia Status,"Background:                    Despite the clinical knowledge accumulated over a century about tumor hypoxia, this biologic parameter remains a major challenge in cancer treatment. Patients presenting with hypoxic tumors are more resistant to radiotherapy and often poor responders to chemotherapy. Treatment failure because of hypoxia is, therefore, very common. Several methods have been trialed to measure and quantify tumor hypoxia, with varied success. Over the last couple of decades, hypoxia-specific functional imaging has started to play an important role in personalized treatment planning and delivery. Yet, there are no gold standards in place, owing to inter- and intrapatient phenotypic variations that further complicate the overall picture. The aim of the current article is to analyze, through the review of the literature, the potential role of radiomics and radiogenomics in patient stratification by tumor hypoxia status.              Methods:                    Search of literature published in English since 2000 was conducted using Medline. Additional articles were retrieved via pearling of identified literature. Publications were reviewed and summarized in text and in a tabulated format.              Results:                    Although still an immature area of science, radiomics has shown its potential in the quantification of hypoxia within the heterogeneous tumor, quantification of changes regarding the degree of hypoxia after radiotherapy and drug delivery, monitoring tumor response to anti-angiogenic therapy, and assisting with patient stratification and outcome prediction based on the hypoxic status.              Conclusions:                    The lack of technique standardization to measure and quantify tumor hypoxia presents an opportunity for data mining and machine learning in radiogenomics.",2019-09-01,0,1843,99,615
2891,29982332,Trends in the development of miRNA bioinformatics tools,"MicroRNAs (miRNAs) are small noncoding RNAs that regulate gene expression via recognition of cognate sequences and interference of transcriptional, translational or epigenetic processes. Bioinformatics tools developed for miRNA study include those for miRNA prediction and discovery, structure, analysis and target prediction. We manually curated 95 review papers and 1000 miRNA bioinformatics tools published since 2003. We classified and ranked them based on citation number or PageRank score, and then performed network analysis and text mining (TM) to study the miRNA tools development trends. Five key trends were observed: (1) miRNA identification and target prediction have been hot spots in the past decade; (2) manual curation and TM are the main methods for collecting miRNA knowledge from literature; (3) most early tools are well maintained and widely used; (4) classic machine learning methods retain their utility; however, novel ones have begun to emerge; (5) disease-associated miRNA tools are emerging. Our analysis yields significant insight into the past development and future directions of miRNA tools.",2019-09-01,25,1124,55,615
1047,31492414,Artificial Intelligence and Clinical Decision Support for Radiologists and Referring Providers,"Recent advances in artificial intelligence (AI) are providing an opportunity to enhance existing clinical decision support (CDS) tools to improve patient safety and drive value-based imaging. We discuss the advantages and potential applications that may be realized with the synergy between AI and CDS systems. From the perspective of both radiologist and ordering provider, CDS could be significantly empowered using AI. CDS enhanced by AI could reduce friction in radiology workflows and can aid AI developers to identify relevant imaging features their tools should be seeking to extract from images. Furthermore, these systems can generate structured data to be used as input to develop machine learning algorithms, which can drive downstream care pathways. For referring providers, an AI-enabled CDS solution could enable an evolution from existing imaging-centric CDS toward decision support that takes into account a holistic patient perspective. More intelligent CDS could suggest imaging examinations in highly complex clinical scenarios, assist on the identification of appropriate imaging opportunities at the health system level, suggest appropriate individualized screening, or aid health care providers to ensure continuity of care. AI has the potential to enable the next generation of CDS, improving patient care and enhancing providers' and radiologists' experience.",2019-09-01,2,1383,94,615
574,30927499,Systems Metabolic Engineering Meets Machine Learning: A New Era for Data-Driven Metabolic Engineering,"The recent increase in high-throughput capacity of 'omics datasets combined with advances and interest in machine learning (ML) have created great opportunities for systems metabolic engineering. In this regard, data-driven modeling methods have become increasingly valuable to metabolic strain design. In this review, the nature of 'omics is discussed and a broad introduction to the ML algorithms combining these datasets into predictive models of metabolism and metabolic rewiring is provided. Next, this review highlights recent work in the literature that utilizes such data-driven methods to inform various metabolic engineering efforts for different classes of application including product maximization, understanding and profiling phenotypes, de novo metabolic pathway design, and creation of robust system-scale models for biotechnology. Overall, this review aims to highlight the potential and promise of using ML algorithms with metabolic engineering and systems biology related datasets.",2019-09-01,2,1000,101,615
572,30937909,Field crop phenomics: enabling breeding for radiation use efficiency and biomass in cereal crops,"Plant phenotyping forms the core of crop breeding, allowing breeders to build on physiological traits and mechanistic science to inform their selection of material for crossing and genetic gain. Recent rapid progress in high-throughput techniques based on machine vision, robotics, and computing (plant phenomics) enables crop physiologists and breeders to quantitatively measure complex and previously intractable traits. By combining these techniques with affordable genomic sequencing and genotyping, machine learning, and genome selection approaches, breeders have an opportunity to make rapid genetic progress. This review focuses on how field-based plant phenomics can enable next-generation physiological breeding in cereal crops for traits related to radiation use efficiency, photosynthesis, and crop biomass. These traits have previously been regarded as difficult and laborious to measure but have recently become a focus as cereal breeders find genetic progress from 'Green Revolution' traits such as harvest index become exhausted. Application of LiDAR, thermal imaging, leaf and canopy spectral reflectance, Chl fluorescence, and machine learning are discussed using wheat and sorghum phenotyping as case studies. A vision of how crop genomics and high-throughput phenotyping could enable the next generation of crop research and breeding is presented.",2019-09-01,22,1366,96,615
571,30943796,An overview of thermal necrosis: present and future,"Introduction: Many orthopaedic procedures require drilling of bone, especially fracture repair cases. Bone drilling results in heat generation due to the friction between the bone and the drill bit. A high-level of heat generation kills bone cells. Bone cell death results in resorption of bone around bone screws.Methods: We searched in the literature for data on parameters that influence drilling bone and could lead to thermal necrosis. The points of view of many orthopaedists and neurosurgeons based upon on previous practices and clinical experience are presented.Results: Several potential complications that lead to thermal necrosis are discussed and highlighted.Discussion: Even in the face of growing evidence as to the negative effects of heat induction during drilling, simple and effective methods for monitoring and cooling in real-time are not in widespread usage today. For that purpose, we propose some suggestions for the future of bone drilling, taking note of recent advances in autonomous robotics, intelligent systems and computer simulation techniques.Conclusions: These advances in prevention of thermal necrosis during bone drilling surgery are expected to reduce the risk of patient injury and costs for the health service.",2019-09-01,6,1250,51,615
905,31261187,Artificial intelligence for pediatric ophthalmology,"Purpose of review:                    Despite the impressive results of recent artificial intelligence applications to general ophthalmology, comparatively less progress has been made toward solving problems in pediatric ophthalmology using similar techniques. This article discusses the unique needs of pediatric patients and how artificial intelligence techniques can address these challenges, surveys recent applications to pediatric ophthalmology, and discusses future directions.              Recent findings:                    The most significant advances involve the automated detection of retinopathy of prematurity, yielding results that rival experts. Machine learning has also been applied to the classification of pediatric cataracts, prediction of postoperative complications following cataract surgery, detection of strabismus and refractive error, prediction of future high myopia, and diagnosis of reading disability. In addition, machine learning techniques have been used for the study of visual development, vessel segmentation in pediatric fundus images, and ophthalmic image synthesis.              Summary:                    Artificial intelligence applications could significantly benefit clinical care by optimizing disease detection and grading, broadening access to care, furthering scientific discovery, and improving clinical efficiency. These methods need to match or surpass physician performance in clinical trials before deployment with patients. Owing to the widespread use of closed-access data sets and software implementations, it is difficult to directly compare the performance of these approaches, and reproducibility is poor. Open-access data sets and software could alleviate these issues and encourage further applications to pediatric ophthalmology.",2019-09-01,5,1795,51,615
1042,31497064,Measuring crops in 3D: using geometry for plant phenotyping,"Using 3D sensing for plant phenotyping has risen within the last years. This review provides an overview on 3D traits for the demands of plant phenotyping considering different measuring techniques, derived traits and use-cases of biological applications. A comparison between a high resolution 3D measuring device and an established measuring tool, the leaf meter, is shown to categorize the possible measurement accuracy. Furthermore, different measuring techniques such as laser triangulation, structure from motion, time-of-flight, terrestrial laser scanning or structured light approaches enable the assessment of plant traits such as leaf width and length, plant size, volume and development on plant and organ level. The introduced traits were shown with respect to the measured plant types, the used measuring technique and the link to their biological use case. These were trait and growth analysis for measurements over time as well as more complex investigation on water budget, drought responses and QTL (quantitative trait loci) analysis. The used processing pipelines were generalized in a 3D point cloud processing workflow showing the single processing steps to derive plant parameters on plant level, on organ level using machine learning or over time using time series measurements. Finally the next step in plant sensing, the fusion of different sensor types namely 3D and spectral measurements is introduced by an example on sugar beet. This multi-dimensional plant model is the key to model the influence of geometry on radiometric measurements and to correct it. This publication depicts the state of the art for 3D measuring of plant traits as they were used in plant phenotyping regarding how the data is acquired, how this data is processed and what kind of traits is measured at the single plant, the miniplot, the experimental field and the open field scale. Future research will focus on highly resolved point clouds on the experimental and field scale as well as on the automated trait extraction of organ traits to track organ development at these scales.",2019-09-01,11,2085,59,615
1057,31481587,Artificial Intelligence in Nuclear Medicine,"Despite the great media attention for artificial intelligence (AI), for many health care professionals the term and the functioning of AI remain a ""black box,"" leading to exaggerated expectations on the one hand and unfounded fears on the other. In this review, we provide a conceptual classification and a brief summary of the technical fundamentals of AI. Possible applications are discussed on the basis of a typical work flow in medical imaging, grouped by planning, scanning, interpretation, and reporting. The main limitations of current AI techniques, such as issues with interpretability or the need for large amounts of annotated data, are briefly addressed. Finally, we highlight the possible impact of AI on the nuclear medicine profession, the associated challenges and, last but not least, the opportunities.",2019-09-01,6,821,43,615
833,31383376,Artificial Intelligence for Drug Toxicity and Safety,"Interventional pharmacology is one of medicine's most potent weapons against disease. These drugs, however, can result in damaging side effects and must be closely monitored. Pharmacovigilance is the field of science that monitors, detects, and prevents adverse drug reactions (ADRs). Safety efforts begin during the development process, using in vivo and in vitro studies, continue through clinical trials, and extend to postmarketing surveillance of ADRs in real-world populations. Future toxicity and safety challenges, including increased polypharmacy and patient diversity, stress the limits of these traditional tools. Massive amounts of newly available data present an opportunity for using artificial intelligence (AI) and machine learning to improve drug safety science. Here, we explore recent advances as applied to preclinical drug safety and postmarketing surveillance with a specific focus on machine and deep learning (DL) approaches.",2019-09-01,6,949,52,615
2220,31173851,"A review on brain tumor diagnosis from MRI images: Practical implications, key achievements, and lessons learned","The successful early diagnosis of brain tumors plays a major role in improving the treatment outcomes and thus improving patient survival. Manually evaluating the numerous magnetic resonance imaging (MRI) images produced routinely in the clinic is a difficult process. Thus, there is a crucial need for computer-aided methods with better accuracy for early tumor diagnosis. Computer-aided brain tumor diagnosis from MRI images consists of tumor detection, segmentation, and classification processes. Over the past few years, many studies have focused on traditional or classical machine learning techniques for brain tumor diagnosis. Recently, interest has developed in using deep learning techniques for diagnosing brain tumors with better accuracy and robustness. This study presents a comprehensive review of traditional machine learning techniques and evolving deep learning techniques for brain tumor diagnosis. This review paper identifies the key achievements reflected in the performance measurement metrics of the applied algorithms in the three diagnosis processes. In addition, this study discusses the key findings and draws attention to the lessons learned as a roadmap for future research.",2019-09-01,4,1203,112,615
842,31362904,"If machines can learn, who needs scientists?","Machine learning has been used in NMR in for decades, but recent developments signal explosive growth is on the horizon. An obstacle to the application of machine learning in NMR is the relative paucity of available training data, despite the existence of numerous public NMR data repositories. Other challenges include the problem of interpreting the results of a machine learning algorithm, and incorporating machine learning into hypothesis-driven research. This perspective imagines the potential of machine learning in NMR and speculates on possible approaches to the hurdles.",2019-09-01,0,581,44,615
2216,31180806,The Ultimate Guide to Bacterial Swarming: An Experimental Model to Study the Evolution of Cooperative Behavior,"Cooperation has fascinated biologists since Darwin. How did cooperative behaviors evolve despite the fitness cost to the cooperator? Bacteria have cooperative behaviors that make excellent models to take on this age-old problem from both proximate (molecular) and ultimate (evolutionary) angles. We delve into Pseudomonas aeruginosa swarming, a phenomenon where billions of bacteria move cooperatively across distances of centimeters in a matter of a few hours. Experiments with swarming have unveiled a strategy called metabolic prudence that stabilizes cooperation, have showed the importance of spatial structure, and have revealed a regulatory network that integrates environmental stimuli and direct cooperative behavior, similar to a machine learning algorithm. The study of swarming elucidates more than proximate mechanisms: It exposes ultimate mechanisms valid to all scales, from cells in cancerous tumors to animals in large communities.",2019-09-01,3,948,110,615
1115,31394043,"Deep Learning: The Good, the Bad, and the Ugly",Artificial vision has often been described as one of the key remaining challenges to be solved before machines can act intelligently. Recent developments in a branch of machine learning known as deep learning have catalyzed impressive gains in machine vision-giving a sense that the problem of vision is getting closer to being solved. The goal of this review is to provide a comprehensive overview of recent deep learning developments and to critically assess actual progress toward achieving human-level visual intelligence. I discuss the implications of the successes and limitations of modern machine vision algorithms for biological vision and the prospect for neuroscience to inform the design of future artificial vision systems.,2019-09-01,16,736,46,615
1759,31557462,Development and Arealization of the Cerebral Cortex,"Adult cortical areas consist of specialized cell types and circuits that support unique higher-order cognitive functions. How this regional diversity develops from an initially uniform neuroepithelium has been the subject of decades of seminal research, and emerging technologies, including single-cell transcriptomics, provide a new perspective on area-specific molecular diversity. Here, we review the early developmental processes that underlie cortical arealization, including both cortex intrinsic and extrinsic mechanisms as embodied by the protomap and protocortex hypotheses, respectively. We propose an integrated model of serial homology whereby intrinsic genetic programs and local factors establish early transcriptomic differences between excitatory neurons destined to give rise to broad ""proto-regions,"" and activity-dependent mechanisms lead to progressive refinement and formation of sharp boundaries between functional areas. Finally, we explore the potential of these basic developmental processes to inform our understanding of the emergence of functional neural networks and circuit abnormalities in neurodevelopmental disorders.",2019-09-01,20,1150,51,615
1760,31557461,Engineering a Less Artificial Intelligence,"Despite enormous progress in machine learning, artificial neural networks still lag behind brains in their ability to generalize to new situations. Given identical training data, differences in generalization are caused by many defining features of a learning algorithm, such as network architecture and learning rule. Their joint effect, called ""inductive bias,"" determines how well any learning algorithm-or brain-generalizes: robust generalization needs good inductive biases. Artificial networks use rather nonspecific biases and often latch onto patterns that are only informative about the statistics of the training data but may not generalize to different scenarios. Brains, on the other hand, generalize across comparatively drastic changes in the sensory input all the time. We highlight some shortcomings of state-of-the-art learning algorithms compared to biological brains and discuss several ideas about how neuroscience can guide the quest for better inductive biases by providing useful constraints on representations and network architecture.",2019-09-01,12,1059,42,615
1110,31399886,Artificial intelligence in cardiovascular imaging: state of the art and implications for the imaging cardiologist,"Healthcare, conceivably more than any other area of human endeavour, has the greatest potential to be affected by artificial intelligence (AI). This potential has been shown by several reports that demonstrate equal or superhuman performance in medical tasks that aim to improve efficiency, diagnosis and prognosis. This review focuses on the state of the art of AI applications in cardiovascular imaging. It provides an overview of the current applications and studies performed, including the potential value, implications, limitations and future directions of AI in cardiovascular imaging.It is envisioned that AI will dramatically change the way doctors practise medicine. In the short term, it will assist physicians with easy tasks, such as automating measurements, making predictions based on big data, and putting clinical findings into an evidence-based context. In the long term, AI will not only assist doctors, it has the potential to significantly improve access to health and well-being data for patients and their caretakers. This empowers patients. From a physician's perspective, reliable AI assistance will be available to support clinical decision-making. Although cardiovascular studies implementing AI are increasing in number, the applications have only just started to penetrate contemporary clinical care.",2019-09-01,11,1329,113,615
1109,31401616,Intelligent Imaging: Artificial Intelligence Augmented Nuclear Medicine,"Artificial intelligence (AI) in nuclear medicine and radiology represents a significant disruptive technology. Although there has been much debate about the impact of AI on the careers of radiologists, the opportunities in nuclear medicine enhance the capability of the physician and at the same time have an impact on the responsibilities of physicists and technologists. This transformative technology requires insight into the principles and opportunities for seamless assimilation into practice without the associated displacement of human resources. This article introduces the current clinical applications of machine learning and deep learning.",2019-09-01,1,651,71,615
1766,31552275,Artificial Intelligence Will Transform Cardiac Imaging-Opportunities and Challenges,"Artificial intelligence (AI) using machine learning techniques will change healthcare as we know it. While healthcare AI applications are currently trailing behind popular AI applications, such as personalized web-based advertising, the pace of research and deployment is picking up and about to become disruptive. Overcoming challenges such as patient and public support, transparency over the legal basis for healthcare data use, privacy preservation, technical challenges related to accessing large-scale data from healthcare systems not designed for Big Data analysis, and deployment of AI in routine clinical practice will be crucial. Cardiac imaging and imaging of other body parts is likely to be at the frontier for the development of applications as pattern recognition and machine learning are a significant strength of AI with practical links to image processing. Many opportunities in cardiac imaging exist where AI will impact patients, medical staff, hospitals, commissioners and thus, the entire healthcare system. This perspective article will outline our vision for AI in cardiac imaging with examples of potential applications, challenges and some lessons learnt in recent years.",2019-09-01,9,1197,83,615
1767,31551916,New Approaches to Studying Silent Mesial Temporal Lobe Seizures in Alzheimer's Disease,"Silent seizures were discovered in mouse models of Alzheimer's disease over 10 years ago, yet it remains unclear whether these seizures are a salient feature of Alzheimer's disease in humans. Seizures that arise early in the course of Alzheimer's disease most likely originate from the mesial temporal lobe, one of the first structures affected by Alzheimer's disease pathology and one of the most epileptogenic regions of the brain. Several factors greatly limit our ability to identify mesial temporal lobe seizures in patients with Alzheimer's disease, however. First, mesial temporal lobe seizures can be difficult to recognize clinically, as their accompanying symptoms are often subtle or even non-existent. Second, electrical activity arising from the mesial temporal lobe is largely invisible on the scalp electroencephalogram (EEG), the mainstay of diagnosis for epilepsy in this population. In this review, we will describe two new approaches being used to study silent mesial temporal lobe seizures in Alzheimer's disease. We will first describe the methodology and application of foramen ovale electrodes, which captured the first recordings of silent mesial temporal lobe seizures in humans with Alzheimer's disease. We will then describe machine learning approaches being developed to non-invasively identify silent mesial temporal lobe seizures on scalp EEG. Both of these tools have the potential to elucidate the role of silent seizures in humans with Alzheimer's disease, which could have important implications for early diagnosis, prognostication, and development of targeted therapies for this population.",2019-09-01,2,1626,86,615
1769,33733108,The Autonomous Mind: The Right to Freedom of Thought in the Twenty-First Century,"To lose freedom of thought (FoT) is to lose our dignity, our democracy and our very selves. Accordingly, the right to FoT receives absolute protection under international human rights law. However, this foundational right has been neither significantly developed nor often utilized. The contours of this right urgently need to be defined due to twenty-first century threats to FoT posed by new technologies. As such, this paper draws on law and psychology to consider what the right to FoT should be in the twenty-first century. After discussing contemporary threats to FoT, and recent developments in our understanding of thought that can inform the development of the right, this paper considers three elements of the right; the rights not to reveal one's thoughts, not to be penalized for one's thoughts, and not to have one's thoughts manipulated. The paper then considers, for each element, why it should exist, how the law currently treats it, and challenges that will shape it going forward. The paper concludes that the law should develop the right to FoT with the clear understanding that what this aims to secure is mental autonomy. This process should hence begin by establishing the core mental processes that enable mental autonomy, such as attentional and cognitive agency. The paper argues that the domain of the right to FoT should be extended to include external actions that are arguably constitutive of thought, including internet searches and diaries, hence shielding them with absolute protection. It is stressed that law must protect us from threats to FoT from both states and corporations, with governments needing to act under the positive aspect of the right to ensure societies are structured to facilitate mental autonomy. It is suggested that in order to support mental autonomy, information should be provided in autonomy-supportive contexts and friction introduced into decision making processes to facilitate second-order thought. The need for public debate about how society wishes to balance risk and mental autonomy is highlighted, and the question is raised as to whether the importance attached to thought has changed in our culture. The urgency of defending FoT is re-iterated.",2019-09-01,0,2215,80,615
1771,31547800,Machine learning for discovering missing or wrong protein function annotations : A comparison using updated benchmark datasets,"Background:                    A massive amount of proteomic data is generated on a daily basis, nonetheless annotating all sequences is costly and often unfeasible. As a countermeasure, machine learning methods have been used to automatically annotate new protein functions. More specifically, many studies have investigated hierarchical multi-label classification (HMC) methods to predict annotations, using the Functional Catalogue (FunCat) or Gene Ontology (GO) label hierarchies. Most of these studies employed benchmark datasets created more than a decade ago, and thus train their models on outdated information. In this work, we provide an updated version of these datasets. By querying recent versions of FunCat and GO yeast annotations, we provide 24 new datasets in total. We compare four HMC methods, providing baseline results for the new datasets. Furthermore, we also evaluate whether the predictive models are able to discover new or wrong annotations, by training them on the old data and evaluating their results against the most recent information.              Results:                    The results demonstrated that the method based on predictive clustering trees, Clus-Ensemble, proposed in 2008, achieved superior results compared to more recent methods on the standard evaluation task. For the discovery of new knowledge, Clus-Ensemble performed better when discovering new annotations in the FunCat taxonomy, whereas hierarchical multi-label classification with genetic algorithm (HMC-GA), a method based on genetic algorithms, was overall superior when detecting annotations that were removed. In the GO datasets, Clus-Ensemble once again had the upper hand when discovering new annotations, HMC-GA performed better for detecting removed annotations. However, in this evaluation, there were less significant differences among the methods.              Conclusions:                    The experiments have showed that protein function prediction is a very challenging task which should be further investigated. We believe that the baseline results associated with the updated datasets provided in this work should be considered as guidelines for future studies, nonetheless the old versions of the datasets should not be disregarded since other tasks in machine learning could benefit from them.",2019-09-01,3,2322,126,615
1772,31547220,Wearable-Based Affect Recognition-A Review,"Affect recognition is an interdisciplinary research field bringing together researchers from natural and social sciences. Affect recognition research aims to detect the affective state of a person based on observables, with the goal to, for example, provide reasoning for the person's decision making or to support mental wellbeing (e.g., stress monitoring). Recently, beside of approaches based on audio, visual or text information, solutions relying on wearable sensors as observables, recording mainly physiological and inertial parameters, have received increasing attention. Wearable systems enable an ideal platform for long-term affect recognition applications due to their rich functionality and form factor, while providing valuable insights during everyday life through integrated sensors. However, existing literature surveys lack a comprehensive overview of state-of-the-art research in wearable-based affect recognition. Therefore, the aim of this paper is to provide a broad overview and in-depth understanding of the theoretical background, methods and best practices of wearable affect and stress recognition. Following a summary of different psychological models, we detail the influence of affective states on the human physiology and the sensors commonly employed to measure physiological changes. Then, we outline lab protocols eliciting affective states and provide guidelines for ground truth generation in field studies. We also describe the standard data processing chain and review common approaches related to the preprocessing, feature extraction and classification steps. By providing a comprehensive summary of the state-of-the-art and guidelines to various aspects, we would like to enable other researchers in the field to conduct and evaluate user studies and develop wearable systems.",2019-09-01,6,1817,42,615
1773,31546906,Early Autism Screening: A Comprehensive Review,"Autistic spectrum disorder (ASD) refers to a neurodevelopmental condition associated with verbal and nonverbal communication, social interactions, and behavioural complications that is becoming increasingly common in many parts of the globe. Identifying individuals on the spectrum has remained a lengthy process for the past few decades due to the fact that some individuals diagnosed with ASD exhibit exceptional skills in areas such as mathematics, arts, and music among others. To improve the accuracy and reliability of autism diagnoses, many scholars have developed pre-diagnosis screening methods to help identify autistic behaviours at an early stage, speed up the clinical diagnosis referral process, and improve the understanding of ASD for the different stakeholders involved, such as parents, caregivers, teachers, and family members. However, the functionality and reliability of those screening tools vary according to different research studies and some have remained questionable. This study evaluates and critically analyses 37 different ASD screening tools in order to identify possible areas that need to be addressed through further development and innovation. More importantly, different criteria associated with existing screening tools, such as accessibility, the fulfilment of Diagnostic and Statistical Manual of Mental Disorders (DSM-5) specifications, comprehensibility among the target audience, performance (specifically sensitivity, specificity, and accuracy), web and mobile availability, and popularity have been investigated.",2019-09-01,7,1558,46,615
1774,31545655,Using Machine Learning and Natural Language Processing to Review and Classify the Medical Literature on Cancer Susceptibility Genes,"Purpose:                    The medical literature relevant to germline genetics is growing exponentially. Clinicians need tools that help to monitor and prioritize the literature to understand the clinical implications of pathogenic genetic variants. We developed and evaluated two machine learning models to classify abstracts as relevant to the penetrance-risk of cancer for germline mutation carriers-or prevalence of germline genetic mutations.              Materials and methods:                    We conducted literature searches in PubMed and retrieved paper titles and abstracts to create an annotated data set for training and evaluating the two machine learning classification models. Our first model is a support vector machine (SVM) which learns a linear decision rule on the basis of the bag-of-ngrams representation of each title and abstract. Our second model is a convolutional neural network (CNN) which learns a complex nonlinear decision rule on the basis of the raw title and abstract. We evaluated the performance of the two models on the classification of papers as relevant to penetrance or prevalence.              Results:                    For penetrance classification, we annotated 3,740 paper titles and abstracts and evaluated the two models using 10-fold cross-validation. The SVM model achieved 88.93% accuracy-percentage of papers that were correctly classified-whereas the CNN model achieved 88.53% accuracy. For prevalence classification, we annotated 3,753 paper titles and abstracts. The SVM model achieved 88.92% accuracy and the CNN model achieved 88.52% accuracy.              Conclusion:                    Our models achieve high accuracy in classifying abstracts as relevant to penetrance or prevalence. By facilitating literature review, this tool could help clinicians and researchers keep abreast of the burgeoning knowledge of gene-cancer associations and keep the knowledge bases for clinical decision support tools up to date.",2019-09-01,3,1978,131,615
1778,31540192,Key Topics in Molecular Docking for Drug Design,"Molecular docking has been widely employed as a fast and inexpensive technique in the past decades, both in academic and industrial settings. Although this discipline has now had enough time to consolidate, many aspects remain challenging and there is still not a straightforward and accurate route to readily pinpoint true ligands among a set of molecules, nor to identify with precision the correct ligand conformation within the binding pocket of a given target molecule. Nevertheless, new approaches continue to be developed and the volume of published works grows at a rapid pace. In this review, we present an overview of the method and attempt to summarise recent developments regarding four main aspects of molecular docking approaches: (i) the available benchmarking sets, highlighting their advantages and caveats, (ii) the advances in consensus methods, (iii) recent algorithms and applications using fragment-based approaches, and (iv) the use of machine learning algorithms in molecular docking. These recent developments incrementally contribute to an increase in accuracy and are expected, given time, and together with advances in computing power and hardware capability, to eventually accomplish the full potential of this area.",2019-09-01,11,1245,47,615
1787,31523704,Artificial Intelligence and Machine Learning in Pathology: The Present Landscape of Supervised Methods,"Increased interest in the opportunities provided by artificial intelligence and machine learning has spawned a new field of health-care research. The new tools under development are targeting many aspects of medical practice, including changes to the practice of pathology and laboratory medicine. Optimal design in these powerful tools requires cross-disciplinary literacy, including basic knowledge and understanding of critical concepts that have traditionally been unfamiliar to pathologists and laboratorians. This review provides definitions and basic knowledge of machine learning categories (supervised, unsupervised, and reinforcement learning), introduces the underlying concept of the bias-variance trade-off as an important foundation in supervised machine learning, and discusses approaches to the supervised machine learning study design along with an overview and description of common supervised machine learning algorithms (linear regression, logistic regression, Naive Bayes, k-nearest neighbor, support vector machine, random forest, convolutional neural networks).",2019-09-01,23,1084,102,615
1100,31416540,Addressing challenges of quantitative methodologies and event interpretation in the study of atrial fibrillation,"Atrial fibrillation (AF) is the commonest arrhythmia, yet the mechanisms of its onset and persistence are incompletely known. Although techniques for quantitative assessment have been investigated, there have been few attempts to integrate this information to advance disease treatment protocols. In this review, key quantitative methods for AF analysis are described, and suggestions are provided for the coordination of the available information, and to develop foci and directions for future research efforts. Quantitative biologists may have an interest in this topic in order to develop machine learning and tools for arrhythmia characterization, but they may perhaps have a minimal background in the clinical methodology and in the types of observed events and mechanistic hypotheses that have thus far been developed. We attempt to address these issues via exploration of the published literature. Although no new data is presented in this review, examples are shown of current lines of investigation, and in particular, how electrogram analysis and whole-chamber quantitative modeling of the left atrium may be useful to characterize fibrillatory patterns of activity, so as to propose avenues for more efficacious acquisition and interpretation of AF data.",2019-09-01,1,1265,112,615
2142,31762579,A Review of Machine Learning Approaches in Assisted Reproductive Technologies,"Introduction:                    Assisted reproductive technologies (ART) are recent improvements in infertility treatment. However, there is no significant increase in pregnancy rates with the aid of ART. Costly and complex process of ART's makes them as challenging issues. Computational prediction models could predict treatment outcome, before the start of an ART cycle.              Aim:                    This review provides an overview on machine learning-based prediction models in ART.              Methods:                    This article was executed based on a literature review through scientific databases search such as PubMed, Scopus, Web of Science and Google Scholar.              Results:                    We identified 20 papers reporting on machine learning-based prediction models in IVF or ICSI settings. All of the models were validated only by internal validation. Therefore, external validation of the models and the impact analysis of them were the missing parts of the all studies.              Conclusion:                    Machine learning-based prediction models provide a clinical decision support tool for both clinicians and patients and lead to improvement in ART success rates.",2019-09-01,5,1218,77,615
1098,31417996,Algorithms for immunochromatographic assay: review and impact on future application,"Lateral flow immunoassay (LFIA) is a critical choice for applications of point-of-care testing (POCT) in clinical and laboratory environments because of its excellent features and versatility. To obtain authentic values of analyte concentrations and reliable detection results, the relevant research has featured the application of a diversity of methods of mathematical analysis to technical analysis to allow for use with a small quantity of data. Accordingly, a number of signal and image processing strategies have also emerged for the application of gold immunochromatographic and fluorescent strips to improve sensitivity and overcome the limitations of correlative hardware systems. Instead of traditional methods to solve the problem, researchers nowadays are interested in machine learning and its more powerful variant, deep learning technology, for LFIA detection. This review emphasizes different models for the POCT of accurate labels as well as signal processing strategies that use artificial intelligence and machine learning. We focus on the analytical mechanism, procedural flow, and the results of the assay, and conclude by summarizing the advantages and limitations of each algorithm. We also discuss the potential for application of and directions of future research on LFIA technology when combined with Artificial Intelligence and deep learning.",2019-09-01,4,1369,83,615
1092,31420515,"Closed-loop cycles of experiment design, execution, and learning accelerate systems biology model development in yeast","One of the most challenging tasks in modern science is the development of systems biology models: Existing models are often very complex but generally have low predictive performance. The construction of high-fidelity models will require hundreds/thousands of cycles of model improvement, yet few current systems biology research studies complete even a single cycle. We combined multiple software tools with integrated laboratory robotics to execute three cycles of model improvement of the prototypical eukaryotic cellular transformation, the yeast (Saccharomyces cerevisiae) diauxic shift. In the first cycle, a model outperforming the best previous diauxic shift model was developed using bioinformatic and systems biology tools. In the second cycle, the model was further improved using automatically planned experiments. In the third cycle, hypothesis-led experiments improved the model to a greater extent than achieved using high-throughput experiments. All of the experiments were formalized and communicated to a cloud laboratory automation system (Eve) for automatic execution, and the results stored on the semantic web for reuse. The final model adds a substantial amount of knowledge about the yeast diauxic shift: 92 genes (+45%), and 1,048 interactions (+147%). This knowledge is also relevant to understanding cancer, the immune system, and aging. We conclude that systems biology software tools can be combined and integrated with laboratory robots in closed-loop cycles.",2019-09-01,0,1489,118,615
2164,31728276,"Data-Driven Materials Science: Status, Challenges, and Perspectives","Data-driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning-typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high-throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data-driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data-driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",2019-09-01,6,1216,67,615
1121,31386605,Data-Driven Approaches to Understanding Visual Neuron Activity,"With modern neurophysiological methods able to record neural activity throughout the visual pathway in the context of arbitrarily complex visual stimulation, our understanding of visual system function is becoming limited by the available models of visual neurons that can be directly related to such data. Different forms of statistical models are now being used to probe the cellular and circuit mechanisms shaping neural activity, understand how neural selectivity to complex visual features is computed, and derive the ways in which neurons contribute to systems-level visual processing. However, models that are able to more accurately reproduce observed neural activity often defy simple interpretations. As a result, rather than being used solely to connect with existing theories of visual processing, statistical modeling will increasingly drive the evolution of more sophisticated theories.",2019-09-01,0,900,62,615
2400,30865306,Precision medicine for the discovery of treatable mechanisms in severe asthma,"Although the complex disease of asthma has been defined as being heterogeneous, the extent of its endophenotypes remains unclear. The pharmacological approach to initiating treatment has, until recently, been based on disease control and severity. The introduction of antibody therapies targeting the Type 2 inflammation pathway for patients with severe asthma has resulted in the recognition of an allergic and an eosinophilic phenotype, which are not mutually exclusive. Concomitantly, molecular phenotyping based on a transcriptomic analysis of bronchial epithelial and sputum cells has identified a Type 2 high inflammation cluster characterized by eosinophilia and recurrent exacerbations, as well as Type 2 low clusters linked with IL-6 trans-signalling, interferon pathways, inflammasome activation and mitochondrial oxidative phosphorylation pathways. Systems biology approaches are establishing the links between these pathways or mechanisms, and clinical and physiologic features. Validation of these pathways contributes to defining endotypes and treatable mechanisms. Precision medicine approaches are necessary to link treatable mechanisms with treatable traits and biomarkers derived from clinical, physiologic and inflammatory features of clinical phenotypes. The deep molecular phenotyping of airway samples along with noninvasive biomarkers linked to bioinformatic and machine learning techniques will enable the rapid detection of molecular mechanisms that transgresses beyond the concept of treatable traits.",2019-09-01,15,1527,77,615
1727,31607962,A Comprehensive Review of Computational Methods for Automatic Prediction of Schizophrenia With Insight Into Indigenous Populations,"Psychiatrists rely on language and speech behavior as one of the main clues in psychiatric diagnosis. Descriptive psychopathology and phenomenology form the basis of a common language used by psychiatrists to describe abnormal mental states. This conventional technique of clinical observation informed early studies on disturbances of thought form, speech, and language observed in psychosis and schizophrenia. These findings resulted in language models that were used as tools in psychosis research that concerned itself with the links between formal thought disorder and language disturbances observed in schizophrenia. The end result was the development of clinical rating scales measuring severity of disturbances in speech, language, and thought form. However, these linguistic measures do not fully capture the richness of human discourse and are time-consuming and subjective when measured against psychometric rating scales. These linguistic measures have not considered the influence of culture on psychopathology. With recent advances in computational sciences, we have seen a re-emergence of novel research using computing methods to analyze free speech for improving prediction and diagnosis of psychosis. Current studies on automated speech analysis examining for semantic incoherence are carried out based on natural language processing and acoustic analysis, which, in some studies, have been combined with machine learning approaches for classification and prediction purposes.",2019-09-01,1,1494,130,615
1024,31522268,"Distress, Suicidality, and Affective Disorders at the Time of Social Networks","Purpose of review:                    We reviewed how scholars recently addressed the complex relationship that binds distress, affective disorders, and suicidal behaviors on the one hand and social networking on the other. We considered the latest machine learning performances in detecting affective-related outcomes from social media data, and reviewed understandings of how, why, and with what consequences distressed individuals use social network sites. Finally, we examined how these insights may concretely instantiate on the individual level with a qualitative case series.              Recent findings:                    Machine learning classifiers are progressively stabilizing with moderate to high performances in detecting affective-related diagnosis, symptoms, and risks from social media linguistic markers. Qualitatively, such markers appear to translate ambivalent and socially constrained motivations such as self-disclosure, passive support seeking, and connectedness reinforcement. Binding data science and psychosocial research appears as the unique condition to ground a translational web-clinic for treating and preventing affective-related issues on social media.",2019-09-01,0,1190,77,615
857,31344297,Quantitation of Femtomolar-Level Protein Biomarkers Using a Simple Microbubbling Digital Assay and Bright-Field Smartphone Imaging,"Quantitating ultra-low concentrations of protein biomarkers is critical for early disease diagnosis and treatment. However, most current point-of-care (POC) assays are limited in sensitivity. Herein, we introduce an ultra-sensitive and facile microbubbling assay for the quantification of protein biomarkers with a digital-readout method that requires only a smartphone camera. We used machine learning to develop a smartphone application for automated image analysis to facilitate accurate and robust counting. Using this method, post-prostatectomy surveillance of prostate specific antigen (PSA) can be achieved with a detection limit (LOD) of 2.1 fm (0.060 pg mL-1 ), and early pregnancy detection using hCG can be achieved with a of 0.034 mIU mL-1 (2.84 pg mL-1 ). This work provides the proof-of-principle of the microbubbling assay with a digital readout as an ultra-sensitive technology with minimal requirement for power and accessories, facilitating future POC applications.",2019-09-01,2,984,130,615
2210,31199919,Pathology Image Analysis Using Segmentation Deep Learning Algorithms,"With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning-based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis.",2019-09-01,19,1405,68,615
1025,31522256,Fatal and Non-fatal Self-Injury in the USA: Critical Review of Current Trends and Innovations in Prevention,"Purpose of review:                    To examine current trends in suicide and self-injury in the USA, as well as potential contributors to their change over time, and to reflect on innovations in prevention and intervention that can guide policies and programs to reduce the burden of suicide and self-injury in the USA.              Recent findings:                    Suicide and non-fatal self-injury are on the rise in the USA. Reasons for such trends over time remain speculative, although they seem linked to coincident increases in mood disorders and drug use and overdose. Promising innovative prevention and intervention programs that engage new technologies, such as machine learning-derived prediction tools and computerized ecologic momentary assessments, are currently in development and require additional evidence. Recent increases in fatal and non-fatal self-harm in the USA raise questions about the causes, interventions, and preventive measures that should be taken. Most innovative prevention efforts target individuals seeking to improve risk prediction and access to evidence-based care. However, as Durkheim pointed out over 100 years ago, suicide rates vary enormously between societal groups, suggesting that certain causal factors of suicide act and, hence, should be targeted at an ecological level. In the next generation of suicide research, it is critical to examine factors beyond the proximal and clinical to allow for a reimagining of prevention that is life course and socially focused.",2019-09-01,2,1521,107,615
910,31256388,Emerging Methods to Objectively Assess Pruritus in Atopic Dermatitis,"Introduction:                    Atopic dermatitis (AD) is an inflammatory skin disease with a chronic, relapsing course. Clinical features of AD vary by age, duration, and severity but can include papules, vesicles, erythema, exudate, xerosis, scaling, and lichenification. However, the most defining and universal symptom of AD is pruritus. Pruritus or itch, defined as an unpleasant urge to scratch, is problematic for many reasons, particularly its negative impact on quality of life. Despite the profoundly negative impact of pruritus on patients with AD, clinicians and researchers lack standardized and validated methods to objectively measure pruritus. The purpose of this review is to discuss emerging methods to assess pruritus in AD by describing objective patient-centered tools developed or enhanced over the last decade that can be utilized by clinicians and researchers alike.              Methods:                    This review is based on a literature search in Medline, Embase, and Web of Science databases. The search was performed in February 2019. The keywords were used ""pruritus,"" ""itch,"" ""atopic dermatitis,"" ""eczema,"" ""measurements,"" ""tools,"" ""instruments,"" ""accelerometer,"" ""wrist actigraphy,"" ""smartwatch,"" ""transducer,"" ""vibration,"" ""brain mapping,"" ""magnetic resonance imaging,"" and ""positron emission tomography."" Only articles written in English were included, and no restrictions were set on study type. To focus on emerging methods, prioritization was given to results from the last decade (2009-2019).              Results:                    The search yielded 49 results in PubMed, 134 results in Embase, and 85 results in Web of Science. Each result was independently reviewed in a standardized manner by two of the authors (M.S., K.L.), and disagreements between reviewers were resolved by consensus. Relevant findings were categorized into the following sections: video surveillance, acoustic surveillance, wrist actigraphy, smart devices, vibration transducers, and neurological imaging. Examples are provided along with descriptions of how each technology works, instances of use in research or clinical practice, and as applicable, reports of validation studies and correlation with other methods.              Conclusion:                    The variety of new and improved methods to evaluate pruritus in AD is welcomed by clinicians, researchers, and patients alike. Future directions include next-generation smart devices as well as exploring new territories, such as identifying biomarkers that correlate to itch and machine-learning programs to identify itch processing in the brain. As these efforts continue, it will be essential to remain patient-centered by developing techniques that minimize discomfort, respect privacy, and provide accurate data that can be used to better manage itch in AD.",2019-09-01,4,2846,68,615
2356,29436887,Machine learning in autistic spectrum disorder behavioral research: A review and ways forward,"Autistic Spectrum Disorder (ASD) is a mental disorder that retards acquisition of linguistic, communication, cognitive, and social skills and abilities. Despite being diagnosed with ASD, some individuals exhibit outstanding scholastic, non-academic, and artistic capabilities, in such cases posing a challenging task for scientists to provide answers. In the last few years, ASD has been investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning to improve diagnostic timing, precision, and quality. Machine learning is a multidisciplinary research topic that employs intelligent techniques to discover useful concealed patterns, which are utilized in prediction to improve decision making. Machine learning techniques such as support vector machines, decision trees, logistic regressions, and others, have been applied to datasets related to autism in order to construct predictive models. These models claim to enhance the ability of clinicians to provide robust diagnoses and prognoses of ASD. However, studies concerning the use of machine learning in ASD diagnosis and treatment suffer from conceptual, implementation, and data issues such as the way diagnostic codes are used, the type of feature selection employed, the evaluation measures chosen, and class imbalances in data among others. A more serious claim in recent studies is the development of a new method for ASD diagnoses based on machine learning. This article critically analyses these recent investigative studies on autism, not only articulating the aforementioned issues in these studies but also recommending paths forward that enhance machine learning use in ASD with respect to conceptualization, implementation, and data. Future studies concerning machine learning in autism research are greatly benefitted by such proposals.",2019-09-01,16,1866,93,615
2195,31222562,"Overview of image-to-image translation by use of deep neural networks: denoising, super-resolution, modality conversion, and reconstruction in medical imaging","Since the advent of deep convolutional neural networks (DNNs), computer vision has seen an extremely rapid progress that has led to huge advances in medical imaging. Every year, many new methods are reported at conferences such as the International Conference on Medical Image Computing and Computer-Assisted Intervention and Machine Learning for Medical Image Reconstruction, or published online at the preprint server arXiv. There is a plethora of surveys on applications of neural networks in medical imaging (see [1] for a relatively recent comprehensive survey). This article does not aim to cover all aspects of the field, but focuses on a particular topic, image-to-image translation. Although the topic may not sound familiar, it turns out that many seemingly irrelevant applications can be understood as instances of image-to-image translation. Such applications include (1) noise reduction, (2) super-resolution, (3) image synthesis, and (4) reconstruction. The same underlying principles and algorithms work for various tasks. Our aim is to introduce some of the key ideas on this topic from a uniform viewpoint. We introduce core ideas and jargon that are specific to image processing by use of DNNs. Having an intuitive grasp of the core ideas of applications of neural networks in medical imaging and a knowledge of technical terms would be of great help to the reader for understanding the existing and future applications. Most of the recent applications which build on image-to-image translation are based on one of two fundamental architectures, called pix2pix and CycleGAN, depending on whether the available training data are paired or unpaired (see Sect. 1.3). We provide codes ([2, 3]) which implement these two architectures with various enhancements. Our codes are available online with use of the very permissive MIT license. We provide a hands-on tutorial for training a model for denoising based on our codes (see Sect. 6). We hope that this article, together with the codes, will provide both an overview and the details of the key algorithms and that it will serve as a basis for the development of new applications.",2019-09-01,3,2145,158,615
912,31254491,Imaging Quality Control in the Era of Artificial Intelligence,"The advent of artificial intelligence (AI) promises to have a transformational impact on quality in medicine, including in radiology. However, experience has shown that quality tools alone are often not sufficient to bring about consistent excellent performance. Specifically, rather than assuming outcome targets are consistently met, in quality control, managers assume that wide variation is likely present unless proven otherwise with objective performance data. In this article, we discuss what we consider to be the eight essential elements required to achieve comprehensive process control, necessary to deliver consistent quality in radiology: a process control framework, performance measures, performance standards and targets, monitoring applications, prediction models, optimization models, feedback mechanisms, and accountability mechanisms. We consider these elements to be universally applicable, including in the application of AI-based models. We also discuss how the lack of specific elements of a quality control program can hinder widespread quality control efforts. We illustrate the concept using the example of a CT radiation dose optimization and process control program previously developed by one of the authors and provide several examples of how AI-based tools might be used for quality control in radiology.",2019-09-01,3,1336,61,615
1064,31466916,Looking beyond the hype: Applied AI and machine learning in translational medicine,"Big data problems are becoming more prevalent for laboratory scientists who look to make clinical impact. A large part of this is due to increased computing power, in parallel with new technologies for high quality data generation. Both new and old techniques of artificial intelligence (AI) and machine learning (ML) can now help increase the success of translational studies in three areas: drug discovery, imaging, and genomic medicine. However, ML technologies do not come without their limitations and shortcomings. Current technical limitations and other limitations including governance, reproducibility, and interpretation will be discussed in this article. Overcoming these limitations will enable ML methods to be more powerful for discovery and reduce ambiguity within translational medicine, allowing data-informed decision-making to deliver the next generation of diagnostics and therapeutics to patients quicker, at lowered costs, and at scale.",2019-09-01,7,958,82,615
2198,31220370,Machine learning and statistical models for predicting indoor air quality,"Indoor air quality (IAQ), as determined by the concentrations of indoor air pollutants, can be predicted using either physically based mechanistic models or statistical models that are driven by measured data. In comparison with mechanistic models mostly used in unoccupied or scenario-based environments, statistical models have great potential to explore IAQ captured in large measurement campaigns or in real occupied environments. The present study carried out the first literature review of the use of statistical models to predict IAQ. The most commonly used statistical modeling methods were reviewed and their strengths and weaknesses discussed. Thirty-seven publications, in which statistical models were applied to predict IAQ, were identified. These studies were all published in the past decade, indicating the emergence of the awareness and application of machine learning and statistical modeling in the field of IAQ. The concentrations of indoor particulate matter (PM2.5 and PM10 ) were the most frequently studied parameters, followed by carbon dioxide and radon. The most popular statistical models applied to IAQ were artificial neural networks, multiple linear regression, partial least squares, and decision trees.",2019-09-01,2,1235,73,615
2950,29800232,Predicting novel microRNA: a comprehensive comparison of machine learning approaches,"Motivation:                    The importance of microRNAs (miRNAs) is widely recognized in the community nowadays because these short segments of RNA can play several roles in almost all biological processes. The computational prediction of novel miRNAs involves training a classifier for identifying sequences having the highest chance of being precursors of miRNAs (pre-miRNAs). The big issue with this task is that well-known pre-miRNAs are usually few in comparison with the hundreds of thousands of candidate sequences in a genome, which results in high class imbalance. This imbalance has a strong influence on most standard classifiers, and if not properly addressed in the model and the experiments, not only performance reported can be completely unrealistic but also the classifier will not be able to work properly for pre-miRNA prediction. Besides, another important issue is that for most of the machine learning (ML) approaches already used (supervised methods), it is necessary to have both positive and negative examples. The selection of positive examples is straightforward (well-known pre-miRNAs). However, it is difficult to build a representative set of negative examples because they should be sequences with hairpin structure that do not contain a pre-miRNA.              Results:                    This review provides a comprehensive study and comparative assessment of methods from these two ML approaches for dealing with the prediction of novel pre-miRNAs: supervised and unsupervised training. We present and analyze the ML proposals that have appeared during the past 10 years in literature. They have been compared in several prediction tasks involving two model genomes and increasing imbalance levels. This work provides a review of existing ML approaches for pre-miRNA prediction and fair comparisons of the classifiers with same features and data sets, instead of just a revision of published software tools. The results and the discussion can help the community to select the most adequate bioinformatics approach according to the prediction task at hand. The comparative results obtained suggest that from low to mid-imbalance levels between classes, supervised methods can be the best. However, at very high imbalance levels, closer to real case scenarios, models including unsupervised and deep learning can provide better performance.",2019-09-01,4,2376,84,615
1061,31472738,The Evolving Use of Electronic Health Records (EHR) for Research,"Electronic health records (EHR) have been implemented successfully in a majority of United States healthcare systems in some form. There has been a rise in secondary uses of EHR, especially for research. EHR data is large, heterogenous, incomplete, noisy, and primarily created for purposes other than research. This presents many challenges, many of which are beginning to be overcome with the application of computer science artificial intelligence techniques, such as natural language processing and machine learning. EHR are gradually being redesigned to facilitate future research, though we are still far from a ""complete EHR.""",2019-10-01,2,633,64,585
1060,31478577,Machine learning applications in epilepsy,"Machine learning leverages statistical and computer science principles to develop algorithms capable of improving performance through interpretation of data rather than through explicit instructions. Alongside widespread use in image recognition, language processing, and data mining, machine learning techniques have received increasing attention in medical applications, ranging from automated imaging analysis to disease forecasting. This review examines the parallel progress made in epilepsy, highlighting applications in automated seizure detection from electroencephalography (EEG), video, and kinetic data, automated imaging analysis and pre-surgical planning, prediction of medication response, and prediction of medical and surgical outcomes using a wide variety of data sources. A brief overview of commonly used machine learning approaches, as well as challenges in further application of machine learning techniques in epilepsy, is also presented. With increasing computational capabilities, availability of effective machine learning algorithms, and accumulation of larger datasets, clinicians and researchers will increasingly benefit from familiarity with these techniques and the significant progress already made in their application in epilepsy.",2019-10-01,13,1264,41,585
1695,31646170,Statistical methods for dementia risk prediction and recommendations for future work: A systematic review,"Introduction:                    Numerous dementia risk prediction models have been developed in the past decade. However, methodological limitations of the analytical tools used may hamper their ability to generate reliable dementia risk scores. We aim to review the used methodologies.              Methods:                    We systematically reviewed the literature from March 2014 to September 2018 for publications presenting a dementia risk prediction model. We critically discuss the analytical techniques used in the literature.              Results:                    In total 137 publications were included in the qualitative synthesis. Three techniques were identified as the most commonly used methodologies: machine learning, logistic regression, and Cox regression.              Discussion:                    We identified three major methodological weaknesses: (1) over-reliance on one data source, (2) poor verification of statistical assumptions of Cox and logistic regression, and (3) lack of validation. The use of larger and more diverse data sets is recommended. Assumptions should be tested thoroughly, and actions should be taken if deviations are detected.",2019-10-01,3,1184,105,585
1783,31530047,A review of medical image detection for cancers in digestive system based on artificial intelligence,"Introduction: At present, cancer imaging examination relies mainly on manual reading of doctors, which requests a high standard of doctors' professional skills, clinical experience, and concentration. However, the increasing amount of medical imaging data has brought more and more challenges to radiologists. The detection of digestive system cancer (DSC) based on artificial intelligence (AI) can provide a solution for automatic analysis of medical images and assist doctors to achieve high-precision intelligent diagnosis of cancers. Areas covered: The main goal of this paper is to introduce the main research methods of the AI based detection of DSC, and provide relevant reference for researchers. Meantime, it summarizes the main problems existing in these methods, and provides better guidance for future research. Expert commentary: The automatic classification, recognition, and segmentation of DSC can be better realized through the methods of machine learning and deep learning, which minimize the internal information of images that are difficult for humans to discover. In the diagnosis of DSC, the use of AI to assist imaging surgeons can achieve cancer detection rapidly and effectively and save doctors' diagnosis time. These can lay the foundation for better clinical diagnosis, treatment planning and accurate quantitative evaluation of DSC.",2019-10-01,1,1361,100,585
1694,31648301,Key challenges facing data-driven multicellular systems biology,"Increasingly sophisticated experiments, coupled with large-scale computational models, have the potential to systematically test biological hypotheses to drive our understanding of multicellular systems. In this short review, we explore key challenges that must be overcome to achieve robust, repeatable data-driven multicellular systems biology. If these challenges can be solved, we can grow beyond the current state of isolated tools and datasets to a community-driven ecosystem of interoperable data, software utilities, and computational modeling platforms. Progress is within our grasp, but it will take community (and financial) commitment.",2019-10-01,10,647,63,585
1741,33693359,"TGx-DDI, a Transcriptomic Biomarker for Genotoxicity Hazard Assessment of Pharmaceuticals and Environmental Chemicals","Genotoxicity testing is an essential component of the safety assessment paradigm required by regulatory agencies world-wide for analysis of drug candidates, and environmental and industrial chemicals. Current genotoxicity testing batteries feature a high incidence of irrelevant positive findings-particularly for in vitro chromosomal damage (CD) assays. The risk management of compounds with positive in vitro findings is a major challenge and requires complex, time consuming, and costly follow-up strategies including animal testing. Thus, regulators are urgently in need of new testing approaches to meet legislated mandates. Using machine learning, we identified a set of transcripts that responds predictably to DNA-damage in human cells that we refer to as the TGx-DDI biomarker, which was originally referred to as TGx-28.65. We proposed to use this biomarker in conjunction with current genotoxicity testing batteries to differentiate compounds with irrelevant ""false"" positive findings in the in vitro CD assays from true DNA damaging agents (i.e., for de-risking agents that are clastogenic in vitro but not in vivo). We validated the performance of the TGx-DDI biomarker to identify true DNA damaging agents, assessed intra- and inter- laboratory reproducibility, and cross-platform performance. Recently, to augment the application of this biomarker, we developed a high-throughput cell-based genotoxicity testing system using the NanoString nCounter technology. Here, we review the status of TGx-DDI development, its integration in the genotoxicity testing paradigm, and progress to date in its qualification at the US Food and Drug Administration (FDA) as a drug development tool. If successfully validated and implemented, the TGx-DDI biomarker assay is expected to significantly augment the current strategy for the assessment of genotoxic hazards for drugs and chemicals.",2019-10-01,1,1890,117,585
872,31319078,Bending the Artificial Intelligence Curve for Radiology: Informatics Tools From ACR and RSNA,"Artificial intelligence (AI) will reshape radiology over the coming years. The radiology community has a strong history of embracing new technology for positive change, and AI is no exception. As with any new technology, rapid, successful implementation faces several challenges that will require creation and adoption of new integration technology. Use cases important to real-world application of AI are described, including clinical registries, AI research, AI product validation, and computer assistance for radiology reporting. Furthermore, the informatics technologies required for successful implementation of the use cases are described, including open Computer-Assisted Radiologist Decision Support, ACR Assist, ACR Data Science Institute use cases, common data elements (radelement.org), RadLex (radlex.org), LOINC/RSNA RadLex Playbook (loinc.org), and Radiology Report Templates (radreport.org).",2019-10-01,2,906,92,585
1720,31618951,A Review of Computational Methods for Cervical Cells Segmentation and Abnormality Classification,"Cervical cancer is the one of the most common cancers in women worldwide, affecting around 570,000 new patients each year. Although there have been great improvements over the years, current screening procedures can still suffer from long and tedious workflows and ambiguities. The increasing interest in the development of computer-aided solutions for cervical cancer screening is to aid with these common practical difficulties, which are especially frequent in the low-income countries where most deaths caused by cervical cancer occur. In this review, an overview of the disease and its current screening procedures is firstly introduced. Furthermore, an in-depth analysis of the most relevant computational methods available on the literature for cervical cells analysis is presented. Particularly, this work focuses on topics related to automated quality assessment, segmentation and classification, including an extensive literature review and respective critical discussion. Since the major goal of this timely review is to support the development of new automated tools that can facilitate cervical screening procedures, this work also provides some considerations regarding the next generation of computer-aided diagnosis systems and future research directions.",2019-10-01,2,1271,96,585
1708,31628551,The application of artificial neural networks in metabolomics: a historical perspective,"Background:                    Metabolomics data, with its complex covariance structure, is typically modelled by projection-based machine learning (ML) methods such as partial least squares (PLS) regression, which project data into a latent structure. Biological data are often non-linear, so it is reasonable to hypothesize that metabolomics data may also have a non-linear latent structure, which in turn would be best modelled using non-linear equations. A non-linear ML method with a similar projection equation structure to PLS is artificial neural networks (ANNs). While ANNs were first applied to metabolic profiling data in the 1990s, the lack of community acceptance combined with limitations in computational capacity and the lack of volume of data for robust non-linear model optimisation inhibited their widespread use. Due to recent advances in computational power, modelling improvements, community acceptance, and the more demanding needs for data science, ANNs have made a recent resurgence in interest across research communities, including a small yet growing usage in metabolomics. As metabolomics experiments become more complex and start to be integrated with other omics data, there is potential for ANNs to become a viable alternative to linear projection methods.              Aim of review:                    We aim to first describe ANNs and their structural equivalence to linear projection-based methods, including PLS regression. We then review the historical, current, and future uses of ANNs in the field of metabolomics.              Key scientific concept of review:                    Is metabolomics ready for the return of artificial neural networks?",2019-10-01,6,1688,87,585
1122,31386306,Improving the Rate of Translation of Tissue Engineering Products,"Over 100 000 research articles and 9000 patents have been published on tissue engineering (TE) in the past 20 years. Yet, very few TE products have made their way to the market during the same period. Experts have proposed a variety of strategies to address the lack of translation of TE products. However, since these proposals are guided by qualitative insights, they are limited in scope and impact. Machine learning is utilized in the current study to analyze the entire body of patents that have been published over the past twenty years and understand patenting trends, topics, areas of application, and exemplifications. This analysis yields surprising and little-known insights about the differences in research priorities and perceptions of innovativeness of tissue engineers in academia and industry, as well as aids to chart true advances in the field during the past twenty years. It is hoped that this analysis and subsequent proposal to improve translational rates of TE products will spur much needed dialogue about this important pursuit.",2019-10-01,3,1054,64,585
1062,31472734,Use of Big Data for Quality Assurance in Radiation Therapy,"The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.",2019-10-01,2,698,58,585
1691,31652616,"Real-Time EMG Based Pattern Recognition Control for Hand Prostheses: A Review on Existing Methods, Challenges and Future Implementation","Upper limb amputation is a condition that significantly restricts the amputees from performing their daily activities. The myoelectric prosthesis, using signals from residual stump muscles, is aimed at restoring the function of such lost limbs seamlessly. Unfortunately, the acquisition and use of such myosignals are cumbersome and complicated. Furthermore, once acquired, it usually requires heavy computational power to turn it into a user control signal. Its transition to a practical prosthesis solution is still being challenged by various factors particularly those related to the fact that each amputee has different mobility, muscle contraction forces, limb positional variations and electrode placements. Thus, a solution that can adapt or otherwise tailor itself to each individual is required for maximum utility across amputees. Modified machine learning schemes for pattern recognition have the potential to significantly reduce the factors (movement of users and contraction of the muscle) affecting the traditional electromyography (EMG)-pattern recognition methods. Although recent developments of intelligent pattern recognition techniques could discriminate multiple degrees of freedom with high-level accuracy, their efficiency level was less accessible and revealed in real-world (amputee) applications. This review paper examined the suitability of upper limb prosthesis (ULP) inventions in the healthcare sector from their technical control perspective. More focus was given to the review of real-world applications and the use of pattern recognition control on amputees. We first reviewed the overall structure of pattern recognition schemes for myo-control prosthetic systems and then discussed their real-time use on amputee upper limbs. Finally, we concluded the paper with a discussion of the existing challenges and future research recommendations.",2019-10-01,11,1877,135,585
1722,31614282,"Visual novelty, curiosity, and intrinsic reward in machine learning and the brain","A strong preference for novelty emerges in infancy and is prevalent across the animal kingdom. When incorporated into reinforcement-based machine learning algorithms, visual novelty can act as an intrinsic reward signal that vastly increases the efficiency of exploration and expedites learning, particularly in situations where external rewards are difficult to obtain. Here we review parallels between recent developments in novelty-driven machine learning algorithms and our understanding of how visual novelty is computed and signaled in the primate brain. We propose that in the visual system, novelty representations are not configured with the principal goal of detecting novel objects, but rather with the broader goal of flexibly generalizing novelty information across different states in the service of driving novelty-based learning.",2019-10-01,4,845,81,585
1091,31424335,A Roadmap for Automatic Surgical Site Infection Detection and Evaluation Using User-Generated Incision Images,"Background: Emerging technologies such as smartphones and wearable sensors have enabled the paradigm shift to new patient-centered healthcare, together with recent mobile health (mHealth) app development. One such promising healthcare app is incision monitoring based on patient-taken incision images. In this review, challenges and potential solution strategies are investigated for surgical site infection (SSI) detection and evaluation using surgical site images taken at home. Methods: Potential image quality issues, feature extraction, and surgical site image analysis challenges are discussed. Recent image analysis and machine learning solutions are reviewed to extract meaningful representations as image markers for incision monitoring. Discussions on opportunities and challenges of applying these methods to derive accurate SSI prediction are provided. Conclusions: Interactive image acquisition as well as customized image analysis and machine learning methods for SSI monitoring will play critical roles in developing sustainable mHealth apps to achieve the expected outcomes of patient-taken incision images for effective out-of-clinic patient-centered healthcare with substantially reduced cost.",2019-10-01,1,1211,109,585
2160,31737349,Pre-hospital and emergency department pathways of care for exacerbations of chronic obstructive pulmonary disease (COPD),"Exacerbations are serious complications of chronic obstructive pulmonary disease (COPD) that often require acute care from pre-hospital and emergency department (ED) services. Despite being a frequent cause of emergency presentations, gaps remain in both literature and practice for emergency care pathways of COPD exacerbations. This review seeks to address these gaps and focuses on the literature of pre-hospital and ED systems of care and how these intersect with patients experiencing an exacerbation of COPD. The literature in this area is expanding rapidly; however, more research is required to further understand exacerbations and how they are addressed by emergency medical services worldwide. For the purpose of this review, the pre-hospital domain includes ambulance and other emergency transport services, and encompasses medical interventions delivered prior to arrival at an ED or hospital. The ED domain is defined as the area of a hospital or free-standing centre where patients arrive to receive emergent medical care prior to admission. In many studies there is a significant overlap between these two domains and frequent intersection and collaboration between services. In both of these domains, for the management of COPD exacerbations, several overarching themes have been identified in the literature. These include: the appropriate delivery of oxygen in the emergency setting; strategies to improve the provision of care in accordance with diagnostic and treatment guidelines; strategies to reduce the requirement for emergency presentations; and, technological advances including machine learning which are helping to improve emergency healthcare systems.",2019-10-01,0,1681,120,585
1698,31641106,Recommendations and future directions for supervised machine learning in psychiatry,"Machine learning methods hold promise for personalized care in psychiatry, demonstrating the potential to tailor treatment decisions and stratify patients into clinically meaningful taxonomies. Subsequently, publication counts applying machine learning methods have risen, with different data modalities, mathematically distinct models, and samples of varying size being used to train and test models with the promise of clinical translation. Consequently, and in part due to the preliminary nature of such works, many studies have reported largely varying degrees of accuracy, raising concerns over systematic overestimation and methodological inconsistencies. Furthermore, a lack of procedural evaluation guidelines for non-expert medical professionals and funding bodies leaves many in the field with no means to systematically evaluate the claims, maturity, and clinical readiness of a project. Given the potential of machine learning methods to transform patient care, albeit, contingent on the rigor of employed methods and their dissemination, we deem it necessary to provide a review of current methods, recommendations, and future directions for applied machine learning in psychiatry. In this review we will cover issues of best practice for model training and evaluation, sources of systematic error and overestimation, model explainability vs. trust, the clinical implementation of AI systems, and finally, future directions for our field.",2019-10-01,9,1451,83,585
1777,31543209,Research Techniques Made Simple: Feature Selection for Biomarker Discovery,"Molecular biomarkers can be powerful tools for aiding in the efficiency and precision of clinical decision-making. Feature selection methods, machine-learning, and biostatistics have been applied to discover subsets of molecular markers that identify target classes of clinical cases. For example, in the field of dermatology, these approaches have been used to develop predictive models that identify skin diseases, ranging from melanoma to psoriasis, based upon a variety of biomarkers. However, a continuous increase in the variety and size of datasets from which candidate biomarkers can be derived, and limitations in the computational tools used to analyze them, have hindered the interpretability of biomarker discovery studies. In this article, the various methods of feature selection are described along with the important steps needed to properly validate the performance of the selected methods. Limitations and suggestions toward uses of these methods are discussed.",2019-10-01,1,979,74,585
1700,31635167,A Review of Force Myography Research and Development,"Information about limb movements can be used for monitoring physical activities or for human-machine-interface applications. In recent years, a technique called Force Myography (FMG) has gained ever-increasing traction among researchers to extract such information. FMG uses force sensors to register the variation of muscle stiffness patterns around a limb during different movements. Using machine learning algorithms, researchers are able to predict many different limb activities. This review paper presents state-of-art research and development on FMG technology in the past 20 years. It summarizes the research progress in both the hardware design and the signal processing techniques. It also discusses the challenges that need to be solved before FMG can be used in an everyday scenario. This paper aims to provide new insight into FMG technology and contribute to its advancement.",2019-10-01,9,889,52,585
1107,31408024,Primer on machine learning: utilization of large data set analyses to individualize pain management,"Purpose of review:                    Pain researchers and clinicians increasingly encounter machine learning algorithms in both research methods and clinical practice. This review provides a summary of key machine learning principles, as well as applications to both structured and unstructured datasets.              Recent findings:                    Aside from increasing use in the analysis of electronic health record data, machine and deep learning algorithms are now key tools in the analyses of neuroimaging and facial expression recognition data used in pain research.              Summary:                    In the coming years, machine learning is likely to become a key component of evidence-based medicine, yet will require additional skills and perspectives for its successful and ethical use in research and clinical settings.",2019-10-01,1,844,99,585
1703,31632973,Translational AI and Deep Learning in Diagnostic Pathology,"There has been an exponential growth in the application of AI in health and in pathology. This is resulting in the innovation of deep learning technologies that are specifically aimed at cellular imaging and practical applications that could transform diagnostic pathology. This paper reviews the different approaches to deep learning in pathology, the public grand challenges that have driven this innovation and a range of emerging applications in pathology. The translation of AI into clinical practice will require applications to be embedded seamlessly within digital pathology workflows, driving an integrated approach to diagnostics and providing pathologists with new tools that accelerate workflow and improve diagnostic consistency and reduce errors. The clearance of digital pathology for primary diagnosis in the US by some manufacturers provides the platform on which to deliver practical AI. AI and computational pathology will continue to mature as researchers, clinicians, industry, regulatory organizations and patient advocacy groups work together to innovate and deliver new technologies to health care providers: technologies which are better, faster, cheaper, more precise, and safe.",2019-10-01,19,1204,58,585
2443,30762522,M/EEG-Based Bio-Markers to Predict the MCI and Alzheimer's Disease: A Review From the ML Perspective,"This paper reviews the state-of-the-art neuromarkers development for the prognosis of Alzheimer's disease (AD) and mild cognitive impairment (MCI). The first part of this paper is devoted to reviewing the recently emerged machine learning (ML) algorithms based on electroencephalography (EEG) and magnetoencephalography (MEG) modalities. In particular, the methods are categorized by different types of neuromarkers. The second part of the review is dedicated to a series of investigations that further highlight the differences between these two modalities. First, several source reconstruction methods are reviewed and their source-level performances explored, followed by an objective comparison between EEG and MEG from multiple perspectives. Finally, a number of the most recent reports on classification of MCI/AD during resting state using EEG/MEG are documented to show the up-to-date performance for this well-recognized data collecting scenario. It is noticed that the MEG modality may be particularly effective in distinguishing between subjects with MCI and healthy controls, a high classification accuracy of more than 98% was reported recently; whereas the EEG seems to be performing well in classifying AD and healthy subjects, which also reached around 98% of the accuracy. A number of influential factors have also been raised and suggested for careful considerations while evaluating the ML-based diagnosis systems in the real-world scenarios.",2019-10-01,5,1461,100,585
1066,31463515,Challenges of big data integration in the life sciences,"Big data has been reported to be revolutionizing many areas of life, including science. It summarizes data that is unprecedentedly large, rapidly generated, heterogeneous, and hard to accurately interpret. This availability has also brought new challenges: How to properly annotate data to make it searchable? What are the legal and ethical hurdles when sharing data? How to store data securely, preventing loss and corruption? The life sciences are not the only disciplines that must align themselves with big data requirements to keep up with the latest developments. The large hadron collider, for instance, generates research data at a pace beyond any current biomedical research center. There are three recent major coinciding events that explain the emergence of big data in the context of research: the technological revolution for data generation, the development of tools for data analysis, and a conceptual change towards open science and data. The true potential of big data lies in pattern discovery in large datasets, as well as the formulation of new models and hypotheses. Confirmation of the existence of the Higgs boson, for instance, is one of the most recent triumphs of big data analysis in physics. Digital representations of biological systems have become more comprehensive. This, in combination with advances in machine learning, creates exciting new research possibilities. In this paper, we review the state of big data in bioanalytical research and provide an overview of the guidelines for its proper usage.",2019-10-01,1,1535,55,585
1072,31451418,Multigene signatures of responses to chemotherapy derived by biochemically-inspired machine learning,"Pharmacogenomic responses to chemotherapy drugs can be modeled by supervised machine learning of expression and copy number of relevant gene combinations. Such biochemical evidence can form the basis of derived gene signatures using cell line data, which can subsequently be examined in patients that have been treated with the same drugs. These gene signatures typically contain elements of multiple biochemical pathways which together comprise multiple origins of drug resistance or sensitivity. The signatures can capture variation in these responses to the same drug among different patients.",2019-10-01,2,596,100,585
1711,31623106,Proteomic Biomarkers for the Detection of Endometrial Cancer,"Endometrial cancer is the leading gynaecological malignancy in the western world and its incidence is rising in tandem with the global epidemic of obesity. Early diagnosis is key to improving survival, which at 5 years is less than 20% in advanced disease and over 90% in early-stage disease. As yet, there are no validated biological markers for its early detection. Advances in high-throughput technologies and machine learning techniques now offer unique and promising perspectives for biomarker discovery, especially through the integration of genomic, transcriptomic, proteomic, metabolomic and imaging data. Because the proteome closely mirrors the dynamic state of cells, tissues and organisms, proteomics has great potential to deliver clinically relevant biomarkers for cancer diagnosis. In this review, we present the current progress in endometrial cancer diagnostic biomarker discovery using proteomics. We describe the various mass spectrometry-based approaches and highlight the challenges inherent in biomarker discovery studies. We suggest novel strategies for endometrial cancer detection exploiting biologically important protein biomarkers and set the scene for future directions in endometrial cancer biomarker research.",2019-10-01,12,1240,60,585
1704,31632915,AI Meets Exascale Computing: Advancing Cancer Research With Large-Scale High Performance Computing,"The application of data science in cancer research has been boosted by major advances in three primary areas: (1) Data: diversity, amount, and availability of biomedical data; (2) Advances in Artificial Intelligence (AI) and Machine Learning (ML) algorithms that enable learning from complex, large-scale data; and (3) Advances in computer architectures allowing unprecedented acceleration of simulation and machine learning algorithms. These advances help build in silico ML models that can provide transformative insights from data including: molecular dynamics simulations, next-generation sequencing, omics, imaging, and unstructured clinical text documents. Unique challenges persist, however, in building ML models related to cancer, including: (1) access, sharing, labeling, and integration of multimodal and multi-institutional data across different cancer types; (2) developing AI models for cancer research capable of scaling on next generation high performance computers; and (3) assessing robustness and reliability in the AI models. In this paper, we review the National Cancer Institute (NCI) -Department of Energy (DOE) collaboration, Joint Design of Advanced Computing Solutions for Cancer (JDACS4C), a multi-institution collaborative effort focused on advancing computing and data technologies to accelerate cancer research on three levels: molecular, cellular, and population. This collaboration integrates various types of generated data, pre-exascale compute resources, and advances in ML models to increase understanding of basic cancer biology, identify promising new treatment options, predict outcomes, and eventually prescribe specialized treatments for patients with cancer.",2019-10-01,0,1700,98,585
925,31230937,Best practices in digital health literacy,"The connection between health literacy and health outcomes includes access and utilization of healthcare services, patient/provider interaction and self-care. Digital approaches can be designed to simplify or expand on a concept, test for understanding, and do not have a time constraint. New technologies, such as artificial intelligence and machine learning, virtual and augmented reality, and blockchain can move the role of technology beyond data collection to a more integrated system. Rather than being a passive participant, digital solutions provide the opportunity for the individual to be an active participant in their health. These solutions can be delivered in a way that builds or enhances the individual's belief that the plan will be successful and more confidence that they can stick with it. Digital solutions allow for the delivery of multi-media education, such as videos, voice, and print, at different reading levels, in multiple languages, using formal and informal teaching methods. By giving the patient a greater voice and empowering them to be active participants in their care, they can develop their decision making and shared decision making skills. The first step in our health literacy instructional model is to address the emotional state of the person. Once the emotional state has been addressed, and an engagement strategy has been deployed the final phase is the delivery of an educational solution. While a clear definition of health literacy and an instructional model are important, further research must be done to continually determine more effective ways to incorporate health technology in the process of improving health outcomes.",2019-10-01,2,1675,41,585
1737,31593641,ASK1 and its role in cardiovascular and other disorders: available treatments and future prospects,"Introduction: Apoptosis signal-regulating kinase 1 (ASK1), also known as MAP3K5, is a member of mitogen-activated protein kinase kinase kinase (MAP3K) family and is well reported as crucial in the regulation of the JNK and P38 pathways. ASK1 is activated in response to a diverse array of stresses such as endoplasmic reticulum stress, lipopolysaccharides, tumor necrosis factor alpha, and reactive oxygen species. The activation of ASK1 induces various stress responses. Areas covered: Considering ASK1 as an important therapeutic drug target, here we have discussed the role of ASK1 in the progression of various diseases. We have also provided an overview of the available inhibitors for ASK1. The success of computational-based approaches toward ASK1 inhibitor design has also been discussed. Expert opinion: A number of reports have outlined the prominent role of ASK1 in the pathogenesis of several diseases. The discovery of novel ASK1 inhibitors would have a wide range of applications in medical science. In-silico techniques have been successfully used in the design of some novel ASK1 inhibitors. The use of machine learning-based approaches in combination with structure-based virtual screening (SBVS) and ligand-based virtual screening (LBVS) will be helpful toward the development of potent ASK1 inhibitors.",2019-10-01,1,1321,98,585
914,31253449,Artificial Intelligence and Machine Learning in Lower Extremity Arthroplasty: A Review,"Background:                    Driven by the rapid development of big data and processing power, artificial intelligence and machine learning (ML) applications are poised to expand orthopedic surgery frontiers. Lower extremity arthroplasty is uniquely positioned to most dramatically benefit from ML applications given its central role in alternative payment models and the value equation.              Methods:                    In this report, we discuss the origins and model specifics behind machine learning, consider its progression into healthcare, and present some of its most recent advances and applications in arthroplasty.              Results:                    A narrative review of artificial intelligence and ML developments is summarized with specific applications to lower extremity arthroplasty, with specific lessons learned from osteoarthritis gait models, joint-specific imaging analysis, and value-based payment models.              Conclusion:                    The advancement and employment of ML provides an opportunity to provide data-driven, high performance medicine that can rapidly improve the science, economics, and delivery of lower extremity arthroplasty.",2019-10-01,5,1194,86,585
1705,31632910,Deep Learning: A Review for the Radiation Oncologist,"Introduction: Deep Learning (DL) is a machine learning technique that uses deep neural networks to create a model. The application areas of deep learning in radiation oncology include image segmentation and detection, image phenotyping, and radiomic signature discovery, clinical outcome prediction, image dose quantification, dose-response modeling, radiation adaptation, and image generation. In this review, we explain the methods used in DL and perform a literature review using the Medline database to identify studies using deep learning in radiation oncology. The search was conducted in April 2018, and identified studies published between 1997 and 2018, strongly skewed toward 2015 and later. Methods: A literature review was performed using PubMed/Medline in order to identify important recent publications to be synthesized into a review of the current status of Deep Learning in radiation oncology, directed at a clinically-oriented reader. The search strategy included the search terms ""radiotherapy"" and ""deep learning."" In addition, reference lists of selected articles were hand searched for further potential hits of relevance to this review. The search was conducted in April 2018, and identified studies published between 1997 and 2018, strongly skewed toward 2015 and later. Results: Studies using DL for image segmentation were identified in Brain (n = 2), Head and Neck (n = 3), Lung (n = 6), Abdominal (n = 2), and Pelvic (n = 6) cancers. Use of Deep Learning has also been reported for outcome prediction, such as toxicity modeling (n = 3), treatment response and survival (n = 2), or treatment planning (n = 5). Conclusion: Over the past few years, there has been a significant number of studies assessing the performance of DL techniques in radiation oncology. They demonstrate how DL-based systems can aid clinicians in their daily work, be it by reducing the time required for or the variability in segmentation, or by helping to predict treatment outcomes and toxicities. It still remains to be seen when these techniques will be employed in routine clinical practice.",2019-10-01,10,2097,52,585
1713,31621574,Phenotypes of osteoarthritis: current state and future implications,"In the most recent years, an extraordinary research effort has emerged to disentangle osteoarthritis heterogeneity, opening new avenues for progressing with therapeutic development and unravelling the pathogenesis of this complex condition. Several phenotypes and endotypes have been proposed albeit none has been sufficiently validated for clinical or research use as yet. This review discusses the latest advances in OA phenotyping including how new modern statistical strategies based on machine learning and big data can help advance this field of research.",2019-10-01,8,561,67,585
1739,31590664,Machine learning in cardiovascular magnetic resonance: basic concepts and applications,"Machine learning (ML) is making a dramatic impact on cardiovascular magnetic resonance (CMR) in many ways. This review seeks to highlight the major areas in CMR where ML, and deep learning in particular, can assist clinicians and engineers in improving imaging efficiency, quality, image analysis and interpretation, as well as patient evaluation. We discuss recent developments in the field of ML relevant to CMR in the areas of image acquisition & reconstruction, image analysis, diagnostic evaluation and derivation of prognostic information. To date, the main impact of ML in CMR has been to significantly reduce the time required for image segmentation and analysis. Accurate and reproducible fully automated quantification of left and right ventricular mass and volume is now available in commercial products. Active research areas include reduction of image acquisition and reconstruction time, improving spatial and temporal resolution, and analysis of perfusion and myocardial mapping. Although large cohort studies are providing valuable data sets for ML training, care must be taken in extending applications to specific patient groups. Since ML algorithms can fail in unpredictable ways, it is important to mitigate this by open source publication of computational processes and datasets. Furthermore, controlled trials are needed to evaluate methods across multiple centers and patient groups.",2019-10-01,13,1406,86,585
1763,31554575,Cardiovascular models for personalised medicine: Where now and where next?,"The aim of this position paper is to provide a brief overview of the current status of cardiovascular modelling and of the processes required and some of the challenges to be addressed to see wider exploitation in both personal health management and clinical practice. In most branches of engineering the concept of the digital twin, informed by extensive and continuous monitoring and coupled with robust data assimilation and simulation techniques, is gaining traction: the Gartner Group listed it as one of the top ten digital trends in 2018. The cardiovascular modelling community is starting to develop a much more systematic approach to the combination of physics, mathematics, control theory, artificial intelligence, machine learning, computer science and advanced engineering methodology, as well as working more closely with the clinical community to better understand and exploit physiological measurements, and indeed to develop jointly better measurement protocols informed by model-based understanding. Developments in physiological modelling, model personalisation, model outcome uncertainty, and the role of models in clinical decision support are addressed and 'where-next' steps and challenges discussed.",2019-10-01,2,1222,74,585
1719,31620840,Artificial intelligence applications for pediatric oncology imaging,"Machine learning algorithms can help to improve the accuracy and efficiency of cancer diagnosis, selection of personalized therapies and prediction of long-term outcomes. Artificial intelligence (AI) describes a subset of machine learning that can identify patterns in data and take actions to reach pre-set goals without specific programming. Machine learning tools can help to identify high-risk populations, prescribe personalized screening tests and enrich patient populations that are most likely to benefit from advanced imaging tests. AI algorithms can also help to plan personalized therapies and predict the impact of genomic variations on the sensitivity of normal and tumor tissue to chemotherapy or radiation therapy. The two main bottlenecks for successful AI applications in pediatric oncology imaging to date are the needs for large data sets and appropriate computer and memory power. With appropriate data entry and processing power, deep convolutional neural networks (CNNs) can process large amounts of imaging data, clinical data and medical literature in very short periods of time and thereby accelerate literature reviews, correct diagnoses and personalized treatments. This article provides a focused review of emerging AI applications that are relevant for the pediatric oncology imaging community.",2019-10-01,4,1323,67,585
856,31344655,Engineering approaches for characterizing soft tissue mechanical properties: A review,"From cancer diagnosis to detailed characterization of arterial wall biomechanics, the elastic property of tissues is widely studied as an early sign of disease onset. The fibrous structural features of tissues are a direct measure of its health and functionality. Alterations in the structural features of tissues are often manifested as local stiffening and are early signs for diagnosing a disease. These elastic properties are measured ex vivo in conventional mechanical testing regimes, however, the heterogeneous microstructure of tissues can be accurately resolved over relatively smaller length scales with enhanced spatial resolution using techniques such as micro-indentation, microelectromechanical (MEMS) based cantilever sensors and optical catheters which also facilitate in vivo assessment of mechanical properties. In this review, we describe several probing strategies (qualitative and quantitative) based on the spatial scale of mechanical assessment and also discuss the potential use of machine learning techniques to compute the mechanical properties of soft tissues. This work details state of the art advancement in probing strategies, associated challenges toward quantitative characterization of tissue biomechanics both from an engineering and clinical standpoint.",2019-10-01,2,1289,85,585
2135,31777668,Artificial Intelligence Applications in Type 2 Diabetes Mellitus Care: Focus on Machine Learning Methods,"Objectives:                    The incidence of type 2 diabetes mellitus has increased significantly in recent years. With the development of artificial intelligence applications in healthcare, they are used for diagnosis, therapeutic decision making, and outcome prediction, especially in type 2 diabetes mellitus. This study aimed to identify the artificial intelligence (AI) applications for type 2 diabetes mellitus care.              Methods:                    This is a review conducted in 2018. We searched the PubMed, Web of Science, and Embase scientific databases, based on a combination of related mesh terms. The article selection process was based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). Finally, 31 articles were selected after inclusion and exclusion criteria were applied. Data gathering was done by using a data extraction form. Data were summarized and reported based on the study objectives.              Results:                    The main applications of AI for type 2 diabetes mellitus care were screening and diagnosis in different stages. Among all of the reviewed AI methods, machine learning methods with 71% (n = 22) were the most commonly applied techniques. Many applications were in multi method forms (23%). Among the machine learning algorithms applications, support vector machine (21%) and naive Bayesian (19%) were the most commonly used methods. The most important variables that were used in the selected studies were body mass index, fasting blood sugar, blood pressure, HbA1c, triglycerides, low-density lipoprotein, high-density lipoprotein, and demographic variables.              Conclusions:                    It is recommended to select optimal algorithms by testing various techniques. Support vector machine and naive Bayesian might achieve better performance than other applications due to the type of variables and targets in diabetes-related outcomes classification.",2019-10-01,5,1958,104,585
2521,30591356,Big Data Analysis and Machine Learning in Intensive Care Units,"Intensive care is an ideal environment for the use of Big Data Analysis (BDA) and Machine Learning (ML), due to the huge amount of information processed and stored in electronic format in relation to such care. These tools can improve our clinical research capabilities and clinical decision making in the future. The present study reviews the foundations of BDA and ML, and explores possible applications in our field from a clinical viewpoint. We also suggest potential strategies to optimize these new technologies and describe a new kind of hybrid healthcare-data science professional with a linking role between clinicians and data.",2019-10-01,4,637,62,585
2176,31695786,Current status and future trends of clinical diagnoses via image-based deep learning,"With the recent developments in deep learning technologies, artificial intelligence (AI) has gradually been transformed from cutting-edge technology into practical applications. AI plays an important role in disease diagnosis and treatment, health management, drug research and development, and precision medicine. Interdisciplinary collaborations will be crucial to develop new AI algorithms for medical applications. In this paper, we review the basic workflow for building an AI model, identify publicly available databases of ocular fundus images, and summarize over 60 papers contributing to the field of AI development.",2019-10-01,13,625,84,585
1689,31656552,A comprehensive survey of error measures for evaluating binary decision making in data science,"Binary decision making is a topic of great interest for many fields, including biomedical science, economics, management, politics, medicine, natural science and social science, and much effort has been spent for developing novel computational methods to address problems arising in the aforementioned fields. However, in order to evaluate the effectiveness of any prediction method for binary decision making, the choice of the most appropriate error measures is of paramount importance. Due to the variety of error measures available, the evaluation process of binary decision making can be a complex task. The main objective of this study is to provide a comprehensive survey of error measures for evaluating the outcome of binary decision making applicable to many data-driven fields. This article is categorized under: Fundamental Concepts of Data and Knowledge > Key Design Issues in Data MiningTechnologies > PredictionAlgorithmic Development > Statistics.",2019-10-01,5,963,94,585
2180,31683734,Plant Disease Detection and Classification by Deep Learning,"Plant diseases affect the growth of their respective species, therefore their early identification is very important. Many Machine Learning (ML) models have been employed for the detection and classification of plant diseases but, after the advancements in a subset of ML, that is, Deep Learning (DL), this area of research appears to have great potential in terms of increased accuracy. Many developed/modified DL architectures are implemented along with several visualization techniques to detect and classify the symptoms of plant diseases. Moreover, several performance metrics are used for the evaluation of these architectures/techniques. This review provides a comprehensive explanation of DL models used to visualize various plant diseases. In addition, some research gaps are identified from which to obtain greater transparency for detecting diseases in plants, even before their symptoms appear clearly.",2019-10-01,14,914,59,585
2191,31661028,Measuring the impact of screening automation on meta-analyses of diagnostic test accuracy,"Background:                    The large and increasing number of new studies published each year is making literature identification in systematic reviews ever more time-consuming and costly. Technological assistance has been suggested as an alternative to the conventional, manual study identification to mitigate the cost, but previous literature has mainly evaluated methods in terms of recall (search sensitivity) and workload reduction. There is a need to also evaluate whether screening prioritization methods leads to the same results and conclusions as exhaustive manual screening. In this study, we examined the impact of one screening prioritization method based on active learning on sensitivity and specificity estimates in systematic reviews of diagnostic test accuracy.              Methods:                    We simulated the screening process in 48 Cochrane reviews of diagnostic test accuracy and re-run 400 meta-analyses based on a least 3 studies. We compared screening prioritization (with technological assistance) and screening in randomized order (standard practice without technology assistance). We examined if the screening could have been stopped before identifying all relevant studies while still producing reliable summary estimates. For all meta-analyses, we also examined the relationship between the number of relevant studies and the reliability of the final estimates.              Results:                    The main meta-analysis in each systematic review could have been performed after screening an average of 30% of the candidate articles (range 0.07 to 100%). No systematic review would have required screening more than 2308 studies, whereas manual screening would have required screening up to 43,363 studies. Despite an average 70% recall, the estimation error would have been 1.3% on average, compared to an average 2% estimation error expected when replicating summary estimate calculations.              Conclusion:                    Screening prioritization coupled with stopping criteria in diagnostic test accuracy reviews can reliably detect when the screening process has identified a sufficient number of studies to perform the main meta-analysis with an accuracy within pre-specified tolerance limits. However, many of the systematic reviews did not identify a sufficient number of studies that the meta-analyses were accurate within a 2% limit even with exhaustive manual screening, i.e., using current practice.",2019-10-01,0,2471,89,585
2192,31661003,Recent advances in nanotheranostics for triple negative breast cancer treatment,"Triple-negative breast cancer (TNBC) is the most complex and aggressive type of breast cancer encountered world widely in women. Absence of hormonal receptors on breast cancer cells necessitates the chemotherapy as the only treatment regime. High propensity to metastasize and relapse in addition to poor prognosis and survival motivated the oncologist, nano-medical scientist to develop novel and efficient nanotherapies to solve such a big TNBC challenge. Recently, the focus for enhanced availability, targeted cellular uptake with minimal toxicity is achieved by nano-carriers. These smart nano-carriers carrying all the necessary arsenals (drugs, tracking probe, and ligand) designed in such a way that specifically targets the TNBC cells at site. Articulating the targeted delivery system with multifunctional molecules for high specificity, tracking, diagnosis, and treatment emerged as theranostic approach. In this review, in addition to classical treatment modalities, recent advances in nanotheranostics for early and effective diagnostic and treatment is discussed. This review highlighted the recently FDA approved immunotherapy and all the ongoing clinical trials for TNBC, in addition to nanoparticle assisted immunotherapy. Futuristic but realistic advancements in artificial intelligence (AI) and machine learning not only improve early diagnosis but also assist clinicians for their workup in TNBC. The novel concept of Nanoparticles induced endothelial leakiness (NanoEL) as a way of tumor invasion is also discussed in addition to classical EPR effect. This review intends to provide basic insight and understanding of the novel nano-therapeutic modalities in TNBC diagnosis and treatment and to sensitize the readers for continue designing the novel nanomedicine. This is the first time that designing nanoparticles with stoichiometric definable number of antibodies per nanoparticle now represents the next level of precision by design in nanomedicine.",2019-10-01,20,1974,79,585
2204,31209389,Completing the picture through correlative characterization,"Natural and manufactured materials rely on complex hierarchical microstructures to deliver a suite of interesting properties. To predict and tailor their performance requires a joined-up knowledge of their multiphase microstructure, interfaces, chemistry and crystallography from the nanoscale to the macroscale. This Perspective reflects on how recent developments in correlative characterization can bring together multiple image modalities and maps of the local chemistry, structure and functionality to form rich multimodal and multiscale correlated datasets. The automated collection and digitization of multidimensional data is an essential part of the picture for developing multiscale modelling and 'big data'-driven machine learning approaches. These are needed to both improve our understanding of existing materials and exploit high-throughput combinatorial synthesis, processing and testing methods to develop materials with bespoke properties.",2019-10-01,2,956,59,585
2222,31173533,Biomicrofluidic Systems for Hematologic Cancer Research and Clinical Applications,"A persistent challenge in developing personalized treatments for hematologic cancers is the lack of patient specific, physiologically relevant disease models to test investigational drugs in clinical trials and to select therapies in a clinical setting. Biomicrofluidic systems and organ-on-a-chip technologies have the potential to change how researchers approach the fundamental study of hematologic cancers and select clinical treatment for individual patient. Here, we review microfluidics cell-based technology with application toward studying hematologic tumor microenvironments (TMEs) for the purpose of drug discovery and clinical treatment selection. We provide an overview of state-of-the-art microfluidic systems designed to address questions related to hematologic TMEs and drug development. Given the need to develop personalized treatment platforms involving this technology, we review pharmaceutical drugs and different modes of immunotherapy for hematologic cancers, followed by key considerations for developing a physiologically relevant microfluidic companion diagnostic tool for mimicking different hematologic TMEs for testing with different drugs in clinical trials. Opportunities lie ahead for engineers to revolutionize conventional drug discovery strategies of hematologic cancers, including integrating cell-based microfluidics technology with machine learning and automation techniques, which may stimulate pharma and regulatory bodies to promote research and applications of microfluidics technology for drug development.",2019-10-01,2,1549,81,585
2228,31158512,Ontology mapping for semantically enabled applications,"In this review, we provide a summary of recent progress in ontology mapping (OM) at a crucial time when biomedical research is under a deluge of an increasing amount and variety of data. This is particularly important for realising the full potential of semantically enabled or enriched applications and for meaningful insights, such as drug discovery, using machine-learning technologies. We discuss challenges and solutions for better ontology mappings, as well as how to select ontologies before their application. In addition, we describe tools and algorithms for ontology mapping, including evaluation of tool capability and quality of mappings. Finally, we outline the requirements for an ontology mapping service (OMS) and the progress being made towards implementation of such sustainable services.",2019-10-01,3,806,54,585
2230,31157465,Distance versus Capillary Flow Dynamics-Based Detection Methods on a Microfluidic Paper-Based Analytical Device (PAD),"In recent years, there has been high interest in paper-based microfluidic sensors or microfluidic paper-based analytical devices (PADs) towards low-cost, portable, and easy-to-use sensing for chemical and biological targets. PAD allows spontaneous liquid flow without any external or internal pumping, as well as an innate filtration capability. Although both optical (colorimetric and fluorescent) and electrochemical detection have been demonstrated on PADs, several limitations still remain, such as the need for additional equipment, vulnerability to ambient lighting perturbation, and inferior sensitivity. Herein, alternative detection methods on PADs are reviewed to resolve these issues, including relatively well studied distance-based measurements and the newer capillary flow dynamics-based method. Detection principles, assay performance, strengths, and weaknesses are explained for these methods, along with their potential future applications towards point-of-care medical diagnostics and other field-based applications.",2019-10-01,1,1038,118,585
2242,31144302,Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence-based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM-Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use.  2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.",2019-10-01,14,1101,91,585
403,30417258,Applying machine learning to continuously monitored physiological data,"The use of machine learning (ML) in healthcare has enormous potential for improving disease detection, clinical decision support, and workflow efficiencies. In this commentary, we review published and potential applications for the use of ML for monitoring within the hospital environment. We present use cases as well as several questions regarding the application of ML to the analysis of the vast amount of complex data that clinicians must interpret in the realm of continuous physiological monitoring. ML, especially employed in bidirectional conjunction with electronic health record data, has the potential to extract much more useful information out of this currently under-analyzed data source from a population level. As a data driven entity, ML is dependent on copious, high quality input data so that error can be introduced by low quality data sources. At present, while ML is being studied in hybrid formulations along with static expert systems for monitoring applications, it is not yet actively incorporated in the formal artificial learning sense of an algorithm constantly learning and updating its rules without external intervention. Finally, innovations in monitoring, including those supported by ML, will pose regulatory and medico-legal challenges, as well as questions regarding precisely how to incorporate these features into clinical care and medical education. Rigorous evaluation of ML techniques compared to traditional methods or other AI methods will be required to validate the algorithms developed with consideration of database limitations and potential learning errors. Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development: Future research is needed to evaluate all AI based programs before clinical implementation in non-research settings.",2019-10-01,9,1863,70,585
521,31029649,"Advanced atherosclerosis imaging by CT: Radiomics, machine learning and deep learning","In the last decade, technical advances in the field of medical imaging significantly improved and broadened the application of coronary CT angiography (CCTA) for the non-invasive assessment of coronary artery disease. Recently, similar breakthroughs are happening in the post-processing, analysis and interpretation of radiological images. Technologies such as radiomics allow to extract significantly more information from scans than what human visual assessment is capable of. This allows the precision phenotyping of diseases based on medical images. The increased amount of information can then be analyzed using novel data analytic techniques such as machine learning (ML) and deep learning (DL), which utilize the power of big data to build predictive models, which seek to mimic human intelligence, artificially. Thanks to big data availability and increased computational power, these novel analytic methods are outperforming conventional statistical techniques. In this current overview we describe the basics of radiomics, ML and DL, highlighting similarities, differences, limitations and potential pitfalls of these techniques. In addition, we provide a brief overview of recently published results on the applications of the aforementioned techniques for the non-invasive assessment of coronary atherosclerosis using CCTA.",2019-10-01,7,1335,85,585
1085,31434042,Ambulatory cardiac bio-signals: From mirage to clinical reality through a decade of progress,"Background:                    Health monitoring is shifting towards continuous, ambulatory and clinically comparable wearable devices. Telemedicine and remote diagnosis could harness the capability of mobile cardiac health information, as the technology on bio-physical signal monitoring has improved significantly.              Objectives:                    The purpose of this review article is (1) to systematically assess the viability of ambulatory electrocardiography (ECG), (2) to provide a systems level understanding of a broad spectrum of wearable heart signal monitoring approaches and (3) to identify areas of improvement in the existing technology needed to attain clinical grade diagnosis.              Results:                    Based on the included literature, we have identified (1) that the developments in ECG monitoring through wearable devices are reaching feasibility, and are capable of delivering diagnostic and prognostic information, (2) that reliable sensing is the major bottleneck in the entire process of ambulatory monitoring, (3) that there is a strong need for artificial intelligence and machine learning techniques to parse and infer the biosignals and (4) that aspects of wearer comfort has largely been ignored in the prevailing developments, which can become a key factor for consumer acceptance.              Conclusions:                    Cardiac health information is crucial for diagnosis and prevention of several disease onsets. Mobile and continuous monitoring can aid avoiding risks involved with acute symptoms. The health information obtained through continuous monitoring can serve as the BigData of heart signals, and can facilitate new treatment methods and devise effective health policies.",2019-10-01,1,1747,92,585
1046,31493727,Screening mammography beyond breast cancer: breast arterial calcifications as a sex-specific biomarker of cardiovascular risk,"Purpose:                    To highlight the importance of quantitative breast arterial calcifications (BAC) assessment for an effective stratification of cardiovascular (CV) risk in women, for whom current preventive strategies are inadequate. BAC, easily detectable on mammograms, are associated with CV disease and represent a potential imaging biomarker for CV disease prevention in women.              Method:                    We summarized the available evidence on this topic.              Results:                    Age, parity, diabetes, and hyperlipidemia were found to positively correlate with BAC. Women with BAC have a higher CV risk than those without BAC: the relative risk was reported to be 1.4 for transient ischemic attack/stroke, 1.5 for thrombosis, 1.8 for myocardial infarction; the reported hazard ratio was 1.32 for coronary artery disease (CAD), 1.52 for heart failure, 1.29 for CV death, 1.44 for death from CAD. However, BAC do not alarm radiologists; when reported, they are commonly mentioned as ""present"", not impacting on CV decision-making. Of 18 published studies, 9 reported only presence/absence of BAC, 4 used a semi-quantitative scale, and 5 a continuous scale (with manual, automatic or semiautomatic segmentation). Various appearance, topological complexity, and vessels overlap make BAC quantification difficult to standardize. Nevertheless, machine learning approaches showed promising results in BAC quantification on mammograms.              Conclusions:                    There is a strong rationale for mammography to become a dual test for breast cancer screening and CV disease prevention. However, robust and automated quantification methods are needed for a deeper insight on the association between BAC and CV disease, to stratifying CV risk and define personalized preventive actions.",2019-10-01,3,1840,125,585
559,30970326,Artificial intelligence in reproductive medicine,"Artificial intelligence (AI) has experienced rapid growth over the past few years, moving from the experimental to the implementation phase in various fields, including medicine. Advances in learning algorithms and theories, the availability of large datasets and improvements in computing power have contributed to breakthroughs in current AI applications. Machine learning (ML), a subset of AI, allows computers to detect patterns from large complex datasets automatically and uses these patterns to make predictions. AI is proving to be increasingly applicable to healthcare, and multiple machine learning techniques have been used to improve the performance of assisted reproductive technology (ART). Despite various challenges, the integration of AI and reproductive medicine is bound to give an essential direction to medical development in the future. In this review, we discuss the basic aspects of AI and machine learning, and we address the applications, potential limitations and challenges of AI. We also highlight the prospects and future directions in the context of reproductive medicine.",2019-10-01,2,1103,48,585
851,31350037,Technology approaches to digital health literacy,"Digital health literacy is an extension of health literacy and uses the same operational definition, but in the context of technology. Technology solutions have the potential to both promote health literacy or be a barrier. To be effective, health technology solutions should go beyond building literacy and numeracy skills to functional and critical skills, such as navigating the healthcare system, communication with healthcare providers, and shared decision making. New and emerging technologies are highlighted: AI/machine learning, voice first, remote patient monitoring, wearables, and apps and web sites. Health technology represents enormous promise in the building of digital health literacy skills and improved health outcomes in patients with cardiovascular and other chronic conditions. This is a promise, however, that is yet to be fulfilled. TOPICS: Hypertension, Rehabilitation, Metabolic Syndrome, Health Policy, Risk Factor.",2019-10-01,5,942,48,585
844,31362251,The effect of short-term exposure to the natural environment on depressive mood: A systematic review and meta-analysis,"Background:                    Research suggests that exposure to the natural environment can improve mood, however, current reviews are limited in scope and there is little understanding of moderators.              Objective:                    To conduct a comprehensive systematic review and meta-analysis of the evidence for the effect of short-term exposure to the natural environment on depressive mood.              Methods:                    Five databases were systematically searched for relevant studies published up to March 2018. Risk of bias was evaluated using the Cochrane Risk of Bias (ROB) tool 1.0 and the Risk of Bias in Non-Randomised Studies of Interventions (ROBINS-I) tool where appropriate. The Grades of Recommendation, Assessment, Development, and Evaluation (GRADE) approach was used to assess the quality of evidence overall. A random-effects meta-analysis was performed. 20 potential moderators of the effect size were coded and the machine learning-based MetaForest algorithm was used to identify relevant moderators. These were then entered into a meta-regression.              Results:                    33 studies met the inclusion criteria. Effect sizes ranged from -2.30 to 0.84, with an unweighted mean effect size of Mg=-0.29,SD=0.60. However, there was significant residual heterogeneity between studies and risk of bias was high. Type of natural environment, type of built environment, gender mix of the sample, and region of study origin, among others, were identified as relevant moderators but were not significant when entered in a meta-regression. The quality of evidence was rated very low to low. An assessment of publication bias was inconclusive.              Conclusions:                    A small effect was found for reduction in depressive mood following exposure to the natural environment. However, the high risk of bias and low quality of studies limits confidence in the results. The variation in effect size also remains largely unexplained. It is recommended that future studies make use of reporting guidelines and aim to reduce the potential for bias where possible.",2019-10-01,6,2130,118,585
835,31377227,"Deep learning in drug discovery: opportunities, challenges and future prospects","Artificial Intelligence (AI) is an area of computer science that simulates the structures and operating principles of the human brain. Machine learning (ML) belongs to the area of AI and endeavors to develop models from exposure to training data. Deep Learning (DL) is another subset of AI, where models represent geometric transformations over many different layers. This technology has shown tremendous potential in areas such as computer vision, speech recognition and natural language processing. More recently, DL has also been successfully applied in drug discovery. Here, I analyze several relevant DL applications and case studies, providing a detailed view of the current state-of-the-art in drug discovery and highlighting not only the problematic issues, but also the successes and opportunities for further advances.",2019-10-01,10,828,79,585
982,31886259,Application of Computational Biology and Artificial Intelligence Technologies in Cancer Precision Drug Discovery,"Artificial intelligence (AI) proves to have enormous potential in many areas of healthcare including research and chemical discoveries. Using large amounts of aggregated data, the AI can discover and learn further transforming these data into ""usable"" knowledge. Being well aware of this, the world's leading pharmaceutical companies have already begun to use artificial intelligence to improve their research regarding new drugs. The goal is to exploit modern computational biology and machine learning systems to predict the molecular behaviour and the likelihood of getting a useful drug, thus saving time and money on unnecessary tests. Clinical studies, electronic medical records, high-resolution medical images, and genomic profiles can be used as resources to aid drug development. Pharmaceutical and medical researchers have extensive data sets that can be analyzed by strong AI systems. This review focused on how computational biology and artificial intelligence technologies can be implemented by integrating the knowledge of cancer drugs, drug resistance, next-generation sequencing, genetic variants, and structural biology in the cancer precision drug discovery.",2019-11-01,2,1177,112,554
1015,31832112,Chronic inflammation: key player and biomarker-set to predict and prevent cancer development and progression based on individualized patient profiles,"A strong relationship exists between tumor and inflammation, which is the hot point in cancer research. Inflammation can promote the occurrence and development of cancer by promoting blood vessel growth, cancer cell proliferation, and tumor invasiveness, negatively regulating immune response, and changing the efficacy of certain anti-tumor drugs. It has been demonstrated that there are a large number of inflammatory factors and inflammatory cells in the tumor microenvironment, and tumor-promoting immunity and anti-tumor immunity exist simultaneously in the tumor microenvironment. The typical relationship between chronic inflammation and tumor has been presented by the relationships between Helicobacter pylori, chronic gastritis, and gastric cancer; between smoking, development of chronic pneumonia, and lung cancer; and between hepatitis virus (mainly hepatitis virus B and C), development of chronic hepatitis, and liver cancer. The prevention of chronic inflammation is a factor that can prevent cancer, so it effectively inhibits or blocks the occurrence, development, and progression of the chronic inflammation process playing important roles in the prevention of cancer. Monitoring of the causes and inflammatory factors in chronic inflammation processes is a useful way to predict cancer and assess the efficiency of cancer prevention. Chronic inflammation-based biomarkers are useful tools to predict and prevent cancer.",2019-11-01,39,1439,149,554
978,31890142,Computational approaches for effective CRISPR guide RNA design and evaluation,"The Clustered Regularly Interspaced Short Palindromic Repeat (CRISPR)/ CRISPR-associated (Cas) system has emerged as the main technology for gene editing. Successful editing by CRISPR requires an appropriate Cas protein and guide RNA. However, low cleavage efficiency and off-target effects hamper the development and application of CRISPR/Cas systems. To predict cleavage efficiency and specificity, numerous computational approaches have been developed for scoring guide RNAs. Most scores are empirical or trained by experimental datasets, and scores are implemented using various computational methods. Herein, we discuss these approaches, focusing mainly on the features or computational methods they utilise. Furthermore, we summarise these tools and give some suggestions for their usage. We also recommend three versatile web-based tools with user-friendly interfaces and preferable functions. The review provides a comprehensive and up-to-date overview of computational approaches for guide RNA design that could help users to select the optimal tools for their research.",2019-11-01,9,1079,77,554
2115,31795151,Estimating Biomechanical Time-Series with Wearable Sensors: A Systematic Review of Machine Learning Techniques,"Wearable sensors have the potential to enable comprehensive patient characterization and optimized clinical intervention. Critical to realizing this vision is accurate estimation of biomechanical time-series in daily-life, including joint, segment, and muscle kinetics and kinematics, from wearable sensor data. The use of physical models for estimation of these quantities often requires many wearable devices making practical implementation more difficult. However, regression techniques may provide a viable alternative by allowing the use of a reduced number of sensors for estimating biomechanical time-series. Herein, we review 46 articles that used regression algorithms to estimate joint, segment, and muscle kinematics and kinetics. We present a high-level comparison of the many different techniques identified and discuss the implications of our findings concerning practical implementation and further improving estimation accuracy. In particular, we found that several studies report the incorporation of domain knowledge often yielded superior performance. Further, most models were trained on small datasets in which case nonparametric regression often performed best. No models were open-sourced, and most were subject-specific and not validated on impaired populations. Future research should focus on developing open-source algorithms using complementary physics-based and machine learning techniques that are validated in clinically impaired populations. This approach may further improve estimation performance and reduce barriers to clinical adoption.",2019-11-01,1,1572,110,554
2114,31795240,A Survey of Teleceptive Sensing for Wearable Assistive Robotic Devices,"Teleception is defined as sensing that occurs remotely, with no physical contact with the object being sensed. To emulate innate control systems of the human body, a control system for a semi- or fully autonomous assistive device not only requires feedforward models of desired movement, but also the environmental or contextual awareness that could be provided by teleception. Several recent publications present teleception modalities integrated into control systems and provide preliminary results, for example, for performing hand grasp prediction or endpoint control of an arm assistive device; and gait segmentation, forward prediction of desired locomotion mode, and activity-specific control of a prosthetic leg or exoskeleton. Collectively, several different approaches to incorporating teleception have been used, including sensor fusion, geometric segmentation, and machine learning. In this paper, we summarize the recent and ongoing published work in this promising new area of research.",2019-11-01,2,1000,70,554
1111,31399699,Artificial intelligence in digital pathology - new tools for diagnosis and precision oncology,"In the past decade, advances in precision oncology have resulted in an increased demand for predictive assays that enable the selection and stratification of patients for treatment. The enormous divergence of signalling and transcriptional networks mediating the crosstalk between cancer, stromal and immune cells complicates the development of functionally relevant biomarkers based on a single gene or protein. However, the result of these complex processes can be uniquely captured in the morphometric features of stained tissue specimens. The possibility of digitizing whole-slide images of tissue has led to the advent of artificial intelligence (AI) and machine learning tools in digital pathology, which enable mining of subvisual morphometric phenotypes and might, ultimately, improve patient management. In this Perspective, we critically evaluate various AI-based computational approaches for digital pathology, focusing on deep neural networks and 'hand-crafted' feature-based methodologies. We aim to provide a broad framework for incorporating AI and machine learning tools into clinical oncology, with an emphasis on biomarker development. We discuss some of the challenges relating to the use of AI, including the need for well-curated validation datasets, regulatory approval and fair reimbursement strategies. Finally, we present potential future opportunities for precision oncology.",2019-11-01,70,1401,93,554
2111,31798635,Machine Learning for Cancer Immunotherapies Based on Epitope Recognition by T Cell Receptors,"In the last years, immunotherapies have shown tremendous success as treatments for multiple types of cancer. However, there are still many obstacles to overcome in order to increase response rates and identify effective therapies for every individual patient. Since there are many possibilities to boost a patient's immune response against a tumor and not all can be covered, this review is focused on T cell receptor-mediated therapies. CD8+ T cells can detect and destroy malignant cells by binding to peptides presented on cell surfaces by MHC (major histocompatibility complex) class I molecules. CD4+ T cells can also mediate powerful immune responses but their peptide recognition by MHC class II molecules is more complex, which is why the attention has been focused on CD8+ T cells. Therapies based on the power of T cells can, on the one hand, enhance T cell recognition by introducing TCRs that preferentially direct T cells to tumor sites (so called TCR-T therapy) or through vaccination to induce T cells in vivo. On the other hand, T cell activity can be improved by immune checkpoint inhibition or other means that help create a microenvironment favorable for cytotoxic T cell activity. The manifold ways in which the immune system and cancer interact with each other require not only the use of large omics datasets from gene, to transcript, to protein, and to peptide but also make the application of machine learning methods inevitable. Currently, discovering and selecting suitable TCRs is a very costly and work intensive in vitro process. To facilitate this process and to additionally allow for highly personalized therapies that can simultaneously target multiple patient-specific antigens, especially neoepitopes, breakthrough computational methods for predicting antigen presentation and TCR binding are urgently required. Particularly, potential cross-reactivity is a major consideration since off-target toxicity can pose a major threat to patient safety. The current speed at which not only datasets grow and are made available to the public, but also at which new machine learning methods evolve, is assuring that computational approaches will be able to help to solve problems that immunotherapies are still facing.",2019-11-01,3,2244,92,554
2153,31749074,Pre- and Paralinguistic Vocal Production in ASD: Birth Through School Age,"Purpose of review:                    We review what is known about how pre-linguistic vocal differences in autism spectrum disorder (ASD) unfold across development and consider whether vocalization features can serve as useful diagnostic indicators.              Recent findings:                    Differences in the frequency and acoustic quality of several vocalization types (e.g., babbles and cries) during the first year of life are associated with later ASD diagnosis. Paralinguistic features (e.g., prosody) measured during early and middle childhood can accurately classify current ASD diagnosis using cross-validated machine learning approaches. Pre-linguistic vocalization differences in infants are promising behavioral markers of later ASD diagnosis. In older children, paralinguistic features hold promise as diagnostic indicators as well as clinical targets. Future research efforts should focus on (1) bridging the gap between basic research and practical implementations of early vocalization-based risk assessment tools, and (2) demonstrating the clinical impact of targeting atypical vocalization features during social skill interventions for older children.",2019-11-01,2,1179,73,554
2109,31799423,"Integrating machine learning and multiscale modeling-perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences","Fueled by breakthrough technology developments, the biological, biomedical, and behavioral sciences are now collecting more data than ever before. There is a critical need for time- and cost-efficient strategies to analyze and interpret these data to advance human health. The recent rise of machine learning as a powerful technique to integrate multimodality, multifidelity data, and reveal correlations between intertwined phenomena presents a special opportunity in this regard. However, machine learning alone ignores the fundamental laws of physics and can result in ill-posed problems or non-physical solutions. Multiscale modeling is a successful strategy to integrate multiscale, multiphysics data and uncover mechanisms that explain the emergence of function. However, multiscale modeling alone often fails to efficiently combine large datasets from different sources and different levels of resolution. Here we demonstrate that machine learning and multiscale modeling can naturally complement each other to create robust predictive models that integrate the underlying physics to manage ill-posed problems and explore massive design spaces. We review the current literature, highlight applications and opportunities, address open questions, and discuss potential challenges and limitations in four overarching topical areas: ordinary differential equations, partial differential equations, data-driven approaches, and theory-driven approaches. Towards these goals, we leverage expertise in applied mathematics, computer science, computational biology, biophysics, biomechanics, engineering mechanics, experimentation, and medicine. Our multidisciplinary perspective suggests that integrating machine learning and multiscale modeling can provide new insights into disease mechanisms, help identify new targets and treatment strategies, and inform decision making for the benefit of human health.",2019-11-01,24,1905,151,554
1018,31824952,Deep Learning for Whole Slide Image Analysis: An Overview,"The widespread adoption of whole slide imaging has increased the demand for effective and efficient gigapixel image analysis. Deep learning is at the forefront of computer vision, showcasing significant improvements over previous methodologies on visual understanding. However, whole slide images have billions of pixels and suffer from high morphological heterogeneity as well as from different types of artifacts. Collectively, these impede the conventional use of deep learning. For the clinical translation of deep learning solutions to become a reality, these challenges need to be addressed. In this paper, we review work on the interdisciplinary attempt of training deep neural networks using whole slide images, and highlight the different ideas underlying these methodologies.",2019-11-01,15,785,57,554
1019,31824921,Comparison Study of Computational Prediction Tools for Drug-Target Binding Affinities,"The drug development is generally arduous, costly, and success rates are low. Thus, the identification of drug-target interactions (DTIs) has become a crucial step in early stages of drug discovery. Consequently, developing computational approaches capable of identifying potential DTIs with minimum error rate are increasingly being pursued. These computational approaches aim to narrow down the search space for novel DTIs and shed light on drug functioning context. Most methods developed to date use binary classification to predict if the interaction between a drug and its target exists or not. However, it is more informative but also more challenging to predict the strength of the binding between a drug and its target. If that strength is not sufficiently strong, such DTI may not be useful. Therefore, the methods developed to predict drug-target binding affinities (DTBA) are of great value. In this study, we provide a comprehensive overview of the existing methods that predict DTBA. We focus on the methods developed using artificial intelligence (AI), machine learning (ML), and deep learning (DL) approaches, as well as related benchmark datasets and databases. Furthermore, guidance and recommendations are provided that cover the gaps and directions of the upcoming work in this research area. To the best of our knowledge, this is the first comprehensive comparison analysis of tools focused on DTBA with reference to AI/ML/DL.",2019-11-01,9,1447,85,554
2159,31737621,Raman Spectroscopy: Guiding Light for the Extracellular Matrix,"The extracellular matrix (ECM) consists of a complex mesh of proteins, glycoproteins, and glycosaminoglycans, and is essential for maintaining the integrity and function of biological tissues. Imaging and biomolecular characterization of the ECM is critical for understanding disease onset and for the development of novel, disease-modifying therapeutics. Recently, there has been a growing interest in the use of Raman spectroscopy to characterize the ECM. Raman spectroscopy is a label-free vibrational technique that offers unique insights into the structure and composition of tissues and cells at the molecular level. This technique can be applied across a broad range of ECM imaging applications, which encompass in vitro, ex vivo, and in vivo analysis. State-of-the-art confocal Raman microscopy imaging now enables label-free assessments of the ECM structure and composition in tissue sections with a remarkably high degree of biomolecular specificity. Further, novel fiber-optic instrumentation has opened up for clinical in vivo ECM diagnostic measurements across a range of tissue systems. A palette of advanced computational methods based on multivariate statistics, spectral unmixing, and machine learning can be applied to Raman data, allowing for the extraction of specific biochemical information of the ECM. Here, we review Raman spectroscopy techniques for ECM characterizations over a variety of exciting applications and tissue systems, including native tissue assessments (bone, cartilage, cardiovascular), regenerative medicine quality assessments, and diagnostics of disease states. We further discuss the challenges in the widespread adoption of Raman spectroscopy in biomedicine. The results of the latest discovery-driven Raman studies are summarized, illustrating the current and potential future applications of Raman spectroscopy in biomedicine.",2019-11-01,6,1874,62,554
1114,31395609,Use of Natural Language Processing to Extract Clinical Cancer Phenotypes from Electronic Medical Records,"Current models for correlating electronic medical records with -omics data largely ignore clinical text, which is an important source of phenotype information for patients with cancer. This data convergence has the potential to reveal new insights about cancer initiation, progression, metastasis, and response to treatment. Insights from this real-world data will catalyze clinical care, research, and regulatory activities. Natural language processing (NLP) methods are needed to extract these rich cancer phenotypes from clinical text. Here, we review the advances of NLP and information extraction methods relevant to oncology based on publications from PubMed as well as NLP and machine learning conference proceedings in the last 3 years. Given the interdisciplinary nature of the fields of oncology and information extraction, this analysis serves as a critical trail marker on the path to higher fidelity oncology phenotypes from real-world data.",2019-11-01,9,954,104,554
2173,31701320,Artificial Intelligence for Mental Health and Mental Illnesses: an Overview,"Purpose of review:                    Artificial intelligence (AI) technology holds both great promise to transform mental healthcare and potential pitfalls. This article provides an overview of AI and current applications in healthcare, a review of recent original research on AI specific to mental health, and a discussion of how AI can supplement clinical practice while considering its current limitations, areas needing additional research, and ethical implications regarding AI technology.              Recent findings:                    We reviewed 28 studies of AI and mental health that used electronic health records (EHRs), mood rating scales, brain imaging data, novel monitoring systems (e.g., smartphone, video), and social media platforms to predict, classify, or subgroup mental health illnesses including depression, schizophrenia or other psychiatric illnesses, and suicide ideation and attempts. Collectively, these studies revealed high accuracies and provided excellent examples of AI's potential in mental healthcare, but most should be considered early proof-of-concept works demonstrating the potential of using machine learning (ML) algorithms to address mental health questions, and which types of algorithms yield the best performance. As AI techniques continue to be refined and improved, it will be possible to help mental health practitioners re-define mental illnesses more objectively than currently done in the DSM-5, identify these illnesses at an earlier or prodromal stage when interventions may be more effective, and personalize treatments based on an individual's unique characteristics. However, caution is necessary in order to avoid over-interpreting preliminary results, and more work is required to bridge the gap between AI in mental health research and clinical care.",2019-11-01,11,1814,75,554
2124,31783696,Artificial Intelligence (AI) in Rare Diseases: Is the Future Brighter?,"The amount of data collected and managed in (bio)medicine is ever-increasing. Thus, there is a need to rapidly and efficiently collect, analyze, and characterize all this information. Artificial intelligence (AI), with an emphasis on deep learning, holds great promise in this area and is already being successfully applied to basic research, diagnosis, drug discovery, and clinical trials. Rare diseases (RDs), which are severely underrepresented in basic and clinical research, can particularly benefit from AI technologies. Of the more than 7000 RDs described worldwide, only 5% have a treatment. The ability of AI technologies to integrate and analyze data from different sources (e.g., multi-omics, patient registries, and so on) can be used to overcome RDs' challenges (e.g., low diagnostic rates, reduced number of patients, geographical dispersion, and so on). Ultimately, RDs' AI-mediated knowledge could significantly boost therapy development. Presently, there are AI approaches being used in RDs and this review aims to collect and summarize these advances. A section dedicated to congenital disorders of glycosylation (CDG), a particular group of orphan RDs that can serve as a potential study model for other common diseases and RDs, has also been included.",2019-11-01,7,1271,70,554
2110,31799422,Challenges of developing a digital scribe to reduce clinical documentation burden,"Clinicians spend a large amount of time on clinical documentation of patient encounters, often impacting quality of care and clinician satisfaction, and causing physician burnout. Advances in artificial intelligence (AI) and machine learning (ML) open the possibility of automating clinical documentation with digital scribes, using speech recognition to eliminate manual documentation by clinicians or medical scribes. However, developing a digital scribe is fraught with problems due to the complex nature of clinical environments and clinical conversations. This paper identifies and discusses major challenges associated with developing automated speech-based documentation in clinical settings: recording high-quality audio, converting audio to transcripts using speech recognition, inducing topic structure from conversation data, extracting medical concepts, generating clinically meaningful summaries of conversations, and obtaining clinical data for AI and ML algorithms.",2019-11-01,5,980,81,554
1770,31549948,Artificial Intelligence for Mammography and Digital Breast Tomosynthesis: Current Concepts and Future Perspectives,"Although computer-aided diagnosis (CAD) is widely used in mammography, conventional CAD programs that use prompts to indicate potential cancers on the mammograms have not led to an improvement in diagnostic accuracy. Because of the advances in machine learning, especially with use of deep (multilayered) convolutional neural networks, artificial intelligence has undergone a transformation that has improved the quality of the predictions of the models. Recently, such deep learning algorithms have been applied to mammography and digital breast tomosynthesis (DBT). In this review, the authors explain how deep learning works in the context of mammography and DBT and define the important technical challenges. Subsequently, they discuss the current status and future perspectives of artificial intelligence-based clinical applications for mammography, DBT, and radiomics. Available algorithms are advanced and approach the performance of radiologists-especially for cancer detection and risk prediction at mammography. However, clinical validation is largely lacking, and it is not clear how the power of deep learning should be used to optimize practice. Further development of deep learning models is necessary for DBT, and this requires collection of larger databases. It is expected that deep learning will eventually have an important role in DBT, including the generation of synthetic images.",2019-11-01,12,1401,114,554
1749,31580731,Micro-nanorobots: important considerations when developing novel drug delivery platforms,"Introduction: There is growing emphasis on the development of bioinspired and biohybrid micro/nanorobots for the targeted drug delivery (TDD). Particularly, stimuli-responsive materials and magnetically triggered systems, identified as the most promising materials and design paradigms. Despite the advances made in fabrication and control, there remains a significant gap in clinical translation. Areas covered: This review discusses the opportunities and challenges about micro/nanorobotics for the TDD as evolutionary evidence in bio-nanotechnology, material science, biohybrid robotics, and many more. Important consideration in context with the material's compatibility/immunogenicity, ethics, and security risk are reported based on the development in artificial intelligence (AI)/machine learning described in literature. The versatility and sophistication of biohybrid components design are being presented, highlighting stimuli-responsive biosystems as smart mechanisms and on-board sensing and control elements. Expert opinion: Focusing on key issues for high controllability at micro- and nano-scale systems in TDD, biohybrid integration strategies, and bioinspired key competences shall be adopted. The promising outlook portraying the commercialization potential and economic viability of micro/nanorobotics will benefit to clinical translation.",2019-11-01,6,1358,88,554
2152,31749705,Applications of Deep-Learning in Exploiting Large-Scale and Heterogeneous Compound Data in Industrial Pharmaceutical Research,"In recent years, the development of high-throughput screening (HTS) technologies and their establishment in an industrialized environment have given scientists the possibility to test millions of molecules and profile them against a multitude of biological targets in a short period of time, generating data in a much faster pace and with a higher quality than before. Besides the structure activity data from traditional bioassays, more complex assays such as transcriptomics profiling or imaging have also been established as routine profiling experiments thanks to the advancement of Next Generation Sequencing or automated microscopy technologies. In industrial pharmaceutical research, these technologies are typically established in conjunction with automated platforms in order to enable efficient handling of screening collections of thousands to millions of compounds. To exploit the ever-growing amount of data that are generated by these approaches, computational techniques are constantly evolving. In this regard, artificial intelligence technologies such as deep learning and machine learning methods play a key role in cheminformatics and bio-image analytics fields to address activity prediction, scaffold hopping, de novo molecule design, reaction/retrosynthesis predictions, or high content screening analysis. Herein we summarize the current state of analyzing large-scale compound data in industrial pharmaceutical research and describe the impact it has had on the drug discovery process over the last two decades, with a specific focus on deep-learning technologies.",2019-11-01,6,1588,125,554
1082,31436850,Artificial intelligence: Implications for the future of work,"Artificial intelligence (AI) is a broad transdisciplinary field with roots in logic, statistics, cognitive psychology, decision theory, neuroscience, linguistics, cybernetics, and computer engineering. The modern field of AI began at a small summer workshop at Dartmouth College in 1956. Since then, AI applications made possible by machine learning (ML), an AI subdiscipline, include Internet searches, e-commerce sites, goods and services recommender systems, image and speech recognition, sensor technologies, robotic devices, and cognitive decision support systems (DSSs). As more applications are integrated into everyday life, AI is predicted to have a globally transformative influence on economic and social structures similar to the effect that other general-purpose technologies, such as steam engines, railroads, electricity, electronics, and the Internet, have had. Novel AI applications in the workplace of the future raise important issues for occupational safety and health. This commentary reviews the origins of AI, use of ML methods, and emerging AI applications embedded in physical objects like sensor technologies, robotic devices, or operationalized in intelligent DSSs. Selected implications on the future of work arising from the use of AI applications, including job displacement from automation and management of human-machine interactions, are also reviewed. Engaging in strategic foresight about AI workplace applications will shift occupational research and practice from a reactive posture to a proactive one. Understanding the possibilities and challenges of AI for the future of work will help mitigate the unfavorable effects of AI on worker safety, health, and well-being.",2019-11-01,4,1706,60,554
2166,31725133,An FP's guide to AI-enabled clinical decision support,"To better understand the capabilities and challenges of artificial intelligence and machine learning, we look at the role they can play in screening for retinopathy and colon cancer.",2019-11-01,0,182,53,554
2146,31756920,"Bubbles, Foam Formation, Stability and Consumer Perception of Carbonated Drinks: A Review of Current, New and Emerging Technologies for Rapid Assessment and Control","Quality control, mainly focused on the assessment of bubble and foam-related parameters, is critical in carbonated beverages, due to their relationship with the chemical components as well as their influence on sensory characteristics such as aroma release, mouthfeel, and perception of tastes and aromas. Consumer assessment and acceptability of carbonated beverages are mainly based on carbonation, foam, and bubbles, as a flat carbonated beverage is usually perceived as low quality. This review focuses on three beverages: beer, sparkling water, and sparkling wine. It explains the characteristics of foam and bubble formation, and the traditional methods, as well as emerging technologies based on robotics and computer vision, to assess bubble and foam-related parameters. Furthermore, it explores the most common methods and the use of advanced techniques using an artificial intelligence approach to assess sensory descriptors both for descriptive analysis and consumers' acceptability. Emerging technologies, based on the combination of robotics, computer vision, and machine learning as an approach to artificial intelligence, have been developed and applied for the assessment of beer and, to a lesser extent, sparkling wine. This, has the objective of assessing the final products quality using more reliable, accurate, affordable, and less time-consuming methods. However, despite carbonated water being an important product, due to its increasing consumption, more research needs to focus on exploring more efficient, repeatable, and accurate methods to assess carbonation and bubble size, distribution and dynamics.",2019-11-01,3,1630,164,554
2141,31766686,Electronic Tongues for Inedible Media,"""Electronic tongues"", ""taste sensors"", and similar devices (further named as ""multisensor systems"", or MSS) have been studied and applied mostly for the analysis of edible analytes. This is not surprising, since the MSS development was sometimes inspired by the mainstream idea that they could substitute human gustatory tests. However, the basic principle behind multisensor systems-a combination of an array of cross-sensitive chemical sensors for liquid analysis and a machine learning engine for multivariate data processing-does not imply any limitations on the application of such systems for the analysis of inedible media. This review deals with the numerous MSS applications for the analysis of inedible analytes, among other things, for agricultural and medical purposes.",2019-11-01,1,781,37,554
2139,31767810,Deep biomarkers of aging and longevity: from research to applications,"Multiple recent advances in machine learning enabled computer systems to exceed human performance in many tasks including voice, text, and speech recognition and complex strategy games. Aging is a complex multifactorial process driven by and resulting in the many minute changes transpiring at every level of the human organism. Deep learning systems trained on the many measurable features changing in time can generalize and learn the many biological processes on the population and individual levels. The deep age predictors can help advance aging research by establishing causal relationships in non-linear systems. Deep aging clocks can be used for identification of novel therapeutic targets, evaluating the efficacy of the various interventions, data quality control, data economics, prediction of health trajectories, mortality, and many other applications. Here we present the current state of development of the deep aging clocks in the context of the pharmaceutical research and development and clinical applications.",2019-11-01,7,1028,69,554
2138,31771246,Wearable-Sensor-based Detection and Prediction of Freezing of Gait in Parkinson's Disease: A Review,"Freezing of gait (FOG) is a serious gait disturbance, common in mid- and late-stage Parkinson's disease, that affects mobility and increases fall risk. Wearable sensors have been used to detect and predict FOG with the ultimate aim of preventing freezes or reducing their effect using gait monitoring and assistive devices. This review presents and assesses the state of the art of FOG detection and prediction using wearable sensors, with the intention of providing guidance on current knowledge, and identifying knowledge gaps that need to be filled and challenges to be considered in future studies. This review searched the Scopus, PubMed, and Web of Science databases to identify studies that used wearable sensors to detect or predict FOG episodes in Parkinson's disease. Following screening, 74 publications were included, comprising 68 publications detecting FOG, seven predicting FOG, and one in both categories. Details were extracted regarding participants, walking task, sensor type and body location, detection or prediction approach, feature extraction and selection, classification method, and detection and prediction performance. The results showed that increasingly complex machine-learning algorithms combined with diverse feature sets improved FOG detection. The lack of large FOG datasets and highly person-specific FOG manifestation were common challenges. Transfer learning and semi-supervised learning were promising for FOG detection and prediction since they provided person-specific tuning while preserving model generalization.",2019-11-01,7,1555,99,554
2137,31772395,Intraoperative hypotension and its prediction,"Intraoperative hypotension (IOH) very commonly accompanies general anaesthesia in patients undergoing major surgical procedures. The development of IOH is unwanted, since it is associated with adverse outcomes such as acute kidney injury and myocardial injury, stroke and mortality. Although the definition of IOH is variable, harm starts to occur below a mean arterial pressure (MAP) threshold of 65 mmHg. The odds of adverse outcome increase for increasing duration and/or magnitude of IOH below this threshold, and even short periods of IOH seem to be associated with adverse outcomes. Therefore, reducing the hypotensive burden by predicting and preventing IOH through proactive appropriate treatment may potentially improve patient outcome. In this review article, we summarise the current state of the prediction of IOH by the use of so-called machine-learning algorithms. Machine-learning algorithms that use high-fidelity data from the arterial pressure waveform, may be used to reveal 'traits' that are unseen by the human eye and are associated with the later development of IOH. These algorithms can use large datasets for 'training', and can subsequently be used by clinicians for haemodynamic monitoring and guiding therapy. A first clinically available application, the hypotension prediction index (HPI), is aimed to predict an impending hypotensive event, and additionally, to guide appropriate treatment by calculated secondary variables to asses preload (dynamic preload variables), contractility (dP/dtmax), and afterload (dynamic arterial elastance, Eadyn). In this narrative review, we summarise the current state of the prediction of hypotension using such novel, automated algorithms and we will highlight HPI and the secondary variables provided to identify the probable origin of the (impending) hypotensive event.",2019-11-01,4,1839,45,554
2167,31720867,Machine Learning and Artificial Intelligence in Neurocritical Care: a Specialty-Wide Disruptive Transformation or a Strategy for Success,"Purpose of review:                    Neurocritical care combines the complexity of both medical and surgical disease states with the inherent limitations of assessing patients with neurologic injury. Artificial intelligence (AI) has garnered interest in the basic management of these complicated patients as data collection becomes increasingly automated.              Recent findings:                    In this opinion article, we highlight the potential AI has in aiding the clinician in several aspects of neurocritical care, particularly in monitoring and managing intracranial pressure, seizures, hemodynamics, and ventilation. The model-based method and data-driven method are currently the two major AI methods for analyzing critical care data. Both are able to analyze the vast quantities of patient data that are accumulated in the neurocritical care unit. AI has the potential to reduce healthcare costs, minimize delays in patient management, and reduce medical errors. However, these systems are an aid to, not a replacement for, the clinician's judgment.",2019-11-01,1,1069,136,554
8,32405554,Imaging in Spine Surgery: Current Concepts and Future Directions,"Objective:                    To review and highlight the historical and recent advances of imaging in spine surgery and to discuss current applications and future directions.              Methods:                    A PubMed review of the current literature was performed on all relevant articles that examined historical and recent imaging techniques used in spine surgery. Studies were examined for their thoroughness in description of various modalities and applications in current and future management.              Results:                    We reviewed 97 articles that discussed past, present, and future applications for imaging in spine surgery. Although most historical approaches relied heavily upon basic radiography, more recent advances have begun to expand upon advanced modalities, including the integration of more sophisticated equipment and artificial intelligence.              Conclusions:                    Since the days of conventional radiography, various modalities have emerged and become integral components of the spinal surgeon's diagnostic armamentarium. As such, it behooves the practitioner to remain informed on the current trends and potential developments in spinal imaging, as rapid adoption and interpretation of new techniques may make significant differences in patient management and outcomes. Future directions will likely become increasingly sophisticated as the implementation of machine learning, and artificial intelligence has become more commonplace in clinical practice.",2019-11-01,2,1523,64,554
2134,31779133,A Survey on Machine-Learning Techniques for UAV-Based Communications,"Unmanned aerial vehicles (UAVs) will be an integral part of the next generation wireless communication networks. Their adoption in various communication-based applications is expected to improve coverage and spectral efficiency, as compared to traditional ground-based solutions. However, this new degree of freedom that will be included in the network will also add new challenges. In this context, the machine-learning (ML) framework is expected to provide solutions for the various problems that have already been identified when UAVs are used for communication purposes. In this article, we provide a detailed survey of all relevant research works, in which ML techniques have been used on UAV-based communications for improving various design and functional aspects such as channel modeling, resource management, positioning, and security.",2019-11-01,3,844,68,554
2133,31779139,Machine Learning Approaches for the Prioritization of Genomic Variants Impacting Pre-mRNA Splicing,"Defects in pre-mRNA splicing are frequently a cause of Mendelian disease. Despite the advent of next-generation sequencing, allowing a deeper insight into a patient's variant landscape, the ability to characterize variants causing splicing defects has not progressed with the same speed. To address this, recent years have seen a sharp spike in the number of splice prediction tools leveraging machine learning approaches, leaving clinical geneticists with a plethora of choices for in silico analysis. In this review, some basic principles of machine learning are introduced in the context of genomics and splicing analysis. A critical comparative approach is then used to describe seven recent machine learning-based splice prediction tools, revealing highly diverse approaches and common caveats. We find that, although great progress has been made in producing specific and sensitive tools, there is still much scope for personalized approaches to prediction of variant impact on splicing. Such approaches may increase diagnostic yields and underpin improvements to patient care.",2019-11-01,4,1083,98,554
2132,31781015,Clinical Risk Score for Predicting Recurrence Following a Cerebral Ischemic Event,"Introduction: Recurrent stroke has a higher rate of death and disability. A number of risk scores have been developed to predict short-term and long-term risk of stroke following an initial episode of stroke or transient ischemic attack (TIA) with limited clinical utilities. In this paper, we review different risk score models and discuss their validity and clinical utilities. Methods: The PubMed bibliographic database was searched for original research articles on the various risk scores for risk of stroke following an initial episode of stroke or TIA. The validation of the models was evaluated by examining the internal and external validation process as well as statistical methodology, the study power, as well as the accuracy and metrics such as sensitivity and specificity. Results: Different risk score models have been derived from different study populations. Validation studies for these risk scores have produced conflicting results. Currently, ABCD2 score with diffusion weighted imaging (DWI) and Recurrence Risk Estimator at 90 days (RRE-90) are the two acceptable models for short-term risk prediction whereas Essen Stroke Risk Score (ESRS) and Stroke Prognosis Instrument-II (SPI-II) can be useful for prediction of long-term risk. Conclusion: The clinical risk scores that currently exist for predicting short-term and long-term risk of recurrent cerebral ischemia are limited in their performance and clinical utilities. There is a need for a better predictive tool which can overcome the limitations of current predictive models. Application of machine learning methods in combination with electronic health records may provide platform for development of new-generation predictive tools.",2019-11-01,4,1714,81,554
2131,31781215,Artificial Intelligence in Interventional Radiology: A Literature Review and Future Perspectives,"The term ""artificial intelligence"" (AI) includes computational algorithms that can perform tasks considered typical of human intelligence, with partial to complete autonomy, to produce new beneficial outputs from specific inputs. The development of AI is largely based on the introduction of artificial neural networks (ANN) that allowed the introduction of the concepts of ""computational learning models,"" machine learning (ML) and deep learning (DL). AI applications appear promising for radiology scenarios potentially improving lesion detection, segmentation, and interpretation with a recent application also for interventional radiology (IR) practice, including the ability of AI to offer prognostic information to both patients and physicians about interventional oncology procedures. This article integrates evidence-reported literature and experience-based perceptions to assist not only residents and fellows who are training in interventional radiology but also practicing colleagues who are approaching to locoregional mini-invasive treatments.",2019-11-01,2,1056,96,554
1004,31850202,Study Progress of Radiomics With Machine Learning for Precision Medicine in Bladder Cancer Management,"Bladder cancer is a fatal cancer that happens in the genitourinary tract with quite high morbidity and mortality annually. The high level of recurrence rate ranging from 50 to 80% makes bladder cancer one of the most challenging and costly diseases to manage. Faced with various problems in existing methods, a recently emerging concept for the measurement of imaging biomarkers and extraction of quantitative features called ""radiomics"" shows great potential in the application of detection, grading, and follow-up management of bladder cancer. Furthermore, machine-learning (ML) algorithms on the basis of ""big data"" are fueling the powers of radiomics for bladder cancer monitoring in the era of precision medicine. Currently, the usefulness of the novel combination of radiomics and ML has been demonstrated by a large number of successful cases. It possesses outstanding strengths including non-invasiveness, low cost, and high efficiency, which may serve as a revolution to tumor assessment and emancipate workforce. However, for the extensive clinical application in the future, more efforts should be made to break down the limitations caused by technology deficiencies, inherent problems during the process of radiomic analysis, as well as the quality of present studies.",2019-11-01,3,1280,101,554
1005,31849692,QuantiMus: A Machine Learning-Based Approach for High Precision Analysis of Skeletal Muscle Morphology,"Skeletal muscle injury provokes a regenerative response, characterized by the de novo generation of myofibers that are distinguished by central nucleation and re-expression of developmentally restricted genes. In addition to these characteristics, myofiber cross-sectional area (CSA) is widely used to evaluate muscle hypertrophic and regenerative responses. Here, we introduce QuantiMus, a free software program that uses machine learning algorithms to quantify muscle morphology and molecular features with high precision and quick processing-time. The ability of QuantiMus to define and measure myofibers was compared to manual measurement or other automated software programs. QuantiMus rapidly and accurately defined total myofibers and measured CSA with comparable performance but quantified the CSA of centrally-nucleated fibers (CNFs) with greater precision compared to other software. It additionally quantified the fluorescence intensity of individual myofibers of human and mouse muscle, which was used to assess the distribution of myofiber type, based on the myosin heavy chain isoform that was expressed. Furthermore, analysis of entire quadriceps cross-sections of healthy and mdx mice showed that dystrophic muscle had an increased frequency of Evans blue dye+ injured myofibers. QuantiMus also revealed that the proportion of centrally nucleated, regenerating myofibers that express embryonic myosin heavy chain (eMyHC) or neural cell adhesion molecule (NCAM) were increased in dystrophic mice. Our findings reveal that QuantiMus has several advantages over existing software. The unique self-learning capacity of the machine learning algorithms provides superior accuracy and the ability to rapidly interrogate the complete muscle section. These qualities increase rigor and reproducibility by avoiding methods that rely on the sampling of representative areas of a section. This is of particular importance for the analysis of dystrophic muscle given the ""patchy"" distribution of muscle pathology. QuantiMus is an open source tool, allowing customization to meet investigator-specific needs and provides novel analytical approaches for quantifying muscle morphology.",2019-11-01,1,2185,102,554
845,31355445,"Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association","In this white paper, experts from the Digital Pathology Association (DPA) define terminology and concepts in the emerging field of computational pathology, with a focus on its application to histology images analyzed together with their associated patient data to extract information. This review offers a historical perspective and describes the potential clinical benefits from research and applications in this field, as well as significant obstacles to adoption. Best practices for implementing computational pathology workflows are presented. These include infrastructure considerations, acquisition of training data, quality assessments, as well as regulatory, ethical, and cyber-security concerns. Recommendations are provided for regulators, vendors, and computational pathology practitioners in order to facilitate progress in the field.  2019 The Authors. The Journal of Pathology published by John Wiley & Sons Ltd on behalf of Pathological Society of Great Britain and Ireland.",2019-11-01,20,990,150,554
1754,31570832,Looking back and thinking forwards - 15 years of cardiology and cardiovascular research,"The first issue of Nature Reviews Cardiology was published in November 2004 under the name Nature Clinical Practice Cardiovascular Medicine. To celebrate our 15th anniversary in 2019, we invited six of our Advisory Board members to discuss what they considered the most important advances in their field of cardiovascular research or clinical practice in the past 15 years and what changes they envision for cardiovascular medicine in the next 15 years. Several practice-changing breakthroughs are described, including advances in procedural techniques to treat arrhythmias and hypertension and the development of novel therapeutic strategies to treat heart failure and pulmonary arterial hypertension, as well as those that target risk factors such as inflammation and elevated LDL-cholesterol levels. Furthermore, these key opinion leaders predict that machine learning technology and data derived from wearable devices will pave the way towards the coveted goal of personalized medicine.",2019-11-01,1,990,87,554
2528,30573283,The value of MR textural analysis in prostate cancer,"Current diagnosis and treatment stratification of patients with suspected prostate cancer relies on a combination of histological and magnetic resonance imaging (MRI) findings. The aim of this article is to provide a brief overview of prostate pathological grading as well as the relevant aspects of multiparametric (MRI) mpMRI, before indicating the potential that magnetic resonance textural analysis (MRTA) offers within prostate cancer. A review of the evidence base on MRTA in prostate cancer will enable discussion of the utility of this field while also indicating recommendations to future research. Radiomic textural analysis allows the assessment of spatial inter-relationships between pixels within an image by use of mathematical methods. First-order textural analysis is better understood and may have more clinical validity than higher-order textural features. Textural features extracted from apparent diffusion coefficient maps have shown the most potential for clinical utility in MRTA of prostate cancers. Future studies should aim to integrate machine learning techniques to better represent the role of MRTA in prostate cancer clinical practice. Nomenclature should be used to reduce misidentification between first-order and second-order energy and entropy. Automated methods of segmentation should be encouraged in order to reduce problems associated with inclusion of normal tissue within regions of interest. The retrospective and small-scale nature of most published studies, make it difficult to draw meaningful conclusions. Future larger prospective studies are required to validate the textural features indicated to have potential in characterisation and/or diagnosis of prostate cancer before translation into routine clinical practice.",2019-11-01,5,1766,52,554
1730,31605874,Common themes and emerging trends for the use of technology to support mental health and psychosocial well-being in limited resource settings: A review of the literature,"There are significant disparities in access to mental health care. With the burgeoning of technologies for health, digital tools have been leveraged within mental health and psychosocial support programming (eMental health). A review of the literature was conducted to understand and identify how eMental health has been used in resource-limited settings in general. PubMed, Ovid Medline and Web of Science were searched. Six-hundred and thirty full-text articles were identified and assessed for eligibility; of those, 67 articles met the inclusion criteria and were analyzed. The most common mental health use cases were for depression (n = 25) and general mental health and well-being (n = 21). Roughly one-third used a website or Internet-enabled intervention (n = 23) and nearly one-third used an SMS intervention (n = 22). Technology was applied to enhance service delivery (n = 32), behavior change communication (n = 26) and data collection (n = 8), and specifically dealt with adherence (n = 7), ecological momentary assessments (n = 7), well-being promotion (n = 5), education (n = 8), telemedicine (n = 28), machine learning (n = 5) and games (n = 2). Emerging trends identified wearables, predictive analytics, robots and virtual reality as promising areas. eMental health interventions that leverage low-tech tools can introduce, strengthen and expand mental health and psychosocial support services and can be a starting point for future, advanced tools.",2019-11-01,5,1468,169,554
906,31260168,Agent-based models of inflammation in translational systems biology: A decade later,"Agent-based modeling is a rule-based, discrete-event, and spatially explicit computational modeling method that employs computational objects that instantiate the rules and interactions among the individual components (""agents"") of system. Agent-based modeling is well suited to translating into a computational model the knowledge generated from basic science research, particularly with respect to translating across scales the mechanisms of cellular behavior into aggregated cell population dynamics manifesting at the tissue and organ level. This capacity has made agent-based modeling an integral method in translational systems biology (TSB), an approach that uses multiscale dynamic computational modeling to explicitly represent disease processes in a clinically relevant fashion. The initial work in the early 2000s using agent-based models (ABMs) in TSB focused on examining acute inflammation and its intersection with wound healing; the decade since has seen vast growth in both the application of agent-based modeling to a wide array of disease processes as well as methodological advancements in the use and analysis of ABM. This report presents an update on an earlier review of ABMs in TSB and presents examples of exciting progress in the modeling of various organs and diseases that involve inflammation. This review also describes developments that integrate the use of ABMs with cutting-edge technologies such as high-performance computing, machine learning, and artificial intelligence, with a view toward the future integration of these methodologies. This article is categorized under: Translational, Genomic, and Systems Medicine > Translational Medicine Models of Systems Properties and Processes > Mechanistic Models Models of Systems Properties and Processes > Organ, Tissue, and Physiological Models Models of Systems Properties and Processes > Organismal Models.",2019-11-01,1,1891,83,554
962,31921555,Progress and Challenges Toward the Rational Design of Oxygen Electrocatalysts Based on a Descriptor Approach,"Oxygen redox catalysis, including the oxygen reduction reaction (ORR) and oxygen evolution reaction (OER), is crucial in determining the electrochemical performance of energy conversion and storage devices such as fuel cells, metal-air batteries,and electrolyzers. The rational design of electrochemical catalysts replaces the traditional trial-and-error methods and thus promotes the R&D process. Identifying descriptors that link structure and activity as well as selectivity of catalysts is the key for rational design. In the past few decades, two types of descriptors including bulk- and surface-based have been developed to probe the structure-property relationships. Correlating the current descriptors to one another will promote the understanding of the underlying physics and chemistry, triggering further development of more universal descriptors for the future design of electrocatalysts. Herein, the current benchmark activity descriptors for oxygen electrocatalysis as well as their applications are reviewed. Particular attention is paid to circumventing the scaling relationship of oxygen-containing intermediates. For hybrid materials, multiple descriptors will show stronger predictive power by considering more factors such as interface reconstruction, confinement effect, multisite adsorption, etc. Machine learning and high-throughput simulations can thus be crucial in assisting the discovery of new multiple descriptors and reaction mechanisms.",2019-11-01,2,1467,108,554
1038,31505121,Vector-Space Models of Semantic Representation From a Cognitive Perspective: A Discussion of Common Misconceptions,"Models that represent meaning as high-dimensional numerical vectors-such as latent semantic analysis (LSA), hyperspace analogue to language (HAL), bound encoding of the aggregate language environment (BEAGLE), topic models, global vectors (GloVe), and word2vec-have been introduced as extremely powerful machine-learning proxies for human semantic representations and have seen an explosive rise in popularity over the past 2 decades. However, despite their considerable advancements and spread in the cognitive sciences, one can observe problems associated with the adequate presentation and understanding of some of their features. Indeed, when these models are examined from a cognitive perspective, a number of unfounded arguments tend to appear in the psychological literature. In this article, we review the most common of these arguments and discuss (a) what exactly these models represent at the implementational level and their plausibility as a cognitive theory, (b) how they deal with various aspects of meaning such as polysemy or compositionality, and (c) how they relate to the debate on embodied and grounded cognition. We identify common misconceptions that arise as a result of incomplete descriptions, outdated arguments, and unclear distinctions between theory and implementation of the models. We clarify and amend these points to provide a theoretical basis for future research and discussions on vector models of semantic representation.",2019-11-01,5,1459,114,554
145,30099485,Recent advances and prospects of computational methods for metabolite identification: a review with emphasis on machine learning approaches,"Metabolomics involves studies of a great number of metabolites, which are small molecules present in biological systems. They play a lot of important functions such as energy transport, signaling, building block of cells and inhibition/catalysis. Understanding biochemical characteristics of the metabolites is an essential and significant part of metabolomics to enlarge the knowledge of biological systems. It is also the key to the development of many applications and areas such as biotechnology, biomedicine or pharmaceuticals. However, the identification of the metabolites remains a challenging task in metabolomics with a huge number of potentially interesting but unknown metabolites. The standard method for identifying metabolites is based on the mass spectrometry (MS) preceded by a separation technique. Over many decades, many techniques with different approaches have been proposed for MS-based metabolite identification task, which can be divided into the following four groups: mass spectra database, in silico fragmentation, fragmentation tree and machine learning. In this review paper, we thoroughly survey currently available tools for metabolite identification with the focus on in silico fragmentation, and machine learning-based approaches. We also give an intensive discussion on advanced machine learning methods, which can lead to further improvement on this task.",2019-11-01,11,1391,139,554
2177,31694700,Quantifying the contribution of microbial immigration in engineered water systems,"Immigration is a process that can influence the assembly of microbial communities in natural and engineered environments. However, it remains challenging to quantitatively evaluate the contribution of this process to the microbial diversity and function in the receiving ecosystems. Currently used methods, i.e., counting shared microbial species, microbial source tracking, and neutral community model, rely on abundance profile to reveal the extent of overlapping between the upstream and downstream communities. Thus, they cannot suggest the quantitative contribution of immigrants to the downstream community function because activities of individual immigrants are not considered after entering the receiving environment. This limitation can be overcome by using an approach that couples a mass balance model with high-throughput DNA sequencing, i.e., ecogenomics-based mass balance. It calculates the net growth rate of individual microbial immigrants and partitions the entire community into active populations that contribute to the community function and inactive ones that carry minimal function. Linking activities of immigrants to their abundance further provides quantification of the contribution from an upstream environment to the downstream community. Considering only active populations can improve the accuracy of identifying key environmental parameters dictating process performance using methods such as machine learning.",2019-11-01,4,1443,81,554
128,30137230,Machine learning meets genome assembly,"Motivation:                    With the recent advances in DNA sequencing technologies, the study of the genetic composition of living organisms has become more accessible for researchers. Several advances have been achieved because of it, especially in the health sciences. However, many challenges which emerge from the complexity of sequencing projects remain unsolved. Among them is the task of assembling DNA fragments from previously unsequenced organisms, which is classified as an NP-hard (nondeterministic polynomial time hard) problem, for which no efficient computational solution with reasonable execution time exists. However, several tools that produce approximate solutions have been used with results that have facilitated scientific discoveries, although there is ample room for improvement. As with other NP-hard problems, machine learning algorithms have been one of the approaches used in recent years in an attempt to find better solutions to the DNA fragment assembly problem, although still at a low scale.              Results:                    This paper presents a broad review of pioneering literature comprising artificial intelligence-based DNA assemblers-particularly the ones that use machine learning-to provide an overview of state-of-the-art approaches and to serve as a starting point for further study in this field.",2019-11-01,1,1354,38,554
1063,31468267,Current Controversies Concerning Capsule Endoscopy,"Video capsule endoscopy became a reality in 2001. This device enabled us to directly view the mucosa of the small intestine for the first time. The main indications for the video capsule remain the detection of small intestinal bleeding and iron deficiency anemia, diagnosis and management of Crohn's disease, and detection of tumors. The device is extraordinarily safe and can be used in the very young to the very old. However, there remain several areas of controversy and difficulty. These are covered in this article and include details of indications and contraindications, whether to prepare patients, whether or not to use simethicone and prokinetics. Detection of location of the capsule remains a major engineering challenge. Reading the videos reliably and quickly remains challenging. However, artificial intelligence and machine learning are already on the horizon to provide assistance. New uses for capsule endoscopy promise more accurate diagnosis and hence improved management of acute gastrointestinal bleeding. The colon capsule may eventually help those who refuse conventional colonoscopy, and robotically controlled capsules may be helpful in screening for serious disease in patients with upper abdominal complaints. The advent of the broadening use of video capsule endoscopy is, though it will be controversial, embraced by some and derided by others; such is the nature of technological development. In the long run, if the use of the video capsule, based on sound evidence-based studies, can be shown to improve the care of our patients and reduce the cost of health care, its use will continue to expand.",2019-11-01,1,1632,50,554
1702,31633462,Evolution of Aesthetic Dentistry,"One of the main goals of dental treatment is to mimic teeth and design smiles in a most natural and aesthetic manner, based on the individual and specific needs of the patient. Possibilities to reach that goal have significantly improved over the last decade through new and specific treatment modalities, steadily enhanced and more aesthetic dental materials, and novel techniques and technologies. This article gives an overview of the evolution of aesthetic dentistry over the past 100 y from a historical point of view and highlights advances in the development of dental research and clinical interventions that have contributed the science and art of aesthetic dentistry. Among the most noteworthy advancements over the past decade are the establishment of universal aesthetic rules and guidelines based on the assessment of natural aesthetic parameters, anatomy, and physiognomy; the development of tooth whitening and advanced restorative as well as prosthetic materials and techniques, supported by the pioneering discovery of dental adhesion; the significant progress in orthodontics and periodontal as well as oral and maxillofacial surgery; and, most recently, the implementation of digital technologies in the 3-dimensional planning and realization of truly natural, individual, and aesthetic smiles. In the future, artificial intelligence and machine learning will likely lead to automation of aesthetic evaluation, smile design, and treatment-planning processes.",2019-11-01,4,1477,32,554
1030,31518831,Comprehensive intra-individual genomic and transcriptional heterogeneity: Evidence-based Colorectal Cancer Precision Medicine,"Despite advances in translating conventional research into multi-modal treatment for colorectal cancer (CRC), therapeutic resistance and relapse remain unresolved in advanced resectable and, particularly, non-resectable disease. Genome and transcriptome sequencing and editing technologies, coupled with interaction mapping and machine learning, are transforming biomedical research, representing the most rational hope to overcome unmet research and clinical challenges. Rapid progress in both bulk and single-cell next-generation sequencing (NGS) analyses in the identification of primary and metastatic intratumor genomic and transcriptional heterogeneity (ITH) and the detection of circulating cell-free DNA (cfDNA) alterations is providing critical insight into the origins and spatiotemporal evolution of genomic clones responsible for early and late therapeutic resistance and relapse. Moreover, DNA and RNA editing pave new avenues towards the discovery of novel drug targets. Breakthrough combinations of sequencing and editing systems with technologies exploring dynamic interaction networks within pioneering studies could delineate how coding and non-coding mutations perturb regulatory networks and gene expression. This review discusses latest data on genomic and transcriptomic landscapes in time and space, as well as early-phase clinical trials on targeted drug combinations, highlighting the transition from research to clinical Colorectal Cancer Precision Medicine, through non-invasive screening, individualized drug response prediction and development of multiple novel drugs. Future studies exploring the potential to target key transcriptional drivers and regulators will contribute to the next-generation pharmaceutical controllability of multi-layered aberrant transcriptional biocircuits.",2019-11-01,9,1814,125,554
1723,31613700,Targeting the Versatile Wnt/-Catenin Pathway in Cancer Biology and Therapeutics: From Concept to Actionable Strategy,"This expert review offers a critical synthesis of the latest insights and approaches at targeting the Wnt/-catenin pathway in various cancers such as colorectal cancer, melanoma, leukemia, and breast and lung cancers. Notably, from organogenesis to cancer, the Wnt/-catenin signaling displays varied and highly versatile biological functions in animals, with virtually all tissues requiring the Wnt/-catenin signaling in one way or the other. Aberrant expression of the members of the Wnt/-catenin has been implicated in many pathological conditions, particularly in human cancers. Mutations in the Wnt/-catenin pathway genes have been noted in diverse cancers. Biochemical and genetic data support the idea that inhibition of Wnt/-catenin signaling is beneficial in cancer therapeutics. The interaction of this important pathway with other signaling systems is also noteworthy, but remains as an area for further research and discovery. In addition, formation of different complexes by components of the Wnt/-catenin pathway and the precise roles of these complexes in the cytoplasmic milieu are yet to be fully elucidated. This article highlights the latest medical technologies in imaging, single-cell omics, use of artificial intelligence (e.g., machine learning techniques), genome sequencing, quantum computing, molecular docking, and computational softwares in modeling interactions between molecules and predicting protein-protein and compound-protein interactions pertinent to the biology and therapeutic value of the Wnt/-catenin signaling pathway. We discuss these emerging technologies in relationship to what is currently needed to move from concept to actionable strategies in translating the Wnt/-catenin laboratory discoveries to Wnt-targeted cancer therapies and diagnostics in the clinic.",2019-11-01,3,1815,117,554
1033,31513757,Remaining challenges in predicting patient outcomes for diffuse large B-cell lymphoma,"Introduction: Diffuse large B-cell lymphoma (DLBCL) is the most common non-Hodgkin lymphoma and is an aggressive malignancy with heterogeneous outcomes. Diverse methods for DLBCL outcomes assessment ranging from clinical to genomic have been developed with variable predictive and prognostic success.Areas covered: The authors provide an overview of the various methods currently used to estimate prognosis in DLBCL patients. Models incorporating cell of origin, genomic features, sociodemographic factors, treatment effectiveness measures, and machine learning are described.Expert opinion: The clinical and genetic heterogeneity of DLBCL presents distinct challenges in predicting response to therapy and overall prognosis. Successful integration of predictive and prognostic tools in clinical trials and in a standard clinical workflow for DLBCL will likely require a combination of methods incorporating clinical, sociodemographic, and molecular factors with the aid of machine learning and high-dimensional data analysis.",2019-11-01,1,1026,85,554
1699,31638733,Artificial-Intelligence-Driven Organic Synthesis-En Route towards Autonomous Synthesis?,AI for chemistry: Automated synthesis can now be performed using an artificial intelligence algorithm to propose the synthetic route and a robotic microfluidic platform to execute the synthesis. This Highlight describes this approach towards small-molecule synthesis and reflects on the significance of this milestone in chemistry.,2019-11-01,1,331,87,554
2283,31067608,Recent methodology progress of deep learning for RNA-protein interaction prediction,"Interactions between RNAs and proteins play essential roles in many important biological processes. Benefitting from the advances of next generation sequencing technologies, hundreds of RNA-binding proteins (RBP) and their associated RNAs have been revealed, which enables the large-scale prediction of RNA-protein interactions using machine learning methods. Till now, a wide range of computational tools and pipelines have been developed, including deep learning models, which have achieved remarkable performance on the identification of RNA-protein binding affinities and sites. In this review, we provide an overview of the successful implementation of various deep learning approaches for predicting RNA-protein interactions, mainly focusing on the prediction of RNA-protein interaction pairs and RBP-binding sites on RNAs. Furthermore, we discuss the advantages and disadvantages of these approaches, and highlight future perspectives on how to design better deep learning models. Finally, we suggest some promising future directions of computational tasks in the study of RNA-protein interactions, especially the interactions between noncoding RNAs and proteins. This article is categorized under: RNA Interactions with Proteins and Other Molecules > Protein-RNA Interactions: Functional Implications RNA Evolution and Genomics > Computational Analyses of RNA RNA Interactions with Proteins and Other Molecules > Protein-RNA Recognition.",2019-11-01,6,1445,83,554
836,31374225,Machine learning and data mining frameworks for predicting drug response in cancer: An overview and a novel in silico screening process based on association rule mining,"A major challenge in cancer treatment is predicting the clinical response to anti-cancer drugs on a personalized basis. The success of such a task largely depends on the ability to develop computational resources that integrate big ""omic"" data into effective drug-response models. Machine learning is both an expanding and an evolving computational field that holds promise to cover such needs. Here we provide a focused overview of: 1) the various supervised and unsupervised algorithms used specifically in drug response prediction applications, 2) the strategies employed to develop these algorithms into applicable models, 3) data resources that are fed into these frameworks and 4) pitfalls and challenges to maximize model performance. In this context we also describe a novel in silico screening process, based on Association Rule Mining, for identifying genes as candidate drivers of drug response and compare it with relevant data mining frameworks, for which we generated a web application freely available at: https://compbio.nyumc.org/drugs/. This pipeline explores with high efficiency large sample-spaces, while is able to detect low frequency events and evaluate statistical significance even in the multidimensional space, presenting the results in the form of easily interpretable rules. We conclude with future prospects and challenges of applying machine learning based drug response prediction in precision medicine.",2019-11-01,8,1436,168,554
110,30184176,Twenty years of bioinformatics research for protease-specific substrate and cleavage site prediction: a comprehensive revisit and benchmarking of existing methods,"The roles of proteolytic cleavage have been intensively investigated and discussed during the past two decades. This irreversible chemical process has been frequently reported to influence a number of crucial biological processes (BPs), such as cell cycle, protein regulation and inflammation. A number of advanced studies have been published aiming at deciphering the mechanisms of proteolytic cleavage. Given its significance and the large number of functionally enriched substrates targeted by specific proteases, many computational approaches have been established for accurate prediction of protease-specific substrates and their cleavage sites. Consequently, there is an urgent need to systematically assess the state-of-the-art computational approaches for protease-specific cleavage site prediction to further advance the existing methodologies and to improve the prediction performance. With this goal in mind, in this article, we carefully evaluated a total of 19 computational methods (including 8 scoring function-based methods and 11 machine learning-based methods) in terms of their underlying algorithm, calculated features, performance evaluation and software usability. Then, extensive independent tests were performed to assess the robustness and scalability of the reviewed methods using our carefully prepared independent test data sets with 3641 cleavage sites (specific to 10 proteases). The comparative experimental results demonstrate that PROSPERous is the most accurate generic method for predicting eight protease-specific cleavage sites, while GPS-CCD and LabCaS outperformed other predictors for calpain-specific cleavage sites. Based on our review, we then outlined some potential ways to improve the prediction performance and ease the computational burden by applying ensemble learning, deep learning, positive unlabeled learning and parallel and distributed computing techniques. We anticipate that our study will serve as a practical and useful guide for interested readers to further advance next-generation bioinformatics tools for protease-specific cleavage site prediction.",2019-11-01,15,2111,162,554
1731,31603503,Big data in yeast systems biology,"Systems biology uses computational and mathematical modeling to study complex interactions in a biological system. The yeast Saccharomyces cerevisiae, which has served as both an important model organism and cell factory, has pioneered both the early development of such models and modeling concepts, and the more recent integration of multi-omics big data in these models to elucidate fundamental principles of biology. Here, we review the advancement of big data technologies to gain biological insight in three aspects of yeast systems biology: gene expression dynamics, cellular metabolism and the regulation network between gene expression and metabolism. The role of big data and complementary modeling approaches, including the expansion of genome-scale metabolic models and machine learning methodologies, are discussed as key drivers in the rapid advancement of yeast systems biology.",2019-11-01,1,893,33,554
1035,31510795,Cardiac Computed Tomography: Assessment of Coronary Inflammation and Other Plaque Features,"Unstable coronary plaques that are prone to erosion and rupture are the major cause of acute coronary syndromes. Our expanding understanding of the biological mechanisms of coronary atherosclerosis and rapid technological advances in the field of medical imaging has established cardiac computed tomography as a first-line diagnostic test in the assessment of suspected coronary artery disease, and as a powerful method of detecting the vulnerable plaque and patient. Cardiac computed tomography can provide a noninvasive, yet comprehensive, qualitative and quantitative assessment of coronary plaque burden, detect distinct high-risk morphological plaque features, assess the hemodynamic significance of coronary lesions and quantify the coronary inflammatory burden by tracking the effects of arterial inflammation on the composition of the adjacent perivascular fat. Furthermore, advances in machine learning, computational fluid dynamic modeling, and the development of targeted contrast agents continue to expand the capabilities of cardiac computed tomography imaging. In our Review, we discuss the current role of cardiac computed tomography in the assessment of coronary atherosclerosis, highlighting its dual function as a clinical and research tool that provides a wealth of structural and functional information, with far-reaching diagnostic and prognostic implications.",2019-11-01,2,1381,90,554
911,31255749,Assessing the effectiveness of artificial intelligence methods for melanoma: A retrospective review,"Background:                    Artificial intelligence methods for the classification of melanoma have been studied extensively. However, few studies compare these methods under the same standards.              Objective:                    To seek the best artificial intelligence method for diagnosis of melanoma.              Methods:                    The contrast test used 2200 dermoscopic images. Image segmentations, feature extractions, and classifications were performed in sequence for evaluation of traditional machine learning algorithms. The recent popular convolutional neural network frameworks were used for transfer learning training classification.              Results:                    The region growing algorithm has the best segmentation performance, with an intersection over union of 70.06% and a false-positive rate of 17.67%. Classification performance was better with logistic regression, with a sensitivity of 76.36% and a specificity of 87.04%. The Inception V3 model (Google, Mountain View, CA) worked best in deep learning algorithms: the accuracy was 93.74%, the sensitivity was 94.36%, and the specificity was 85.64%.              Limitations:                    There was no division in the severity of melanoma samples used in this experiment. The data set was relatively small for deep learning.              Conclusion:                    The performance of traditional machine learning is satisfactory for the small data set of melanoma dermoscopic images, and the potential for deep learning in the future big data era is enormous.",2019-11-01,6,1575,99,554
2179,31690036,Comprehensive Outline of Whole Exome Sequencing Data Analysis Tools Available in Clinical Oncology,"Whole exome sequencing (WES) enables the analysis of all protein coding sequences in the human genome. This technology enables the investigation of cancer-related genetic aberrations that are predominantly located in the exonic regions. WES delivers high-throughput results at a reasonable price. Here, we review analysis tools enabling utilization of WES data in clinical and research settings. Technically, WES initially allows the detection of single nucleotide variants (SNVs) and copy number variations (CNVs), and data obtained through these methods can be combined and further utilized. Variant calling algorithms for SNVs range from standalone tools to machine learning-based combined pipelines. Tools for CNV detection compare the number of reads aligned to a dedicated segment. Both SNVs and CNVs help to identify mutations resulting in pharmacologically druggable alterations. The identification of homologous recombination deficiency enables the use of PARP inhibitors. Determining microsatellite instability and tumor mutation burden helps to select patients eligible for immunotherapy. To pave the way for clinical applications, we have to recognize some limitations of WES, including its restricted ability to detect CNVs, low coverage compared to targeted sequencing, and the missing consensus regarding references and minimal application requirements. Recently, Galaxy became the leading platform in non-command line-based WES data processing. The maturation of next-generation sequencing is reinforced by Food and Drug Administration (FDA)-approved methods for cancer screening, detection, and follow-up. WES is on the verge of becoming an affordable and sufficiently evolved technology for everyday clinical use.",2019-11-01,5,1731,98,554
1983,32915223,Artificial Intelligence and global health: opportunities and challenges,"Artificial Intelligence (AI) offers unprecedented opportunities and challenges for humanity. If AI can be positioned and leveraged correctly, it can rapidly accelerate progress on achieving the United Nations' Sustainable Development Goals (SDGs), including SDG #3: 'Ensure healthy lives and promote wellbeing for all at all ages'. Achieving this goal could have a transformative impact on global health. An ethical, transparent and responsible approach to AI development will result in AI translating data into contextually relevant knowledge, conclusions, and impactful actions.",2019-11-01,0,580,71,554
976,31890467,Calibration and validation of accelerometry to measure physical activity in adult clinical groups: A systematic review,"A growing body of research calibrating and validating accelerometers to classify physical activity intensities has led to a range of cut-points. However, the applicability of current calibration protocols to clinical populations remains to be addressed. The aim of this review was to evaluate the accuracy of the methods for calibrating and validating of accelerometers to estimate physical activity intensity thresholds for clinical populations. Six databases were searched between March and July to 2017 using text words and subject headings. Studies developing moderate-to-vigorous intensity physical activity cut-points for adult clinical populations were included. The risk of bias was assessed using the health measurement instruments and a specific checklist for calibration studies. A total of 543,741 titles were found and 323 articles were selected for full-text assessment, with 11 meeting the inclusion criteria. Twenty-three different methods for calibration were identified using different models of ActiGraph and Actical accelerometers. Disease-specific cut-points ranged from 591 to 2717 countsmin-1 and were identified for two main groups of clinical conditions: neuromusculoskeletal disorders and metabolic diseases. The heterogeneity in the available clinical protocols hinders the applicability and comparison of the developed cut-points. As such, a mixed protocol containing a controlled laboratory exercise test and activities of daily-life is suggested. It is recommended that this be combined with a statistical approach that allows for adjustments according to disease severity or the use of machine learning models. Finally, this review highlights the generalisation of cut-points developed on healthy populations to clinical populations is inappropriate.",2019-11-01,0,1782,118,554
453,30316551,A Survey on Machine Learning Approaches for Automatic Detection of Voice Disorders,"The human voice production system is an intricate biological device capable of modulating pitch and loudness. Inherent internal and/or external factors often damage the vocal folds and result in some change of voice. The consequences are reflected in body functioning and emotional standing. Hence, it is paramount to identify voice changes at an early stage and provide the patient with an opportunity to overcome any ramification and enhance their quality of life. In this line of work, automatic detection of voice disorders using machine learning techniques plays a key role, as it is proven to help ease the process of understanding the voice disorder. In recent years, many researchers have investigated techniques for an automated system that helps clinicians with early diagnosis of voice disorders. In this paper, we present a survey of research work conducted on automatic detection of voice disorders and explore how it is able to identify the different types of voice disorders. We also analyze different databases, feature extraction techniques, and machine learning approaches used in these research works.",2019-11-01,3,1120,82,554
1742,31585696,Ethics of Artificial Intelligence in Radiology: Summary of the Joint European and North American Multisociety Statement,"This is a condensed summary of an international multisociety statement on ethics of artificial intelligence (AI) in radiology produced by the ACR, European Society of Radiology, RSNA, Society for Imaging Informatics in Medicine, European Society of Medical Imaging Informatics, Canadian Association of Radiologists, and American Association of Physicists in Medicine. AI has great potential to increase efficiency and accuracy throughout radiology, but it also carries inherent pitfalls and biases. Widespread use of AI-based intelligent and autonomous systems in radiology can increase the risk of systemic errors with high consequence and highlights complex ethical and societal issues. Currently, there is little experience using AI for patient care in diverse clinical settings. Extensive research is needed to understand how to best deploy AI in clinical practice. This statement highlights our consensus that ethical use of AI in radiology should promote well-being, minimize harm, and ensure that the benefits and harms are distributed among stakeholders in a just manner. We believe AI should respect human rights and freedoms, including dignity and privacy. It should be designed for maximum transparency and dependability. Ultimate responsibility and accountability for AI remains with its human designers and operators for the foreseeable future. The radiology community should start now to develop codes of ethics and practice for AI that promote any use that helps patients and the common good and should block use of radiology data and algorithms for financial gain without those two attributes.",2019-11-01,5,1609,119,554
2183,31677058,Non-invasive continuous blood pressure monitoring systems: current and proposed technology issues and challenges,"High blood pressure (BP) or hypertension is the single most crucial adjustable risk factor for cardiovascular diseases (CVDs) and monitoring the arterial blood pressure (ABP) is an efficient way to detect and control the prevalence of the cardiovascular health of patients. Therefore, monitoring the regulation of BP during patients' daily life plays a critical role in the ambulatory setting and the latest mobile health technology. In recent years, many studies have been conducted to explore the feasibility and performance of such techniques in the health care system. The ultimate aim of these studies is to find and develop an alternative to conventional BP monitoring by using cuff-less, easy-to-use, fast, and cost-effective devices for controlling and lowering the physical harm of CVDs to the human body. However, most of the current studies are at the prototype phase and face a range of issues and challenges to meet clinical standards. This review focuses on the description and analysis of the latest continuous and cuff-less methods along with their key challenges and barriers. Particularly, most advanced and standard technologies including pulse transit time (PTT), ultrasound, pulse arrival time (PAT), and machine learning are investigated. The accuracy, portability, and comfort of use of these technologies, and the ability to integrate to the wearable healthcare system are discussed. Finally, the future directions for further study are suggested.",2019-11-01,4,1471,112,554
1274,32039134,Deep Learning for Deep Chemistry: Optimizing the Prediction of Chemical Patterns,"Computational Chemistry is currently a synergistic assembly between ab initio calculations, simulation, machine learning (ML) and optimization strategies for describing, solving and predicting chemical data and related phenomena. These include accelerated literature searches, analysis and prediction of physical and quantum chemical properties, transition states, chemical structures, chemical reactions, and also new catalysts and drug candidates. The generalization of scalability to larger chemical problems, rather than specialization, is now the main principle for transforming chemical tasks in multiple fronts, for which systematic and cost-effective solutions have benefited from ML approaches, including those based on deep learning (e.g. quantum chemistry, molecular screening, synthetic route design, catalysis, drug discovery). The latter class of ML algorithms is capable of combining raw input into layers of intermediate features, enabling bench-to-bytes designs with the potential to transform several chemical domains. In this review, the most exciting developments concerning the use of ML in a range of different chemical scenarios are described. A range of different chemical problems and respective rationalization, that have hitherto been inaccessible due to the lack of suitable analysis tools, is thus detailed, evidencing the breadth of potential applications of these emerging multidimensional approaches. Focus is given to the models, algorithms and methods proposed to facilitate research on compound design and synthesis, materials design, prediction of binding, molecular activity, and soft matter behavior. The information produced by pairing Chemistry and ML, through data-driven analyses, neural network predictions and monitoring of chemical systems, allows (i) prompting the ability to understand the complexity of chemical data, (ii) streamlining and designing experiments, (ii) discovering new molecular targets and materials, and also (iv) planning or rethinking forthcoming chemical challenges. In fact, optimization engulfs all these tasks directly.",2019-11-01,5,2090,80,554
1065,31465619,Machine learning and big data analytics in bipolar disorder: A position paper from the International Society for Bipolar Disorders Big Data Task Force,"Objectives:                    The International Society for Bipolar Disorders Big Data Task Force assembled leading researchers in the field of bipolar disorder (BD), machine learning, and big data with extensive experience to evaluate the rationale of machine learning and big data analytics strategies for BD.              Method:                    A task force was convened to examine and integrate findings from the scientific literature related to machine learning and big data based studies to clarify terminology and to describe challenges and potential applications in the field of BD. We also systematically searched PubMed, Embase, and Web of Science for articles published up to January 2019 that used machine learning in BD.              Results:                    The results suggested that big data analytics has the potential to provide risk calculators to aid in treatment decisions and predict clinical prognosis, including suicidality, for individual patients. This approach can advance diagnosis by enabling discovery of more relevant data-driven phenotypes, as well as by predicting transition to the disorder in high-risk unaffected subjects. We also discuss the most frequent challenges that big data analytics applications can face, such as heterogeneity, lack of external validation and replication of some studies, cost and non-stationary distribution of the data, and lack of appropriate funding.              Conclusion:                    Machine learning-based studies, including atheoretical data-driven big data approaches, provide an opportunity to more accurately detect those who are at risk, parse-relevant phenotypes as well as inform treatment selection and prognosis. However, several methodological challenges need to be addressed in order to translate research findings to clinical settings.",2019-11-01,6,1834,150,554
172,30053138,Sequencing era methods for identifying signatures of selection in the genome,"Insights into genetic loci which are under selection and their functional roles contribute to increased understanding of the patterns of phenotypic variation we observe today. The availability of whole-genome sequence data, for humans and other species, provides opportunities to investigate adaptation and evolution at unprecedented resolution. Many analytical methods have been developed to interrogate these large data sets and characterize signatures of selection in the genome. We review here recently developed methods and consider the impact of increased computing power and data availability on the detection of selection signatures. Consideration of demography, recombination and other confounding factors is important, and use of a range of methods in combination is a powerful route to resolving different forms of selection in genome sequence data. Overall, a substantial improvement in methods for application to whole-genome sequencing is evident, although further work is required to develop robust and computationally efficient approaches which may increase reproducibility across studies.",2019-11-01,3,1105,76,554
2440,30770893,Deep neural networks in psychiatry,"Machine and deep learning methods, today's core of artificial intelligence, have been applied with increasing success and impact in many commercial and research settings. They are powerful tools for large scale data analysis, prediction and classification, especially in very data-rich environments (""big data""), and have started to find their way into medical applications. Here we will first give an overview of machine learning methods, with a focus on deep and recurrent neural networks, their relation to statistics, and the core principles behind them. We will then discuss and review directions along which (deep) neural networks can be, or already have been, applied in the context of psychiatry, and will try to delineate their future potential in this area. We will also comment on an emerging area that so far has been much less well explored: by embedding semantically interpretable computational models of brain dynamics or behavior into a statistical machine learning context, insights into dysfunction beyond mere prediction and classification may be gained. Especially this marriage of computational models with statistical inference may offer insights into neural and behavioral mechanisms that could open completely novel avenues for psychiatric treatment.",2019-11-01,21,1274,34,554
153,30084867,"LncFinder: an integrated platform for long non-coding RNA identification utilizing sequence intrinsic composition, structural information and physicochemical property","Discovering new long non-coding RNAs (lncRNAs) has been a fundamental step in lncRNA-related research. Nowadays, many machine learning-based tools have been developed for lncRNA identification. However, many methods predict lncRNAs using sequence-derived features alone, which tend to display unstable performances on different species. Moreover, the majority of tools cannot be re-trained or tailored by users and neither can the features be customized or integrated to meet researchers' requirements. In this study, features extracted from sequence-intrinsic composition, secondary structure and physicochemical property are comprehensively reviewed and evaluated. An integrated platform named LncFinder is also developed to enhance the performance and promote the research of lncRNA identification. LncFinder includes a novel lncRNA predictor using the heterologous features we designed. Experimental results show that our method outperforms several state-of-the-art tools on multiple species with more robust and satisfactory results. Researchers can additionally employ LncFinder to extract various classic features, build classifier with numerous machine learning algorithms and evaluate classifier performance effectively and efficiently. LncFinder can reveal the properties of lncRNA and mRNA from various perspectives and further inspire lncRNA-protein interaction prediction and lncRNA evolution analysis. It is anticipated that LncFinder can significantly facilitate lncRNA-related research, especially for the poorly explored species. LncFinder is released as R package (https://CRAN.R-project.org/package=LncFinder). A web server (http://bmbl.sdstate.edu/lncfinder/) is also developed to maximize its availability.",2019-11-01,13,1727,166,554
1055,31486179,Machine Learning Interatomic Potentials as Emerging Tools for Materials Science,"Atomic-scale modeling and understanding of materials have made remarkable progress, but they are still fundamentally limited by the large computational cost of explicit electronic-structure methods such as density-functional theory. This Progress Report shows how machine learning (ML) is currently enabling a new degree of realism in materials modeling: by ""learning"" electronic-structure data, ML-based interatomic potentials give access to atomistic simulations that reach similar accuracy levels but are orders of magnitude faster. A brief introduction to the new tools is given, and then, applications to some select problems in materials science are highlighted: phase-change materials for memory devices; nanoparticle catalysts; and carbon-based electrodes for chemical sensing, supercapacitors, and batteries. It is hoped that the present work will inspire the development and wider use of ML-based interatomic potentials in diverse areas of materials research.",2019-11-01,8,969,79,554
1273,32039237,Machine Learning for Assessment of Coronary Artery Disease in Cardiac CT: A Survey,"Cardiac computed tomography (CT) allows rapid visualization of the heart and coronary arteries with high spatial resolution. However, analysis of cardiac CT scans for manifestation of coronary artery disease is time-consuming and challenging. Machine learning (ML) approaches have the potential to address these challenges with high accuracy and consistent performance. In this mini review, we present a survey of the literature on ML-based analysis of coronary artery disease in cardiac CT. We summarize ML methods for detection and characterization of atherosclerotic plaque as well as anatomically and functionally significant coronary artery stenosis.",2019-11-01,6,655,82,554
2188,33693089,Application of Neural Networks to 12-Lead Electrocardiography - Current Status and Future Directions,"The 12-lead electrocardiogram (ECG) is a fast, non-invasive, powerful tool to diagnose or to evaluate the risk of various cardiac diseases. The vast majority of arrhythmias are diagnosed solely on 12-lead ECG. Initial detection of myocardial ischemia such as myocardial infarction (MI), acute coronary syndrome (ACS) and effort angina is also dependent upon 12-lead ECG. ECG reflects the electrophysiological state of the heart through body mass, and thus contains important information on the electricity-dependent function of the human heart. Indeed, 12-lead ECG data are complex. Therefore, the clinical interpretation of 12-lead ECG requires intense training, but still is prone to interobserver variability. Even with rich clinically relevant data, non-trained physicians cannot efficiently use this powerful tool. Furthermore, recent studies have shown that 12-lead ECG may contain information that is not recognized even by well-trained experts but which can be extracted by computer. Artificial intelligence (AI) based on neural networks (NN) has emerged as a strong tool to extract valuable information from ECG for clinical decision making. This article reviews the current status of the application of NN-based AI to the interpretation of 12-lead ECG and also discusses the current problems and future directions.",2019-11-01,0,1324,100,554
1000,31861438,Knowledge Generation with Rule Induction in Cancer Omics,"The explosion of omics data availability in cancer research has boosted the knowledge of the molecular basis of cancer, although the strategies for its definitive resolution are still not well established. The complexity of cancer biology, given by the high heterogeneity of cancer cells, leads to the development of pharmacoresistance for many patients, hampering the efficacy of therapeutic approaches. Machine learning techniques have been implemented to extract knowledge from cancer omics data in order to address fundamental issues in cancer research, as well as the classification of clinically relevant sub-groups of patients and for the identification of biomarkers for disease risk and prognosis. Rule induction algorithms are a group of pattern discovery approaches that represents discovered relationships in the form of human readable associative rules. The application of such techniques to the modern plethora of collected cancer omics data can effectively boost our understanding of cancer-related mechanisms. In fact, the capability of these methods to extract a huge amount of human readable knowledge will eventually help to uncover unknown relationships between molecular attributes and the malignant phenotype. In this review, we describe applications and strategies for the usage of rule induction approaches in cancer omics data analysis. In particular, we explore the canonical applications and the future challenges and opportunities posed by multi-omics integration problems.",2019-12-01,1,1501,56,524
1081,31437696,Can Machine Learning help us in dealing with treatment resistant depression? A review,"Background:                    About one third of patients treated with antidepressant do not show sufficient symptoms relief and up to 15% of patients remain symptomatic even after multiple trials are applied, configuring a state called treatment resistant depression (TRD). A clear definition of this state and the understanding of underlying mechanisms contributing to chronic disability caused by major depressive disorder is still unknown. Therefore, Machine Learning (ML) techniques emerged in the last years as interesting approaches to deal with such complex problems.              Methods:                    We performed a bibliographic search on Pubmed, Google Scholar and Medline of clinical, imaging, genetic and EEG ML classification studies on treatment-responding depression and TRD as well as studies trying to predict response to a specific treatment in already established TRD. The inclusion criteria were met by eleven studies. Seven focused on the definition of predictors of TRD onset while four attempted to predict the response to specific treatments in TRD.              Results:                    The results showed that it seems possible to classify between responders MDD and TRD with good accuracies based on clinical variables. Moreover, some studies reported the possibility of using EEG measures to predict response to different pharmacological and non-pharmacological treatments in established TRD.              Limitations:                    The definition of TRD, the selection of variables together with ML algorithms and pipelines varies across the studies, ultimately determining the unfeasibility to implement these models in clinical practice.              Conclusions:                    The findings suggest that ML could be a valid approach to increase our understanding of TRD and to better classify and stratify this disorder, which may ultimately help clinicians in the assessment of major depressive disorders.",2019-12-01,5,1959,85,524
992,31878065,Tackling Faults in the Industry 4.0 Era-A Survey of Machine-Learning Solutions and Key Aspects,"The recent advancements in the fields of artificial intelligence (AI) and machine learning (ML) have affected several research fields, leading to improvements that could not have been possible with conventional optimization techniques. Among the sectors where AI/ML enables a plethora of opportunities, industrial manufacturing can expect significant gains from the increased process automation. At the same time, the introduction of the Industrial Internet of Things (IIoT), providing improved wireless connectivity for real-time manufacturing data collection and processing, has resulted in the culmination of the fourth industrial revolution, also known as Industry 4.0. In this survey, we focus on the vital processes of fault detection, prediction and prevention in Industry 4.0 and present recent developments in ML-based solutions. We start by examining various proposed cloud/fog/edge architectures, highlighting their importance for acquiring manufacturing data in order to train the ML algorithms. In addition, as faults might also occur from sources beyond machine degradation, the potential of ML in safeguarding cyber-security is thoroughly discussed. Moreover, a major concern in the Industry 4.0 ecosystem is the role of human operators and workers. Towards this end, a detailed overview of ML-based human-machine interaction techniques is provided, allowing humans to be in-the-loop of the manufacturing processes in a symbiotic manner with minimal errors. Finally, open issues in these relevant fields are given, stimulating further research.",2019-12-01,3,1559,94,524
994,33489002,Machine learning applications in drug development,"Due to the huge amount of biological and medical data available today, along with well-established machine learning algorithms, the design of largely automated drug development pipelines can now be envisioned. These pipelines may guide, or speed up, drug discovery; provide a better understanding of diseases and associated biological phenomena; help planning preclinical wet-lab experiments, and even future clinical trials. This automation of the drug development process might be key to the current issue of low productivity rate that pharmaceutical companies currently face. In this survey, we will particularly focus on two classes of methods: sequential learning and recommender systems, which are active biomedical fields of research.",2019-12-01,2,741,49,524
1040,31498973,Machine learning applications in the diagnosis of leukemia: Current trends and future directions,"Machine learning (ML) offers opportunities to advance pathological diagnosis, especially with increasing trends in digitalizing microscopic images. Diagnosing leukemia is time-consuming and challenging in many areas globally and there is a growing trend in utilizing ML techniques for its diagnosis. In this review, we aimed to describe the literature of ML utilization in the diagnosis of the four common types of leukemia: acute lymphocytic leukemia (ALL), chronic lymphocytic leukemia (CLL), acute myeloid leukemia (AML), and chronic myelogenous leukemia (CML). Using a strict selection criterion, utilizing MeSH terminology and Boolean logic, an electronic search of MEDLINE and IEEE Xplore Digital Library was performed. The electronic search was complemented by handsearching of references of related studies and the top results of Google Scholar. The full texts of 58 articles were reviewed, out of which, 22 studies were included. The number of studies discussing ALL, AML, CLL, and CML was 12, 8, 3, and 1, respectively. No studies were prospectively applying algorithms in real-world scenarios. Majority of studies had small and homogenous samples and used supervised learning for classification tasks. 91% of the studies were performed after 2010, and 74% of the included studies applied ML algorithms to microscopic diagnosis of leukemia. The included studies illustrated the need to develop the field of ML research, including the transformation from solely designing algorithms to practically applying them clinically.",2019-12-01,4,1532,96,524
1083,31435868,Deep Learning and Neurology: A Systematic Review,"Deciphering the massive volume of complex electronic data that has been compiled by hospital systems over the past decades has the potential to revolutionize modern medicine, as well as present significant challenges. Deep learning is uniquely suited to address these challenges, and recent advances in techniques and hardware have poised the field of medical machine learning for transformational growth. The clinical neurosciences are particularly well positioned to benefit from these advances given the subtle presentation of symptoms typical of neurologic disease. Here we review the various domains in which deep learning algorithms have already provided impetus for change-areas such as medical image analysis for the improved diagnosis of Alzheimer's disease and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of Alzheimer's, autism spectrum disorder, and attention deficit hyperactivity disorder; and mining of microscopic electroencephalogram signals and granular genetic signatures. We additionally note important challenges in the integration of deep learning tools in the clinical setting and discuss the barriers to tackling the challenges that currently exist.",2019-12-01,11,1301,48,524
1839,32729254,Machine learning approaches to study glioblastoma: A review of the last decade of applications,"Background:                    Glioblastoma (GB, formally glioblastoma multiforme) is a malignant type of brain cancer that currently has no cure and is characterized by being highly heterogeneous with high rates of re-incidence and therapy resistance. Thus, it is urgent to characterize the mechanisms of GB pathogenesis to help researchers identify novel therapeutic targets to cure this devastating disease. Recently, a promising approach to identifying novel therapeutic targets is the integration of tumor omics data with clinical information using machine learning (ML) techniques.              Recent findings:                    ML has become a valuable addition to the researcher's toolbox, thanks to its flexibility, multidimensional approach, and a growing community of users. The goal of this review is to introduce basic concepts and applications of ML for studying GB to clinicians and practitioners who are new to data science. ML applications include exploring large data sets, finding new relevant patterns, predicting outcomes, or merely understanding associations of the complex molecular networks presented within the tumor. Here, we review ML applications published between 2008 and 2018 and discuss ML strategies intending to identify new potential therapeutic targets to improve the management and treatment of GB.              Conclusions:                    ML applications to study GB vary in purpose and complexity, with positive results. In GB studies, ML is often used to analyze high-dimensional datasets with prediction or classification as a primary goal. Despite the strengths of ML techniques, they are not fail-safe and methodological issues can occur in GB studies that use them. This is why researchers need to be aware of these issues when planning and appraising studies that apply ML to the study of GB.",2019-12-01,0,1843,94,524
991,31878333,Advances in Structure Modeling Methods for Cryo-Electron Microscopy Maps,"Cryo-electron microscopy (cryo-EM) has now become a widely used technique for structure determination of macromolecular complexes. For modeling molecular structures from density maps of different resolutions, many algorithms have been developed. These algorithms can be categorized into rigid fitting, flexible fitting, and de novo modeling methods. It is also observed that machine learning (ML) techniques have been increasingly applied following the rapid progress of the ML field. Here, we review these different categories of macromolecule structure modeling methods and discuss their advances over time.",2019-12-01,3,609,72,524
1027,31521378,Machine learning in the electrocardiogram,"The electrocardiogram is the most widely used diagnostic tool that records the electrical activity of the heart and, therefore, its use for identifying markers for early diagnosis and detection is of paramount importance. In the last years, the huge increase of electronic health records containing a systematised collection of different type of digitalised medical data, together with new tools to analyse this large amount of data in an efficient way have re-emerged the field of machine learning in healthcare innovation. This review describes the most recent machine learning-based systems applied to the electrocardiogram as well as pros and cons in the use of these techniques. Machine learning, including deep learning, have shown to be powerful tools for aiding clinicians in patient screening and risk stratification tasks. However, they do not provide the physiological basis of classification outcomes. Computational modelling and simulation can help in the interpretation and understanding of key physiologically meaningful ECG biomarkers extracted from machine learning techniques.",2019-12-01,4,1094,41,524
2404,30855159,Methodological advances in statistical prediction,"Thirty years ago, Dawes, Faust, and Meehl (1989) argued that mental health professionals should routinely use statistical prediction rules to describe and diagnose clients, predict behaviors, and formulate treatment plans. Subsequent research has supported their claim that statistical prediction performs well when compared to clinical judgment. However, many of the things we thought we knew about statistical prediction have changed. The purpose of this literature review is to describe methodological advances in statistical prediction. Three broad areas are covered. First, while statistical prediction rules are valuable for criterion-referenced assessment (e.g., predicting violence, recidivism, treatment outcomes), they are valuable only for some norm-referenced assessment tasks (e.g., diagnosis but not describing personality and psychopathology). Second, statistical prediction is particularly prominent for the prediction of violence and criminal recidivism. Results from this area will be used to describe the validity of traditional clinical judgment, structured professional judgment, and statistical prediction. The results support the use of both structured professional judgment and statistical prediction. The effect of allowing professionals to override statistical predictions consistently led to lower validity. Third, issues in building statistical prediction rules are described, including the assignment of weights to predictors, the emergence of new statistical analyses (e.g., machine learning), and the role of theory. As research has progressed, statistical prediction has become one of the most exciting areas of psychological assessment. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",2019-12-01,3,1731,49,524
988,31881663,Applications and Trends of Machine Learning in Genomics and Phenomics for Next-Generation Breeding,"Crops are the major source of food supply and raw materials for the processing industry. A balance between crop production and food consumption is continually threatened by plant diseases and adverse environmental conditions. This leads to serious losses every year and results in food shortages, particularly in developing countries. Presently, cutting-edge technologies for genome sequencing and phenotyping of crops combined with progress in computational sciences are leading a revolution in plant breeding, boosting the identification of the genetic basis of traits at a precision never reached before. In this frame, machine learning (ML) plays a pivotal role in data-mining and analysis, providing relevant information for decision-making towards achieving breeding targets. To this end, we summarize the recent progress in next-generation sequencing and the role of phenotyping technologies in genomics-assisted breeding toward the exploitation of the natural variation and the identification of target genes. We also explore the application of ML in managing big data and predictive models, reporting a case study using microRNAs (miRNAs) to identify genes related to stress conditions.",2019-12-01,5,1195,98,524
987,31883552,De novo Molecular Design with Generative Long Short-term Memory,"Drug discovery benefits from computational models aiding the identification of new chemical matter with bespoke properties. The field of de novo drug design has been particularly revitalized by adaptation of generative machine learning models from the field of natural language processing. These deep neural network models are trained on recognizing molecular structures and generate new molecular entities without relying on pre-determined sets of molecular building blocks and chemical transformations for virtual molecule construction. Implicit representation of chemical knowledge provides an alternative to formulating the molecular design task in terms of the established, explicit chemical vocabulary. Here, we review de novo molecular design approaches from the field of 'artificial intelligence', focusing on instances of deep generative models, and highlight the prospective application of long short-term memory models to hit and lead finding in medicinal chemistry.",2019-12-01,5,977,63,524
984,31884734,Medical Big Data Is Not Yet Available: Why We Need Realism Rather than Exaggeration,"Most people are now familiar with the concepts of big data, deep learning, machine learning, and artificial intelligence (AI) and have a vague expectation that AI using medical big data can be used to improve the quality of medical care. However, the expectation that big data could change the field of medicine is inconsistent with the current reality. The clinical meaningfulness of the results of research using medical big data needs to be examined. Medical staff needs to be clear about the purpose of AI that utilizes medical big data and to focus on the quality of this data, rather than the quantity. Further, medical professionals should understand the necessary precautions for using medical big data, as well as its advantages. No doubt that someday, medical big data will play an essential role in healthcare; however, at present, it seems too early to actively use it in clinical practice. The field continues to work toward developing medical big data and making it appropriate for healthcare. Researchers should continue to engage in empirical research to ensure that appropriate processes are in place to empirically evaluate the results of its use in healthcare.",2019-12-01,3,1179,83,524
983,31885533,Dragonfly Algorithm and Its Applications in Applied Science Survey,"One of the most recently developed heuristic optimization algorithms is dragonfly by Mirjalili. Dragonfly algorithm has shown its ability to optimizing different real-world problems. It has three variants. In this work, an overview of the algorithm and its variants is presented. Moreover, the hybridization versions of the algorithm are discussed. Furthermore, the results of the applications that utilized the dragonfly algorithm in applied science are offered in the following area: machine learning, image processing, wireless, and networking. It is then compared with some other metaheuristic algorithms. In addition, the algorithm is tested on the CEC-C06 2019 benchmark functions. The results prove that the algorithm has great exploration ability and its convergence rate is better than the other algorithms in the literature, such as PSO and GA. In general, in this survey, the strong and weak points of the algorithm are discussed. Furthermore, some future works that will help in improving the algorithm's weak points are recommended. This study is conducted with the hope of offering beneficial information about dragonfly algorithm to the researchers who want to study the algorithm.",2019-12-01,1,1196,66,524
979,31888592,DeepFHR: intelligent prediction of fetal Acidemia using fetal heart rate signals based on convolutional neural network,"Background:                    Fetal heart rate (FHR) monitoring is a screening tool used by obstetricians to evaluate the fetal state. Because of the complexity and non-linearity, a visual interpretation of FHR signals using common guidelines usually results in significant subjective inter-observer and intra-observer variability.              Objective:                    Therefore, computer aided diagnosis (CAD) systems based on advanced artificial intelligence (AI) technology have recently been developed to assist obstetricians in making objective medical decisions.              Methods:                    In this work, we present an 8-layer deep convolutional neural network (CNN) framework to automatically predict fetal acidemia. After signal preprocessing, the input 2-dimensional (2D) images are obtained using the continuous wavelet transform (CWT), which provides a better way to observe and capture the hidden characteristic information of the FHR signals in both the time and frequency domains. Unlike the conventional machine learning (ML) approaches, this work does not require the execution of complex feature engineering, i.e., feature extraction and selection. In fact, 2D CNN model can self-learn useful features from the input data with the prerequisite of not losing informative features, representing the tremendous advantage of deep learning (DL) over ML.              Results:                    Based on the test open-access database (CTU-UHB), after comprehensive experimentation, we achieved better classification performance using the optimal CNN configuration compared to other state-of-the-art methods: the averaged ten-fold cross-validation of the accuracy, sensitivity, specificity, quality index defined as the geometric mean of the sensitivity and specificity, and the area under the curve yielded results of 98.34, 98.22, 94.87, 96.53 and 97.82%, respectively CONCLUSIONS: Once the proposed CNN model is successfully trained, the corresponding CAD system can be served as an effective tool to predict fetal asphyxia objectively and accurately.",2019-12-01,2,2085,118,524
977,31890287,Wide-field imaging of sickle retinopathy,"Background:                    Wide-field imaging is a newer retinal imaging technology, capturing up to 200 degrees of the retina in a single photograph. Individuals with sickle cell retinopathy commonly exhibit peripheral retinal ischemia. Patients with proliferative sickle cell retinopathy develop pathologic retinal neovascularization of the peripheral retina which may progress into sight-threatening sequelae of vitreous hemorrhage and/or retinal detachment. The purpose of this review is to provide an overview of current and future applications of wide-field retinal imaging for sickle cell retinopathy, and recommend indications for best use.              Main body:                    There are several advantages to wide-field imaging in the clinical management of sickle cell disease patients. Retrospective and prospective studies support the success of wide-field imaging in detecting more sickle cell induced retinal microvascular abnormalities than traditional non-wide-field imaging. Clinicians can easily capture a greater extent of the retinal periphery in a patient's clinical baseline imaging to follow the changes at an earlier point and determine the rate of progression over time. Wide-field imaging minimizes patient and photographer burden, necessitating less photos and technical skill to capture the peripheral retina. Minimizing the number of necessary images can be especially helpful for pediatric patients with sickle cell retinopathy. Wide-field imaging has already been successful in identifying new biomarkers and risk factors for the development of proliferative sickle cell retinopathy. While these advantages should be considered, clinicians need to perform a careful risk-benefit analysis before ordering this test. Although wide-field fluorescein angiography successfully detects additional pathologic abnormalities compared to traditional imaging, a recent research study suggests that peripheral changes differentially detected by wide-field imaging may not change clinical management for most sickle cell patients.              Conclusions:                    While wide-field imaging may not carry a clinically significant direct benefit to all patients, it shows future promise in expanding our knowledge of sickle cell retinopathy. Clinicians may monitor peripheral retinal pathology such as retinal ischemia and retinal neovascularization over progressive time points, and use sequential wide-field retinal images to monitor response to treatment. Future applications for wide-field imaging may include providing data to facilitate machine learning, and potential use in tele-ophthalmology screening for proliferative sickle retinopathy.",2019-12-01,2,2685,40,524
968,31905969,"Epigenetics Analysis and Integrated Analysis of Multiomics Data, Including Epigenetic Data, Using Artificial Intelligence in the Era of Precision Medicine","To clarify the mechanisms of diseases, such as cancer, studies analyzing genetic mutations have been actively conducted for a long time, and a large number of achievements have already been reported. Indeed, genomic medicine is considered the core discipline of precision medicine, and currently, the clinical application of cutting-edge genomic medicine aimed at improving the prevention, diagnosis and treatment of a wide range of diseases is promoted. However, although the Human Genome Project was completed in 2003 and large-scale genetic analyses have since been accomplished worldwide with the development of next-generation sequencing (NGS), explaining the mechanism of disease onset only using genetic variation has been recognized as difficult. Meanwhile, the importance of epigenetics, which describes inheritance by mechanisms other than the genomic DNA sequence, has recently attracted attention, and, in particular, many studies have reported the involvement of epigenetic deregulation in human cancer. So far, given that genetic and epigenetic studies tend to be accomplished independently, physiological relationships between genetics and epigenetics in diseases remain almost unknown. Since this situation may be a disadvantage to developing precision medicine, the integrated understanding of genetic variation and epigenetic deregulation appears to be now critical. Importantly, the current progress of artificial intelligence (AI) technologies, such as machine learning and deep learning, is remarkable and enables multimodal analyses of big omics data. In this regard, it is important to develop a platform that can conduct multimodal analysis of medical big data using AI as this may accelerate the realization of precision medicine. In this review, we discuss the importance of genome-wide epigenetic and multiomics analyses using AI in the era of precision medicine.",2019-12-01,11,1890,154,524
975,31890956,A bibliometric study on intelligent techniques of bankruptcy prediction for corporate firms,"Bibliometric analysis is an effective method to carry out quantitative study of academic output to address the research trends on a given area of investigation through analysing existing documents. This paper aims to explore the application of intelligent techniques in bankruptcy predictions so as to assess its progress and describe the research trend through bibliometric analysis over the last five decades. The results indicate that, although there is a significant increase in publication number since the 2008 financial crisis, the collaboration among authors is weak, especially at the international dimension. Also, the findings provide a comprehensive view of interdisciplinary research on bankruptcy modelling in finance, business management and computer science fields. The authors sought to contribute to the theoretical development of bankruptcy prediction modeling by bringing new knowledge and key insights. Artificial intelligent techniques are now serving as important alternatives to statistical methods and demonstrate very promising results. This paper has both theoretical and practical implications. First, it provides insights for scholars into the theoretical evolution and intellectual structure for conducting future research in this field. Second, it sheds light on identifying under-explored machine learning techniques applied in bankruptcy prediction which can be crucial in management and decision-making for corporate firm managers and policy makers.",2019-12-01,0,1483,91,524
1020,31823128,Ethical considerations about artificial intelligence for prognostication in intensive care,"Background:                    Prognosticating the course of diseases to inform decision-making is a key component of intensive care medicine. For several applications in medicine, new methods from the field of artificial intelligence (AI) and machine learning have already outperformed conventional prediction models. Due to their technical characteristics, these methods will present new ethical challenges to the intensivist.              Results:                    In addition to the standards of data stewardship in medicine, the selection of datasets and algorithms to create AI prognostication models must involve extensive scrutiny to avoid biases and, consequently, injustice against individuals or groups of patients. Assessment of these models for compliance with the ethical principles of beneficence and non-maleficence should also include quantification of predictive uncertainty. Respect for patients' autonomy during decision-making requires transparency of the data processing by AI models to explain the predictions derived from these models. Moreover, a system of continuous oversight can help to maintain public trust in this technology. Based on these considerations as well as recent guidelines, we propose a pathway to an ethical implementation of AI-based prognostication. It includes a checklist for new AI models that deals with medical and technical topics as well as patient- and system-centered issues.              Conclusion:                    AI models for prognostication will become valuable tools in intensive care. However, they require technical refinement and a careful implementation according to the standards of medical ethics.",2019-12-01,4,1670,90,524
1021,31821153,"How New Technologies Can Improve Prediction, Assessment, and Intervention in Obsessive-Compulsive Disorder (e-OCD): Review","Background:                    New technologies are set to profoundly change the way we understand and manage psychiatric disorders, including obsessive-compulsive disorder (OCD). Developments in imaging and biomarkers, along with medical informatics, may well allow for better assessments and interventions in the future. Recent advances in the concept of digital phenotype, which involves using computerized measurement tools to capture the characteristics of a given psychiatric disorder, is one paradigmatic example.              Objective:                    The impact of new technologies on health professionals' practice in OCD care remains to be determined. Recent developments could disrupt not just their clinical practices, but also their beliefs, ethics, and representations, even going so far as to question their professional culture. This study aimed to conduct an extensive review of new technologies in OCD.              Methods:                    We conducted the review by looking for titles in the PubMed database up to December 2017 that contained the following terms: [Obsessive] AND [Smartphone] OR [phone] OR [Internet] OR [Device] OR [Wearable] OR [Mobile] OR [Machine learning] OR [Artificial] OR [Biofeedback] OR [Neurofeedback] OR [Momentary] OR [Computerized] OR [Heart rate variability] OR [actigraphy] OR [actimetry] OR [digital] OR [virtual reality] OR [Tele] OR [video].              Results:                    We analyzed 364 articles, of which 62 were included. Our review was divided into 3 parts: prediction, assessment (including diagnosis, screening, and monitoring), and intervention.              Conclusions:                    The review showed that the place of connected objects, machine learning, and remote monitoring has yet to be defined in OCD. Smartphone assessment apps and the Web Screening Questionnaire demonstrated good sensitivity and adequate specificity for detecting OCD symptoms when compared with a full-length structured clinical interview. The ecological momentary assessment procedure may also represent a worthy addition to the current suite of assessment tools. In the field of intervention, CBT supported by smartphone, internet, or computer may not be more effective than that delivered by a qualified practitioner, but it is easy to use, well accepted by patients, reproducible, and cost-effective. Finally, new technologies are enabling the development of new therapies, including biofeedback and virtual reality, which focus on the learning of coping skills. For them to be used, these tools must be properly explained and tailored to individual physician and patient profiles.",2019-12-01,2,2652,122,524
1034,31513441,4D- quantitative structure-activity relationship modeling: making a comeback,"Introduction: Predictive Quantitative Structure-Activity Relationship (QSAR) modeling has become an essential methodology for rapidly assessing various properties of chemicals. The vast majority of these QSAR models utilize numerical descriptors derived from the two- and/or three-dimensional structures of molecules. However, the conformation-dependent characteristics of flexible molecules and their dynamic interactions with biological target(s) is/are not encoded by these descriptors, leading to limited prediction performances and reduced interpretability. 2D/3D QSAR models are successful for virtual screening, but typically suffer at lead optimization stages. That is why conformation-dependent 4D-QSAR modeling methods were developed two decades ago. However, these methods have always suffered from the associated computational cost. Recently, 4D-QSAR has been experiencing a significant come-back due to rapid advances in GPU-accelerated molecular dynamic simulations and modern machine learning techniques. Areas covered: Herein, the authors briefly review the literature regarding 4D-QSAR modeling and describe its modern workflow called MD-QSAR. Challenges and current limitations are also highlighted. Expert opinion: The development of hyper-predictive MD-QSAR models could represent a disruptive technology for analyzing, understanding, and optimizing dynamic protein-ligand interactions with countless applications for drug discovery and chemical toxicity assessment. Therefore, there has never been a better time and relevance for molecular modeling teams to engage in hyper-predictive MD-QSAR modeling.",2019-12-01,5,1623,76,524
1073,31451330,"Artificial Intelligence in Nephrology: Core Concepts, Clinical Applications, and Perspectives","Artificial intelligence is playing an increasingly important role in many fields of medicine, assisting physicians in most steps of patient management. In nephrology, artificial intelligence can already be used to improve clinical care, hemodialysis prescriptions, and follow-up of transplant recipients. However, many nephrologists are still unfamiliar with the basic principles of medical artificial intelligence. This review seeks to provide an overview of medical artificial intelligence relevant to the practicing nephrologist, in all fields of nephrology. We define the core concepts of artificial intelligence and machine learning and cover the basics of the functioning of neural networks and deep learning. We also discuss the most recent clinical applications of artificial intelligence in nephrology and medicine; as an example, we describe how artificial intelligence can predict the occurrence of progressive immunoglobulin A nephropathy. Finally, we consider the future of artificial intelligence in clinical nephrology and its impact on medical practice, and conclude with a discussion of the ethical issues that the use of artificial intelligence raises in terms of clinical decision making, physician-patient relationship, patient privacy, and data collection.",2019-12-01,14,1277,93,524
998,31861734,"Convolutional-Neural Network-Based Image Crowd Counting: Review, Categorization, Analysis, and Performance Evaluation","Traditional handcrafted crowd-counting techniques in an image are currently transformed via machine-learning and artificial-intelligence techniques into intelligent crowd-counting techniques. This paradigm shift offers many advanced features in terms of adaptive monitoring and the control of dynamic crowd gatherings. Adaptive monitoring, identification/recognition, and the management of diverse crowd gatherings can improve many crowd-management-related tasks in terms of efficiency, capacity, reliability, and safety. Despite many challenges, such as occlusion, clutter, and irregular object distribution and nonuniform object scale, convolutional neural networks are a promising technology for intelligent image crowd counting and analysis. In this article, we review, categorize, analyze (limitations and distinctive features), and provide a detailed performance evaluation of the latest convolutional-neural-network-based crowd-counting techniques. We also highlight the potential applications of convolutional-neural-network-based crowd-counting techniques. Finally, we conclude this article by presenting our key observations, providing strong foundation for future research directions while designing convolutional-neural-network-based crowd-counting techniques. Further, the article discusses new advancements toward understanding crowd counting in smart cities using the Internet of Things (IoT).",2019-12-01,2,1408,117,524
1088,31429375,Improving PET Imaging Acquisition and Analysis With Machine Learning: A Narrative Review With Focus on Alzheimer's Disease and Oncology,"Machine learning (ML) algorithms have found increasing utility in the medical imaging field and numerous applications in the analysis of digital biomarkers within positron emission tomography (PET) imaging have emerged. Interest in the use of artificial intelligence in PET imaging for the study of neurodegenerative diseases and oncology stems from the potential for such techniques to streamline decision support for physicians providing early and accurate diagnosis and allowing personalized treatment regimens. In this review, the use of ML to improve PET image acquisition and reconstruction is presented, along with an overview of its applications in the analysis of PET images for the study of Alzheimer's disease and oncology.",2019-12-01,3,734,135,524
2281,31071473,Precision diagnostics based on machine learning-derived imaging signatures,"The complexity of modern multi-parametric MRI has increasingly challenged conventional interpretations of such images. Machine learning has emerged as a powerful approach to integrating diverse and complex imaging data into signatures of diagnostic and predictive value. It has also allowed us to progress from group comparisons to imaging biomarkers that offer value on an individual basis. We review several directions of research around this topic, emphasizing the use of machine learning in personalized predictions of clinical outcome, in breaking down broad umbrella diagnostic categories into more detailed and precise subtypes, and in non-invasively estimating cancer molecular characteristics. These methods and studies contribute to the field of precision medicine, by introducing more specific diagnostic and predictive biomarkers of clinical outcome, therefore pointing to better matching of treatments to patients.",2019-12-01,5,927,74,524
648,29074332,Improving individual predictions: Machine learning approaches for detecting and attacking heterogeneity in schizophrenia (and other psychiatric diseases),"Psychiatric diseases are very heterogeneous both in clinical manifestation and etiology. With the recent rise of using machine learning techniques to attempt to diagnose and prognose these disorders, the issue of heterogeneity becomes increasingly important. With the growing interest in personalized medicine, it becomes even more important to not only classify someone as a patient with a certain disorder, its treatment needs a more precise definition of the underlying neurobiology, since different biological origins of the same disease may require (very) different treatments. We review the possible contributions that machine learning techniques could make to explore the heterogeneous nature of psychiatric disorders with a focus on schizophrenia. First we will review how heterogeneity shows up and how machine learning, or multivariate pattern recognition methods in general, can be used to discover it. Secondly, we will discuss the possible uses of these techniques to attack heterogeneity, leading to improved predictions and understanding of the neurobiological background of the disorder.",2019-12-01,8,1103,153,524
553,30973516,Artificial Intelligence and Machine Learning in Anesthesiology,"Commercial applications of artificial intelligence and machine learning have made remarkable progress recently, particularly in areas such as image recognition, natural speech processing, language translation, textual analysis, and self-learning. Progress had historically languished in these areas, such that these skills had come to seem ineffably bound to intelligence. However, these commercial advances have performed best at single-task applications in which imperfect outputs and occasional frank errors can be tolerated.The practice of anesthesiology is different. It embodies a requirement for high reliability, and a pressured cycle of interpretation, physical action, and response rather than any single cognitive act. This review covers the basics of what is meant by artificial intelligence and machine learning for the practicing anesthesiologist, describing how decision-making behaviors can emerge from simple equations. Relevant clinical questions are introduced to illustrate how machine learning might help solve them-perhaps bringing anesthesiology into an era of machine-assisted discovery.",2019-12-01,8,1111,62,524
939,31956754,"Machine Learning in Catalysis, From Proposal to Practicing","Recently, machine learning (ML) methods have gained popularity and have performed as powerfully predictive tools in various areas of academic and industrious activities. In comparison, their application in catalysis has been underdeveloped. Relying on the rapid development of different algorithms and their implementation, it is the right timing to harvest the potential of ML in catalysis across academy and industry spectra. Herein, we discuss the current applications in the field of homogeneous and heterogeneous catalysis by using various ML approaches. To the best of our knowledge, modern statistical learning techniques will be a strong tool for computational optimization and discovery. This in turn will accurately extract the underlying mechanism in the model that converts readily available data and precatalysts into their promising and useful ones.",2019-12-01,2,863,58,524
2246,31133758,Deep learning for cellular image analysis,"Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field's progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs' experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.",2019-12-01,76,1119,41,524
111,30184111,Molecular differential diagnosis of uterine leiomyomas and leiomyosarcomas,"Uterine leiomyomas (LM) and leiomyosarcomas (LMS) are considered biologically unrelated tumors due to their cytogenetic and molecular disparity. Yet, these tumors share morphological and molecular characteristics that cannot be differentiated through current clinical diagnostic tests, and thus cannot be definitively classified as benign or malignant until surgery. Newer approaches are needed for the identification of these tumors, as has been done for other tissues. The application of next generation sequencing enables the detection of new mutations that, when coupled to machine learning bioinformatic tools, advances our understanding of chromosomal instability. These approaches in the context of LM and LMS could allow the discovery of genetic variants and possible genomic markers. Additionally, the potential clinical utility of circulating cell-free tumor DNA could revolutionize the noninvasive detection and monitoring of these tumors. Here, we seek to provide a perspective on the molecular background of LM and LMS, recognizing their distinct molecular features that may lead to improved diagnosis and personalized treatments, which would have a measurable impact on women's reproductive health.",2019-12-01,2,1212,74,524
2239,31145684,Regression analysis for detecting epileptic seizure with different feature extracting strategies,"Due to the excitability of neurons in the brain, a neurological disorder is produced known as epilepsy. The brain activity of patients suffering from epilepsy is monitored through electroencephalography (EEG). The multivariate nature of features from time domain, frequency domain, complexity and wavelet entropy based, and the statistical features were extracted from healthy and epileptic subjects using the Bonn University database and seizure and non-seizure intervals using the CHB MIT database. The robust machine learning regression methods based on regression, support vector regression (SVR), regression tree (RT), ensemble regression, Gaussian process regression (GPR) were employed for detecting and predicting epileptic seizures. Performance was measured in terms of root mean square error (RMSE), squared error, mean square error (MSE) and mean absolute error (MAE). Moreover, detailed optimization was performed using a RT to predict the selected features from each feature category. A deeper analysis was conducted on features and tree regression methods where optimal RMSE and MSE results were obtained. The best optimal performance was obtained using the ensemble boosted regression tree (BRT) and exponential GPR with an RMSE of 0.47, an MSE (0.22), an R Square (RS) (0.25) and an MAE (0.30) using the Bonn University database and support vector machine (SVM) fine Gaussian with RMSE (0.63634), RS (0.03), MSE (0.40493) and MAE (0.31744); squared exponential GPR and rational quadratic GPR with an RMSE of 0.63841, an RS (0.03), an MSE (0.40757) and an MAE (0.3472) was obtained using the CHB MIT database. A further deeper analysis for the prediction of selected features was performed on an RT to compute the optimal feasible point, observed and estimated function values, function evaluation time, objective function evaluation time and overall elapsed time.",2019-12-01,0,1879,96,524
2221,31173849,Machine learning in resting-state fMRI analysis,"Machine learning techniques have gained prominence for the analysis of resting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, we present an overview of various unsupervised and supervised machine learning applications to rs-fMRI. We offer a methodical taxonomy of machine learning methods in resting-state fMRI. We identify three major divisions of unsupervised learning methods with regard to their applications to rs-fMRI, based on whether they discover principal modes of variation across space, time or population. Next, we survey the algorithms and rs-fMRI feature representations that have driven the success of supervised subject-level predictions. The goal is to provide a high-level overview of the burgeoning field of rs-fMRI from the perspective of machine learning applications.",2019-12-01,7,809,47,524
2218,31175395,Physician centred imaging interpretation is dying out - why should I be a nuclear medicine physician?,"Radiomics, machine learning, and, more generally, artificial intelligence (AI) provide unique tools to improve the performances of nuclear medicine in all aspects. They may help rationalise the operational organisation of imaging departments, optimise resource allocations, and improve image quality while decreasing radiation exposure and maintaining qualitative accuracy. There is already convincing data that show AI detection, and interpretation algorithms can perform with equal or higher diagnostic accuracy in various specific indications than experts in the field. Preliminary data strongly suggest that AI will be able to process imaging data and information well beyond what is visible to the human eye, and it will be able to integrate features to provide signatures that may further drive personalised medicine. As exciting as these prospects are, they currently remain essentially projects with a long way to go before full validation and routine clinical implementation. AI uses a language that is totally unfamiliar to nuclear medicine physicians, who have not been trained to manage the highly complex concepts that rely primarily on mathematics, computer sciences, and engineering. Nuclear medicine physicians are mostly familiar with biology, pharmacology, and physics, yet, considering the disruptive nature of AI in medicine, we need to start acquiring the knowledge that will keep us in the position of being actors and not merely witnesses of the wonders developed by other stakeholders in front of our incredulous eyes. This will allow us to remain a useful and valid interface between the image, the data, and the patients and free us to pursue other, one might say nobler tasks, such as treating, caring and communicating with our patients or conducting research and development.",2019-12-01,1,1804,101,524
2214,31190176,What can artificial intelligence teach us about the molecular mechanisms underlying disease?,"While molecular imaging with positron emission tomography or single-photon emission computed tomography already reports on tumour molecular mechanisms on a macroscopic scale, there is increasing evidence that there are multiple additional features within medical images that can further improve tumour characterization, treatment prediction and prognostication. Early reports have already revealed the power of radiomics to personalize and improve patient management and outcomes. What remains unclear is how these additional metrics relate to underlying molecular mechanisms of disease. Furthermore, the ability to deal with increasingly large amounts of data from medical images and beyond in a rapid, reproducible and transparent manner is essential for future clinical practice. Here, artificial intelligence (AI) may have an impact. AI encompasses a broad range of 'intelligent' functions performed by computers, including language processing, knowledge representation, problem solving and planning. While rule-based algorithms, e.g. computer-aided diagnosis, have been in use for medical imaging since the 1990s, the resurgent interest in AI is related to improvements in computing power and advances in machine learning (ML). In this review we consider why molecular and cellular processes are of interest and which processes have already been exposed to AI and ML methods as reported in the literature. Non-small-cell lung cancer is used as an exemplar and the focus of this review as the most common tumour type in which AI and ML approaches have been tested and to illustrate some of the concepts.",2019-12-01,3,1607,92,524
2207,31203421,"Why imaging data alone is not enough: AI-based integration of imaging, omics, and clinical data","Artificial intelligence (AI) is currently regaining enormous interest due to the success of machine learning (ML), and in particular deep learning (DL). Image analysis, and thus radiomics, strongly benefits from this research. However, effectively and efficiently integrating diverse clinical, imaging, and molecular profile data is necessary to understand complex diseases, and to achieve accurate diagnosis in order to provide the best possible treatment. In addition to the need for sufficient computing resources, suitable algorithms, models, and data infrastructure, three important aspects are often neglected: (1) the need for multiple independent, sufficiently large and, above all, high-quality data sets; (2) the need for domain knowledge and ontologies; and (3) the requirement for multiple networks that provide relevant relationships among biological entities. While one will always get results out of high-dimensional data, all three aspects are essential to provide robust training and validation of ML models, to provide explainable hypotheses and results, and to achieve the necessary trust in AI and confidence for clinical applications.",2019-12-01,8,1155,95,524
2190,31664499,Review of high-content screening applications in toxicology,"High-content screening (HCS) technology combining automated microscopy and quantitative image analysis can address biological questions in academia and the pharmaceutical industry. Various HCS experimental applications have been utilized in the research field of in vitro toxicology. In this review, we describe several HCS application approaches used for studying the mechanism of compound toxicity, highlight some challenges faced in the toxicological community, and discuss the future directions of HCS in regards to new models, new reagents, data management, and informatics. Many specialized areas of toxicology including developmental toxicity, genotoxicity, developmental neurotoxicity/neurotoxicity, hepatotoxicity, cardiotoxicity, and nephrotoxicity will be examined. In addition, several newly developed cellular assay models including induced pluripotent stem cells (iPSCs), three-dimensional (3D) cell models, and tissues-on-a-chip will be discussed. New genome-editing technologies (e.g., CRISPR/Cas9), data analyzing tools for imaging, and coupling with high-content assays will be reviewed. Finally, the applications of machine learning to image processing will be explored. These new HCS approaches offer a huge step forward in dissecting biological processes, developing drugs, and making toxicology studies easier.",2019-12-01,4,1332,59,524
2187,31668208,Artificial Intelligence in Radiation Oncology,"The integration of artificial intelligence in the radiation oncologist's workflow has multiple applications and significant potential. From the initial patient encounter, artificial intelligence may aid in pretreatment disease outcome and toxicity prediction. It may subsequently aid in treatment planning, and enhanced dose optimization. Artificial intelligence may also optimize the quality assurance process and support a higher level of safety, quality, and efficiency of care. This article describes components of the radiation consultation, planning, and treatment process and how the thoughtful integration of artificial intelligence may improve shared decision making, planning efficiency, planning quality, patient safety, and patient outcomes.",2019-12-01,3,753,45,524
2557,30529297,Predicting eukaryotic protein secretion without signals,"Predicting unconventional protein secretion is a much harder problem than predicting signal peptide-based protein secretion, both due to the small number of examples and due to the heterogeneity and the limited knowledge of the pathways involved, especially in eukaryotes. However, the idea that secreted proteins share certain properties regardless of the secretion pathway used made it possible to construct the prediction method SecretomeP in 2004. Here, we take a critical look at SecretomeP and its successors, and we also discuss whether multi-category subcellular location predictors can be used to predict unconventional protein secretion in eukaryotes. A new benchmark shows SecretomeP to perform much worse than initially estimated, casting doubt on the underlying hypothesis. On a more positive note, recent developments in machine learning may have the potential to construct new methods which can not only predict unconventional protein secretion but also point out which parts of a sequence are important for secretion.",2019-12-01,1,1033,55,524
1269,32042829,Gene-gene interaction: the curse of dimensionality,"Identified genetic variants from genome wide association studies frequently show only modest effects on the disease risk, leading to the ""missing heritability"" problem. An avenue, to account for a part of this ""missingness"" is to evaluate gene-gene interactions (epistasis) thereby elucidating their effect on complex diseases. This can potentially help with identifying gene functions, pathways, and drug targets. However, the exhaustive evaluation of all possible genetic interactions among millions of single nucleotide polymorphisms (SNPs) raises several issues, otherwise known as the ""curse of dimensionality"". The dimensionality involved in the epistatic analysis of such exponentially growing SNPs diminishes the usefulness of traditional, parametric statistical methods. With the immense popularity of multifactor dimensionality reduction (MDR), a non-parametric method, proposed in 2001, that classifies multi-dimensional genotypes into one- dimensional binary approaches, led to the emergence of a fast-growing collection of methods that were based on the MDR approach. Moreover, machine-learning (ML) methods such as random forests and neural networks (NNs), deep-learning (DL) approaches, and hybrid approaches have also been applied profusely, in the recent years, to tackle this dimensionality issue associated with whole genome gene-gene interaction studies. However, exhaustive searching in MDR based approaches or variable selection in ML methods, still pose the risk of missing out on relevant SNPs. Furthermore, interpretability issues are a major hindrance for DL methods. To minimize this loss of information, Python based tools such as PySpark can potentially take advantage of distributed computing resources in the cloud, to bring back smaller subsets of data for further local analysis. Parallel computing can be a powerful resource that stands to fight this ""curse"". PySpark supports all standard Python libraries and C extensions thus making it convenient to write codes to deliver dramatic improvements in processing speed for extraordinarily large sets of data.",2019-12-01,2,2091,50,524
1488,32581642,Applications of Machine Learning in miRNA Discovery and Target Prediction,"MicroRNA (miRNA) is a small non-coding molecule that is involved in gene regulation and RNA silencing by complementary on their targets. Experimental methods for target prediction can be time-consuming and expensive. Thus, the application of the computational approach is implicated to enlighten these complications with experimental studies. However, there is still a need for an optimized approach in miRNA biology. Therefore, machine learning (ML) would initiate a new era of research in miRNA biology towards potential diseases biomarker. In this article, we described the application of ML approaches in miRNA discovery and target prediction with functions and future prospective. The implementation of a new era of computational methodologies in this direction would initiate further advanced levels of discoveries in miRNA.",2019-12-01,1,830,73,524
2181,31679788,Elements of qualitative cognition: An information topology perspective,"Elementary quantitative and qualitative aspects of consciousness are investigated conjointly from the biology, neuroscience, physic and mathematic point of view, by the mean of a theory written with Bennequin that derives and extends information theory within algebraic topology. Information structures, that accounts for statistical dependencies within n-body interacting systems are interpreted a la Leibniz as a monadic-panpsychic framework where consciousness is information and physical, and arise from collective interactions. The electrodynamic intrinsic nature of consciousness, sustained by an analogical code, is illustrated by standard neuroscience and psychophysic results. It accounts for the diversity of the learning mechanisms, including adaptive and homeostatic processes on multiple scales, and details their expression within information theory. The axiomatization and logic of cognition are rooted in measure theory expressed within a topos intrinsic probabilistic constructive logic. Information topology provides a synthesis of the main models of consciousness (Neural Assemblies, Integrated Information, Global Neuronal Workspace, Free Energy Principle) within a formal Gestalt theory, an expression of information structures and patterns in correspondence with Galois cohomology and discrete symmetries. The methods provide new formalization of deep neural network with homologicaly imposed architecture applied to challenges in AI-machine learning.",2019-12-01,0,1473,70,524
2100,31815179,Bridging the gap between military prolonged field care monitoring and exploration spaceflight: the compensatory reserve,"The concept of prolonged field care (PFC), or medical care applied beyond doctrinal planning timelines, is the top priority capability gap across the US Army. PFC is the idea that combat medics must be prepared to provide medical care to serious casualties in the field without the support of robust medical infrastructure or resources in the event of delayed medical evacuation. With limited resources, significant distances to travel before definitive care, and an inability to evacuate in a timely fashion, medical care during exploration spaceflight constitutes the ultimate example PFC. One of the main capability gaps for PFC in both military and spaceflight settings is the need for technologies for individualized monitoring of a patient's physiological status. A monitoring capability known as the compensatory reserve measurement (CRM) meets such a requirement. CRM is a small, portable, wearable technology that uses a machine learning and feature extraction-based algorithm to assess real-time changes in hundreds of specific features of arterial waveforms. Future development and advancement of CRM still faces engineering challenges to develop ruggedized wearable sensors that can measure waveforms for determining CRM from multiple sites on the body and account for less than optimal conditions (sweat, water, dirt, blood, movement, etc.). We show here the utility of a military wearable technology, CRM, which can be translated to space exploration.",2019-12-01,1,1465,119,524
2113,31796060,Text-mining clinically relevant cancer biomarkers for curation into the CIViC database,"Background:                    Precision oncology involves analysis of individual cancer samples to understand the genes and pathways involved in the development and progression of a cancer. To improve patient care, knowledge of diagnostic, prognostic, predisposing, and drug response markers is essential. Several knowledgebases have been created by different groups to collate evidence for these associations. These include the open-access Clinical Interpretation of Variants in Cancer (CIViC) knowledgebase. These databases rely on time-consuming manual curation from skilled experts who read and interpret the relevant biomedical literature.              Methods:                    To aid in this curation and provide the greatest coverage for these databases, particularly CIViC, we propose the use of text mining approaches to extract these clinically relevant biomarkers from all available published literature. To this end, a group of cancer genomics experts annotated sentences that discussed biomarkers with their clinical associations and achieved good inter-annotator agreement. We then used a supervised learning approach to construct the CIViCmine knowledgebase.              Results:                    We extracted 121,589 relevant sentences from PubMed abstracts and PubMed Central Open Access full-text papers. CIViCmine contains over 87,412 biomarkers associated with 8035 genes, 337 drugs, and 572 cancer types, representing 25,818 abstracts and 39,795 full-text publications.              Conclusions:                    Through integration with CIVIC, we provide a prioritized list of curatable clinically relevant cancer biomarkers as well as a resource that is valuable to other knowledgebases and precision cancer analysts in general. All data is publically available and distributed with a Creative Commons Zero license. The CIViCmine knowledgebase is available at http://bionlp.bcgsc.ca/civicmine/.",2019-12-01,3,1926,86,524
2116,31791160,Clinical utility of pre-endoscopy risk scores in upper gastrointestinal bleeding,"Introduction: Acute upper-gastrointestinal bleeding (AUGIB) is a common medical emergency, with an incidence of 103-172 per 100,000 in the United Kingdom (UK) and mortality of 2% to 10%. Early and accurate prediction of the severity of an AUGIB episode may help guide management, including in or outpatient management, level of care required, and timing of endoscopy. This article aims to address the clinical utility of the various pre-endoscopic risk assessment tools used in AUGIB.Areas covered: The authors undertook a literature review of the current evidence on the pre-endoscopic risk assessment scores. Additional the authors discuss the recently published novel risk assessment scores.Expert opinion: The evidence shows that GBS is the most clinically useful risk assessment score in correctly identifying very low-risk patients suitable for outpatient management. At present, research is ongoing to assess machine learning in the assessment of patients presenting with AUGIB. More research is needed but it shows promise for the future.",2019-12-01,1,1046,80,524
2120,33404545,Engineering Plant Cytochrome P450s for Enhanced Synthesis of Natural Products: Past Achievements and Future Perspectives,"Cytochrome P450s (P450s) are the most versatile catalysts and are widely used by plants to synthesize a vast array of structurally diverse specialized metabolites that not only play essential ecological roles but also constitute a valuable resource for the development of new drugs. To accelerate the metabolic engineering of these high-value metabolites, it is imperative to identify and characterize pathway P450s, and to further improve their activities through protein engineering. In this review, we focus on P450 engineering and summarize the major strategies for enhancing the stability and activity of P450s and successful cases of P450 engineering. Studies in which the functions of P450s were altered to create de novo metabolic pathways or novel compounds are discussed as well. We also overview emerging tools, specifically DNA synthesis, machine learning, and de novo protein design, as well as the evolutionary patterns of P450s unveiled from a massive number of DNA sequences that could be integrated to accelerate the engineering of these enzymes. These approaches would greatly aid in the exploitation of plant-specialized metabolites or derivatives for various uses including medical applications.",2019-12-01,0,1215,120,524
2125,33386099,Proteochemometrics - recent developments in bioactivity and selectivity modeling,"Proteochemometrics is a machine learning based modeling approach relying on a combination of ligand and protein descriptors. With ongoing developments in machine learning and increases in public data the technique is more frequently applied in early drug discovery, typically in ligand-target binding prediction. Common applications include improvements to single target quantitative structure-activity relationship models, protein selectivity and promiscuity modeling, and large-scale deep learning approaches. The increase in predictive power using proteochemometrics is observed in multi-target bioactivity modeling, opening the door to more extensive studies covering whole protein families. On top of that, with deep learning fueling more complex and larger scale models, proteochemometrics allows faster and higher quality computational models supporting the design, make, test cycle.",2019-12-01,0,890,80,524
2126,33386098,Selecting machine-learning scoring functions for structure-based virtual screening,"Interest in docking technologies has grown parallel to the ever increasing number and diversity of 3D models for macromolecular therapeutic targets. Structure-Based Virtual Screening (SBVS) aims at leveraging these experimental structures to discover the necessary starting points for the drug discovery process. It is now established that Machine Learning (ML) can strongly enhance the predictive accuracy of scoring functions for SBVS by exploiting large datasets from targets, molecules and their associations. However, with greater choice, the question of which ML-based scoring function is the most suitable for prospective use on a given target has gained importance. Here we analyse two approaches to select an existing scoring function for the target along with a third approach consisting in generating a scoring function tailored to the target. These analyses required discussing the limitations of popular SBVS benchmarks, the alternatives to benchmark scoring functions for SBVS and how to generate them or use them using freely-available software.",2019-12-01,1,1060,82,524
2127,33386097,Practical considerations for active machine learning in drug discovery,"Active machine learning enables the automated selection of the most valuable next experiments to improve predictive modelling and hasten active retrieval in drug discovery. Although a long established theoretical concept and introduced to drug discovery approximately 15 years ago, the deployment of active learning technology in the discovery pipelines across academia and industry remains slow. With the recent re-discovered enthusiasm for artificial intelligence as well as improved flexibility of laboratory automation, active learning is expected to surge and become a key technology for molecular optimizations. This review recapitulates key findings from previous active learning studies to highlight the challenges and opportunities of applying adaptive machine learning to drug discovery. Specifically, considerations regarding implementation, infrastructural integration, and expected benefits are discussed. By focusing on these practical aspects of active learning, this review aims at providing insights for scientists planning to implement active learning workflows in their discovery pipelines.",2019-12-01,0,1109,70,524
2128,33386095,On failure modes in molecule generation and optimization,"There has been a wave of generative models for molecules triggered by advances in the field of Deep Learning. These generative models are often used to optimize chemical compounds towards particular properties or a desired biological activity. The evaluation of generative models remains challenging and suggested performance metrics or scoring functions often do not cover all relevant aspects of drug design projects. In this work, we highlight some unintended failure modes in molecular generation and optimization and how these evade detection by current performance metrics.",2019-12-01,0,579,56,524
2129,33386093,The art of atom descriptor design,"This review provides an overview of descriptions of atoms applied to the understanding of phenomena like chemical reactivity and selectivity, pKa values, Site of Metabolism prediction, or hydrogen bond strengths, but also the substitution of quantum mechanical calculations by machine learning models for energies, forces or even spectrosocopic properties and finally the fast calculation of atomic charges for force field parametrization. The descriptor space ranges from derivatives of the wavefunctions or electron density via quantum mechanics derived descriptors to classical descriptions of atoms and their embedding in a molecule. The common denominator for all approaches is the thorough understanding of the physics of the chemical problem that guided the design of the atom descriptor. Quantum mechanics (QM) and machine learning (ML) finally are converging to a new discipline, namely QM/ML.",2019-12-01,1,902,33,524
2130,31782286,Global perspective on carotid intima-media thickness and plaque: should the current measurement guidelines be revisited?,"Carotid intima-media thickness (cIMT) and carotid plaque (CP) currently act as risk predictors for CVD/Stroke risk assessment. Over 2000 articles have been published that cover either use cIMT/CP or alterations of cIMT/CP and additional image-based phenotypes to associate cIMT related markers with CVD/Stroke risk. These articles have shown variable results, which likely reflect a lack of standardization in the tools for measurement, risk stratification, and risk assessment. Guidelines for cIMT/CP measurement are influenced by major factors like the atherosclerosis disease itself, conventional risk factors, 10-year measurement tools, types of CVD/Stroke risk calculators, incomplete validation of measurement tools, and the fast pace of computer technology advancements. This review discusses the following major points: 1) the American Society of Echocardiography and Mannheim guidelines for cIMT/CP measurements; 2) forces that influence the guidelines; and 3) calculators for risk stratification and assessment under the influence of advanced intelligence methods. The review also presents the knowledge-based learning strategies such as machine and deep learning which may play a future role in CVD/stroke risk assessment. We conclude that both machine learning and non-machine learning strategies will flourish for current and 10-year CVD/Stroke risk prediction as long as they integrate image-based phenotypes with conventional risk factors.",2019-12-01,5,1454,120,524
1571,32140182,"The greater inflammatory pathway-high clinical potential by innovative predictive, preventive, and personalized medical approach","Background and limitations:                    Impaired wound healing (WH) and chronic inflammation are hallmarks of non-communicable diseases (NCDs). However, despite WH being a recognized player in NCDs, mainstream therapies focus on (un)targeted damping of the inflammatory response, leaving WH largely unaddressed, owing to three main factors. The first is the complexity of the pathway that links inflammation and wound healing; the second is the dual nature, local and systemic, of WH; and the third is the limited acknowledgement of genetic and contingent causes that disrupt physiologic progression of WH.              Proposed approach:                    Here, in the frame of Predictive, Preventive, and Personalized Medicine (PPPM), we integrate and revisit current literature to offer a novel systemic view on the cues that can impact on the fate (acute or chronic inflammation) of WH, beyond the compartmentalization of medical disciplines and with the support of advanced computational biology.              Conclusions:                    This shall open to a broader understanding of the causes for WH going awry, offering new operational criteria for patients' stratification (prediction and personalization). While this may also offer improved options for targeted prevention, we will envisage new therapeutic strategies to reboot and/or boost WH, to enable its progression across its physiological phases, the first of which is a transient acute inflammatory response versus the chronic low-grade inflammation characteristic of NCDs.",2019-12-01,11,1553,128,524
2149,31755802,An overview of deep learning algorithms and water exchange in colonoscopy in improving adenoma detection,"Introduction: Among the Gastrointestinal (GI) Endoscopy Editorial Board top 10 topics in advances in endoscopy in 2018, water exchange colonoscopy and artificial intelligence were both considered important advances. Artificial intelligence holds the potential to increase and water exchange significantly increases adenoma detection.Areas covered: The authors searched MEDLINE (1998-2019) using the following medical subject terms: water-aided, water-assisted and water exchange colonoscopy, adenoma, artificial intelligence, deep learning, computer-assisted detection, and neural networks. Additional related studies were manually searched from the reference lists of publications. Only fully published journal articles in English were reviewed. The latest date of the search was Aug10, 2019. Artificial intelligence, machine learning, and deep learning contribute to the promise of real-time computer-aided detection diagnosis. By emphasizing near-complete suction of infused water during insertion, water exchange provides salvage cleaning and decreases cleaning-related multi-tasking distractions during withdrawal, increasing adenoma detection. The review will address how artificial intelligence and water exchange can complement each other in improving adenoma detection during colonoscopy.Expert opinion: In 5 years, research on artificial intelligence will likely achieve real-time application and evaluation of factors contributing to quality colonoscopy. Better understanding and more widespread use of water exchange will be possible.",2019-12-01,1,1546,104,524
2182,31677692,The Future Directions of Research in Cardiac Anesthesiology,"This article provides an overview of knowledge gaps that need to be addressed in cardiac anesthesia, including mitigating the inflammatory effects of cardiopulmonary bypass, defining myocardial infarction after cardiac surgery, improving perioperative neurologic outcomes, and the optimal management of patients undergoing valve replacement. In addition, emerging approaches to research conduct are discussed, including the use of new analytical techniques like machine learning, pragmatic trials, and adaptive designs.",2019-12-01,0,519,59,524
1268,32047333,"Artificial intelligence, machine learning, computer-aided diagnosis, and radiomics: advances in imaging towards to precision medicine","The discipline of radiology and diagnostic imaging has evolved greatly in recent years. We have observed an exponential increase in the number of exams performed, subspecialization of medical fields, and increases in accuracy of the various imaging methods, making it a challenge for the radiologist to ""know everything about all exams and regions"". In addition, imaging exams are no longer only qualitative and diagnostic, providing now quantitative information on disease severity, as well as identifying biomarkers of prognosis and treatment response. In view of this, computer-aided diagnosis systems have been developed with the objective of complementing diagnostic imaging and helping the therapeutic decision-making process. With the advent of artificial intelligence, ""big data"", and machine learning, we are moving toward the rapid expansion of the use of these tools in daily life of physicians, making each patient unique, as well as leading radiology toward the concept of multidisciplinary approach and precision medicine. In this article, we will present the main aspects of the computational tools currently available for analysis of images and the principles of such analysis, together with the main terms and concepts involved, as well as examining the impact that the development of artificial intelligence has had on radiology and diagnostic imaging.",2019-12-01,9,1370,133,524
492,30230414,An accessible and efficient autism screening method for behavioural data and predictive analyses,"Autism spectrum disorder is associated with significant healthcare costs, and early diagnosis can substantially reduce these. Unfortunately, waiting times for an autism spectrum disorder diagnosis are lengthy due to the fact that current diagnostic procedures are time-consuming and not cost-effective. Overall, the economic impact of autism and the increase in the number of autism spectrum disorder cases across the world reveal an urgent need for the development of easily implemented and effective screening methods. This article proposes a new mobile application to overcome the problem by offering users and the health community a friendly, time-efficient and accessible mobile-based autism spectrum disorder screening tool called ASDTests. The proposed ASDTests app can be used by health professionals to assist their practice or to inform individuals whether they should pursue formal clinical diagnosis. Unlike existing autism screening apps being tested, the proposed app covers a larger audience since it contains four different tests, one each for toddlers, children, adolescents and adults as well as being available in 11 different languages. More importantly, the proposed app is a vital tool for data collection related to autism spectrum disorder for toddlers, children, adolescent and adults since initially over 1400 instances of cases and controls have been collected. Feature and predictive analyses demonstrate small groups of autistic traits improving the efficiency and accuracy of screening processes. In addition, classifiers derived using machine learning algorithms report promising results with respect to sensitivity, specificity and accuracy rates.",2019-12-01,7,1679,96,524
913,31254036,Next generation research applications for hybrid PET/MR and PET/CT imaging using deep learning,"Introduction:                    Recently there have been significant advances in the field of machine learning and artificial intelligence (AI) centered around imaging-based applications such as computer vision. In particular, the tremendous power of deep learning algorithms, primarily based on convolutional neural network strategies, is becoming increasingly apparent and has already had direct impact on the fields of radiology and nuclear medicine. While most early applications of computer vision to radiological imaging have focused on classification of images into disease categories, it is also possible to use these methods to improve image quality. Hybrid imaging approaches, such as PET/MRI and PET/CT, are ideal for applying these methods.              Methods:                    This review will give an overview of the application of AI to improve image quality for PET imaging directly and how the additional use of anatomic information from CT and MRI can lead to further benefits. For PET, these performance gains can be used to shorten imaging scan times, with improvement in patient comfort and motion artifacts, or to push towards lower radiotracer doses. It also opens the possibilities for dual tracer studies, more frequent follow-up examinations, and new imaging indications. How to assess quality and the potential effects of bias in training and testing sets will be discussed.              Conclusion:                    Harnessing the power of these new technologies to extract maximal information from hybrid PET imaging will open up new vistas for both research and clinical applications with associated benefits in patient care.",2019-12-01,11,1662,94,524
1755,31568792,Investigation of machine learning techniques on proteomics: A comprehensive survey,"Proteomics is the extensive investigation of proteins which has empowered the recognizable proof of consistently expanding quantities of protein. Proteins are necessary part of living life form, with numerous capacities. The proteome is the complete arrangement of proteins that are created or altered by a life form or framework of the organism. Proteome fluctuates with time and unambiguous prerequisites, or stresses, that a cell or organism experiences. Proteomics is an interdisciplinary area that has derived from the hereditary data of different genome ventures. Much proteomics information is gathered with the assistance of high throughput techniques, for example, mass spectrometry and microarray. It would regularly take weeks or months to analyze the information and perform examinations by hand. Therefore, scholars and scientific experts are teaming up with computer science researchers and mathematicians to make projects and pipeline to computationally examine the protein information. Utilizing bioinformatics procedures, scientists are prepared to do quicker investigation and protein information storing. The goal of this paper is to brief about the review of machine learning procedures and its application in the field of proteomics.",2019-12-01,0,1254,82,524
921,31240330,"Radiomics in nuclear medicine: robustness, reproducibility, standardization, and how to avoid data analysis traps and replication crisis","Radiomics in nuclear medicine is rapidly expanding. Reproducibility of radiomics studies in multicentre settings is an important criterion for clinical translation. We therefore performed a meta-analysis to investigate reproducibility of radiomics biomarkers in PET imaging and to obtain quantitative information regarding their sensitivity to variations in various imaging and radiomics-related factors as well as their inherent sensitivity. Additionally, we identify and describe data analysis pitfalls that affect the reproducibility and generalizability of radiomics studies. After a systematic literature search, 42 studies were included in the qualitative synthesis, and data from 21 were used for the quantitative meta-analysis. Data concerning measurement agreement and reliability were collected for 21 of 38 different factors associated with image acquisition, reconstruction, segmentation and radiomics-specific processing steps. Variations in voxel size, segmentation and several reconstruction parameters strongly affected reproducibility, but the level of evidence remained weak. Based on the meta-analysis, we also assessed inherent sensitivity to variations of 110 PET image biomarkers. SUVmean and SUVmax were found to be reliable, whereas image biomarkers based on the neighbourhood grey tone difference matrix and most biomarkers based on the size zone matrix were found to be highly sensitive to variations, and should be used with care in multicentre settings. Lastly, we identify 11 data analysis pitfalls. These pitfalls concern model validation and information leakage during model development, but also relate to reporting and the software used for data analysis. Avoiding such pitfalls is essential for minimizing bias in the results and to enable reproduction and validation of radiomics studies.",2019-12-01,32,1823,136,524
2483,30670469,Recording and Decoding of Vagal Neural Signals Related to Changes in Physiological Parameters and Biomarkers of Disease,"Our bodies have built-in neural reflexes that continuously monitor organ function and maintain physiological homeostasis. Whereas the field of bioelectronic medicine has mainly focused on the stimulation of neural circuits to treat various conditions, recent studies have started to investigate the possibility of leveraging the sensory arm of these reflexes to diagnose disease states. To accomplish this, neural signals emanating from the body's built-in biosensors and propagating through peripheral nerves must be recorded and decoded to identify the presence or levels of relevant biomarkers of disease. The process of acquiring these signals poses several technical challenges related to the neural interfaces, surgical techniques, and data-processing framework needed to record and analyze them. However, these challenges can be addressed with a rigorous experimental approach and new advances in implantable electrodes, signal processing, and machine learning methods. Outlined in this review are studies decoding vagus nerve activity as it related to inflammatory, metabolic, and cardiopulmonary biomarkers. Successfully decoding peripheral nerve activity related to disease states will not only enable the development of real-time diagnostic devices, but also help advancing truly closed-loop neuromodulation technologies.",2019-12-01,6,1332,119,524
843,31362571,Probing the 3D architecture of the plant nucleus with microscopy approaches: challenges and solutions,"The eukaryotic cell nucleus is a central organelle whose architecture determines genome function at multiple levels. Deciphering nuclear organizing principles influencing cellular responses and identity is a timely challenge. Despite many similarities between plant and animal nuclei, plant nuclei present intriguing specificities. Complementary to molecular and biochemical approaches, 3D microscopy is indispensable for resolving nuclear architecture. However, novel solutions are required for capturing cell-specific, sub-nuclear and dynamic processes. We provide a pointer for utilising high-to-super-resolution microscopy and image processing to probe plant nuclear architecture in 3D at the best possible spatial and temporal resolution and at quantitative and cell-specific levels. High-end imaging and image-processing solutions allow the community now to transcend conventional practices and benefit from continuously improving approaches. These promise to deliver a comprehensive, 3D view of plant nuclear architecture and to capture spatial dynamics of the nuclear compartment in relation to cellular states and responses. Abbreviations: 3D and 4D: Three and Four dimensional; AI: Artificial Intelligence; ant: antipodal nuclei (ant); CLSM: Confocal Laser Scanning Microscopy; CTs: Chromosome Territories; DL: Deep Learning; DLIm: Dynamic Live Imaging; ecn: egg nucleus; FACS: Fluorescence-Activated Cell Sorting; FISH: Fluorescent In Situ Hybridization; FP: Fluorescent Proteins (GFP, RFP, CFP, YFP, mCherry); FRAP: Fluorescence Recovery After Photobleaching; GPU: Graphics Processing Unit; KEEs: KNOT Engaged Elements; INTACT: Isolation of Nuclei TAgged in specific Cell Types; LADs: Lamin-Associated Domains; ML: Machine Learning; NA: Numerical Aperture; NADs: Nucleolar Associated Domains; PALM: Photo-Activated Localization Microscopy; Pixel: Picture element; pn: polar nuclei; PSF: Point Spread Function; RHF: Relative Heterochromatin Fraction; SIM: Structured Illumination Microscopy; SLIm: Static Live Imaging; SMC: Spore Mother Cell; SNR: Signal to Noise Ratio; SRM: Super-Resolution Microscopy; STED: STimulated Emission Depletion; STORM: STochastic Optical Reconstruction Microscopy; syn: synergid nuclei; TADs: Topologically Associating Domains; Voxel: Volumetric pixel.",2019-12-01,5,2293,101,524
1735,31601480,Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging,"Artificial intelligence (AI) in medical imaging is a potentially disruptive technology. An understanding of the principles and application of radiomics, artificial neural networks, machine learning, and deep learning is an essential foundation to weave design solutions that accommodate ethical and regulatory requirements, and to craft AI-based algorithms that enhance outcomes, quality, and efficiency. Moreover, a more holistic perspective of applications, opportunities, and challenges from a programmatic perspective contributes to ethical and sustainable implementation of AI solutions.",2019-12-01,4,592,74,524
1726,31608730,Molecular and biomarker-based diagnostics in early sepsis: current challenges and future perspectives,"Introduction: Sepsis, defined as a life-threatening organ dysfunction resulting from dysregulated host response to infection, is still a major challenge for healthcare systems. Early diagnosis is highly needed, yet challenging, due to the non-specificity of clinical symptoms. Rapid and targeted application of therapy strategies is crucial for patient's outcome.Areas covered: Faster and better diagnostics with high accuracy is promised by novel host response biomarkers and a wide variety of direct pathogen identification technologies, which have emerged over the last years. This review will cover both - host response-guided diagnostics and methods for direct pathogen detection. Some of the markers and technologies are already market-ready, others are more likely aspirants. We will discuss them in terms of their performance and benefit for use in clinical diagnostics.Expert opinion: Latest technological advances enable the development of promising diagnostic tests, detecting the host response as well as identifying pathogens without the need of cultivation. However, the syndrome's heterogeneity makes it difficult to develop a universal test suitable for routine use. Moreover, the robustness of the biomarkers and technologies still has to be verified. Combining these technologies and clinical routine parameters with bioinformatic methods (e.g., machine-learning algorithms) may revolutionize sepsis diagnostics.",2019-12-01,1,1430,101,524
1725,31610899,A survey of adaptive resonance theory neural network models for engineering applications,"This survey samples from the ever-growing family of adaptive resonance theory (ART) neural network models used to perform the three primary machine learning modalities, namely, unsupervised, supervised and reinforcement learning. It comprises a representative list from classic to contemporary ART models, thereby painting a general picture of the architectures developed by researchers over the past 30 years. The learning dynamics of these ART models are briefly described, and their distinctive characteristics such as code representation, long-term memory, and corresponding geometric interpretation are discussed. Useful engineering properties of ART (speed, configurability, explainability, parallelization and hardware implementation) are examined along with current challenges. Finally, a compilation of online software libraries is provided. It is expected that this overview will be helpful to new and seasoned ART researchers.",2019-12-01,1,937,88,524
927,31229667,Role of deep learning in infant brain MRI analysis,"Deep learning algorithms and in particular convolutional networks have shown tremendous success in medical image analysis applications, though relatively few methods have been applied to infant MRI data due numerous inherent challenges such as inhomogenous tissue appearance across the image, considerable image intensity variability across the first year of life, and a low signal to noise setting. This paper presents methods addressing these challenges in two selected applications, specifically infant brain tissue segmentation at the isointense stage and presymptomatic disease prediction in neurodevelopmental disorders. Corresponding methods are reviewed and compared, and open issues are identified, namely low data size restrictions, class imbalance problems, and lack of interpretation of the resulting deep learning solutions. We discuss how existing solutions can be adapted to approach these issues as well as how generative models seem to be a particularly strong contender to address them.",2019-12-01,3,1004,50,524
1724,31611150,Applications of Artificial Intelligence in Cardiology. The Future is Already Here,"There is currently no other hot topic like the ability of current technology to develop capabilities similar to those of human beings, even in medicine. This ability to simulate the processes of human intelligence with computer systems is known as artificial intelligence (AI). This article aims to clarify the various terms that still sound foreign to us, such as AI, machine learning (ML), deep learning (DL), and big data. It also provides an in-depth description of the concept of AI and its types; the learning techniques and technology used by ML; cardiac imaging analysis with DL; and the contribution of this technological revolution to classical statistics, as well as its current limitations, legal aspects, and initial applications in cardiology. To do this, we conducted a detailed PubMed search on the evolution of original contributions on AI to the various areas of application in cardiology in the last 5 years and identified 673 research articles. We provide 19 detailed examples from distinct areas of cardiology that, by using AI, have shown diagnostic and therapeutic improvements, and which will aid understanding of ML and DL methodology.",2019-12-01,5,1160,81,524
999,31861476,Development Trends and Perspectives of Future Sensors and MEMS/NEMS,"With the fast development of the fifth-generation cellular network technology (5G), the future sensors and microelectromechanical systems (MEMS)/nanoelectromechanical systems (NEMS) are presenting a more and more critical role to provide information in our daily life. This review paper introduces the development trends and perspectives of the future sensors and MEMS/NEMS. Starting from the issues of the MEMS fabrication, we introduced typical MEMS sensors for their applications in the Internet of Things (IoTs), such as MEMS physical sensor, MEMS acoustic sensor, and MEMS gas sensor. Toward the trends in intelligence and less power consumption, MEMS components including MEMS/NEMS switch, piezoelectric micromachined ultrasonic transducer (PMUT), and MEMS energy harvesting were investigated to assist the future sensors, such as event-based or almost zero-power. Furthermore, MEMS rigid substrate toward NEMS flexible-based for flexibility and interface was discussed as another important development trend for next-generation wearable or multi-functional sensors. Around the issues about the big data and human-machine realization for human beings' manipulation, artificial intelligence (AI) and virtual reality (VR) technologies were finally realized using sensor nodes and its wave identification as future trends for various scenarios.",2019-12-01,3,1347,67,524
1108,31401617,Intelligent Imaging: Anatomy of Machine Learning and Deep Learning,"The emergence of artificial intelligence (AI) in nuclear medicine and radiology has been accompanied by AI commentators and experts predicting that AI would make radiologists, in particular, extinct. More realistic perspectives suggest significant changes will occur in medical practice. There is no escaping the disruptive technology associated with AI, neural networks, and deep learning, the most significant perhaps since the early days of Roentgen, Becquerel, and Curie. AI is an omen, but it need not be foreshadowing a negative event; rather, it is heralding great opportunity. The key to sustainability lies not in resisting AI but in having a deep understanding and exploiting the capabilities of AI in nuclear medicine while mastering those capabilities unique to the human resources.",2019-12-01,2,794,66,524
1784,31527580,Machine learning for radiomics-based multimodality and multiparametric modeling,"Due to the recent developments of both hardware and software technologies, multimodality medical imaging techniques have been increasingly applied in clinical practice and research studies. Previously, the application of multimodality imaging in oncology has been mainly related to combining anatomical and functional imaging to improve diagnostic specificity and/or target definition, such as positron emission tomography/computed tomography (PET/CT) and single-photon emission CT (SPECT)/CT. More recently, the fusion of various images, such as multiparametric magnetic resonance imaging (MRI) sequences, different PET tracer images, PET/MRI, has become more prevalent, which has enabled more comprehensive characterization of the tumor phenotype. In order to take advantage of these valuable multimodal data for clinical decision making using radiomics, we present two ways to implement the multimodal image analysis, namely radiomic (handcrafted feature) based and deep learning (machine learned feature) based methods. Applying advanced machine (deep) learning algorithms across multimodality images have shown better results compared with single modality modeling for prognostic and/or prediction of clinical outcomes. This holds great potentials for providing more personalized treatment for patients and achieve better outcomes.",2019-12-01,8,1336,79,524
1758,31562756,Accuracy of Machine Learning Algorithms for the Diagnosis of Autism Spectrum Disorder: Systematic Review and Meta-Analysis of Brain Magnetic Resonance Imaging Studies,"Background:                    In the recent years, machine learning algorithms have been more widely and increasingly applied in biomedical fields. In particular, their application has been drawing more attention in the field of psychiatry, for instance, as diagnostic tests/tools for autism spectrum disorder (ASD). However, given their complexity and potential clinical implications, there is an ongoing need for further research on their accuracy.              Objective:                    This study aimed to perform a systematic review and meta-analysis to summarize the available evidence for the accuracy of machine learning algorithms in diagnosing ASD.              Methods:                    The following databases were searched on November 28, 2018: MEDLINE, EMBASE, CINAHL Complete (with Open Dissertations), PsycINFO, and Institute of Electrical and Electronics Engineers Xplore Digital Library. Studies that used a machine learning algorithm partially or fully for distinguishing individuals with ASD from control subjects and provided accuracy measures were included in our analysis. The bivariate random effects model was applied to the pooled data in a meta-analysis. A subgroup analysis was used to investigate and resolve the source of heterogeneity between studies. True-positive, false-positive, false-negative, and true-negative values from individual studies were used to calculate the pooled sensitivity and specificity values, draw Summary Receiver Operating Characteristics curves, and obtain the area under the curve (AUC) and partial AUC (pAUC).              Results:                    A total of 43 studies were included for the final analysis, of which a meta-analysis was performed on 40 studies (53 samples with 12,128 participants). A structural magnetic resonance imaging (sMRI) subgroup meta-analysis (12 samples with 1776 participants) showed a sensitivity of 0.83 (95% CI 0.76-0.89), a specificity of 0.84 (95% CI 0.74-0.91), and AUC/pAUC of 0.90/0.83. A functional magnetic resonance imaging/deep neural network subgroup meta-analysis (5 samples with 1345 participants) showed a sensitivity of 0.69 (95% CI 0.62-0.75), specificity of 0.66 (95% CI 0.61-0.70), and AUC/pAUC of 0.71/0.67.              Conclusions:                    The accuracy of machine learning algorithms for diagnosis of ASD was considered acceptable by few accuracy measures only in cases of sMRI use; however, given the many limitations indicated in our study, further well-designed studies are warranted to extend the potential use of machine learning algorithms to clinical settings.              Trial registration:                    PROSPERO CRD42018117779; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=117779.",2019-12-01,3,2750,166,524
1707,31629922,Transforming healthcare with big data analytics and artificial intelligence: A systematic mapping study,"The domain of healthcare has always been flooded with a huge amount of complex data, coming in at a very fast-pace. A vast amount of data is generated in different sectors of healthcare industry: data from hospitals and healthcare providers, medical insurance, medical equipment, life sciences and medical research. With the advancement in technology, there is a huge potential for utilization of this data for transforming healthcare. The application of analytics, machine learning and artificial intelligence over big data enables identification of patterns and correlations and hence provides actionable insights for improving the delivery of healthcare. There have been many contributions to the literature in this topic, but we lack a comprehensive view of the current state of research and application. This paper focuses on assessing the available literature in order to provide the researchers with evidence that enable fostering further development in this area. A systematic mapping study was conducted to identify and analyze research on big data analytics and artificial intelligence in healthcare, in which 2421 articles between 2013 and February 2019 were evaluated. The results of this study will help understand the needs in application of these technologies in healthcare by identifying the areas that require additional research. It will hence provide the researchers and industry experts with a base for future work.",2019-12-01,6,1435,103,524
1104,31411491,Effects of Distance Measure Choice on K-Nearest Neighbor Classifier Performance: A Review,"The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested examples and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures available? This review attempts to answer this question through evaluating the performance (measured by accuracy, precision, and recall) of the KNN using a large number of distance measures, tested on a number of real-world data sets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances. We found that a recently proposed nonconvex distance performed the best when applied on most data sets comparing with the other tested distances. In addition, the performance of the KNN with this top performing distance degraded only 20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing with other distances.",2019-12-01,3,1492,89,524
1701,31634699,Clinical applications of artificial intelligence in sepsis: A narrative review,"Many studies have been published on a variety of clinical applications of artificial intelligence (AI) for sepsis, while there is no overview of the literature. The aim of this review is to give an overview of the literature and thereby identify knowledge gaps and prioritize areas with high priority for further research. A literature search was conducted in PubMed from inception to February 2019. Search terms related to AI were combined with terms regarding sepsis. Articles were included when they reported an area under the receiver operator characteristics curve (AUROC) as outcome measure. Fifteen articles on diagnosis of sepsis with AI models were included. The best performing model reached an AUROC of 0.97. There were also seven articles on prognosis, predicting mortality over time with an AUROC of up to 0.895. Finally, there were three articles on assistance of treatment of sepsis, where the use of AI was associated with the lowest mortality rates. Of the articles, twenty-two were judged to be at high risk of bias or had major concerns regarding applicability. This was mostly because predictor variables in these models, such as blood pressure, were also part of the definition of sepsis, which led to overestimation of the performance. We conclude that AI models have great potential for improving early identification of patients who may benefit from administration of antibiotics. Current AI prediction models to diagnose sepsis are at major risks of bias when the diagnosis criteria are part of the predictor variables in the model. Furthermore, generalizability of these models is poor due to overfitting and a lack of standardized protocols for the construction and validation of the models. Until these problems have been resolved, a large gap remains between the creation of an AI algorithm and its implementation in clinical practice.",2019-12-01,8,1864,78,524
1099,31416598,Cardiac arrhythmia detection using deep learning: A review,"Due to its simplicity and low cost, analyzing an electrocardiogram (ECG) is the most common technique for detecting cardiac arrhythmia. The massive amount of ECG data collected every day, in home and hospital, may preclude data review by human operators/technicians. Therefore, several methods are proposed for either fully automatic arrhythmia detection or event selection for further verification by human experts. Traditional machine learning approaches have made significant progress in the past years. However, those methods rely on hand-crafted feature extraction, which requires in-depth domain knowledge and preprocessing of the signal (e.g., beat detection). This, plus the high variability in wave morphology among patients and the presence of noise, make it challenging for computerized interpretation to achieve high accuracy. Recent advances in deep learning make it possible to perform automatic high-level feature extraction and classification. Therefore, deep learning approaches have gained interest in arrhythmia detection. In this work, we reviewed the recent advancement of deep learning methods for automatic arrhythmia detection. We summarized existing literature from five aspects: utilized dataset, application, type of input data, model architecture, and performance evaluation. We also reported limitations of reviewed papers and potential future opportunities.",2019-12-01,3,1387,58,524
871,31319675,Unsupervised Learning Techniques for the Investigation of Chronic Rhinosinusitis,"Objectives:                    This article reviews the principles of unsupervised learning, a novel technique which has increasingly been reported as a tool for the investigation of chronic rhinosinusitis (CRS). It represents a paradigm shift from the traditional approach to investigating CRS based upon the clinically recognized phenotypes of ""with polyps"" and ""without polyps"" and instead relies upon the application of complex mathematical models to derive subgroups which can then be further examined. This review article reports on the principles which underlie this investigative technique and some of the published examples in CRS.              Methods:                    This review summarizes the different types of unsupervised learning techniques which have been described and briefly expounds upon their useful applications. A literature review of studies which have unsupervised learning is then presented to provide a practical guide to its uses and some of the new directions of investigations suggested by their findings.              Results:                    The commonest unsupervised learning technique applied to rhinology research is cluster analysis, which can be further subdivided into hierarchical and non-hierarchical approaches. The mathematical principles which underpin these approaches are explained within this article. Studies which have used these techniques can be broadly divided into those which have used clinical data only and that which includes biomarkers. Studies which include biomarkers adhere closely to the established canon of CRS disease phenotypes, while those that use clinical data may diverge from the typical ""polyp versus non-polyp"" phenotypes and reflect subgroups of patients who share common symptom modifiers.              Summary:                    Artificial intelligence is increasingly influential in health care research and machine learning techniques have been reported in the investigation of CRS, promising several interesting new avenues for research. However, when critically appraising studies which use this technique, the reader needs to be au fait with the limitations and appropriate uses of its application.",2019-12-01,0,2188,80,524
892,31280350,"Artificial intelligence, machine (deep) learning and radio(geno)mics: definitions and nuclear medicine imaging applications","Techniques from the field of artificial intelligence, and more specifically machine (deep) learning methods, have been core components of most recent developments in the field of medical imaging. They are already being exploited or are being considered to tackle most tasks, including image reconstruction, processing (denoising, segmentation), analysis and predictive modelling. In this review we introduce and define these key concepts and discuss how the techniques from this field can be applied to nuclear medicine imaging applications with a particular focus on radio(geno)mics.",2019-12-01,8,584,123,524
2165,31727507,Discovering the Computational Relevance of Brain Network Organization,"Understanding neurocognitive computations will require not just localizing cognitive information distributed throughout the brain but also determining how that information got there. We review recent advances in linking empirical and simulated brain network organization with cognitive information processing. Building on these advances, we offer a new framework for understanding the role of connectivity in cognition: network coding (encoding/decoding) models. These models utilize connectivity to specify the transfer of information via neural activity flow processes, successfully predicting the formation of cognitive representations in empirical neural data. The success of these models supports the possibility that localized neural functions mechanistically emerge (are computed) from distributed activity flow processes that are specified primarily by connectivity patterns.",2020-01-01,4,883,69,493
926,31229952,EULAR points to consider for the use of big data in rheumatic and musculoskeletal diseases,"Background:                    Tremendous opportunities for health research have been unlocked by the recent expansion of big data and artificial intelligence. However, this is an emergent area where recommendations for optimal use and implementation are needed. The objective of these European League Against Rheumatism (EULAR) points to consider is to guide the collection, analysis and use of big data in rheumatic and musculoskeletal disorders (RMDs).              Methods:                    A multidisciplinary task force of 14 international experts was assembled with expertise from a range of disciplines including computer science and artificial intelligence. Based on a literature review of the current status of big data in RMDs and in other fields of medicine, points to consider were formulated. Levels of evidence and strengths of recommendations were allocated and mean levels of agreement of the task force members were calculated.              Results:                    Three overarching principles and 10 points to consider were formulated. The overarching principles address ethical and general principles for dealing with big data in RMDs. The points to consider cover aspects of data sources and data collection, privacy by design, data platforms, data sharing and data analyses, in particular through artificial intelligence and machine learning. Furthermore, the points to consider state that big data is a moving field in need of adequate reporting of methods and benchmarking, careful data interpretation and implementation in clinical practice.              Conclusion:                    These EULAR points to consider discuss essential issues and provide a framework for the use of big data in RMDs.",2020-01-01,9,1729,90,493
996,31867668,Machine learning and its applications in plant molecular studies,"The advent of high-throughput genomic technologies has resulted in the accumulation of massive amounts of genomic information. However, biologists are challenged with how to effectively analyze these data. Machine learning can provide tools for better and more efficient data analysis. Unfortunately, because many plant biologists are unfamiliar with machine learning, its application in plant molecular studies has been restricted to a few species and a limited set of algorithms. Thus, in this study, we provide the basic steps for developing machine learning frameworks and present a comprehensive overview of machine learning algorithms and various evaluation metrics. Furthermore, we introduce sources of important curated plant genomic data and R packages to enable plant biologists to easily and quickly apply appropriate machine learning algorithms in their research. Finally, we discuss current applications of machine learning algorithms for identifying various genes related to resistance to biotic and abiotic stress. Broad application of machine learning and the accumulation of plant sequencing data will advance plant molecular studies.",2020-01-01,3,1151,64,493
1935,33237824,"Multimodality cardiac imaging in the 21st century: evolution, advances and future opportunities for innovation","Cardiovascular imaging has significantly evolved since the turn of the century. Progress in the last two decades has been marked by advances in every modality used to image the heart, including echocardiography, cardiac magnetic resonance, cardiac CT and nuclear cardiology. There has also been a dramatic increase in hybrid and fusion modalities that leverage the unique capabilities of two imaging techniques simultaneously, as well as the incorporation of artificial intelligence and machine learning into the clinical workflow. These advances in non-invasive cardiac imaging have guided patient management and improved clinical outcomes. The technological developments of the past 20 years have also given rise to new imaging subspecialities and increased the demand for dedicated cardiac imagers who are cross-trained in multiple modalities. This state-of-the-art review summarizes the evolution of multimodality cardiac imaging in the 21st century and highlights opportunities for future innovation.",2020-01-01,0,1005,110,493
929,31968694,Synergistic Approach of Ultrafast Spectroscopy and Molecular Simulations in the Characterization of Intramolecular Charge Transfer in Push-Pull Molecules,"The comprehensive characterization of Intramolecular Charge Transfer (ICT) stemming in push-pull molecules with a delocalized -system of electrons is noteworthy for a bespoke design of organic materials, spanning widespread applications from photovoltaics to nanomedicine imaging devices. Photo-induced ICT is characterized by structural reorganizations, which allows the molecule to adapt to the new electronic density distribution. Herein, we discuss recent photophysical advances combined with recent progresses in the computational chemistry of photoactive molecular ensembles. We focus the discussion on femtosecond Transient Absorption Spectroscopy (TAS) enabling us to follow the transition from a Locally Excited (LE) state to the ICT and to understand how the environment polarity influences radiative and non-radiative decay mechanisms. In many cases, the charge transfer transition is accompanied by structural rearrangements, such as the twisting or molecule planarization. The possibility of an accurate prediction of the charge-transfer occurring in complex molecules and molecular materials represents an enormous advantage in guiding new molecular and materials design. We briefly report on recent advances in ultrafast multidimensional spectroscopy, in particular, Two-Dimensional Electronic Spectroscopy (2DES), in unraveling the ICT nature of push-pull molecular systems. A theoretical description at the atomistic level of photo-induced molecular transitions can predict with reasonable accuracy the properties of photoactive molecules. In this framework, the review includes a discussion on the advances from simulation and modeling, which have provided, over the years, significant information on photoexcitation, emission, charge-transport, and decay pathways. Density Functional Theory (DFT) coupled with the Time-Dependent (TD) framework can describe electronic properties and dynamics for a limited system size. More recently, Machine Learning (ML) or deep learning approaches, as well as free-energy simulations containing excited state potentials, can speed up the calculations with transferable accuracy to more complex molecules with extended system size. A perspective on combining ultrafast spectroscopy with molecular simulations is foreseen for optimizing the design of photoactive compounds with tunable properties.",2020-01-01,1,2351,153,493
936,31960282,Artificial Intelligence and Polyp Detection,"Purpose of review:                    This review highlights the history, recent advances, and ongoing challenges of artificial intelligence (AI) technology in colonic polyp detection.              Recent findings:                    Hand-crafted AI algorithms have recently given way to convolutional neural networks with the ability to detect polyps in real-time. The first randomized controlled trial comparing an AI system to standard colonoscopy found a 9% increase in adenoma detection rate, but the improvement was restricted to polyps smaller than 10 mm and the results need validation. As this field rapidly evolves, important issues to consider include standardization of outcomes, dataset availability, real-world applications, and regulatory approval. AI has shown great potential for improving colonic polyp detection while requiring minimal training for endoscopists. The question of when AI will enter endoscopic practice depends on whether the technology can be integrated into existing hardware and an assessment of its added value for patient care.",2020-01-01,3,1066,43,493
931,31963480,Can Artificial Intelligence Improve the Management of Pneumonia,"The use of artificial intelligence (AI) to support clinical medical decisions is a rather promising concept. There are two important factors that have driven these advances: the availability of data from electronic health records (EHR) and progress made in computational performance. These two concepts are interrelated with respect to complex mathematical functions such as machine learning (ML) or neural networks (NN). Indeed, some published articles have already demonstrated the potential of these approaches in medicine. When considering the diagnosis and management of pneumonia, the use of AI and chest X-ray (CXR) images primarily have been indicative of early diagnosis, prompt antimicrobial therapy, and ultimately, better prognosis. Coupled with this is the growing research involving empirical therapy and mortality prediction, too. Maximizing the power of NN, the majority of studies have reported high accuracy rates in their predictions. As AI can handle large amounts of data and execute mathematical functions such as machine learning and neural networks, AI can be revolutionary in supporting the clinical decision-making processes. In this review, we describe and discuss the most relevant studies of AI in pneumonia.",2020-01-01,2,1237,63,493
1907,33285855,High-Dimensional Brain in a High-Dimensional World: Blessing of Dimensionality,"High-dimensional data and high-dimensional representations of reality are inherent features of modern Artificial Intelligence systems and applications of machine learning. The well-known phenomenon of the ""curse of dimensionality"" states: many problems become exponentially difficult in high dimensions. Recently, the other side of the coin, the ""blessing of dimensionality"", has attracted much attention. It turns out that generic high-dimensional datasets exhibit fairly simple geometric properties. Thus, there is a fundamental tradeoff between complexity and simplicity in high dimensional spaces. Here we present a brief explanatory review of recent ideas, results and hypotheses about the blessing of dimensionality and related simplifying effects relevant to machine learning and neuroscience.",2020-01-01,1,800,78,493
2121,31786740,Evolving the pulmonary nodules diagnosis from classical approaches to deep learning-aided decision support: three decades' development course and future prospect,"Purpose:                    Lung cancer is the commonest cause of cancer deaths worldwide, and its mortality can be reduced significantly by performing early diagnosis and screening. Since the 1960s, driven by the pressing needs to accurately and effectively interpret the massive volume of chest images generated daily, computer-assisted diagnosis of pulmonary nodule has opened up new opportunities to relax the limitation from physicians' subjectivity, experiences and fatigue. And the fair access to the reliable and affordable computer-assisted diagnosis will fight the inequalities in incidence and mortality between populations. It has been witnessed that significant and remarkable advances have been achieved since the 1980s, and consistent endeavors have been exerted to deal with the grand challenges on how to accurately detect the pulmonary nodules with high sensitivity at low false-positive rate as well as on how to precisely differentiate between benign and malignant nodules. There is a lack of comprehensive examination of the techniques' development which is evolving the pulmonary nodules diagnosis from classical approaches to machine learning-assisted decision support. The main goal of this investigation is to provide a comprehensive state-of-the-art review of the computer-assisted nodules detection and benign-malignant classification techniques developed over three decades, which have evolved from the complicated ad hoc analysis pipeline of conventional approaches to the simplified seamlessly integrated deep learning techniques. This review also identifies challenges and highlights opportunities for future work in learning models, learning algorithms and enhancement schemes for bridging current state to future prospect and satisfying future demand.              Conclusion:                    It is the first literature review of the past 30 years' development in computer-assisted diagnosis of lung nodules. The challenges indentified and the research opportunities highlighted in this survey are significant for bridging current state to future prospect and satisfying future demand. The values of multifaceted driving forces and multidisciplinary researches are acknowledged that will make the computer-assisted diagnosis of pulmonary nodules enter into the main stream of clinical medicine and raise the state-of-the-art clinical applications as well as increase both welfares of physicians and patients. We firmly hold the vision that fair access to the reliable, faithful, and affordable computer-assisted diagnosis for early cancer diagnosis would fight the inequalities in incidence and mortality between populations, and save more lives.",2020-01-01,7,2682,161,493
2140,31767194,"The digital surgeon: How big data, automation, and artificial intelligence will change surgical practice","Exponential growth in computing power, data storage, and sensing technology has led to a world in which we can both capture and analyze incredible amounts of data. The evolution of machine learning has further advanced the ability of computers to develop insights from massive data sets that are beyond the capacity of human analysis. The convergence of computational power, data storage, connectivity, and Artificial Intelligence (AI) has led to health technologies that, to date, have focused on diagnostic areas such as radiology and pathology. The question remains how the digital revolution will translate in the realm of surgery. There are three main areas where the authors believe that AI could impact surgery in the near future: enhancement of training modalities, cognitive enhancement of the surgeon, and procedural automation. While the promise of Big Data, AI, and Automation is high, there have been unanticipated missteps in the use of such technologies that are worth considering as we evaluate how such technologies could/should be adopted in surgical practice. Surgeons must be prepared to adopt smarter training modalities, supervise the learning of machines that can enhance cognitive function, and ultimately oversee autonomous surgery without allowing for a decay in the surgeon's operating skills.",2020-01-01,3,1320,104,493
2122,31786504,Ethical considerations in artificial intelligence,"With artificial intelligence (AI) precipitously perched at the apex of the hype curve, the promise of transforming the disparate fields of healthcare, finance, journalism, and security and law enforcement, among others, is enormous. For healthcare - particularly radiology - AI is anticipated to facilitate improved diagnostics, workflow, and therapeutic planning and monitoring. And, while it is also causing some trepidation among radiologists regarding its uncertain impact on the demand and training of our current and future workforce, most of us welcome the potential to harness AI for transformative improvements in our ability to diagnose disease more accurately and earlier in the populations we serve.",2020-01-01,2,711,49,493
2288,31058383,A roadmap to integrate astrocytes into Systems Neuroscience,"Systems neuroscience is still mainly a neuronal field, despite the plethora of evidence supporting the fact that astrocytes modulate local neural circuits, networks, and complex behaviors. In this article, we sought to identify which types of studies are necessary to establish whether astrocytes, beyond their well-documented homeostatic and metabolic functions, perform computations implementing mathematical algorithms that sub-serve coding and higher-brain functions. First, we reviewed Systems-like studies that include astrocytes in order to identify computational operations that these cells may perform, using Ca2+ transients as their encoding language. The analysis suggests that astrocytes may carry out canonical computations in a time scale of subseconds to seconds in sensory processing, neuromodulation, brain state, memory formation, fear, and complex homeostatic reflexes. Next, we propose a list of actions to gain insight into the outstanding question of which variables are encoded by such computations. The application of statistical analyses based on machine learning, such as dimensionality reduction and decoding in the context of complex behaviors, combined with connectomics of astrocyte-neuronal circuits, is, in our view, fundamental undertakings. We also discuss technical and analytical approaches to study neuronal and astrocytic populations simultaneously, and the inclusion of astrocytes in advanced modeling of neural circuits, as well as in theories currently under exploration such as predictive coding and energy-efficient coding. Clarifying the relationship between astrocytic Ca2+ and brain coding may represent a leap forward toward novel approaches in the study of astrocytes in health and disease.",2020-01-01,8,1738,59,493
2172,31702942,Critical Issues in Dental and Medical Management of Obstructive Sleep Apnea,"This critical review focuses on obstructive sleep apnea (OSA) and its management from a dental medicine perspective. OSA is characterized by 10-s cessation of breathing (apnea) or reduction in airflow (hypopnea) 5 times per hour with a drop in oxygen and/or rise in carbon dioxide. It can be associated with sleepiness and fatigue, impaired mood and cognition, cardiometabolic complications, and risk for transportation and work accidents. Although sleep apnea is diagnosed by a sleep physician, its management is interdisciplinary. The dentist's role includes 1) screening patients for OSA risk factors (e.g., retrognathia, high arched palate, enlarged tonsils or tongue, enlarged tori, high Mallampati score, poor sleep, supine sleep position, obesity, hypertension, morning headache or orofacial pain, bruxism); 2) referring to an appropriate health professional as indicated; and 3) providing oral appliance therapy followed by regular dental and sleep medical follow-up. In addition to the device features and provider expertise, anatomic, behavioral, demographic, and neurophysiologic characteristics can influence oral appliance effectiveness in managing OSA. Therefore, OSA treatment should be tailored to each patient individually. This review highlights some of the putative action mechanisms related to oral appliance effectiveness and proposes future research directions.",2020-01-01,1,1385,75,493
839,31371027,Machine learning and glioma imaging biomarkers,"Aim:                    To review how machine learning (ML) is applied to imaging biomarkers in neuro-oncology, in particular for diagnosis, prognosis, and treatment response monitoring.              Materials and methods:                    The PubMed and MEDLINE databases were searched for articles published before September 2018 using relevant search terms. The search strategy focused on articles applying ML to high-grade glioma biomarkers for treatment response monitoring, prognosis, and prediction.              Results:                    Magnetic resonance imaging (MRI) is typically used throughout the patient pathway because routine structural imaging provides detailed anatomical and pathological information and advanced techniques provide additional physiological detail. Using carefully chosen image features, ML is frequently used to allow accurate classification in a variety of scenarios. Rather than being chosen by human selection, ML also enables image features to be identified by an algorithm. Much research is applied to determining molecular profiles, histological tumour grade, and prognosis using MRI images acquired at the time that patients first present with a brain tumour. Differentiating a treatment response from a post-treatment-related effect using imaging is clinically important and also an area of active study (described here in one of two Special Issue publications dedicated to the application of ML in glioma imaging).              Conclusion:                    Although pioneering, most of the evidence is of a low level, having been obtained retrospectively and in single centres. Studies applying ML to build neuro-oncology monitoring biomarker models have yet to show an overall advantage over those using traditional statistical methods. Development and validation of ML models applied to neuro-oncology require large, well-annotated datasets, and therefore multidisciplinary and multi-centre collaborations are necessary.",2020-01-01,9,1975,46,493
853,31348869,Artificial Intelligence in Drug Treatment,"The most common applications of artificial intelligence (AI) in drug treatment have to do with matching patients to their optimal drug or combination of drugs, predicting drug-target or drug-drug interactions, and optimizing treatment protocols. This review outlines some of the recently developed AI methods aiding the drug treatment and administration process. Selection of the best drug(s) for a patient typically requires the integration of patient data, such as genetics or proteomics, with drug data, like compound chemical descriptors, to score the therapeutic efficacy of drugs. The prediction of drug interactions often relies on similarity metrics, assuming that drugs with similar structures or targets will have comparable behavior or may interfere with each other. Optimizing the dosage schedule for administration of drugs is performed using mathematical models to interpret pharmacokinetic and pharmacodynamic data. The recently developed and powerful models for each of these tasks are addressed, explained, and analyzed here.",2020-01-01,1,1042,41,493
950,31936321,Machine-Learning-Assisted De Novo Design of Organic Molecules and Polymers: Opportunities and Challenges,"Organic molecules and polymers have a broad range of applications in biomedical, chemical, and materials science fields. Traditional design approaches for organic molecules and polymers are mainly experimentally-driven, guided by experience, intuition, and conceptual insights. Though they have been successfully applied to discover many important materials, these methods are facing significant challenges due to the tremendous demand of new materials and vast design space of organic molecules and polymers. Accelerated and inverse materials design is an ideal solution to these challenges. With advancements in high-throughput computation, artificial intelligence (especially machining learning, ML), and the growth of materials databases, ML-assisted materials design is emerging as a promising tool to flourish breakthroughs in many areas of materials science and engineering. To date, using ML-assisted approaches, the quantitative structure property/activity relation for material property prediction can be established more accurately and efficiently. In addition, materials design can be revolutionized and accelerated much faster than ever, through ML-enabled molecular generation and inverse molecular design. In this perspective, we review the recent progresses in ML-guided design of organic molecules and polymers, highlight several successful examples, and examine future opportunities in biomedical, chemical, and materials science fields. We further discuss the relevant challenges to solve in order to fully realize the potential of ML-assisted materials design for organic molecules and polymers. In particular, this study summarizes publicly available materials databases, feature representations for organic molecules, open-source tools for feature generation, methods for molecular generation, and ML models for prediction of material properties, which serve as a tutorial for researchers who have little experience with ML before and want to apply ML for various applications. Last but not least, it draws insights into the current limitations of ML-guided design of organic molecules and polymers. We anticipate that ML-assisted materials design for organic molecules and polymers will be the driving force in the near future, to meet the tremendous demand of new materials with tailored properties in different fields.",2020-01-01,7,2343,104,493
2273,31079952,Putting machine learning into motion: applications in cardiovascular imaging,"Heart and circulatory diseases cause a quarter of all deaths in the UK and cardiac imaging offers an effective tool for early diagnosis and risk-stratification to improve premature death and disability. This domain of radiology is unique in that assessing flow and motion is essential for understanding and quantifying normal physiology and disease processes. Conventional image interpretation relies on manual analysis but this often fails to capture important prognostic features in the complex disturbances of cardiovascular physiology. Machine learning (ML) in cardiovascular imaging promises to be a transformative tool and addresses an unmet need for patient-specific management, accurate prediction of future events, and the discovery of tractable molecular mechanisms of disease. This review discusses the potential of ML across every aspect of image analysis including efficient acquisition, segmentation and motion tracking, disease classification, prediction tasks and modelling of genotype-phenotype interactions; however, significant challenges remain in access to high-quality data at scale, robust validation, and clinical interpretability.",2020-01-01,2,1155,76,493
949,31938923,Of Machines and Men: Intelligent Diagnosis and the Shape of Things to Come,"Artificial Intelligence (AI), although well established in many areas of everyday life, has only recently been trialed in the diagnosis and management of common clinical conditions. This editorial review highlights progress to date and suggests further improvements in and trials of AI in the management of conditions such as hypertension.",2020-01-01,0,339,74,493
525,31021749,A Comparison of Control Strategies in Commercial and Research Knee Prostheses,"Goal:                    To provide an overview of control strategies in commercial and research microprocessor-controlled prosthetic knees (MPKs).              Methods:                    Five commercially available MPKs described in patents, and five research MPKs reported in scientific literature were compared. Their working principles, intent recognition, and walking controller were analyzed. Speed and slope adaptability of the walking controller was considered as well.              Results:                    Whereas commercial MPKs are mostly passive, i.e., do not inject energy in the system, and employ heuristic rule-based intent classifiers, research MPKs are all powered and often utilize machine learning algorithms for intention detection. Both commercial and research MPKs rely on finite state machine impedance controllers for walking. Yet while commercial MPKs require a prosthetist to adjust impedance settings, scientific research is focused on reducing the tunable parameter space and developing unified controllers, independent of subject anthropometrics, walking speed, and ground slope.              Conclusion:                    The main challenges in the field of powered, active MPKs (A-MPKs) to boost commercial viability are first to demonstrate the benefit of A-MPKs compared to passive MPKs or mechanical non-microprocessor knees using biomechanical, performance-based and patient-reported metrics. Second, to evaluate control strategies and intent recognition in an uncontrolled environment, preferably outside the laboratory setting. And third, even though research MPKs favor sophisticated algorithms, to maintain the possibility of practical and comprehensible tuning of control parameters, considering optimal control cannot be known a priori.              Significance:                    This review identifies main challenges in the development of A-MPKs, which have thus far hindered their broad availability on the market.",2020-01-01,2,1968,77,493
501,31049651,Statistical learning approaches in the genetic epidemiology of complex diseases,"In this paper, we give an overview of methodological issues related to the use of statistical learning approaches when analyzing high-dimensional genetic data. The focus is set on regression models and machine learning algorithms taking genetic variables as input and returning a classification or a prediction for the target variable of interest; for example, the present or future disease status, or the future course of a disease. After briefly explaining the basic motivation and principle of these methods, we review different procedures that can be used to evaluate the accuracy of the obtained models and discuss common flaws that may lead to over-optimistic conclusions with respect to their prediction performance and usefulness.",2020-01-01,3,738,79,493
2250,31129651,Controversies in diagnosis: contemporary debates in the diagnostic safety literature,"Since the 2015 publication of the National Academy of Medicine's (NAM) Improving Diagnosis in Health Care (Improving Diagnosis in Health Care. In: Balogh EP, Miller BT, Ball JR, editors. Improving Diagnosis in Health Care. Washington (DC): National Academies Press, 2015.), literature in diagnostic safety has grown rapidly. This update was presented at the annual international meeting of the Society to Improve Diagnosis in Medicine (SIDM). We focused our literature search on articles published between 2016 and 2018 using keywords in Pubmed and the Agency for Healthcare Research and Quality (AHRQ)'s Patient Safety Network's running bibliography of diagnostic error literature (Diagnostic Errors Patient Safety Network: Agency for Healthcare Research and Quality; Available from: https://psnet.ahrq.gov/search?topic=Diagnostic-Errors&f_topicIDs=407). Three key topics emerged from our review of recent abstracts in diagnostic safety. First, definitions of diagnostic error and related concepts are evolving since the NAM's report. Second, medical educators are grappling with new approaches to teaching clinical reasoning and diagnosis. Finally, the potential of artificial intelligence (AI) to advance diagnostic excellence is coming to fruition. Here we present contemporary debates around these three topics in a pro/con format.",2020-01-01,1,1336,84,493
874,31313504,Hybrid modeling frameworks of tumor development and treatment,"Tumors are complex multicellular heterogeneous systems comprised of components that interact with and modify one another. Tumor development depends on multiple factors: intrinsic, such as genetic mutations, altered signaling pathways, or variable receptor expression; and extrinsic, such as differences in nutrient supply, crosstalk with stromal or immune cells, or variable composition of the surrounding extracellular matrix. Tumors are also characterized by high cellular heterogeneity and dynamically changing tumor microenvironments. The complexity increases when this multiscale, multicomponent system is perturbed by anticancer treatments. Modeling such complex systems and predicting how tumors will respond to therapies require mathematical models that can handle various types of information and combine diverse theoretical methods on multiple temporal and spatial scales, that is, hybrid models. In this update, we discuss the progress that has been achieved during the last 10 years in the area of the hybrid modeling of tumors. The classical definition of hybrid models refers to the coupling of discrete descriptions of cells with continuous descriptions of microenvironmental factors. To reflect on the direction that the modeling field has taken, we propose extending the definition of hybrid models to include of coupling two or more different mathematical frameworks. Thus, in addition to discussing recent advances in discrete/continuous modeling, we also discuss how these two mathematical descriptions can be coupled with theoretical frameworks of optimal control, optimization, fluid dynamics, game theory, and machine learning. All these methods will be illustrated with applications to tumor development and various anticancer treatments. This article is characterized under: Analytical and Computational Methods > Computational Methods Translational, Genomic, and Systems Medicine > Therapeutic Methods Models of Systems Properties and Processes > Organ, Tissue, and Physiological Models.",2020-01-01,6,2013,61,493
2037,33937813,Computer-aided Assessment of Catheters and Tubes on Radiographs: How Good Is Artificial Intelligence for Assessment?,"Catheters are the second most common abnormal finding on radiographs. The position of catheters must be assessed on all radiographs because serious complications can arise if catheters are malpositioned. However, due to the large number of radiographs obtained each day, there can be substantial delays between the time a radiograph is obtained and when it is interpreted by a radiologist. Computer-aided approaches hold the potential to assist in prioritizing radiographs with potentially malpositioned catheters for interpretation and automatically insert text indicating the placement of catheters in radiology reports, thereby improving radiologists' efficiency. After 50 years of research in computer-aided diagnosis, there is still a paucity of study in this area. With the development of deep learning approaches, the problem of catheter assessment is far more solvable. This review provides an overview of current algorithms and identifies key challenges in building a reliable computer-aided diagnosis system for assessment of catheters on radiographs. This review may serve to further the development of machine learning approaches for this important use case. Supplemental material is available for this article.  RSNA, 2020.",2020-01-01,0,1237,116,493
951,31936210,Practices and Trends of Machine Learning Application in Nanotoxicology,"Machine Learning (ML) techniques have been applied in the field of nanotoxicology with very encouraging results. Adverse effects of nanoforms are affected by multiple features described by theoretical descriptors, nano-specific measured properties, and experimental conditions. ML has been proven very helpful in this field in order to gain an insight into features effecting toxicity, predicting possible adverse effects as part of proactive risk analysis, and informing safe design. At this juncture, it is important to document and categorize the work that has been carried out. This study investigates and bookmarks ML methodologies used to predict nano (eco)-toxicological outcomes in nanotoxicology during the last decade. It provides a review of the sequenced steps involved in implementing an ML model, from data pre-processing, to model implementation, model validation, and applicability domain. The review gathers and presents the step-wise information on techniques and procedures of existing models that can be used readily to assemble new nanotoxicological in silico studies and accelerates the regulation of in silico tools in nanotoxicology. ML applications in nanotoxicology comprise an active and diverse collection of ongoing efforts, although it is still in their early steps toward a scientific accord, subsequent guidelines, and regulation adoption. This study is an important bookend to a decade of ML applications to nanotoxicology and serves as a useful guide to further in silico applications.",2020-01-01,5,1519,70,493
954,31934647,Photoplethysmography based atrial fibrillation detection: a review,"Atrial fibrillation (AF) is a cardiac rhythm disorder associated with increased morbidity and mortality. It is the leading risk factor for cardioembolic stroke and its early detection is crucial in both primary and secondary stroke prevention. Continuous monitoring of cardiac rhythm is today possible thanks to consumer-grade wearable devices, enabling transformative diagnostic and patient management tools. Such monitoring is possible using low-cost easy-to-implement optical sensors that today equip the majority of wearables. These sensors record blood volume variations-a technology known as photoplethysmography (PPG)-from which the heart rate and other physiological parameters can be extracted to inform about user activity, fitness, sleep, and health. Recently, new wearable devices were introduced as being capable of AF detection, evidenced by large prospective trials in some cases. Such devices would allow for early screening of AF and initiation of therapy to prevent stroke. This review is a summary of a body of work on AF detection using PPG. A thorough account of the signal processing, machine learning, and deep learning approaches used in these studies is presented, followed by a discussion of their limitations and challenges towards clinical applications.",2020-01-01,11,1281,66,493
956,31927437,"Computational approaches in cancer multidrug resistance research: Identification of potential biomarkers, drug targets and drug-target interactions","Like physics in the 19th century, biology and molecular biology in particular, has been fertilized and enhanced like few other scientific fields, by the incorporation of mathematical methods. In the last decades, a whole new scientific field, bioinformatics, has developed with an output of over 30,000 papers a year (Pubmed search using the keyword ""bioinformatics""). Huge databases of mass throughput data have been established, with ArrayExpress alone containing more than 2.7 million assays (October 2019). Computational methods have become indispensable tools in molecular biology, particularly in one of the most challenging areas of cancer research, multidrug resistance (MDR). However, confronted with a plethora of different algorithms, approaches, and methods, the average researcher faces key questions: Which methods do exist? Which methods can be used to tackle the aims of a given study? Or, more generally, how do I use computational biology/bioinformatics to bolster my research? The current review is aimed at providing guidance to existing methods with relevance to MDR research. In particular, we provide an overview on: a) the identification of potential biomarkers using expression data; b) the prediction of treatment response by machine learning methods; c) the employment of network approaches to identify gene/protein regulatory networks and potential key players; d) the identification of drug-target interactions; e) the use of bipartite networks to identify multidrug targets; f) the identification of cellular subpopulations with the MDR phenotype; and, finally, g) the use of molecular modeling methods to guide and enhance drug discovery. This review shall serve as a guide through some of the basic concepts useful in MDR research. It shall give the reader some ideas about the possibilities in MDR research by using computational tools, and, finally, it shall provide a short overview of relevant literature.",2020-01-01,8,1941,147,493
2056,33501168,"Symbolic, Distributed, and Distributional Representations for Natural Language Processing in the Era of Deep Learning: A Survey","Natural language is inherently a discrete symbolic representation of human knowledge. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: discrete symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and discrete symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols may certainly lead to radically new deep learning networks. In this paper we make a survey that aims to renew the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how discrete symbols are represented inside neural networks.",2020-01-01,0,907,127,493
2208,31202770,Myocardial Mechanics in Patients With Normal LVEF and Diastolic Dysfunction,"Heart failure with preserved ejection fraction (HFpEF) is a complex clinical entity that is poorly understood yet present in up to 5.5% of the general population. Proven therapies for this disorder are lacking, even though it has a similar prognosis to that of heart failure with reduced ejection fraction (HFrEF). Innovative imaging techniques have provided in-depth understanding of the unique pattern of left ventricular mechanics in patients with HFpEF who progress through preclinical (Stages A to B) and clinical (Stages C to D) American College of Cardiology/American Heart Association heart failure stages. This review highlights the mechanical basis of this disorder from the cellular and myofiber level to chamber dysfunction. As each chamber of the heart is examined, specific biomarkers and echocardiographic parameters with diagnostic and prognostic values are discussed. Finally, novel phenotyping methods including machine learning are reviewed that integrate these mechanics into clinical groups to advise and treat patients.",2020-01-01,6,1041,75,493
1031,31518513,Big Data and Artificial Intelligence Modeling for Drug Discovery,"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health.",2020-01-01,9,1043,64,493
2162,31733673,Biomarkers of Infection and Sepsis,"The role of biomarkers for detection of sepsis has come a long way. Molecular biomarkers are taking front stage at present, but machine learning and other computational measures using bigdata sets are promising. Clinical research in sepsis is hampered by lack of specificity of the diagnosis; sepsis is a syndrome with no uniformly agreed definition. This lack of diagnostic precision means there is no gold standard for this diagnosis. The final conclusion is expert opinion, which is not bad but not perfect. Perhaps machine learning will displace expert opinion as the final and most accurate definition for sepsis.",2020-01-01,4,618,34,493
1001,31853838,Advancing neuro-oncology of glial tumors from big data and multidisciplinary studies,"Introduction:                    Multidisciplinary studies for glial tumors has produced an enormous amount of information including imaging, histology, and a large cohort of molecular data (i.e. genomics, epigenomics, metabolomics, proteomics, etc.). The big data resources are made possible through open access that offers great potential for new biomarker or therapeutic intervention via deep-learning and/or machine learning for integrated multi-omics analysis. An equally important effort to define the hallmarks of glial tumors will also advance precision neuro-oncology and inform patient-specific therapeutics. This review summarizes past studies regarding tumor classification, hallmarks of cancer, and hypothetical mechanisms. Leveraging on advanced big data approaches and ongoing cross-disciplinary endeavors, this review also discusses how to integrate multiple layers of big data toward the goal of precision medicine.              Results:                    In addition to basic research of cancer biology, the results from integrated multi-omics analysis will highlight biological processes and potential candidates as biomarkers or therapeutic targets. Ultimately, these collective resources built upon an armamentarium of accessible data can re-form clinical and molecular data to stratify patient-tailored therapy.              Conclusion:                    We envision that a comprehensive understanding of the link between molecular signatures, tumor locations, and patients' history will identify a molecular taxonomy of glial tumors to advance the improvements in early diagnosis, prevention, and treatment.",2020-01-01,1,1632,84,493
1734,31602005,Recent advances in the detection of base modifications using the Nanopore sequencer,"DNA and RNA modifications have important functions, including the regulation of gene expression. Existing methods based on short-read sequencing for the detection of modifications show difficulty in determining the modification patterns of single chromosomes or an entire transcript sequence. Furthermore, the kinds of modifications for which detection methods are available are very limited. The Nanopore sequencer is a single-molecule, long-read sequencer that can directly sequence RNA as well as DNA. Moreover, the Nanopore sequencer detects modifications on long DNA and RNA molecules. In this review, we mainly focus on base modification detection in the DNA and RNA of mammals using the Nanopore sequencer. We summarize current studies of modifications using the Nanopore sequencer, detection tools using statistical tests or machine learning, and applications of this technology, such as analyses of open chromatin, DNA replication, and RNA metabolism.",2020-01-01,9,960,83,493
1736,31593701,Application of Artificial Intelligence to Gastroenterology and Hepatology,"Since 2010, substantial progress has been made in artificial intelligence (AI) and its application to medicine. AI is explored in gastroenterology for endoscopic analysis of lesions, in detection of cancer, and to facilitate the analysis of inflammatory lesions or gastrointestinal bleeding during wireless capsule endoscopy. AI is also tested to assess liver fibrosis and to differentiate patients with pancreatic cancer from those with pancreatitis. AI might also be used to establish prognoses of patients or predict their response to treatments, based on multiple factors. We review the ways in which AI may help physicians make a diagnosis or establish a prognosis and discuss its limitations, knowing that further randomized controlled studies will be required before the approval of AI techniques by the health authorities.",2020-01-01,30,830,73,493
1305,31998109,Lizard Brain: Tackling Locally Low-Dimensional Yet Globally Complex Organization of Multi-Dimensional Datasets,"Machine learning deals with datasets characterized by high dimensionality. However, in many cases, the intrinsic dimensionality of the datasets is surprisingly low. For example, the dimensionality of a robot's perception space can be large and multi-modal but its variables can have more or less complex non-linear interdependencies. Thus multidimensional data point clouds can be effectively located in the vicinity of principal varieties possessing locally small dimensionality, but having a globally complicated organization which is sometimes difficult to represent with regular mathematical objects (such as manifolds). We review modern machine learning approaches for extracting low-dimensional geometries from multi-dimensional data and their applications in various scientific fields.",2020-01-01,4,792,110,493
1752,31571334,Effects of oral and oropharyngeal cancer on speech intelligibility using acoustic analysis: Systematic review,"Background:                    The development of automatic tools based on acoustic analysis allows to overcome the limitations of perceptual assessment for patients with head and neck cancer. The aim of this study is to provide a systematic review of literature describing the effects of oral and oropharyngeal cancer on speech intelligibility using acoustic analysis.              Methods:                    Two databases (PubMed and Embase) were surveyed. The selection process, according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) statement, led to a final set of 22 articles.              Results:                    Nasalance is studied mainly in oropharyngeal patients. The vowels are mostly studied using formant analysis and vowel space area, the consonants by means of spectral moments with specific parameters according to their phonetic characteristic. Machine learning methods allow classifying ""intelligible"" or ""unintelligible"" speech for T3 or T4 tumors.              Conclusions:                    The development of comprehensive models combining different acoustic measures would allow a better consideration of the functional impact of the speech disorder.",2020-01-01,1,1218,109,493
1756,31567618,"Recent and Upcoming Technological Developments in Computed Tomography: High Speed, Low Dose, Deep Learning, Multienergy","The advent of computed tomography (CT) has revolutionized radiology, and this revolution is still going on. Starting as a pure head scanner, modern CT systems are now able to perform whole-body examinations within a couple of seconds in isotropic resolution, single-rotation whole-organ perfusion, and temporal resolution to fulfill the needs of cardiac CT. Because of the increasing number of CT examinations in all age groups and overall medical-driven radiation exposure, dose reduction remains a hot topic. Although fast gantry rotation, broad detector arrays, and different dual-energy solutions were main topics in the past years, new techniques such as photon counting detectors, powerful x-ray tubes for low-kV scanning, automated image preprocessing, and machine learning algorithms have moved into focus today.The aim of this article is to give an overview of the technical specifications of up-to-date available CT systems and recent hardware and software innovations for CT systems in the near future.",2020-01-01,7,1013,119,493
1757,31562965,Big Data Defined: A Practical Review for Neurosurgeons,"Background:                    Modern science and healthcare generate vast amounts of data, and, coupled with the increasingly inexpensive and accessible computing, a tremendous opportunity exists to use these data to improve care. A better understanding of data science and its relationship to neurosurgical practice will be increasingly important as we transition into this modern ""big data"" era.              Methods:                    A review of the literature was performed for key articles referencing big data for neurosurgical care or related topics.              Results:                    In the present report, we first defined the nature and scope of data science from a technical perspective. We then discussed its relationship to the modern neurosurgical practice, highlighting key references, which might form a useful introductory reading list.              Conclusions:                    Numerous challenges exist going forward; however, organized neurosurgery has an important role in fostering and facilitating these efforts to merge data science with neurosurgical practice.",2020-01-01,0,1098,54,493
1768,31550922,Artificial intelligence in healthcare: An essential guide for health leaders,"Artificial Intelligence (AI) is evolving rapidly in healthcare, and various AI applications have been developed to solve some of the most pressing problems that health organizations currently face. It is crucial for health leaders to understand the state of AI technologies and the ways that such technologies can be used to improve the efficiency, safety, and access of health services, achieving value-based care. This article provides a guide to understand the fundamentals of AI technologies (ie, machine learning, natural language processing, and AI voice assistants) as well as their proper use in healthcare. It also provides practical recommendations to help decision-makers develop an AI strategy that can support their digital healthcare transformation.",2020-01-01,4,763,76,493
1781,31537505,Essential Elements of Natural Language Processing: What the Radiologist Should Know,"Natural language is ubiquitous in the workflow of medical imaging. Radiologists create and consume free text in their daily work, some of which can be amenable to enhancements through automatic processing. Recent advancements in deep learning and ""artificial intelligence"" have had a significant positive impact on natural language processing (NLP). This article discusses the history of how researchers have extracted data and encoded natural language information for analytical processing, starting from NLP's humble origins in hand-curated, linguistic rules. The evolution of medical NLP including vectorization, word embedding, classification, as well as its use in automated speech recognition, are also explored. Finally, the article will discuss the role of machine learning and neural networks in the context of significant, if incremental, improvements in NLP.",2020-01-01,1,869,83,493
1782,31533522,An overview and metanalysis of machine and deep learning-based CRISPR gRNA design tools,"The CRISPR-Cas9 system has become the most promising and versatile tool for genetic manipulation applications. Albeit the technology has been broadly adopted by both academic and pharmaceutic societies, the activity (on-target) and specificity (off-target) of CRISPR-Cas9 are decisive factors for any application of the technology. Several in silico gRNA activity and specificity predicting models and web tools have been developed, making it much more convenient and precise for conducting CRISPR gene editing studies. In this review, we present an overview and comparative analysis of machine and deep learning (MDL)-based algorithms, which are believed to be the most effective and reliable methods for the prediction of CRISPR gRNA on- and off-target activities. As an increasing number of sequence features and characteristics are discovered and are incorporated into the MDL models, the prediction outcome is getting closer to experimental observations. We also introduced the basic principle of CRISPR activity and specificity and summarized the challenges they faced, aiming to facilitate the CRISPR communities to develop more accurate models for applying.",2020-01-01,6,1165,87,493
1785,31526946,Half a century of computer methods and programs in biomedicine: A bibliometric analysis from 1970 to 2017,"Background and objective:                    Computer Methods and Programs in Biomedicine (CMPB) is a leading international journal that presents developments about computing methods and their application in biomedical research. The journal published its first issue in 1970. In 2020, the journal celebrates the 50th anniversary. Motivated by this event, this article presents a bibliometric analysis of the publications of the journal during this period (1970-2017).              Methods:                    The objective is to identify the leading trends occurring in the journal by analysing the most cited papers, keywords, authors, institutions and countries. For doing so, the study uses the Web of Science Core Collection database. Additionally, the work presents a graphical mapping of the bibliographic information by using the visualization of similarities (VOS) viewer software. This is done to analyze bibliographic coupling, co-citation and co-occurrence of keywords.              Results:                    CMPB is identified as a leading and core journal for biomedical researchers. The journal is strongly connected to IEEE Transactions on Biomedical Engineering and IEEE Transactions on Medical Imaging. Paper from Wang, Jacques, Zheng (published in 1995) is its most cited document. The top author in this journal is James Geoffrey Chase and the top contributing institution is Uppsala U (Sweden). Most of the papers in CMPB are from the USA followed by the UK and Italy. China and Taiwan are the only Asian countries to appear in the top 10 publishing in CMPB. A keyword co-occurrences analysis revealed strong co-occurrences for classification, picture archiving and communication system (PACS), heart rate variability, survival analysis and simulation. Keywords analysis for the last decade revealed that machine learning for a variety of healthcare problems (including image processing and analysis) dominated other research fields in CMPB.              Conclusions:                    It can be concluded that CMPB is a world-renowned publication outlet for biomedical researchers which has been growing in a number of publications since 1970. The analysis also conclude that the journal is very international with publications from all over the world although today European countries are the most productive ones.",2020-01-01,1,2339,105,493
1097,31419421,Scientific Authors in a Changing World of Scholarly Communication: What Does the Future Hold?,"Scholarly communication in science, technology, and medicine has been organized around journal-based scientific publishing for the past 350 years. Scientific publishing has unique business models and includes stakeholders with conflicting interests-publishers, funders, libraries, and scholars who create, curate, and consume the literature. Massive growth and change in scholarly communication, coinciding with digitalization, have amplified stresses inherent in traditional scientific publishing, as evidenced by overwhelmed editors and reviewers, increased retraction rates, emergence of pseudo-journals, strained library budgets, and debates about the metrics of academic recognition for scholarly achievements. Simultaneously, several open access models are gaining traction and online technologies offer opportunities to augment traditional tasks of scientific publishing, develop integrated discovery services, and establish global and equitable scholarly communication through crowdsourcing, software development, big data management, and machine learning. These rapidly evolving developments raise financial, legal, and ethical dilemmas that require solutions, while successful strategies are difficult to predict. Key challenges and trends are reviewed from the authors' perspective about how to engage the scholarly community in this multifaceted process.",2020-01-01,3,1366,93,493
1308,31993440,Challenges of Integrative Disease Modeling in Alzheimer's Disease,"Dementia-related diseases like Alzheimer's Disease (AD) have a tremendous social and economic cost. A deeper understanding of its underlying pathophysiologies may provide an opportunity for earlier detection and therapeutic intervention. Previous approaches for characterizing AD were targeted at single aspects of the disease. Yet, due to the complex nature of AD, the success of these approaches was limited. However, in recent years, advancements in integrative disease modeling, built on a wide range of AD biomarkers, have taken a global view on the disease, facilitating more comprehensive analysis and interpretation. Integrative AD models can be sorted in two primary types, namely hypothetical models and data-driven models. The latter group split into two subgroups: (i) Models that use traditional statistical methods such as linear models, (ii) Models that take advantage of more advanced artificial intelligence approaches such as machine learning. While many integrative AD models have been published over the last decade, their impact on clinical practice is limited. There exist major challenges in the course of integrative AD modeling, namely data missingness and censoring, imprecise human-involved priori knowledge, model reproducibility, dataset interoperability, dataset integration, and model interpretability. In this review, we highlight recent advancements and future possibilities of integrative modeling in the field of AD research, showcase and discuss the limitations and challenges involved, and finally, propose avenues to address several of these challenges.",2020-01-01,2,1591,65,493
1312,31986218,Holistic cancer genome profiling for every patient,"Technological advances in the ability to read the human genome have accelerated the speed of sequencing, such that today we can perform whole genome sequencing (WGS) in one day. Until recently, genomic studies have largely been limited to seeking novel scientific discoveries. The application of new insights gained through cancer WGS into the clinical domain, have been relatively limited. Looking ahead, a vast amount of data can be generated by genomic studies. Of note, excellent organisation of genomic and clinical data permits the application of machine-learning methods which can lead to the development of clinical algorithms that could assist future clinicians and genomicists in the analysis and interpretation of individual cancer genomes. Here, we describe what can be gleaned from holistic whole cancer genome profiling and argue that we must build the infrastructure and educational frameworks to support the modern clinical genomicist to prepare for a future where WGS will be the norm.",2020-01-01,2,1002,50,493
2467,30706465,A contemporary review of machine learning in otolaryngology-head and neck surgery,"One of the key challenges with big data is leveraging the complex network of information to yield useful clinical insights. The confluence of massive amounts of health data and a desire to make inferences and insights on these data has produced a substantial amount of interest in machine-learning analytic methods. There has been a drastic increase in the otolaryngology literature volume describing novel applications of machine learning within the past 5 years. In this timely contemporary review, we provide an overview of popular machine-learning techniques, and review recent machine-learning applications in otolaryngology-head and neck surgery including neurotology, head and neck oncology, laryngology, and rhinology. Investigators have realized significant success in validated models with model sensitivities and specificities approaching 100%. Challenges remain in the implementation of machine-learning algorithms. This may be in part the unfamiliarity of these techniques to clinician leaders on the front lines of patient care. Spreading awareness and confidence in machine learning will follow with further validation and proof-of-value analyses that demonstrate model performance superiority over established methods. We are poised to see a greater influx of machine-learning applications to clinical problems in otolaryngology-head and neck surgery, and it is prudent for providers to understand the potential benefits and limitations of these technologies. Laryngoscope, 130:45-51, 2020.",2020-01-01,6,1506,81,493
1313,31984250,Combination of Single-Molecule Electrical Measurements and Machine Learning for the Identification of Single Biomolecules,"The development of a next-generation DNA sequencer has provided a method for electrically measuring single molecules. Methods for electrically measuring one molecule are roughly divided into methods for measuring tunneling and ion currents. These methods enable identification of a single molecule of DNA, a RNA nucleotide, or a single protein based on current histograms. However, overlapping of current histograms of molecules with similar properties has been a major barrier to identifying single molecules with high accuracy. This barrier was broken by introducing machine learning. Combining single-molecule electrical measurement and machine learning enables high-precision identification of single molecules. Highly accurate discrimination has been demonstrated for DNA nucleotides, RNA nucleotides, amino acids, sugars, viruses, and bacteria. This combination enables quantitative evaluation of molecular recognition ability. Furthermore, a device structure suitable for high-precision identification has been designed. Combining single-molecule electrical measurement with machine learning enables digital analytical chemistry that can count certain types of molecules. Digital analytical chemistry enables comprehensive analysis of chemical reactions. This new analytical method will lead to the discovery of unknown or missed valuable molecules.",2020-01-01,1,1356,121,493
1304,31998708,Engineering Tissue Fabrication With Machine Intelligence: Generating a Blueprint for Regeneration,"Regenerating lost or damaged tissue is the primary goal of Tissue Engineering. 3D bioprinting technologies have been widely applied in many research areas of tissue regeneration and disease modeling with unprecedented spatial resolution and tissue-like complexity. However, the extraction of tissue architecture and the generation of high-resolution blueprints are challenging tasks for tissue regeneration. Traditionally, such spatial information is obtained from a collection of microscopic images and then combined together to visualize regions of interest. To fabricate such engineered tissues, rendered microscopic images are transformed to code to inform a 3D bioprinting process. If this process is augmented with data-driven approaches and streamlined with machine intelligence, identification of an optimal blueprint can become an achievable task for functional tissue regeneration. In this review, our perspective is guided by an emerging paradigm to generate a blueprint for regeneration with machine intelligence. First, we reviewed recent articles with respect to our perspective for machine intelligence-driven information retrieval and fabrication. After briefly introducing recent trends in information retrieval methods from publicly available data, our discussion is focused on recent works that use machine intelligence to discover tissue architectures from imaging and spectral data. Then, our focus is on utilizing optimization approaches to increase print fidelity and enhance biomimicry with machine learning (ML) strategies to acquire a blueprint ready for 3D bioprinting.",2020-01-01,1,1596,97,493
1303,31998756,Machine Learning Approaches for Myocardial Motion and Deformation Analysis,"Information about myocardial motion and deformation is key to differentiate normal and abnormal conditions. With the advent of approaches relying on data rather than pre-conceived models, machine learning could either improve the robustness of motion quantification or reveal patterns of motion and deformation (rather than single parameters) that differentiate pathologies. We review machine learning strategies for extracting motion-related descriptors and analyzing such features among populations, keeping in mind constraints specific to the cardiac application.",2020-01-01,3,566,74,493
1294,32010196,Integrating Computational Methods to Investigate the Macroecology of Microbiomes,"Studies in microbiology have long been mostly restricted to small spatial scales. However, recent technological advances, such as new sequencing methodologies, have ushered an era of large-scale sequencing of environmental DNA data from multiple biomes worldwide. These global datasets can now be used to explore long standing questions of microbial ecology. New methodological approaches and concepts are being developed to study such large-scale patterns in microbial communities, resulting in new perspectives that represent a significant advances for both microbiology and macroecology. Here, we identify and review important conceptual, computational, and methodological challenges and opportunities in microbial macroecology. Specifically, we discuss the challenges of handling and analyzing large amounts of microbiome data to understand taxa distribution and co-occurrence patterns. We also discuss approaches for modeling microbial communities based on environmental data, including information on biological interactions to make full use of available Big Data. Finally, we summarize the methods presented in a general approach aimed to aid microbiologists in addressing fundamental questions in microbial macroecology, including classical propositions (such as ""everything is everywhere, but the environment selects"") as well as applied ecological problems, such as those posed by human induced global environmental changes.",2020-01-01,2,1434,80,493
1261,32055502,Teledermatology and its Current Perspective,"Teledermatology is one of the most important and commonly employed subsets of telemedicine, a special alternative to face-to-face (FTF) doctor--patient consultation that refers to the use of electronic telecommunication tools to facilitate the provision of healthcare between the ""seeker"" and ""provider."" It is used for consultation, education, second opinion, and monitoring medical conditions. This article will review basic concepts, the integration of noninvasive imaging technique images, artificial intelligence, and the current ethical and legal issues.",2020-01-01,7,560,43,493
1271,32039241,Image-Based Cardiac Diagnosis With Machine Learning: A Review,"Cardiac imaging plays an important role in the diagnosis of cardiovascular disease (CVD). Until now, its role has been limited to visual and quantitative assessment of cardiac structure and function. However, with the advent of big data and machine learning, new opportunities are emerging to build artificial intelligence tools that will directly assist the clinician in the diagnosis of CVDs. This paper presents a thorough review of recent works in this field and provide the reader with a detailed presentation of the machine learning methods that can be further exploited to enable more automated, precise and early diagnosis of most CVDs.",2020-01-01,13,644,61,493
1272,32039240,Artificial Intelligence for Cardiac Imaging-Genetics Research,"Cardiovascular conditions remain the leading cause of mortality and morbidity worldwide, with genotype being a significant influence on disease risk. Cardiac imaging-genetics aims to identify and characterize the genetic variants that influence functional, physiological, and anatomical phenotypes derived from cardiovascular imaging. High-throughput DNA sequencing and genotyping have greatly accelerated genetic discovery, making variant interpretation one of the key challenges in contemporary clinical genetics. Heterogeneous, low-fidelity phenotyping and difficulties integrating and then analyzing large-scale genetic, imaging and clinical datasets using traditional statistical approaches have impeded process. Artificial intelligence (AI) methods, such as deep learning, are particularly suited to tackle the challenges of scalability and high dimensionality of data and show promise in the field of cardiac imaging-genetics. Here we review the current state of AI as applied to imaging-genetics research and discuss outstanding methodological challenges, as the field moves from pilot studies to mainstream applications, from one dimensional global descriptors to high-resolution models of whole-organ shape and function, from univariate to multivariate analysis and from candidate gene to genome-wide approaches. Finally, we consider the future directions and prospects of AI imaging-genetics for ultimately helping understand the genetic and environmental underpinnings of cardiovascular health and disease.",2020-01-01,1,1518,61,493
1275,32038544,Computer-Aided Design of Antimicrobial Peptides: Are We Generating Effective Drug Candidates?,"Antimicrobial peptides (AMPs), especially antibacterial peptides, have been widely investigated as potential alternatives to antibiotic-based therapies. Indeed, naturally occurring and synthetic AMPs have shown promising results against a series of clinically relevant bacteria. Even so, this class of antimicrobials has continuously failed clinical trials at some point, highlighting the importance of AMP optimization. In this context, the computer-aided design of AMPs has put together crucial information on chemical parameters and bioactivities in AMP sequences, thus providing modes of prediction to evaluate the antibacterial potential of a candidate sequence before synthesis. Quantitative structure-activity relationship (QSAR) computational models, for instance, have greatly contributed to AMP sequence optimization aimed at improved biological activities. In addition to machine-learning methods, the de novo design, linguistic model, pattern insertion methods, and genetic algorithms, have shown the potential to boost the automated design of AMPs. However, how successful have these approaches been in generating effective antibacterial drug candidates? Bearing this in mind, this review will focus on the main computational strategies that have generated AMPs with promising activities against pathogenic bacteria, as well as anti-infective potential in different animal models, including sepsis and cutaneous infections. Moreover, we will point out recent studies on the computer-aided design of antibiofilm peptides. As expected from automated design strategies, diverse candidate sequences with different structural arrangements have been generated and deposited in databases. We will, therefore, also discuss the structural diversity that has been engendered.",2020-01-01,12,1778,93,493
1276,32038482,Nodular Thyroid Disease in the Era of Precision Medicine,"Management of thyroid nodules in the era of precision medicine is continuously changing. Neck ultrasound plays a pivotal role in the diagnosis and several ultrasound stratification systems have been proposed in order to predict malignancy and help clinicians in therapeutic and follow-up decision. Ultrasound elastosonography is another powerful diagnostic technique and can be an added value to stratify the risk of malignancy of thyroid nodules. Moreover, the development of new techniques in the era of ""Deep Learning,"" has led to a creation of machine-learning algorithms based on ultrasound examinations that showed similar accuracy to that obtained by expert radiologists. Despite new technologies in thyroid imaging, diagnostic surgery in 50-70% of patients with indeterminate cytology is still performed. Molecular tests can increase accuracy in diagnosis when performed on ""indeterminate"" nodules. However, the more updated tools that can be used to this purpose in order to ""rule out"" (Afirma GSC) or ""rule in"" (Thyroseq v3) malignancy, have a main limitation: the high costs. In the last years various image-guided procedures have been proposed as alternative and less invasive approaches to surgery for symptomatic thyroid nodules. These minimally invasive techniques (laser and radio-frequency ablation, high intensity focused ultrasound and percutaneous microwave ablation) results in nodule shrinkage and improvement of local symptoms, with a lower risk of complications and minor costs compared to surgery. Finally, ultrasound-guided ablation therapy was introduced with promising results as a feasible treatment for low-risk papillary thyroid microcarcinoma or cervical lymph node metastases.",2020-01-01,4,1709,56,493
1277,32038208,Intra- and Inter-subject Variability in EEG-Based Sensorimotor Brain Computer Interface: A Review,"Brain computer interfaces (BCI) for the rehabilitation of motor impairments exploit sensorimotor rhythms (SMR) in the electroencephalogram (EEG). However, the neurophysiological processes underpinning the SMR often vary over time and across subjects. Inherent intra- and inter-subject variability causes covariate shift in data distributions that impede the transferability of model parameters amongst sessions/subjects. Transfer learning includes machine learning-based methods to compensate for inter-subject and inter-session (intra-subject) variability manifested in EEG-derived feature distributions as a covariate shift for BCI. Besides transfer learning approaches, recent studies have explored psychological and neurophysiological predictors as well as inter-subject associativity assessment, which may augment transfer learning in EEG-based BCI. Here, we highlight the importance of measuring inter-session/subject performance predictors for generalized BCI frameworks for both normal and motor-impaired people, reducing the necessity for tedious and annoying calibration sessions and BCI training.",2020-01-01,10,1107,97,493
1249,32063919,Heterogeneous Multi-Layered Network Model for Omics Data Integration and Analysis,"Advances in next-generation sequencing and high-throughput techniques have enabled the generation of vast amounts of diverse omics data. These big data provide an unprecedented opportunity in biology, but impose great challenges in data integration, data mining, and knowledge discovery due to the complexity, heterogeneity, dynamics, uncertainty, and high-dimensionality inherited in the omics data. Network has been widely used to represent relations between entities in biological system, such as protein-protein interaction, gene regulation, and brain connectivity (i.e. network construction) as well as to infer novel relations given a reconstructed network (aka link prediction). Particularly, heterogeneous multi-layered network (HMLN) has proven successful in integrating diverse biological data for the representation of the hierarchy of biological system. The HMLN provides unparalleled opportunities but imposes new computational challenges on establishing causal genotype-phenotype associations and understanding environmental impact on organisms. In this review, we focus on the recent advances in developing novel computational methods for the inference of novel biological relations from the HMLN. We first discuss the properties of biological HMLN. Then we survey four categories of state-of-the-art methods (matrix factorization, random walk, knowledge graph, and deep learning). Thirdly, we demonstrate their applications to omics data integration and analysis. Finally, we outline strategies for future directions in the development of new HMLN models.",2020-01-01,3,1571,81,493
1318,31980099,Clinical Decision Support Systems for Triage in the Emergency Department using Intelligent Systems: a Review,"Motivation:                    Emergency Departments' (ED) modern triage systems implemented worldwide are solely based upon medical knowledge and experience. This is a limitation of these systems, since there might be hidden patterns that can be explored in big volumes of clinical historical data. Intelligent techniques can be applied to these data to develop clinical decision support systems (CDSS) thereby providing the health professionals with objective criteria. Therefore, it is of foremost importance to identify what has been hampering the application of such systems for ED triage.              Objectives:                    The objective of this paper is to assess how intelligent CDSS for triage have been contributing to the improvement of quality of care in the ED as well as to identify the challenges they have been facing regarding implementation.              Methods:                    We applied a standard scoping review method with the manual search of 6 digital libraries, namely: ScienceDirect, IEEE Xplore, Google Scholar, Springer, MedlinePlus and Web of Knowledge. Search queries were created and customized for each digital library in order to acquire the information. The core search consisted of searching in the papers' title, abstract and key words for the topics ""triage"", ""emergency department""/""emergency room"" and concepts within the field of intelligent systems.              Results:                    From the review search, we found that logistic regression was the most frequently used technique for model design and the area under the receiver operating curve (AUC) the most frequently used performance measure. Beside triage priority, the most frequently used variables for modelling were patients' age, gender, vital signs and chief complaints. The main contributions of the selected papers consisted in the improvement of a patient's prioritization, prediction of need for critical care, hospital or Intensive Care Unit (ICU) admission, ED Length of Stay (LOS) and mortality from information available at the triage.              Conclusions:                    In the papers where CDSS were validated in the ED, the authors found that there was an improvement in the health professionals' decision-making thereby leading to better clinical management and patients' outcomes. However, we found that more than half of the studies lacked this implementation phase. We concluded that for these studies, it is necessary to validate the CDSS and to define key performance measures in order to demonstrate the extent to which incorporation of CDSS at triage can actually improve care.",2020-01-01,4,2629,108,493
1285,32023986,Machine Learning and Multidrug-Resistant Gram-Negative Bacteria: An Interesting Combination for Current and Future Research,"The dissemination of multidrug-resistant Gram-negative bacteria (MDR-GNB) is associated with increased morbidity and mortality in several countries. Machine learning (ML) is a branch of artificial intelligence that consists of conferring on computers the ability to learn from data. In this narrative review, we discuss three existing examples of the application of ML algorithms for assessing three different types of risk: (i) the risk of developing a MDR-GNB infection, (ii) the risk of MDR-GNB etiology in patients with an already clinically evident infection, and (iii) the risk of anticipating the emergence of MDR in GNB through the misuse of antibiotics. In the next few years, we expect to witness an increasingly large number of research studies perfecting the application of ML techniques in the field of MDR-GNB infections. Very importantly, this cannot be separated from the availability of a continuously refined and updated ethical framework allowing an appropriate use of the large datasets of medical data needed to build efficient ML-based support systems that could be shared through appropriate standard infrastructures.",2020-01-01,2,1140,123,493
1697,31644464,Current status of functional MRI of osteoarthritis for diagnosis and prognosis,"Purpose of review:                    Osteoarthritis is a major source of disability, pain and socioeconomic cost worldwide. The epidemiology of the disorder is multifactorial including genetic, biological and biomechanical components, some of them detectable by MRI. This review provides the most recent update on MRI biomarkers which can provide functional information of the joint structures for diagnosis, prognosis and treatment response monitoring in osteoarthritis trials.              Recent findings:                    Compositional or functional MRI can provide clinicians with valuable information on glycosaminoglycan content (chemical exchange saturation transfer, sodium MRI, T1) and collagen organization (T2, T2, apparent diffusion coefficient, magnetization transfer) in joint structures. Other parameters may also provide useful information, such as volumetric measurements of joint structures or advanced image data postprocessing and analysis. Automated tools seem to have a great potential to be included in these efforts providing standardization and acceleration of the image data analysis process.              Summary:                    Functional or compositional MRI has great potential to provide noninvasive imaging biomarkers for osteoarthritis. Osteoarthritis as a whole joint condition needs to be diagnosed in early stages to facilitate selection of patients into clinical trials and/or to measure treatment effectiveness. Advanced evaluation including machine learning, neural networks and multidimensional data analysis allow for wall-to-wall understanding of parameter interactions and their role in clinical evaluation of osteoarthritis.",2020-01-01,2,1677,78,493
1706,31629933,Radiomics in stratification of pancreatic cystic lesions: Machine learning in action,"Pancreatic cystic lesions (PCLs) are well-known precursors of pancreatic cancer. Their diagnosis can be challenging as their behavior varies from benign to malignant disease. Precise and timely management of malignant pancreatic cysts might prevent transformation to pancreatic cancer. However, the current consensus guidelines, which rely on standard imaging features to predict cyst malignancy potential, are conflicting and unclear. This has led to an increased interest in radiomics, a high-throughput extraction of comprehensible data from standard of care images. Radiomics can be used as a diagnostic and prognostic tool in personalized medicine. It utilizes quantitative image analysis to extract features in conjunction with machine learning and artificial intelligence (AI) methods like support vector machines, random forest, and convolutional neural network for feature selection and classification. Selected features can then serve as imaging biomarkers to predict high-risk PCLs. Radiomics studies conducted heretofore on PCLs have shown promising results. This cost-effective approach would help us to differentiate benign PCLs from malignant ones and potentially guide clinical decision-making leading to better utilization of healthcare resources. In this review, we discuss the process of radiomics, its myriad applications such as diagnosis, prognosis, and prediction of therapy response. We also discuss the outcomes of studies involving radiomic analysis of PCLs and pancreatic cancer, and challenges associated with this novel field along with possible solutions. Although these studies highlight the potential benefit of radiomics in the prevention and optimal treatment of pancreatic cancer, further studies are warranted before incorporating radiomics into the clinical decision support system.",2020-01-01,5,1819,84,493
1709,31625725,An Overview of Machine Learning and Big Data for Drug Toxicity Evaluation,"Drug toxicity evaluation is an essential process of drug development as it is reportedly responsible for the attrition of approximately 30% of drug candidates. The rapid increase in the number and types of large toxicology data sets together with the advances in computational methods may be used to improve many steps in drug safety evaluation. The development of in silico models to screen and understand mechanisms of drug toxicity may be particularly beneficial in the early stages of drug development where early toxicity assessment can most reduce expenses and labor time. To facilitate this, machine learning methods have been employed to evaluate drug toxicity but are often limited by small and less diverse data sets. Recent advances in machine learning methods together with the rapid increase in big toxicity data such as molecular descriptors, toxicogenomics, and high-throughput bioactivity data may help alleviate some of the current challenges. In this article, the most common machine learning methods used in toxicity assessment are reviewed together with examples of toxicity studies that have used machine learning methodology. Furthermore, a comprehensive overview of the different types of toxicity tools and data sets available to build in silico toxicity prediction models has been provided to give an overview of the current big toxicity data landscape and highlight opportunities and challenges related to them.",2020-01-01,8,1437,73,493
1290,32013105,Toward a Standardized Strategy of Clinical Metabolomics for the Advancement of Precision Medicine,"Despite the tremendous success, pitfalls have been observed in every step of a clinical metabolomics workflow, which impedes the internal validity of the study. Furthermore, the demand for logistics, instrumentations, and computational resources for metabolic phenotyping studies has far exceeded our expectations. In this conceptual review, we will cover inclusive barriers of a metabolomics-based clinical study and suggest potential solutions in the hope of enhancing study robustness, usability, and transferability. The importance of quality assurance and quality control procedures is discussed, followed by a practical rule containing five phases, including two additional ""pre-pre-"" and ""post-post-"" analytical steps. Besides, we will elucidate the potential involvement of machine learning and demonstrate that the need for automated data mining algorithms to improve the quality of future research is undeniable. Consequently, we propose a comprehensive metabolomics framework, along with an appropriate checklist refined from current guidelines and our previously published assessment, in the attempt to accurately translate achievements in metabolomics into clinical and epidemiological research. Furthermore, the integration of multifaceted multi-omics approaches with metabolomics as the pillar member is in urgent need. When combining with other social or nutritional factors, we can gather complete omics profiles for a particular disease. Our discussion reflects the current obstacles and potential solutions toward the progressing trend of utilizing metabolomics in clinical research to create the next-generation healthcare system.",2020-01-01,3,1650,97,493
1291,32012941,Mass Spectrometry-Based Multivariate Proteomic Tests for Prediction of Outcomes on Immune Checkpoint Blockade Therapy: The Modern Analytical Approach,"The remarkable success of immune checkpoint inhibitors (ICIs) has given hope of cure for some patients with advanced cancer; however, the fraction of responding patients is 15-35%, depending on tumor type, and the proportion of durable responses is even smaller. Identification of biomarkers with strong predictive potential remains a priority. Until now most of the efforts were focused on biomarkers associated with the assumed mechanism of action of ICIs, such as levels of expression of programmed death-ligand 1 (PD-L1) and mutation load in tumor tissue, as a proxy of immunogenicity; however, their performance is unsatisfactory. Several assays designed to capture the complexity of the disease by measuring the immune response in tumor microenvironment show promise but still need validation in independent studies. The circulating proteome contains an additional layer of information characterizing tumor-host interactions that can be integrated into multivariate tests using modern machine learning techniques. Here we describe several validated serum-based proteomic tests and their utility in the context of ICIs. We discuss test performances, demonstrate their independence from currently used biomarkers, and discuss various aspects of associated biological mechanisms. We propose that serum-based multivariate proteomic tests add a missing piece to the puzzle of predicting benefit from ICIs.",2020-01-01,1,1406,149,493
1292,32012695,The Roles of the NLRP3 Inflammasome in Neurodegenerative and Metabolic Diseases and in Relevant Advanced Therapeutic Interventions,"Inflammasomes are intracellular multiprotein complexes in the cytoplasm that regulate inflammation activation in the innate immune system in response to pathogens and to host self-derived molecules. Recent advances greatly improved our understanding of the activation of nucleotide-binding oligomerization domain-like receptor (NLR) family pyrin domain containing 3 (NLRP3) inflammasomes at the molecular level. The NLRP3 belongs to the subfamily of NLRP which activates caspase 1, thus causing the production of proinflammatory cytokines (interleukin 1 and interleukin 18) and pyroptosis. This inflammasome is involved in multiple neurodegenerative and metabolic disorders including Alzheimer's disease, multiple sclerosis, type 2 diabetes mellitus, and gout. Therefore, therapeutic targeting to the NLRP3 inflammasome complex is a promising way to treat these diseases. Recent research advances paved the way toward drug research and development using a variety of machine learning-based and artificial intelligence-based approaches. These state-of-the-art approaches will lead to the discovery of better drugs after the training of such a system.",2020-01-01,10,1150,130,493
1243,32082358,"Probing lncRNA-Protein Interactions: Data Repositories, Models, and Algorithms","Identifying lncRNA-protein interactions (LPIs) is vital to understanding various key biological processes. Wet experiments found a few LPIs, but experimental methods are costly and time-consuming. Therefore, computational methods are increasingly exploited to capture LPI candidates. We introduced relevant data repositories, focused on two types of LPI prediction models: network-based methods and machine learning-based methods. Machine learning-based methods contain matrix factorization-based techniques and ensemble learning-based techniques. To detect the performance of computational methods, we compared parts of LPI prediction models on Leave-One-Out cross-validation (LOOCV) and fivefold cross-validation. The results show that SFPEL-LPI obtained the best performance of AUC. Although computational models have efficiently unraveled some LPI candidates, there are many limitations involved. We discussed future directions to further boost LPI predictive performance.",2020-01-01,3,976,78,493
2679,32420327,A Survey on Recent Advances in Wearable Fall Detection Systems,"With advances in medicine and healthcare systems, the average life expectancy of human beings has increased to more than 80 yrs. As a result, the demographic old-age dependency ratio (people aged 65 or above relative to those aged 15-64) is expected to increase, by 2060, from 28% to 50% in the European Union and from 33% to 45% in Asia (Ageing Report European Economy, 2015). Therefore, the percentage of people who need additional care is also expected to increase. For instance, per studies conducted by the National Program for Health Care of the Elderly (NPHCE), elderly population in India will increase to 12% of the national population by 2025 with 8%-10% requiring utmost care. Geriatric healthcare has gained a lot of prominence in recent years, with specific focus on fall detection systems (FDSs) because of their impact on public lives. According to a World Health Organization report, the frequency of falls increases with increase in age and frailty. Older people living in nursing homes fall more often than those living in the community and 40% of them experience recurrent falls (World Health Organization, 2007). Machine learning (ML) has found its application in geriatric healthcare systems, especially in FDSs. In this paper, we examine the requirements of a typical FDS. Then we present a survey of the recent work in the area of fall detection systems, with focus on the application of machine learning. We also analyze the challenges in FDS systems based on the literature survey.",2020-01-01,1,1510,62,493
1090,31426055,"Integrating sleep, neuroimaging, and computational approaches for precision psychiatry","In advancing precision psychiatry, we focus on what imaging technology and computational approaches offer for the future of diagnostic subtyping and personalized tailoring of interventions for sleep impairment in mood and anxiety disorders. Current diagnostic criteria for mood and anxiety tend to lump different forms of sleep disturbance together. Parsing the biological features of sleep impairment and brain circuit dysfunction is one approach to identifying subtypes within these disorders that are mechanistically coherent and offer targets for intervention. We focus on two large-scale neural circuits implicated in sleep impairment and in mood and anxiety disorders: the default mode network and negative affective network. Through a synthesis of existing knowledge about these networks, we pose a testable framework for understanding how hyper- versus hypo-engagement of these networks may underlie distinct features of mood and sleep impairment. Within this framework we consider whether poor sleep quality may have an explanatory role in previously observed associations between network dysfunction and mood symptoms. We expand this framework to future directions including the potential for connecting circuit-defined subtypes to more distal features derived from digital phenotyping and wearable technologies, and how new discovery may be advanced through machine learning approaches.",2020-01-01,5,1397,86,493
1267,32047606,Objective Pain Assessment: a Key for the Management of Chronic Pain,"The individual and social burdens associated with chronic pain have been escalating globally. Accurate pain measurement facilitates early diagnosis, disease progression monitoring and therapeutic efficacy evaluation, thus is a key for the management of chronic pain. Although the ""golden standards"" of pain measurement are self-reported scales in clinical practice, the reliability of these subjective methods could be easily affected by patients' physiological and psychological status, as well as the assessors' predispositions. Therefore, objective pain assessment has attracted substantial attention recently. Previous studies of functional magnetic resonance imaging (fMRI) revealed that certain cortices and subcortical areas are commonly activated in subjects suffering from pain. Dynamic pain connectome analysis also found various alterations of neural network connectivity that are correlated with the severity of clinical pain symptoms. Electroencephalograph (EEG) demonstrated suppressed spontaneous oscillations during pain experience. Spectral power and coherence analysis of EEG also identified signatures of different types of chronic pain. Furthermore, fMRI and EEG can visualize objective brain activities modulated by analgesics in a mechanism-based way, thus bridge the gaps between animal studies and clinical trials. Using fMRI and EEG, researchers are able to predict therapeutic efficacy and identify personalized optimal first-line regimens. In the future, the emergence of magnetic resonance spectroscopy and cell labelling in MRI would encourage the investigation on metabolic and cellular pain biomarkers. The incorporation of machine learning algorithms with neuroimaging or behavior analysis could further enhance the specificity and accuracy of objective pain assessments.",2020-01-01,0,1803,67,493
2492,30653364,Recent advances in computational tools and resources for the self-management of type 2 diabetes,"Background: While healthcare systems are investing resources on type 2 diabetes patients, self-management is becoming the new trend for these patients. Due to the pervasiveness of computing devices, a number of computerized systems are emerging to support the self-management of patients.Objective: The primary objective of this review is to identify and categorize the computational tools that exist for the self-management of type 2 diabetes, and to identify challenges that need to be addressed.Results: The tools have been categorized into web applications, mobile applications, games and ubiquitous diabetes management systems. We provide a detailed description of the salient features of each category along with a comparison of the various tools, listing their challenges and practical implications. A list of platforms that can be used to develop new tools for the self-management of type 2 diabetes, namely mobile applications development, sensor development, cloud computing, social media, and machine learning and predictive analysis platforms, are also provided.Discussions: This paper identifies a number of challenges in the existing categories of computational tools and consequently presents possible avenues for future research. Failure to address these issues will negatively impact on the adoption rate of the self-management tools and applications.",2020-01-01,0,1368,95,493
1412,32655294,A Brief Survey for MicroRNA Precursor Identification Using Machine Learning Methods,"MicroRNAs, a group of short non-coding RNA molecules, could regulate gene expression. Many diseases are associated with abnormal expression of miRNAs. Therefore, accurate identification of miRNA precursors is necessary. In the past 10 years, experimental methods, comparative genomics methods, and artificial intelligence methods have been used to identify pre-miRNAs. However, experimental methods and comparative genomics methods have their disadvantages, such as time-consuming. In contrast, machine learning-based method is a better choice. Therefore, the review summarizes the current advances in pre-miRNA recognition based on computational methods, including the construction of benchmark datasets, feature extraction methods, prediction algorithms, and the results of the models. And we also provide valid information about the predictors currently available. Finally, we give the future perspectives on the identification of pre-miRNAs. The review provides scholars with a whole background of pre-miRNA identification by using machine learning methods, which can help researchers have a clear understanding of progress of the research in this field.",2020-01-01,0,1158,83,493
1023,31818379,Machine Learning Principles for Radiology Investigators,"Artificial intelligence and deep learning are areas of high interest for radiology investigators at present. However, the field of machine learning encompasses multiple statistics-based techniques useful for investigators, which may be complementary to deep learning approaches. After a refresher in basic statistical concepts, relevant considerations for machine learning practitioners are reviewed: regression, classification, decision boundaries, and bias-variance tradeoff. Regularization, ground truth, and populations are discussed along with compute and data management principles. Advanced statistical machine learning techniques including bootstrapping, bagging, boosting, decision trees, random forest, XGboost, and support vector machines are reviewed along with relevant examples from the radiology literature.",2020-01-01,1,822,55,493
1581,32128436,Automated assessment of psychiatric disorders using speech: A systematic review,"Objective:                    There are many barriers to accessing mental health assessments including cost and stigma. Even when individuals receive professional care, assessments are intermittent and may be limited partly due to the episodic nature of psychiatric symptoms. Therefore, machine-learning technology using speech samples obtained in the clinic or remotely could one day be a biomarker to improve diagnosis and treatment. To date, reviews have only focused on using acoustic features from speech to detect depression and schizophrenia. Here, we present the first systematic review of studies using speech for automated assessments across a broader range of psychiatric disorders.              Methods:                    We followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines. We included studies from the last 10 years using speech to identify the presence or severity of disorders within the Diagnostic and Statistical Manual of Mental Disorders (DSM-5). For each study, we describe sample size, clinical evaluation method, speech-eliciting tasks, machine learning methodology, performance, and other relevant findings.              Results:                    1395 studies were screened of which 127 studies met the inclusion criteria. The majority of studies were on depression, schizophrenia, and bipolar disorder, and the remaining on post-traumatic stress disorder, anxiety disorders, and eating disorders. 63% of studies built machine learning predictive models, and the remaining 37% performed null-hypothesis testing only. We provide an online database with our search results and synthesize how acoustic features appear in each disorder.              Conclusion:                    Speech processing technology could aid mental health assessments, but there are many obstacles to overcome, especially the need for comprehensive transdiagnostic and longitudinal studies. Given the diverse types of data sets, feature extraction, computational methodologies, and evaluation criteria, we provide guidelines for both acquiring data and building machine learning models with a focus on testing hypotheses, open science, reproducibility, and generalizability.              Level of evidence:                    3a.",2020-01-01,10,2281,79,493
1507,32249208,Psychiatric Illnesses as Disorders of Network Dynamics,"This review provides a dynamical systems perspective on mental illness. After a brief introduction to the theory of dynamical systems, we focus on the common assumption in theoretical and computational neuroscience that phenomena at subcellular, cellular, network, cognitive, and even societal levels could be described and explained in terms of dynamical systems theory. As such, dynamical systems theory may also provide a framework for understanding mental illnesses. The review examines a number of core dynamical systems phenomena and relates each of these to aspects of mental illnesses. This provides an outline of how a broad set of phenomena in serious and common mental illnesses and neurological conditions can be understood in dynamical systems terms. It suggests that the dynamical systems level may provide a central, hublike level of convergence that unifies and links multiple biophysical and behavioral phenomena in the sense that diverse biophysical changes can give rise to the same dynamical phenomena and, vice versa, similar changes in dynamics may yield different behavioral symptoms depending on the brain area where these changes manifest. We also briefly outline current methodological approaches for inferring dynamical systems from data such as electroencephalography, functional magnetic resonance imaging, or self-reports, and we discuss the implications of a dynamical view for the diagnosis, prognosis, and treatment of psychiatric conditions. We argue that a consideration of dynamics could play a potentially transformative role in the choice and target of interventions.",2020-01-01,3,1605,54,493
1022,31818386,School of Block-Review of Blockchain for the Radiologists,"Blockchain, the underlying technology for Bitcoin, is a distributed digital ledger technology that enables record verification by many independent parties rather than a centralized authority, therefore making it more difficult to tamper with the data. This emerging technology has the potential to enhance various authentication and verification processes in image sharing and data security. It has the potential to promote patient-centered healthcare by giving greater control to patients over their own data. Blockchain can also be utilized for administrative tasks, such as credentialing, claims adjudication, and billing management. It can also be utilized to enhance software supporting research and clinical trials. Blockchain complements artificial intelligence (AI) and these can work synergistically to create better solutions. Although many challenges exist for increased adoption of blockchain within radiology and healthcare in general, it can play a major role in our practice and consequently, it is important for medical imaging professionals to become familiar with the technology.",2020-01-01,0,1097,57,493
1495,32257027,Artificial Intelligence and Machine Learning: A New Disruptive Force in Orthopaedics,"Orthopaedics as a surgical discipline requires a combination of good clinical acumen, good surgical skill, a reasonable physical strength and most of all, good understanding of technology. The last few decades have seen rapid adoption of new technologies into orthopaedic practice, power tools, new implants, CAD-CAM design, 3-D printing, additive manufacturing just to name a few. The new disruption in orthopaedics in the current time and era is undoubtedly the advent of artificial intelligence and robotics. As these technologies take root and innovative applications continue to be incorporated into the main-stream orthopedics, as we know it today, it is imperative to look at and understand the basics of artificial intelligence and what work is being done in the field today. This article takes the form of a loosely structured narrative review and will introduce the reader to key concepts in the field of artificial intelligence as well as some of the directions in application of the same in orthopaedics. Some of the recent work has been summarised and we present our viewpoint at the conclusion as to why we must consider artificial intelligence as a disrupting positive influence on orthopaedic surgery.",2020-01-01,3,1217,84,493
1016,31831878,Drug repurposing to improve treatment of rheumatic autoimmune inflammatory diseases,"The past century has been characterized by intensive efforts, within both academia and the pharmaceutical industry, to introduce new treatments to individuals with rheumatic autoimmune inflammatory diseases (RAIDs), often by 'borrowing' treatments already employed in one RAID or previously used in an entirely different disease, a concept known as drug repurposing. However, despite sharing some clinical manifestations and immune dysregulation, disease pathogenesis and phenotype vary greatly among RAIDs, and limited understanding of their aetiology has made repurposing drugs for RAIDs challenging. Nevertheless, the past century has been characterized by different 'waves' of repurposing. Early drug repurposing occurred in academia and was based on serendipitous observations or perceived disease similarity, often driven by the availability and popularity of drug classes. Since the 1990s, most biologic therapies have been developed for one or several RAIDs and then tested among the others, with varying levels of success. The past two decades have seen data-driven repurposing characterized by signature-based approaches that rely on molecular biology and genomics. Additionally, many data-driven strategies employ computational modelling and machine learning to integrate multiple sources of data. Together, these repurposing periods have led to advances in the treatment for many RAIDs.",2020-01-01,8,1398,83,493
1006,31846526,"The coming 15 years in gynaecological pathology: digitisation, artificial intelligence, and new technologies","Surgical pathology forms the cornerstone of modern oncological medicine, owing to the wealth of clinically relevant information that can be obtained from tissue morphology. Although several ancillary testing modalities have been added to surgical pathology, the way in which we view and interpret tissue morphology has remained largely unchanged since the inception of our profession. In this review, we discuss new technological advances that promise to transform the way in which we access tissue morphology and how we use it to guide patient care.",2020-01-01,2,550,108,493
1884,33323492,Machine Learning for Child and Adolescent Health: A Systematic Review,"Context:                    In the last few decades, data acquisition and processing has seen tremendous amount of growth, thus sparking interest in machine learning (ML) within the health care system.              Objective:                    Our aim for this review is to provide an evidence map of the current available evidence on ML in pediatrics and adolescent medicine and provide insight for future research.              Data sources:                    A literature search was conducted by using Medline, the Cochrane Library, the Cumulative Index to Nursing and Allied Health Literature Plus, Web of Science Library, and EBSCO Dentistry & Oral Science Source.              Study selection:                    Articles in which an ML model was assessed for the diagnosis, prediction, or management of any condition in children and adolescents (0-18 years) were included.              Data extraction:                    Data were extracted for year of publication, geographical location, age range, number of participants, disease or condition under investigation, study methodology, reference standard, type, category, and performance of ML algorithms.              Results:                    The review included 363 studies, with subspecialties such as psychiatry, neonatology, and neurology having the most literature. A majority of the studies were from high-income (82%; n = 296) and upper middle-income countries (15%; n = 56), whereas only 3% (n = 11) were from low middle-income countries. Neural networks and ensemble methods were most commonly tested in the 1990s, whereas deep learning and clustering emerged rapidly in the current decade.              Limitations:                    Only studies conducted in the English language could be used in this review.              Conclusions:                    The interest in ML has been growing across various subspecialties and countries, suggesting a potential role in health service delivery for children and adolescents in the years to come.",2020-01-01,0,2016,69,493
1413,32655293,A Mini-review of the Computational Methods Used in Identifying RNA 5-Methylcytosine Sites,"RNA 5-methylcytosine (m5C) is one of the pillars of post-transcriptional modification (PTCM). A growing body of evidence suggests that m5C plays a vital role in RNA metabolism. Accurate localization of RNA m5C sites in tissue cells is the premise and basis for the in-depth understanding of the functions of m5C. However, the main experimental methods of detecting m5C sites are limited to varying degrees. Establishing a computational model to predict modification sites is an excellent complement to wet experiments for identifying m5C sites. In this review, we summarized some available m5C predictors and discussed the characteristics of these methods.",2020-01-01,1,656,89,493
1458,32612753,Deep learning methods in protein structure prediction,"Protein Structure Prediction is a central topic in Structural Bioinformatics. Since the '60s statistical methods, followed by increasingly complex Machine Learning and recently Deep Learning methods, have been employed to predict protein structural information at various levels of detail. In this review, we briefly introduce the problem of protein structure prediction and essential elements of Deep Learning (such as Convolutional Neural Networks, Recurrent Neural Networks and basic feed-forward Neural Networks they are founded on), after which we discuss the evolution of predictive methods for one-dimensional and two-dimensional Protein Structure Annotations, from the simple statistical methods of the early days, to the computationally intensive highly-sophisticated Deep Learning algorithms of the last decade. In the process, we review the growth of the databases these algorithms are based on, and how this has impacted our ability to leverage knowledge about evolution and co-evolution to achieve improved predictions. We conclude this review outlining the current role of Deep Learning techniques within the wider pipelines to predict protein structures and trying to anticipate what challenges and opportunities may arise next.",2020-01-01,10,1243,53,493
1279,32034573,Integrating radiomics into holomics for personalised oncology: from algorithms to bedside,"Radiomics, artificial intelligence, and deep learning figure amongst recent buzzwords in current medical imaging research and technological development. Analysis of medical big data in assessment and follow-up of personalised treatments has also become a major research topic in the area of precision medicine. In this review, current research trends in radiomics are analysed, from handcrafted radiomics feature extraction and statistical analysis to deep learning. Radiomics algorithms now include genomics and immunomics data to improve patient stratification and prediction of treatment response. Several applications have already shown conclusive results demonstrating the potential of including other ""omics"" data to existing imaging features. We also discuss further challenges of data harmonisation and management infrastructure to shed a light on the much-needed integration of radiomics and all other ""omics"" into clinical workflows. In particular, we point to the emerging paradigm shift in the implementation of big data infrastructures to facilitate databanks growth, data extraction and the development of expert software tools. Secured access, sharing, and integration of all health data, called ""holomics"", will accelerate the revolution of personalised medicine and oncology as well as expand the role of imaging specialists.",2020-02-01,4,1342,89,462
1547,32185167,Use of Computational Modeling to Study Joint Degeneration: A Review,"Osteoarthritis (OA), a degenerative joint disease, is the most common chronic condition of the joints, which cannot be prevented effectively. Computational modeling of joint degradation allows to estimate the patient-specific progression of OA, which can aid clinicians to estimate the most suitable time window for surgical intervention in osteoarthritic patients. This paper gives an overview of the different approaches used to model different aspects of joint degeneration, thereby focusing mostly on the knee joint. The paper starts by discussing how OA affects the different components of the joint and how these are accounted for in the models. Subsequently, it discusses the different modeling approaches that can be used to answer questions related to OA etiology, progression and treatment. These models are ordered based on their underlying assumptions and technologies: musculoskeletal models, Finite Element models, (gene) regulatory models, multiscale models and data-driven models (artificial intelligence/machine learning). Finally, it is concluded that in the future, efforts should be made to integrate the different modeling techniques into a more robust computational framework that should not only be efficient to predict OA progression but also easily allow a patient's individualized risk assessment as screening tool for use in clinical practice.",2020-02-01,1,1370,67,462
2163,31732118,"Digital transformation of academic medicine: Breaking barriers, borders, and boredom","Academic medicine is experiencing an exponential increase in knowledge, evidenced by approximately 2.5 million new articles published each year. As a result, staying apprised of practice-changing findings as a busy clinician is nearly impossible. The traditional methods of staying up to date through reading textbooks and journal articles or attending an annual conference are no longer enough. These old approaches do not distribute knowledge equally around the world or inform practitioners adequately of what they need to provide the best patient care. Luckily, digital technology, which contributed to our ability to generate this explosion in research, also holds the solution. We believe the improved filtration and curation of new knowledge will come through the combination of three elements: machine learning, crowd-sourcing, and new digital platforms. Machine learning can be harnessed to identify high-quality research while avoiding unconscious bias towards authors, institutions, or positions, and to create personalized reading lists that encompass essential articles while also addressing personal knowledge gaps. The crowd can also serve to curate the best research through an open-source platform that exposes each step of the research process, from developing questions through discussion of findings, functionally replacing editorial boards with crowd peer-review. Finally, embracing new digital platforms and multimedia delivery formats will move academic medicine into the 21st century, broadening its reach to diverse, international, and multigenerational learners. The digital age will continue to change life as we know it, but we have the power - and the responsibility - to control how it transforms academic medicine. LEVEL OF EVIDENCE: V (Expert).",2020-02-01,0,1776,84,462
1577,32133344,In silico Strategies to Support Fragment-to-Lead Optimization in Drug Discovery,"Fragment-based drug (or lead) discovery (FBDD or FBLD) has developed in the last two decades to become a successful key technology in the pharmaceutical industry for early stage drug discovery and development. The FBDD strategy consists of screening low molecular weight compounds against macromolecular targets (usually proteins) of clinical relevance. These small molecular fragments can bind at one or more sites on the target and act as starting points for the development of lead compounds. In developing the fragments attractive features that can translate into compounds with favorable physical, pharmacokinetics and toxicity (ADMET-absorption, distribution, metabolism, excretion, and toxicity) properties can be integrated. Structure-enabled fragment screening campaigns use a combination of screening by a range of biophysical techniques, such as differential scanning fluorimetry, surface plasmon resonance, and thermophoresis, followed by structural characterization of fragment binding using NMR or X-ray crystallography. Structural characterization is also used in subsequent analysis for growing fragments of selected screening hits. The latest iteration of the FBDD workflow employs a high-throughput methodology of massively parallel screening by X-ray crystallography of individually soaked fragments. In this review we will outline the FBDD strategies and explore a variety of in silico approaches to support the follow-up fragment-to-lead optimization of either: growing, linking, and merging. These fragment expansion strategies include hot spot analysis, druggability prediction, SAR (structure-activity relationships) by catalog methods, application of machine learning/deep learning models for virtual screening and several de novo design methods for proposing synthesizable new compounds. Finally, we will highlight recent case studies in fragment-based drug discovery where in silico methods have successfully contributed to the development of lead compounds.",2020-02-01,10,1985,79,462
2151,31753325,A machine learning approach to predict surgical learning curves,"Background:                    Contemporary surgical training programs rely on the repetition of selected surgical motor tasks. Such methodology is inherently open ended with no control on the time taken to attain a set level of proficiency, given the trainees' intrinsic differences in initial skill levels and learning abilities. Hence, an efficient training program should aim at tailoring the surgical training protocols to each trainee. In this regard, a predictive model using information from the initial learning stage to predict learning curve characteristics should facilitate the whole surgical training process.              Methods:                    This paper analyzes learning curve data to train a multivariate supervised machine learning model. One factor is extracted to define the trainees' learning ability. An unsupervised machine learning model is also utilized for trainee classification. When established, the model can predict robustly the learning curve characteristics based on the first few trials.              Results:                    We show that the information present in the first 10 trials of surgical tasks can be utilized to predict the number of trials required to achieve proficiency (R2=0.72) and the final performance level (R2=0.89). Furthermore, only a single factor, learning index, is required to describe the learning process and to classify learners with unique learning characteristics.              Conclusion:                    Using machine learning models, we show, for the first time, that the first few trials contain sufficient information to predict learning curve characteristics and that a single factor can capture the complex learning behavior. Using such models holds the potential for personalization of training regimens, leading to greater efficiency and lower costs.",2020-02-01,1,1837,63,462
1519,32232112,Biomarkers and neuromodulation techniques in substance use disorders,"Addictive disorders are a severe health concern. Conventional therapies have just moderate success and the probability of relapse after treatment remains high. Brain stimulation techniques, such as transcranial Direct Current Stimulation (tDCS) and Deep Brain Stimulation (DBS), have been shown to be effective in reducing subjectively rated substance craving. However, there are few objective and measurable parameters that reflect neural mechanisms of addictive disorders and relapse. Key electrophysiological features that characterize substance related changes in neural processing are Event-Related Potentials (ERP). These high temporal resolution measurements of brain activity are able to identify neurocognitive correlates of addictive behaviours. Moreover, ERP have shown utility as biomarkers to predict treatment outcome and relapse probability. A future direction for the treatment of addiction might include neural interfaces able to detect addiction-related neurophysiological parameters and deploy neuromodulation adapted to the identified pathological features in a closed-loop fashion. Such systems may go beyond electrical recording and stimulation to employ sensing and neuromodulation in the pharmacological domain as well as advanced signal analysis and machine learning algorithms. In this review, we describe the state-of-the-art in the treatment of addictive disorders with electrical brain stimulation and its effect on addiction-related neurophysiological markers. We discuss advanced signal processing approaches and multi-modal neural interfaces as building blocks in future bioelectronics systems for treatment of addictive disorders.",2020-02-01,0,1663,68,462
1525,32226593,Exploring 3D chromatin contacts in gene regulation: The evolution of approaches for the identification of functional enhancer-promoter interaction,"Mechanisms underlying gene regulation are key to understand how multicellular organisms with various cell types develop from the same genetic blueprint. Dynamic interactions between enhancers and genes are revealed to play central roles in controlling gene transcription, but the determinants to link functional enhancer-promoter pairs remain elusive. A major challenge is the lack of reliable approach to detect and verify functional enhancer-promoter interactions (EPIs). In this review, we summarized the current methods for detecting EPIs and described how developing techniques facilitate the identification of EPI through assessing the merits and drawbacks of these methods. We also reviewed recent state-of-art EPI prediction methods in terms of their rationale, data usage and characterization. Furthermore, we briefly discussed the evolved strategies for validating functional EPIs.",2020-02-01,3,891,146,462
1561,32158767,From Compressed-Sensing to Artificial Intelligence-Based Cardiac MRI Reconstruction,"Cardiac magnetic resonance (CMR) imaging is an important tool for the non-invasive assessment of cardiovascular disease. However, CMR suffers from long acquisition times due to the need of obtaining images with high temporal and spatial resolution, different contrasts, and/or whole-heart coverage. In addition, both cardiac and respiratory-induced motion of the heart during the acquisition need to be accounted for, further increasing the scan time. Several undersampling reconstruction techniques have been proposed during the last decades to speed up CMR acquisition. These techniques rely on acquiring less data than needed and estimating the non-acquired data exploiting some sort of prior information. Parallel imaging and compressed sensing undersampling reconstruction techniques have revolutionized the field, enabling 2- to 3-fold scan time accelerations to become standard in clinical practice. Recent scientific advances in CMR reconstruction hinge on the thriving field of artificial intelligence. Machine learning reconstruction approaches have been recently proposed to learn the non-linear optimization process employed in CMR reconstruction. Unlike analytical methods for which the reconstruction problem is explicitly defined into the optimization process, machine learning techniques make use of large data sets to learn the key reconstruction parameters and priors. In particular, deep learning techniques promise to use deep neural networks (DNN) to learn the reconstruction process from existing datasets in advance, providing a fast and efficient reconstruction that can be applied to all newly acquired data. However, before machine learning and DNN can realize their full potentials and enter widespread clinical routine for CMR image reconstruction, there are several technical hurdles that need to be addressed. In this article, we provide an overview of the recent developments in the area of artificial intelligence for CMR image reconstruction. The underlying assumptions of established techniques such as compressed sensing and low-rank reconstruction are briefly summarized, while a greater focus is given to recent advances in dictionary learning and deep learning based CMR reconstruction. In particular, approaches that exploit neural networks as implicit or explicit priors are discussed for 2D dynamic cardiac imaging and 3D whole-heart CMR imaging. Current limitations, challenges, and potential future directions of these techniques are also discussed.",2020-02-01,2,2492,83,462
2143,31761063,Imaging of Central Nervous System Tumors Based on the 2016 World Health Organization Classification,"The 2016 World Health Organization Classification of Tumors of the Central Nervous System (CNS) incorporated well-established molecular markers known to drive tumorigenesis and tumor behavior into the existing classification of CNS tumors based on histopathologic appearance. This integrated classification system has led to a major restructuring of the diffuse gliomas. In addition, it resulted in the categorization of medulloblastomas into four distinct molecular subgroups. Radiogenomic studies have revealed key imaging differences between certain genetic groups and may aid in the diagnosis, longitudinal assessment of treatment response, and evaluation of tumor recurrence in patients with brain tumors.",2020-02-01,3,710,99,462
802,33000054,Predicting cardiac arrest in the emergency department,"In-hospital cardiac arrest remains a leading cause of death: roughly 300,000 in-hospital cardiac arrests occur each year in the United States, 10% of which occur in the emergency department. ED-based cardiac arrest may represent a subset of in-hospital cardiac arrest with a higher proportion of reversible etiologies and a higher potential for neurologically intact survival. Patients presenting to the ED have become increasingly complex, have a high burden of critical illness, and face crowded departments with thinly stretched resources. As a result, patients in the ED are vulnerable to unrecognized clinical deterioration that may lead to ED-based cardiac arrest. Efforts to identify patients who may progress to ED-based cardiac arrest have traditionally been approached through identification of critically ill patients at triage and the identification of patients who unexpectedly deteriorate during their stay in the ED. Interventions to facilitate appropriate triage and resource allocation, as well as earlier identification of patients at risk of deterioration in the ED, could potentially allow for both prevention of cardiac arrest and optimization of outcomes from ED-based cardiac arrest. This review will discuss the epidemiology of ED-based cardiac arrest, as well as commonly used approaches to predict ED-based cardiac arrest and highlight areas that require further research to improve outcomes for this population.",2020-02-01,0,1439,53,462
2150,31754741,"Highlights of the special scientific sessions of the 46th Annual Scientific Meeting of the International Skeletal Society (ISS) 2019, Vancouver, Canada","This paper summarizes the highlights of the Scientific Sessions of the 46th Annual Scientific Meeting of the International Skeletal Society (ISS) which was hosted in Vancouver, Canada, in September 2019.",2020-02-01,0,203,151,462
1284,32024055,Precision Psychiatry Applications with Pharmacogenomics: Artificial Intelligence and Machine Learning Approaches,"A growing body of evidence now suggests that precision psychiatry, an interdisciplinary field of psychiatry, precision medicine, and pharmacogenomics, serves as an indispensable foundation of medical practices by offering the accurate medication with the accurate dose at the accurate time to patients with psychiatric disorders. In light of the latest advancements in artificial intelligence and machine learning techniques, numerous biomarkers and genetic loci associated with psychiatric diseases and relevant treatments are being discovered in precision psychiatry research by employing neuroimaging and multi-omics. In this review, we focus on the latest developments for precision psychiatry research using artificial intelligence and machine learning approaches, such as deep learning and neural network algorithms, together with multi-omics and neuroimaging data. Firstly, we review precision psychiatry and pharmacogenomics studies that leverage various artificial intelligence and machine learning techniques to assess treatment prediction, prognosis prediction, diagnosis prediction, and the detection of potential biomarkers. In addition, we describe potential biomarkers and genetic loci that have been discovered to be associated with psychiatric diseases and relevant treatments. Moreover, we outline the limitations in regard to the previous precision psychiatry and pharmacogenomics studies. Finally, we present a discussion of directions and challenges for future research.",2020-02-01,7,1491,112,462
2145,31759574,Neuroimaging in Schizophrenia,"Schizophrenia is a chronic psychotic disorder with a lifetime prevalence of about 1%. Onset is typically in adolescence or early adulthood; characteristic symptoms include positive symptoms, negative symptoms, and impairments in cognition. Neuroimaging studies have shown substantive evidence of brain structural, functional, and neurochemical alterations that are more pronounced in the association cortex and subcortical regions. These abnormalities are not sufficiently specific to be of diagnostic value, but there may be a role for imaging techniques to provide predictions of outcome. Incorporating multimodal imaging datasets using machine learning approaches may offer better diagnostic and predictive value in schizophrenia.",2020-02-01,4,733,29,462
1570,32140203,Exploring the computational methods for protein-ligand binding site prediction,"Proteins participate in various essential processes in vivo via interactions with other molecules. Identifying the residues participating in these interactions not only provides biological insights for protein function studies but also has great significance for drug discoveries. Therefore, predicting protein-ligand binding sites has long been under intense research in the fields of bioinformatics and computer aided drug discovery. In this review, we first introduce the research background of predicting protein-ligand binding sites and then classify the methods into four categories, namely, 3D structure-based, template similarity-based, traditional machine learning-based and deep learning-based methods. We describe representative algorithms in each category and elaborate on machine learning and deep learning-based prediction methods in more detail. Finally, we discuss the trends and challenges of the current research such as molecular dynamics simulation based cryptic binding sites prediction, and highlight prospective directions for the near future.",2020-02-01,0,1066,78,462
2144,31759575,Widespread Morphometric Abnormalities in Major Depression: Neuroplasticity and Potential for Biomarker Development,Major depression is common and debilitating. Identifying neurobiological subtypes that comprise the disorder and predict clinical outcome are key challenges. Genetic and environmental factors leading to major depression are expressed in neural structure and function. Volumetric decreases in gray matter have been demonstrated in corticolimbic circuits involved in emotion regulation. MR imaging observable abnormalities reflect cytoarchitectonic alterations within a local neuroendocrine milieu with systemic effects. Multivariate pattern analysis offers the potential to identify the neurobiological subtypes and predictors of clinical outcome. It is essential to characterize disease heterogeneity by incorporating data-driven inductive and symptom-based deductive approaches in an iterative process.,2020-02-01,0,803,114,462
1319,31978628,Artificial intelligence approaches to predicting and detecting cognitive decline in older adults: A conceptual review,"Preserving cognition and mental capacity is critical to aging with autonomy. Early detection of pathological cognitive decline facilitates the greatest impact of restorative or preventative treatments. Artificial Intelligence (AI) in healthcare is the use of computational algorithms that mimic human cognitive functions to analyze complex medical data. AI technologies like machine learning (ML) support the integration of biological, psychological, and social factors when approaching diagnosis, prognosis, and treatment of disease. This paper serves to acquaint clinicians and other stakeholders with the use, benefits, and limitations of AI for predicting, diagnosing, and classifying mild and major neurocognitive impairments, by providing a conceptual overview of this topic with emphasis on the features explored and AI techniques employed. We present studies that fell into six categories of features used for these purposes: (1) sociodemographics; (2) clinical and psychometric assessments; (3) neuroimaging and neurophysiology; (4) electronic health records and claims; (5) novel assessments (e.g., sensors for digital data); and (6) genomics/other omics. For each category we provide examples of AI approaches, including supervised and unsupervised ML, deep learning, and natural language processing. AI technology, still nascent in healthcare, has great potential to transform the way we diagnose and treat patients with neurocognitive disorders.",2020-02-01,5,1458,117,462
1297,32008107,Policy Implications of Artificial Intelligence and Machine Learning in Diabetes Management,"Purpose of review:                    Machine learning (ML) is increasingly being studied for the screening, diagnosis, and management of diabetes and its complications. Although various models of ML have been developed, most have not led to practical solutions for real-world problems. There has been a disconnect between ML developers, regulatory bodies, health services researchers, clinicians, and patients in their efforts. Our aim is to review the current status of ML in various aspects of diabetes care and identify key challenges that must be overcome to leverage ML to its full potential.              Recent findings:                    ML has led to impressive progress in development of automated insulin delivery systems and diabetic retinopathy screening tools. Compared with these, use of ML in other aspects of diabetes is still at an early stage. The Food & Drug Administration (FDA) is adopting some innovative models to help bring technologies to the market in an expeditious and safe manner. ML has great potential in managing diabetes and the future is in furthering the partnership of regulatory bodies with health service researchers, clinicians, developers, and patients to improve the outcomes of populations and individual patients with diabetes.",2020-02-01,1,1273,90,462
83,32296743,Applied machine learning and artificial intelligence in rheumatology,"Machine learning as a field of artificial intelligence is increasingly applied in medicine to assist patients and physicians. Growing datasets provide a sound basis with which to apply machine learning methods that learn from previous experiences. This review explains the basics of machine learning and its subfields of supervised learning, unsupervised learning, reinforcement learning and deep learning. We provide an overview of current machine learning applications in rheumatology, mainly supervised learning methods for e-diagnosis, disease detection and medical image analysis. In the future, machine learning will be likely to assist rheumatologists in predicting the course of the disease and identifying important disease factors. Even more interestingly, machine learning will probably be able to make treatment propositions and estimate their expected benefit (e.g. by reinforcement learning). Thus, in future, shared decision-making will not only include the patient's opinion and the rheumatologist's empirical and evidence-based experience, but it will also be influenced by machine-learned evidence.",2020-02-01,6,1116,68,462
1314,31983042,"Machine Learning and Artificial Intelligence: Definitions, Applications, and Future Directions","Purpose of review:                    With the unprecedented advancement of data aggregation and deep learning algorithms, artificial intelligence (AI) and machine learning (ML) are poised to transform the practice of medicine. The field of orthopedics, in particular, is uniquely suited to harness the power of big data, and in doing so provide critical insight into elevating the many facets of care provided by orthopedic surgeons. The purpose of this review is to critically evaluate the recent and novel literature regarding ML in the field of orthopedics and to address its potential impact on the future of musculoskeletal care.              Recent findings:                    Recent literature demonstrates that the incorporation of ML into orthopedics has the potential to elevate patient care through alternative patient-specific payment models, rapidly analyze imaging modalities, and remotely monitor patients. Just as the business of medicine was once considered outside the domain of the orthopedic surgeon, we report evidence that demonstrates these emerging applications of AI warrant ownership, leverage, and application by the orthopedic surgeon to better serve their patients and deliver optimal, value-based care.",2020-02-01,5,1234,94,462
1270,32041303,Solution of Levinthal's Paradox and a Physical Theory of Protein Folding Times,"""How do proteins fold?"" Researchers have been studying different aspects of this question for more than 50 years. The most conceptual aspect of the problem is how protein can find the global free energy minimum in a biologically reasonable time, without exhaustive enumeration of all possible conformations, the so-called ""Levinthal's paradox."" Less conceptual but still critical are aspects about factors defining folding times of particular proteins and about perspectives of machine learning for their prediction. We will discuss in this review the key ideas and discoveries leading to the current understanding of folding kinetics, including the solution of Levinthal's paradox, as well as the current state of the art in the prediction of protein folding times.",2020-02-01,0,766,78,462
2046,33733124,An Introductory Review of Deep Learning for Prediction Models With Big Data,"Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist's toolbox. Importantly, those core architectural building blocks can be composed flexibly-in an almost Lego-like manner-to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.",2020-02-01,2,1166,75,462
2065,33431041,The C++ programming language in cheminformatics and computational chemistry,"This paper describes salient features of the C++ programming language and its programming ecosystem, with emphasis on how the language affects scientific software development. Brief history of C++ and its predecessor the C language is provided. Most important aspects of the language that define models of programming are described in greater detail and illustrated with code examples. Special attention is paid to the interoperability between C++ and other high-level languages commonly used in cheminformatics, machine learning, data processing and statistical computing.",2020-02-01,0,573,75,462
1309,31991452,Quantification in Musculoskeletal Imaging Using Computational Analysis and Machine Learning: Segmentation and Radiomics,"Although still limited in clinical practice, quantitative analysis is expected to increase the value of musculoskeletal (MSK) imaging. Segmentation aims at isolating the tissues and/or regions of interest in the image and is crucial to the extraction of quantitative features such as size, signal intensity, or image texture. These features may serve to support the diagnosis and monitoring of disease. Radiomics refers to the process of extracting large amounts of features from radiologic images and combining them with clinical, biological, genetic, or any other type of complementary data to build diagnostic, prognostic, or predictive models. The advent of machine learning offers promising prospects for automatic segmentation and integration of large amounts of data. We present commonly used segmentation methods and describe the radiomics pipeline, highlighting the challenges to overcome for adoption in clinical practice. We provide some examples of applications from the MSK literature.",2020-02-01,0,998,119,462
1493,32258124,Tabu Search and Machine-Learning Classification of Benign and Malignant Proliferative Breast Lesions,Breast cancer is the most diagnosed cancer among women around the world. The development of computer-aided diagnosis tools is essential to help pathologists to accurately interpret and discriminate between malignant and benign tumors. This paper proposes the development of an automated proliferative breast lesion diagnosis based on machine-learning algorithms. We used Tabu search to select the most significant features. The evaluation of the feature is based on the dependency degree of each attribute in the rough set. The categorization of reduced features was built using five machine-learning algorithms. The proposed models were applied to the BIDMC-MGH and Wisconsin Diagnostic Breast Cancer datasets. The performance measures of the used models were evaluated owing to five criteria. The top performing models were AdaBoost and logistic regression. Comparisons with others works prove the efficiency of the proposed method for superior diagnosis of breast cancer against the reviewed classification techniques.,2020-02-01,1,1021,100,462
1494,32257670,Artificial Intelligence: A New Paradigm in Obstetrics and Gynecology Research and Clinical Practice,"Artificial intelligence (AI) is growing exponentially in various fields, including medicine. This paper reviews the pertinent aspects of AI in obstetrics and gynecology (OB/GYN) and how these can be applied to improve patient outcomes and reduce the healthcare costs and workload for clinicians. Herein, we will address current AI uses in OB/GYN, and the use of AI as a tool to interpret fetal heart rate (FHR) and cardiotocography (CTG) to aid in the detection of preterm labor, pregnancy complications, and review discrepancies in its interpretation between clinicians to reduce maternal and infant morbidity and mortality. AI systems can be used as tools to create algorithms identifying asymptomatic women with short cervical length who are at risk of preterm birth. Additionally, the benefits of using the vast data capacity of AI storage can assist in determining the risk factors for preterm labor using multiomics and extensive genomic data. In the field of gynecological surgery, the use of augmented reality helps surgeons detect vital structures, thus decreasing complications, reducing operative time, and helping surgeons in training to practice in a realistic setting. Using three-dimensional (3D) printers can provide materials that mimic real tissues and also helps trainees to practice on a realistic model. Furthermore, 3D imaging allows better depth perception than its two-dimensional (2D) counterpart, allowing the surgeon to create preoperative plans according to tissue depth and dimensions. Although AI has some limitations, this new technology can improve the prognosis and management of patients, reduce healthcare costs, and help OB/GYN practitioners to reduce their workload and increase their efficiency and accuracy by incorporating AI systems into their daily practice. AI has the potential to guide practitioners in decision-making, reaching a diagnosis, and improving case management. It can reduce healthcare costs by decreasing medical errors and providing more dependable predictions. AI systems can accurately provide information on the large array of patients in clinical settings, although more robust data is required.",2020-02-01,2,2158,99,462
1504,32250086,Research Progress and Prospect of Machine Learning in Bone Age Assessment,"Bone age assessment has always been one of the key issues and difficulties in forensic science. With the gradual development of machine learning in many industries, it has been widely introduced to imageology, genomics, oncology, pathology, surgery and other medical research fields in recent years. The reason why the above research fields can be closely combined with machine learning, is because the research subjects of the above branches of medicine belong to the computer vision category. Machine learning provides unique advantages for computer vision research and has made breakthroughs in medical image recognition. Based on the advantages of machine learning in image recognition, it was combined with bone age assessment research, in order to construct a recognition model suitable for forensic skeletal images. This paper reviews the research progress in bone age assessment made by scholars at home and abroad using machine learning technology in recent years.",2020-02-01,0,973,73,462
1505,32250084,New Opportunities and Challenges for Forensic Medicine in the Era of Artificial Intelligence Technology,"Traditional forensic identification relies on forensic experts to manually extract information and provide identification opinions based on medicine, biology and other fields of knowledge combined with personal work experience, which is not only time-consuming and require great effort, but also affected by subjective factors that are difficult to overcome. In the era of big data, the booming development of artificial intelligence brings new ideas to forensic medicine. In recent years, forensic researchers at home and abroad have conducted many studies based on artificial intelligence technology, such as face recognition, age and gender identification, DNA analysis, postmortem interval estimation, injury and cause of death identification, showing the feasibility and advantages of using artificial intelligence technology to solve forensic identification problems. As a new means of technology that has adapted to the development of the times, artificial intelligence has brought new vitality to forensic medicine, but at the same time also some new challenges. How to deal with these challenges scientifically and form a new mode of 'artificial intelligence plus forensic medicine' with artificial intelligence and forensic medicine developing collaboratively is a new direction for the development of forensic medicine in the era of big data.",2020-02-01,0,1353,103,462
2099,31815564,Perspectives on the current developments with neuromodulation for the treatment of epilepsy,"Introduction: As deep brain stimulation revolutionized the treatment of movement disorders in the late 80s, neuromodulation in the treatment of epilepsy will undoubtedly undergo transformative changes in the years to come with the exponential growth of technological development moving into mainstream practice; the appearance of companies such as Facebook, Google, Neuralink within the realm of brain-computer interfaces points to this trend.Areas covered: This perspective piece will talk about the history of brain stimulation in epilepsy, current-approved treatments, technical developments and the future of neurostimulation.Expert opinion: Further understanding of the brain alongside machine learning and innovative technology will be the future of neuromodulation for the treatment of epilepsy. All of these innovations and advances should pave the way toward overcoming the vexing underutilization of surgery in the therapeutic armamentarium against medically refractory seizures, given the implicit advantage of a neuromodulatory rather than neurodestructive approach.",2020-02-01,1,1078,91,462
2101,31813895,Analysis Method of the Ion Current-Time Waveform Obtained from Low Aspect Ratio Solid-state Nanopores,"Low aspect ratio nanopores are expected to be applied to the detection of viruses and bacteria because of their high spatial resolution. Multiphysics simulations have revealed that the ion current-time waveform obtained from low aspect ratio nanopores contains information on not only the volume of viruses and bacteria, but also the structure, surface charge, and flow dynamics. Analysis using machine learning extracts information about these analytes from the ion current-time waveform. The combination of low aspect ratio nanopores, multiphysics simulation, and machine learning has made it possible to distinguish different types of viruses and bacteria with high accuracy.",2020-02-01,0,678,101,462
1585,32116995,PET/MRI Radiomics in Patients With Brain Metastases,"Although a variety of imaging modalities are used or currently being investigated for patients with brain tumors including brain metastases, clinical image interpretation to date uses only a fraction of the underlying complex, high-dimensional digital information from routinely acquired imaging data. The growing availability of high-performance computing allows the extraction of quantitative imaging features from medical images that are usually beyond human perception. Using machine learning techniques and advanced statistical methods, subsets of such imaging features are used to generate mathematical models that represent characteristic signatures related to the underlying tumor biology and might be helpful for the assessment of prognosis or treatment response, or the identification of molecular markers. The identification of appropriate, characteristic image features as well as the generation of predictive or prognostic mathematical models is summarized under the term radiomics. This review summarizes the current status of radiomics in patients with brain metastases.",2020-02-01,10,1085,51,462
511,31041822,Overview of established and emerging immunohistochemical biomarkers and their role in correlative studies in MRI,"Clinical practice in radiology and pathology requires professional expertise and many years of training to visually evaluate and interpret abnormal phenotypic features in medical images and tissue sections to generate diagnoses that guide patient management and treatment. Recent advances in digital image analysis methods and machine learning have led to significant interest in extracting additional information from medical and digital whole-slide images in radiology and pathology, respectively. This has led to significant interest and research in radiomics and pathomics to correlate phenotypic features of disease with image analytics in order to identify image-based biomarkers. The expanding role of big data in radiology and pathology parallels the development and role of immunohistochemistry (IHC) in the daily practice of pathology. IHC methods were initially developed to provide additional information to help classify tumors and then transformed into an indispensable tool to guide treatment in many types of cancer. IHC markers are used in daily practice to identify specific types of cells and highlight their distributions in tissues in order to distinguish benign from neoplastic cells, determine tumor origin, subclassify neoplasms, and support and confirm diagnoses. In this regard, radiomics, pathomics, and IHC methods are very similar since they enable the extraction of image-based features to characterize various properties of diseases. Due to the dramatic advancements in recent radiomics research, we provide a brief overview of the role of established and emerging IHC biomarkers in various tumor types that have been correlated with radiologic biomarkers to improve diagnostic accuracy, predict prognosis, guide patient management, and select treatment strategies. Level of Evidence: 5 Technical Efficacy: Stage 3 J. Magn. Reson. Imaging 2020;51:341-354.",2020-02-01,1,1886,112,462
2117,31790958,"Evolution, current challenges, and future possibilities in the objective assessment of aesthetic outcome of breast cancer locoregional treatment","The Breast Cancer overall survival rate has raised impressively in the last 20 years mainly due to improved screening and effectiveness of treatments. This increase in survival paralleled the awareness over the long-lasting impact of the side effects of treatments on patient quality of life, emphasizing the motto ""a longer but better life for breast cancer patients"". In breast cancer more strikingly than in other cancers, besides the side effects of systemic treatments, there is the visible impact of surgery and radiotherapy on patients' body image. This has sparked interest on the development of tools for the aesthetic evaluation of Breast Cancer locoregional treatments, which evolved from manual, subjective approaches to computerized, automated solutions. However, although studied for almost four decades, past solutions were not mature enough to become a standard. Recent advancements in machine learning have inspired trends toward deep-learning-based medical image analysis, also bringing new promises to the field of aesthetic assessment of locoregional treatments. In this paper, a review and discussion of the previous state-of-the-art methods in the field is conducted and the extracted knowledge is used to understand the evolution and current challenges. The aim of this paper is to delve into the current opportunities as well as motivate and guide future research in the aesthetic assessment of Breast Cancer locoregional treatments.",2020-02-01,3,1457,144,462
2118,31789703,Retinal vessel changes in cerebrovascular disease,"Purpose of review:                    The retina is growingly recognized as a window into cerebrovascular and systemic vascular conditions. The utility of noninvasive retinal vessel biomarkers in cerebrovascular risk assessment has expanded due to advances in retinal imaging techniques and machine learning-based digital analysis. The purpose of this review is to underscore the latest evidence linking retinal vascular abnormalities with stroke and vascular-related cognitive disorders; to highlight modern developments in retinal vascular imaging modalities and software-based vasculopathy quantification.              Recent findings:                    Longitudinal studies undertaken for extended periods indicate that retinal vascular changes can predict cerebrovascular disorders (CVD). Cerebrovascular ties to dementia provoked recent explorations of retinal vessel imaging tools for conceivable early cognitive decline detection. Innovative biomedical engineering technologies and advanced dynamic and functional retinal vascular imaging methods have recently been added to the armamentarium, allowing an unbiased and comprehensive analysis of the retinal vasculature. Improved artificial intelligence-based deep learning algorithms have boosted the application of retinal imaging as a clinical and research tool to screen, risk stratify, and monitor with precision CVD and vascular cognitive impairment.              Summary:                    Mounting evidence supports the use of quantitative retinal vessel analysis in predicting CVD, from clinical stroke to neuroimaging markers of stroke and neurodegeneration.",2020-02-01,2,1627,49,462
1301,32002838,Applications of Computer Modeling and Simulation in Cartilage Tissue Engineering,"Background:                    Advances in cartilage tissue engineering have demonstrated noteworthy potential for developing cartilage for implantation onto sites impacted by joint degeneration and injury. To supplement resource-intensive in vivo and in vitro studies required for cartilage tissue engineering, computational models and simulations can assist in enhancing experimental design.              Methods:                    Research articles pertinent to cartilage tissue engineering and computer modeling were identified, reviewed, and summarized. Various applications of computer modeling for cartilage tissue engineering are highlighted, limitations of in silico modeling are addressed, and suggestions for future work are enumerated.              Results:                    Computational modeling can help better characterize shear stresses generated by bioreactor fluid flow, refine scaffold geometry, customize the mechanical properties of engineered cartilage tissue, and model rates of cell growth and dynamics. Thus, results from in silico studies can help resourcefully enhance in vitro and in vivo studies; however, the limitations of these studies, such as the underlying assumptions and simplifications applied in each model, should always be addressed and justified where applicable. In silico models should also seek validation and verification when possible.              Conclusion:                    Future studies may adopt similar approaches to supplement in vitro trials and further investigate effects of mechanical stimulation on chondrocyte and stem cell dynamics. Additionally, as precision medicine, machine learning, and powerful open-source software become more popular and accessible, applications of multi-scale and multiphysics computational models in cartilage tissue engineering are expected to increase.",2020-02-01,2,1850,80,462
2123,31786416,Machine learning with multiparametric magnetic resonance imaging of the breast for early prediction of response to neoadjuvant chemotherapy,"In patients with locally advanced breast cancer undergoing neoadjuvant chemotherapy (NAC), some patients achieve a complete pathologic response (pCR), some achieve a partial response, and some do not respond at all or even progress. Accurate prediction of treatment response has the potential to improve patient care by improving prognostication, enabling de-escalation of toxic treatment that has little benefit, facilitating upfront use of novel targeted therapies, and avoiding delays to surgery. Visual inspection of a patient's tumor on multiparametric MRI is insufficient to predict that patient's response to NAC. However, machine learning and deep learning approaches using a mix of qualitative and quantitative MRI features have recently been applied to predict treatment response early in the course of or even before the start of NAC. This is a novel field but the data published so far has shown promising results. We provide an overview of the machine learning and deep learning models developed to date, as well as discuss some of the challenges to clinical implementation.",2020-02-01,3,1087,139,462
1298,32007208,MRI-guided adaptive radiotherapy for liver tumours: visualising the future,"MRI-guided radiotherapy is a novel and rapidly evolving technology that might enhance the risk-benefit ratio. Through direct visualisation of the tumour and the nearby healthy tissues, the radiation oncologist can deliver highly accurate treatment even to mobile targets. Each individual treatment can be customised to changing anatomy, potentially reducing the risk of radiation-related toxicities while simultaneously increasing the dose delivered to the tumour. MRI-guided radiotherapy offers a new tool for the radiation oncologist, and creates an opportunity to achieve durable local control of liver tumours that might not otherwise be possible. Future work will allow us to expand the population eligible for curative-intent radiotherapy, optimise and customise radiation doses to specific tumours, and hopefully create opportunities for improving outcomes through machine learning and radiomics-based approaches. This Review outlines the current and future applications for MRI-guided radiotherapy with respect to metastatic and primary liver cancers.",2020-02-01,5,1059,74,462
1549,32175423,Radiographic assessment of the cup orientation after total hip arthroplasty: a literature review,"Optimal acetabular cup orientation is of substantial importance to good long-term function and low complication rates after total hip arthroplasty (THA). The radiographic anteversion (RA) and inclination (RI) angles of the cup are typically studied due to the practicability, simplicity, and ease of interpretation of their measurements. A great number of methods have been developed to date, most of which have been performed on pelvic or hip anteroposterior radiographs. However, there are primarily two influencing factors for these methods: X-ray offset and pelvic rotation. In addition, there are three types of pelvic rotations about the transverse, longitudinal, and anteroposterior axes of the body. Their effects on the RA and RI angles of the cup are interactively correlated with the position and true orientation of the cup. To date, various fitted or analytical models have been established to disclose the correlations between the X-ray offset and pelvic rotation and the RA and RI angles of the cup. Most of these models do not incorporate all the potential influencing parameters. Advanced methods for performing X-ray offset and pelvic rotation corrections are mainly performed on a single pelvic AP radiograph, two synchronized radiographs, or a two-dimensional/three-dimensional (2D-3D) registration system. Some measurement systems, originally developed for evaluating implant migration or wear, could also be used for correcting the X-ray offset and pelvic rotation simultaneously, but some drawbacks still exist with these systems. Above all, the 2D-3D registration technique might be an alternative and powerful tool for accurately measuring cup orientation. In addition to the current methods used for postoperative assessment, navigation systems and augmented reality are also used for the preoperative planning and intraoperative guidance of cup placement. With the continuing development of artificial intelligence and machine learning, these techniques could be incorporated into robot-assisted orthopaedic surgery in the future.",2020-02-01,2,2057,96,462
1233,32093027,Application of Artificial Intelligence Techniques to Predict Survival in Kidney Transplantation: A Review,"A key issue in the field of kidney transplants is the analysis of transplant recipients' survival. By means of the information obtained from transplant patients, it is possible to analyse in which cases a transplant has a higher likelihood of success and the factors on which it will depend. In general, these analyses have been conducted by applying traditional statistical techniques, as the amount and variety of data available about kidney transplant processes were limited. However, two main changes have taken place in this field in the last decade. Firstly, the digitalisation of medical information through the use of electronic health records (EHRs), which store patients' medical histories electronically. This facilitates automatic information processing through specialised software. Secondly, medical Big Data has provided access to vast amounts of data on medical processes. The information currently available on kidney transplants is huge and varied by comparison to that initially available for this kind of study. This new context has led to the use of other non-traditional techniques more suitable to conduct survival analyses in these new conditions. Specifically, this paper provides a review of the main machine learning methods and tools that are being used to conduct kidney transplant patient and graft survival analyses.",2020-02-01,3,1347,105,462
952,31935669,Artificial intelligence in digital breast pathology: Techniques and applications,"Breast cancer is the most common cancer and second leading cause of cancer-related death worldwide. The mainstay of breast cancer workup is histopathological diagnosis - which guides therapy and prognosis. However, emerging knowledge about the complex nature of cancer and the availability of tailored therapies have exposed opportunities for improvements in diagnostic precision. In parallel, advances in artificial intelligence (AI) along with the growing digitization of pathology slides for the primary diagnosis are a promising approach to meet the demand for more accurate detection, classification and prediction of behaviour of breast tumours. In this article, we cover the current and prospective uses of AI in digital pathology for breast cancer, review the basics of digital pathology and AI, and outline outstanding challenges in the field.",2020-02-01,6,852,80,462
1011,31839552,Voice patterns in schizophrenia: A systematic review and Bayesian meta-analysis,"Voice atypicalities have been a characteristic feature of schizophrenia since its first definitions. They are often associated with core negative symptoms such as flat affect and alogia, and with the social impairments seen in the disorder. This suggests that voice atypicalities may represent a marker of clinical features and social functioning in schizophrenia. We systematically reviewed and meta-analyzed the evidence for distinctive acoustic patterns in schizophrenia, as well as their relation to clinical features. We identified 46 articles, including 55 studies with a total of 1254 patients with schizophrenia and 699 healthy controls. Summary effect sizes (Hedges'g and Pearson's r) estimates were calculated using multilevel Bayesian modeling. We identified weak atypicalities in pitch variability (g = -0.55) related to flat affect, and stronger atypicalities in proportion of spoken time, speech rate, and pauses (g's between -0.75 and -1.89) related to alogia and flat affect. However, the effects were mostly modest (with the important exception of pause duration) compared to perceptual and clinical judgments, and characterized by large heterogeneity between studies. Moderator analyses revealed that tasks with a more demanding cognitive and social component showed larger effects both in contrasting patients and controls and in assessing symptomatology. In conclusion, studies of acoustic patterns are a promising but, yet unsystematic avenue for establishing markers of schizophrenia. We outline recommendations towards more cumulative, open, and theory-driven research.",2020-02-01,6,1592,79,462
970,31902468,Computational prediction of cytochrome P450 inhibition and induction,"Cytochrome P450 (CYP) enzymes play an important role in the phase I metabolism of many xenobiotics. Most drug-drug interactions (DDIs) associated with CYP are caused by either CYP inhibition or induction. The early detection of potential DDIs is highly desirable in the pharmaceutical industry because DDIs can cause serious adverse events, which can lead to poor patient health and drug development failures. Recently, many computational studies predicting CYP inhibition and induction have been reported. The current computational modeling approaches for CYP metabolism are classified as ligand- and structure-based; various techniques, such as quantitative structure-activity relationships, machine learning, docking, and molecular dynamic simulation, are involved in both the approaches. Recently, combining these two approaches have resulted in improvements in the prediction accuracy of DDIs. In this review, we present important, recent developments in the computational prediction of the inhibition of four clinically crucial CYP isoforms (CYP1A2, 2C9, 2D6, and 3A4) and three nuclear receptors (aryl hydrocarbon receptor, constitutive androstane receptor, and pregnane X receptor) involved in the induction of CYP1A2, 2B6, and 3A4, respectively.",2020-02-01,3,1254,68,462
974,31893575,FOLFOX treatment response prediction in metastatic or recurrent colorectal cancer patients via machine learning algorithms,"Early identification of metastatic or recurrent colorectal cancer (CRC) patients who will be sensitive to FOLFOX (5-FU, leucovorin and oxaliplatin) therapy is very important. We performed microarray meta-analysis to identify differentially expressed genes (DEGs) between FOLFOX responders and nonresponders in metastatic or recurrent CRC patients, and found that the expression levels of WASHC4, HELZ, ERN1, RPS6KB1, and APPBP2 were downregulated, while the expression levels of IRF7, EML3, LYPLA2, DRAP1, RNH1, PKP3, TSPAN17, LSS, MLKL, PPP1R7, GCDH, C19ORF24, and CCDC124 were upregulated in FOLFOX responders compared with nonresponders. Subsequent functional annotation showed that DEGs were significantly enriched in autophagy, ErbB signaling pathway, mitophagy, endocytosis, FoxO signaling pathway, apoptosis, and antifolate resistance pathways. Based on those candidate genes, several machine learning algorithms were applied to the training set, then performances of models were assessed via the cross validation method. Candidate models with the best tuning parameters were applied to the test set and the final model showed satisfactory performance. In addition, we also reported that MLKL and CCDC124 gene expression were independent prognostic factors for metastatic CRC patients undergoing FOLFOX therapy.",2020-02-01,1,1318,122,462
1743,31584645,Mining social media for prescription medication abuse monitoring: a review and proposal for a data-centric framework,"Objective:                    Prescription medication (PM) misuse and abuse is a major health problem globally, and a number of recent studies have focused on exploring social media as a resource for monitoring nonmedical PM use. Our objectives are to present a methodological review of social media-based PM abuse or misuse monitoring studies, and to propose a potential generalizable, data-centric processing pipeline for the curation of data from this resource.              Materials and methods:                    We identified studies involving social media, PMs, and misuse or abuse (inclusion criteria) from Medline, Embase, Scopus, Web of Science, and Google Scholar. We categorized studies based on multiple characteristics including but not limited to data size; social media source(s); medications studied; and primary objectives, methods, and findings.              Results:                    A total of 39 studies met our inclusion criteria, with 31 (79.5%) published since 2015. Twitter has been the most popular resource, with Reddit and Instagram gaining popularity recently. Early studies focused mostly on manual, qualitative analyses, with a growing trend toward the use of data-centric methods involving natural language processing and machine learning.              Discussion:                    There is a paucity of standardized, data-centric frameworks for curating social media data for task-specific analyses and near real-time surveillance of nonmedical PM use. Many existing studies do not quantify human agreements for manual annotation tasks or take into account the presence of noise in data.              Conclusion:                    The development of reproducible and standardized data-centric frameworks that build on the current state-of-the-art methods in data and text mining may enable effective utilization of social media data for understanding and monitoring nonmedical PM use.",2020-02-01,5,1926,116,462
1863,32704411,Artificial Intelligence in Retinopathy of Prematurity Diagnosis,"Retinopathy of prematurity (ROP) is a leading cause of childhood blindness worldwide. The diagnosis of ROP is subclassified by zone, stage, and plus disease, with each area demonstrating significant intra- and interexpert subjectivity and disagreement. In addition to improved efficiencies for ROP screening, artificial intelligence may lead to automated, quantifiable, and objective diagnosis in ROP. This review focuses on the development of artificial intelligence for automated diagnosis of plus disease in ROP and highlights the clinical and technical challenges of both the development and implementation of artificial intelligence in the real world.",2020-02-01,4,656,63,462
948,31939856,"Artificial Intelligence in Anesthesiology: Current Techniques, Clinical Applications, and Limitations","Artificial intelligence has been advancing in fields including anesthesiology. This scoping review of the intersection of artificial intelligence and anesthesia research identified and summarized six themes of applications of artificial intelligence in anesthesiology: (1) depth of anesthesia monitoring, (2) control of anesthesia, (3) event and risk prediction, (4) ultrasound guidance, (5) pain management, and (6) operating room logistics. Based on papers identified in the review, several topics within artificial intelligence were described and summarized: (1) machine learning (including supervised, unsupervised, and reinforcement learning), (2) techniques in artificial intelligence (e.g., classical machine learning, neural networks and deep learning, Bayesian methods), and (3) major applied fields in artificial intelligence.The implications of artificial intelligence for the practicing anesthesiologist are discussed as are its limitations and the role of clinicians in further developing artificial intelligence for use in clinical care. Artificial intelligence has the potential to impact the practice of anesthesiology in aspects ranging from perioperative support to critical care delivery to outpatient pain management.",2020-02-01,7,1237,101,462
1087,31431299,Designing Eukaryotic Gene Expression Regulation Using Machine Learning,"Controlling the expression of genes is one of the key challenges of synthetic biology. Until recently fine-tuned control has been out of reach, particularly in eukaryotes owing to their complexity of gene regulation. With advances in machine learning (ML) and in particular with increasing dataset sizes, models predicting gene expression levels from regulatory sequences can now be successfully constructed. Such models form the cornerstone of algorithms that allow users to design regulatory regions to achieve a specific gene expression level. In this review we discuss strategies for data collection, data encoding, ML practices, design algorithm choices, and finally model interpretation. Ultimately, these developments will provide synthetic biologists with highly specific genetic building blocks to rationally engineer complex pathways and circuits.",2020-02-01,2,857,70,462
986,31883846,Artificial intelligence approaches using natural language processing to advance EHR-based clinical research,"The wide adoption of electronic health record systems in health care generates big real-world data that open new venues to conduct clinical research. As a large amount of valuable clinical information is locked in clinical narratives, natural language processing techniques as an artificial intelligence approach have been leveraged to extract information from clinical narratives in electronic health records. This capability of natural language processing potentially enables automated chart review for identifying patients with distinctive clinical characteristics in clinical care and reduces methodological heterogeneity in defining phenotype, obscuring biological heterogeneity in research concerning allergy, asthma, and immunology. This brief review discusses the current literature on the secondary use of electronic health record data for clinical research concerning allergy, asthma, and immunology and highlights the potential, challenges, and implications of natural language processing techniques.",2020-02-01,5,1011,107,462
1017,31830558,Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges,"Cancer is an aggressive disease with a low median survival rate. Ironically, the treatment process is long and very costly due to its high recurrence and mortality rates. Accurate early diagnosis and prognosis prediction of cancer are essential to enhance the patient's survival rate. Developments in statistics and computer engineering over the years have encouraged many scientists to apply computational methods such as multivariate statistical analysis to analyze the prognosis of the disease, and the accuracy of such analyses is significantly higher than that of empirical predictions. Furthermore, as artificial intelligence (AI), especially machine learning and deep learning, has found popular applications in clinical cancer research in recent years, cancer prediction performance has reached new heights. This article reviews the literature on the application of AI to cancer diagnosis and prognosis, and summarizes its advantages. We explore how AI assists cancer diagnosis and prognosis, specifically with regard to its unprecedented accuracy, which is even higher than that of general statistical applications in oncology. We also demonstrate ways in which these methods are advancing the field. Finally, opportunities and challenges in the clinical implementation of AI are discussed. Hence, this article provides a new perspective on how AI technology can help improve cancer diagnosis and prognosis, and continue improving human health in the future.",2020-02-01,11,1467,87,462
940,31955441,Invited Review: DNA methylation-based classification of paediatric brain tumours,"DNA methylation-based machine learning algorithms represent powerful diagnostic tools that are currently emerging for several fields of tumour classification. For various reasons, paediatric brain tumours have been the main driving forces behind this rapid development and brain tumour classification tools are likely further advanced than in any other field of cancer diagnostics. In this review, we will discuss the main characteristics that were important for this rapid advance, namely the high clinical need for improvement of paediatric brain tumour diagnostics, the robustness of methylated DNA and the consequential possibility to generate high-quality molecular data from archival formalin-fixed paraffin-embedded pathology specimens, the implementation of a single array platform by most laboratories allowing data exchange and data pooling to an unprecedented extent, as well as the high suitability of the data format for machine learning. We will further discuss the four most central output qualities of DNA methylation profiling in a diagnostic setting (tumour classification, tumour sub-classification, copy number analysis and guidance for additional molecular testing) individually for the most frequent types of paediatric brain tumours. Lastly, we will discuss DNA methylation profiling as a tool for the detection of new paediatric brain tumour classes and will give an overview of the rapidly growing family of new tumours identified with the aid of this technique.",2020-02-01,4,1487,80,462
1013,31833725,Artificial intelligence and neural networks in urology: current clinical applications,"Introduction:                    As we enter the era of ""big data,"" an increasing amount of complex health-care data will become available. These data are often redundant, ""noisy,"" and characterized by wide variability. In order to offer a precise and transversal view of a clinical scenario the artificial intelligence (AI) with machine learning (ML) algorithms and Artificial neuron networks (ANNs) process were adopted, with a promising wide diffusion in the near future. The present work aims to provide a comprehensive and critical overview of the current and potential applications of AI and ANNs in urology.              Evidence acquisition:                    A non-systematic review of the literature was performed by screening Medline, PubMed, the Cochrane Database, and Embase to detect pertinent studies regarding the application of AI and ANN in Urology.              Evidence synthesis:                    The main application of AI in urology is the field of genitourinary cancers. Focusing on prostate cancer, AI was applied for the prediction of prostate biopsy results. For bladder cancer, the prediction of recurrence-free probability and diagnostic evaluation were analysed with ML algorithms. For kidney and testis cancer, anecdotal experiences were reported for staging and prediction of diseases recurrence. More recently, AI has been applied in non-oncological diseases like stones and functional urology.              Conclusions:                    AI technologies are growing their role in health care; but, up to now, their ""real-life"" implementation remains limited. However, in the near future, the potential of AI-driven era could change the clinical practice in Urology, improving overall patient outcomes.",2020-02-01,9,1739,85,462
1012,31838165,Autoantigenomics: Holistic characterization of autoantigen repertoires for a better understanding of autoimmune diseases,"Autoimmune diseases are mostly characterized by autoantibodies in the patients' serum or cerebrospinal fluid, representing diagnostic or prognostic biomarkers. For decades, research has focused on single autoantigens or panels of single autoantigens. In this article, we advocate to broaden the focus by addressing the entire autoantigen repertoire in a systemic ""omics-like"" way. This approach aims to capture the enormous biodiversity in the sets of targeted antigens and pave the way toward a more holistic understanding of the concerted character of antibody-related humoral immune responses. Ongoing technological progress permits high-throughput screenings of thousands of autoantigens in parallel, e.g., via protein microarrays, phage display, or immunoprecipitation with mass spectrometry. We argue that the time is right for combining omics and autoantibody screening approaches into ""autoantigenomics"" as a novel omics subcategory. In this article, we introduce the concept of autoantigenomics, describe its roots and application options, and demarcate the method from related holistic approaches such as systems serology or immune-related transcriptomics and proteomics. We suggest the following extendable method set to be applied to autoantigen repertoires: (1) principal component analysis, (2) hierarchical cluster analysis, (3) partial least-square discriminant analysis or orthogonal projections to latent structures discriminant analysis, (4) analysis of the repertoire sizes in disease groups and clinical subgroups, (5) overrepresentation analyses using databases like those of Gene Ontology, Reactome Pathway, or DisGeNET, (6) analysis of pathways that are significantly targeted by specific repertoires, and (7) machine learning approaches. In an unsupervised way, these methods can identify clusters of autoantigens sharing certain functional or spatial properties, or clusters of patients comprising clinical subgroups potentially useful for patient stratification. In a supervised way, these methods can lead to prediction models that may eventually assist diagnosis and prognosis. The untargeted autoantigenomics approach allows for the systematic survey of antibody-related humoral immune responses. This may enhance our understanding of autoimmune diseases in a more comprehensive way compared to current single or panel autoantibodies approaches.",2020-02-01,4,2375,120,462
1009,31841881,Artificial intelligence applications for thoracic imaging,"Artificial intelligence is a hot topic in medical imaging. The development of deep learning methods and in particular the use of convolutional neural networks (CNNs), have led to substantial performance gain over the classic machine learning techniques. Multiple usages are currently being evaluated, especially for thoracic imaging, such as such as lung nodule evaluation, tuberculosis or pneumonia detection or quantification of diffuse lung diseases. Chest radiography is a near perfect domain for the development of deep learning algorithms for automatic interpretation, requiring large annotated datasets, in view of the high number of procedures and increasing data availability. Current algorithms are able to detect up to 14 common anomalies, when present as isolated findings. Chest computed tomography is another major field of application for artificial intelligence, especially in the perspective of large scale lung cancer screening. It is important for radiologists to apprehend, contribute actively and lead this new era of radiology powered by artificial intelligence. Such a perspective requires understanding new terms and concepts associated with machine learning. The objective of this paper is to provide useful definitions for understanding the methods used and their possibilities, and report current and future developments for thoracic imaging. Prospective validation of AI tools will be required before reaching routine clinical implementation.",2020-02-01,12,1470,57,462
981,31886649,"Quantitative Structure-Selectivity Relationships in Enantioselective Catalysis: Past, Present, and Future","The dawn of the 21st century has brought with it a surge of research related to computer-guided approaches to catalyst design. In the past two decades, chemoinformatics, the application of informatics to solve problems in chemistry, has increasingly influenced prediction of activity and mechanistic investigations of organic reactions. The advent of advanced statistical and machine learning methods, as well as dramatic increases in computational speed and memory, has contributed to this emerging field of study. This review summarizes strategies to employ quantitative structure-selectivity relationships (QSSR) in asymmetric catalytic reactions. The coverage is structured by initially introducing the basic features of these methods. Subsequent topics are discussed according to increasing complexity of molecular representations. As the most applied subfield of QSSR in enantioselective catalysis, the application of local parametrization approaches and linear free energy relationships (LFERs) along with multivariate modeling techniques is described first. This section is followed by a description of global parametrization methods, the first of which is continuous chirality measures (CCM) because it is a single parameter derived from the global structure of a molecule. Chirality codes, global, multivariate descriptors, are then introduced followed by molecular interaction fields (MIFs), a global descriptor class that typically has the highest dimensionality. To highlight the current reach of QSSR in enantioselective transformations, a comprehensive collection of examples is presented. When combined with traditional experimental approaches, chemoinformatics holds great promise to predict new catalyst structures, rationalize mechanistic behavior, and profoundly change the way chemists discover and optimize reactions.",2020-02-01,5,1839,105,462
1862,32704419,Applications of Artificial Intelligence to Electronic Health Record Data in Ophthalmology,"Widespread adoption of electronic health records (EHRs) has resulted in the collection of massive amounts of clinical data. In ophthalmology in particular, the volume range of data captured in EHR systems has been growing rapidly. Yet making effective secondary use of this EHR data for improving patient care and facilitating clinical decision-making has remained challenging due to the complexity and heterogeneity of these data. Artificial intelligence (AI) techniques present a promising way to analyze these multimodal data sets. While AI techniques have been extensively applied to imaging data, there are a limited number of studies employing AI techniques with clinical data from the EHR. The objective of this review is to provide an overview of different AI methods applied to EHR data in the field of ophthalmology. This literature review highlights that the secondary use of EHR data has focused on glaucoma, diabetic retinopathy, age-related macular degeneration, and cataracts with the use of AI techniques. These techniques have been used to improve ocular disease diagnosis, risk assessment, and progression prediction. Techniques such as supervised machine learning, deep learning, and natural language processing were most commonly used in the articles reviewed.",2020-02-01,2,1280,89,462
989,31881449,Machine learning for protein folding and dynamics,"Many aspects of the study of protein folding and dynamics have been affected by the recent advances in machine learning. Methods for the prediction of protein structures from their sequences are now heavily based on machine learning tools. The way simulations are performed to explore the energy landscape of protein systems is also changing as force-fields are started to be designed by means of machine learning methods. These methods are also used to extract the essential information from large simulation datasets and to enhance the sampling of rare events such as folding/unfolding transitions. While significant challenges still need to be tackled, we expect these methods to play an important role on the study of protein folding and dynamics in the near future. We discuss here the recent advances on all these fronts and the questions that need to be addressed for machine learning approaches to become mainstream in protein simulation.",2020-02-01,4,946,49,462
1010,31840577,Studying language in context using the temporal generalization method,"The temporal generalization method (TGM) is a data analysis technique that can be used to test if the brain's representation for particular stimuli (e.g. sounds, images) is maintained, or if it changes as a function of time (King J-R, Dehaene S. 2014 Characterizing the dynamics of mental representations: the temporal generalization method. Trends Cogn. Sci. 18, 203-210. (doi:10.1016/j.tics.2014.01.002)). The TGM involves training models to predict the stimuli or condition using a time window from a recording of brain activity, and testing the resulting models at all possible time windows. This is repeated for all possible training windows to create a full matrix of accuracy for every combination of train/test window. The results of a TGM indicate when brain activity patterns are consistent (i.e. the trained model performs well even when tested on a different time window), and when they are inconsistent, allowing us to track neural representations over time. The TGM has been used to study the representation of images and sounds during a variety of tasks, but has been less readily applied to studies of language. Here, we give an overview of the method itself, discuss how the TGM has been used to analyse two studies of language in context and explore how the TGM could be applied to further our understanding of semantic composition. This article is part of the theme issue 'Towards mechanistic models of meaning composition'.",2020-02-01,2,1443,69,462
1692,31651163,Ab Initio Simulations and Materials Chemistry in the Age of Big Data,"In this perspective, we discuss computational advances in the last decades, both in algorithms as well as in technologies, that enabled the development, widespread use, and maturity of simulation methods for molecular and materials systems. Such advances led to the generation of large amounts of data, which required the creation of several computational databases. Within this scenario, with the democratization of data access, the field now encounters several opportunities for data-driven approaches toward chemical and materials problems. Specifically, machine learning methods for predictions of novel materials or properties are being increasingly used with great success. However, black box usage fails in many instances; several technical details require expert knowledge in order for the predictions to be useful, such as with descriptors and algorithm selection. These approaches represent a direction for further developments, notably allowing advances for both developed and emerging countries with modest computational infrastructures.",2020-02-01,1,1049,68,462
1241,32085599,A Review of Data Analytic Applications in Road Traffic Safety. Part 1: Descriptive and Predictive Modeling,"This part of the review aims to reduce the start-up burden of data collection and descriptive analytics for statistical modeling and route optimization of risk associated with motor vehicles. From a data-driven bibliometric analysis, we show that the literature is divided into two disparate research streams: (a) predictive or explanatory models that attempt to understand and quantify crash risk based on different driving conditions, and (b) optimization techniques that focus on minimizing crash risk through route/path-selection and rest-break scheduling. Translation of research outcomes between these two streams is limited. To overcome this issue, we present publicly available high-quality data sources (different study designs, outcome variables, and predictor variables) and descriptive analytic techniques (data summarization, visualization, and dimension reduction) that can be used to achieve safer-routing and provide code to facilitate data collection/exploration by practitioners/researchers. Then, we review the statistical and machine learning models used for crash risk modeling. We show that (near) real-time crash risk is rarely considered, which might explain why the optimization models (reviewed in Part 2) have not capitalized on the research outcomes from the first stream.",2020-02-01,4,1300,106,462
957,31926317,Key indicators of phase transition for clinical trials through machine learning,"A significant number of drugs fail during the clinical testing stage. To understand the attrition of drugs through the regulatory process, here we review and advance machine-learning (ML) and natural language-processing algorithms to investigate the importance of factors in clinical trials that are linked with failure in Phases II and III. We find that clinical trial phase transitions can be predicted with an average accuracy of 80%. Identifying these trials provides information to sponsors facing difficult decisions about whether these higher risk trials should be modified or halted. We also find common protocol characteristics across therapeutic areas that are linked to phase success, including the number of endpoints and the complexity of the eligibility criteria.",2020-02-01,0,777,79,462
961,31922162,Recent advances in fast-scan cyclic voltammetry,"Fast-scan cyclic voltammetry (FSCV) at carbon-fiber microelectrodes (CFMEs) is a versatile electrochemical technique to probe neurochemical dynamics in vivo. Progress in FSCV methodology continues to address analytical challenges arising from biological needs to measure low concentrations of neurotransmitters at specific sites. This review summarizes recent advances in FSCV method development in three areas: (1) waveform optimization, (2) electrode development, and (3) data analysis. First, FSCV waveform parameters such as holding potential, switching potential, and scan rate have been optimized to monitor new neurochemicals. The new waveform shapes introduce better selectivity toward specific molecules such as serotonin, histamine, hydrogen peroxide, octopamine, adenosine, guanosine, and neuropeptides. Second, CFMEs have been modified with nanomaterials such as carbon nanotubes or replaced with conducting polymers to enhance sensitivity, selectivity, and antifouling properties. Different geometries can be obtained by 3D-printing, manufacturing arrays, or fabricating carbon nanopipettes. Third, data analysis is important to sort through the thousands of CVs obtained. Recent developments in data analysis include preprocessing by digital filtering, principal components analysis for distinguishing analytes, and developing automated algorithms to detect peaks. Future challenges include multisite measurements, machine learning, and integration with other techniques. Advances in FSCV will accelerate research in neurochemistry to answer new biological questions about dynamics of signaling in the brain.",2020-02-01,10,1622,47,462
1693,31648775,A review of spatial approaches in road safety,"Spatial analyses of crashes have been adopted in road safety for decades in order to determine how crashes are affected by neighboring locations, how the influence of parameters varies spatially and which locations warrant interventions more urgently. The aim of the present research is to critically review the existing literature on different spatial approaches through which researchers handle the dimension of space in its various aspects in their studies and analyses. Specifically, the use of different areal unit levels in spatial road safety studies is investigated, different modelling approaches are discussed, and the corresponding study design characteristics are summarized in respective tables including traffic, road environment and area parameters and spatial aggregation approaches. Developments in famous issues in spatial analysis such as the boundary problem, the modifiable areal unit problem and spatial proximity structures are also discussed. Studies focusing on spatially analyzing vulnerable road users are reviewed as well. Regarding spatial models, the application, advantages and disadvantages of various functional/econometric approaches, Bayesian models and machine learning methods are discussed. Based on the reviewed studies, present challenges and future research directions are determined.",2020-02-01,3,1325,45,462
1861,32704420,"Introduction to Machine Learning, Neural Networks, and Deep Learning","Purpose:                    To present an overview of current machine learning methods and their use in medical research, focusing on select machine learning techniques, best practices, and deep learning.              Methods:                    A systematic literature search in PubMed was performed for articles pertinent to the topic of artificial intelligence methods used in medicine with an emphasis on ophthalmology.              Results:                    A review of machine learning and deep learning methodology for the audience without an extensive technical computer programming background.              Conclusions:                    Artificial intelligence has a promising future in medicine; however, many challenges remain.              Translational relevance:                    The aim of this review article is to provide the nontechnical readers a layman's explanation of the machine learning methods being used in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine.",2020-02-01,5,1100,68,462
1248,32065835,"Reading patterns of proteome damage by glycation, oxidation and nitration: quantitation by stable isotopic dilution analysis LC-MS/MS","Liquid chromatography-tandem mass spectrometry (LC-MS/MS) provides a high sensitivity, high specificity multiplexed method for concurrent detection of adducts formed by protein glycation, oxidation and nitration, also called AGEomics. Combined with stable isotopic dilution analysis, it provides for robust quantitation of protein glycation, oxidation and nitration adduct analytes. It is the reference method for such measurements. LC-MS/MS has been used to measure glycated, oxidized and nitrated amino acids - also called glycation, oxidation and nitration free adducts, with a concurrent quantitation of the amino acid metabolome in physiological fluids. Similar adduct residues in proteins may be quantitated with prior exhaustive enzymatic hydrolysis. It has also been applied to quantitation of other post-translation modifications, such as citrullination and formation of N-(-glutamyl)lysine crosslink by transglutaminases. Application to cellular and extracellular proteins gives estimates of the steady-state levels of protein modification by glycation, oxidation and nitration, and measurement of the accumulation of glycation, oxidation and nitration adducts in cell culture medium and urinary excretion gives an indication of flux of adduct formation. Measurement of glycation, oxidation and nitration free adducts in plasma and urine provides for estimates of renal clearance of free adducts. Diagnostic potential in clinical studies has been enhanced by the combination of estimates of multiple adducts in optimized diagnostic algorithms by machine learning. Recent applications have been in early-stage detection of metabolic, vascular and renal disease, and arthritis, metabolic control and risk of developing vascular complication in diabetes, and a blood test for autism.",2020-02-01,2,1792,133,462
1026,31521533,The genetic architecture of Parkinson's disease,"Parkinson's disease is a complex neurodegenerative disorder for which both rare and common genetic variants contribute to disease risk, onset, and progression. Mutations in more than 20 genes have been associated with the disease, most of which are highly penetrant and often cause early onset or atypical symptoms. Although our understanding of the genetic basis of Parkinson's disease has advanced considerably, much remains to be done. Further disease-related common genetic variability remains to be identified and the work in identifying rare risk alleles has only just begun. To date, genome-wide association studies have identified 90 independent risk-associated variants. However, most of them have been identified in patients of European ancestry and we know relatively little of the genetics of Parkinson's disease in other populations. We have a limited understanding of the biological functions of the risk alleles that have been identified, although Parkinson's disease risk variants appear to be in close proximity to known Parkinson's disease genes and lysosomal-related genes. In the past decade, multiple efforts have been made to investigate the genetic architecture of Parkinson's disease, and emerging technologies, such as machine learning, single-cell RNA sequencing, and high-throughput screens, will improve our understanding of genetic risk.",2020-02-01,49,1366,47,462
1262,32054042,Deep Learning in Physiological Signal Data: A Survey,"Deep Learning (DL), a successful promising approach for discriminative and generative tasks, has recently proved its high potential in 2D medical imaging analysis; however, physiological data in the form of 1D signals have yet to be beneficially exploited from this novel approach to fulfil the desired medical tasks. Therefore, in this paper we survey the latest scientific research on deep learning in physiological signal data such as electromyogram (EMG), electrocardiogram (ECG), electroencephalogram (EEG), and electrooculogram (EOG). We found 147 papers published between January 2018 and October 2019 inclusive from various journals and publishers. The objective of this paper is to conduct a detailed study to comprehend, categorize, and compare the key parameters of the deep-learning approaches that have been used in physiological signal analysis for various medical applications. The key parameters of deep-learning approach that we review are the input data type, deep-learning task, deep-learning model, training architecture, and dataset sources. Those are the main key parameters that affect system performance. We taxonomize the research works using deep-learning method in physiological signal analysis based on: (1) physiological signal data perspective, such as data modality and medical application; and (2) deep-learning concept perspective such as training architecture and dataset sources.",2020-02-01,7,1414,52,462
1247,32066289,Machine learning for predicting cardiac events: what does the future hold?,"Introduction: With the increase in the number of patients with cardiovascular diseases, better risk-prediction models for cardiovascular events are needed. Statistical-based risk-prediction models for cardiovascular events (CVEs) are available, but they lack the ability to predict individual-level risk. Machine learning (ML) methods are especially equipped to handle complex data and provide accurate risk-prediction models at the individual level.Areas covered: In this review, the authors summarize the literature comparing the performance of machine learning methods to that of traditional, statistical-based models in predicting CVEs. They provide a brief summary of ML methods and then discuss risk-prediction models for CVEs such as major adverse cardiovascular events, heart failure and arrhythmias.Expert opinion: Current evidence supports the superiority of ML methods over statistical-based models in predicting CVEs. Statistical models are applicable at the population level and are subject to overfitting, while ML methods can provide an individualized risk level for CVEs. Further prospective research on ML-guided treatments to prevent CVEs is needed.",2020-02-01,2,1167,74,462
1246,32067019,A review of mathematical representations of biomolecular data,"Recently, machine learning (ML) has established itself in various worldwide benchmarking competitions in computational biology, including Critical Assessment of Structure Prediction (CASP) and Drug Design Data Resource (D3R) Grand Challenges. However, the intricate structural complexity and high ML dimensionality of biomolecular datasets obstruct the efficient application of ML algorithms in the field. In addition to data and algorithm, an efficient ML machinery for biomolecular predictions must include structural representation as an indispensable component. Mathematical representations that simplify the biomolecular structural complexity and reduce ML dimensionality have emerged as a prime winner in D3R Grand Challenges. This review is devoted to the recent advances in developing low-dimensional and scalable mathematical representations of biomolecules in our laboratory. We discuss three classes of mathematical approaches, including algebraic topology, differential geometry, and graph theory. We elucidate how the physical and biological challenges have guided the evolution and development of these mathematical apparatuses for massive and diverse biomolecular data. We focus the performance analysis on protein-ligand binding predictions in this review although these methods have had tremendous success in many other applications, such as protein classification, virtual screening, and the predictions of solubility, solvation free energies, toxicity, partition coefficients, protein folding stability changes upon mutation, etc.",2020-02-01,3,1549,61,462
1003,31850639,Periodontics in the USA: An introduction,"The United States continues to be an incubator for new concepts and approaches to the diagnosis, treatment, and prevention of periodontal diseases. This volume of Periodontology 2000 presents some of these newer areas of research and paradigms that have emerged in the United States from both long-established and new investigators. These areas include: (1) more comprehensive approaches to assessing the total periodontal microbiome, including bacteria, viruses, and fungi, and their interactions with both the local and systemic inflammatory and immune responses, as well as with other oral and systemic conditions and diseases; (2) new developments for a more comprehensive characterization of the patient genome, transcriptome, and proteome profiles and the role of these profiles in periodontal disease pathogenesis; (3) new developments in nonsurgical approaches to periodontal diseases, including broad-based lines of attack using natural antimicrobials and host-modulation therapies and more focused approaches that target specific interactions in the host response; and (4) new big data analysis, machine learning, and imaging approaches, both for understanding the pathogenesis of periodontal diseases and for developing improved risk-assessment tools and better treatment outcomes.",2020-02-01,0,1292,40,462
1548,32180833,Latest Advances in Cardiac CT,"Recent rapid technological advancements in cardiac CT have improved image quality and reduced radiation exposure to patients. Furthermore, key insights from large cohort trials have helped delineate cardiovascular disease risk as a function of overall coronary plaque burden and the morphological appearance of individual plaques. The advent of CT-derived fractional flow reserve promises to establish an anatomical and functional test within one modality. Recent data examining the short-term impact of CT-derived fractional flow reserve on downstream care and clinical outcomes have been published. In addition, machine learning is a concept that is being increasingly applied to diagnostic medicine. Over the coming decade, machine learning will begin to be integrated into cardiac CT, and will potentially make a tangible difference to how this modality evolves. The authors have performed an extensive literature review and comprehensive analysis of the recent advances in cardiac CT. They review how recent advances currently impact on clinical care and potential future directions for this imaging modality.",2020-02-01,0,1114,29,462
863,31327699,A Narrative Review of Analytics in Pediatric Cardiac Anesthesia and Critical Care Medicine,"Congenital heart disease (CHD) is one of the most common birth anomalies, and the care of children with CHD has improved over the past 4 decades. However, children with CHD who undergo general anesthesia remain at increased risk for morbidity and mortality. The proliferation of electronic health record systems and sophisticated patient monitors affords the opportunity to capture and analyze large amounts of CHD patient data, and the application of novel, effective analytics methods to these data can enable clinicians to enhance their care of pediatric CHD patients. This narrative review covers recent efforts to leverage analytics in pediatric cardiac anesthesia and critical care to improve the care of children with CHD.",2020-02-01,1,729,90,462
1578,32132323,Imaging Brain Mechanisms of Functional Somatic Syndromes: Potential as a Biomarker?,"When patients present with persistent bodily complaints that cannot be explained by a symptom-linked organic pathology (medically unexplained symptoms), they are diagnosed with 'functional' somatic syndromes (FSS). Despite their prevalence, the management of FSS is notoriously challenging in clinical practice. This may be because FSS are heterogeneous disorders in terms of etiopathogenesis. They include patients with primarily peripheral dysfunction, primarily centrally driven somatic symptoms, and a mix of both. Brain-imaging studies, particularly data-driven pattern recognition methods using machine learning algorithms, could provide brain-based biomarkers for these clinical conditions. In this review, we provide an overview of our brain imaging data on brain-body interactions in one of the most well-known FSS, irritable bowel syndrome (IBS), and discuss the possible development of a brain-based biomarker for FSS. Anticipation of unpredictable pain, which commonly elicits fear in FSS patients, induced increased activity in brain areas associated with hypervigilance during rectal distention and non-distention conditions in IBS. This was coupled with dysfunctional inhibitory influence of the medial prefrontal cortex (mPFC) and pregenual anterior cingulate cortex (pACC) on stress regulation systems, resulting in the activated autonomic nervous system (ANS) and neuroendocrine system stimulated by corticotropin-releasing hormone (CRH). IBS subjects with higher alexithymia, a risk factor for FSS, showed stronger activity in the insula during rectal distention but reduced subjective sensitivity. Reduced top-down regulation of the ANS and CRH system by mPFC and pACC, discordance between the insula response to stimulation and subjective sensation of pain, and stronger threat responses in hypervigilance-related areas may be a candidate brain-based biomarker.",2020-03-01,0,1882,83,433
1574,32138284,Artificial Intelligence in Acute Kidney Injury Risk Prediction,"Acute kidney injury (AKI) is a frequent complication in hospitalized patients, which is associated with worse short and long-term outcomes. It is crucial to develop methods to identify patients at risk for AKI and to diagnose subclinical AKI in order to improve patient outcomes. The advances in clinical informatics and the increasing availability of electronic medical records have allowed for the development of artificial intelligence predictive models of risk estimation in AKI. In this review, we discussed the progress of AKI risk prediction from risk scores to electronic alerts to machine learning methods.",2020-03-01,5,615,62,433
1496,32256422,Neuroprediction and A.I. in Forensic Psychiatry and Criminal Justice: A Neurolaw Perspective,"Advances in the use of neuroimaging in combination with A.I., and specifically the use of machine learning techniques, have led to the development of brain-reading technologies which, in the nearby future, could have many applications, such as lie detection, neuromarketing or brain-computer interfaces. Some of these could, in principle, also be used in forensic psychiatry. The application of these methods in forensic psychiatry could, for instance, be helpful to increase the accuracy of risk assessment and to identify possible interventions. This technique could be referred to as 'A.I. neuroprediction,' and involves identifying potential neurocognitive markers for the prediction of recidivism. However, the future implications of this technique and the role of neuroscience and A.I. in violence risk assessment remain to be established. In this paper, we review and analyze the literature concerning the use of brain-reading A.I. for neuroprediction of violence and rearrest to identify possibilities and challenges in the future use of these techniques in the fields of forensic psychiatry and criminal justice, considering legal implications and ethical issues. The analysis suggests that additional research is required on A.I. neuroprediction techniques, and there is still a great need to understand how they can be implemented in risk assessment in the field of forensic psychiatry. Besides the alluring potential of A.I. neuroprediction, we argue that its use in criminal justice and forensic psychiatry should be subjected to thorough harms/benefits analyses not only when these technologies will be fully available, but also while they are being researched and developed.",2020-03-01,0,1689,92,433
966,31907954,Applying Machine Learning in Liver Disease and Transplantation: A Comprehensive Review,"Machine learning (ML) utilizes artificial intelligence to generate predictive models efficiently and more effectively than conventional methods through detection of hidden patterns within large data sets. With this in mind, there are several areas within hepatology where these methods can be applied. In this review, we examine the literature pertaining to machine learning in hepatology and liver transplant medicine. We provide an overview of the strengths and limitations of ML tools and their potential applications to both clinical and molecular data in hepatology. ML has been applied to various types of data in liver disease research, including clinical, demographic, molecular, radiological, and pathological data. We anticipate that use of ML tools to generate predictive algorithms will change the face of clinical practice in hepatology and transplantation. This review will provide readers with the opportunity to learn about the ML tools available and potential applications to questions of interest in hepatology.",2020-03-01,3,1029,86,433
84,32296706,Artificial Intelligence Applications in Dermatology: Where Do We Stand?,"Artificial intelligence (AI) has become a progressively prevalent Research Topic in medicine and is increasingly being applied to dermatology. There is a need to understand this technology's progress to help guide and shape the future for medical care providers and recipients. We reviewed the literature to evaluate the types of publications on the subject, the specific dermatological topics addressed by AI, and the most challenging barriers to its implementation. A substantial number of original articles and commentaries have been published to date and only few detailed reviews exist. Most AI applications focus on differentiating between benign and malignant skin lesions, however; others exist pertaining to ulcers, inflammatory skin diseases, allergen exposure, dermatopathology, and gene expression profiling. Applications commonly analyze and classify images, however, other tools such as risk assessment calculators are becoming increasingly available. Although many applications are technologically feasible, important implementation barriers have been identified including systematic biases, difficulty of standardization, interpretability, and acceptance by physicians and patients alike. This review provides insight into future research needs and possibilities. There is a strong need for clinical investigation in dermatology providing evidence of success overcoming the identified barriers. With these research goals in mind, an appropriate role for AI in dermatology may be achieved in not so distant future.",2020-03-01,3,1529,71,433
1054,31488886,Artificial intelligence for diabetic retinopathy screening: a review,"Diabetes is a global eye health issue. Given the rising in diabetes prevalence and ageing population, this poses significant challenge to perform diabetic retinopathy (DR) screening for these patients. Artificial intelligence (AI) using machine learning and deep learning have been adopted by various groups to develop automated DR detection algorithms. This article aims to describe the state-of-art AI DR screening technologies that have been described in the literature, some of which are already commercially available. All these technologies were designed using different training datasets and technical methodologies. Although many groups have published robust diagnostic performance of the AI algorithms for DR screening, future research is required to address several challenges, for examples medicolegal implications, ethics, and clinical deployment model in order to expedite the translation of these novel technologies into the healthcare setting.",2020-03-01,17,958,68,433
1032,31513851,Optical coherence tomography diagnostic signs in posterior uveitis,"A diagnostic sign refers to a quantifiable biological parameter that is measured and evaluated as an indicator of normal biological, pathogenic, or pharmacologic responses to a therapeutic intervention. When used in translational research discussions, the term itself often alludes to a sign used to accelerate or aid in diagnosis or monitoring and provide insight into ""personalized"" medicine. Many new diagnostic signs are being developed that involve imaging technology. Optical coherence tomography is an imaging technique that provides in vivo quasi-histological images of the ocular tissues and as such it's able to capture the structural and functional modifications that accompany inflammation and infection of the posterior part of the eye. From the hyperreflective inflammatory cells and deposits in the vitreous and on the hyaloid, to the swollen photoreceptors bodies in multiple evanescent white dots syndrome, and from optical differences in the subretinal fluid compartments in Vogt-Koyanagi-Harada disease to the hyporeflective granulomas in the choroid, these tomographical signs can be validated to reach the status of biomarkers. Such non-invasive imaging diagnostic signs of inflammation can be very useful to clinicians seeking to make a diagnosis and can represent a dataset for machine learning to offer a more empirical approach to the detection of posterior uveitis.",2020-03-01,5,1391,66,433
971,31898014,AI-based computer-aided diagnosis (AI-CAD): the latest review to read first,"The third artificial intelligence (AI) boom is coming, and there is an inkling that the speed of its evolution is quickly increasing. In games like chess, shogi, and go, AI has already defeated human champions, and the fact that it is able to achieve autonomous driving is also being realized. Under these circumstances, AI has evolved and diversified at a remarkable pace in medical diagnosis, especially in diagnostic imaging. Therefore, this commentary focuses on AI in medical diagnostic imaging and explains the recent development trends and practical applications of computer-aided detection/diagnosis using artificial intelligence, especially deep learning technology, as well as some topics surrounding it.",2020-03-01,6,714,75,433
2098,31816343,Supervised and unsupervised algorithms for bioinformatics and data science,"Bioinformatics refers to an ever evolving huge field of research based on millions of algorithms, designated to several data banks. Such algorithms are either supervised or unsupervised. In this article, a detailed overview of the supervised and unsupervised techniques is presented with the aid of examples. The aim of this article is to provide the readers with the basic understanding of the state of the art models, which are key ingredients of explainable machine learning in the field of bioinformatics.",2020-03-01,0,509,74,433
1587,32114857,Advancements in predicting outcomes in patients with glioma: a surgical perspective,"Introduction: Diffuse glioma is a challenging neurosurgical entity. Although surgery does not provide a cure, it may greatly influence survival, brain function, and quality of life. Surgical treatment is by nature highly personalized and outcome prediction is very complex. To engage and succeed in this balancing act it is important to make best use of the information available to the neurosurgeon.Areas covered: This narrative review provides an update on advancements in predicting outcomes in patients with glioma that are relevant to neurosurgeons.Expert opinion: The classical 'gut feeling' is notoriously unreliable and better prediction strategies for patients with glioma are warranted. There are numerous tools readily available for the neurosurgeon in predicting tumor biology and survival. Predicting extent of resection, functional outcome, and quality of life remains difficult. Although machine-learning approaches are currently not readily available in daily clinical practice, there are several ongoing efforts with the use of big data sets that are likely to create new prediction models and refine the existing models.",2020-03-01,1,1138,83,433
1511,32244292,Applications of Machine Learning Predictive Models in the Chronic Disease Diagnosis,"This paper reviews applications of machine learning (ML) predictive models in the diagnosis of chronic diseases. Chronic diseases (CDs) are responsible for a major portion of global health costs. Patients who suffer from these diseases need lifelong treatment. Nowadays, predictive models are frequently applied in the diagnosis and forecasting of these diseases. In this study, we reviewed the state-of-the-art approaches that encompass ML models in the primary diagnosis of CD. This analysis covers 453 papers published between 2015 and 2019, and our document search was conducted from PubMed (Medline), and Cumulative Index to Nursing and Allied Health Literature (CINAHL) libraries. Ultimately, 22 studies were selected to present all modeling methods in a precise way that explains CD diagnosis and usage models of individual pathologies with associated strengths and limitations. Our outcomes suggest that there are no standard methods to determine the best approach in real-time clinical practice since each method has its advantages and disadvantages. Among the methods considered, support vector machines (SVM), logistic regression (LR), clustering were the most commonly used. These models are highly applicable in classification, and diagnosis of CD and are expected to become more important in medical practice in the near future.",2020-03-01,5,1342,83,433
1510,32244919,From Bivariate to Multivariate Analysis of Cytometric Data: Overview of Computational Methods and Their Application in Vaccination Studies,"Flow and mass cytometry are used to quantify the expression of multiple extracellular or intracellular molecules on single cells, allowing the phenotypic and functional characterization of complex cell populations. Multiparametric flow cytometry is particularly suitable for deep analysis of immune responses after vaccination, as it allows to measure the frequency, the phenotype, and the functional features of antigen-specific cells. When many parameters are investigated simultaneously, it is not feasible to analyze all the possible bi-dimensional combinations of marker expression with classical manual analysis and the adoption of advanced automated tools to process and analyze high-dimensional data sets becomes necessary. In recent years, the development of many tools for the automated analysis of multiparametric cytometry data has been reported, with an increasing record of publications starting from 2014. However, the use of these tools has been preferentially restricted to bioinformaticians, while few of them are routinely employed by the biomedical community. Filling the gap between algorithms developers and final users is fundamental for exploiting the advantages of computational tools in the analysis of cytometry data. The potentialities of automated analyses range from the improvement of the data quality in the pre-processing steps up to the unbiased, data-driven examination of complex datasets using a variety of algorithms based on different approaches. In this review, an overview of the automated analysis pipeline is provided, spanning from the pre-processing phase to the automated population analysis. Analysis based on computational tools might overcame both the subjectivity of manual gating and the operator-biased exploration of expected populations. Examples of applications of automated tools that have successfully improved the characterization of different cell populations in vaccination studies are also presented.",2020-03-01,2,1961,138,433
958,31924424,Contrast-Enhanced Ultrasound Quantification: From Kinetic Modeling to Machine Learning,"Ultrasound contrast agents (UCAs) have opened up immense diagnostic possibilities by combined use of indicator dilution principles and dynamic contrast-enhanced ultrasound (DCE-US) imaging. UCAs are microbubbles encapsulated in a biocompatible shell. With a rheology comparable to that of red blood cells, UCAs provide an intravascular indicator for functional imaging of the (micro)vasculature by quantitative DCE-US. Several models of the UCA intravascular kinetics have been proposed to provide functional quantitative maps, aiding diagnosis of different pathological conditions. This article is a comprehensive review of the available methods for quantitative DCE-US imaging based on temporal, spatial and spatiotemporal analysis of the UCA kinetics. The recent introduction of novel UCAs that are targeted to specific vascular receptors has advanced DCE-US to a molecular imaging modality. In parallel, new kinetic models of increased complexity have been developed. The extraction of multiple quantitative maps, reflecting complementary variables of the underlying physiological processes, requires an integrative approach to their interpretation. A probabilistic framework based on emerging machine-learning methods represents nowadays the ultimate approach, improving the diagnostic accuracy of DCE-US imaging by optimal combination of the extracted complementary information. The current value and future perspective of all these advances are critically discussed.",2020-03-01,2,1473,86,433
1264,32049747,Machine learning in nephrology: scratching the surface,"Machine learning shows enormous potential in facilitating decision-making regarding kidney diseases. With the development of data preservation and processing, as well as the advancement of machine learning algorithms, machine learning is expected to make remarkable breakthroughs in nephrology. Machine learning models have yielded many preliminaries to moderate and several excellent achievements in the fields, including analysis of renal pathological images, diagnosis and prognosis of chronic kidney diseases and acute kidney injury, as well as management of dialysis treatments. However, it is just scratching the surface of the field; at the same time, machine learning and its applications in renal diseases are facing a number of challenges. In this review, we discuss the application status, challenges and future prospects of machine learning in nephrology to help people further understand and improve the capacity for prediction, detection, and care quality in kidney diseases.",2020-03-01,2,989,54,433
1524,32226594,NanoSolveIT Project: Driving nanoinformatics research to develop innovative and integrated tools for in silico nanosafety assessment,"Nanotechnology has enabled the discovery of a multitude of novel materials exhibiting unique physicochemical (PChem) properties compared to their bulk analogues. These properties have led to a rapidly increasing range of commercial applications; this, however, may come at a cost, if an association to long-term health and environmental risks is discovered or even just perceived. Many nanomaterials (NMs) have not yet had their potential adverse biological effects fully assessed, due to costs and time constraints associated with the experimental assessment, frequently involving animals. Here, the available NM libraries are analyzed for their suitability for integration with novel nanoinformatics approaches and for the development of NM specific Integrated Approaches to Testing and Assessment (IATA) for human and environmental risk assessment, all within the NanoSolveIT cloud-platform. These established and well-characterized NM libraries (e.g. NanoMILE, NanoSolutions, NANoREG, NanoFASE, caLIBRAte, NanoTEST and the Nanomaterial Registry (>2000 NMs)) contain physicochemical characterization data as well as data for several relevant biological endpoints, assessed in part using harmonized Organisation for Economic Co-operation and Development (OECD) methods and test guidelines. Integration of such extensive NM information sources with the latest nanoinformatics methods will allow NanoSolveIT to model the relationships between NM structure (morphology), properties and their adverse effects and to predict the effects of other NMs for which less data is available. The project specifically addresses the needs of regulatory agencies and industry to effectively and rapidly evaluate the exposure, NM hazard and risk from nanomaterials and nano-enabled products, enabling implementation of computational 'safe-by-design' approaches to facilitate NM commercialization.",2020-03-01,6,1881,132,433
1732,31603244,Machine learning techniques for protein function prediction,"Proteins play important roles in living organisms, and their function is directly linked with their structure. Due to the growing gap between the number of proteins being discovered and their functional characterization (in particular as a result of experimental limitations), reliable prediction of protein function through computational means has become crucial. This paper reviews the machine learning techniques used in the literature, following their evolution from simple algorithms such as logistic regression to more advanced methods like support vector machines and modern deep neural networks. Hyperparameter optimization methods adopted to boost prediction performance are presented. In parallel, the metamorphosis in the features used by these algorithms from classical physicochemical properties and amino acid composition, up to text-derived features from biomedical literature and learned feature representations using autoencoders, together with feature selection and dimensionality reduction techniques, are also reviewed. The success stories in the application of these techniques to both general and specific protein function prediction are discussed.",2020-03-01,3,1170,59,433
1002,31850970,Molecular prediction of metastasis in cutaneous squamous cell carcinoma,"Purpose of review:                    Cutaneous squamous cell carcinoma (cSCC) is a highly prevalent malignancy frequently occurring on body surfaces chronically exposed to ultraviolet radiation. While a large majority of tumors remain localized to the skin and immediate subcutaneous tissue and are cured with surgical excision, a small subset of patients with cSCC will develop metastatic disease. Risk stratification for cSCC is performed using clinical staging systems, but given a high mutational burden and advances in targeted and immunotherapy, there is growing interest in molecular predictors of high-risk disease.              Recent findings:                    Recent literature on the risk for metastasis in cSCC includes notable findings in genes involved in cell-cycle regulation, tumor suppression, tissue invasion and microenvironment, interactions with the host-immune system, and epigenetic regulation.              Summary:                    cSCC is a highly mutated tumor with complex carcinogenesis. Regulators of tumor growth and local invasion are numerous and increasingly well-understood but drivers of metastasis are less established. Areas of importance include central system regulators (NOTCH, miRNAs), proteins involved in tissue invasion (podoplanin, E-cadherin), and targets of existing and emerging therapeutics (PD-1, epidermal growth factor receptor). Given the complexity of cSCC carcinogenesis, the use of machine learning algorithms and computational genomics may provide ultimate insight and prospective studies are needed to verify clinical relevance.",2020-03-01,1,1594,71,433
993,31876546,Computational analysis of flow cytometry data in hematological malignancies: future clinical practice?,"Purpose of review:                    This review outlines the advancements that have been made in computational analysis for clinical flow cytometry data in hematological malignancies.              Recent findings:                    In recent years, computational analysis methods have been applied to clinical flow cytometry data of hematological malignancies with promising results. Most studies combined dimension reduction (principle component analysis) or clustering methods (FlowSOM, generalized mixture models) with machine learning classifiers (support vector machines, random forest). For diagnosis and classification of hematological malignancies, many studies have reported results concordant with manual expert analysis, including B-cell chronic lymphoid leukemia detection and acute leukemia classification. Other studies, e.g. concerning diagnosis of myelodysplastic syndromes and classification of lymphoma, have shown to be able to increase diagnostic accuracy. With respect to treatment response monitoring, studies have focused on, for example, computational minimal residual disease detection in multiple myeloma and posttreatment classification of healthy or diseased in acute myeloid leukemia. The results of these studies are encouraging, although accurate relapse prediction remains challenging. To facilitate clinical implementation, collaboration and (prospective) validation in multicenter setting are necessary.              Summary:                    Computational analysis methods for clinical flow cytometry data hold the potential to increase ease of use, objectivity and accuracy in the clinical work-up of hematological malignancies.",2020-03-01,5,1669,102,433
1543,32195365,A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases,"Autoimmune diseases are chronic, multifactorial conditions. Through machine learning (ML), a branch of the wider field of artificial intelligence, it is possible to extract patterns within patient data, and exploit these patterns to predict patient outcomes for improved clinical management. Here, we surveyed the use of ML methods to address clinical problems in autoimmune disease. A systematic review was conducted using MEDLINE, embase and computers and applied sciences complete databases. Relevant papers included ""machine learning"" or ""artificial intelligence"" and the autoimmune diseases search term(s) in their title, abstract or key words. Exclusion criteria: studies not written in English, no real human patient data included, publication prior to 2001, studies that were not peer reviewed, non-autoimmune disease comorbidity research and review papers. 169 (of 702) studies met the criteria for inclusion. Support vector machines and random forests were the most popular ML methods used. ML models using data on multiple sclerosis, rheumatoid arthritis and inflammatory bowel disease were most common. A small proportion of studies (7.7% or 13/169) combined different data types in the modelling process. Cross-validation, combined with a separate testing set for more robust model evaluation occurred in 8.3% of papers (14/169). The field may benefit from adopting a best practice of validation, cross-validation and independent testing of ML models. Many models achieved good predictive results in simple scenarios (e.g. classification of cases and controls). Progression to more complex predictive models may be achievable in future through integration of multiple data types.",2020-03-01,7,1692,110,433
1540,32197324,A Review on Applications of Computational Methods in Drug Screening and Design,"Drug development is one of the most significant processes in the pharmaceutical industry. Various computational methods have dramatically reduced the time and cost of drug discovery. In this review, we firstly discussed roles of multiscale biomolecular simulations in identifying drug binding sites on the target macromolecule and elucidating drug action mechanisms. Then, virtual screening methods (e.g., molecular docking, pharmacophore modeling, and QSAR) as well as structure- and ligand-based classical/de novo drug design were introduced and discussed. Last, we explored the development of machine learning methods and their applications in aforementioned computational methods to speed up the drug discovery process. Also, several application examples of combining various methods was discussed. A combination of different methods to jointly solve the tough problem at different scales and dimensions will be an inevitable trend in drug screening and design.",2020-03-01,17,965,78,433
1554,32165361,Artificial Intelligence in the Management of Intracranial Aneurysms: Current Status and Future Perspectives,"Intracranial aneurysms with subarachnoid hemorrhage lead to high morbidity and mortality. It is of critical importance to detect aneurysms, identify risk factors of rupture, and predict treatment response of aneurysms to guide clinical interventions. Artificial intelligence has received worldwide attention for its impressive performance in image-based tasks. Artificial intelligence serves as an adjunct to physicians in a series of clinical settings, which substantially improves diagnostic accuracy while reducing physicians' workload. Computer-assisted diagnosis systems of aneurysms based on MRA and CTA using deep learning have been evaluated, and excellent performances have been reported. Artificial intelligence has also been used in automated morphologic calculation, rupture risk stratification, and outcomes prediction with the implementation of machine learning methods, which have exhibited incremental value. This review summarizes current advances of artificial intelligence in the management of aneurysms, including detection and prediction. The challenges and future directions of clinical implementations of artificial intelligence are briefly discussed.",2020-03-01,3,1174,107,433
1536,32204390,Smart Containers Schedulers for Microservices Provision in Cloud-Fog-IoT Networks. Challenges and Opportunities,"Docker containers are the lightweight-virtualization technology prevailing today for the provision of microservices. This work raises and discusses two main challenges in Docker containers' scheduling in cloud-fog-internet of things (IoT) networks. First, the convenience to integrate intelligent containers' schedulers based on soft-computing in the dominant open-source containers' management platforms: Docker Swarm, Google Kubernetes and Apache Mesos. Secondly, the need for specific intelligent containers' schedulers for the different interfaces in cloud-fog-IoT networks: cloud-to-fog, fog-to-IoT and cloud-to-fog. The goal of this work is to support the optimal allocation of microservices provided by the main cloud service providers today and used by millions of users worldwide in applications such as smart health, content delivery networks, smart health, etc. Particularly, the improvement is studied in terms of quality of service (QoS) parameters such as latency, load balance, energy consumption and runtime, based on the analysis of previous works and implementations. Moreover, the scientific-technical impact of smart containers' scheduling in the market is also discussed, showing the possible repercussion of the raised opportunities in the research line.",2020-03-01,0,1276,111,433
1295,32008639,Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model,"The recent increase in accessible medical and clinical laboratory ""Big Data"" has led to a corresponding increase in the use of machine-learning tools to develop integrative diagnostic models incorporating both existing and new test data. The rise of direct-to-consumer (DTC) testing paradigms raises the possibility of predictive models that use these new sources. This article discusses several distinct challenges raised by the DTC approach, including issues of centralized data collection, ascertainment bias, linkage to medical outcomes, and standardization/harmonization of results. Several solutions to maximize the promise of machine-learning data analytics for DTC data are suggested.",2020-03-01,0,692,91,433
901,31268707,Opportunities and Challenges in Phenotypic Screening for Neurodegenerative Disease Research,"Toxic misfolded proteins potentially underly many neurodegenerative diseases, but individual targets which regulate these proteins and their downstream detrimental effects are often unknown. Phenotypic screening is an unbiased method to screen for novel targets and therapeutic molecules and span the range from primitive model organisms such as Sacchaomyces cerevisiae, which allow for high-throughput screening to patient-derived cell-lines that have a close connection to the disease biology but are limited in screening capacity. This perspective will review current phenotypic models, as well as the chemical screening strategies most often employed. Advances in in 3D cell cultures, high-content screens, robotic microscopy, CRISPR screening, and use of machine learning methods to process the enormous amount of data generated by these screens are certain to change the paradigm for phenotypic screening and will be discussed.",2020-03-01,5,933,91,433
1237,32087008,"The promise of toxicogenomics for genetic toxicology: past, present and future","Toxicogenomics, the application of genomics to toxicology, was described as 'a new era' for toxicology. Standard toxicity tests typically involve a number of short-term bioassays that are costly, time consuming, require large numbers of animals and generally focus on a single end point. Toxicogenomics was heralded as a way to improve the efficiency of toxicity testing by assessing gene regulation across the genome, allowing rapid classification of compounds based on characteristic expression profiles. Gene expression microarrays could measure and characterise genome-wide gene expression changes in a single study and while transcriptomic profiles that can discriminate between genotoxic and non-genotoxic carcinogens have been identified, challenges with the approach limited its application. As such, toxicogenomics did not transform the field of genetic toxicology in the way it was predicted. More recently, next generation sequencing (NGS) technologies have revolutionised genomics owing to the fact that hundreds of billions of base pairs can be sequenced simultaneously cheaper and quicker than traditional Sanger methods. In relation to genetic toxicology, and thousands of cancer genomes have been sequenced with single-base substitution mutational signatures identified, and mutation signatures have been identified following treatment of cells with known or suspected environmental carcinogens. RNAseq has been applied to detect transcriptional changes following treatment with genotoxins; modified RNAseq protocols have been developed to identify adducts in the genome and Duplex sequencing is an example of a technique that has recently been developed to accurately detect mutation. Machine learning, including MutationSeq and SomaticSeq, has also been applied to somatic mutation detection and improvements in automation and/or the application of machine learning algorithms may allow high-throughput mutation sequencing in the future. This review will discuss the initial promise of transcriptomics for genetic toxicology, and how the development of NGS technologies and new machine learning algorithms may finally realise that promise.",2020-03-01,0,2157,78,433
1534,32207266,Machine Learning Applications in Endocrinology and Metabolism Research: An Overview,"Machine learning (ML) applications have received extensive attention in endocrinology research during the last decade. This review summarizes the basic concepts of ML and certain research topics in endocrinology and metabolism where ML principles have been actively deployed. Relevant studies are discussed to provide an overview of the methodology, main findings, and limitations of ML, with the goal of stimulating insights into future research directions. Clear, testable study hypotheses stem from unmet clinical needs, and the management of data quality (beyond a focus on quantity alone), open collaboration between clinical experts and ML engineers, the development of interpretable high-performance ML models beyond the black-box nature of some algorithms, and a creative environment are the core prerequisites for the foreseeable changes expected to be brought about by ML and artificial intelligence in the field of endocrinology and metabolism, with actual improvements in clinical practice beyond hype. Of note, endocrinologists will continue to play a central role in these developments as domain experts who can properly generate, refine, analyze, and interpret data with a combination of clinical expertise and scientific rigor.",2020-03-01,0,1243,83,433
1302,32000922,Dysmorphology in a Genomic Era,Dysmorphology is the practice of defining the morphologic phenotype of syndromic disorders. Genomic sequencing has advanced our understanding of human variation and molecular dysmorphology has evolved in response to the science of relating embryologic developmental implications of abnormal gene signaling pathways to the resultant phenotypic presentation. Machine learning has enabled the application of deep convoluted neural networks to recognize the comparative likeness of these phenotypes relative to the causal genotype or disrupted gene pathway.,2020-03-01,0,553,30,433
514,31038827,Advanced polysomnographic analysis for OSA: A pathway to personalized management?,"Obstructive sleep apnea (OSA) is a highly heterogeneous disorder, with diverse pathways to disease, expression of disease, susceptibility to co-morbidities and response to therapy, and is ideally suited to precision medicine approaches. Clinically, the content of the information-rich polysomnogram (PSG) is not currently fully utilized in determining patient management. Novel PSG parameters such as hypoxic burden, pulse transit time, cardiopulmonary coupling and the frequency representations of PSG sensor signals could predict a variety of cardiovascular disease, cancer and neurodegeneration co-morbidities. The PSG can also be used to identify key pathophysiological parameters such as loop gain, arousal threshold and muscle compensation which can enhance understanding of the causes of OSA in an individual, and thereby guide choices on therapy. Machine learning methods performing their own parameter extraction coupled with large PSG data sets offer an exciting opportunity for discovering new links between the PSG variables and disease outcomes. By exploiting existing and emerging analytical methods, the PSG may offer a pathway to personalized management for OSA.",2020-03-01,2,1178,81,433
2259,31111616,Quantitative cardiac MRI,"Cardiac MRI has become an indispensable imaging modality in the investigation of patients with suspected heart disease. It has emerged as the gold standard test for cardiac function, volumes, and mass and allows noninvasive tissue characterization and the assessment of myocardial perfusion. Quantitative MRI already has a key role in the development and incorporation of machine learning in clinical imaging, potentially offering major improvements in both workflow efficiency and diagnostic accuracy. As the clinical applications of a wide range of quantitative cardiac MRI techniques are being explored and validated, we are expanding our capabilities for earlier detection, monitoring, and risk stratification of disease, potentially guiding personalized management decisions in various cardiac disease models. In this article we review established and emerging quantitative techniques, their clinical applications, highlight novel advances, and appraise their clinical diagnostic potential. Level of Evidence: 2 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2020;51:693-711.",2020-03-01,4,1085,24,433
1533,32207586,The ways of using machine learning in dentistry,"Innovative computer techniques are starting to be employed not only in academic research, but also in commercial production, finding use in many areas of dentistry. This is conducive to the digitalization of dentistry and its increasing treatment and diagnostic demands. In many areas of dentistry, such as orthodontics and maxillofacial surgery, but also periodontics or prosthetics, only a correct diagnosis ensures the correct treatment plan, which is the only way to restore the patient's health. The diagnosis and treatment plan is based on the specialist's knowledge, but is subject to a large, multi-factorial risk of error. Therefore, the introduction of multiparametric pattern recognition methods (statistics, machine learning and artificial intelligence (AI)) is a great hope for both the physicians and the patients. However, the general use of clinical decision support systems (CDSS) in a dental clinic is not yet realistic and requires work in many aspects - methodical, technological and business. The article presents a review of the latest attempts to apply AI, such as CDSS or genetic algorithms (GAs) in research and clinical dentistry, taking under consideration all of the main dental specialties. Work on the introduction of public CDSS has been continued for years. The article presents the latest achievements in this field, analyzing their real-life application and credibility.",2020-03-01,0,1404,47,433
2566,30516837,From high definition precision healthcare to precision public oral health: opportunities and challenges,"In anticipation of a major transformation in healthcare, this review provides highlights that anticipate the near future for oral public health (and beyond). Personalized or precision healthcare reflects the expectation that advances in genomics, imaging, and other domains will extend our risk assessment, diagnostic, and prognostic capabilities, and enables more effective prevention and therapeutic options for all Americans. Meanwhile, the current healthcare system does not meet cost, access, or quality criteria for all Americans. It is now an imperative that the success of ""smart,"" quality, and cost-effective high definition precision healthcare requires a public health perspective for several reasons: a) to enhance generalizability, b) to assess methods of implementation, and c) to focus on both risk and prevention in large and small populations, thereby providing a balance between the generation of long-term knowledge and short-term health gains. Sensitivity and resolution, reasonable cost, access to all Americans, coordinated comprehensive care, and advances in whole genome sequencing (WGS) and big data analyses, coupled to other advances in biotechnology and digital/artificial intelligence/machine learning devices, and the behavioral, social, and environmental sciences, offer remarkable opportunities to improve the health and wellness of the American people [genotype + phenotype + environment + behavior = high definition healthcare]. The opportunity is to significantly improve the well-being and life expectancy of all people across the lifespan including the least-advantaged people in our society and potentially increase access, reduce the national costs, and improve health outcomes.",2020-03-01,1,1717,103,433
1532,32211130,Prediction of the miRNA interactome - Established methods and upcoming perspectives,"MicroRNAs (miRNAs) are well-studied small noncoding RNAs involved in post-transcriptional gene regulation in a wide range of organisms, including mammals. Their function is mediated by base pairing with their target RNAs. Although many features required for miRNA-mediated repression have been described, the identification of functional interactions is still challenging. In the last two decades, numerous Machine Learning (ML) models have been developed to predict their putative targets. In this review, we summarize the biological knowledge and the experimental data used to develop these ML models. Recently, Deep Neural Network-based models have also emerged in miRNA interaction modeling. We thus outline established and emerging models to give a perspective on the future developments needed to improve the identification of genes directly regulated by miRNAs.",2020-03-01,3,868,83,433
930,31965266,Machine learning for the prediction of sepsis: a systematic review and meta-analysis of diagnostic test accuracy,"Purpose:                    Early clinical recognition of sepsis can be challenging. With the advancement of machine learning, promising real-time models to predict sepsis have emerged. We assessed their performance by carrying out a systematic review and meta-analysis.              Methods:                    A systematic search was performed in PubMed, Embase.com and Scopus. Studies targeting sepsis, severe sepsis or septic shock in any hospital setting were eligible for inclusion. The index test was any supervised machine learning model for real-time prediction of these conditions. Quality of evidence was assessed using the Grading of Recommendations Assessment, Development and Evaluation (GRADE) methodology, with a tailored Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) checklist to evaluate risk of bias. Models with a reported area under the curve of the receiver operating characteristic (AUROC) metric were meta-analyzed to identify strongest contributors to model performance.              Results:                    After screening, a total of 28 papers were eligible for synthesis, from which 130 models were extracted. The majority of papers were developed in the intensive care unit (ICU, n = 15; 54%), followed by hospital wards (n = 7; 25%), the emergency department (ED, n = 4; 14%) and all of these settings (n = 2; 7%). For the prediction of sepsis, diagnostic test accuracy assessed by the AUROC ranged from 0.68-0.99 in the ICU, to 0.96-0.98 in-hospital and 0.87 to 0.97 in the ED. Varying sepsis definitions limit pooling of the performance across studies. Only three papers clinically implemented models with mixed results. In the multivariate analysis, temperature, lab values, and model type contributed most to model performance.              Conclusion:                    This systematic review and meta-analysis show that on retrospective data, individual machine learning models can accurately predict sepsis onset ahead of time. Although they present alternatives to traditional scoring systems, between-study heterogeneity limits the assessment of pooled results. Systematic reporting and clinical implementation studies are needed to bridge the gap between bytes and bedside.",2020-03-01,24,2233,112,433
1558,32164200,Applications of Deep Learning for Dense Scenes Analysis in Agriculture: A Review,"Deep Learning (DL) is the state-of-the-art machine learning technology, which shows superior performance in computer vision, bioinformatics, natural language processing, and other areas. Especially as a modern image processing technology, DL has been successfully applied in various tasks, such as object detection, semantic segmentation, and scene analysis. However, with the increase of dense scenes in reality, due to severe occlusions, and small size of objects, the analysis of dense scenes becomes particularly challenging. To overcome these problems, DL recently has been increasingly applied to dense scenes and has begun to be used in dense agricultural scenes. The purpose of this review is to explore the applications of DL for dense scenes analysis in agriculture. In order to better elaborate the topic, we first describe the types of dense scenes in agriculture, as well as the challenges. Next, we introduce various popular deep neural networks used in these dense scenes. Then, the applications of these structures in various agricultural tasks are comprehensively introduced in this review, including recognition and classification, detection, counting and yield estimation. Finally, the surveyed DL applications, limitations and the future work for analysis of dense images in agriculture are summarized.",2020-03-01,2,1322,80,433
1527,32218708,Using Artificial Intelligence in Infection Prevention,"Purpose of review:                    Artificial intelligence (AI) offers huge potential in infection prevention and control (IPC). We explore its potential IPC benefits in epidemiology, laboratory infection diagnosis, and hand hygiene.              Recent findings:                    AI has the potential to detect transmission events during outbreaks or predict high-risk patients, enabling development of tailored IPC interventions. AI offers opportunities to enhance diagnostics with objective pattern recognition, standardize the diagnosis of infections with IPC implications, and facilitate the dissemination of IPC expertise. AI hand hygiene applications can deliver behavior change, though it requires further evaluation in different clinical settings. However, staff can become dependent on automatic reminders, and performance returns to baseline if feedback is removed.              Summary:                    Advantages for IPC include speed, consistency, and capability of handling infinitely large datasets. However, many challenges remain; improving the availability of high-quality representative datasets and consideration of biases within preexisting databases are important challenges for future developments. AI in itself will not improve IPC; this requires culture and behavior change. Most studies to date assess performance retrospectively so there is a need for prospective evaluation in the real-life, often chaotic, clinical setting. Close collaboration with IPC experts to interpret outputs and ensure clinical relevance is essential.",2020-03-01,3,1563,53,433
1565,32150991,The Application of Deep Learning in Cancer Prognosis Prediction,"Deep learning has been applied to many areas in health care, including imaging diagnosis, digital pathology, prediction of hospital admission, drug design, classification of cancer and stromal cells, doctor assistance, etc. Cancer prognosis is to estimate the fate of cancer, probabilities of cancer recurrence and progression, and to provide survival estimation to the patients. The accuracy of cancer prognosis prediction will greatly benefit clinical management of cancer patients. The improvement of biomedical translational research and the application of advanced statistical analysis and machine learning methods are the driving forces to improve cancer prognosis prediction. Recent years, there is a significant increase of computational power and rapid advancement in the technology of artificial intelligence, particularly in deep learning. In addition, the cost reduction in large scale next-generation sequencing, and the availability of such data through open source databases (e.g., TCGA and GEO databases) offer us opportunities to possibly build more powerful and accurate models to predict cancer prognosis more accurately. In this review, we reviewed the most recent published works that used deep learning to build models for cancer prognosis prediction. Deep learning has been suggested to be a more generic model, requires less data engineering, and achieves more accurate prediction when working with large amounts of data. The application of deep learning in cancer prognosis has been shown to be equivalent or better than current approaches, such as Cox-PH. With the burst of multi-omics data, including genomics data, transcriptomics data and clinical information in cancer studies, we believe that deep learning would potentially improve cancer prognosis.",2020-03-01,8,1781,63,433
934,31960635,"Future Directions in Coronary CT Angiography: CT-Fractional Flow Reserve, Plaque Vulnerability, and Quantitative Plaque Assessment","Coronary computed tomography angiography (CCTA) is a well-validated and noninvasive imaging modality for the assessment of coronary artery disease (CAD) in patients with stable ischemic heart disease and acute coronary syndromes (ACSs). CCTA not only delineates the anatomy of the heart and coronary arteries in detail, but also allows for intra- and extraluminal imaging of coronary arteries. Emerging technologies have promoted new CCTA applications, resulting in a comprehensive assessment of coronary plaques and their clinical significance. The application of computational fluid dynamics to CCTA resulted in a robust tool for noninvasive assessment of coronary blood flow hemodynamics and determination of hemodynamically significant stenosis. Detailed evaluation of plaque morphology and identification of high-risk plaque features by CCTA have been confirmed as predictors of future outcomes, identifying patients at risk for ACSs. With quantitative coronary plaque assessment, the progression of the CAD or the response to therapy could be monitored by CCTA. The aim of this article is to review the future directions of emerging applications in CCTA, such as computed tomography (CT)-fractional flow reserve, imaging of vulnerable plaque features, and quantitative plaque imaging. We will also briefly discuss novel methods appearing in the coronary imaging scenario, such as machine learning, radiomics, and spectral CT.",2020-03-01,1,1431,130,433
1317,31981309,Deep Learning-Based Single-Cell Optical Image Studies,"Optical imaging technology that has the advantages of high sensitivity and cost-effectiveness greatly promotes the progress of nondestructive single-cell studies. Complex cellular image analysis tasks such as three-dimensional reconstruction call for machine-learning technology in cell optical image research. With the rapid developments of high-throughput imaging flow cytometry, big data cell optical images are always obtained that may require machine learning for data analysis. In recent years, deep learning has been prevalent in the field of machine learning for large-scale image processing and analysis, which brings a new dawn for single-cell optical image studies with an explosive growth of data availability. Popular deep learning techniques offer new ideas for multimodal and multitask single-cell optical image research. This article provides an overview of the basic knowledge of deep learning and its applications in single-cell optical image studies. We explore the feasibility of applying deep learning techniques to single-cell optical image analysis, where popular techniques such as transfer learning, multimodal learning, multitask learning, and end-to-end learning have been reviewed. Image preprocessing and deep learning model training methods are then summarized. Applications based on deep learning techniques in the field of single-cell optical image studies are reviewed, which include image segmentation, super-resolution image reconstruction, cell tracking, cell counting, cross-modal image reconstruction, and design and control of cell imaging systems. In addition, deep learning in popular single-cell optical imaging techniques such as label-free cell optical imaging, high-content screening, and high-throughput optical imaging cytometry are also mentioned. Finally, the perspectives of deep learning technology for single-cell optical image analysis are discussed.  2020 International Society for Advancement of Cytometry.",2020-03-01,4,1962,53,433
1007,31845543,"Artificial intelligence, machine learning and the pediatric airway","Artificial intelligence and machine learning are rapidly expanding fields with increasing relevance in anesthesia and, in particular, airway management. The ability of artificial intelligence and machine learning algorithms to recognize patterns from large volumes of complex data makes them attractive for use in pediatric anesthesia airway management. The purpose of this review is to introduce artificial intelligence, machine learning, and deep learning to the pediatric anesthesiologist. Current evidence and developments in artificial intelligence, machine learning, and deep learning relevant to pediatric airway management are presented. We critically assess the current evidence on the use of artificial intelligence and machine learning in the assessment, diagnosis, monitoring, procedure assistance, and predicting outcomes during pediatric airway management. Further, we discuss the limitations of these technologies and offer areas for focused research that may bring pediatric airway management anesthesiology into the era of artificial intelligence and machine learning.",2020-03-01,0,1085,66,433
985,31884065,Machine Learning Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology: A Narrative Review,"Histopathology is a method used for breast cancer diagnosis. Machine learning (ML) methods have achieved success for supervised learning tasks in the medical domain. In this article, we investigate the impact of ML for the diagnosis of breast cancer using histopathology images of conventional photomicroscopy. Cancer diagnosis is the identification of images as cancer or noncancer, and this involves image preprocessing, feature extraction, classification, and performance analysis. In this article, different approaches to perform these necessary steps are reviewed. We find that most ML research for breast cancer diagnosis has been focused on deep learning. Based on inferences from the recent research activities, we discuss how ML methods can benefit conventional microscopy-based breast cancer diagnosis. Finally, we discuss the research gaps of ML approaches for the implementation in a real pathology environment and propose future research guidelines.",2020-03-01,4,962,108,433
1564,32154629,An overview of the first 5 years of the ENIGMA obsessive-compulsive disorder working group: The power of worldwide collaboration,"Neuroimaging has played an important part in advancing our understanding of the neurobiology of obsessive-compulsive disorder (OCD). At the same time, neuroimaging studies of OCD have had notable limitations, including reliance on relatively small samples. International collaborative efforts to increase statistical power by combining samples from across sites have been bolstered by the ENIGMA consortium; this provides specific technical expertise for conducting multi-site analyses, as well as access to a collaborative community of neuroimaging scientists. In this article, we outline the background to, development of, and initial findings from ENIGMA's OCD working group, which currently consists of 47 samples from 34 institutes in 15 countries on 5 continents, with a total sample of 2,323 OCD patients and 2,325 healthy controls. Initial work has focused on studies of cortical thickness and subcortical volumes, structural connectivity, and brain lateralization in children, adolescents and adults with OCD, also including the study on the commonalities and distinctions across different neurodevelopment disorders. Additional work is ongoing, employing machine learning techniques. Findings to date have contributed to the development of neurobiological models of OCD, have provided an important model of global scientific collaboration, and have had a number of clinical implications. Importantly, our work has shed new light on questions about whether structural and functional alterations found in OCD reflect neurodevelopmental changes, effects of the disease process, or medication impacts. We conclude with a summary of ongoing work by ENIGMA-OCD, and a consideration of future directions for neuroimaging research on OCD within and beyond ENIGMA.",2020-03-01,2,1765,128,433
1563,32155930,A Survey of Heart Anomaly Detection Using Ambulatory Electrocardiogram (ECG),"Cardiovascular diseases (CVDs) are the number one cause of death globally. An estimated 17.9 million people die from CVDs each year, representing 31% of all global deaths. Most cardiac patients require early detection and treatment. Therefore, many products to monitor patient's heart conditions have been introduced on the market. Most of these devices can record a patient's bio-metric signals both in resting and in exercising situations. However, reading the massive amount of raw electrocardiogram (ECG) signals from the sensors is very time-consuming. Automatic anomaly detection for the ECG signals could act as an assistant for doctors to diagnose a cardiac condition. This paper reviews the current state-of-the-art of this technology discusses the pros and cons of the devices and algorithms found in the literature and the possible research directions to develop the next generation of ambulatory monitoring systems.",2020-03-01,0,927,76,433
1530,32215571,Generative and discriminative model-based approaches to microscopic image restoration and segmentation,"Image processing is one of the most important applications of recent machine learning (ML) technologies. Convolutional neural networks (CNNs), a popular deep learning-based ML architecture, have been developed for image processing applications. However, the application of ML to microscopic images is limited as microscopic images are often 3D/4D, that is, the image sizes can be very large, and the images may suffer from serious noise generated due to optics. In this review, three types of feature reconstruction applications to microscopic images are discussed, which fully utilize the recent advancements in ML technologies. First, multi-frame super-resolution is introduced, based on the formulation of statistical generative model-based techniques such as Bayesian inference. Second, data-driven image restoration is introduced, based on supervised discriminative model-based ML technique. In this application, CNNs are demonstrated to exhibit preferable restoration performance. Third, image segmentation based on data-driven CNNs is introduced. Image segmentation has become immensely popular in object segmentation based on electron microscopy (EM); therefore, we focus on EM image processing.",2020-04-01,1,1203,102,402
27,32372937,"Attention in Psychology, Neuroscience, and Machine Learning","Attention is the important ability to flexibly control limited computational resources. It has been studied in conjunction with many other topics in neuroscience and psychology including awareness, vigilance, saliency, executive control, and learning. It has also recently been applied in several domains in machine learning. The relationship between the study of biological attention and its use as a tool to enhance artificial neural networks is not always clear. This review starts by providing an overview of how attention is conceptualized in the neuroscience and psychology literature. It then covers several use cases of attention in machine learning, indicating their biological counterparts where they exist. Finally, the ways in which artificial attention can be further inspired by biology for the production of complex and integrative systems is explored.",2020-04-01,1,867,59,402
1580,32128792,An Introduction to Machine Learning,"In the last few years, machine learning (ML) and artificial intelligence have seen a new wave of publicity fueled by the huge and ever-increasing amount of data and computational power as well as the discovery of improved learning algorithms. However, the idea of a computer learning some abstract concept from data and applying them to yet unseen situations is not new and has been around at least since the 1950s. Many of these basic principles are very familiar to the pharmacometrics and clinical pharmacology community. In this paper, we want to introduce the foundational ideas of ML to this community such that readers obtain the essential tools they need to understand publications on the topic. Although we will not go into the very details and theoretical background, we aim to point readers to relevant literature and put applications of ML in molecular biology as well as the fields of pharmacometrics and clinical pharmacology into perspective.",2020-04-01,4,957,35,402
1502,32250525,Propensity score methods in real-world epidemiology: A practical guide for first-time users,"Real-world epidemiology gives us the unique opportunity to observe large numbers of people, and the actions and events that characterize their encounters with healthcare providers. However, the heterogeneity and sheer diversity of the population and healthcare systems makes it impossible for researchers to compare ""like with like"" when attempting to draw causal inferences about interventions and outcomes. The critical issue in epidemiological datasets relates to high risk of bias due to confounders that stem from baseline differences between groups. Propensity score (PS) techniques are statistical approaches that have been used to tackle potential imbalance in the comparison groups. The PS is the estimated probability (based on measured baseline covariates) that the patient receives a particular intervention. Patients that share similar PS will most likely have the same distributions of underlying covariates included in the PS. Implementation of PS methods may achieve better balance of covariates, but there is no consensus on the best way of capturing all relevant confounders for incorporation into the PS model. Should covariates be selected by clinical or epidemiological experts, or would data-driven algorithms (machine learning) offer more efficient and reliable methods of estimating PS and controlling for confounding? The PS can be incorporated into the analysis in different ways, each with its own strengths and limitations, and researchers must choose the best fit for their study objectives. PS methods are particularly advantageous in situations where there are large numbers of measured covariates but relatively few outcome events captured in healthcare administrative databases.",2020-04-01,0,1711,91,402
2178,31691863,Computer-aided diagnosis in rheumatic diseases using ultrasound: an overview,"Clinical evaluation of rheumatic and musculoskeletal diseases through images is a challenge for the beginner rheumatologist since image diagnosis is an expert task with a long learning curve. The aim of this work was to present a narrative review on the main ultrasound computer-aided diagnosis systems that may help clinicians thanks to the progress made in the application of artificial intelligence techniques. We performed a literature review searching for original articles in seven repositories, from 1970 to 2019, and identified 11 main methods currently used in ultrasound computer-aided diagnosis systems. Also, we found that rheumatoid arthritis, osteoarthritis, systemic lupus erythematosus, and idiopathic inflammatory myopathies are the four musculoskeletal and rheumatic diseases most studied that use these innovative systems, with an overall accuracy of > 75%.",2020-04-01,0,876,76,402
1551,32171918,Machine learning models for drug-target interactions: current knowledge and future directions,"Predicting the binding affinity between compounds and proteins with reasonable accuracy is crucial in drug discovery. Computational prediction of binding affinity between compounds and targets greatly enhances the probability of finding lead compounds by reducing the number of wet-lab experiments. Machine-learning and deep-learning techniques using ligand-based and target-based approaches have been used to predict binding affinities, thereby saving time and cost in drug discovery efforts. In this review, we discuss about machine-learning and deep-learning models used in virtual screening to improve drug-target interaction (DTI) prediction. We also highlight current knowledge and future directions to guide further development in this field.",2020-04-01,4,749,93,402
2158,31742424,CAD and AI for breast cancer-recent development and challenges,"Computer-aided diagnosis (CAD) has been a popular area of research and development in the past few decades. In CAD, machine learning methods and multidisciplinary knowledge and techniques are used to analyze the patient information and the results can be used to assist clinicians in their decision making process. CAD may analyze imaging information alone or in combination with other clinical data. It may provide the analyzed information directly to the clinician or correlate the analyzed results with the likelihood of certain diseases based on statistical modeling of the past cases in the population. CAD systems can be developed to provide decision support for many applications in the patient care processes, such as lesion detection, characterization, cancer staging, treatment planning and response assessment, recurrence and prognosis prediction. The new state-of-the-art machine learning technique, known as deep learning (DL), has revolutionized speech and text recognition as well as computer vision. The potential of major breakthrough by DL in medical image analysis and other CAD applications for patient care has brought about unprecedented excitement of applying CAD, or artificial intelligence (AI), to medicine in general and to radiology in particular. In this paper, we will provide an overview of the recent developments of CAD using DL in breast imaging and discuss some challenges and practical issues that may impact the advancement of artificial intelligence and its integration into clinical workflow.",2020-04-01,4,1531,62,402
953,31934891,Environmental mixtures and children's health: identifying appropriate statistical approaches,"Purpose of review:                    Biomonitoring studies have shown that children are constantly exposed to complex patterns of chemical and nonchemical exposures. Here, we briefly summarize the rationale for studying multiple exposures, also called mixture, in relation to child health and key statistical approaches that can be used. We discuss advantages over traditional methods, limitations and appropriateness of the context.              Recent findings:                    New approaches allow pediatric researchers to answer increasingly complex questions related to environmental mixtures. We present methods to identify the most relevant exposures among a high-multitude of variables, via shrinkage and variable selection techniques, and identify the overall mixture effect, via Weighted Quantile Sum and Bayesian Kernel Machine regressions. We then describe novel extensions that handle high-dimensional exposure data and allow identification of critical exposure windows.              Summary:                    Recent advances in statistics and machine learning enable researchers to identify important mixture components, estimate joint mixture effects and pinpoint critical windows of exposure. Despite many advantages over single chemical approaches, measurement error and biases may be amplified in mixtures research, requiring careful study planning and design. Future research requires increased collaboration between epidemiologists, statisticians and data scientists, and further integration with causal inference methods.",2020-04-01,1,1548,92,402
1572,32139886,Big data in digital healthcare: lessons learnt and recommendations for general practice,"Big Data will be an integral part of the next generation of technological developments-allowing us to gain new insights from the vast quantities of data being produced by modern life. There is significant potential for the application of Big Data to healthcare, but there are still some impediments to overcome, such as fragmentation, high costs, and questions around data ownership. Envisioning a future role for Big Data within the digital healthcare context means balancing the benefits of improving patient outcomes with the potential pitfalls of increasing physician burnout due to poor implementation leading to added complexity. Oncology, the field where Big Data collection and utilization got a heard start with programs like TCGA and the Cancer Moon Shot, provides an instructive example as we see different perspectives provided by the United States (US), the United Kingdom (UK) and other nations in the implementation of Big Data in patient care with regards to their centralization and regulatory approach to data. By drawing upon global approaches, we propose recommendations for guidelines and regulations of data use in healthcare centering on the creation of a unique global patient ID that can integrate data from a variety of healthcare providers. In addition, we expand upon the topic by discussing potential pitfalls to Big Data such as the lack of diversity in Big Data research, and the security and transparency risks posed by machine learning algorithms.",2020-04-01,5,1480,87,402
30,32365645,Edge Machine Learning for AI-Enabled IoT Devices: A Review,"In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors' data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning ""Hello World"".",2020-04-01,2,1400,58,402
2103,31811431,Deep learning: definition and perspectives for thoracic imaging,"Relevance and penetration of machine learning in clinical practice is a recent phenomenon with multiple applications being currently under development. Deep learning-and especially convolutional neural networks (CNNs)-is a subset of machine learning, which has recently entered the field of thoracic imaging. The structure of neural networks, organized in multiple layers, allows them to address complex tasks. For several clinical situations, CNNs have demonstrated superior performance as compared with classical machine learning algorithms and in some cases achieved comparable or better performance than clinical experts. Chest radiography, a high-volume procedure, is a natural application domain because of the large amount of stored images and reports facilitating the training of deep learning algorithms. Several algorithms for automated reporting have been developed. The training of deep learning algorithm CT images is more complex due to the dimension, variability, and complexity of the 3D signal. The role of these methods is likely to increase in clinical practice as a complement of the radiologist's expertise. The objective of this review is to provide definitions for understanding the methods and their potential applications for thoracic imaging. KEY POINTS:  Deep learning outperforms other machine learning techniques for number of tasks in radiology.  Convolutional neural network is the most popular deep learning architecture in medical imaging.  Numerous deep learning algorithms are being currently developed; some of them may become part of clinical routine in the near future.",2020-04-01,7,1610,63,402
26,32373125,Biomarkers for Allogeneic HCT Outcomes,"Allogeneic hematopoietic cell transplantation (HCT) remains the only curative therapy for many hematological malignant and non-malignant disorders. However, key obstacles to the success of HCT include graft-versus-host disease (GVHD) and disease relapse due to absence of graft-versus-tumor (GVT) effect. Over the last decade, advances in ""omics"" technologies and systems biology analysis, have allowed for the discovery and validation of blood biomarkers that can be used as diagnostic test and prognostic test (that risk-stratify patients before disease occurrence) for acute and chronic GVHD and recently GVT. There are also predictive biomarkers that categorize patients based on their likely to respond to therapy. Newer mathematical analysis such as machine learning is able to identify different predictors of GVHD using clinical characteristics pre-transplant and possibly in the future combined with other biomarkers. Biomarkers are not only useful to identify patients with higher risk of disease progression, but also help guide treatment decisions and/or provide a basis for specific therapeutic interventions. This review summarizes biomarkers definition, omics technologies, acute, chronic GVHD and GVT biomarkers currently used in clinic or with potential as targets for existing or new drugs focusing on novel published work.",2020-04-01,2,1341,38,402
11,32395545,"Bridging the ""last mile"" gap between AI implementation and operation: ""data awareness"" that matters","Interest in the application of machine learning (ML) techniques to medicine is growing fast and wide because of their ability to endow decision support systems with so-called artificial intelligence, particularly in those medical disciplines that extensively rely on digital imaging. Nonetheless, achieving a pragmatic and ecological validation of medical AI systems in real-world settings is difficult, even when these systems exhibit very high accuracy in laboratory settings. This difficulty has been called the ""last mile of implementation."" In this review of the concept, we claim that this metaphorical mile presents two chasms: the hiatus of human trust and the hiatus of machine experience. The former hiatus encompasses all that can hinder the concrete use of AI at the point of care, including availability and usability issues, but also the contradictory phenomena of cognitive ergonomics, such as automation bias (overreliance on technology) and prejudice against the machine (clearly the opposite). The latter hiatus, on the other hand, relates to the production and availability of a sufficient amount of reliable and accurate clinical data that is suitable to be the ""experience"" with which a machine can be trained. In briefly reviewing the existing literature, we focus on this latter hiatus of the last mile, as it has been largely neglected by both ML developers and doctors. In doing so, we argue that efforts to cross this chasm require data governance practices and a focus on data work, including the practices of data awareness and data hygiene. To address the challenge of bridging the chasms in the last mile of medical AI implementation, we discuss the six main socio-technical challenges that must be overcome in order to build robust bridges and deploy potentially effective AI in real-world clinical settings.",2020-04-01,3,1839,99,402
1509,32245523,Machine learning and clinical epigenetics: a review of challenges for diagnosis and classification,"Background:                    Machine learning is a sub-field of artificial intelligence, which utilises large data sets to make predictions for future events. Although most algorithms used in machine learning were developed as far back as the 1950s, the advent of big data in combination with dramatically increased computing power has spurred renewed interest in this technology over the last two decades.              Main body:                    Within the medical field, machine learning is promising in the development of assistive clinical tools for detection of e.g. cancers and prediction of disease. Recent advances in deep learning technologies, a sub-discipline of machine learning that requires less user input but more data and processing power, has provided even greater promise in assisting physicians to achieve accurate diagnoses. Within the fields of genetics and its sub-field epigenetics, both prime examples of complex data, machine learning methods are on the rise, as the field of personalised medicine is aiming for treatment of the individual based on their genetic and epigenetic profiles.              Conclusion:                    We now have an ever-growing number of reported epigenetic alterations in disease, and this offers a chance to increase sensitivity and specificity of future diagnostics and therapies. Currently, there are limited studies using machine learning applied to epigenetics. They pertain to a wide variety of disease states and have used mostly supervised machine learning methods.",2020-04-01,2,1537,98,402
1513,32243801,How Machine Learning Will Transform Biomedicine,"This Perspective explores the application of machine learning toward improved diagnosis and treatment. We outline a vision for how machine learning can transform three broad areas of biomedicine: clinical diagnostics, precision treatments, and health monitoring, where the goal is to maintain health through a range of diseases and the normal aging process. For each area, early instances of successful machine learning applications are discussed, as well as opportunities and challenges for machine learning. When these challenges are met, machine learning promises a future of rigorous, outcomes-based medicine with detection, diagnosis, and treatment strategies that are continuously adapted to individual and environmental differences.",2020-04-01,11,739,47,402
1588,32113754,Symposium review: Dairy Brain-Informing decisions on dairy farms using data analytics,"Management decisions can be informed by near-real-time data streams to improve the economics of the farm and to positively benefit the overall health of a dairy herd or the larger environment. Decision support tools can use data management services and analytics to exploit data streams from farm and other economic, health, and agricultural sources. We will describe a decision support tool that couples data analytics tools to underlying cow, herd, and economic data with an application programming interface. This interface allows the user to interact with a collection of dairy applications without fully exposing the intricacies of the underlying system model and understand the effects of different decisions on outputs of interest. The collection of these applications will form the basis of the Dairy Brain decision support system, which will provide management suggestions to farmers at a single animal or farm level. Dairy operations data will be gathered, cleaned, organized, and disseminated through an agricultural data hub, exploiting newly developed ontologies for integration of multiple data sources. Models of feed efficiency, culling, or other dairy operations (such as large capital expenditures, outsourcing opportunities, and interactions with regulators) form the basis of analytical approaches, operationalized via tools that help secure information and control uncertainties. The applications will be independently generated to provide flexibility, and use tools and modeling approaches from the data science, simulation, machine learning, and optimization disciplines to provide specific recommendations to decision makers. The Dairy Brain is a decision support system that couples data analytics tools with a suite of applications that integrate cow, herd, and economic data to inform management, operational, and animal health improving practices. Research challenges that remain include dealing with increased variability as predictions go from herd or pen level down to individual cow level and choosing the appropriate tool or technique to deal with a specific problem.",2020-04-01,1,2100,85,402
1526,32220387,Artificial Intelligence for Diagnosis of Acute Coronary Syndromes: A Meta-analysis of Machine Learning Approaches,"Background:                    Machine learning (ML) encompasses a wide variety of methods by which artificial intelligence learns to perform tasks when exposed to data. Although detection of myocardial infarction has been facilitated with introduction of troponins, the diagnosis of acute coronary syndromes (ACS) without myocardial damage (without elevation of serum troponin) remains subjective, and its accuracy remains highly dependent on clinical skills of the health care professionals. Application of a ML algorithm may expedite management of ACS for either early discharge or early initiation of ACS management. We aim to summarize the published studies of ML for diagnosis of ACS.              Methods:                    We searched electronic databases, including PubMed, Embase, and Web of Science from inception up to January 13, 2019, for studies that evaluated ML algorithms for the diagnosis of ACS in patients presenting with chest pain. We then used random-effects bivariate meta-analysis models to summarize the studies.              Results:                    We retained 9 studies that evaluated ML in a total of 6292 patients. The prevalence of ACS in the evaluated cohorts ranged from relatively rare (7%) to common (57%). The pooled sensitivity and specificity were 0.95 and 0.90, respectively. The positive predictive values ranged from 0.64 to 1.0, and the negative predictive values ranged from 0.91 to 1.0. The positive and negative likelihood ratios ranged from 1.6 to 33.0 and 0.01 to 0.13, respectively.              Conclusions:                    The excellent sensitivity, negative likelihood ratio, and negative predictive values suggest that ML may be useful as an initial triage tool for ruling out ACS.",2020-04-01,1,1742,113,402
2168,31711929,Somatic mutations - Evolution within the individual,"With the rapid advancement of sequencing technologies over the last two decades, it is becoming feasible to detect rare variants from somatic tissue samples. Studying such somatic mutations can provide deep insights into various senescence-related diseases, including cancer, inflammation, and sporadic psychiatric disorders. While it is still a difficult task to identify true somatic mutations, relentless efforts to combine experimental and computational methods have made it possible to obtain reliable data. Furthermore, state-of-the-art machine learning approaches have drastically improved the efficiency and sensitivity of these methods. Meanwhile, we can regard somatic mutations as a counterpart of germline mutations, and it is possible to apply well-formulated mathematical frameworks developed for population genetics and molecular evolution to analyze this 'somatic evolution'. For example, retrospective cell lineage tracing is a promising technique to elucidate the mechanism of pre-diseases using single-cell RNA-sequencing (scRNA-seq) data.",2020-04-01,0,1058,51,402
1586,32115658,Implementing machine learning methods for imaging flow cytometry,"In this review, we focus on the applications of machine learning methods for analyzing image data acquired in imaging flow cytometry technologies. We propose that the analysis approaches can be categorized into two groups based on the type of data, raw imaging signals or features explicitly extracted from images, being analyzed by a trained model. We hope that this categorization is helpful for understanding uniqueness, differences and opportunities when the machine learning-based analysis is implemented in recently developed 'imaging' cell sorters.",2020-04-01,0,555,64,402
1514,32240163,Multiview learning for understanding functional multiomics,"The molecular mechanisms and functions in complex biological systems currently remain elusive. Recent high-throughput techniques, such as next-generation sequencing, have generated a wide variety of multiomics datasets that enable the identification of biological functions and mechanisms via multiple facets. However, integrating these large-scale multiomics data and discovering functional insights are, nevertheless, challenging tasks. To address these challenges, machine learning has been broadly applied to analyze multiomics. This review introduces multiview learning-an emerging machine learning field-and envisions its potentially powerful applications to multiomics. In particular, multiview learning is more effective than previous integrative methods for learning data's heterogeneity and revealing cross-talk patterns. Although it has been applied to various contexts, such as computer vision and speech recognition, multiview learning has not yet been widely applied to biological data-specifically, multiomics data. Therefore, this paper firstly reviews recent multiview learning methods and unifies them in a framework called multiview empirical risk minimization (MV-ERM). We further discuss the potential applications of each method to multiomics, including genomics, transcriptomics, and epigenomics, in an aim to discover the functional and mechanistic interpretations across omics. Secondly, we explore possible applications to different biological systems, including human diseases (e.g., brain disorders and cancers), plants, and single-cell analysis, and discuss both the benefits and caveats of using multiview learning to discover the molecular mechanisms and functions of these systems.",2020-04-01,6,1713,58,402
1516,32238826,Meta-Analysis Based on Nonconvex Regularization,"The widespread applications of high-throughput sequencing technology have produced a large number of publicly available gene expression datasets. However, due to the gene expression datasets have the characteristics of small sample size, high dimensionality and high noise, the application of biostatistics and machine learning methods to analyze gene expression data is a challenging task, such as the low reproducibility of important biomarkers in different studies. Meta-analysis is an effective approach to deal with these problems, but the current methods have some limitations. In this paper, we propose the meta-analysis based on three nonconvex regularization methods, which are L1/2 regularization (meta-Half), Minimax Concave Penalty regularization (meta-MCP) and Smoothly Clipped Absolute Deviation regularization (meta-SCAD). The three nonconvex regularization methods are effective approaches for variable selection developed in recent years. Through the hierarchical decomposition of coefficients, our methods not only maintain the flexibility of variable selection and improve the efficiency of selecting important biomarkers, but also summarize and synthesize scientific evidence from multiple studies to consider the relationship between different datasets. We give the efficient algorithms and the theoretical property for our methods. Furthermore, we apply our methods to the simulation data and three publicly available lung cancer gene expression datasets, and compare the performance with state-of-the-art methods. Our methods have good performance in simulation studies, and the analysis results on the three publicly available lung cancer gene expression datasets are clinically meaningful. Our methods can also be extended to other areas where datasets are heterogeneous.",2020-04-01,0,1796,47,402
2148,31755816,Sepsis 2019: What Surgeons Need to Know,"The definition of sepsis continues to be as dynamic as the management strategies used to treat this. Sepsis-3 has replaced the earlier systemic inflammatory response syndrome (SIRS)-based diagnoses with the rapid Sequential Organ Failure Assessment (SOFA) score assisting in predicting overall prognosis with regards to mortality. Surgeons have an important role in ensuring adequate source control while recognizing the threat of carbapenem-resistance in gram-negative organisms. Rapid diagnostic tests are being used increasingly for the early identification of multi-drug-resistant organisms (MDROs), with a key emphasis on the multidisciplinary alert of results. Novel, higher generation antibiotic agents have been developed for resistance in ESKCAPE (Enterococcus faecium, Staphylococcus aureus, Klebsiella pneumoniae, Acinetobacter baumannii, Pseudomonas aeruginosa, and Enterobacter species) organisms while surgeons have an important role in the prevention of spread. The Study to Optimize Peritoneal Infection Therapy (STOP-IT) trial has challenged the previous paradigm of length of antibiotic treatment whereas biomarkers such as procalcitonin are playing a prominent role in individualizing therapy. Several novel therapies for refractory septic shock, while still investigational, are gaining prominence rapidly (such as vitamin C) whereas others await further clinical trials. Management strategies presented as care bundles continue to be updated by the Surviving Sepsis Campaign, yet still remain controversial in its global adoption. We have broadened our temporal and epidemiologic perspective of sepsis by understanding it both as an acute, time-sensitive, life-threatening illness to a chronic condition that increases the risk of mortality up to five years post-discharge. Artificial intelligence, machine learning, and bedside scoring systems can assist the clinician in predicting post-operative sepsis. The public health role of the surgeon is key. This includes collaboration and multi-disciplinary antibiotic stewardship at a hospital level. It also requires controlling pharmaceutical sales and the unregulated dispensing of antibiotic agents globally through policy initiatives to control emerging resistance through prevention.",2020-04-01,0,2257,39,402
2119,31789682,Possibility of Deep Learning in Medical Imaging Focusing Improvement of Computed Tomography Image Quality,"Deep learning (DL), part of a broader family of machine learning methods, is based on learning data representations rather than task-specific algorithms. Deep learning can be used to improve the image quality of clinical scans with image noise reduction. We review the ability of DL to reduce the image noise, present the advantages and disadvantages of computed tomography image reconstruction, and examine the potential value of new DL-based computed tomography image reconstruction.",2020-04-01,3,485,105,402
1518,32233691,Thermal ablation of biological tissues in disease treatment: A review of computational models and future directions,"Percutaneous thermal ablation has proven to be an effective modality for treating both benign and malignant tumours in various tissues. Among these modalities, radiofrequency ablation (RFA) is the most promising and widely adopted approach that has been extensively studied in the past decades. Microwave ablation (MWA) is a newly emerging modality that is gaining rapid momentum due to its capability of inducing rapid heating and attaining larger ablation volumes, and its lesser susceptibility to the heat sink effects as compared to RFA. Although the goal of both these therapies is to attain cell death in the target tissue by virtue of heating above 50C, their underlying mechanism of action and principles greatly differs. Computational modelling is a powerful tool for studying the effect of electromagnetic interactions within the biological tissues and predicting the treatment outcomes during thermal ablative therapies. Such a priori estimation can assist the clinical practitioners during treatment planning with the goal of attaining successful tumour destruction and preservation of the surrounding healthy tissue and critical structures. This review provides current state-of-the-art developments and associated challenges in the computational modelling of thermal ablative techniques, viz., RFA and MWA, as well as touch upon several promising avenues in the modelling of laser ablation, nanoparticles assisted magnetic hyperthermia and non-invasive RFA. The application of RFA in pain relief has been extensively reviewed from modelling point of view. Additionally, future directions have also been provided to improve these models for their successful translation and integration into the hospital work flow.",2020-04-01,3,1728,115,402
1508,32246300,Myths and facts about artificial intelligence: why machine- and deep-learning will not replace interventional radiologists,"Artificial intelligence (AI) is revolutionizing healthcare and transforming the clinical practice of physicians across the world. Radiology has a strong affinity for machine learning and is at the forefront of the paradigm shift, as machines compete with humans for cognitive abilities. AI is a computer science simulation of the human mind that utilizes algorithms based on collective human knowledge and the best available evidence to process various forms of inputs and deliver desired outcomes, such as clinical diagnoses and optimal treatment options. Despite the overwhelmingly positive uptake of the technology, warnings have been published about the potential dangers of AI. Concerns have been expressed reflecting opinions that future medicine based on AI will render radiologists irrelevant. Thus, how much of this is based on reality? To answer these questions, it is important to examine the facts, clarify where AI really stands and why many of these speculations are untrue. We aim to debunk the 6 top myths regarding AI in the future of radiologists.",2020-04-01,3,1065,122,402
1566,32143918,Review of computational methods for the detection and classification of polyps in colonoscopy imaging,"Computer-aided diagnosis (CAD) is a tool with great potential to help endoscopists in the tasks of detecting and histologically classifying colorectal polyps. In recent years, different technologies have been described and their potential utility has been increasingly evidenced, which has generated great expectations among scientific societies. However, most of these works are retrospective and use images of different quality and characteristics which are analysed off line. This review aims to familiarise gastroenterologists with computational methods and the particularities of endoscopic imaging, which have an impact on image processing analysis. Finally, the publicly available image databases, needed to compare and confirm the results obtained with different methods, are presented.",2020-04-01,0,794,101,402
49,32344674,"Predictive Maintenance for Pump Systems and Thermal Power Plants: State-of-the-Art Review, Trends and Challenges","Thermal power plants are an important asset in the current energy infrastructure, delivering ancillary services, power, and heat to their respective consumers. Faults on critical components, such as large pumping systems, can lead to material damage and opportunity losses. Pumps plays an essential role in various industries and as such clever maintenance can ensure cost reductions and high availability. Prognostics and Health Management, PHM, is the study utilizing data to estimate the current and future conditions of a system. Within the field of PHM, Predictive Maintenance, PdM, has been gaining increased attention. Data-driven models can be built to estimate the remaining-useful-lifetime of complex systems that would be difficult to identify by man. With the increased attention that the Predictive Maintenance field is receiving, review papers become increasingly important to understand what research has been conducted and what challenges need to be addressed. This paper does so by initially conceptualising the PdM field. A structured overview of literature in regard to application within PdM is presented, before delving into the domain of thermal power plants and pump systems. Finally, related challenges and trends will be outlined. This paper finds that a large number of experimental data-driven models have been successfully deployed, but the PdM field would benefit from more industrial case studies. Furthermore, investigations into the scale-ability of models would benefit industries that are looking into large-scale implementations. Here, examining a method for automatic maintenance of the developed model will be of interest. This paper can be used to understand the PdM field as a broad concept but does also provide a niche understanding of the domain in focus.",2020-04-01,1,1797,112,402
1239,32086017,Technological advances in field studies of pollinator ecology and the future of e-ecology,"Our review looks at recent advances in technologies applied to studying pollinators in the field. These include RFID, radar and lidar for detecting and tracking pollinators; wireless sensor networks (e.g. 'smart' hives); automated visual and audio monitoring systems including vision motion software for monitoring fine-scale pollinator behaviours over extended periods; and automated species identification systems based on machine learning that can vastly reduce the bottleneck in (big) data analysis. An improved e-ecology platform that leverages these tools is needed for ecologists to acquire and understand large spatiotemporal datasets, and thus inform knowledge gaps in environmental policy-making. Developing the next generation of e-ecology tools will require synergistic partnerships between academia and industry and significant investment in a cross-disciplinary scientific consortia.",2020-04-01,2,897,89,402
70,32316682,Machine Learning Approaches for Quality Assessment of Protein Structures,"Protein structures play a very important role in biomedical research, especially in drug discovery and design, which require accurate protein structures in advance. However, experimental determinations of protein structure are prohibitively costly and time-consuming, and computational predictions of protein structures have not been perfected. Methods that assess the quality of protein models can help in selecting the most accurate candidates for further work. Driven by this demand, many structural bioinformatics laboratories have developed methods for estimating model accuracy (EMA). In recent years, EMA by machine learning (ML) have consistently ranked among the top-performing methods in the community-wide CASP challenge. Accordingly, we systematically review all the major ML-based EMA methods developed within the past ten years. The methods are grouped by their employed ML approach-support vector machine, artificial neural networks, ensemble learning, or Bayesian learning-and their significances are discussed from a methodology viewpoint. To orient the reader, we also briefly describe the background of EMA, including the CASP challenge and its evaluation metrics, and introduce the major ML/DL techniques. Overall, this review provides an introductory guide to modern research on protein quality assessment and directions for future research in this area.",2020-04-01,0,1375,72,402
1300,32004537,"Transitioning Chemistry, Manufacturing, and Controls Content With a Structured Data Management Solution: Streamlining Regulatory Submissions","The process of assembling regulatory documents for submission to multiple global health agencies can present a repetitive cycle of authoring, editing, and data verification, which increases in complexity as changes are made for approved products, particularly from a chemistry, manufacturing, and controls (CMC) perspective. Currently, pharmaceutical companies rely on a workflow that involves manual CMC change management across documents. Similarly, when regulators review submissions, they provide feedback and insight into regulatory decision making in a narrative format. As accelerated review pathways are increasingly used and pressure mounts to bring products to market quickly, innovative solutions for assembling, distributing, and reviewing regulatory information are being considered. Structured content management (SCM) solutions, in which data are collated into centrally organized content blocks for use across different documents, may aid in the efficient processing of data and create opportunities for automation and machine learning in its interpretation. The US Food and Drug Administration (FDA) has recently created initiatives that encourage application of SCM for CMC data, though many challenges could impede their success and efficiency. The goal is for industry and health authorities to collaborate in the development of SCM for CMC applications, to potentially streamline compilation of quality data in regulatory submissions.",2020-04-01,1,1455,140,402
73,32313813,Different fundus imaging modalities and technical factors in AI screening for diabetic retinopathy: a review,"Background:                    Effective screening is a desirable method for the early detection and successful treatment for diabetic retinopathy, and fundus photography is currently the dominant medium for retinal imaging due to its convenience and accessibility. Manual screening using fundus photographs has however involved considerable costs for patients, clinicians and national health systems, which has limited its application particularly in less-developed countries. The advent of artificial intelligence, and in particular deep learning techniques, has however raised the possibility of widespread automated screening.              Main text:                    In this review, we first briefly survey major published advances in retinal analysis using artificial intelligence. We take care to separately describe standard multiple-field fundus photography, and the newer modalities of ultra-wide field photography and smartphone-based photography. Finally, we consider several machine learning concepts that have been particularly relevant to the domain and illustrate their usage with extant works.              Conclusions:                    In the ophthalmology field, it was demonstrated that deep learning tools for diabetic retinopathy show clinically acceptable diagnostic performance when using colour retinal fundus images. Artificial intelligence models are among the most promising solutions to tackle the burden of diabetic retinopathy management in a comprehensive manner. However, future research is crucial to assess the potential clinical deployment, evaluate the cost-effectiveness of different DL systems in clinical practice and improve clinical acceptance.",2020-04-01,3,1690,108,402
1229,32101448,Radiomics: from qualitative to quantitative imaging,"Historically, medical imaging has been a qualitative or semi-quantitative modality. It is difficult to quantify what can be seen in an image, and to turn it into valuable predictive outcomes. As a result of advances in both computational hardware and machine learning algorithms, computers are making great strides in obtaining quantitative information from imaging and correlating it with outcomes. Radiomics, in its two forms ""handcrafted and deep,"" is an emerging field that translates medical images into quantitative data to yield biological information and enable radiologic phenotypic profiling for diagnosis, theragnosis, decision support, and monitoring. Handcrafted radiomics is a multistage process in which features based on shape, pixel intensities, and texture are extracted from radiographs. Within this review, we describe the steps: starting with quantitative imaging data, how it can be extracted, how to correlate it with clinical and biological outcomes, resulting in models that can be used to make predictions, such as survival, or for detection and classification used in diagnostics. The application of deep learning, the second arm of radiomics, and its place in the radiomics workflow is discussed, along with its advantages and disadvantages. To better illustrate the technologies being used, we provide real-world clinical applications of radiomics in oncology, showcasing research on the applications of radiomics, as well as covering its limitations and its future direction.",2020-04-01,13,1505,51,402
74,32313006,Translating research findings into clinical practice: a systematic and critical review of neuroimaging-based clinical tools for brain disorders,"A pivotal aim of psychiatric and neurological research is to promote the translation of the findings into clinical practice to improve diagnostic and prognostic assessment of individual patients. Structural neuroimaging holds much promise, with neuroanatomical measures accounting for up to 40% of the variance in clinical outcome. Building on these findings, a number of imaging-based clinical tools have been developed to make diagnostic and prognostic inferences about individual patients from their structural Magnetic Resonance Imaging scans. This systematic review describes and compares the technical characteristics of the available tools, with the aim to assess their translational potential into real-world clinical settings. The results reveal that a total of eight tools. All of these were specifically developed for neurological disorders, and as such are not suitable for application to psychiatric disorders. Furthermore, most of the tools were trained and validated in a single dataset, which can result in poor generalizability, or using a small number of individuals, which can cause overoptimistic results. In addition, all of the tools rely on two strategies to detect brain abnormalities in single individuals, one based on univariate comparison, and the other based on multivariate machine-learning algorithms. We discuss current barriers to the adoption of these tools in clinical practice and propose a checklist of pivotal characteristics that should be included in an ""ideal"" neuroimaging-based clinical tool for brain disorders.",2020-04-01,3,1555,143,402
1225,32111372,Electronic health records for the diagnosis of rare diseases,"With the emergence of electronic health records, the reuse of clinical data offers new perspectives for the diagnosis and management of patients with rare diseases. However, there are many obstacles to the repurposing of clinical data. The development of decision support systems depends on the ability to recruit patients, extract and integrate the patients' data, mine and stratify these data, and integrate the decision support algorithm into patient care. This last step requires an adaptability of the electronic health records to integrate learning health system tools. In this literature review, we examine the research that provides solutions to unlock these barriers and accelerate translational research: structured electronic health records and free-text search engines to find patients, data warehouses and natural language processing to extract phenotypes, machine learning algorithms to classify patients, and similarity metrics to diagnose patients. Medical informatics is experiencing an impellent request to develop decision support systems, and this requires ethical considerations for clinicians and patients to ensure appropriate use of health data.",2020-04-01,2,1169,60,402
2468,30706370,Meta-analysis of the moral brain: patterns of neural engagement assessed using multilevel kernel density analysis,"The neuroimaging literature in moral cognition has rapidly developed in the last decade with more than 200 publications on the topic. Neuroimaging based models generally agree that limbic regions work with medial prefrontal and temporal regions during moral processing to integrate emotional, social, and cognitive elements into decision-making. However, no quantitative work has been done examining neural response across types of moral cognition tasks. This paper uses Multilevel Kernel Density Analysis (MKDA) to conduct neuroimaging meta-analyses of the moral cognitive literature. MKDA replicated previous findings of the neural correlates of moral cognition: the left amygdala, medial prefrontal cortex, bilateral temporoparietal junction, and posterior cingulate. Random forest algorithms classified neural features as belonging to simple/utilitarian moral dilemmas, explicit/implicit moral tasks, and word/picture moral stimuli tasks; in combination with univariate contrast analyses, these results indicated a distinct pattern of processing for each of the members of these paradigm pairs. Overall, the results emphasize that the task selected for use in a moral cognition neuroimaging study is vital for the elicitation and interpretation of results. It also replicates and re-establishes the neural basis for moral processing, especially important in light of implementation errors in previous meta-analysis.",2020-04-01,0,1419,113,402
938,31957003,Will Artificial Intelligence for Drug Discovery Impact Clinical Pharmacology?,"As the field of artificial intelligence and machine learning (AI/ML) for drug discovery is rapidly advancing, we address the question ""What is the impact of recent AI/ML trends in the area of Clinical Pharmacology?"" We address difficulties and AI/ML developments for target identification, their use in generative chemistry for small molecule drug discovery, and the potential role of AI/ML in clinical trial outcome evaluation. We briefly discuss current trends in the use of AI/ML in health care and the impact of AI/ML context of the daily practice of clinical pharmacologists.",2020-04-01,5,580,77,402
81,32299466,Diagnosis support systems for rare diseases: a scoping review,"Introduction:                    Rare diseases affect approximately 350 million people worldwide. Delayed diagnosis is frequent due to lack of knowledge of most clinicians and a small number of expert centers. Consequently, computerized diagnosis support systems have been developed to address these issues, with many relying on rare disease expertise and taking advantage of the increasing volume of generated and accessible health-related data. Our objective is to perform a review of all initiatives aiming to support the diagnosis of rare diseases.              Methods:                    A scoping review was conducted based on methods proposed by Arksey and O'Malley. A charting form for relevant study analysis was developed and used to categorize data.              Results:                    Sixty-eight studies were retained at the end of the charting process. Diagnosis targets varied from 1 rare disease to all rare diseases. Material used for diagnosis support consisted mostly of phenotype concepts, images or fluids. Fifty-seven percent of the studies used expert knowledge. Two-thirds of the studies relied on machine learning algorithms, and one-third used simple similarities. Manual algorithms were encountered as well. Most of the studies presented satisfying performance of evaluation by comparison with references or with external validation. Fourteen studies provided online tools, most of which aimed to support the diagnosis of all rare diseases by considering queries based on phenotype concepts.              Conclusion:                    Numerous solutions relying on different materials and use of various methodologies are emerging with satisfying preliminary results. However, the variability of approaches and evaluation processes complicates the comparison of results. Efforts should be made to adequately validate these tools and guarantee reproducibility and explicability.",2020-04-01,1,1911,61,402
1245,32073494,The rise and fall of the model for end-stage liver disease score and the need for an optimized machine learning approach for liver allocation,"Purpose of review:                    The Model for End-Stage Liver Disease (MELD) has been used to rank liver transplant candidates since 2002, and at the time bringing much needed objectivity to the liver allocation process. However, and despite numerous revisions to the MELD score, current liver allocation still does not allow for equitable access to all waitlisted liver candidates.              Recent findings:                    An optimized prediction of mortality (OPOM) was developed utilizing novel machine-learning optimal classification tree models trained to predict a liver candidate's 3-month waitlist mortality or removal. When compared to MELD and MELD-Na, OPOM more accurately and objectively prioritized candidates for liver transplantation based on disease severity. In simulation analysis, OPOM allowed for more equitable allocation of livers with a resultant significant number of additional lives saved every year when compared with MELD-based allocation.              Summary:                    Machine learning technology holds the potential to help guide transplant clinical practice, and thus potentially guide national organ allocation policy.",2020-04-01,0,1175,141,402
2609,32547805,Quantified Self-Using Consumer Wearable Device: Predicting Physical and Mental Health,"Objectives:                    Recently, wearable device technology has gained more popularity in supporting a healthy lifestyle. Hence, researchers have begun to put significant efforts into studying the direct and indirect benefits of wearable devices for health and wellbeing. This paper summarizes recent studies on the use of consumer wearable devices to improve physical activity, mental health, and health consciousness.              Methods:                    A thorough literature search was performed from several reputable databases, such as PubMed, Scopus, ScienceDirect, arXiv, and bioRxiv mainly using ""wearable device research"" as a keyword, no earlier than 2018. As a result, 25 of the most recent and relevant papers included in this review cover several topics, such as previous literature reviews (9 papers), wearable device accuracy (3 papers), self-reported data collection tools (3 papers), and wearable device intervention (10 papers).              Results:                    All the chosen studies are discussed based on the wearable device used, complementary data, study design, and data processing method. All these previous studies indicate that wearable devices are used either to validate their benefits for general wellbeing or for more serious medical contexts, such as cardiovascular disorders and post-stroke treatment.              Conclusions:                    Despite their huge potential for adoption in clinical settings, wearable device accuracy and validity remain the key challenge to be met. Some lessons learned and future projections, such as combining traditional study design with statistical and machine learning methods, are highlighted in this paper to provide a useful overview for other researchers carrying out similar research.",2020-04-01,0,1785,85,402
941,31954954,Computational approaches for detection of cardiac rhythm abnormalities: Are we there yet?,"The analysis of an electrocardiogram (ECG) is able to provide vital information on the electrical activity of the heart and is crucial for the accurate diagnosis of cardiac arrhythmias. Due to the nature of some arrhythmias, this might be a time-consuming and difficult to accomplish process. The advent of novel machine learning technologies in this field has a potential to revolutionise the use of the ECG. In this review, we outline key advances in ECG analysis for atrial, ventricular and complex multiform arrhythmias, as well as discuss the current limitations of the technology and the barriers that must be overcome before clinical integration is feasible.",2020-04-01,1,665,89,402
1320,31972477,Machine learning approaches for analyzing and enhancing molecular dynamics simulations,"Molecular dynamics (MD) has become a powerful tool for studying biophysical systems, due to increasing computational power and availability of software. Although MD has made many contributions to better understanding these complex biophysical systems, there remain methodological difficulties to be surmounted. First, how to make the deluge of data generated in running even a microsecond long MD simulation human comprehensible. Second, how to efficiently sample the underlying free energy surface and kinetics. In this short perspective, we summarize machine learning based ideas that are solving both of these limitations, with a focus on their key theoretical underpinnings and remaining challenges.",2020-04-01,5,703,86,402
1397,33071613,Recent Development of Machine Learning Methods in Microbial Phosphorylation Sites,"A variety of protein post-translational modifications has been identified that control many cellular functions. Phosphorylation studies in mycobacterial organisms have shown critical importance in diverse biological processes, such as intercellular communication and cell division. Recent technical advances in high-precision mass spectrometry have determined a large number of microbial phosphorylated proteins and phosphorylation sites throughout the proteome analysis. Identification of phosphorylated proteins with specific modified residues through experimentation is often labor-intensive, costly and time-consuming. All these limitations could be overcome through the application of machine learning (ML) approaches. However, only a limited number of computational phosphorylation site prediction tools have been developed so far. This work aims to present a complete survey of the existing ML-predictors for microbial phosphorylation. We cover a variety of important aspects for developing a successful predictor, including operating ML algorithms, feature selection methods, window size, and software utility. Initially, we review the currently available phosphorylation site databases of the microbiome, the state-of-the-art ML approaches, working principles, and their performances. Lastly, we discuss the limitations and future directions of the computational ML methods for the prediction of phosphorylation.",2020-04-01,0,1421,81,402
86,32293466,Machine learning applied to retinal image processing for glaucoma detection: review and perspective,"Introduction:                    This is a systematic review on the main algorithms using machine learning (ML) in retinal image processing for glaucoma diagnosis and detection. ML has proven to be a significant tool for the development of computer aided technology. Furthermore, secondary research has been widely conducted over the years for ophthalmologists. Such aspects indicate the importance of ML in the context of retinal image processing.              Methods:                    The publications that were chosen to compose this review were gathered from Scopus, PubMed, IEEEXplore and Science Direct databases. Then, the papers published between 2014 and 2019 were selected . Researches that used the segmented optic disc method were excluded. Moreover, only the methods which applied the classification process were considered. The systematic analysis was performed in such studies and, thereupon, the results were summarized.              Discussion:                    Based on architectures used for ML in retinal image processing, some studies applied feature extraction and dimensionality reduction to detect and isolate important parts of the analyzed image. Differently, other works utilized a deep convolutional network. Based on the evaluated researches, the main difference between the architectures is the number of images demanded for processing and the high computational cost required to use deep learning techniques.              Conclusions:                    All the analyzed publications indicated it was possible to develop an automated system for glaucoma diagnosis. The disease severity and its high occurrence rates justify the researches which have been carried out. Recent computational techniques, such as deep learning, have shown to be promising technologies in fundus imaging. Although such a technique requires an extensive database and high computational costs, the studies show that the data augmentation and transfer learning techniques have been applied as an alternative way to optimize and reduce networks training.",2020-04-01,1,2064,99,402
87,32290639,Physiological and Behavior Monitoring Systems for Smart Healthcare Environments: A Review,"Healthcare optimization has become increasingly important in the current era, where numerous challenges are posed by population ageing phenomena and the demand for higher quality of the healthcare services. The implementation of Internet of Things (IoT) in the healthcare ecosystem has been one of the best solutions to address these challenges and therefore to prevent and diagnose possible health impairments in people. The remote monitoring of environmental parameters and how they can cause or mediate any disease, and the monitoring of human daily activities and physiological parameters are among the vast applications of IoT in healthcare, which has brought extensive attention of academia and industry. Assisted and smart tailored environments are possible with the implementation of such technologies that bring personal healthcare to any individual, while living in their preferred environments. In this paper we address several requirements for the development of such environments, namely the deployment of physiological signs monitoring systems, daily activity recognition techniques, as well as indoor air quality monitoring solutions. The machine learning methods that are most used in the literature for activity recognition and body motion analysis are also referred. Furthermore, the importance of physical and cognitive training of the elderly population through the implementation of exergames and immersive environments is also addressed.",2020-04-01,4,1459,89,402
88,32285013,The need for a system view to regulate artificial intelligence/machine learning-based software as medical device,"Artificial intelligence (AI) and Machine learning (ML) systems in medicine are poised to significantly improve health care, for example, by offering earlier diagnoses of diseases or recommending optimally individualized treatment plans. However, the emergence of AI/ML in medicine also creates challenges, which regulators must pay attention to. Which medical AI/ML-based products should be reviewed by regulators? What evidence should be required to permit marketing for AI/ML-based software as a medical device (SaMD)? How can we ensure the safety and effectiveness of AI/ML-based SaMD that may change over time as they are applied to new data? The U.S. Food and Drug Administration (FDA), for example, has recently proposed a discussion paper to address some of these issues. But it misses an important point: we argue that regulators like the FDA need to widen their scope from evaluating medical AI/ML-based products to assessing systems. This shift in perspective-from a product view to a system view-is central to maximizing the safety and efficacy of AI/ML in health care, but it also poses significant challenges for agencies like the FDA who are used to regulating products, not systems. We offer several suggestions for regulators to make this challenging but important transition.",2020-04-01,5,1292,112,402
942,31954511,From Summary Statistics to Gene Trees: Methods for Inferring Positive Selection,"Methods to detect signals of natural selection from genomic data have traditionally emphasized the use of simple summary statistics. Here, we review a new generation of methods that consider combinations of conventional summary statistics and/or richer features derived from inferred gene trees and ancestral recombination graphs (ARGs). We also review recent advances in methods for population genetic simulation and ARG reconstruction. Finally, we describe opportunities for future work on a variety of related topics, including the genetics of speciation, estimation of selection coefficients, and inference of selection on polygenic traits. Together, these emerging methods offer promising new directions in the study of natural selection.",2020-04-01,2,743,79,402
91,32276469,"Transcriptomics in Toxicogenomics, Part III: Data Modelling for Risk Assessment","Transcriptomics data are relevant to address a number of challenges in Toxicogenomics (TGx). After careful planning of exposure conditions and data preprocessing, the TGx data can be used in predictive toxicology, where more advanced modelling techniques are applied. The large volume of molecular profiles produced by omics-based technologies allows the development and application of artificial intelligence (AI) methods in TGx. Indeed, the publicly available omics datasets are constantly increasing together with a plethora of different methods that are made available to facilitate their analysis, interpretation and the generation of accurate and stable predictive models. In this review, we present the state-of-the-art of data modelling applied to transcriptomics data in TGx. We show how the benchmark dose (BMD) analysis can be applied to TGx data. We review read across and adverse outcome pathways (AOP) modelling methodologies. We discuss how network-based approaches can be successfully employed to clarify the mechanism of action (MOA) or specific biomarkers of exposure. We also describe the main AI methodologies applied to TGx data to create predictive classification and regression models and we address current challenges. Finally, we present a short description of deep learning (DL) and data integration methodologies applied in these contexts. Modelling of TGx data represents a valuable tool for more accurate chemical safety assessment. This review is the third part of a three-article series on Transcriptomics in Toxicogenomics.",2020-04-01,3,1555,79,402
92,32276442,Integration of Novel Sensors and Machine Learning for Predictive Maintenance in Medium Voltage Switchgear to Enable the Energy and Mobility Revolutions,"The development of renewable energies and smart mobility has profoundly impacted the future of the distribution grid. An increasing bidirectional energy flow stresses the assets of the distribution grid, especially medium voltage switchgear. This calls for improved maintenance strategies to prevent critical failures. Predictive maintenance, a maintenance strategy relying on current condition data of assets, serves as a guideline. Novel sensors covering thermal, mechanical, and partial discharge aspects of switchgear, enable continuous condition monitoring of some of the most critical assets of the distribution grid. Combined with machine learning algorithms, the demands put on the distribution grid by the energy and mobility revolutions can be handled. In this paper, we review the current state-of-the-art of all aspects of condition monitoring for medium voltage switchgear. Furthermore, we present an approach to develop a predictive maintenance system based on novel sensors and machine learning. We show how the existing medium voltage grid infrastructure can adapt these new needs on an economic scale.",2020-04-01,1,1118,151,402
96,32269464,Machine Learning Methods for Precision Medicine Research Designed to Reduce Health Disparities: A Structured Tutorial,"Precision medicine research designed to reduce health disparities often involves studying multi-level datasets to understand how diseases manifest disproportionately in one group over another, and how scarce health care resources can be directed precisely to those most at risk for disease. In this article, we provide a structured tutorial for medical and public health researchers on the application of machine learning methods to conduct precision medicine research designed to reduce health disparities. We review key terms and concepts for understanding machine learning papers, including supervised and unsupervised learning, regularization, cross-validation, bagging, and boosting. Metrics are reviewed for evaluating machine learners and major families of learning approaches, including tree-based learning, deep learning, and ensemble learning. We highlight the advantages and disadvantages of different learning approaches, describe strategies for interpreting ""black box"" models, and demonstrate the application of common methods in an example dataset with open-source statistical code in R.",2020-04-01,0,1102,117,402
1316,31982325,Next-generation drug repurposing using human genetics and network biology,"Drug repurposing has attracted increased attention, especially in the context of drug discovery rates that remain too low despite a recent wave of approvals for biological therapeutics (e.g. gene therapy). These new biological entities-based treatments have high costs that are difficult to justify for small markets that include rare diseases. Drug repurposing, involving the identification of single or combinations of existing drugs based on human genetics data and network biology approaches represents a next-generation approach that has the potential to increase the speed of drug discovery at a lower cost. This Pharmacological Perspective reviews progress and perspectives in combining human genetics, especially genome-wide association studies, with network biology to drive drug repurposing for rare and common diseases with monogenic or polygenic etiologies. Also, highlighted here are important features of this next generation approach to drug repurposing, which can be combined with machine learning methods to meet the challenges of personalized medicine.",2020-04-01,12,1070,73,402
2676,32425581,Challenges and Future Prospects of Precision Medicine in Psychiatry,"Precision medicine is increasingly recognized as a promising approach to improve disease treatment, taking into consideration the individual clinical and biological characteristics shared by specific subgroups of patients. In specific fields such as oncology and hematology, precision medicine has already started to be implemented in the clinical setting and molecular testing is routinely used to select treatments with higher efficacy and reduced adverse effects. The application of precision medicine in psychiatry is still in its early phases. However, there are already examples of predictive models based on clinical data or combinations of clinical, neuroimaging and biological data. While the power of single clinical predictors would remain inadequate if analyzed only with traditional statistical approaches, these predictors are now increasingly used to impute machine learning models that can have adequate accuracy even in the presence of relatively small sample size. These models have started to be applied to disentangle relevant clinical questions that could lead to a more effective management of psychiatric disorders, such as prediction of response to the mood stabilizer lithium, resistance to antidepressants in major depressive disorder or stratification of the risk and outcome prediction in schizophrenia. In this narrative review, we summarized the most important findings in precision medicine in psychiatry based on studies that constructed machine learning models using clinical, neuroimaging and/or biological data. Limitations and barriers to the implementation of precision psychiatry in the clinical setting, as well as possible solutions and future perspectives, will be presented.",2020-04-01,3,1716,67,402
69,32317161,Biomarkers for posttraumatic epilepsy,"A biomarker is a characteristic that can be objectively measured as an indicator of normal biologic processes, pathogenic processes, or responses to an exposure or intervention, including therapeutic interventions. Biomarker modalities include molecular, histologic, radiographic, or physiologic characteristics. To improve the understanding and use of biomarker terminology in biomedical research, clinical practice, and medical product development, the Food and Drug Administration (FDA)-National Institutes of Health (NIH) Joint Leadership Council developed the BEST Resource (Biomarkers, EndpointS, and other Tools). The seven BEST biomarker categories include the following: (a) susceptibility/risk biomarkers, (b) diagnostic biomarkers, (c) monitoring biomarkers, (d) prognostic biomarkers, (e) predictive biomarkers, (f) pharmacodynamic/response biomarkers, and (g) safety biomarkers. We hypothesize some potential overlap between the reported biomarkers of traumatic brain injury (TBI), epilepsy, and posttraumatic epilepsy (PTE). Here, we tested this hypothesis by reviewing studies focusing on biomarker discovery for posttraumatic epileptogenesis and epilepsy. The biomarker modalities reviewed here include plasma/serum and cerebrospinal fluid molecular biomarkers, imaging biomarkers, and electrophysiologic biomarkers. Most of the reported biomarkers have an area under the receiver operating characteristic curve greater than 0.800, suggesting both high sensitivity and high specificity. Our results revealed little overlap in the biomarker candidates between TBI, epilepsy, and PTE. In addition to using single parameters as biomarkers, machine learning approaches have highlighted the potential for utilizing patterns of markers as biomarkers. Although published data suggest the possibility of identifying biomarkers for PTE, we are still in the early phase of the development curve. Many of the seven biomarker categories lack PTE-related biomarkers. Thus, further exploration using proper, statistically powered, and standardized study designs with validation cohorts, and by developing and applying novel analytical methods, is needed for PTE biomarker discovery.",2020-04-01,0,2184,37,402
66,32318338,Computational Oncology in the Multi-Omics Era: State of the Art,"Cancer is the quintessential complex disease. As technologies evolve faster each day, we are able to quantify the different layers of biological elements that contribute to the emergence and development of malignancies. In this multi-omics context, the use of integrative approaches is mandatory in order to gain further insights on oncological phenomena, and to move forward toward the precision medicine paradigm. In this review, we will focus on computational oncology as an integrative discipline that incorporates knowledge from the mathematical, physical, and computational fields to further the biomedical understanding of cancer. We will discuss the current roles of computation in oncology in the context of multi-omic technologies, which include: data acquisition and processing; data management in the clinical and research settings; classification, diagnosis, and prognosis; and the development of models in the research setting, including their use for therapeutic target identification. We will discuss the machine learning and network approaches as two of the most promising emerging paradigms, in computational oncology. These approaches provide a foundation on how to integrate different layers of biological description into coherent frameworks that allow advances both in the basic and clinical settings.",2020-04-01,9,1323,63,402
1259,32056910,Feeling down? A systematic review of the gut microbiota in anxiety/depression and irritable bowel syndrome,"Background Anxiety/depression and irritable bowel syndrome (IBS) are highly prevalent and burdensome conditions, whose co-occurrence is estimated between 44 and 84%. Shared gut microbiota alterations have been identified in these separate disorders relative to controls; however, studies have not adequately considered their comorbidity. This review set out to identify case-control studies comparing the gut microbiota in anxiety/depression, IBS, and both conditions comorbidly relative to each other and to controls, as well as gut microbiota investigations including measures of both IBS and anxiety/depression. Methods Four databases were systematically searched using comprehensive search terms (OVID Medline, Embase, PsycINFO, and PubMed), following PRISMA guidelines. Results Systematic review identified 17 studies (10 human, 7 animal). Most studies investigated the gut microbiota and anxiety/depression symptoms in IBS cohorts. Participants with IBS and high anxiety/depression symptoms had lower alpha diversity compared to controls and IBS-only cohorts. Machine learning and beta diversity distinguished between IBS participants with and without anxiety/depression by their gut microbiota. Comorbid IBS and anxiety/depression also had higher abundance of Proteobacteria, Prevotella/Prevotellaceae, Bacteroides and lower Lachnospiraceae relative to controls. Limitations A large number of gut microbiota estimation methods and statistical techniques were utilized; therefore, meta-analysis was not possible. Conclusions Well-designed case-control and longitudinal studies are required to disentangle whether the gut microbiota is predicted as a continuum of gastrointestinal and anxiety/depression symptom severity, or whether reported dysbiosis is unique to IBS and anxiety/depression comorbidity. These findings may inform the development of targeted treatment through the gut microbiota for individuals with both anxiety/depression and IBS.",2020-04-01,7,1954,106,402
1258,32057098,Methylation-based algorithms for diagnosis: experience from neuro-oncology,"Brain tumours are the most common tumour-related cause of death in young people. Survivors are at risk of significant disability, at least in part related to the effects of treatment. Therefore, there is a need for a precise diagnosis that stratifies patients for the most suitable treatment, matched to the underlying biology of their tumour. Although traditional histopathology has been accurate in predicting treatment responses in many cases, molecular profiling has revealed a remarkable, previously unappreciated, level of biological complexity in the classification of these tumours. Among different molecular technologies, DNA methylation profiling has had the most pronounced impact on brain tumour classification. Furthermore, using machine learning-based algorithms, DNA methylation profiling is changing diagnostic practice. This can be regarded as an exemplar for how molecular pathology can influence diagnostic practice and illustrates some of the unanticipated benefits and risks.  2020 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.",2020-04-01,1,1091,74,402
2629,32514496,Evaluation of liver tumour response by imaging,"The goal of assessing tumour response on imaging is to identify patients who are likely to benefit - or not - from anticancer treatment, especially in relation to survival. The World Health Organization was the first to develop assessment criteria. This early score, which assessed tumour burden by standardising lesion size measurements, laid the groundwork for many of the criteria that followed. This was then improved by the Response Evaluation Criteria in Solid Tumours (RECIST) which was quickly adopted by the oncology community. At the same time, many interventional oncology treatments were developed to target specific features of liver tumours that result in significant changes in tumours but have little effect on tumour size. New criteria focusing on the viable part of tumours were therefore designed to provide more appropriate feedback to guide patient management. Targeted therapy has resulted in a breakthrough that challenges conventional response criteria due to the non-linear relationship between response and tumour size, requiring the development of methods that emphasize the appearance of tumours. More recently, research into functional and quantitative imaging has created new opportunities in liver imaging. These results have suggested that certain parameters could serve as early predictors of response or could predict later tumour response at baseline. These approaches have now been extended by machine learning and deep learning. This clinical review focuses on the progress made in the evaluation of liver tumours on imaging, discussing the rationale for this approach, addressing challenges and controversies in the field, and suggesting possible future developments.",2020-04-01,5,1705,46,402
40,32354020,A Review of Deep Learning Methods for Antibodies,"Driven by its successes across domains such as computer vision and natural language processing, deep learning has recently entered the field of biology by aiding in cellular image classification, finding genomic connections, and advancing drug discovery. In drug discovery and protein engineering, a major goal is to design a molecule that will perform a useful function as a therapeutic drug. Typically, the focus has been on small molecules, but new approaches have been developed to apply these same principles of deep learning to biologics, such as antibodies. Here we give a brief background of deep learning as it applies to antibody drug development, and an in-depth explanation of several deep learning algorithms that have been proposed to solve aspects of both protein design in general, and antibody design in particular.",2020-04-01,1,832,48,402
42,32351968,"Malnutrition, Health and the Role of Machine Learning in Clinical Setting","Nutrition plays a vital role in health and the recovery process. Deficiencies in macronutrients and micronutrients can impact the development and progression of various disorders. However, malnutrition screening tools and their utility in the clinical setting remain largely understudied. In this study, we summarize the importance of nutritional adequacy and its association with neurological, cardiovascular, and immune-related disorders. We also examine general and specific malnutrition assessment tools utilized in healthcare settings. Since the implementation of the screening process in 2016, malnutrition data from hospitalized patients in the Geisinger Health System is presented and discussed as a case study. Clinical data from five Geisinger hospitals shows that ~10% of all admitted patients are acknowledged for having some form of nutritional deficiency, from which about 60-80% of the patients are targeted for a more comprehensive assessment. Finally, we conclude that with a reflection on how technological advances, specifically machine learning-based algorithms, can be integrated into electronic health records to provide decision support system to care providers in the identification and management of patients at higher risk of malnutrition.",2020-04-01,2,1265,73,402
854,31347226,Magnetic resonance fingerprinting review part 2: Technique and directions,"Magnetic resonance fingerprinting (MRF) is a general framework to quantify multiple MR-sensitive tissue properties with a single acquisition. There have been numerous advances in MRF in the years since its inception. In this work we highlight some of the recent technical developments in MRF, focusing on sequence optimization, modifications for reconstruction and pattern matching, new methods for partial volume analysis, and applications of machine and deep learning. Level of Evidence: 2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:993-1007.",2020-04-01,7,561,73,402
43,32351543,Reaching the End-Game for GWAS: Machine Learning Approaches for the Prioritization of Complex Disease Loci,"Genome-wide association studies (GWAS) have revealed thousands of genetic loci that underpin the complex biology of many human traits. However, the strength of GWAS - the ability to detect genetic association by linkage disequilibrium (LD) - is also its limitation. Whilst the ever-increasing study size and improved design have augmented the power of GWAS to detect effects, differentiation of causal variants or genes from other highly correlated genes associated by LD remains the real challenge. This has severely hindered the biological insights and clinical translation of GWAS findings. Although thousands of disease susceptibility loci have been reported, causal genes at these loci remain elusive. Machine learning (ML) techniques offer an opportunity to dissect the heterogeneity of variant and gene signals in the post-GWAS analysis phase. ML models for GWAS prioritization vary greatly in their complexity, ranging from relatively simple logistic regression approaches to more complex ensemble models such as random forests and gradient boosting, as well as deep learning models, i.e., neural networks. Paired with functional validation, these methods show important promise for clinical translation, providing a strong evidence-based approach to direct post-GWAS research. However, as ML approaches continue to evolve to meet the challenge of causal gene identification, a critical assessment of the underlying methodologies and their applicability to the GWAS prioritization problem is needed. This review investigates the landscape of ML applications in three parts: selected models, input features, and output model performance, with a focus on prioritizations of complex disease associated loci. Overall, we explore the contributions ML has made towards reaching the GWAS end-game with consequent wide-ranging translational impact.",2020-04-01,4,1848,106,402
45,32349232,Real-Time Hand Gesture Recognition Using Surface Electromyography and Machine Learning: A Systematic Literature Review,"Today, daily life is composed of many computing systems, therefore interacting with them in a natural way makes the communication process more comfortable. Human-Computer Interaction (HCI) has been developed to overcome the communication barriers between humans and computers. One form of HCI is Hand Gesture Recognition (HGR), which predicts the class and the instant of execution of a given movement of the hand. One possible input for these models is surface electromyography (EMG), which records the electrical activity of skeletal muscles. EMG signals contain information about the intention of movement generated by the human brain. This systematic literature review analyses the state-of-the-art of real-time hand gesture recognition models using EMG data and machine learning. We selected and assessed 65 primary studies following the Kitchenham methodology. Based on a common structure of machine learning-based systems, we analyzed the structure of the proposed models and standardized concepts in regard to the types of models, data acquisition, segmentation, preprocessing, feature extraction, classification, postprocessing, real-time processing, types of gestures, and evaluation metrics. Finally, we also identified trends and gaps that could open new directions of work for future research in the area of gesture recognition using EMG.",2020-04-01,5,1351,118,402
1255,32060219,Introduction to Radiomics,"Radiomics is a rapidly evolving field of research concerned with the extraction of quantitative metrics-the so-called radiomic features-within medical images. Radiomic features capture tissue and lesion characteristics such as heterogeneity and shape and may, alone or in combination with demographic, histologic, genomic, or proteomic data, be used for clinical problem solving. The goal of this continuing education article is to provide an introduction to the field, covering the basic radiomics workflow: feature calculation and selection, dimensionality reduction, and data processing. Potential clinical applications in nuclear medicine that include PET radiomics-based prediction of treatment response and survival will be discussed. Current limitations of radiomics, such as sensitivity to acquisition parameter variations, and common pitfalls will also be covered.",2020-04-01,14,873,25,402
1251,32062767,Big data and data processing in rheumatology: bioethical perspectives,"Big data analytics and processing through artificial intelligence (AI) are increasingly being used in the health sector. This includes both clinical and research settings, and newly in specialties like rheumatology. It is, however, important to consider how these new methodologies are used, and particularly the sensitivities associated with personal information. Based on current applications in rheumatology, this article provides a narrative review of the bioethical perspectives of big data. It presents examples of databases, data analytic methods, and AI in this specialty to address four main ethical issues: privacy and confidentiality, informed consent, the impact on the medical profession, and justice. The use of big data and AI processing in healthcare has great potential to improve the quality of clinical care, including through better diagnosis, treatment, and prognosis. They may also increase patient and societal participation and engagement in healthcare and research. Developing these methodologies and using the information generated from them in line with ethical standards could positively affect the design of global health policies and introduce a new phase in the democratization of health.Key Points Current applications of big data, data analytics, and AI in rheumatology-including registries, machine learning algorithms, and consumer-facing platforms-raise issues in four main bioethical areas: privacy and confidentiality, informed consent, the impact on the medical profession, and justice. Bioethical concerns about rheumatology registries require careful consideration of privacy provisions, set within the context of local, national, and regional law. Machine learning and big data aid diagnosis, treatment, and prognosis, but the final decision about the use of information from algorithms should be left to rheumatology specialists to maintain the promise of fiduciary obligations in the physician-patient relationship. International collaboration in big data projects and increased patient engagement could be ways to counteract health inequalities in the practice of rheumatology, even on a global scale.",2020-04-01,2,2150,69,402
67,32318028,"The Computational Diet: A Review of Computational Methods Across Diet, Microbiome, and Health","Food and human health are inextricably linked. As such, revolutionary impacts on health have been derived from advances in the production and distribution of food relating to food safety and fortification with micronutrients. During the past two decades, it has become apparent that the human microbiome has the potential to modulate health, including in ways that may be related to diet and the composition of specific foods. Despite the excitement and potential surrounding this area, the complexity of the gut microbiome, the chemical composition of food, and their interplay in situ remains a daunting task to fully understand. However, recent advances in high-throughput sequencing, metabolomics profiling, compositional analysis of food, and the emergence of electronic health records provide new sources of data that can contribute to addressing this challenge. Computational science will play an essential role in this effort as it will provide the foundation to integrate these data layers and derive insights capable of revealing and understanding the complex interactions between diet, gut microbiome, and health. Here, we review the current knowledge on diet-health-gut microbiota, relevant data sources, bioinformatics tools, machine learning capabilities, as well as the intellectual property and legislative regulatory landscape. We provide guidance on employing machine learning and data analytics, identify gaps in current methods, and describe new scenarios to be unlocked in the next few years in the context of current knowledge.",2020-04-01,5,1549,93,402
48,32346642,How Far Will Clinical Application of AI Applications Advance for Colorectal Cancer Diagnosis?,"Integrating artificial intelligence (AI) applications into colonoscopy practice is being accelerated as deep learning technologies emerge. In this field, most of the preceding research has focused on polyp detection and characterization, which can mitigate inherent human errors accompanying colonoscopy procedures. On the other hand, more challenging research areas are currently capturing attention: the automated prediction of invasive cancers. Colorectal cancers (CRCs) harbor potential lymph node metastasis when they invade deeply into submucosal layers, which should be resected surgically rather than endoscopically. However, pretreatment discrimination of deeply invasive submucosal CRCs is considered difficult, according to previous prospective studies (e.g., <70% sensitivity), leading to an increased number of unnecessary surgeries for large adenomas or slightly invasive submucosal CRCs. AI is now expected to overcome this challenging hurdle because it is considered to provide better performance in predicting invasive cancer than non-expert endoscopists. In this review, we introduce five relevant publications in this area. Unfortunately, progress in this research area is in a very preliminary phase, compared to that of automated polyp detection and characterization, because of the lack of number of invasive CRCs used for machine learning. However, this issue will be overcome with more target images and cases. The research field of AI for invasive CRCs is just starting but could be a game changer of patient care in the near future, given rapidly growing technologies, and research will gradually increase.",2020-04-01,1,1632,93,402
50,32340618,Imaging biomarkers in neurodegeneration: current and future practices,"There is an increasing role for biological markers (biomarkers) in the understanding and diagnosis of neurodegenerative disorders. The application of imaging biomarkers specifically for the in vivo investigation of neurodegenerative disorders has increased substantially over the past decades and continues to provide further benefits both to the diagnosis and understanding of these diseases. This review forms part of a series of articles which stem from the University College London/University of Gothenburg course ""Biomarkers in neurodegenerative diseases"". In this review, we focus on neuroimaging, specifically positron emission tomography (PET) and magnetic resonance imaging (MRI), giving an overview of the current established practices clinically and in research as well as new techniques being developed. We will also discuss the use of machine learning (ML) techniques within these fields to provide additional insights to early diagnosis and multimodal analysis.",2020-04-01,5,976,69,402
52,32339122,A review of computational approaches for evaluation of rehabilitation exercises,"Recent advances in data analytics and computer-aided diagnostics stimulate the vision of patient-centric precision healthcare, where treatment plans are customized based on the health records and needs of every patient. In physical rehabilitation, the progress in machine learning and the advent of affordable and reliable motion capture sensors have been conducive to the development of approaches for automated assessment of patient performance and progress toward functional recovery. The presented study reviews computational approaches for evaluating patient performance in rehabilitation programs using motion capture systems. Such approaches will play an important role in supplementing traditional rehabilitation assessment performed by trained clinicians, and in assisting patients participating in home-based rehabilitation. The reviewed computational methods for exercise evaluation are grouped into three main categories: discrete movement score, rule-based, and template-based approaches. The review places an emphasis on the application of machine learning methods for movement evaluation in rehabilitation. Related work in the literature on data representation, feature engineering, movement segmentation, and scoring functions is presented. The study also reviews existing sensors for capturing rehabilitation movements and provides an informative listing of pertinent benchmark datasets. The significance of this paper is in being the first to provide a comprehensive review of computational methods for evaluation of patient performance in rehabilitation programs.",2020-04-01,3,1582,79,402
57,32331327,"Emotion Recognition Using Eye-Tracking: Taxonomy, Review and Current Challenges","The ability to detect users' emotions for the purpose of emotion engineering is currently one of the main endeavors of machine learning in affective computing. Among the more common approaches to emotion detection are methods that rely on electroencephalography (EEG), facial image processing and speech inflections. Although eye-tracking is fast in becoming one of the most commonly used sensor modalities in affective computing, it is still a relatively new approach for emotion detection, especially when it is used exclusively. In this survey paper, we present a review on emotion recognition using eye-tracking technology, including a brief introductory background on emotion modeling, eye-tracking devices and approaches, emotion stimulation methods, the emotional-relevant features extractable from eye-tracking data, and most importantly, a categorical summary and taxonomy of the current literature which relates to emotion recognition using eye-tracking. This review concludes with a discussion on the current open research problems and prospective future research directions that will be beneficial for expanding the body of knowledge in emotion detection using eye-tracking as the primary sensor modality.",2020-04-01,4,1217,79,402
1286,32022759,Machine Intelligence in Cardiovascular Medicine,"The computer science technology trend called artificial intelligence (AI) is not new. Both machine learning and deep learning AI applications have recently begun to impact cardiovascular medicine. Scientists working in the AI domain have long recognized the importance of data quality and provenance to AI algorithm efficiency and accuracy. A diverse array of cardiovascular raw data sources of variable quality-electronic medical records, radiological picture archiving and communication systems, laboratory results, omics, etc.-are available to train AI algorithms for predictive modeling of clinical outcomes (in-hospital mortality, acute coronary syndrome risk stratification, etc.), accelerated image interpretation (edge detection, tissue characterization, etc.) and enhanced phenotyping of heterogeneous conditions (heart failure with preserved ejection fraction, hypertension, etc.). A number of software as medical device narrow AI products for cardiac arrhythmia characterization and advanced image deconvolution are now Food and Drug Administration approved, and many others are in the pipeline. Present and future health professionals using AI-infused analytics and wearable devices have 3 critical roles to play in their informed development and ethical application in practice: (1) medical domain experts providing clinical context to computer and data scientists, (2) data stewards assuring the quality, relevance and provenance of data inputs, and (3) real-time and post-hoc interpreters of AI black box solutions and recommendations to patients. The next wave of so-called contextual adaption AI technologies will more closely approximate human decision-making, potentially augmenting cardiologists' real-time performance in emergency rooms, catheterization laboratories, imaging suites, and clinics. However, before such higher order AI technologies are adopted in the clinical setting and by healthcare systems, regulatory agencies, and industry must jointly develop robust AI standards of practice and transparent technology insertion rule sets.",2020-04-01,0,2065,47,402
58,32326049,Bioinformatics Methods for Mass Spectrometry-Based Proteomics Data Analysis,"Recent advances in mass spectrometry (MS)-based proteomics have enabled tremendous progress in the understanding of cellular mechanisms, disease progression, and the relationship between genotype and phenotype. Though many popular bioinformatics methods in proteomics are derived from other omics studies, novel analysis strategies are required to deal with the unique characteristics of proteomics data. In this review, we discuss the current developments in the bioinformatics methods used in proteomics and how they facilitate the mechanistic understanding of biological processes. We first introduce bioinformatics software and tools designed for mass spectrometry-based protein identification and quantification, and then we review the different statistical and machine learning methods that have been developed to perform comprehensive analysis in proteomics studies. We conclude with a discussion of how quantitative protein data can be used to reconstruct protein interactions and signaling networks.",2020-04-01,3,1008,75,402
788,33022628,Electronic skins and machine learning for intelligent soft robots,"Soft robots have garnered interest for real-world applications because of their intrinsic safety embedded at the material level. These robots use deformable materials capable of shape and behavioral changes and allow conformable physical contact for manipulation. Yet, with the introduction of soft and stretchable materials to robotic systems comes a myriad of challenges for sensor integration, including multimodal sensing capable of stretching, embedment of high-resolution but large-area sensor arrays, and sensor fusion with an increasing volume of data. This Review explores the emerging confluence of e-skins and machine learning, with a focus on how roboticists can combine recent developments from the two fields to build autonomous, deployable soft robots, integrated with capabilities for informative touch and proprioception to stand up to the challenges of real-world environments.",2020-04-01,2,895,65,402
64,32323066,Artificial Intelligence in radiotherapy: state of the art and future directions,"Recent advances in computing capability allowed the development of sophisticated predictive models to assess complex relationships within observational data, described as Artificial Intelligence. Medicine is one of the several fields of application and Radiation oncology could benefit from these approaches, particularly in patients' medical records, imaging, baseline pathology, planning or instrumental data. Artificial Intelligence systems could simplify many steps of the complex workflow of radiotherapy such as segmentation, planning or delivery. However, Artificial Intelligence could be considered as a ""black box"" in which human operator may only understand input and output predictions and its application to the clinical practice remains a challenge. The low transparency of the overall system is questionable from manifold points of view (ethical included). Given the complexity of this issue, we collected the basic definitions to help the clinician to understand current literature, and overviewed experiences regarding implementation of AI within radiotherapy clinical workflow, aiming to describe this field from the clinician perspective.",2020-04-01,3,1156,79,402
1287,32022730,ICU management based on big data,"Purpose of review:                    The availability of large datasets and computational power has prompted a revolution in Intensive Care. Data represent a great opportunity for clinical practice, benchmarking, and research. Machine learning algorithms can help predict events in a way the human brain can simply not process. This possibility comes with benefits and risks for the clinician, as finding associations does not mean proving causality.              Recent findings:                    Current applications of Data Science still focus on data documentation and visualization, and on basic rules to identify critical lab values. Recently, algorithms have been put in place for prediction of outcomes such as length of stay, mortality, and development of complications. These results have begun being implemented for more efficient allocation of resources and in benchmarking processes, to allow identification of successful practices and margins for improvement. In parallel, machine learning models are increasingly being applied in research to expand medical knowledge.              Summary:                    Data have always been part of the work of intensivists, but the current availability has not been completely exploited. The intensive care community has to embrace and guide the data science revolution in order to decline it in favor of patients' care.",2020-04-01,0,1379,32,402
2641,32499001,Automated machine learning: Review of the state-of-the-art and opportunities for healthcare,"Objective:                    This work aims to provide a review of the existing literature in the field of automated machine learning (AutoML) to help healthcare professionals better utilize machine learning models ""off-the-shelf"" with limited data science expertise. We also identify the potential opportunities and barriers to using AutoML in healthcare, as well as existing applications of AutoML in healthcare.              Methods:                    Published papers, accompanied with code, describing work in the field of AutoML from both a computer science perspective or a biomedical informatics perspective were reviewed. We also provide a short summary of a series of AutoML challenges hosted by ChaLearn.              Results:                    A review of 101 papers in the field of AutoML revealed that these automated techniques can match or improve upon expert human performance in certain machine learning tasks, often in a shorter amount of time. The main limitation of AutoML at this point is the ability to get these systems to work efficiently on a large scale, i.e. beyond small- and medium-size retrospective datasets.              Discussion:                    The utilization of machine learning techniques has the demonstrated potential to improve health outcomes, cut healthcare costs, and advance clinical research. However, most hospitals are not currently deploying machine learning solutions. One reason for this is that health care professionals often lack the machine learning expertise that is necessary to build a successful model, deploy it in production, and integrate it with the clinical workflow. In order to make machine learning techniques easier to apply and to reduce the demand for human experts, automated machine learning (AutoML) has emerged as a growing field that seeks to automatically select, compose, and parametrize machine learning models, so as to achieve optimal performance on a given task and/or dataset.              Conclusion:                    While there have already been some use cases of AutoML in the healthcare field, more work needs to be done in order for there to be widespread adoption of AutoML in healthcare.",2020-04-01,9,2187,91,402
65,32322599,Application of machine learning in ophthalmic imaging modalities,"In clinical ophthalmology, a variety of image-related diagnostic techniques have begun to offer unprecedented insights into eye diseases based on morphological datasets with millions of data points. Artificial intelligence (AI), inspired by the human multilayered neuronal system, has shown astonishing success within some visual and auditory recognition tasks. In these tasks, AI can analyze digital data in a comprehensive, rapid and non-invasive manner. Bioinformatics has become a focus particularly in the field of medical imaging, where it is driven by enhanced computing power and cloud storage, as well as utilization of novel algorithms and generation of data in massive quantities. Machine learning (ML) is an important branch in the field of AI. The overall potential of ML to automatically pinpoint, identify and grade pathological features in ocular diseases will empower ophthalmologists to provide high-quality diagnosis and facilitate personalized health care in the near future. This review offers perspectives on the origin, development, and applications of ML technology, particularly regarding its applications in ophthalmic imaging modalities.",2020-04-01,2,1164,64,402
1265,32049743,Localizing the epileptogenic zone,"Purpose of review:                    Epilepsy surgery is the therapy of choice for 30-40% of people with focal drug-resistant epilepsy. Currently only 60% of well selected patients become postsurgically seizure-free underlining the need for better tools to identify the epileptogenic zone. This article reviews the latest neurophysiological advances for EZ localization with emphasis on ictal EZ identification, interictal EZ markers, and noninvasive neurophysiological mapping procedures.              Recent findings:                    We will review methods for computerized EZ assessment, summarize computational network approaches for outcome prediction and individualized surgical planning. We will discuss electrical stimulation as an option to reduce the time needed for presurgical work-up. We will summarize recent research regarding high-frequency oscillations, connectivity measures, and combinations of multiple markers using machine learning. This latter was shown to outperform single markers. The role of NREM sleep for best identification of the EZ interictally will be discussed. We will summarize recent large-scale studies using electrical or magnetic source imaging for clinical decision-making.              Summary:                    New approaches based on technical advancements paired with artificial intelligence are on the horizon for better EZ identification. They are ultimately expected to result in a more efficient, less invasive, and less time-demanding presurgical investigation.",2020-04-01,2,1518,33,402
997,31863465,Beyond the Randomized Clinical Trial: Innovative Data Science to Close the Pediatric Evidence Gap,"Despite the application of advanced statistical and pharmacometric approaches to pediatric trial data, a large pediatric evidence gap still remains. Here, we discuss how to collect more data from children by using real-world data from electronic health records, mobile applications, wearables, and social media. The large datasets collected with these approaches enable and may demand the use of artificial intelligence and machine learning to allow the data to be analyzed for decision making. Applications of this approach are presented, which include the prediction of future clinical complications, medical image analysis, identification of new pediatric end points and biomarkers, the prediction of treatment nonresponders, and the prediction of placebo-responders for trial enrichment. Finally, we discuss how to bring machine learning from science to pediatric clinical practice. We conclude that advantage should be taken of the current opportunities offered by innovations in data science and machine learning to close the pediatric evidence gap.",2020-04-01,2,1055,97,402
2678,32424119,AI for social good: unlocking the opportunity for positive impact,"Advances in machine learning (ML) and artificial intelligence (AI) present an opportunity to build better tools and solutions to help address some of the world's most pressing challenges, and deliver positive social impact in accordance with the priorities outlined in the United Nations' 17 Sustainable Development Goals (SDGs). The AI for Social Good (AI4SG) movement aims to establish interdisciplinary partnerships centred around AI applications towards SDGs. We provide a set of guidelines for establishing successful long-term collaborations between AI researchers and application-domain experts, relate them to existing AI4SG projects and identify key opportunities for future AI applications targeted towards social good.",2020-05-01,0,729,65,372
2610,32547596,Gene Regulatory Network Inference: Connecting Plant Biology and Mathematical Modeling,"Plant responses to environmental and intrinsic signals are tightly controlled by multiple transcription factors (TFs). These TFs and their regulatory connections form gene regulatory networks (GRNs), which provide a blueprint of the transcriptional regulations underlying plant development and environmental responses. This review provides examples of experimental methodologies commonly used to identify regulatory interactions and generate GRNs. Additionally, this review describes network inference techniques that leverage gene expression data to predict regulatory interactions. These computational and experimental methodologies yield complex networks that can identify new regulatory interactions, driving novel hypotheses. Biological properties that contribute to the complexity of GRNs are also described in this review. These include network topology, network size, transient binding of TFs to DNA, and competition between multiple upstream regulators. Finally, this review highlights the potential of machine learning approaches to leverage gene expression data to predict phenotypic outputs.",2020-05-01,1,1103,85,372
2673,32429394,Application and Algorithm of Ground-Penetrating Radar for Plant Root Detection: A Review,"Attention to the natural environment is equivalent to observing the space in which we live. Plant roots, which are important organs of plants, require our close attention. The method of detecting root system without damaging plants has gradually become mainstream. At the same time, machine learning has been achieving good results in recent years; it has helped develop many tools to help us detect the underground environment of plants. Therefore, this article will introduce some existing content related to root detection technology and machine detection algorithms for root detection, proving that machine learning root detection technology has good recognition capabilities.",2020-05-01,0,680,88,372
2618,32528977,Mini Review: Deep Learning for Atrial Segmentation From Late Gadolinium-Enhanced MRIs,"Segmentation and 3D reconstruction of the human atria is of crucial importance for precise diagnosis and treatment of atrial fibrillation, the most common cardiac arrhythmia. However, the current manual segmentation of the atria from medical images is a time-consuming, labor-intensive, and error-prone process. The recent emergence of artificial intelligence, particularly deep learning, provides an alternative solution to the traditional methods that fail to accurately segment atrial structures from clinical images. This has been illustrated during the recent 2018 Atrial Segmentation Challenge for which most of the challengers developed deep learning approaches for atrial segmentation, reaching high accuracy (>90% Dice score). However, as significant discrepancies exist between the approaches developed, many important questions remain unanswered, such as which deep learning architectures and methods to ensure reliability while achieving the best performance. In this paper, we conduct an in-depth review of the current state-of-the-art of deep learning approaches for atrial segmentation from late gadolinium-enhanced MRIs, and provide critical insights for overcoming the main hindrances faced in this task.",2020-05-01,0,1221,85,372
2619,32528777,Artificial Intelligence in Modern Medicine - The Evolving Necessity of the Present and Role in Transforming the Future of Medical Care,"The dexterity of computer systems to resemble and mimic human intelligence is artificial intelligence. Artificial intelligence has reformed the diagnostic and therapeutic precision and competence in various fields of medicine. Artificial intelligence appears to play a bright role in medical diagnosis. Computer systems using artificial intelligence help in the assessment of medical images and enormous data. This research aims to identify how artificial intelligence-based technology is reforming the art of medicine. Artificial intelligence empowers providers in improving efficiency and overall healthcare. Newer machine learning techniques lead the automatic diagnostic systems. Areas of medicine such as medical imaging, automated clinical decision-making support have made significant advances with respect to artificial intelligence technology. With improved diagnosis and prognosis, artificial intelligence possesses the capability to revolutionize various fields of medicine. Artificial intelligence has its own limitations and cannot replace a bedside clinician. In the evolving modern medical digital world, physicians need to support artificial intelligence rather than fear it replacing trained physicians for improved healthcare.",2020-05-01,2,1244,134,372
2630,32514380,"Artificial Intelligence, Data Sensors and Interconnectivity: Future Opportunities for Heart Failure","A higher proportion of patients with heart failure have benefitted from a wide and expanding variety of sensor-enabled implantable devices than any other patient group. These patients can now also take advantage of the ever-increasing availability and affordability of consumer electronics. Wearable, on- and near-body sensor technologies, much like implantable devices, generate massive amounts of data. The connectivity of all these devices has created opportunities for pooling data from multiple sensors - so-called interconnectivity - and for artificial intelligence to provide new diagnostic, triage, risk-stratification and disease management insights for the delivery of better, more personalised and cost-effective healthcare. Artificial intelligence is also bringing important and previously inaccessible insights from our conventional cardiac investigations. The aim of this article is to review the convergence of artificial intelligence, sensor technologies and interconnectivity and the way in which this combination is set to change the care of patients with heart failure.",2020-05-01,1,1088,99,372
2175,31696270,Initial experience with 3D CT cinematic rendering of acute pancreatitis and associated complications,"Inflammation of the pancreas can present with a wide range of imaging findings from mild enlargement of the gland and surrounding infiltrative fat stranding through extensive glandular necrosis. Complications of pancreatitis are varied and include infected fluid collections, pseudocysts, and vascular findings such as pseudoaneurysms and thromboses. Cross-sectional imaging with computed tomography (CT) is one of the mainstays of evaluating patients with pancreatitis. New methods that allow novel visualization volumetric CT data may improve diagnostic yield for the detection of findings that provide prognostic information in pancreatitis patients or can drive new avenues of research such as machine learning. Cinematic rendering (CR) is a photorealistic visualization method for volumetric imaging data that are being investigated for a variety of potential applications including the life-like display of complex anatomy and visual characterization of mass lesions. In this review, we describe the CR appearance of different types of pancreatitis and complications of pancreatitis. We also note possible future directions for research into the utility of CR for pancreatitis.",2020-05-01,0,1183,100,372
2635,32510054,On the Interpretability of Artificial Intelligence in Radiology: Challenges and Opportunities,"As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI ""interpretable"" have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists' opinions on the topic and suggests trends and challenges that need to be addressed to effectively streamline interpretability methods in clinical practice. Supplemental material is available for this article.  RSNA, 2020 See also the commentary by Gastounioti and Kontos in this issue.",2020-05-01,12,796,93,372
2636,32509998,"Aging - Oxidative stress, antioxidants and computational modeling","Aging is a degenerative, biological, time-dependent, universally conserved process thus designed as one of the highest known risk factors for morbidity and mortality. Every individual has its own aging mechanisms as both environmental conditions (75%) and genetics (25%) account for aging. Several theories have been proposed until now but not even a single theory solves this mystery. There are still some queries un-answered to the scientific community regarding mechanisms behind aging. However, oxidative stress theory (OST) is considered one of the famous theories that sees mitochondria as one of the leading organelles which largely contribute to the aging process. Many reactive oxygen species (ROS) are produced endogenously and exogenously that are associated with aging. But the mitochondrial ROS contribute largely to the aging process as mitochondrial dysfunction due to oxidative stress is considered one of the contributors toward aging. Although ROS is known to damage cell machinery, new evidence suggests their role in signal transduction to regulate biological and physiological processes. Moreover, besides mitochondria, other important cell organelles such as peroxisome and endoplasmic reticulum also produce ROS that contribute to aging. However, nature has provided humans with free radical scavengers called antioxidants that protect from harmful effects of ROS. Future predictions regarding aging, biochemical mechanisms involved, biomarkers internal and external factors can be easily done with machine learning algorithms and other computational models. This review explains important aspects of aging, the contribution of ROS producing organelles in aging, importance of antioxidants fighting against ROS, different computational models developed to understand the complexities of the aging.",2020-05-01,6,1820,65,372
2170,31706869,Artificial Intelligence and Machine Learning in Cardiovascular Health Care,"Background:                    This review article provides an overview of artificial intelligence (AI) and machine learning (ML) as it relates to cardiovascular health care.              Methods:                    An overview of the terminology and algorithms used in ML as it relates to health care are provided by the author. Articles published up to August 1, 2019, in the field of AI and ML in cardiovascular medicine are also reviewed and placed in the context of the potential role these approaches will have in clinical practice in the future.              Results:                    AI is a broader term referring to the ability of machines to perform intelligent tasks, and ML is a subset of AI that refers to the ability of machines to learn independently and make accurate predictions. An expanding body of literature has been published using ML in cardiovascular health care. Moreover, ML has been applied in the settings of automated imaging interpretation, natural language processing and data extraction from electronic health records, and predictive analytics. Examples include automated interpretation of chest roentgenograms, electrocardiograms, echocardiograms, and angiography; identification of patients with early heart failure using clinical notes evaluated by ML; and predicting mortality or complications following percutaneous or surgical cardiovascular procedures.              Conclusions:                    Although there is an expanding body of literature on AI and ML in cardiovascular medicine, the future these fields will have in clinical practice remains to be paved. In particular, there is a promising role in providing automated imaging interpretation, automated data extraction and quality control, and clinical risk prediction, although these techniques require further refinement and evaluation.",2020-05-01,4,1840,74,372
2640,32500035,Next Generation Sequencing and Machine Learning Technologies Are Painting the Epigenetic Portrait of Glioblastoma,"Even with a rare occurrence of only 1.35% of cancer cases in the United States of America, brain tumors are considered as one of the most lethal malignancies. The most aggressive and invasive type of brain tumor, glioblastoma, accounts for 60-70% of all gliomas and presents with life expectancy of only 12-18 months. Despite trimodal treatment and advances in diagnostic and therapeutic methods, there are no significant changes in patient outcome. Our understanding of glioblastoma was significantly improved with the introduction of next generation sequencing technologies. This led to the identification of different genetic and molecular subtypes, which greatly improve glioblastoma diagnosis. Still, because of the poor life expectancy, novel diagnostic, and treatment methods are broadly explored. Epigenetic modifications like methylation and changes in histone acetylation are such examples. Recently, in addition to genetic and molecular characteristics, epigenetic profiling of glioblastomas is also used for sample classification. Further advancement of next generation sequencing technologies is expected to identify in detail the epigenetic signature of glioblastoma that can open up new therapeutic opportunities for glioblastoma patients. This should be complemented with the use of computational power i.e., machine and deep learning algorithms for objective diagnostics and design of individualized therapies. Using a combination of phenotypic, genotypic, and epigenetic parameters in glioblastoma diagnostics will bring us closer to precision medicine where therapies will be tailored to suit the genetic profile and epigenetic signature of the tumor, which will grant longer life expectancy and better quality of life. Still, a number of obstacles including potential bias, availability of data for minorities in heterogeneous populations, data protection, and validation and independent testing of the learning algorithms have to be overcome on the way.",2020-05-01,1,1974,113,372
2646,32486411,Advances in Smart Environment Monitoring Systems Using IoT and Sensors,"Air quality, water pollution, and radiation pollution are major factors that pose genuine challenges in the environment. Suitable monitoring is necessary so that the world can achieve sustainable growth, by maintaining a healthy society. In recent years, the environment monitoring has turned into a smart environment monitoring (SEM) system, with the advances in the internet of things (IoT) and the development of modern sensors. Under this scenario, the present manuscript aims to accomplish a critical review of noteworthy contributions and research studies on SEM, that involve monitoring of air quality, water quality, radiation pollution, and agriculture systems. The review is divided on the basis of the purposes where SEM methods are applied, and then each purpose is further analyzed in terms of the sensors used, machine learning techniques involved, and classification methods used. The detailed analysis follows the extensive review which has suggested major recommendations and impacts of SEM research on the basis of discussion results and research trends analyzed. The authors have critically studied how the advances in sensor technology, IoT and machine learning methods make environment monitoring a truly smart monitoring system. Finally, the framework of robust methods of machine learning; denoising methods and development of suitable standards for wireless sensor networks (WSNs), has been suggested.",2020-05-01,2,1425,70,372
2648,32482370,Enhancing sepsis management through machine learning techniques: A review,"Sepsis is a major public health problem and a leading cause of death in the world, where delay in the beginning of treatment, along with clinical guidelines non-adherence have been proved to be associated with higher mortality. Machine Learning is increasingly being adopted in developing innovative Clinical Decision Support Systems in many areas of medicine, showing a great potential for automatic prediction of diverse patient conditions, as well as assistance in clinical decision making. In this context, this work conducts a narrative review to provide an overview of how specific Machine Learning techniques can be used to improve sepsis management, discussing the main tasks addressed, the most popular methods and techniques, as well as the obtained results, in terms of both intelligent system accuracy and clinical outcomes improvement.",2020-05-01,0,848,73,372
2649,32481542,Radiomics Applications in Renal Tumor Assessment: A Comprehensive Review of the Literature,"Radiomics texture analysis offers objective image information that could otherwise not be obtained by radiologists' subjective radiological interpretation. We investigated radiomics applications in renal tumor assessment and provide a comprehensive review. A detailed search of original articles was performed using the PubMed-MEDLINE database until 20 March 2020 to identify English literature relevant to radiomics applications in renal tumor assessment. In total, 42 articles were included in the analysis and divided into four main categories: renal mass differentiation, nuclear grade prediction, gene expression-based molecular signatures, and patient outcome prediction. The main area of research involves accurately differentiating benign and malignant renal masses, specifically between renal cell carcinoma (RCC) subtypes and from angiomyolipoma without visible fat and oncocytoma. Nuclear grade prediction may enhance proper patient selection for risk-stratified treatment. Radiomics-predicted gene mutations may serve as surrogate biomarkers for high-risk disease, while predicting patients' responses to targeted therapies and their outcomes will help develop personalized treatment algorithms. Studies generally reported the superiority of radiomics over expert radiological interpretation. Radiomics provides an alternative to subjective image interpretation for improving renal tumor diagnostic accuracy. Further incorporation of clinical and imaging data into radiomics algorithms will augment tumor prediction accuracy and enhance individualized medicine.",2020-05-01,3,1573,90,372
2653,32472247,Chest Pain Evaluation in the Emergency Department: Risk Scores and High-Sensitivity Cardiac Troponin,"Purpose of review:                    As many as 10 million patients present annually to the emergency department in the USA with symptoms concerning for acute myocardial infarction. The use of risk scores for patients with chest pain or equivalent without ST-segment elevation on the electrocardiogram. The adaptation in the USA of high sensitivity troponin assays requires rethinking of how to best optimize troponin testing within a risk score.              Recent findings:                    Patients are risk stratified using a combination of validated risk scores, biomarkers, and both noninvasive and invasive testing. The advent of high-sensitivity troponins has served to augment existing risk scores in the identification of low-risk patients for early discharge, as well as led to the introduction of new rapid rule-out protocols by which acute myocardial infarction can be excluded by biomarker evaluation more quickly. The emergence of machine learning algorithms may further enhance provider's ability to quickly diagnose or exclude myocardial infarction in the emergency department. The addition of high sensitive troponin assays to established emergency department risk scores is providing new opportunities to improve the timeliness and accuracy of the evaluation of patients presenting with a possible myocardial infarction. Utilizing the time between troponin measures as a variable combined with clinical risk factors with new algorithms may further serve to improve diagnostic accuracy.",2020-05-01,0,1508,100,372
2654,32472189,Artificial Intelligence in Intracoronary Imaging,"Purpose of review:                    This paper investigates present uses and future potential of artificial intelligence (AI) applied to intracoronary imaging technologies.              Recent findings:                    Advances in data analytics and digitized medical imaging have enabled clinical application of AI to improve patient outcomes and reduce costs through better diagnosis and enhanced workflow. Applications of AI to IVUS and IVOCT have produced improvements in image segmentation, plaque analysis, and stent evaluation. Machine learning algorithms are able to predict future coronary events through the use of imaging results, clinical evaluations, laboratory tests, and demographics. The application of AI to intracoronary imaging holds significant promise for improved understanding and treatment of coronary heart disease. Even in these early stages, AI has demonstrated the ability to improve the prediction of cardiac events. Large curated data sets and databases are needed to speed the development of AI and enable testing and comparison among algorithms.",2020-05-01,0,1082,48,372
2657,32466283,A Survey of Marker-Less Tracking and Registration Techniques for Health & Environmental Applications to Augmented Reality and Ubiquitous Geospatial Information Systems,"Most existing augmented reality (AR) applications are suitable for cases in which only a small number of real world entities are involved, such as superimposing a character on a single surface. In this case, we only need to calculate pose of the camera relative to that surface. However, when an AR health or environmental application involves a one-to-one relationship between an entity in the real-world and the corresponding object in the computer model (geo-referenced object), we need to estimate the pose of the camera in reference to a common coordinate system for better geo-referenced object registration in the real-world. New innovations in developing cheap sensors, computer vision techniques, machine learning, and computing power have helped to develop applications with more precise matching between a real world and a virtual content. AR Tracking techniques can be divided into two subcategories: marker-based and marker-less approaches. This paper provides a comprehensive overview of marker-less registration and tracking techniques and reviews their most important categories in the context of ubiquitous Geospatial Information Systems (GIS) and AR focusing to health and environmental applications. Basic ideas, advantages, and disadvantages, as well as challenges, are discussed for each subcategory of tracking and registration techniques. We need precise enough virtual models of the environment for both calibrations of tracking and visualization. Ubiquitous GISs can play an important role in developing AR in terms of providing seamless and precise spatial data for outdoor (e.g., environmental applications) and indoor (e.g., health applications) environments.",2020-05-01,0,1687,167,372
2659,32462093,Current challenges and possible future developments in personalized psychiatry with an emphasis on psychotic disorders,"A personalized medicine approach seems to be particularly applicable to psychiatry. Indeed, considering mental illness as deregulation, unique to each patient, of molecular pathways, governing the development and functioning of the brain, seems to be the most justified way to understand and treat disorders of this medical category. In order to extract correct information about the implicated molecular pathways, data can be drawn from sampling phenotypic and genetic biomarkers and then analyzed by a machine learning algorithm. This review describes current difficulties in the field of personalized psychiatry and gives several examples of possibly actionable biomarkers of psychotic and other psychiatric disorders, including several examples of genetic studies relevant to personalized psychiatry. Most of these biomarkers are not yet ready to be introduced in clinical practice. In a next step, a perspective on the path personalized psychiatry may take in the future is given, paying particular attention to machine learning algorithms that can be used with the goal of handling multidimensional datasets.",2020-05-01,0,1114,118,372
2660,32457844,Extracellular Vesicles in Renal Cell Carcinoma: Multifaceted Roles and Potential Applications Identified by Experimental and Computational Methods,"Renal cell carcinoma (RCC) is the most common type of kidney cancer. Increasingly evidences indicate that extracellular vesicles (EVs) orchestrate multiple processes in tumorigenesis, metastasis, immune evasion, and drug response of RCC. EVs are lipid membrane-bound vesicles in nanometer size and secreted by almost all cell types into the extracellular milieu. A myriad of bioactive molecules such as RNA, DNA, protein, and lipid are able to be delivered via EVs for the intercellular communication. Hence, the abundant content of EVs is appealing reservoir for biomarker identification through computational analysis and experimental validation. EVs with excellent biocompatibility and biodistribution are natural platforms that can be engineered to offer achievable drug delivery strategies for RCC therapies. Moreover, the multifaceted roles of EVs in RCC progression also provide substantial targets and facilitate EVs-based drug discovery, which will be accelerated by using artificial intelligence approaches. In this review, we summarized the vital roles of EVs in occurrence, metastasis, immune evasion, and drug resistance of RCC. Furthermore, we also recapitulated and prospected the EVs-based potential applications in RCC, including biomarker identification, drug vehicle development as well as drug target discovery.",2020-05-01,2,1331,146,372
2663,32451639,A review of epileptic seizure detection using machine learning classifiers,"Epilepsy is a serious chronic neurological disorder, can be detected by analyzing the brain signals produced by brain neurons. Neurons are connected to each other in a complex way to communicate with human organs and generate signals. The monitoring of these brain signals is commonly done using Electroencephalogram (EEG) and Electrocorticography (ECoG) media. These signals are complex, noisy, non-linear, non-stationary and produce a high volume of data. Hence, the detection of seizures and discovery of the brain-related knowledge is a challenging task. Machine learning classifiers are able to classify EEG data and detect seizures along with revealing relevant sensible patterns without compromising performance. As such, various researchers have developed number of approaches to seizure detection using machine learning classifiers and statistical features. The main challenges are selecting appropriate classifiers and features. The aim of this paper is to present an overview of the wide varieties of these techniques over the last few years based on the taxonomy of statistical features and machine learning classifiers-'black-box' and 'non-black-box'. The presented state-of-the-art methods and ideas will give a detailed understanding about seizure detection and classification, and research directions in the future.",2020-05-01,8,1331,74,372
2669,32435959,Viewpoint on Time Series and Interrupted Time Series Optimum Modeling for Predicting Arthritic Disease Outcomes,"Purpose of review:                    The propose of this viewpoint is to improve or facilitate the clinical decision-making in the management/treatment strategies of arthritis patients through knowing, understanding, and having access to an interactive process allowing assessment of the patient disease outcome in the future.              Recent findings:                    In recent years, the time series (TS) concept has become the center of attention as a predictive model for making forecast of unseen data values. TS and one of its technologies, the interrupted TS (ITS) analysis (TS with one or more interventions), predict the next period(s) value(s) of a given patient based on their past and current information. Traditional TS/ITS methods involve segmented regression-based technologies (linear and nonlinear), while stochastic (linear modeling) and artificial intelligence approaches, including machine learning (complex nonlinear relationships between variables), are also used; however, each have limitations. We will briefly describe TS/ITS, provide examples of their application in arthritic diseases; describe their methods, challenges, and limitations; and propose a combined (stochastic and artificial intelligence) procedure in post-intervention that will optimize ITS modeling. This combined method will increase the accuracy of ITS modeling by profiting from the advantages of both stochastic and nonlinear models to capture all ITS deterministic and stochastic components. In addition, this combined method will allow ITS outcomes to be predicted as continuous variables without having to consider the time lag produced between the pre- and post-intervention periods, thus minimizing the prediction error not only for the given data but also for all possible future patterns in ITS. The use of reliable prediction methodologies for arthritis patients will permit treatment of not only the disease, but also the patient with the disease, ensuring the best outcome prediction for the patient.",2020-05-01,1,2016,111,372
2670,32434436,Oral microbiome-systemic link studies: perspectives on current limitations and future artificial intelligence-based approaches,"In the past decade, there has been a tremendous increase in studies on the link between oral microbiome and systemic diseases. However, variations in study design and confounding variables across studies often lead to inconsistent observations. In this narrative review, we have discussed the potential influence of study design and confounding variables on the current sequencing-based oral microbiome-systemic disease link studies. The current limitations of oral microbiome-systemic link studies on type 2 diabetes mellitus, rheumatoid arthritis, pregnancy, atherosclerosis, and pancreatic cancer are discussed in this review, followed by our perspective on how artificial intelligence (AI), particularly machine learning and deep learning approaches, can be employed for predicting systemic disease and host metadata from the oral microbiome. The application of AI for predicting systemic disease as well as host metadata requires the establishment of a global database repository with microbiome sequences and annotated host metadata. However, this task requires collective efforts from researchers working in the field of oral microbiome to establish more comprehensive datasets with appropriate host metadata. Development of AI-based models by incorporating consistent host metadata will allow prediction of systemic diseases with higher accuracies, bringing considerable clinical benefits.",2020-05-01,0,1397,126,372
2672,32434009,Deep Learning for Dermatologists: Part I Fundamental Concepts,"Artificial intelligence (AI) is generating substantial interest in the field of medicine. One form of artificial intelligence, deep learning, has led to rapid advances in automated image analysis. In 2017, an algorithm demonstrated the ability to diagnose certain skin cancers from clinical photographs with the accuracy of an expert dermatologist. Subsequently, deep learning has been applied to a range of dermatology applications. Though experts will never be replaced by AI, it will certainly impact the specialty of dermatology. In this first article of a two-part series, the basic concepts of deep learning will be reviewed with the goal of laying the groundwork for effective communication between clinicians and technical colleagues. In part two of the series, the clinical applications of deep learning in dermatology will be reviewed considering limitations and opportunities.",2020-05-01,1,887,61,372
2674,32429287,Metabolomics and Multi-Omics Integration: A Survey of Computational Methods and Resources,"As researchers are increasingly able to collect data on a large scale from multiple clinical and omics modalities, multi-omics integration is becoming a critical component of metabolomics research. This introduces a need for increased understanding by the metabolomics researcher of computational and statistical analysis methods relevant to multi-omics studies. In this review, we discuss common types of analyses performed in multi-omics studies and the computational and statistical methods that can be used for each type of analysis. We pinpoint the caveats and considerations for analysis methods, including required parameters, sample size and data distribution requirements, sources of a priori knowledge, and techniques for the evaluation of model accuracy. Finally, for the types of analyses discussed, we provide examples of the applications of corresponding methods to clinical and basic research. We intend that our review may be used as a guide for metabolomics researchers to choose effective techniques for multi-omics analyses relevant to their field of study.",2020-05-01,6,1076,89,372
2675,32428608,Deep learning for dermatologists: Part II. Current applications,"Because of a convergence of the availability of large data sets, graphics-specific computer hardware, and important theoretical advancements, artificial intelligence has recently contributed to dramatic progress in medicine. One type of artificial intelligence known as deep learning has been particularly impactful for medical image analysis. Deep learning applications have shown promising results in dermatology and other specialties, including radiology, cardiology, and ophthalmology. The modern clinician will benefit from an understanding of the basic features of deep learning to effectively use new applications and to better gauge their utility and limitations. In this second article of a 2-part series, we review the existing and emerging clinical applications of deep learning in dermatology and discuss future opportunities and limitations. Part 1 of this series offered an introduction to the basic concepts of deep learning to facilitate effective communication between clinicians and technical experts.",2020-05-01,3,1019,63,372
0,32416782,Artificial intelligence and the future of global health,"Concurrent advances in information technology infrastructure and mobile computing power in many low and middle-income countries (LMICs) have raised hopes that artificial intelligence (AI) might help to address challenges unique to the field of global health and accelerate achievement of the health-related sustainable development goals. A series of fundamental questions have been raised about AI-driven health interventions, and whether the tools, methods, and protections traditionally used to make ethical and evidence-based decisions about new technologies can be applied to AI. Deployment of AI has already begun for a broad range of health issues common to LMICs, with interventions focused primarily on communicable diseases, including tuberculosis and malaria. Types of AI vary, but most use some form of machine learning or signal processing. Several types of machine learning methods are frequently used together, as is machine learning with other approaches, most often signal processing. AI-driven health interventions fit into four categories relevant to global health researchers: (1) diagnosis, (2) patient morbidity or mortality risk assessment, (3) disease outbreak prediction and surveillance, and (4) health policy and planning. However, much of the AI-driven intervention research in global health does not describe ethical, regulatory, or practical considerations required for widespread use or deployment at scale. Despite the field remaining nascent, AI-driven health interventions could lead to improved health outcomes in LMICs. Although some challenges of developing and deploying these interventions might not be unique to these settings, the global health community will need to work quickly to establish guidelines for development, testing, and use, and develop a user-driven research agenda to facilitate equitable and ethical use.",2020-05-01,10,1862,55,372
1567,32141963,Artificial Intelligence Pertaining to Cardiothoracic Imaging and Patient Care: Beyond Image Interpretation,"Artificial intelligence (AI) is a broad field of computational science that includes many subsets. Today the most widely used subset in medical imaging is machine learning (ML). Many articles have focused on the use of ML for pattern recognition to detect and potentially diagnose various pathologies. However, AI algorithm development is now directed toward workflow management. AI can impact patient care at multiple stages of their imaging experience and assist in efficient and effective scheduling, imaging performance, worklist prioritization, image interpretation, and quality assurance. The purpose of this manuscript was to review the potential AI applications in radiology focusing on workflow management and discuss how ML will affect cardiothoracic imaging.",2020-05-01,0,769,106,372
1733,31602691,Unsupervised machine learning for exploratory data analysis in imaging mass spectrometry,"Imaging mass spectrometry (IMS) is a rapidly advancing molecular imaging modality that can map the spatial distribution of molecules with high chemical specificity. IMS does not require prior tagging of molecular targets and is able to measure a large number of ions concurrently in a single experiment. While this makes it particularly suited for exploratory analysis, the large amount and high-dimensional nature of data generated by IMS techniques make automated computational analysis indispensable. Research into computational methods for IMS data has touched upon different aspects, including spectral preprocessing, data formats, dimensionality reduction, spatial registration, sample classification, differential analysis between IMS experiments, and data-driven fusion methods to extract patterns corroborated by both IMS and other imaging modalities. In this work, we review unsupervised machine learning methods for exploratory analysis of IMS data, with particular focus on (a) factorization, (b) clustering, and (c) manifold learning. To provide a view across the various IMS modalities, we have attempted to include examples from a range of approaches including matrix assisted laser desorption/ionization, desorption electrospray ionization, and secondary ion mass spectrometry-based IMS. This review aims to be an entry point for both (i) analytical chemists and mass spectrometry experts who want to explore computational techniques; and (ii) computer scientists and data mining specialists who want to enter the IMS field.  2019 The Authors. Mass Spectrometry Reviews published by Wiley Periodicals, Inc. Mass SpecRev 00:1-47, 2019.",2020-05-01,8,1651,88,372
1738,31592719,Implementing Artificial Intelligence and Digital Health in Resource-Limited Settings? Top 10 Lessons We Learned in Congenital Heart Defects and Cardiology,"Artificial intelligence (AI) is one of the key drivers of digital health. Digital health and AI applications in medicine and biology are emerging worldwide, not only in resource-rich but also resource-limited regions. AI predates to the mid-20th century, but the current wave of AI builds in part on machine learning (ML), big data, and algorithms that can learn from massive amounts of online user data from patients or healthy persons. There are lessons to be learned from AI applications in different medical specialties and across developed and resource-limited contexts. A case in point is congenital heart defects (CHDs) that continue to plague sub-Saharan Africa, which calls for innovative approaches to improve risk prediction and performance of the available diagnostics. Beyond CHDs, AI in cardiology is a promising context as well. The current suite of digital health applications in CHD and cardiology include complementary technologies such as neural networks, ML, natural language processing and deep learning, not to mention embedded digital sensors. Algorithms that build on these advances are beginning to complement traditional medical expertise while inviting us to redefine the concepts and definitions of expertise in molecular diagnostics and precision medicine. We examine and share here the lessons learned in current attempts to implement AI and digital health in CHD for precision risk prediction and diagnosis in resource-limited settings. These top 10 lessons on AI and digital health summarized in this expert review are relevant broadly beyond CHD in cardiology and medical innovations. As with AI itself that calls for systems approaches to data capture, analysis, and interpretation, both developed and developing countries can usefully learn from their respective experiences as digital health continues to evolve worldwide.",2020-05-01,1,1858,154,372
1426,32642182,Path to precision: prevention of post-operative atrial fibrillation,"Development of post-operative atrial fibrillation (POAF) following open-heart surgery is a significant clinical and economic burden. Despite advancements in medical therapies, the incidence of POAF remains elevated at 25-40%. Early work focused on detecting arrhythmias from electrocardiograms as well as identifying pre-operative risk factors from medical records. However, further progress has been stagnant, and a deeper understanding of pathogenesis and significant influences is warranted. With the advent of more complex machine learning (ML) algorithms and high-throughput sequencing, we have an unprecedented ability to capture and predict POAF in real-time. Integration of multimodal heterogeneous data and application of ML can generate a paradigm shift for diagnosis and treatment. This will require a concerted effort to consolidate and streamline real-time data. Herein, we will review the current literature and emerging opportunities aimed at predictive targets and new insights into the mechanisms underlying long-term sequelae of POAF.",2020-05-01,0,1052,67,372
68,32317574,Machine Learning/Deep Neuronal Network: Routine Application in Chest Computed Tomography and Workflow Considerations,"The constantly increasing number of computed tomography (CT) examinations poses major challenges for radiologists. In this article, the additional benefits and potential of an artificial intelligence (AI) analysis platform for chest CT examinations in routine clinical practice will be examined. Specific application examples include AI-based, fully automatic lung segmentation with emphysema quantification, aortic measurements, detection of pulmonary nodules, and bone mineral density measurement. This contribution aims to appraise this AI-based application for value-added diagnosis during routine chest CT examinations and explore future development perspectives.",2020-05-01,3,668,116,372
1457,32612756,Seq-ing answers: Current data integration approaches to uncover mechanisms of transcriptional regulation,"Advancements in the field of next generation sequencing lead to the generation of ever-more data, with the challenge often being how to combine and reconcile results from different OMICs studies such as genome, epigenome and transcriptome. Here we provide an overview of the standard processing pipelines for ChIP-seq and RNA-seq as well as common downstream analyses. We describe popular multi-omics data integration approaches used to identify target genes and co-factors, and we discuss how machine learning techniques may predict transcriptional regulators and gene expression.",2020-05-01,2,581,104,372
1552,32168002,Recent developments in pediatric retina,"Purpose of review:                    Pediatric retina is an exciting, but also challenging field, where patient age and cooperation can limit ease of diagnosis of a broad range of congenital and acquired diseases, inherited retinal degenerations are mostly untreatable and surgical outcomes can be quite different from those for adults. This review aims to highlight some recent advances and trends that are improving our ability to care for children with retinal conditions.              Recent findings:                    Studies have demonstrated the feasibility of multimodal imaging even in nonsedated infants, with portable optical coherence tomography (OCT) and OCT angiography in particular offering structural insights into diverse pediatric retinal conditions. Encouraging long-term outcomes of subretinal voretigene neparvovec-rzyl injection for RPE65 mutation-associated Leber congenital amaurosis have inspired research on the optimization of subretinal gene delivery and gene therapy for other inherited retinal degenerations. In retinopathy of prematurity, machine learning and smartphone-based imaging can facilitate screening, and studies have highlighted favorable outcomes from intravitreal anti-vascular endothelial growth factor (anti-VEGF) injections. A nomogram for pediatric pars plana sclerotomy site placement may improve safety in complex surgeries.              Summary:                    Multimodal imaging, gene therapy, machine learning and surgical innovation have been and will continue to be important to advances in pediatric retina.",2020-05-01,0,1571,39,372
1779,31539636,Machine learning for clinical decision support in infectious diseases: a narrative review of current applications,"Background:                    Machine learning (ML) is a growing field in medicine. This narrative review describes the current body of literature on ML for clinical decision support in infectious diseases (ID).              Objectives:                    We aim to inform clinicians about the use of ML for diagnosis, classification, outcome prediction and antimicrobial management in ID.              Sources:                    References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, ACM Digital Library, arXiV and IEEE Xplore Digital Library up to July 2019.              Content:                    We found 60 unique ML-clinical decision support systems (ML-CDSS) aiming to assist ID clinicians. Overall, 37 (62%) focused on bacterial infections, 10 (17%) on viral infections, nine (15%) on tuberculosis and four (7%) on any kind of infection. Among them, 20 (33%) addressed the diagnosis of infection, 18 (30%) the prediction, early detection or stratification of sepsis, 13 (22%) the prediction of treatment response, four (7%) the prediction of antibiotic resistance, three (5%) the choice of antibiotic regimen and two (3%) the choice of a combination antiretroviral therapy. The ML-CDSS were developed for intensive care units (n = 24, 40%), ID consultation (n = 15, 25%), medical or surgical wards (n = 13, 20%), emergency department (n = 4, 7%), primary care (n = 3, 5%) and antimicrobial stewardship (n = 1, 2%). Fifty-three ML-CDSS (88%) were developed using data from high-income countries and seven (12%) with data from low- and middle-income countries (LMIC). The evaluation of ML-CDSS was limited to measures of performance (e.g. sensitivity, specificity) for 57 ML-CDSS (95%) and included data in clinical practice for three (5%).              Implications:                    Considering comprehensive patient data from socioeconomically diverse healthcare settings, including primary care and LMICs, may improve the ability of ML-CDSS to suggest decisions adapted to various clinical contexts. Currents gaps identified in the evaluation of ML-CDSS must also be addressed in order to know the potential impact of such tools for clinicians and patients.",2020-05-01,14,2230,113,372
1550,32173518,Precision health: A pragmatic approach to understanding and addressing key factors in autoimmune diseases,"The past decade has witnessed a significant paradigm shift in the clinical approach to autoimmune diseases, lead primarily by initiatives in precision medicine, precision health and precision public health initiatives. An understanding and pragmatic implementation of these approaches require an understanding of the drivers, gaps and limitations of precision medicine. Gaining the trust of the public and patients is paramount but understanding that technologies such as artificial intelligences and machine learning still require context that can only be provided by human input or what is called augmented machine learning. The role of genomics, the microbiome and proteomics, such as autoantibody testing, requires continuing refinement through research and pragmatic approaches to their use in applied precision medicine.",2020-05-01,2,826,105,372
2045,33733152,On Consequentialism and Fairness,"Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage ""fair"" outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.",2020-05-01,0,1251,32,372
1234,32091446,Machine Learning and Deep Neural Networks Applications in Coronary Flow Assessment: The Case of Computed Tomography Fractional Flow Reserve,"Coronary computed tomography angiography (cCTA) is a reliable and clinically proven method for the evaluation of coronary artery disease. cCTA data sets can be used to derive fractional flow reserve (FFR) as CT-FFR. This method has respectable results when compared in previous trials to invasive FFR, with the aim of detecting lesion-specific ischemia. Results from previous studies have shown many benefits, including improved therapeutic guidance to efficiently justify the management of patients with suspected coronary artery disease and enhanced outcomes and reduced health care costs. More recently, a technical approach to the calculation of CT-FFR using an artificial intelligence deep machine learning (ML) algorithm has been introduced. ML algorithms provide information in a more objective, reproducible, and rational manner and with improved diagnostic accuracy in comparison to cCTA. This review gives an overview of the technical background, clinical validation, and implementation of ML applications in CT-FFR.",2020-05-01,1,1026,139,372
1542,32195886,Machine Learning and Deep Neural Networks Applications in Computed Tomography for Coronary Artery Disease and Myocardial Perfusion,"During the latest years, artificial intelligence, and especially machine learning (ML), have experienced a growth in popularity due to their versatility and potential in solving complex problems. In fact, ML allows the efficient handling of big volumes of data, allowing to tackle issues that were unfeasible before, especially with deep learning, which utilizes multilayered neural networks. Cardiac computed tomography (CT) is also experiencing a rise in examination numbers, and ML might help handle the increasing derived information. Moreover, cardiac CT presents some fields wherein ML may be pivotal, such as coronary calcium scoring, CT angiography, and perfusion. In particular, the main applications of ML involve image preprocessing and postprocessing, and the development of risk assessment models based on imaging findings. Concerning image preprocessing, ML can help improve image quality by optimizing acquisition protocols or removing artifacts that may hinder image analysis and interpretation. ML in image postprocessing might help perform automatic segmentations and shorten examination processing times, also providing tools for tissue characterization, especially concerning plaques. The development of risk assessment models from ML using data from cardiac CT could aid in the stratification of patients who undergo cardiac CT in different risk classes and better tailor their treatment to individual conditions. While ML is a powerful tool with great potential, applications in the field of cardiac CT are still expanding, and not yet routinely available in clinical practice due to the need for extensive validation. Nevertheless, ML is expected to have a big impact on cardiac CT in the near future.",2020-05-01,1,1724,130,372
1535,32205581,Artificial intelligence driven next-generation renal histomorphometry,"Purpose of review:                    Successful integration of artificial intelligence into extant clinical workflows is contingent upon a number of factors including clinician comprehension and interpretation of computer vision. This article discusses how image analysis and machine learning have enabled comprehensive characterization of kidney morphology for development of automated diagnostic and prognostic renal pathology applications.              Recent findings:                    The primordial digital pathology informatics work employed classical image analysis and machine learning to prognosticate renal disease. Although this classical approach demonstrated tremendous potential, subsequent advancements in hardware technology rendered artificial neural networks '(ANNs) the method of choice for machine vision in computational pathology'. Offering rapid and reproducible detection, characterization and classification of kidney morphology, ANNs have facilitated the development of diagnostic and prognostic applications. In addition, modern machine learning with ANNs has revealed novel biomarkers in kidney disease, demonstrating the potential for machine vision to elucidate novel pathologic mechanisms beyond extant clinical knowledge.              Summary:                    Despite the revolutionary developments potentiated by modern machine learning, several challenges remain, including data quality control and curation, image annotation and ontology, integration of multimodal data and interpretation of machine vision or 'opening the black box'. Resolution of these challenges will not only revolutionize diagnostic pathology but also pave the way for precision medicine and integration of artificial intelligence in the process of care.",2020-05-01,1,1768,69,372
36,32357923,How to develop a meaningful radiomic signature for clinical use in oncologic patients,"During the last decade, there is an increasing usage of quantitative methods in Radiology in an effort to reduce the diagnostic variability associated with a subjective manner of radiological interpretation. Combined approaches where visual assessment made by the radiologist is augmented by quantitative imaging biomarkers are gaining attention. Advances in machine learning resulted in the rise of radiomics that is a new methodology referring to the extraction of quantitative information from medical images. Radiomics are based on the development of computational models, referred to as ""Radiomic Signatures"", trying to address either unmet clinical needs, mostly in the field of oncologic imaging, or to compare radiomics performance with that of radiologists. However, to explore this new technology, initial publications did not consider best practices in the field of machine learning resulting in publications with questionable clinical value. In this paper, our effort was concentrated on how to avoid methodological mistakes and consider critical issues in the workflow of the development of clinically meaningful radiomic signatures.",2020-05-01,5,1146,85,372
1523,32228365,New Machine Learning Applications to Accelerate Personalized Medicine in Breast Cancer: Rise of the Support Vector Machines,"Artificial intelligence, machine learning, health care robots, and algorithms for clinical decision-making are currently being sought after in diverse fields of clinical medicine and bioengineering. The field of personalized medicine stands to benefit from new technologies so as to harness the omics big data, for example, to individualize and accelerate cancer diagnostics and therapeutics in particular. In this overarching context, breast cancer is one of the most common malignancies worldwide with multiple underlying molecular etiologies and each subtype displaying diverse clinical outcomes. Disease stratification for breast cancer is, therefore, vital to its effective and individualized clinical care. The support vector machine (SVM) is a rising machine learning approach that offers robust classification of high-dimensional big data into small numbers of data points (support vectors), achieving differentiation of subgroups in a short amount of time. Considering the rapid timelines required for both diagnosis and treatment of most aggressive cancers, this new machine learning technique has important clinical and public applications and implications for high-throughput data analysis and contextualization. This expert review describes and examines, first, the SVM models employed to forecast breast cancer subtypes using diverse systems science data, including transcriptomics, epigenetics, proteomics, and radiomics, as well as biological pathway, clinical, pathological, and biochemical data. Then, we compare the performance of the present SVM and other diagnostic and therapeutic prediction models across the data types. We conclude by emphasizing that data integration is a critical bottleneck in systems science, cancer research and development, and health care innovation and that SVM and machine learning approaches offer new solutions and ways forward in biomedical, bioengineering, and clinical applications.",2020-05-01,1,1937,123,372
1517,32235273,Applications of machine learning methods in kidney disease: hope or hype?,"Purpose of review:                    The universal adoption of electronic health records, improvement in technology, and the availability of continuous monitoring has generated large quantities of healthcare data. Machine learning is increasingly adopted by nephrology researchers to analyze this data in order to improve the care of their patients.              Recent findings:                    In this review, we provide a broad overview of the different types of machine learning algorithms currently available and how researchers have applied these methods in nephrology research. Current applications have included prediction of acute kidney injury and chronic kidney disease along with progression of kidney disease. Researchers have demonstrated the ability of machine learning to read kidney biopsy samples, identify patient outcomes from unstructured data, and identify subtypes in complex diseases. We end with a discussion on the ethics and potential pitfalls of machine learning.              Summary:                    Machine learning provides researchers with the ability to analyze data that were previously inaccessible. While still burgeoning, several studies show promising results, which will enable researchers to perform larger scale studies and clinicians the ability to provide more personalized care. However, we must ensure that implementation aids providers and does not lead to harm to patients.",2020-05-01,1,1428,73,372
1947,33205097,Avoid Oversimplifications in Machine Learning: Going beyond the Class-Prediction Accuracy,"Class-prediction accuracy provides a quick but superficial way of determining classifier performance. It does not inform on the reproducibility of the findings or whether the selected or constructed features used are meaningful and specific. Furthermore, the class-prediction accuracy oversummarizes and does not inform on how training and learning have been accomplished: two classifiers providing the same performance in one validation can disagree on many future validations. It does not provide explainability in its decision-making process and is not objective, as its value is also affected by class proportions in the validation set. Despite these issues, this does not mean we should omit the class-prediction accuracy. Instead, it needs to be enriched with accompanying evidence and tests that supplement and contextualize the reported accuracy. This additional evidence serves as augmentations and can help us perform machine learning better while avoiding naive reliance on oversimplified metrics.",2020-05-01,2,1008,89,372
859,31343790,Artificial intelligence in the interpretation of breast cancer on MRI,"Advances in both imaging and computers have led to the rise in the potential use of artificial intelligence (AI) in various tasks in breast imaging, going beyond the current use in computer-aided detection to include diagnosis, prognosis, response to therapy, and risk assessment. The automated capabilities of AI offer the potential to enhance the diagnostic expertise of clinicians, including accurate demarcation of tumor volume, extraction of characteristic cancer phenotypes, translation of tumoral phenotype features to clinical genotype implications, and risk prediction. The combination of image-specific findings with the underlying genomic, pathologic, and clinical features is becoming of increasing value in breast cancer. The concurrent emergence of newer imaging techniques has provided radiologists with greater diagnostic tools and image datasets to analyze and interpret. Integrating an AI-based workflow within breast imaging enables the integration of multiple data streams into powerful multidisciplinary applications that may lead the path to personalized patient-specific medicine. In this article we describe the goals of AI in breast cancer imaging, in particular MRI, and review the literature as it relates to the current application, potential, and limitations in breast cancer. Level of Evidence: 3 Technical Efficacy: Stage 3 J. Magn. Reson. Imaging 2020;51:1310-1324.",2020-05-01,9,1397,69,372
918,31246376,A review of approaches for analysing obstructive sleep apnoea-related patterns in pulse oximetry data,"Overnight pulse oximetry allows the relatively non-invasive estimation of peripheral blood haemoglobin oxygen saturations (SpO2 ), and forms part of the typical polysomnogram (PSG) for investigation of obstructive sleep apnoea (OSA). While the raw SpO2 signal can provide detailed information about OSA-related pathophysiology, this information is typically summarized with simple statistics such as the oxygen desaturation index (ODI, number of desaturations per hour). As such, this study reviews the technical methods for quantifying OSA-related patterns in oximetry data. The technical methods described in literature can be broadly grouped into four categories: (i) Describing the detailed characteristics of desaturations events; (ii) Time series statistics; (iii) Analysis of power spectral distribution (i.e. frequency domain analysis); and (d) Non-linear analysis. These are described and illustrated with examples of oximetry traces. The utilization of these techniques is then described in two applications. First, the application of detailed oximetry analysis allows the accurate automated classification of PSG-defined OSA. Second, quantifications which better characterize the severity of desaturation events are better predictors of OSA-related epidemiological outcomes than standard clinical metrics. Finally, methodological considerations and further applications and opportunities are considered.",2020-05-01,2,1414,101,372
933,31960771,Carotid artery ultrasound image analysis: A review of the literature,"Stroke is one of the prominent causes of death in the recent days. The existence of susceptible plaque in the carotid artery can be used in ascertaining the possibilities of cardiovascular diseases and long-term disabilities. The imaging modality used for early screening of the disease is B-mode ultrasound image of the person in the artery area. The objective of this article is to give a widespread review of the imaging modes and methods used for studying the carotid artery for identifying stroke, atherosclerosis and related cardiovascular diseases. We encompass the review in methods used for artery wall tracking, intima-media, and lumen segmentation which will help in finding the extent of the disease. Due to the characteristics of the imaging modality used, the images have speckle noise which worsens the image quality. Adaptive homomorphic filtering with wavelet and contourlet transforms, Levy Shrink, gamma distribution were used for image denoising. Learning-based neural network approaches for denoising give better edge preservation. Domain knowledge-based segmentation approaches have proved to provide more accurate intima-media thickness measurements. There is a requirement of useful fully automatic segmentation approaches, 3D, 4D systems, and plaque motion analysis. Taking into consideration the image priors like geometry, imaging physics, intensity and temporal data, image analysis has to be performed. Encouragingly more research has focused on content-specific segmentation and classification techniques. With the evaluation of machine learning algorithms, classifying the image as with or without a fat deposit has gained better accuracy and sensitivity. Machine learning-based approaches like self-organizing map, k-nearest neighborhood and support vector machine achieve promising accuracy and sensitivity in classification. The literature reveals that there is more scope in identifying a patient-specific model in a fully automatic manner.",2020-05-01,2,1975,68,372
937,31958430,"Engineering Stability, Viscosity, and Immunogenicity of Antibodies by Computational Design","In recent years, computational methods have garnered much attention in protein engineering. A large number of computational methods have been developed to analyze the sequences and structures of proteins and have been used to predict the various properties. Antibodies are one of the emergent protein therapeutics, and thus, methods to control their physicochemical properties are highly desirable. However, despite the tremendous efforts of past decades, computational methods to predict the physicochemical properties of antibodies are still in their infancy. Experimental validations are certainly required for real-world applications, and the results should be interpreted with caution. Among the various properties of antibodies, we focus in this review on stability, viscosity, and immunogenicity, and we present the current status of computational methods to engineer such properties.",2020-05-01,1,891,90,372
946,31950592,Radiomics and Machine Learning in Oral Healthcare,"The increasing storage of information, data, and forms of knowledge has led to the development of new technologies that can help to accomplish complex tasks in different areas, such as in dentistry. In this context, the role of computational methods, such as radiomics and Artificial Intelligence (AI) applications, has been progressing remarkably for dentomaxillofacial radiology (DMFR). These tools bring new perspectives for diagnosis, classification, and prediction of oral diseases, treatment planning, and for the evaluation and prediction of outcomes, minimizing the possibilities of human errors. A comprehensive review of the state-of-the-art of using radiomics and machine learning (ML) for imaging in oral healthcare is presented in this paper. Although the number of published studies is still relatively low, the preliminary results are very promising and in a near future, an augmented dentomaxillofacial radiology (ADMFR) will combine the use of radiomics-based and AI-based analyses with the radiologist's evaluation. In addition to the opportunities and possibilities, some challenges and limitations have also been discussed for further investigations.",2020-05-01,6,1170,49,372
980,31887283,Machine Learning Characterization of COPD Subtypes: Insights From the COPDGene Study,"COPD is a heterogeneous syndrome. Many COPD subtypes have been proposed, but there is not yet consensus on how many COPD subtypes there are and how they should be defined. The COPD Genetic Epidemiology Study (COPDGene), which has generated 10-year longitudinal chest imaging, spirometry, and molecular data, is a rich resource for relating COPD phenotypes to underlying genetic and molecular mechanisms. In this article, we place COPDGene clustering studies in context with other highly cited COPD clustering studies, and summarize the main COPD subtype findings from COPDGene. First, most manifestations of COPD occur along a continuum, which explains why continuous aspects of COPD or disease axes may be more accurate and reproducible than subtypes identified through clustering methods. Second, continuous COPD-related measures can be used to create subgroups through the use of predictive models to define cut-points, and we review COPDGene research on blood eosinophil count thresholds as a specific example. Third, COPD phenotypes identified or prioritized through machine learning methods have led to novel biological discoveries, including novel emphysema genetic risk variants and systemic inflammatory subtypes of COPD. Fourth, trajectory-based COPD subtyping captures differences in the longitudinal evolution of COPD, addressing a major limitation of clustering analyses that are confounded by disease severity. Ongoing longitudinal characterization of subjects in COPDGene will provide useful insights about the relationship between lung imaging parameters, molecular markers, and COPD progression that will enable the identification of subtypes based on underlying disease processes and distinct patterns of disease progression, with the potential to improve the clinical relevance and reproducibility of COPD subtypes.",2020-05-01,4,1834,84,372
1263,32049766,Anomalies in language as a biomarker for schizophrenia,"Purpose of review:                    After more than a century of neuroscience research, reproducible, clinically relevant biomarkers for schizophrenia have not yet been established. This article reviews current advances in evaluating the use of language as a diagnostic or prognostic tool in schizophrenia.              Recent findings:                    The development of computational linguistic tools to quantify language disturbances is rapidly gaining ground in the field of schizophrenia research. Current applications are the use of semantic space models and acoustic analyses focused on phonetic markers. These features are used in machine learning models to distinguish patients with schizophrenia from healthy controls or to predict conversion to psychosis in high-risk groups, reaching accuracy scores (generally ranging from 80 to 90%) that exceed clinical raters. Other potential applications for a language biomarker in schizophrenia are monitoring of side effects, differential diagnostics and relapse prevention.              Summary:                    Language disturbances are a key feature of schizophrenia. Although in its early stages, the emerging field of research focused on computational linguistics suggests an important role for language analyses in the diagnosis and prognosis of schizophrenia. Spoken language as a biomarker for schizophrenia has important advantages because it can be objectively and reproducibly quantified. Furthermore, language analyses are low-cost, time efficient and noninvasive in nature.",2020-05-01,2,1547,54,372
1244,32079904,"Machine Learning and Deep Neural Networks: Applications in Patient and Scan Preparation, Contrast Medium, and Radiation Dose Optimization","Artificial intelligence (AI) algorithms are dependent on a high amount of robust data and the application of appropriate computational power and software. AI offers the potential for major changes in cardiothoracic imaging. Beyond image processing, machine learning and deep learning have the potential to support the image acquisition process. AI applications may improve patient care through superior image quality and have the potential to lower radiation dose with AI-driven reconstruction algorithms and may help avoid overscanning. This review summarizes recent promising applications of AI in patient and scan preparation as well as contrast medium and radiation dose optimization.",2020-05-01,2,688,137,372
1103,31411796,Big Data in sleep apnoea: Opportunities and challenges,"Sleep apnoea is now regarded as a highly prevalent systemic, multimorbid, chronic disease requiring a combination of long-term home-based treatments. Optimization of personalized treatment strategies requires accurate patient phenotyping. Data to describe the broad variety of phenotypes can come from electronic health records, health insurance claims, socio-economic administrative databases, environmental monitoring, social media, etc. Connected devices in and outside homes collect vast amount of data amassed in databases. All this contributes to 'Big Data' that, if used appropriately, has great potential for the benefit of health, well-being and therapeutics. Sleep apnoea is particularly well placed with regards to Big Data because the primary treatment is positive airway pressure (PAP). PAP devices, used every night over long periods by millions of patients across the world, generate an enormous amount of data. In this review, we discuss how different types of Big Data have, and could be, used to improve our understanding of sleep-disordered breathing, to identify undiagnosed sleep apnoea, to personalize treatment and to adapt health policies and better allocate resources. We discuss some of the challenges of Big Data including the need for appropriate data management, compilation and analysis techniques employing innovative statistical approaches alongside machine learning/artificial intelligence; closer collaboration between data scientists and physicians; and respect of the ethical and regulatory constraints of collecting and using Big Data. Lastly, we consider how Big Data can be used to overcome the limitations of randomized clinical trials and advance real-life evidence-based medicine for sleep apnoea.",2020-05-01,1,1739,54,372
1223,32112901,Biofluid diagnostics by FTIR spectroscopy: A platform technology for cancer detection,"Fourier Transform Infrared Spectroscopy (FTIR) has been largely employed by scientific researchers to improve diagnosis and treatment of cancer, using various biofluids and tissues. The technology has proved to be easy to use, rapid and cost-effective for analysis on human blood serum to discriminate between cancer versus healthy control samples. The high sensitivity and specificity achievable during samples classification aided by machine learning algorithms, offers an opportunity to transform cancer referral pathways, as it has been demonstrated in a unique and recent prospective clinical validation study on brain tumours. We herein highlight the importance of early detection in cancer research using FTIR, discussing the technique, the suitability of serum for analysis and previous studies, with special focus on pre-clinical factors and clinical translation requirements and development.",2020-05-01,3,901,85,372
1242,32083960,Entering the era of computationally driven drug development,"Historically, failure rates in drug development are high; increased sophistication and investment throughout the process has shifted the reasons for attrition, but the overall success rates have remained stubbornly and consistently low. Only 8% of new entities entering clinical testing gain regulatory approval, indicating that significant obstacles still exist for efficient therapeutic development. The continued high failure rate can be partially attributed to the inability to link drug exposure with the magnitude of observed safety and efficacy-related pharmacodynamic (PD) responses; frequently, this is a result of nonclinical models exhibiting poor prediction of human outcomes across a wide range of disease conditions, resulting in faulty evaluation of drug toxicology and efficacy. However, the increasing quality and standardization of experimental methods in preclinical stages of testing has created valuable data sets within companies that can be leveraged to further improve the efficiency and accuracy of preclinical prediction for both pharmacokinetics (PK) and PD. Models of Quantitative structure-activity relationships (QSAR), physiologically based pharmacokinetics (PBPK), and PK/PD relationships have also improved efficiency. Founded on a core understanding of biochemistry and physiological interactions of xenobiotics, these in silico methods have the potential to increase the probability of compound success in clinical trials. Integration of traditional computational methods with machine-learning approaches and existing internal pharma databases stands to make a fundamental impact on the speed and accuracy of predictions during the process of drug development and approval.",2020-05-01,4,1708,59,372
1569,32141255,Understanding the Molecular Mechanisms of Asthma through Transcriptomics,"The transcriptome represents the complete set of RNA transcripts that are produced by the genome under a specific circumstance or in a specific cell. High-throughput methods, including microarray and bulk RNA sequencing, as well as recent advances in biostatistics based on machine learning approaches provides a quick and effective way of identifying novel genes and pathways related to asthma, which is a heterogeneous disease with diverse pathophysiological mechanisms. In this manuscript, we briefly review how to analyze transcriptome data and then provide a summary of recent transcriptome studies focusing on asthma pathogenesis and asthma drug responses. Studies reviewed here are classified into 2 classes based on the tissues utilized: blood and airway cells.",2020-05-01,2,769,72,372
35,32358210,Technology Use for Adolescent Health and Wellness,"As avid users of technology, adolescents are a key demographic to engage when designing and developing technology applications for health. There are multiple opportunities for improving adolescent health, from promoting preventive behaviors to providing guidance for adolescents with chronic illness in supporting treatment adherence and transition to adult health care systems. This article will provide a brief overview of current technologies and then highlight new technologies being used specifically for adolescent health, such as artificial intelligence, virtual and augmented reality, and machine learning. Because there is paucity of evidence in this field, we will make recommendations for future research.",2020-05-01,11,716,49,372
1232,32096675,Time-lapse technology for embryo culture and selection,"Culturing of human embryos in optimal conditions is crucial for a successful in vitro fertilisation (IVF) programme. In addition, the capacity to assess and rank embryos correctly for quality will allow for transfer of the potentially 'best' embryo first, thereby shortening the time to pregnancy, although not improving cumulative pregnancy and live birth rates. It will also encourage and facilitate the implementation of single embryo transfers, thereby increasing safety for mother and offspring. Time-lapse technology introduces the concept of stable culture conditions, in connection with the possibility of continuous viewing and documenting of the embryo throughout development. However, so far, even when embryo quality scoring is based on large datasets, or when using the time-lapse technology, the morphokinetic scores are still mainly based on subjective and intermittent annotations of morphology and timings. Also, the construction of powerful algorithms for widespread use is hampered by large variations in culture conditions between individual IVF laboratories. New methodology, involving machine learning, where every image from the time-lapse documentation is analysed by a computer programme, looking for patterns that link to outcome, may in the future provide a more accurate and non-biased embryo selection.",2020-05-01,2,1331,54,372
3,32411818,Report on computational assessment of Tumor Infiltrating Lymphocytes from the International Immuno-Oncology Biomarker Working Group,"Assessment of tumor-infiltrating lymphocytes (TILs) is increasingly recognized as an integral part of the prognostic workflow in triple-negative (TNBC) and HER2-positive breast cancer, as well as many other solid tumors. This recognition has come about thanks to standardized visual reporting guidelines, which helped to reduce inter-reader variability. Now, there are ripe opportunities to employ computational methods that extract spatio-morphologic predictive features, enabling computer-aided diagnostics. We detail the benefits of computational TILs assessment, the readiness of TILs scoring for computational assessment, and outline considerations for overcoming key barriers to clinical translation in this arena. Specifically, we discuss: 1. ensuring computational workflows closely capture visual guidelines and standards; 2. challenges and thoughts standards for assessment of algorithms including training, preanalytical, analytical, and clinical validation; 3. perspectives on how to realize the potential of machine learning models and to overcome the perceptual and practical limits of visual scoring.",2020-05-01,3,1115,131,372
9,32403240,Applications of Artificial Intelligence to Prostate Multiparametric MRI (mpMRI): Current and Emerging Trends,"Prostate carcinoma is one of the most prevalent cancers worldwide. Multiparametric magnetic resonance imaging (mpMRI) is a non-invasive tool that can improve prostate lesion detection, classification, and volume quantification. Machine learning (ML), a branch of artificial intelligence, can rapidly and accurately analyze mpMRI images. ML could provide better standardization and consistency in identifying prostate lesions and enhance prostate carcinoma management. This review summarizes ML applications to prostate mpMRI and focuses on prostate organ segmentation, lesion detection and segmentation, and lesion characterization. A literature search was conducted to find studies that have applied ML methods to prostate mpMRI. To date, prostate organ segmentation and volume approximation have been well executed using various ML techniques. Prostate lesion detection and segmentation are much more challenging tasks for ML and were attempted in several studies. They largely remain unsolved problems due to data scarcity and the limitations of current ML algorithms. By contrast, prostate lesion characterization has been successfully completed in several studies because of better data availability. Overall, ML is well situated to become a tool that enhances radiologists' accuracy and speed.",2020-05-01,3,1299,108,372
23,32378359,Modeling the Physicochemical Properties of Natural Deep Eutectic Solvents,"Natural deep eutectic solvents (NADES) are mixtures of naturally derived compounds with a significantly decreased melting point owing to specific interactions among the constituents. NADES have benign properties (low volatility, flammability, toxicity, cost) and tailorable physicochemical properties (by altering the type and molar ratio of constituents); hence, they are often considered to be a green alternative to common organic solvents. Modeling the relation between their composition and properties is crucial though, both for understanding and predicting their behavior. Several efforts have been made to this end. This Review aims at structuring the present knowledge as an outline for future research. First, the key properties of NADES are reviewed and related to their structure on the basis of the available experimental data. Second, available modeling methods applicable to NADES are reviewed. At the molecular level, DFT and molecular dynamics allow density differences and vibrational spectra to be interpreted, and interaction energies to be computed. Additionally, properties at the level of the bulk medium can be explained and predicted by semi-empirical methods based on ab initio methods (COSMO-RS) and equation of state models (PC-SAFT). Finally, methods based on large datasets are discussed: models based on group-contribution methods and machine learning. A combination of bulk-medium and dataset modeling allows qualitative prediction and interpretation of phase equilibria properties on the one hand, and quantitative prediction of melting point, density, viscosity, surface tension, and refractive index on the other. Multiscale modeling, combining molecular and macroscale methods, is expected to strongly enhance the predictability of NADES properties and their interaction with solutes, and thus yield truly tailorable solvents to accommodate (bio)chemical reactions.",2020-05-01,0,1901,73,372
17,32391171,"Applications of radiomics in precision diagnosis, prognostication and treatment planning of head and neck squamous cell carcinomas","Recent advancements in computational power, machine learning, and artificial intelligence technology have enabled automated evaluation of medical images to generate quantitative diagnostic and prognostic biomarkers. Such objective biomarkers are readily available and have the potential to improve personalized treatment, precision medicine, and patient selection for clinical trials. In this article, we explore the merits of the most recent addition to the ""-omics"" concept for the broader field of head and neck cancer - ""Radiomics"". This review discusses radiomics studies focused on (molecular) characterization, classification, prognostication and treatment guidance for head and neck squamous cell carcinomas (HNSCC). We review the underlying hypothesis, general concept and typical workflow of radiomic analysis, and elaborate on current and future challenges to be addressed before routine clinical application.",2020-05-01,6,920,130,372
5,32410356,Quantitative Prostate MRI,"Prostate MRI is reported in clinical practice using the Prostate Imaging and Data Reporting System (PI-RADS). PI-RADS aims to standardize, as much as possible, the acquisition, interpretation, reporting, and ultimately the performance of prostate MRI. PI-RADS relies upon mainly subjective analysis of MR imaging findings, with very few incorporated quantitative features. The shortcomings of PI-RADS are mainly: low-to-moderate interobserver agreement and modest accuracy for detection of clinically significant tumors in the transition zone. The use of a more quantitative analysis of prostate MR imaging findings is therefore of interest. Quantitative MR imaging features including: tumor size and volume, tumor length of capsular contact, tumor apparent diffusion coefficient (ADC) metrics, tumor T1 and T2 relaxation times, tumor shape, and texture analyses have all shown value for improving characterization of observations detected on prostate MRI and for differentiating between tumors by their pathological grade and stage. Quantitative analysis may therefore improve diagnostic accuracy for detection of cancer and could be a noninvasive means to predict patient prognosis and guide management. Since quantitative analysis of prostate MRI is less dependent on an individual users' assessment, it could also improve interobserver agreement. Semi- and fully automated analysis of quantitative (radiomic) MRI features using artificial neural networks represent the next step in quantitative prostate MRI and are now being actively studied. Validation, through high-quality multicenter studies assessing diagnostic accuracy for clinically significant prostate cancer detection, in the domain of quantitative prostate MRI is needed. This article reviews advances in quantitative prostate MRI, highlighting the strengths and limitations of existing and emerging techniques, as well as discussing opportunities and challenges for evaluation of prostate MRI in clinical practice when using quantitative assessment. LEVEL OF EVIDENCE: 5 TECHNICAL EFFICACY: Stage 2.",2020-05-01,2,2067,25,372
28,32370835,Artificial Intelligence in Cardiology: Present and Future,"Artificial intelligence (AI) is a nontechnical, popular term that refers to machine learning of various types but most often to deep neural networks. Cardiology is at the forefront of AI in medicine. For this review, we searched PubMed and MEDLINE databases with no date restriction using search terms related to AI and cardiology. Articles were selected for inclusion on the basis of relevance. We highlight the major achievements in recent years in nearly all areas of cardiology and underscore the mounting evidence suggesting how AI will take center stage in the field. Artificial intelligence requires a close collaboration among computer scientists, clinical investigators, clinicians, and other users in order to identify the most relevant problems to be solved. Best practices in the generation and implementation of AI include the selection of ideal data sources, taking into account common challenges during the interpretation, validation, and generalizability of findings, and addressing safety and ethical concerns before final implementation. The future of AI in cardiology and in medicine in general is bright as the collaboration between investigators and clinicians continues to excel.",2020-05-01,6,1201,57,372
6,32408531,"Massive MIMO Systems for 5G and Beyond Networks-Overview, Recent Trends, Challenges, and Future Research Direction","The global bandwidth shortage in the wireless communication sector has motivated the study and exploration of wireless access technology known as massive Multiple-Input Multiple-Output (MIMO). Massive MIMO is one of the key enabling technology for next-generation networks, which groups together antennas at both transmitter and the receiver to provide high spectral and energy efficiency using relatively simple processing. Obtaining a better understating of the massive MIMO system to overcome the fundamental issues of this technology is vital for the successful deployment of 5G-and beyond-networks to realize various applications of the intelligent sensing system. In this paper, we present a comprehensive overview of the key enabling technologies required for 5G and 6G networks, highlighting the massive MIMO systems. We discuss all the fundamental challenges related to pilot contamination, channel estimation, precoding, user scheduling, energy efficiency, and signal detection in a massive MIMO system and discuss some state-of-the-art mitigation techniques. We outline recent trends such as terahertz communication, ultra massive MIMO (UM-MIMO), visible light communication (VLC), machine learning, and deep learning for massive MIMO systems. Additionally, we discuss crucial open research issues that direct future research in massive MIMO systems for 5G and beyond networks.",2020-05-01,7,1388,114,372
1576,32133724,Artificial intelligence in oncology,"Artificial intelligence (AI) has contributed substantially to the resolution of a variety of biomedical problems, including cancer, over the past decade. Deep learning, a subfield of AI that is highly flexible and supports automatic feature extraction, is increasingly being applied in various areas of both basic and clinical cancer research. In this review, we describe numerous recent examples of the application of AI in oncology, including cases in which deep learning has efficiently solved problems that were previously thought to be unsolvable, and we address obstacles that must be overcome before such application can become more widespread. We also highlight resources and datasets that can help harness the power of AI for cancer research. The development of innovative approaches to and applications of AI will yield important insights in oncology in the coming decade.",2020-05-01,5,882,35,372
15,32393561,Artificial Intelligence and Primary Care Research: A Scoping Review,"Purpose:                    Rapid increases in technology and data motivate the application of artificial intelligence (AI) to primary care, but no comprehensive review exists to guide these efforts. Our objective was to assess the nature and extent of the body of research on AI for primary care.              Methods:                    We performed a scoping review, searching 11 published or gray literature databases with terms pertaining to AI (eg, machine learning, bayes* network) and primary care (eg, general pract*, nurse). We performed title and abstract and then full-text screening using Covidence. Studies had to involve research, include both AI and primary care, and be published in Eng-lish. We extracted data and summarized studies by 7 attributes: purpose(s); author appointment(s); primary care function(s); intended end user(s); health condition(s); geographic location of data source; and AI subfield(s).              Results:                    Of 5,515 unique documents, 405 met eligibility criteria. The body of research focused on developing or modifying AI methods (66.7%) to support physician diagnostic or treatment recommendations (36.5% and 13.8%), for chronic conditions, using data from higher-income countries. Few studies (14.1%) had even a single author with a primary care appointment. The predominant AI subfields were supervised machine learning (40.0%) and expert systems (22.2%).              Conclusions:                    Research on AI for primary care is at an early stage of maturity. For the field to progress, more interdisciplinary research teams with end-user engagement and evaluation studies are needed.",2020-05-01,2,1657,67,372
21,32384762,Virtual Network Embedding for Multi-Domain Heterogeneous Converged Optical Networks: Issues and Challenges,"The emerging 5G applications and the connectivity of billions of devices have driven the investigation of multi-domain heterogeneous converged optical networks. To support emerging applications with their diverse quality of service requirements, network slicing has been proposed as a promising technology. Network virtualization is an enabler for network slicing, where the physical network can be partitioned into different configurable slices in the multi-domain heterogeneous converged optical networks. An efficient resource allocation mechanism for multiple virtual networks in network virtualization is one of the main challenges referred as virtual network embedding (VNE). This paper is a survey on the state-of-the-art works for the VNE problem towards multi-domain heterogeneous converged optical networks, providing the discussion on future research issues and challenges. In this paper, we describe VNE in multi-domain heterogeneous converged optical networks with enabling network orchestration technologies and analyze the literature about VNE algorithms with various network considerations for each network domain. The basic VNE problem with various motivations and performance metrics for general scenarios is discussed. A VNE algorithm taxonomy is presented and discussed by classifying the major VNE algorithms into three categories according to existing literature. We analyze and compare the attributes of algorithms such as node and link embedding methods, objectives, and network architecture, which can give a selection or baseline for future work of VNE. Finally, we explore some broader perspectives in future research issues and challenges on 5G scenario, field trail deployment, and machine learning-based algorithms.",2020-05-01,1,1745,106,372
1573,32138567,Advances in the use of cell penetrating peptides for respiratory drug delivery,"Introduction: Respiratory diseases are leading causes of death in the world, still inhalation therapies are the largest fail in drug development. There is an evident need to develop new therapies. Biomolecules represent apotential therapeutic agent in this regard, however their translation to the clinic is hindered by the lack of tools to efficiently deliver molecules. Cell penetrating peptides (CPPs) have arisen as apotential strategy for intracellular delivery that could theoretically enable the translation of new therapies.Areas covered: In this review, the use of CPPs as astrategy to deliver different molecules (cargoes) to treat lung-relateddiseases will be the focus. Abrief description of these molecules and the innovative methods in designing new CPPs is presented. The delivery of different cargoes (proteins, peptides, poorly soluble drugs and nucleic acids) using CPPs is discussed, focusing on benefits to treat different respiratory diseases like inflammatory disorders, cystic fibrosis and lung cancer.Expert opinion: The advantages of using CPPs to deliver biomolecules and poorly soluble drugs to the lungs is evident. This field has advanced in the past few years toward targeted intracellular delivery, although further studies are needed to fully understand its potential and limitations in vitro and in vivo.",2020-05-01,1,1337,78,372
1589,32112907,Imaging of intratumoral heterogeneity in high-grade glioma,"High-grade glioma (HGG), and particularly Glioblastoma (GBM), can exhibit pronounced intratumoral heterogeneity that confounds clinical diagnosis and management. While conventional contrast-enhanced MRI lacks the capability to resolve this heterogeneity, advanced MRI techniques and PET imaging offer a spectrum of physiologic and biophysical image features to improve the specificity of imaging diagnoses. Published studies have shown how integrating these advanced techniques can help better define histologically distinct targets for surgical and radiation treatment planning, and help evaluate the regional heterogeneity of tumor recurrence and response assessment following standard adjuvant therapy. Application of texture analysis and machine learning (ML) algorithms has also enabled the emerging field of radiogenomics, which can spatially resolve the regional and genetically distinct subpopulations that coexist within a single GBM tumor. This review focuses on the latest advances in neuro-oncologic imaging and their clinical applications for the assessment of intratumoral heterogeneity.",2020-05-01,3,1101,58,372
12,32394199,A practical guide to amplicon and metagenomic analysis of microbiome data,"Advances in high-throughput sequencing (HTS) have fostered rapid developments in the field of microbiome research, and massive microbiome datasets are now being generated. However, the diversity of software tools and the complexity of analysis pipelines make it difficult to access this field. Here, we systematically summarize the advantages and limitations of microbiome methods. Then, we recommend specific pipelines for amplicon and metagenomic analyses, and describe commonly-used software and databases, to help researchers select the appropriate tools. Furthermore, we introduce statistical and visualization methods suitable for microbiome analysis, including alpha- and beta-diversity, taxonomic composition, difference comparisons, correlation, networks, machine learning, evolution, source tracing, and common visualization styles to help researchers make informed choices. Finally, a step-by-step reproducible analysis guide is introduced. We hope this review will allow researchers to carry out data analysis more effectively and to quickly select the appropriate tools in order to efficiently mine the biological significance behind the data.",2020-05-01,9,1156,73,372
1,32414188,Future Is Unlicensed: Private 5G Unlicensed Network for Connecting Industries of Future,"This paper aims to unlock the unlicensed band potential in realizing the Industry 4.0 communication goals of the Fifth-Generation (5G) and beyond. New Radio in the Unlicensed band (NR-U) is a new NR Release 16 mode of operation that has the capability to offer the necessary technology for cellular operators to integrate the unlicensed spectrum into 5G networks. NR-U enables both uplink and downlink operation in unlicensed bands, supporting 5G advanced features of ultra-high-speed, high bandwidth, low latency, and improvement in the reliability of wireless communications, which is essential to address massive-scale and highly-diverse future industrial networks. This paper highlights NR-U as a next-generation communication technology for smart industrial network communication and discusses the technology trends adopted by 5G in support of the Industry 4.0 revolution. However, due to operation in the shared/unlicensed spectrum, NR-U possesses several regulatory and coexistence challenges, limiting its application for operationally intensive environments such as manufacturing, supply chain, transportation systems, and energy. Thus, we discuss the significant challenges and potential solution approaches such as shared maximum channel occupancy time (MCOT), handover skipping, the self-organized network (SON), the adaptive back-off mechanism, and the multi-domain coexistence approach to overcome the unlicensed/shared band challenges and boost the realization of NR-U technology in mission-critical industrial applications. Further, we highlight the role of machine learning in providing the necessary intelligence and adaptation mechanisms for the realization of industrial 5G communication goals.",2020-05-01,0,1714,87,372
2681,32418341,"Artificial intelligence-based clinical decision support in modern medical physics: Selection, acceptance, commissioning, and quality assurance","Background:                    Recent advances in machine and deep learning based on an increased availability of clinical data have fueled renewed interest in computerized clinical decision support systems (CDSSs). CDSSs have shown great potential to improve healthcare, increase patient safety and reduce costs. However, the use of CDSSs is not without pitfalls, as an inadequate or faulty CDSS can potentially deteriorate the quality of healthcare and put patients at risk. In addition, the adoption of a CDSS might fail because its intended users ignore the output of the CDSS due to lack of trust, relevancy or actionability.              Aim:                    In this article, we provide guidance based on literature for the different aspects involved in the adoption of a CDSS with a special focus on machine and deep learning based systems: selection, acceptance testing, commissioning, implementation and quality assurance.              Results:                    A rigorous selection process will help identify the CDSS that best fits the preferences and requirements of the local site. Acceptance testing will make sure that the selected CDSS fulfills the defined specifications and satisfies the safety requirements. The commissioning process will prepare the CDSS for safe clinical use at the local site. An effective implementation phase should result in an orderly roll out of the CDSS to the well-trained end-users whose expectations have been managed. And finally, quality assurance will make sure that the performance of the CDSS is maintained and that any issues are promptly identified and solved.              Conclusion:                    We conclude that a systematic approach to the adoption of a CDSS will help avoid pitfalls, improve patient safety and increase the chances of success.",2020-06-01,1,1815,142,341
1321,31971112,Comparative Analysis of Classification Methods with PCA and LDA for Diabetes,"Background:                    The modern society is extremely prone to many life-threatening diseases, which can be easily controlled as well as cured if diagnosed at an early stage. The development and implementation of a disease diagnostic system have gained huge popularity over the years. In the current scenario, there are certain factors such as environment, sedentary lifestyle, genetic (hereditary) are the major factors behind the life threatening diseases such as 'diabetes.' Moreover, diabetes has achieved the status of the modern man's leading chronic disease. So one of the prime needs of this generation is to develop a state-of-the-art expert system which can predict diabetes at a very early stage with a minimum of complexity and in an expedited manner. The primary objective of this work is to develop an indigenous and efficient diagnostic technique for detection of diabetes. Method & Discussion: The proposed methodology comprises of two phases: In the first phase The Pima Indian Diabetes Dataset (PIDD) has been collected from the UCI machine learning repository databases and Localized Diabetes Dataset (LDD) has been gathered from Bombay Medical Hall, Upper Bazar Ranchi, Jharkhand, India. In the second phase, the dataset has been processed through two different approaches. The first approach entails classification through Adaboost, Classification via Regression (CVR), Radial Basis Function Network (RBFN), K-Nearest Neighbor (KNN) on Pima Indian Diabetes Dataset and Localized Diabetes Dataset. In the second approach, Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) have been applied as a feature reduction method followed by using the same set of classification methods used in the first approach. Among all of the implemented classification methods, PCA_CVR achieves the maximum performance for both the above mentioned datasets.              Conclusion:                    In this article, comparative analysis of outcomes obtained by with and without the use of PCA and LDA for the same set of classification method has been done w.r.t performance assessment. Finally, it has been concluded that PCA & LDA both are useful to remove the insignificant features, decreasing the expense and computation time while improving the ROC and accuracy. The used methodology may similarly be applied to other medical diseases.",2020-06-01,1,2377,76,341
2682,32418340,Computer-aided diagnosis in the era of deep learning,"Computer-aided diagnosis (CAD) has been a major field of research for the past few decades. CAD uses machine learning methods to analyze imaging and/or nonimaging patient data and makes assessment of the patient's condition, which can then be used to assist clinicians in their decision-making process. The recent success of the deep learning technology in machine learning spurs new research and development efforts to improve CAD performance and to develop CAD for many other complex clinical tasks. In this paper, we discuss the potential and challenges in developing CAD tools using deep learning technology or artificial intelligence (AI) in general, the pitfalls and lessons learned from CAD in screening mammography and considerations needed for future implementation of CAD or AI in clinical use. It is hoped that the past experiences and the deep learning technology will lead to successful advancement and lasting growth in this new era of CAD, thereby enabling CAD to deliver intelligent aids to improve health care.",2020-06-01,3,1027,52,341
1333,33184585,Cardiac Magnetic Resonance in Pulmonary Hypertension-an Update,"Purpose of review:                    This article reviews advances over the past 3 years in cardiac magnetic resonance (CMR) imaging in pulmonary hypertension (PH). We aim to bring the reader up-to-date with CMR applications in diagnosis, prognosis, 4D flow, strain analysis, T1 mapping, machine learning and ongoing research.              Recent findings:                    CMR volumetric and functional metrics are now established as valuable prognostic markers in PH. This imaging modality is increasingly used to assess treatment response and improves risk stratification when incorporated into PH risk scores. Emerging techniques such as myocardial T1 mapping may play a role in the follow-up of selected patients. Myocardial strain may be used as an early marker for right and left ventricular dysfunction and a predictor for mortality. Machine learning has offered a glimpse into future possibilities. Ongoing research of new PH therapies is increasingly using CMR as a clinical endpoint.              Summary:                    The last 3 years have seen several large studies establishing CMR as a valuable diagnostic and prognostic tool in patients with PH, with CMR increasingly considered as an endpoint in clinical trials of PH therapies. Machine learning approaches to improve automation and accuracy of CMR metrics and identify imaging features of PH is an area of active research interest with promising clinical utility.",2020-06-01,0,1440,62,341
2594,32575560,"Current Applications, Opportunities, and Limitations of AI for 3D Imaging in Dental Research and Practice","The increasing use of three-dimensional (3D) imaging techniques in dental medicine has boosted the development and use of artificial intelligence (AI) systems for various clinical problems. Cone beam computed tomography (CBCT) and intraoral/facial scans are potential sources of image data to develop 3D image-based AI systems for automated diagnosis, treatment planning, and prediction of treatment outcome. This review focuses on current developments and performance of AI for 3D imaging in dentomaxillofacial radiology (DMFR) as well as intraoral and facial scanning. In DMFR, machine learning-based algorithms proposed in the literature focus on three main applications, including automated diagnosis of dental and maxillofacial diseases, localization of anatomical landmarks for orthodontic and orthognathic treatment planning, and general improvement of image quality. Automatic recognition of teeth and diagnosis of facial deformations using AI systems based on intraoral and facial scanning will very likely be a field of increased interest in the future. The review is aimed at providing dental practitioners and interested colleagues in healthcare with a comprehensive understanding of the current trend of AI developments in the field of 3D imaging in dental medicine.",2020-06-01,1,1279,105,341
2686,32418335,Genomics models in radiotherapy: From mechanistic to machine learning,"Machine learning (ML) provides a broad framework for addressing high-dimensional prediction problems in classification and regression. While ML is often applied for imaging problems in medical physics, there are many efforts to apply these principles to biological data toward questions of radiation biology. Here, we provide a review of radiogenomics modeling frameworks and efforts toward genomically guided radiotherapy. We first discuss medical oncology efforts to develop precision biomarkers. We next discuss similar efforts to create clinical assays for normal tissue or tumor radiosensitivity. We then discuss modeling frameworks for radiosensitivity and the evolution of ML to create predictive models for radiogenomics.",2020-06-01,4,729,69,341
2685,32418336,Machine and deep learning methods for radiomics,"Radiomics is an emerging area in quantitative image analysis that aims to relate large-scale extracted imaging information to clinical and biological endpoints. The development of quantitative imaging methods along with machine learning has enabled the opportunity to move data science research towards translation for more personalized cancer treatments. Accumulating evidence has indeed demonstrated that noninvasive advanced imaging analytics, that is, radiomics, can reveal key components of tumor phenotype for multiple three-dimensional lesions at multiple time points over and beyond the course of treatment. These developments in the use of CT, PET, US, and MR imaging could augment patient stratification and prognostication buttressing emerging targeted therapeutic approaches. In recent years, deep learning architectures have demonstrated their tremendous potential for image segmentation, reconstruction, recognition, and classification. Many powerful open-source and commercial platforms are currently available to embark in new research areas of radiomics. Quantitative imaging research, however, is complex and key statistical principles should be followed to realize its full potential. The field of radiomics, in particular, requires a renewed focus on optimal study design/reporting practices and standardization of image acquisition, feature calculation, and rigorous statistical analysis for the field to move forward. In this article, the role of machine and deep learning as a major computational vehicle for advanced model building of radiomics-based signatures or classifiers, and diverse clinical applications, working principles, research opportunities, and available computational platforms for radiomics will be reviewed with examples drawn primarily from oncology. We also address issues related to common applications in medical physics, such as standardization, feature extraction, model building, and validation.",2020-06-01,10,1945,47,341
2684,32418337,Machine learning techniques for biomedical image segmentation: An overview of technical aspects and introduction to state-of-art applications,"In recent years, significant progress has been made in developing more accurate and efficient machine learning algorithms for segmentation of medical and natural images. In this review article, we highlight the imperative role of machine learning algorithms in enabling efficient and accurate segmentation in the field of medical imaging. We specifically focus on several key studies pertaining to the application of machine learning methods to biomedical image segmentation. We review classical machine learning algorithms such as Markov random fields, k-means clustering, random forest, etc. Although such classical learning models are often less accurate compared to the deep-learning techniques, they are often more sample efficient and have a less complex structure. We also review different deep-learning architectures, such as the artificial neural networks (ANNs), the convolutional neural networks (CNNs), and the recurrent neural networks (RNNs), and present the segmentation results attained by those learning models that were published in the past 3 yr. We highlight the successes and limitations of each machine learning paradigm. In addition, we discuss several challenges related to the training of different machine learning models, and we present some heuristics to address those challenges.",2020-06-01,3,1308,141,341
1478,32596246,Review of Machine Learning in Predicting Dermatological Outcomes,"Artificial intelligence is a broad branch of computer science that has garnered significant interest in the field of medicine because of its problem solving, decision making and pattern recognition abilities. Machine learning, a subset of artificial intelligence, hones in on the ability of computers to receive data and learn for themselves, manipulating algorithms as they organize the information they are processing. Dermatology is at a particular advantage in the implementation of machine learning due to the availability of large clinical image databases that can be used for machine training and interpretation. While numerous studies have implemented machine learning in the diagnostic aspect of dermatology, less research has been conducted on the use of machine learning in predicting long-term outcomes in skin disease, with only a few studies published to date. Such an approach would assist physicians in selecting the best treatment methods, save patients' time, reduce treatment costs and improve the quality of treatment overall by reducing the amount of trial-and-error in the treatment process. In this review, we aim to provide a brief and relevant introduction to basic artificial intelligence processes, and to consolidate and examine the published literature on the use of machine learning in predicting clinical outcomes in dermatology.",2020-06-01,0,1360,64,341
1388,33088903,Towards accurate models for predicting smartphone applications' QoE with data from a living lab study,"Progressively, smartphones have become the pocket Swiss army knife for everyone. They support their users needs to accomplish tasks in numerous contexts. However, the applications executing those tasks are regularly not performing as they should, and the user-perceived experience is altered. In this paper, we present our approach to model and predict the Quality of Experience (QoE) of mobile applications used over WiFi or cellular network. We aimed to create predictive QoE models and to derive recommendations for mobile application developers to create QoE aware applications. Previous works on smartphone applications' QoE prediction only focus on qualitative or quantitative data. We collected both qualitative and quantitative data ""in the wild"" through our living lab. We ran a 4-week-long study with 38 Android phone users. We focused on frequently used and highly interactive applications. The participants rated their mobile applications' expectation and QoE and in various contexts resulting in a total of 6086 ratings. Simultaneously, our smartphone logger (mQoL-Log) collected background information such as network information, user physical activity, battery statistics, and more. We apply various data aggregation approaches and features selection processes to train multiple predictive QoE models. We obtain better model performances using ratings acquired within 14.85 minutes after the application usage. Additionally, we boost our models' performance with the users expectation as a new feature. We create an on-device prediction model with on-smartphone only features. We compare its performance metrics against the previous model. The on-device model performs below the full features models. Surprisingly, among the following top three features: the intended task to accomplish with the app, application's name (e.g., WhatsApp, Spotify), and network Quality of Service (QoS), the user physical activity is the most important feature (e.g., if walking). Finally, we share our recommendations with the application developers, and we discuss the implications of QoE and expectations in mobile application design.",2020-06-01,0,2134,101,341
2595,32570374,Emerging Concepts and Applied Machine Learning Research in Patients with Drug-Induced Repolarization Disorders,"The paper presents a review of current research to develop predictive models for automated detection of drug-induced repolarization disorders and shows a feasibility study for developing machine learning tools trained on massive multimodal datasets of narrative, textual and electrocardiographic records. The goal is to reduce drug-induced long QT and associated complications (Torsades-de-Pointes, sudden cardiac death), by identifying prescription patterns with pro-arrhythmic propensity using a validated electronic application for the detection of adverse drug events with data mining and natural language processing; and to compute individual-based predictive scores in order to further identify clinical conditions, concomitant diseases, or other variables that correlate with higher risk of pro-arrhythmic situations.",2020-06-01,0,824,110,341
2683,32418338,Machine learning for radiation outcome modeling and prediction,"Aims:                    This review paper intends to summarize the application of machine learning to radiotherapy outcome modeling based on structured and un-structured radiation oncology datasets.              Materials and methods:                    The most appropriate machine learning approaches for structured datasets in terms of accuracy and interpretability are identified. For un-structured datasets, deep learning algorithms are explored and a critical view of the use of these approaches in radiation oncology is also provided.              Conclusions:                    We discuss the challenges in radiotherapy outcome prediction, and suggest to improve radiation outcome modeling by developing appropriate machine learning approaches where both accuracy and interpretability are taken into account.",2020-06-01,1,818,62,341
1825,32760624,Artificial Intelligence: Is It Armageddon for Breast Radiologists?,"Artificial Intelligence (AI) has taken radiology by storm, in particular, mammogram interpretation, and we have seen a recent surge in the number of publications on potential uses of AI in breast radiology. Breast cancer exerts a lot of burden on the National Health Service (NHS) and is the second most common cancer in the UK as of 2018. New cases of breast cancer have been on the rise in the past decade, while the survival rate has been improving. The NHS breast cancer screening program led to an improvement in survival rate. The expansion of the screening program led to more mammograms, thereby putting more work on the hands of radiologists, and the issue of double reading further worsens the workload. The introduction of computer-aided detection (CAD) systems to help radiologists was found not to have the expected outcome of improving the performance of readers. Unreliability of CAD systems has led to the explosion of studies and development of applications with the potential use in breast imaging. The purported success recorded with the use of machine learning in breast radiology has led to people postulating ideas that AI will replace breast radiologists. Of course, AI has many applications and potential uses in radiology, but will it replace radiologists? We reviewed many articles on the use of AI in breast radiology to give future radiologists and radiologists full information on this topic. This article focuses on explaining the basic principles and terminology of AI in radiology, potential uses, and limitations of AI in radiology. We have also analysed articles and answered the question of whether AI will replace radiologists.",2020-06-01,0,1663,66,341
2605,32560074,The Future of Protein Secondary Structure Prediction Was Invented by Oleg Ptitsyn,"When Oleg Ptitsyn and his group published the first secondary structure prediction for a protein sequence, they started a research field that is still active today. Oleg Ptitsyn combined fundamental rules of physics with human understanding of protein structures. Most followers in this field, however, use machine learning methods and aim at the highest (average) percentage correctly predicted residues in a set of proteins that were not used to train the prediction method. We show that one single method is unlikely to predict the secondary structure of all protein sequences, with the exception, perhaps, of future deep learning methods based on very large neural networks, and we suggest that some concepts pioneered by Oleg Ptitsyn and his group in the 70s of the previous century likely are today's best way forward in the protein secondary structure prediction field.",2020-06-01,0,876,81,341
1307,31994465,Outwitting an Old Neglected Nemesis: A Review on Leveraging Integrated Data-Driven Approaches to Aid in Unraveling of Leishmanicides of Therapeutic Potential,"The global prevalence of leishmaniasis has increased with skyrocketed mortality in the past decade. The causative agent of leishmaniasis is Leishmania species, which infects populations in almost all the continents. Prevailing treatment regimens are consistently inefficient with reported side effects, toxicity and drug resistance. This review complements existing ones by discussing the current state of treatment options, therapeutic bottlenecks including chemoresistance and toxicity, as well as drug targets. It further highlights innovative applications of nanotherapeutics-based formulations, inhibitory potential of leishmanicides, anti-microbial peptides and organometallic compounds on leishmanial species. Moreover, it provides essential insights into recent machine learning-based models that have been used to predict novel leishmanicides and also discusses other new models that could be adopted to develop fast, efficient, robust and novel algorithms to aid in unraveling the next generation of anti-leishmanial drugs. A plethora of enriched functional genomic, proteomic, structural biology, high throughput bioassay and drug-related datasets are currently warehoused in both general and leishmania-specific databases. The warehoused datasets are essential inputs for training and testing algorithms to augment the prediction of biotherapeutic entities. In addition, we demonstrate how pharmacoinformatics techniques including ligand-, structure- and pharmacophore-based virtual screening approaches have been utilized to screen ligand libraries against both modeled and experimentally solved 3D structures of essential drug targets. In the era of data-driven decision-making, we believe that highlighting intricately linked topical issues relevant to leishmanial drug discovery offers a one-stop-shop opportunity to decipher critical literature with the potential to unlock implicit breakthroughs.",2020-06-01,0,1914,157,341
1238,32086157,Patient-derived model systems and the development of next-generation anticancer therapeutics,"Anticancer drug discovery and development using conventional cell line and animal models has traditionally had a low overall success rate. Despite yielding game-changing new therapeutics, 10-20 new molecules have to be brought to the clinic to obtain one new approval, making this approach costly and inefficient. The use of in vitro experimental models based on primary human tumour tissues has the potential to provide a representation of human cancer biology that is closer to an actual patient and to 'bridge the translational gap' between preclinical and clinical research. Here, we review recent advances in the use of human tumour samples for preclinical research through organoid development or as primary patient materials. While challenges still remain regarding analysis, validation and scalability, evidence is mounting for the applicability of both models as preclinical research tools.",2020-06-01,0,899,92,341
2441,30768796,Radiation Therapy Quality Assurance Tasks and Tools: The Many Roles of Machine Learning,"The recent explosion in machine learning efforts in the quality assurance (QA) space has produced a variety of proofs-of-concept many with promising results. Expected outcomes of model implementation include improvements in planning time, plan quality, advanced dosimetric QA, predictive machine maintenance, increased safety checks, and developments key for new QA paradigms driven by adaptive planning. In this article, we outline several areas of research and discuss some of the unique challenges each area presents.",2020-06-01,3,520,87,341
2579,30472716,Machine Learning and Imaging Informatics in Oncology,"In the era of personalized and precision medicine, informatics technologies utilizing machine learning (ML) and quantitative imaging are witnessing a rapidly increasing role in medicine in general and in oncology in particular. This expanding role ranges from computer-aided diagnosis to decision support of treatments with the potential to transform the current landscape of cancer management. In this review, we aim to provide an overview of ML methodologies and imaging informatics techniques and their recent application in modern oncology. We will review example applications of ML in oncology from the literature, identify current challenges and highlight future potentials.",2020-06-01,4,680,52,341
2394,30872241,"Harnessing the Power of Machine Learning in Dementia Informatics Research: Issues, Opportunities, and Challenges","Dementia is a chronic and degenerative condition affecting millions globally. The care of patients with dementia presents an ever-continuing challenge to healthcare systems in the 21st century. Medical and health sciences have generated unprecedented volumes of data related to health and wellbeing for patients with dementia due to advances in information technology, such as genetics, neuroimaging, cognitive assessment, free texts, routine electronic health records, etc. Making the best use of these diverse and strategic resources will lead to high-quality care of patients with dementia. As such, machine learning becomes a crucial factor in achieving this objective. The aim of this paper is to provide a state-of-the-art review of machine learning methods applied to health informatics for dementia care. We collate and review the existing scientific methodologies and identify the relevant issues and challenges when faced with big health data. Machine learning has demonstrated promising applications to neuroimaging data analysis for dementia care, while relatively less effort has been made to make use of integrated heterogeneous data via advanced machine learning approaches. We further indicate future potential and research directions in applying advanced machine learning, such as deep learning, to dementia informatics.",2020-06-01,2,1337,112,341
1250,32062850,DNA Methylation-Based Biomarkers of Environmental Exposures for Human Population Studies,"Purpose of review:                    This manuscript orients the reader to the underlying motivations of environmental biomarker development for human population studies and provides the foundation for applying these novel biomarkers in future research. In this review, we focus our attention on the DNA methylation-based biomarkers of (i) smoking, among adults and pregnant women, (ii) lifetime cannabis use, (iii) alcohol consumption, and (iv) cumulative exposure to lead.              Recent findings:                    Prior environmental exposures and lifestyle modulate DNA methylation levels. Exposure-related DNA methylation changes can either be persistent or reversible once the exposure is no longer present, and this combination of both persistent and reversible changes has essential value for biomarker development. Here, we present available biomarkers representing past and cumulative exposures using individual DNA methylation profiles. In the present work, we describe how the field of environmental epigenetics can leverage machine learning algorithms to develop exposure biomarkers and reduce problems of misreporting exposures or limited access technology. We emphasize the crucial role of the individual DNA methylation profiles in those predictions, providing a summary of each biomarker, and highlighting their advantages, and limitations. Future research can cautiously leverage these DNA methylation-based biomarkers to understand the onset and progression of diseases.",2020-06-01,2,1497,88,341
1254,32060715,Radiomics of computed tomography and magnetic resonance imaging in renal cell carcinoma-a systematic review and meta-analysis,"Objectives:                    (1) To assess the methodological quality of radiomics studies investigating histological subtypes, therapy response, and survival in patients with renal cell carcinoma (RCC) and (2) to determine the risk of bias in these radiomics studies.              Methods:                    In this systematic review, literature published since 2000 on radiomics in RCC was included and assessed for methodological quality using the Radiomics Quality Score. The risk of bias was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool and a meta-analysis of radiomics studies focusing on differentiating between angiomyolipoma without visible fat and RCC was performed.              Results:                    Fifty-seven studies investigating the use of radiomics in renal cancer were identified, including 4590 patients in total. The average Radiomics Quality Score was 3.41 (9.4% of total) with good inter-rater agreement (ICC 0.96, 95% CI 0.93-0.98). Three studies validated results with an independent dataset, one used a publically available validation dataset. None of the studies shared the code, images, or regions of interest. The meta-analysis showed moderate heterogeneity among the included studies and an odds ratio of 6.24 (95% CI 4.27-9.12; p < 0.001) for the differentiation of angiomyolipoma without visible fat from RCC.              Conclusions:                    Radiomics algorithms show promise for answering clinical questions where subjective interpretation is challenging or not established. However, the generalizability of findings to prospective cohorts needs to be demonstrated in future trials for progression towards clinical translation. Improved sharing of methods including code and images could facilitate independent validation of radiomics signatures.              Key points:                     Studies achieved an average Radiomics Quality Score of 10.8%. Common reasons for low Radiomics Quality Scores were unvalidated results, retrospective study design, absence of open science, and insufficient control for multiple comparisons.  A previous training phase allowed reaching almost perfect inter-rater agreement in the application of the Radiomics Quality Score.  Meta-analysis of radiomics studies distinguishing angiomyolipoma without visible fat from renal cell carcinoma show moderate diagnostic odds ratios of 6.24 and moderate methodological diversity.",2020-06-01,6,2446,125,341
1444,32625083,Artificial Intelligence and Machine Learning Applied at the Point of Care,"Introduction:                    The increasing availability of healthcare data and rapid development of big data analytic methods has opened new avenues for use of Artificial Intelligence (AI)- and Machine Learning (ML)-based technology in medical practice. However, applications at the point of care are still scarce.              Objective:                    Review and discuss case studies to understand current capabilities for applying AI/ML in the healthcare setting, and regulatory requirements in the US, Europe and China.              Methods:                    A targeted narrative literature review of AI/ML based digital tools was performed. Scientific publications (identified in PubMed) and grey literature (identified on the websites of regulatory agencies) were reviewed and analyzed.              Results:                    From the regulatory perspective, AI/ML-based solutions can be considered medical devices (i.e., Software as Medical Device, SaMD). A case series of SaMD is presented. First, tools for monitoring and remote management of chronic diseases are presented. Second, imaging applications for diagnostic support are discussed. Finally, clinical decision support tools to facilitate the choice of treatment and precision dosing are reviewed. While tested and validated algorithms for precision dosing exist, their implementation at the point of care is limited, and their regulatory and commercialization pathway is not clear. Regulatory requirements depend on the level of risk associated with the use of the device in medical practice, and can be classified into administrative (manufacturing and quality control), software-related (design, specification, hazard analysis, architecture, traceability, software risk analysis, cybersecurity, etc.), clinical evidence (including patient perspectives in some cases), non-clinical evidence (dosing validation and biocompatibility/toxicology) and other, such as e.g. benefit-to-risk determination, risk assessment and mitigation. There generally is an alignment between the US and Europe. China additionally requires that the clinical evidence is applicable to the Chinese population and recommends that a third-party central laboratory evaluates the clinical trial results.              Conclusions:                    The number of promising AI/ML-based technologies is increasing, but few have been implemented widely at the point of care. The need for external validation, implementation logistics, and data exchange and privacy remain the main obstacles.",2020-06-01,0,2541,73,341
1266,32048244,Machine Learning for Pulmonary and Critical Care Medicine: A Narrative Review,"Machine learning (ML) is a discipline of computer science in which statistical methods are applied to data in order to classify, predict, or optimize, based on previously observed data. Pulmonary and critical care medicine have seen a surge in the application of this methodology, potentially delivering improvements in our ability to diagnose, treat, and better understand a multitude of disease states. Here we review the literature and provide a detailed overview of the recent advances in ML as applied to these areas of medicine. In addition, we discuss both the significant benefits of this work as well as the challenges in the implementation and acceptance of this non-traditional methodology for clinical purposes.",2020-06-01,4,723,77,341
1480,32594407,Non-invasive Imaging Techniques: From Histology to In Vivo Imaging : Chapter of Imaging in Oncology,"In this chapter, we will introduce and review molecular-sensitive imaging techniques, which close the gap between ex vivo and in vivo analysis. In detail, we will introduce spontaneous Raman spectral imaging, coherent anti-Stokes Raman scattering (CARS), stimulated Raman scattering (SRS), second-harmonic generation (SHG) and third-harmonic generation (THG), two-photon excited fluorescence (TPEF), and fluorescence lifetime imaging (FLIM). After reviewing these imaging techniques, we shortly introduce chemometric methods and machine learning techniques, which are needed to use these imaging techniques in diagnostic applications.",2020-06-01,0,634,99,341
1481,32594406,Image-Guided Radiooncology: The Potential of Radiomics in Clinical Application,"Medical imaging plays an imminent role in today's radiation oncology workflow. Predominantly based on semantic image analysis, malignant tumors are diagnosed, staged, and therapy decisions are made. The field of ""radiomics"" promises to extract complementary, objective information from medical images. In radiomics, predefined quantitative features including intensity statistics, texture, shape, or filtering techniques are combined into statistical or machine learning models to predict clinical or biological outcomes. Alternatively, deep neural networks can directly analyze medical images and provide predictions. A large number of research studies could demonstrate that radiomics prediction models may provide significant benefits in the radiation oncology workflow including diagnostics, tumor characterization, target volume segmentation, prognostic stratification, and prediction of therapy response or treatment-related toxicities. This chapter provides an overview of techniques within the radiomics toolbox, potential clinical application, and current limitations. A literature overview of four selected malignant entities including non-small cell lung cancer, head and neck squamous cell carcinomas, soft tissue sarcomas, and gliomas is given.",2020-06-01,1,1257,78,341
1280,32030670,Deep Learning Technique for Musculoskeletal Analysis,"Advancements in musculoskeletal analysis have been achieved by adopting deep learning technology in image recognition and analysis. Unlike musculoskeletal modeling based on computational anatomy, deep learning-based methods can obtain muscle information automatically. Through analysis of image features, both approaches can obtain muscle characteristics such as shape, volume, and area, and derive additional information by analyzing other image textures. In this chapter, we first discuss the necessity of musculoskeletal analysis and the required image processing technology. Then, the limitations of skeletal muscle recognition based on conventional handcrafted features are discussed, and developments in skeletal muscle recognition using machine learning and deep learning technology are described. Next, a technique for analyzing musculoskeletal systems using whole-body computed tomography (CT) images is shown. This study aims to achieve automatic recognition of skeletal muscles throughout the body and automatic classification of atrophic muscular disease using only image features, to demonstrate an application of whole-body musculoskeletal analysis driven by deep learning. Finally, we discuss future development of musculoskeletal analysis that effectively combines deep learning with handcrafted feature-based modeling techniques.",2020-06-01,0,1346,52,341
1281,32030669,Techniques and Applications in Skin OCT Analysis,"The skin is the largest organ of our body. Skin disease abnormalities which occur within the skin layers are difficult to examine visually and often require biopsies to make a confirmation on a suspected condition. Such invasive methods are not well-accepted by children and women due to the possibility of scarring. Optical coherence tomography (OCT) is a non-invasive technique enabling in vivo examination of sub-surface skin tissue without the need for excision of tissue. However, one of the challenges in OCT imaging is the interpretation and analysis of OCT images. In this review, we discuss the various methodologies in skin layer segmentation and how it could potentially improve the management of skin diseases. We also present a review of works which use advanced machine learning techniques to achieve layers segmentation and detection of skin diseases. Lastly, current challenges in analysis and applications are also discussed.",2020-06-01,0,942,48,341
1282,32030661,Medical Image Synthesis via Deep Learning,"Medical images have been widely used in clinics, providing visual representations of under-skin tissues in human body. By applying different imaging protocols, diverse modalities of medical images with unique characteristics of visualization can be produced. Considering the cost of scanning high-quality single modality images or homogeneous multiple modalities of images, medical image synthesis methods have been extensively explored for clinical applications. Among them, deep learning approaches, especially convolutional neural networks (CNNs) and generative adversarial networks (GANs), have rapidly become dominating for medical image synthesis in recent years. In this chapter, based on a general review of the medical image synthesis methods, we will focus on introducing typical CNNs and GANs models for medical image synthesis. Especially, we will elaborate our recent work about low-dose to high-dose PET image synthesis, and cross-modality MR image synthesis, using these models.",2020-06-01,3,993,41,341
1283,32030660,Deep Learning in Medical Image Analysis,"Deep learning is the state-of-the-art machine learning approach. The success of deep learning in many pattern recognition applications has brought excitement and high expectations that deep learning, or artificial intelligence (AI), can bring revolutionary changes in health care. Early studies of deep learning applied to lesion detection or classification have reported superior performance compared to those by conventional techniques or even better than radiologists in some tasks. The potential of applying deep-learning-based medical image analysis to computer-aided diagnosis (CAD), thus providing decision support to clinicians and improving the accuracy and efficiency of various diagnostic and treatment processes, has spurred new research and development efforts in CAD. Despite the optimism in this new era of machine learning, the development and implementation of CAD or AI tools in clinical practice face many challenges. In this chapter, we will discuss some of these issues and efforts needed to develop robust deep-learning-based CAD tools and integrate these tools into the clinical workflow, thereby advancing towards the goal of providing reliable intelligent aids for patient care.",2020-06-01,4,1203,39,341
1288,32016902,What Next for Quantum Mechanics in Structure-Based Drug Discovery?,"There is significant potential for electronic structure methods to improve the quality of the predictions furnished by the tools of computer-aided drug design, which typically rely on empirically derived functions. In this perspective, we consider some recent examples of how quantum mechanics has been applied in predicting protein-ligand geometries, protein-ligand binding affinities and ligand strain on binding. We then outline several significant developments in quantum mechanics methodology likely to influence these approaches: in particular, we note the advent of more computationally expedient ab initio quantum mechanical methods that can provide chemical accuracy for larger molecular systems than hitherto possible. We highlight the emergence of increasingly accurate semiempirical quantum mechanical methods and the associated role of machine learning and molecular databases in their development. Indeed, the convergence of improved algorithms for solving and analyzing electronic structure, modern machine learning methods, and increasingly comprehensive benchmark data sets of molecular geometries and energies provides a context in which the potential of quantum mechanics will be increasingly realized in driving future developments and applications in structure-based drug discovery.",2020-06-01,0,1303,66,341
1289,32016900,QM Calculations in ADMET Prediction,"In recent years, there has been an increase in the application of quantum mechanics (QM) methods to describe properties related to the ADMET profile of small molecules. The application of these methods allows calculating useful descriptors and physiochemical properties contributing to ADMET prediction. Considering that QM methods are the only one that describe the electronic state of a molecules, such methods are particularly useful for studying the metabolism of drugs; furthermore, the introduction of mixed QM and molecular mechanics (QM/MM) is also increasing the understanding of drug interaction with cytochromes from a mechanistic point of view. Finally, combining the increase number of experimental data with machine learning algorithms and QM-derived descriptors allowed the creation of an end-user software capable of affecting the drug discovery process.",2020-06-01,0,870,35,341
1299,32006410,CRISPR/Cas9 Guide RNA Design Rules for Predicting Activity,"A critical stage in performing gene editing experiments using the CRISPR/Cas9 system is the design of guide RNA (gRNA). In this chapter, we conduct a review of the current gRNA design rules for maximizing on-target Cas9 activity while minimizing off-target activity. In addition, we present some of the currently available computational tools for gRNA activity prediction and assay design.",2020-06-01,0,389,58,341
1306,31995745,A survey on machine and statistical learning for longitudinal analysis of neuroimaging data in Alzheimer's disease,"Background and objectives:                    Recently, longitudinal studies of Alzheimer's disease have gathered a substantial amount of neuroimaging data. New methods are needed to successfully leverage and distill meaningful information on the progression of the disease from the deluge of available data. Machine learning has been used successfully for many different tasks, including neuroimaging related problems. In this paper, we review recent statistical and machine learning applications in Alzheimer's disease using longitudinal neuroimaging.              Methods:                    We search for papers using longitudinal imaging data, focused on Alzheimer's Disease and published between 2007 and 2019 on four different search engines.              Results:                    After the search, we obtain 104 relevant papers. We analyze their approach to typical challenges in longitudinal data analysis, such as missing data and variability in the number and extent of acquisitions.              Conclusions:                    Reviewed works show that machine learning methods using longitudinal data have potential for disease progression modelling and computer-aided diagnosis. We compare results and models, and propose future research directions in the field.",2020-06-01,0,1279,114,341
2593,32576276,"""Right-to-Try"" experimental drugs: an overview","The ""Right-to-Try"" experimental drugs act passed by Donald Trump in 2018 provides an opportunity of early access to experimental drugs for the treatment of life-threatening diseases and a potential boon to many young and under-capitalized biotechnology or pharmaceutical companies. The pros and cons of experimental drugs, including a number of ""cutting edge"" scientific, clinical, and a number of synergistic approaches such as artificial intelligence, machine learning, big data, data refineries, electronic health records, data driven clinical decisions and risk mitigation are reviewed.",2020-06-01,0,590,46,341
1946,33205106,A Review of Super-Resolution Single-Molecule Localization Microscopy Cluster Analysis and Quantification Methods,"Single-molecule localization microscopy (SMLM) is a relatively new imaging modality, winning the 2014 Nobel Prize in Chemistry, and considered as one of the key super-resolution techniques. SMLM resolution goes beyond the diffraction limit of light microscopy and achieves resolution on the order of 10-20 nm. SMLM thus enables imaging single molecules and study of the low-level molecular interactions at the subcellular level. In contrast to standard microscopy imaging that produces 2D pixel or 3D voxel grid data, SMLM generates big data of 2D or 3D point clouds with millions of localizations and associated uncertainties. This unprecedented breakthrough in imaging helps researchers employ SMLM in many fields within biology and medicine, such as studying cancerous cells and cell-mediated immunity and accelerating drug discovery. However, SMLM data quantification and interpretation methods have yet to keep pace with the rapid advancement of SMLM imaging. Researchers have been actively exploring new computational methods for SMLM data analysis to extract biosignatures of various biological structures and functions. In this survey, we describe the state-of-the-art clustering methods adopted to analyze and quantify SMLM data and examine the capabilities and shortcomings of the surveyed methods. We classify the methods according to (1) the biological application (i.e., the imaged molecules/structures), (2) the data acquisition (such as imaging modality, dimension, resolution, and number of localizations), and (3) the analysis details (2D versus 3D, field of view versus region of interest, use of machine-learning and multi-scale analysis, biosignature extraction, etc.). We observe that the majority of methods that are based on second-order statistics are sensitive to noise and imaging artifacts, have not been applied to 3D data, do not leverage machine-learning formulations, and are not scalable for big-data analysis. Finally, we summarize state-of-the-art methodology, discuss some key open challenges, and identify future opportunities for better modeling and design of an integrated computational pipeline to address the key challenges.",2020-06-01,10,2164,112,341
1402,32670510,Assessment of vector-host-pathogen relationships using data mining and machine learning,"Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.",2020-06-01,1,1938,87,341
1477,32596403,Using Artificial Intelligence Resources in Dialysis and Kidney Transplant Patients: A Literature Review,"Background:                    The purpose of this review is to depict current research and impact of artificial intelligence/machine learning (AI/ML) algorithms on dialysis and kidney transplantation. Published studies were presented from two points of view: What medical aspects were covered? What AI/ML algorithms have been used?              Methods:                    We searched four electronic databases or studies that used AI/ML in hemodialysis (HD), peritoneal dialysis (PD), and kidney transplantation (KT). Sixty-nine studies were split into three categories: AI/ML and HD, PD, and KT, respectively. We identified 43 trials in the first group, 8 in the second, and 18 in the third. Then, studies were classified according to the type of algorithm.              Results:                    AI and HD trials covered: (a) dialysis service management, (b) dialysis procedure, (c) anemia management, (d) hormonal/dietary issues, and (e) arteriovenous fistula assessment. PD studies were divided into (a) peritoneal technique issues, (b) infections, and (c) cardiovascular event prediction. AI in transplantation studies were allocated into (a) management systems (ML used as pretransplant organ-matching tools), (b) predicting graft rejection, (c) tacrolimus therapy modulation, and (d) dietary issues.              Conclusions:                    Although guidelines are reluctant to recommend AI implementation in daily practice, there is plenty of evidence that AI/ML algorithms can predict better than nephrologists: volumes, Kt/V, and hypotension or cardiovascular events during dialysis. Altogether, these trials report a robust impact of AI/ML on quality of life and survival in G5D/T patients. In the coming years, one would probably witness the emergence of AI/ML devices that facilitate the management of dialysis patients, thus increasing the quality of life and survival.",2020-06-01,2,1891,103,341
2637,32506273,E-Health in Hypertension Management: an Insight into the Current and Future Role of Blood Pressure Telemonitoring,"Purpose of review:                    Out-of-office blood pressure (BP) monitoring techniques, including home and ambulatory BP monitoring, are currently recommended by hypertension guidelines worldwide to confirm the diagnosis of hypertension and to monitor the appropriateness of treatment. However, such techniques are not always effectively implemented or timely available in the routine clinical practice. In recent years, the widespread availability of e-health solutions has stimulated the development of blood pressure telemonitoring (BPT) systems, which allow remote BP tracking and tighter and more efficient monitoring of patients' health status.              Recent findings:                    There is currently strong evidence that BPT may be of benefit for hypertension screening and diagnosis and for improving hypertension management. The advantage is more significant when BPT is coupled with multimodal interventions involving a physician, a nurse or pharmacist, and including education on lifestyle and risk factors and drug management. Several randomized controlled studies documented enhanced hypertension management and improved BP control of hypertensive patients through BPT. Potential additional effects of BPT are represented by improved compliance to treatment, intensification, and optimization of drug use, improved quality of life, reduction in risk of developing cardiovascular complications, and cost-saving. Applications based on m-health and making use of wearables or smartwatches integrated with machine learning models are particularly promising for the future development of efficient BPT solutions, and they will provide remarkable support decision tools for doctors. BPT and telehealth will soon disrupt hypertension management. However, which approach will be the most effective and whether it will be sustainable in the long-term still need to be elucidated.",2020-06-01,2,1902,113,341
1456,32612866,Current status and future directions of high-throughput ADME screening in drug discovery,"During the last decade high-throughput in vitro absorption, distribution, metabolism and excretion (HT-ADME) screening has become an essential part of any drug discovery effort of synthetic molecules. The conduct of HT-ADME screening has been ""industrialized"" due to the extensive development of software and automation tools in cell culture, assay incubation, sample analysis and data analysis. The HT-ADME assay portfolio continues to expand in emerging areas such as drug-transporter interactions, early soft spot identification, and ADME screening of peptide drug candidates. Additionally, thanks to the very large and high-quality HT-ADME data sets available in many biopharma companies, in silico prediction of ADME properties using machine learning has also gained much momentum in recent years. In this review, we discuss the current state-of-the-art practices in HT-ADME screening including assay portfolio, assay automation, sample analysis, data processing, and prediction model building. In addition, we also offer perspectives in future development of this exciting field.",2020-06-01,0,1085,88,341
1454,32617332,The combination of brain-computer interfaces and artificial intelligence: applications and challenges,"Brain-computer interfaces (BCIs) have shown great prospects as real-time bidirectional links between living brains and actuators. Artificial intelligence (AI), which can advance the analysis and decoding of neural activity, has turbocharged the field of BCIs. Over the past decade, a wide range of BCI applications with AI assistance have emerged. These ""smart"" BCIs including motor and sensory BCIs have shown notable clinical success, improved the quality of paralyzed patients' lives, expanded the athletic ability of common people and accelerated the evolution of robots and neurophysiological discoveries. However, despite technological improvements, challenges remain with regard to the long training periods, real-time feedback, and monitoring of BCIs. In this article, the authors review the current state of AI as applied to BCIs and describe advances in BCI applications, their challenges and where they could be headed in the future.",2020-06-01,1,944,101,341
2634,32510252,Artificial intelligence: improving the efficiency of cardiovascular imaging,"Introduction:                    Artificial intelligence (AI) describes the use of computational techniques to mimic human intelligence. In healthcare, this typically involves large medical datasets being used to predict a diagnosis, identify new disease genotypes or phenotypes, or guide treatment strategies. Noninvasive imaging remains a cornerstone for the diagnosis, risk stratification, and management of patients with cardiovascular disease. AI can facilitate every stage of the imaging process, from acquisition and reconstruction, to segmentation, measurement, interpretation, and subsequent clinical pathways.              Areas covered:                    In this paper, we review state-of-the-art AI techniques and their current applications in cardiac imaging, and discuss the future role of AI as a precision medicine tool.              Expert opinion:                    Cardiovascular medicine is primed for scalable AI applications which can interpret vast amounts of clinical and imaging data in greater depth than ever before. AI-augmented medical systems have the potential to improve workflow and provide reproducible and objective quantitative results which can inform clinical decisions. In the foreseeable future, AI may work in the background of cardiac image analysis software and routine clinical reporting, automatically collecting data and enabling real-time diagnosis and risk stratification.",2020-06-01,2,1422,75,341
1453,32617334,Application of artificial intelligence in anterior segment ophthalmic diseases: diversity and standardization,"Artificial intelligence (AI) based on machine learning (ML) and deep learning (DL) techniques has gained tremendous global interest in this era. Recent studies have demonstrated the potential of AI systems to provide improved capability in various tasks, especially in image recognition field. As an image-centric subspecialty, ophthalmology has become one of the frontiers of AI research. Trained on optical coherence tomography, slit-lamp images and even ordinary eye images, AI can achieve robust performance in the detection of glaucoma, corneal arcus and cataracts. Moreover, AI models based on other forms of data also performed satisfactorily. Nevertheless, several challenges with AI application in ophthalmology have also arisen, including standardization of data sets, validation and applicability of AI models, and ethical issues. In this review, we provided a summary of the state-of-the-art AI application in anterior segment ophthalmic diseases, potential challenges in clinical implementation and our prospects.",2020-06-01,1,1026,109,341
1866,32695795,Artificial Intelligence in Cardiac Imaging With Statistical Atlases of Cardiac Anatomy,"In many cardiovascular pathologies, the shape and motion of the heart provide important clues to understanding the mechanisms of the disease and how it progresses over time. With the advent of large-scale cardiac data, statistical modeling of cardiac anatomy has become a powerful tool to provide automated, precise quantification of the status of patient-specific heart geometry with respect to reference populations. Powered by supervised or unsupervised machine learning algorithms, statistical cardiac shape analysis can be used to automatically identify and quantify the severity of heart diseases, to provide morphometric indices that are optimally associated with clinical factors, and to evaluate the likelihood of adverse outcomes. Recently, statistical cardiac atlases have been integrated with deep neural networks to enable anatomical consistency of cardiac segmentation, registration, and automated quality control. These combinations have already shown significant improvements in performance and avoid gross anatomical errors that could make the results unusable. This current trend is expected to grow in the near future. Here, we aim to provide a mini review highlighting recent advances in statistical atlasing of cardiac function in the context of artificial intelligence in cardiac imaging.",2020-06-01,1,1310,86,341
2628,32514649,Convolutional neural networks for brain tumour segmentation,"The introduction of quantitative image analysis has given rise to fields such as radiomics which have been used to predict clinical sequelae. One growing area of interest for analysis is brain tumours, in particular glioblastoma multiforme (GBM). Tumour segmentation is an important step in the pipeline in the analysis of this pathology. Manual segmentation is often inconsistent as it varies between observers. Automated segmentation has been proposed to combat this issue. Methodologies such as convolutional neural networks (CNNs) which are machine learning pipelines modelled on the biological process of neurons (called nodes) and synapses (connections) have been of interest in the literature. We investigate the role of CNNs to segment brain tumours by firstly taking an educational look at CNNs and perform a literature search to determine an example pipeline for segmentation. We then investigate the future use of CNNs by exploring a novel field-radiomics. This examines quantitative features of brain tumours such as shape, texture, and signal intensity to predict clinical outcomes such as survival and response to therapy.",2020-06-01,2,1136,59,341
2626,32517304,MRI Segmentation and Classification of Human Brain Using Deep Learning for Diagnosis of Alzheimer's Disease: A Survey,"Many neurological diseases and delineating pathological regions have been analyzed, and the anatomical structure of the brain researched with the aid of magnetic resonance imaging (MRI). It is important to identify patients with Alzheimer's disease (AD) early so that preventative measures can be taken. A detailed analysis of the tissue structures from segmented MRI leads to a more accurate classification of specific brain disorders. Several segmentation methods to diagnose AD have been proposed with varying complexity. Segmentation of the brain structure and classification of AD using deep learning approaches has gained attention as it can provide effective results over a large set of data. Hence, deep learning methods are now preferred over state-of-the-art machine learning methods. We aim to provide an outline of current deep learning-based segmentation approaches for the quantitative analysis of brain MRI for the diagnosis of AD. Here, we report how convolutional neural network architectures are used to analyze the anatomical brain structure and diagnose AD, discuss how brain MRI segmentation improves AD classification, describe the state-of-the-art approaches, and summarize their results using publicly available datasets. Finally, we provide insight into current issues and discuss possible future research directions in building a computer-aided diagnostic system for AD.",2020-06-01,3,1396,117,341
2604,32560091,"COVID-19 Diagnostics, Tools, and Prevention","The Coronavirus Disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), outbreak from Wuhan City, Hubei province, China in 2019 has become an ongoing global health emergency. The emerging virus, SARS-CoV-2, causes coughing, fever, muscle ache, and shortness of breath or dyspnea in symptomatic patients. The pathogenic particles that are generated by coughing and sneezing remain suspended in the air or attach to a surface to facilitate transmission in an aerosol form. This review focuses on the recent trends in pandemic biology, diagnostics methods, prevention tools, and policies for COVID-19 management. To meet the growing demand for medical supplies during the COVID-19 era, a variety of personal protective equipment (PPE) and ventilators have been developed using do-it-yourself (DIY) manufacturing. COVID-19 diagnosis and the prediction of virus transmission are analyzed by machine learning algorithms, simulations, and digital monitoring. Until the discovery of a clinically approved vaccine for COVID-19, pandemics remain a public concern. Therefore, technological developments, biomedical research, and policy development are needed to decipher the coronavirus mechanism and epidemiological characteristics, prevent transmission, and develop therapeutic drugs.",2020-06-01,13,1320,43,341
2625,32517778,The use of machine learning in rare diseases: a scoping review,"Background:                    Emerging machine learning technologies are beginning to transform medicine and healthcare and could also improve the diagnosis and treatment of rare diseases. Currently, there are no systematic reviews that investigate, from a general perspective, how machine learning is used in a rare disease context. This scoping review aims to address this gap and explores the use of machine learning in rare diseases, investigating, for example, in which rare diseases machine learning is applied, which types of algorithms and input data are used or which medical applications (e.g., diagnosis, prognosis or treatment) are studied.              Methods:                    Using a complex search string including generic search terms and 381 individual disease names, studies from the past 10 years (2010-2019) that applied machine learning in a rare disease context were identified on PubMed. To systematically map the research activity, eligible studies were categorized along different dimensions (e.g., rare disease group, type of algorithm, input data), and the number of studies within these categories was analyzed.              Results:                    Two hundred eleven studies from 32 countries investigating 74 different rare diseases were identified. Diseases with a higher prevalence appeared more often in the studies than diseases with a lower prevalence. Moreover, some rare disease groups were investigated more frequently than to be expected (e.g., rare neurologic diseases and rare systemic or rheumatologic diseases), others less frequently (e.g., rare inborn errors of metabolism and rare skin diseases). Ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%) were the algorithms most commonly applied in the studies. Only a small proportion of studies evaluated their algorithms on an external data set (11.8%) or against a human expert (2.4%). As input data, images (32.2%), demographic data (27.0%) and ""omics"" data (26.5%) were used most frequently. Most studies used machine learning for diagnosis (40.8%) or prognosis (38.4%) whereas studies aiming to improve treatment were relatively scarce (4.7%). Patient numbers in the studies were small, typically ranging from 20 to 99 (35.5%).              Conclusion:                    Our review provides an overview of the use of machine learning in rare diseases. Mapping the current research activity, it can guide future work and help to facilitate the successful application of machine learning in rare diseases.",2020-06-01,0,2556,62,341
1428,32637935,Artificial intelligence in luminal endoscopy,"Artificial intelligence is a strong focus of interest for global health development. Diagnostic endoscopy is an attractive substrate for artificial intelligence with a real potential to improve patient care through standardisation of endoscopic diagnosis and to serve as an adjunct to enhanced imaging diagnosis. The possibility to amass large data to refine algorithms makes adoption of artificial intelligence into global practice a potential reality. Initial studies in luminal endoscopy involve machine learning and are retrospective. Improvement in diagnostic performance is appreciable through the adoption of deep learning. Research foci in the upper gastrointestinal tract include the diagnosis of neoplasia, including Barrett's, squamous cell and gastric where prospective and real-time artificial intelligence studies have been completed demonstrating a benefit of artificial intelligence-augmented endoscopy. Deep learning applied to small bowel capsule endoscopy also appears to enhance pathology detection and reduce capsule reading time. Prospective evaluation including the first randomised trial has been performed in the colon, demonstrating improved polyp and adenoma detection rates; however, these appear to be relevant to small polyps. There are potential additional roles of artificial intelligence relevant to improving the quality of endoscopic examinations, training and triaging of referrals. Further large-scale, multicentre and cross-platform validation studies are required for the robust incorporation of artificial intelligence-augmented diagnostic luminal endoscopy into our routine clinical practice.",2020-06-01,0,1633,44,341
1429,32637301,Calibration and validation of accelerometry using cut-points to assess physical activity in paediatric clinical groups: A systematic review,"Regular physical activity is associated with physiological and psychosocial benefits in both healthy and clinical populations. However, little is known about tailoring the analysis of physical activity using accelerometers to the specific characteristics of chronic conditions. Whilst accelerometry is broadly used to assess physical activity, recommendations on calibration in paediatric clinical groups are warranted. The aim of this systematic review was to provide a critical overview of protocols used to calibrate accelerometry in children and adolescents with clinical conditions, as well as to develop recommendations for calibration and validation of accelerometry in such populations. The search was performed between March to July 2017 using text words and subject headings in six databases. Studies had to develop moderate-to-vigorous intensity physical activity (MVPA) cut-points for paediatric clinical populations to be included. Risk of bias was assessed using a specific checklist. A total of 540,630 titles were identified, with 323 full-text articles assessed. Five studies involving 347 participants aged 9 to 15 years were included. Twenty-four MVPA cut-points were reported across seven clinical conditions, 16 of which were developed for different models of ActiGraph, seven for Actical and one for Tritrac-R3D. Statistical approaches included mixed regression, machine learning and receiver operating characteristic analyses. Disease-specific MVPA cut-points ranged from 152 to 735 counts15 s-1, with lower cut-points found for inherited muscle disease and higher cut-points associated with intellectual disabilities. The lower MVPA cut-points for diseases characterised by both ambulatory and metabolic impairments likely reflect the higher energetic demands associated with those conditions.",2020-06-01,0,1818,139,341
2616,32532187,A Review of Drug Side Effect Identification Methods,"Drug side effects have become an important indicator for evaluating the safety of drugs. There are two main factors in the frequent occurrence of drug safety problems; on the one hand, the clinical understanding of drug side effects is insufficient, leading to frequent adverse drug reactions, while on the other hand, due to the long-term period and complexity of clinical trials, side effects of approved drugs on the market cannot be reported in a timely manner. Therefore, many researchers have focused on developing methods to identify drug side effects. In this review, we summarize the methods of identifying drug side effects and common databases in this field. We classified methods of identifying side effects into four categories: biological experimental, machine learning, text mining and network methods. We point out the key points of each kind of method. In addition, we also explain the advantages and disadvantages of each method. Finally, we propose future research directions.",2020-06-01,0,995,51,341
2612,32542455,Machine Learning for 3D Kinematic Analysis of Movements in Neurorehabilitation,"Purpose of review:                    Recent advances in the machine learning field, especially in deep learning, provide the opportunity for automated, detailed, and unbiased analysis of motor behavior. Although there has not yet been wide use of these techniques in the motor rehabilitation field, they have great potential. In this review, I describe how the current state of machine learning can be applied to 3D kinematic analysis, and how this will have an impact on neurorehabilitation.              Recent findings:                    Applications of deep learning methods, in the form of convolutional neural networks, have been revolutionary for image analysis such as face recognition and object detection in images, exceeding human level performance. Recent studies have shown applicability of these deep learning approaches to human posture and movement classification. It is to be expected that portable stereo-camera systems will bring 3D pose estimation into the clinical setting and allow the assessment of movement quality in response to interventions. Advances in machine learning can help automate the process of obtaining 3D kinematics of human movements and to identify/classify patterns of movement.",2020-06-01,1,1222,78,341
2611,32545768,Machine Learning Applications for Mass Spectrometry-Based Metabolomics,"The metabolome of an organism depends on environmental factors and intracellular regulation and provides information about the physiological conditions. Metabolomics helps to understand disease progression in clinical settings or estimate metabolite overproduction for metabolic engineering. The most popular analytical metabolomics platform is mass spectrometry (MS). However, MS metabolome data analysis is complicated, since metabolites interact nonlinearly, and the data structures themselves are complex. Machine learning methods have become immensely popular for statistical analysis due to the inherent nonlinear data representation and the ability to process large and heterogeneous data rapidly. In this review, we address recent developments in using machine learning for processing MS spectra and show how machine learning generates new biological insights. In particular, supervised machine learning has great potential in metabolomics research because of the ability to supply quantitative predictions. We review here commonly used tools, such as random forest, support vector machines, artificial neural networks, and genetic algorithms. During processing steps, the supervised machine learning methods help peak picking, normalization, and missing data imputation. For knowledge-driven analysis, machine learning contributes to biomarker detection, classification and regression, biochemical pathway identification, and carbon flux determination. Of important relevance is the combination of different omics data to identify the contributions of the various regulatory levels. Our overview of the recent publications also highlights that data quality determines analysis quality, but also adds to the challenge of choosing the right model for the data. Machine learning methods applied to MS-based metabolomics ease data analysis and can support clinical decisions, guide metabolic engineering, and stimulate fundamental biological discoveries.",2020-06-01,9,1959,70,341
1430,32637044,Deep learning models in genomics; are we there yet?,"With the evolution of biotechnology and the introduction of the high throughput sequencing, researchers have the ability to produce and analyze vast amounts of genomics data. Since genomics produce big data, most of the bioinformatics algorithms are based on machine learning methodologies, and lately deep learning, to identify patterns, make predictions and model the progression or treatment of a disease. Advances in deep learning created an unprecedented momentum in biomedical informatics and have given rise to new bioinformatics and computational biology research areas. It is evident that deep learning models can provide higher accuracies in specific tasks of genomics than the state of the art methodologies. Given the growing trend on the application of deep learning architectures in genomics research, in this mini review we outline the most prominent models, we highlight possible pitfalls and discuss future directions. We foresee deep learning accelerating changes in the area of genomics, especially for multi-scale and multimodal data analysis for precision medicine.",2020-06-01,4,1086,51,341
2638,32503819,Coronavirus Disease (COVID-19): A Machine Learning Bibliometric Analysis,"Background/aim:                    To evaluate the research trends in coronavirus disease (COVID-19).              Materials and methods:                    A bibliometric analysis was performed using a machine learning bibliometric methodology. Information regarding publication outputs, countries, institutions, journals, keywords, funding and citation counts was retrieved from Scopus database.              Results:                    A total of 1883 eligible papers were returned. An exponential increase in the COVID-19 publications occurred in the last months. As expected, China produced the majority of articles, followed by the United States of America, the United Kingdom and Italy. There is greater collaboration between highly contributing authors and institutions. The ""BMJ"" published the highest number of papers (n=129) and ""The Lancet"" had the most citations (n=1439). The most ubiquitous topic was COVID-19 clinical features.              Conclusion:                    This bibliometric analysis presents the most influential references related to COVID-19 during this time and could be useful to improve understanding and management of COVID-19.",2020-06-01,13,1165,72,341
1867,32695678,Integrated Multi-Omics Analyses in Oncology: A Review of Machine Learning Methods and Tools,"In recent years, high-throughput sequencing technologies provide unprecedented opportunity to depict cancer samples at multiple molecular levels. The integration and analysis of these multi-omics datasets is a crucial and critical step to gain actionable knowledge in a precision medicine framework. This paper explores recent data-driven methodologies that have been developed and applied to respond major challenges of stratified medicine in oncology, including patients' phenotyping, biomarker discovery, and drug repurposing. We systematically retrieved peer-reviewed journals published from 2014 to 2019, select and thoroughly describe the tools presenting the most promising innovations regarding the integration of heterogeneous data, the machine learning methodologies that successfully tackled the complexity of multi-omics data, and the frameworks to deliver actionable results for clinical practice. The review is organized according to the applied methods: Deep learning, Network-based methods, Clustering, Features Extraction, and Transformation, Factorization. We provide an overview of the tools available in each methodological group and underline the relationship among the different categories. Our analysis revealed how multi-omics datasets could be exploited to drive precision oncology, but also current limitations in the development of multi-omics data integration.",2020-06-01,7,1388,91,341
2601,32562100,Machine Learning-Based Segmentation of Left Ventricular Myocardial Fibrosis from Magnetic Resonance Imaging,"Purpose of review:                    Myocardial fibrosis (MF) arises due to myocardial infarction and numerous cardiac diseases. MF may lead to several heart disorders, such as heart failure, arrhythmias, and ischemia. Cardiac magnetic resonance (CMR) imaging techniques, such as late gadolinium enhancement (LGE) CMR, enable non-invasive assessment of MF in the left ventricle (LV). Manual assessment of MF on CMR is a tedious and time-consuming task that is subject to high observer variability. Automated segmentation and quantification of MF is important for risk stratification and treatment planning in patients with heart disorders. This article aims to review the machine learning (ML)-based methodologies developed for MF quantification in the LV using CMR images.              Recent findings:                    With the availability of relatively large labeled datasets supervised learning methods based on both conventional ML and state-of-the-art deep learning (DL) methods have been successfully applied for automated segmentation of MF. The incorporation of ML algorithms into imaging techniques such as 3D LGE CMR permits fast characterization of MF on CMR imaging and may enhance the diagnosis and prognosis of patients with heart disorders. Concurrently, the studies using cine CMR images have revealed that accurate segmentation of MF on non-contrast CMR imaging might be possible. The application of ML/DL tools in CMR image interpretation is likely to result in accurate and efficient quantification of MF.",2020-06-01,0,1529,107,341
2651,32479180,Reporting and Implementing Interventions Involving Machine Learning and Artificial Intelligence,"Increasingly, interventions aimed at improving care are likely to use such technologies as machine learning and artificial intelligence. However, health care has been relatively late to adopt them. This article provides clinical examples in which machine learning and artificial intelligence are already in use in health care and appear to deliver benefit. Three key bottlenecks toward increasing the pace of diffusion and adoption are methodological issues in evaluation of artificial intelligence-based interventions, reporting standards to enable assessment of model performance, and issues that need to be addressed for an institution to adopt these interventions. Methodological best practices will include external validation, ideally at a different site; use of proactive learning algorithms to correct for site-specific biases and increase robustness as algorithms are deployed across multiple sites; addressing subgroup performance; and communicating to providers the uncertainty of predictions. Regarding reporting, especially important issues are the extent to which implementing standardized approaches for introducing clinical decision support has been followed, describing the data sources, reporting on data assumptions, and addressing biases. Although most health care organizations in the United States have adopted electronic health records, they may be ill prepared to adopt machine learning and artificial intelligence. Several steps can enable this: preparing data, developing tools to get suggestions to clinicians in useful ways, and getting clinicians engaged in the process. Open challenges and the role of regulation in this area are briefly discussed. Although these techniques have enormous potential to improve care and personalize recommendations for individuals, the hype regarding them is tremendous. Organizations will need to approach this domain carefully with knowledgeable partners to obtain the hoped-for benefits and avoid failures.",2020-06-01,3,1971,95,341
2161,31734566,Machine learning for target discovery in drug development,"The discovery of macromolecular targets for bioactive agents is currently a bottleneck for the informed design of chemical probes and drug leads. Typically, activity profiling against genetically manipulated cell lines or chemical proteomics is pursued to shed light on their biology and deconvolute drug-target networks. By taking advantage of the ever-growing wealth of publicly available bioactivity data, learning algorithms now provide an attractive means to generate statistically motivated research hypotheses and thereby prioritize biochemical screens. Here, we highlight recent successes in machine intelligence for target identification and discuss challenges and opportunities for drug discovery.",2020-06-01,4,707,57,341
1431,32637040,Constructing knowledge graphs and their biomedical applications,"Knowledge graphs can support many biomedical applications. These graphs represent biomedical concepts and relationships in the form of nodes and edges. In this review, we discuss how these graphs are constructed and applied with a particular focus on how machine learning approaches are changing these processes. Biomedical knowledge graphs have often been constructed by integrating databases that were populated by experts via manual curation, but we are now seeing a more robust use of automated systems. A number of techniques are used to represent knowledge graphs, but often machine learning methods are used to construct a low-dimensional representation that can support many different applications. This representation is designed to preserve a knowledge graph's local and/or global structure. Additional machine learning methods can be applied to this representation to make predictions within genomic, pharmaceutical, and clinical domains. We frame our discussion first around knowledge graph construction and then around unifying representational learning techniques and unifying applications. Advances in machine learning for biomedicine are creating new opportunities across many domains, and we note potential avenues for future work with knowledge graphs that appear particularly promising.",2020-06-01,3,1305,63,341
1403,32670474,Artificial Intelligence in Cardiovascular Imaging,"The number of cardiovascular imaging studies is growing exponentially, and so is the need to improve clinical workflow efficiency and avoid missed diagnoses. With the availability and use of large datasets, artificial intelligence (AI) has the potential to improve patient care at every stage of the imaging chain. Current literature indicates that in the short-term, AI has the capacity to reduce human error and save time in the clinical workflow through automated segmentation of cardiac structures. In the future, AI may expand the informational value of diagnostic images based on images alone or a combination of images and clinical variables, thus facilitating disease detection, prognosis, and decision making. This review describes the role of AI, specifically machine learning, in multimodality imaging, including echocardiography, nuclear imaging, computed tomography, and cardiac magnetic resonance, and highlights current uses of AI as well as potential challenges to its widespread implementation.",2020-06-01,0,1011,49,341
1836,32733875,Midbrain Dopaminergic Neuron Development at the Single Cell Level: In vivo and in Stem Cells,"Parkinson's disease (PD) is a progressive neurodegenerative disorder that predominantly affects dopaminergic (DA) neurons of the substantia nigra. Current treatment options for PD are symptomatic and typically involve the replacement of DA neurotransmission by DA drugs, which relieve the patients of some of their motor symptoms. However, by the time of diagnosis, patients have already lost about 70% of their substantia nigra DA neurons and these drugs offer only temporary relief. Therefore, cell replacement therapy has garnered much interest as a potential treatment option for PD. Early studies using human fetal tissue for transplantation in PD patients provided proof of principle for cell replacement therapy, but they also highlighted the ethical and practical difficulties associated with using human fetal tissue as a cell source. In recent years, advancements in stem cell research have made human pluripotent stem cells (hPSCs) an attractive source of material for cell replacement therapy. Studies on how DA neurons are specified and differentiated in the developing mouse midbrain have allowed us to recapitulate many of the positional and temporal cues needed to generate DA neurons in vitro. However, little is known about the developmental programs that govern human DA neuron development. With the advent of single-cell RNA sequencing (scRNA-seq) and bioinformatics, it has become possible to analyze precious human samples with unprecedented detail and extract valuable high-quality information from large data sets. This technology has allowed the systematic classification of cell types present in the human developing midbrain along with their gene expression patterns. By studying human development in such an unbiased manner, we can begin to elucidate human DA neuron development and determine how much it differs from our knowledge of the rodent brain. Importantly, this molecular description of the function of human cells has become and will increasingly be a reference to define, evaluate, and engineer cell types for PD cell replacement therapy and disease modeling.",2020-06-01,0,2098,92,341
1470,32601582,Clinical applications and performance of intelligent systems in dental and maxillofacial radiology: A review,"Intelligent systems (i.e., artificial intelligence), particularly deep learning, are machines able to mimic the cognitive functions of humans to perform tasks of problem-solving and learning. This field deals with computational models that can think and act intelligently, like the human brain, and construct algorithms that can learn from data to make predictions. Artificial intelligence is becoming important in radiology due to its ability to detect abnormalities in radiographic images that are unnoticed by the naked human eye. These systems have reduced radiologists' workload by rapidly recording and presenting data, and thereby monitoring the treatment response with a reduced risk of cognitive bias. Intelligent systems have an important role to play and could be used by dentists as an adjunct to other imaging modalities in making appropriate diagnoses and treatment plans. In the field of maxillofacial radiology, these systems have shown promise for the interpretation of complex images, accurate localization of landmarks, characterization of bone architecture, estimation of oral cancer risk, and the assessment of metastatic lymph nodes, periapical pathologies, and maxillary sinus pathologies. This review discusses the clinical applications and scope of intelligent systems such as machine learning, artificial intelligence, and deep learning programs in maxillofacial imaging.",2020-06-01,2,1397,108,341
1467,32602844,Artificial Intelligence Education and Tools for Medical and Health Informatics Students: Systematic Review,"Background:                    The use of artificial intelligence (AI) in medicine will generate numerous application possibilities to improve patient care, provide real-time data analytics, and enable continuous patient monitoring. Clinicians and health informaticians should become familiar with machine learning and deep learning. Additionally, they should have a strong background in data analytics and data visualization to use, evaluate, and develop AI applications in clinical practice.              Objective:                    The main objective of this study was to evaluate the current state of AI training and the use of AI tools to enhance the learning experience.              Methods:                    A comprehensive systematic review was conducted to analyze the use of AI in medical and health informatics education, and to evaluate existing AI training practices. PRISMA-P (Preferred Reporting Items for Systematic Reviews and Meta-Analysis Protocols) guidelines were followed. The studies that focused on the use of AI tools to enhance medical education and the studies that investigated teaching AI as a new competency were categorized separately to evaluate recent developments.              Results:                    This systematic review revealed that recent publications recommend the integration of AI training into medical and health informatics curricula.              Conclusions:                    To the best of our knowledge, this is the first systematic review exploring the current state of AI education in both medicine and health informatics. Since AI curricula have not been standardized and competencies have not been determined, a framework for specialized AI training in medical and health informatics education is proposed.",2020-06-01,4,1771,106,341
2597,32567255,Drawing Guidelines for Receiver Operating Characteristic Curve in Preparation of Manuscripts,"The appropriate plot effectively conveys the author's conclusions to the readers. The Journal of Korean Medical Science provides a series of special articles to show you how to make consistent and excellent plots easier. In the second article, drawing receiver operating characteristic (ROC) curve is introduced. A ROC curve is a graphic plot that illustrates the diagnostic ability as its discrimination threshold is varied. It is widely used as logistic regression analysis as machine learning becomes widespread. It has great visual effect in comparing various diagnostic tools.",2020-06-01,0,581,92,341
1827,32754171,Systematic Multi-Omics Integration (MOI) Approach in Plant Systems Biology,"Across all facets of biology, the rapid progress in high-throughput data generation has enabled us to perform multi-omics systems biology research. Transcriptomics, proteomics, and metabolomics data can answer targeted biological questions regarding the expression of transcripts, proteins, and metabolites, independently, but a systematic multi-omics integration (MOI) can comprehensively assimilate, annotate, and model these large data sets. Previous MOI studies and reviews have detailed its usage and practicality on various organisms including human, animals, microbes, and plants. Plants are especially challenging due to large poorly annotated genomes, multi-organelles, and diverse secondary metabolites. Hence, constructive and methodological guidelines on how to perform MOI for plants are needed, particularly for researchers newly embarking on this topic. In this review, we thoroughly classify multi-omics studies on plants and verify workflows to ensure successful omics integration with accurate data representation. We also propose three levels of MOI, namely element-based (level 1), pathway-based (level 2), and mathematical-based integration (level 3). These MOI levels are described in relation to recent publications and tools, to highlight their practicality and function. The drawbacks and limitations of these MOI are also discussed for future improvement toward more amenable strategies in plant systems biology.",2020-06-01,6,1438,74,341
1464,32605077,Brain-Computer Interface-Based Humanoid Control: A Review,"A Brain-Computer Interface (BCI) acts as a communication mechanism using brain signals to control external devices. The generation of such signals is sometimes independent of the nervous system, such as in Passive BCI. This is majorly beneficial for those who have severe motor disabilities. Traditional BCI systems have been dependent only on brain signals recorded using Electroencephalography (EEG) and have used a rule-based translation algorithm to generate control commands. However, the recent use of multi-sensor data fusion and machine learning-based translation algorithms has improved the accuracy of such systems. This paper discusses various BCI applications such as tele-presence, grasping of objects, navigation, etc. that use multi-sensor fusion and machine learning to control a humanoid robot to perform a desired task. The paper also includes a review of the methods and system design used in the discussed applications.",2020-06-01,0,939,57,341
1844,32728381,Machine learning in oncology: a review,"Machine learning is a set of techniques that promise to greatly enhance our data-processing capability. In the field of oncology, ML presents itself with a wealth of possible applications to the research and the clinical context, such as automated diagnosis and precise treatment modulation. In this paper, we will review the principal applications of ML techniques in oncology and explore in detail how they work. This will allow us to discuss the issues and challenges that ML faces in this field, and ultimately gain a greater understanding of ML techniques and how they can improve oncological research and practice.",2020-06-01,0,620,38,341
2661,32457178,Monitoring Big Data During Mechanical Ventilation in the ICU,"The electronic health record allows the assimilation of large amounts of clinical and laboratory data. Big data describes the analysis of large data sets using computational modeling to reveal patterns, trends, and associations. How can big data be used to predict ventilator discontinuation or impending compromise, and how can it be incorporated into the clinical workflow? This article will serve 2 purposes. First, a general overview is provided for the layperson and introduces key concepts, definitions, best practices, and things to watch out for when reading a paper that incorporates machine learning. Second, recent publications at the intersection of big data, machine learning, and mechanical ventilation are presented.",2020-06-01,0,731,60,341
1410,32656188,On the Conformational Dynamics of -Amyloid Forming Peptides: A Computational Perspective,"Understanding the conformational dynamics of proteins and peptides involved in important functions is still a difficult task in computational structural biology. Because such conformational transitions in -amyloid (A) forming peptides play a crucial role in many neurological disorders, researchers from different scientific fields have been trying to address issues related to the folding of A forming peptides together. Many theoretical models have been proposed in the recent years for studying A peptides using mathematical, physicochemical, and molecular dynamics simulation, and machine learning approaches. In this article, we have comprehensively reviewed the developmental advances in the theoretical models for A peptide folding and interactions, particularly in the context of neurological disorders. Furthermore, we have extensively reviewed the advances in molecular dynamics simulation as a tool used for studying the conversions between polymorphic amyloid forms and applications of using machine learning approaches in predicting A peptides and aggregation-prone regions in proteins. We have also provided details on the theoretical advances in the study of A peptides, which would enhance our understanding of these peptides at the molecular level and eventually lead to the development of targeted therapies for certain acute neurological disorders such as Alzheimer's disease in the future.",2020-06-01,2,1416,89,341
2598,32566759,Machine learning approaches to drug response prediction: challenges and recent progress,"Cancer is a leading cause of death worldwide. Identifying the best treatment using computational models to personalize drug response prediction holds great promise to improve patient's chances of successful recovery. Unfortunately, the computational task of predicting drug response is very challenging, partially due to the limitations of the available data and partially due to algorithmic shortcomings. The recent advances in deep learning may open a new chapter in the search for computational drug response prediction models and ultimately result in more accurate tools for therapy response. This review provides an overview of the computational challenges and advances in drug response prediction, and focuses on comparing the machine learning techniques to be of utmost practical use for clinicians and machine learning non-experts. The incorporation of new data modalities such as single-cell profiling, along with techniques that rapidly find effective drug combinations will likely be instrumental in improving cancer care.",2020-06-01,5,1033,87,341
1411,32655344,A Review of Sensory Feedback in Upper-Limb Prostheses From the Perspective of Human Motor Control,"This manuscript reviews historical and recent studies that focus on supplementary sensory feedback for use in upper limb prostheses. It shows that the inability of many studies to speak to the issue of meaningful performance improvements in real-life scenarios is caused by the complexity of the interactions of supplementary sensory feedback with other types of feedback along with other portions of the motor control process. To do this, the present manuscript frames the question of supplementary feedback from the perspective of computational motor control, providing a brief review of the main advances in that field over the last 20 years. It then separates the studies on the closed-loop prosthesis control into distinct categories, which are defined by relating the impact of feedback to the relevant components of the motor control framework, and reviews the work that has been done over the last 50+ years in each of those categories. It ends with a discussion of the studies, along with suggestions for experimental construction and connections with other areas of research, such as machine learning.",2020-06-01,5,1111,97,341
2600,32562154,Artificial Intelligence-Enabled ECG: a Modern Lens on an Old Technology,"Purpose of review:                    To (i) review the concept of artificial intelligence (AI); (ii) summarize recent developments in artificial intelligence-enabled electrocardiogram (AI-ECG); (iii) address notable inherent limitations and challenges of AI-ECG; and (iv) discuss the future direction of the field.              Recent findings:                    Advancements in machine learning and computing methods have led to application of AI-ECG and potential new applications to patient care. Further study is needed to verify previous findings in diverse populations as well as begin to confront the limitations needed for clinical implementation. Nearly one century after the Nobel Prize was awarded to Willem Einthoven for demonstrating that an electrocardiogram (ECG) could record the electrical signature of the heart, the ECG remains one of the most important diagnostic tests in modern medicine. We now stand at the edge of true ECG innovation. Simultaneous advancements in computing power, wireless technology, digitized data availability, and machine learning have led to the birth of AI-ECG algorithms with novel capabilities and real potential for clinical application. AI has the potential to improve diagnostic accuracy and efficiency by providing fully automated, unbiased, and unambiguous ECG analysis along with promising new findings that may unlock new value in the ECG. These breakthroughs may cause a paradigm shift in clinical workflow as well as patient monitoring and management.",2020-06-01,1,1511,71,341
2655,32468528,Artificial Neural Networks in Computer-Aided Drug Design: An Overview of Recent Advances,"Computer-aided drug design (CADD) is the framework in which the huge amount of data accumulated by high-throughput experimental methods used in drug design is quantitatively studied. Its objectives include pattern recognition, biomarker identification and/or classification, etc. In order to achieve these objectives, machine learning algorithms and especially artificial neural networks (ANNs) have been used over ADMET factor testing and QSAR modeling evaluation. This paper provides an overview of the current trends in CADD-applied ANNs, since their use was re-boosted over a decade ago.",2020-06-01,0,591,88,341
1462,32605325,"Path Planning Strategies to Optimize Accuracy, Quality, Build Time and Material Use in Additive Manufacturing: A Review","Additive manufacturing (AM) is the process of joining materials layer by layer to fabricate products based on 3D models. Due to the layer-by-layer nature of AM, parts with complex geometries, integrated assemblies, customized geometry or multifunctional designs can now be manufactured more easily than traditional subtractive manufacturing. Path planning in AM is an important step in the process of manufacturing products. The final fabricated qualities, properties, etc., will be different when using different path strategies, even using the same AM machine and process parameters. Currently, increasing research studies have been published on path planning strategies with different aims. Due to the rapid development of path planning in AM and various newly proposed strategies, there is a lack of comprehensive reviews on this topic. Therefore, this paper gives a comprehensive understanding of the current status and challenges of AM path planning. This paper reviews and discusses path planning strategies in three categories: improving printed qualities, saving materials/time and achieving objective printed properties. The main findings of this review include: new path planning strategies can be developed by combining some of the strategies in literature with better performance; a path planning platform can be developed to help select the most suitable path planning strategy with required properties; research on path planning considering energy consumption can be carried out in the future; a benchmark model for testing the performance of path planning strategies can be designed; the trade-off among different fabricated properties can be considered as a factor in future path planning design processes; and lastly, machine learning can be a powerful tool to further improve path planning strategies in the future.",2020-06-01,2,1834,119,341
1463,32605178,A Survey of IoT Security Based on a Layered Architecture of Sensing and Data Analysis,"The Internet of Things (IoT) is leading today's digital transformation. Relying on a combination of technologies, protocols, and devices such as wireless sensors and newly developed wearable and implanted sensors, IoT is changing every aspect of daily life, especially recent applications in digital healthcare. IoT incorporates various kinds of hardware, communication protocols, and services. This IoT diversity can be viewed as a double-edged sword that provides comfort to users but can lead also to a large number of security threats and attacks. In this survey paper, a new compacted and optimized architecture for IoT is proposed based on five layers. Likewise, we propose a new classification of security threats and attacks based on new IoT architecture. The IoT architecture involves a physical perception layer, a network and protocol layer, a transport layer, an application layer, and a data and cloud services layer. First, the physical sensing layer incorporates the basic hardware used by IoT. Second, we highlight the various network and protocol technologies employed by IoT, and review the security threats and solutions. Transport protocols are exhibited and the security threats against them are discussed while providing common solutions. Then, the application layer involves application protocols and lightweight encryption algorithms for IoT. Finally, in the data and cloud services layer, the main important security features of IoT cloud platforms are addressed, involving confidentiality, integrity, authorization, authentication, and encryption protocols. The paper is concluded by presenting the open research issues and future directions towards securing IoT, including the lack of standardized lightweight encryption algorithms, the use of machine-learning algorithms to enhance security and the related challenges, the use of Blockchain to address security challenges in IoT, and the implications of IoT deployment in 5G and beyond.",2020-06-01,1,1964,85,341
2656,32468523,Neuroeducation and Computer Programming: A Review,"Over the past 5 years, a significant number of studies focused on computer programming and code writing (software development, code comprehension, program debugging, code optimization, developer training), using the capabilities of brain imaging techniques and of biomarkers. With the use of the aforementioned techniques, researchers have explored the role of programming experience and knowledge, the relation between coding and writing, and the possibilities of improving program debugging with machine learning techniques. In this paper, a review of existing literature and discussion of research issues that should be examined in the future are explored. Research may link the neuroscientific field with training issues in programming, so as to contribute to the learning process.",2020-06-01,0,785,49,341
995,31874386,Recent advances in glycoinformatic platforms for glycomics and glycoproteomics,"Protein glycosylation is the most complex and prevalent post-translation modification in terms of the number of proteins modified and the diversity generated. To understand the functional roles of glycoproteins it is important to gain an insight into the repertoire of oligosaccharides present. The comparison and relative quantitation of glycoforms combined with site-specific identification and occupancy are necessary steps in this direction. Computational platforms have continued to mature assisting researchers with the interpretation of such glycomics and glycoproteomics data sets, but frequently support dedicated workflows and users rely on the manual interpretation of data to gain insights into the glycoproteome. The growth of site-specific knowledge has also led to the implementation of machine-learning algorithms to predict glycosylation which is now being integrated into glycoproteomics pipelines. This short review describes commercial and open-access databases and software with an emphasis on those that are actively maintained and designed to support current analytical workflows.",2020-06-01,12,1103,78,341
1230,32101126,Decoding Protein-protein Interactions: An Overview,"Drug discovery has focused on the paradigm ""one drug, one target"" for a long time. However, small molecules can act at multiple macromolecular targets, which serves as the basis for drug repurposing. In an effort to expand the target space, and given advances in X-ray crystallography, protein-protein interactions have become an emerging focus area of drug discovery enterprises. Proteins interact with other biomolecules and it is this intricate network of interactions that determines the behavior of the system and its biological processes. In this review, we briefly discuss networks in disease, followed by computational methods for protein-protein complex prediction. Computational methodologies and techniques employed towards objectives such as protein-protein docking, protein-protein interactions, and interface predictions are described extensively. Docking aims at producing a complex between proteins, while interface predictions identify a subset of residues on one protein that could interact with a partner, and protein-protein interaction sites address whether two proteins interact. In addition, approaches to predict hot spots and binding sites are presented along with a representative example of our internal project on the chemokine CXC receptor 3 B-isoform and predictive modeling with IP10 and PF4.",2020-06-01,1,1323,50,341
55,32334372,A review of original articles published in the emerging field of radiomics,"Purpose:                    To determine the characteristics of and trends in research in the emerging field of radiomics through bibliometric and hotspot analyses of relevant original articles published between 2013 and 2018.              Methods:                    We evaluated 553 original articles concerning radiomics, published in a total of 61 peer-reviewed journals between 2013 and 2018. The following information was retrieved for each article: radiological subspecialty, imaging technique(s), machine learning technique(s), sample size, study setting and design, statistical result(s), study purpose, software used for feature calculation, funding declarations, author number, first author's affiliation, study origin, and journal name. Qualitative and quantitative analyses were performed for the manually extracted data for identification and visualization of the trends in radiomics research.              Results:                    The annual growth rate in the number of published papers was 177.82% (p < 0.001). The characteristics and trends of research hotspots in the field of radiomics were clarified and visualized in this study. It was found that the field of radiomics is at a more mature stage for lung, breast, and prostate cancers than for other sites. Radiomics studies primarily focused on radiological characterization (215) and monitoring (182). Logistic regression and LASSO were the two most commonly used techniques for feature selection. Non-clinical researchers without a medical background dominated radiomics studies (70.52%), the vast majority of which only highlighted positive results (97.80%) while downplaying negative findings.              Conclusions:                    The reporting of quantifiable knowledge about the characteristics and trajectories of radiomics can inform researchers about the gaps in the field of radiomics and guide its future direction.",2020-06-01,8,1910,74,341
2064,33437151,Artificial Intelligence in Drug Discovery: A Comprehensive Review of Data-driven and Machine Learning Approaches,"As expenditure on drug development increases exponentially, the overall drug discovery process requires a sustainable revolution. Since artificial intelligence (AI) is leading the fourth industrial revolution, AI can be considered as a viable solution for unstable drug research and development. Generally, AI is applied to fields with sufficient data such as computer vision and natural language processing, but there are many efforts to revolutionize the existing drug discovery process by applying AI. This review provides a comprehensive, organized summary of the recent research trends in AI-guided drug discovery process including target identification, hit identification, ADMET prediction, lead optimization, and drug repositioning. The main data sources in each field are also summarized in this review. In addition, an in-depth analysis of the remaining challenges and limitations will be provided, and proposals for promising future directions in each of the aforementioned areas.",2020-06-01,0,991,112,341
60,32324706,E-health and multiple sclerosis,"Purpose of review:                    To outline recent applications of e-health data and digital tools for improving the care and management of healthcare for people with multiple sclerosis.              Recent findings:                    The digitization of most clinical data, along with developments in communication technologies, miniaturization of sensors and computational advances are enabling aggregation and clinically meaningful analyses of real-world data from patient registries, digital patient-reported outcomes and electronic health records (EHR). These data are allowing more confident descriptions of prognoses for multiple sclerosis patients and the long-term relative benefits and safety of disease-modifying treatments (DMT). Registries allow detailed, multiple sclerosis-specific data to be shared between clinicians more easily, provide data needed to improve the impact of DMT and, with EHR, characterize clinically relevant interactions between multiple sclerosis and other diseases. Wearable sensors provide continuous, long-term measures of performance dynamics in relevant ecological settings. In conjunction with telemedicine and online apps, they promise a major expansion of the scope for patients to manage aspects of their own care. Advances in disease understanding, decision support and self-management using these Big Data are being accelerated by machine learning and artificial intelligence.              Summary:                    Both health professionals and patients can employ e-health approaches and tools for development of a more patient-centred learning health system.",2020-06-01,8,1617,31,341
61,32324659,State of the art in clinical decision support applications in pediatric perioperative medicine,"Purpose of review:                    The goal of this review is to describe the recent improvements in clinical decision tools applied to the increasingly large and complex datasets in the pediatric ambulatory and inpatient setting.              Recent findings:                    Clinical decision support has evolved beyond simple static alerts to complex dynamic alerts for: diagnosis, medical decision-making, monitoring of physiological, laboratory, and pharmacologic inputs, and adherence to institutional and national guidelines for both the patient and the healthcare team. Artificial intelligence and machine learning have enabled advances in predicting outcomes, such as sepsis and early deterioration, and assisting in procedural technique.              Summary:                    With more than a decade of electronic medical data generation, clinical decision support tools have begun to evolve into more sophisticated and complex algorithms capable of transforming large datasets into succinct, timely, and pertinent summaries for treatment and management of pediatric patients. Future developments will need to leverage patient-generated health data, integrated device data, and provider-entered data to complete the continuum of patient care and will likely demonstrate improvements in patient outcomes.",2020-06-01,0,1322,94,341
62,32324658,Augmented intelligence in pediatric anesthesia and pediatric critical care,"Purpose of review:                    Acute care technologies, including novel monitoring devices, big data, increased computing capabilities, machine-learning algorithms and automation, are converging. This enables the application of augmented intelligence for improved outcome predictions, clinical decision-making, and offers unprecedented opportunities to improve patient outcomes, reduce costs, and improve clinician workflow. This article briefly explores recent work in the areas of automation, artificial intelligence and outcome prediction models in pediatric anesthesia and pediatric critical care.              Recent findings:                    Recent years have yielded little published research into pediatric physiological closed loop control (a type of automation) beyond studies focused on glycemic control for type 1 diabetes. However, there has been a greater range of research in augmented decision-making, leveraging artificial intelligence and machine-learning techniques, in particular, for pediatric ICU outcome prediction.              Summary:                    Most studies focusing on artificial intelligence demonstrate good performance on prediction or classification, whether they use traditional statistical tools or novel machine-learning approaches. Yet the challenges of implementation, user acceptance, ethics and regulation cannot be underestimated. Areas in which there is easy access to routinely labeled data and robust outcomes, such as those collected through national networks and quality improvement programs, are likely to be at the forefront of the adoption of these advances.",2020-06-01,1,1624,74,341
1744,31584379,Recent Advancement in Predicting Subcellular Localization of Mycobacterial Protein with Machine Learning Methods,"Mycobacterium tuberculosis (MTB) can cause the terrible tuberculosis (TB), which is reported as one of the most dreadful epidemics. Although many biochemical molecular drugs have been developed to cope with this disease, the drug resistance-especially the multidrug-resistant (MDR) and extensively drug-resistance (XDR)-poses a huge threat to the treatment. However, traditional biochemical experimental method to tackle TB is time-consuming and costly. Benefited by the appearance of the enormous genomic and proteomic sequence data, TB can be treated via sequence-based biological computational approach-bioinformatics. Studies on predicting subcellular localization of mycobacterial protein (MBP) with high precision and efficiency may help figure out the biological function of these proteins and then provide useful insights for protein function annotation as well as drug design. In this review, we reported the progress that has been made in computational prediction of subcellular localization of MBP including the following aspects: 1) Construction of benchmark datasets. 2) Methods of feature extraction. 3) Techniques of feature selection. 4) Application of several published prediction algorithms. 5) The published results. 6) The further study on prediction of subcellular localization of MBP.",2020-06-01,0,1306,112,341
1745,31584374,Application of Machine Learning Methods in Predicting Nuclear Receptors and their Families,"Nuclear receptors (NRs) are a superfamily of ligand-dependent transcription factors that are closely related to cell development, differentiation, reproduction, homeostasis, and metabolism. According to the alignments of the conserved domains, NRs are classified and assigned the following seven subfamilies or eight subfamilies: (1) NR1: thyroid hormone like (thyroid hormone, retinoic acid, RAR-related orphan receptor, peroxisome proliferator activated, vitamin D3- like), (2) NR2: HNF4-like (hepatocyte nuclear factor 4, retinoic acid X, tailless-like, COUP-TFlike, USP), (3) NR3: estrogen-like (estrogen, estrogen-related, glucocorticoid-like), (4) NR4: nerve growth factor IB-like (NGFI-B-like), (5) NR5: fushi tarazu-F1 like (fushi tarazu-F1 like), (6) NR6: germ cell nuclear factor like (germ cell nuclear factor), and (7) NR0: knirps like (knirps, knirpsrelated, embryonic gonad protein, ODR7, trithorax) and DAX like (DAX, SHP), or dividing NR0 into (7) NR7: knirps like and (8) NR8: DAX like. Different NRs families have different structural features and functions. Since the function of a NR is closely correlated with which subfamily it belongs to, it is highly desirable to identify NRs and their subfamilies rapidly and effectively. The knowledge acquired is essential for a proper understanding of normal and abnormal cellular mechanisms. With the advent of the post-genomics era, huge amounts of sequence-known proteins have increased explosively. Conventional methods for accurately classifying the family of NRs are experimental means with high cost and low efficiency. Therefore, it has created a greater need for bioinformatics tools to effectively recognize NRs and their subfamilies for the purpose of understanding their biological function. In this review, we summarized the application of machine learning methods in the prediction of NRs from different aspects. We hope that this review will provide a reference for further research on the classification of NRs and their families.",2020-06-01,2,2008,90,341
1555,32164862,General principles of machine learning for brain-computer interfacing,"Brain-computer interfaces (BCIs) are systems that translate brain activity patterns into commands that can be executed by an artificial device. This enables the possibility of controlling devices such as a prosthetic arm or exoskeleton, a wheelchair, typewriting applications, or games directly by modulating our brain activity. For this purpose, BCI systems rely on signal processing and machine learning algorithms to decode the brain activity. This chapter provides an overview of the main steps required to do such a process, including signal preprocessing, feature extraction and selection, and decoding. Given the large amount of possible methods that can be used for these processes, a comprehensive review of them is beyond the scope of this chapter, and it is focused instead on the general principles that should be taken into account, as well as discussing good practices on how these methods should be applied and evaluated for proper design of reliable BCI systems.",2020-06-01,0,978,69,341
1553,32167235,Deep learning a boon for biophotonics?,"This review covers original articles using deep learning in the biophotonic field published in the last years. In these years deep learning, which is a subset of machine learning mostly based on artificial neural network geometries, was applied to a number of biophotonic tasks and has achieved state-of-the-art performances. Therefore, deep learning in the biophotonic field is rapidly growing and it will be utilized in the next years to obtain real-time biophotonic decision-making systems and to analyze biophotonic data in general. In this contribution, we discuss the possibilities of deep learning in the biophotonic field including image classification, segmentation, registration, pseudostaining and resolution enhancement. Additionally, we discuss the potential use of deep learning for spectroscopic data including spectral data preprocessing and spectral classification. We conclude this review by addressing the potential applications and challenges of using deep learning for biophotonic data.",2020-06-01,1,1007,38,341
2053,33511330,Biomarkers of COVID-19 and technologies to combat SARS-CoV-2,"Due to the unprecedented public health crisis caused by COVID-19, our first contribution to the newly launching journal, Advances in Biomarker Sciences and Technology, has abruptly diverted to focus on the current pandemic. As the number of new COVID-19 cases and deaths continue to rise steadily around the world, the common goal of healthcare providers, scientists, and government officials worldwide has been to identify the best way to detect the novel coronavirus, named SARS-CoV-2, and to treat the viral infection - COVID-19. Accurate detection, timely diagnosis, effective treatment, and future prevention are the vital keys to management of COVID-19, and can help curb the viral spread. Traditionally, biomarkers play a pivotal role in the early detection of disease etiology, diagnosis, treatment and prognosis. To assist myriad ongoing investigations and innovations, we developed this current article to overview known and emerging biomarkers for SARS-CoV-2 detection, COVID-19 diagnostics, treatment and prognosis, and ongoing work to identify and develop more biomarkers for new drugs and vaccines. Moreover, biomarkers of socio-psychological stress, the high-technology quest for new virtual drug screening, and digital applications are described.",2020-06-01,3,1262,60,341
1780,31538879,Deep Learning in the Study of Protein-Related Interactions,"Protein-related interaction prediction is critical to understanding life processes, biological functions, and mechanisms of drug action. Experimental methods used to determine proteinrelated interactions have always been costly and inefficient. In recent years, advances in biological and medical technology have provided us with explosive biological and physiological data, and deep learning-based algorithms have shown great promise in extracting features and learning patterns from complex data. At present, deep learning in protein research has emerged. In this review, we provide an introductory overview of the deep neural network theory and its unique properties. Mainly focused on the application of this technology in protein-related interactions prediction over the past five years, including protein-protein interactions prediction, protein-RNA\DNA, Protein- drug interactions prediction, and others. Finally, we discuss some of the challenges that deep learning currently faces.",2020-06-01,0,990,58,341
89,32281460,Association between diabetic retinopathy and interleukin-related gene polymorphisms: a machine learning aided meta-analysis,"Background:                    Diabetic retinopathy (DR) is a severe complication of diabetes and a common cause of visual loss in adults. We aimed to assess the correlation between IL gene-related SNPs and the incidence of DR and attempted to predict DR with combined mutation site detection.              Methods:                    A systematic search of databases was performed up to August 2019. Five genetic models were used to analyze associations. Machine learning methods were implemented to improve SNP-related disease prediction.              Results:                    Sixteen trials assessing a total of 7221 patients were included in our meta-analysis. IL6/rs1800795, rs1800796, and IL10/rs1800896 were analyzed. For the IL-6 gene, there was no significant association between rs1800795 and the incidence of DR (allelic model: OR, 1.091; 95% CI, 0.892-1.334; p = .396). There was no significant correlation between rs1800796 (allelic model: OR, 1.135; 95% CI, 0.678-1.901; p = .63), rs1800896 (allelic model: OR, 1.047; 95% CI, 0.788-1.392; p = .752) and the incidence of DR. Unfortunately, the machine learning results also showed that the combined detection of two SNPs could not accurately predict DR occurrence.              Conclusion:                    rs1800795 and rs1800796 in the IL-6 gene and rs1800896 in IL-10 gene are not related to the incidence of DR. Mutations in multiple SNPs for each DR patient still need to be specifically assessed to increase prediction accuracy.",2020-06-01,0,1502,123,341
1545,32192349,Biomarkers of neoadjuvant/adjuvant chemotherapy for breast cancer,"The improvement of tumor biomarkers prepared for clinical use is a long process. A good biomarker should predict not only prognosis but also the response to therapies. In this review, we describe the biomarkers of neoadjuvant/adjuvant chemotherapy for breast cancer, considering different breast cancer subtypes. In hormone receptor (HR)-positive/human epidermal growth factor 2 (HER2)-negative breast cancers, various genomic markers highly associated with proliferation have been tested. Among them, only two genomic signatures, the 21-gene recurrence score and 70-gene signature, have been reported in prospective randomized clinical trials and met the primary endpoint. However, these genomic markers did not suffice in HER2-positive and triple-negative (TN) breast cancers, which present only classical clinical and pathological information (tumor size, nodal or distant metastatic status) for decision making in the adjuvant setting in daily clinic. Recently, patients with residual invasive cancer after neoadjuvant chemotherapy are at a high-risk of recurrence for metastasis, which, in turn, make these patients best applicants for clinical trials. Two clinical trials have shown improved outcomes with post-operative capecitabine and ado-trastuzumab emtansine treatment in patients with either TN or HER2-positive breast cancer, respectively, who had residual disease after neoadjuvant chemotherapy. Furthermore, tumor-infiltrating lymphocytes (TILs) have been reported to have a predictive value for prognosis and response to chemotherapy from the retrospective analyses. So far, TILs have to not be used to either withhold or prescribe chemotherapy based on the absence of standardized evaluation guidelines and confirmed information. To overcome the low reproducibility of evaluations of TILs, gene signatures or digital image analysis and machine learning algorithms with artificial intelligence may be useful for standardization of assessment for TILs in the future.",2020-06-01,0,1981,65,341
1231,32100604,Nanotoxicology data for in silico tools: a literature review,"The exercise of non-testing approaches in nanoparticles (NPs) hazard assessment is necessary for the risk assessment, considering cost and time efficiency, to identify, assess, and classify potential risks. One strategy for investigating the toxicological properties of a variety of NPs is by means of computational tools that decode how nano-specific features relate to toxicity and enable its prediction. This literature review records systematically the data used in published studies that predict nano (eco)-toxicological endpoints using machine learning models. Instead of seeking mechanistic interpretations this review maps the pathways followed, involving biological features in relation to NPs exposure, their physico-chemical characteristics and the most commonly predicted outcomes. The results, derived from published research of the last decade, are summarized visually, providing prior-based data mining paradigms to be readily used by the nanotoxicology community in computational studies.",2020-06-01,4,1004,60,341
1538,32201286,Applications of artificial intelligence in multimodality cardiovascular imaging: A state-of-the-art review,"There has been a tidal wave of recent interest in artificial intelligence (AI), machine learning and deep learning approaches in cardiovascular (CV) medicine. In the era of modern medicine, AI and electronic health records hold the promise to improve the understanding of disease conditions and bring a personalized approach to CV care. The field of CV imaging (CVI), incorporating echocardiography, cardiac computed tomography, cardiac magnetic resonance imaging and nuclear imaging, with sophisticated imaging techniques and high volumes of imaging data, is primed to be at the forefront of the revolution in precision cardiology. This review provides a contemporary overview of the CVI imaging applications of AI, including a critique of the strengths and potential limitations of deep learning approaches.",2020-06-01,1,809,106,341
396,30439700,Modern Information Technology for Cancer Research: What's in IT for Me? An Overview of Technologies and Approaches,"Information technology (IT) can enhance or change many scenarios in cancer research for the better. In this paper, we introduce several examples, starting with clinical data reuse and collaboration including data sharing in research networks. Key challenges are semantic interoperability and data access (including data privacy). We deal with gathering and analyzing genomic information, where cloud computing, uncertainties and reproducibility challenge researchers. Also, new sources for additional phenotypical data are shown in patient-reported outcome and machine learning in imaging. Last, we focus on therapy assistance, introducing tools used in molecular tumor boards and techniques for computer-assisted surgery. We discuss the need for metadata to aggregate and analyze data sets reliably. We conclude with an outlook towards a learning health care system in oncology, which connects bench and bedside by employing modern IT solutions.",2020-06-01,2,946,114,341
2035,32821348,The role of artificial intelligence in colon polyps detection,"Over the past few decades, artificial intelligence (AI) has evolved dramatically and is believed to have a significant impact on all aspects of technology and daily life. The use of AI in the healthcare system has been rapidly growing, owing to the large amount of data. Various methods of AI including machine learning, deep learning and convolutional neural network (CNN) have been used in diagnostic imaging, which have helped physicians in the accurate diagnosis of diseases and determination of appropriate treatment for them. Using and collecting a huge number of digital images and medical records has led to the creation of big data over a time period. Currently, considerations regarding the diagnosis of various presentations in all endoscopic procedures and imaging findings are solely handled by endoscopists. Moreover, AI has shown to be highly effective in the field of gastroenterology in terms of diagnosis, prognosis, and image processing. Herein, this review aimed to discuss different aspects of AI use for early detection and treatment of gastroenterology diseases.",2020-06-01,0,1085,61,341
51,32339630,Electrospun nanofiber-based cancer sensors: A review,"Cancer is a malignancy engendering enormous global mortality, steering extensive research for early diagnosis and efficacious prognosis leading to emergence of cancer sensing technologies for multitudinous biomarkers. In this context, nanofibers, imparting high surface area, facile production, morphology control, and synergistic properties attainable, are poised to be inevitable in futuristic sensing devices for predictive diagnostics when integrated with artificial intelligence and machine learning. To this end, fundamentals governing the sensor response and their analytical performance have been discussed. The headways in organic and inorganic nanofibers for biomarker gas sensing, fluid sample sensing and imaging have been supplemented with discussions on materials for nanofiber formation, along with sensitizing materials, and formation of sensing elements by processes like surface deposition on nanofibers, immobilising, calcination, etc. and their effect on final sensing device properties. The review culminates by summarising the conceptual understanding of the hitherto progress leading to achievement of excellent analytical performance giving detection limits to the order of 1.6 pM concentration and response time of as low as 0.5 s. Current bottlenecks in this state of the art have been delineated and pathways for future research are discussed.",2020-06-01,0,1370,52,341
2070,33410425,"COVID-19: Advances in diagnostic tools, treatment strategies, and vaccine development","An unprecedented worldwide spread of the SARS-CoV-2 has imposed severe challenges on healthcare facilities and medical infrastructure. The global research community faces urgent calls for the development of rapid diagnostic tools, effective treatment protocols, and most importantly, vaccines against the pathogen. Pooling together expertise across broad domains to innovate effective solutions is the need of the hour. With these requirements in mind, in this review, we provide detailed critical accounts on the leading efforts at developing diagnostics tools, therapeutic agents, and vaccine candidates. Importantly, we furnish the reader with a multidisciplinary perspective on how conventional methods like serology and RT-PCR, as well as cutting-edge technologies like CRISPR/Cas and artificial intelligence/machine learning, are being employed to inform and guide such investigations. We expect this narrative to serve a broad audience of both active and aspiring researchers in the field of biomedical sciences and engineering and help inspire radical new approaches towards effective detection, treatment, and prevention of this global pandemic.",2020-06-01,2,1154,85,341
47,32347446,Artificial Intelligence and Machine Learning for HIV Prevention: Emerging Approaches to Ending the Epidemic,"Purpose of review:                    We review applications of artificial intelligence (AI), including machine learning (ML), in the field of HIV prevention.              Recent findings:                    ML approaches have been used to identify potential candidates for preexposure prophylaxis (PrEP) in healthcare settings in the USA and Denmark and in a population-based research setting in Eastern Africa. Although still in the proof-of-concept stage, other applications include ML with smartphone-collected and social media data to promote real-time HIV risk reduction, virtual reality tools to facilitate HIV serodisclosure, and chatbots for HIV education. ML has also been used for causal inference in HIV prevention studies. ML has strong potential to improve delivery of PrEP, with this approach moving from development to implementation. Development and evaluation of AI and ML strategies for HIV prevention may benefit from an implementation science approach, including qualitative assessments with end users, and should be developed and evaluated with attention to equity.",2020-06-01,3,1087,107,341
44,32349877,Dentronics: Towards robotics and artificial intelligence in dentistry,"Objectives:                    This paper provides an overview of existing applications and concepts of robotic systems and artificial intelligence in dentistry. This review aims to provide the community with novel inputs and argues for an increased utilization of these recent technological developments, referred to as Dentronics, in order to advance dentistry.              Methods:                    First, background on developments in robotics, artificial intelligence (AI) and machine learning (ML) are reviewed that may enable novel assistive applications in dentistry (Sec A). Second, a systematic technology review that evaluates existing state-of-the-art applications in AI, ML and robotics in the context of dentistry is presented (Sec B).              Results:                    A systematic literature research in pubmed yielded in a total of 558 results. 41 studies related to ML, 53 studies related to AI and 49 original research papers on robotics application in dentistry were included. ML and AI have been applied in dental research to analyze large amounts of data to eventually support dental decision making, diagnosis, prognosis and treatment planning with the help of data-driven analysis algorithms based on machine learning. So far, only few robotic applications have made it to reality, mostly restricted to pilot use cases.              Significance:                    The authors believe that dentistry can greatly benefit from the current rise of digital human-centered automation and be transformed towards a new robotic, ML and AI-enabled era. In the future, Dentronics will enhance reliability, reproducibility, accuracy and efficiency in dentistry through the democratized use of modern dental technologies, such as medical robot systems and specialized artificial intelligence. Dentronics will increase our understanding of disease pathogenesis, improve risk-assessment-strategies, diagnosis, disease prediction and finally lead to better treatment outcomes.",2020-06-01,4,1996,69,341
2155,31746287,An Overview of Computational Tools of Nucleic Acid Binding Site Prediction for Site-specific Proteins and Nucleases,"Understanding the interaction mechanism of proteins and nucleic acids is one of the most fundamental problems for genome editing with engineered nucleases. Due to some limitations of experimental investigations, computational methods have played an important role in obtaining the knowledge of protein-nucleic acid interaction. Over the past few years, dozens of computational tools have been used for identification of nucleic acid binding site for site-specific proteins and design of site-specific nucleases because of their significant advantages in genome editing. Here, we review existing widely-used computational tools for target prediction of site-specific proteins as well as off-target prediction of site-specific nucleases. This article provides a list of on-line prediction tools according to their features followed by the description of computational methods used by these tools, which range from various sequence mapping algorithms (like Bowtie, FetchGWI and BLAST) to different machine learning methods (such as Support Vector Machine, hidden Markov models, Random Forest, elastic network and deep neural networks). We also make suggestions on the further development in improving the accuracy of prediction methods. This survey will provide a reference guide for computational biologists working in the field of genome editing.",2020-06-01,0,1345,115,341
2147,31756390,Data generation and network reconstruction strategies for single cell transcriptomic profiles of CRISPR-mediated gene perturbations,"Recent advances in single-cell RNA-sequencing (scRNA-seq) in combination with CRISPR/Cas9 technologies have enabled the development of methods for large-scale perturbation studies with transcriptional readouts. These methods are highly scalable and have the potential to provide a wealth of information on the biological networks that underlie cellular response. Here we discuss how to overcome several key challenges to generate and analyse data for the confident reconstruction of models of the underlying cellular network. Some challenges are generic, and apply to analysing any single-cell transcriptomic data, while others are specific to combined single-cell CRISPR/Cas9 data, in particular barcode swapping, knockdown efficiency, multiplicity of infection and potential confounding factors. We also provide a curated collection of published data sets to aid the development of analysis strategies. Finally, we discuss several network reconstruction approaches, including co-expression networks and Bayesian networks, as well as their limitations, and highlight the potential of Nested Effects Models for network reconstruction from scRNA-seq data. This article is part of a Special Issue entitled: Transcriptional Profiles and Regulatory Gene Networks edited by Dr. Dr. Federico Manuel Giorgi and Dr. Shaun Mahony.",2020-06-01,1,1321,131,341
1575,32135196,Review and comparative analysis of machine learning-based phage virion protein identification methods,"Phage virion protein (PVP) identification plays key role in elucidating relationships between phages and hosts. Moreover, PVP identification can facilitate the design of related biochemical entities. Recently, several machine learning approaches have emerged for this purpose and have shown their potential capacities. In this study, the proposed PVP identifiers are systemically reviewed, and the related algorithms and tools are comprehensively analyzed. We summarized the common framework of these PVP identifiers and constructed our own novel identifiers based upon the framework. Furthermore, we focus on a performance comparison of all PVP identifiers by using a training dataset and an independent dataset. Highlighting the pros and cons of these identifiers demonstrates that g-gap DPC (dipeptide composition) features are capable of representing characteristics of PVPs. Moreover, SVM (support vector machine) is proven to be the more effective classifier to distinguish PVPs and non-PVPs.",2020-06-01,0,998,101,341
2106,31806421,The pathogenesis of systemic lupus erythematosus: Harnessing big data to understand the molecular basis of lupus,"Systemic lupus erythematosus (SLE) is a chronic, systemic autoimmune disease that causes damage to multiple organ systems. Despite decades of research and available murine models that capture some aspects of the human disease, new treatments for SLE lag behind other autoimmune diseases such as Rheumatoid Arthritis and Crohn's disease. Big data genomic assays have transformed our understanding of SLE by providing important insights into the molecular heterogeneity of this multigenic disease. Gene wide association studies have demonstrated more than 100 risk loci, supporting a model of multiple genetic hits increasing SLE risk in a non-linear fashion, and providing evidence of ancestral diversity in susceptibility loci. Epigenetic studies to determine the role of methylation, acetylation and non-coding RNAs have provided new understanding of the modulation of gene expression in SLE patients and identified new drug targets and biomarkers for SLE. Gene expression profiling has led to a greater understanding of the role of myeloid cells in the pathogenesis of SLE, confirmed roles for T and B cells in SLE, promoted clinical trials based on the prominent interferon signature found in SLE patients, and identified candidate biomarkers and cellular signatures to further drug development and drug repurposing. Gene expression studies are advancing our understanding of the underlying molecular heterogeneity in SLE and providing hope that patient stratification will expedite new therapies based on personal molecular signatures. Although big data analyses present unique interpretation challenges, both computationally and biologically, advances in machine learning applications may facilitate the ability to predict changes in SLE disease activity and optimize therapeutic strategies.",2020-06-01,16,1796,112,341
4,32410553,Application of Artificial Intelligence in Pharmaceutical and Biomedical Studies,"Background:                    Artificial intelligence (AI) is the way to model human intelligence to accomplish certain tasks without much intervention of human beings. The term AI was first used in 1956 with The Logic Theorist program, which was designed to simulate problem-solving ability of human beings. There have been a significant amount of research works using AI in order to determine the advantages and disadvantages of its applicabication and, future perspectives that impact different areas of society. Even the remarkable impact of AI can be transferred to the field of healthcare with its use in pharmaceutical and biomedical studies crucial for the socioeconomic development of the population in general within different studies, we can highlight those that have been conducted with the objective of treating diseases, such as cancer, neurodegenerative diseases, among others. In parallel, the long process of drug development also requires the application of AI to accelerate research in medical care.              Methods:                    This review is based on research material obtained from PubMed up to Jan 2020. The search terms include ""artificial intelligence"", ""machine learning"" in the context of research on pharmaceutical and biomedical applications.              Results:                    This study aimed to highlight the importance of AI in the biomedical research and also recent studies that support the use of AI to generate tools using patient data to improve outcomes. Other studies have demonstrated the use of AI to create prediction models to determine response to cancer treatment.              Conclusion:                    The application of AI in the field of pharmaceutical and biomedical studies has been extensive, including cancer research, for diagnosis as well as prognosis of the disease state. It has become a tool for researchers in the management of complex data, ranging from obtaining complementary results to conventional statistical analyses. AI increases the precision in the estimation of treatment effect in cancer patients and determines prediction outcomes.",2020-06-01,0,2128,79,341
7,32408309,Oncology Informatics: Status Quo and Outlook,"Oncology has undergone rapid progress, with emerging developments in areas including cancer stem cells, molecularly targeted therapies, genomic analyses, and individually tailored immunotherapy. These advances have expanded the tools available in the fight against cancer. Some of these have seen broad media coverage resulting in justified public attention. However, these achievements have only been possible due to rapid developments in the expanding field of biomedical informatics and information technology (IT). Artificial intelligence, radiomics, electronic health records, and electronic patient-reported outcome measures (ePROMS) are only a few of the developments enabling further progress in oncology. The promising impact of IT in oncology will only become reality through a multidisciplinary approach to the complex challenges ahead.",2020-06-01,1,847,44,341
10,32396837,Opening the Black Box: Interpretable Machine Learning for Geneticists,"Because of its ability to find complex patterns in high dimensional and heterogeneous data, machine learning (ML) has emerged as a critical tool for making sense of the growing amount of genetic and genomic data available. While the complexity of ML models is what makes them powerful, it also makes them difficult to interpret. Fortunately, efforts to develop approaches that make the inner workings of ML models understandable to humans have improved our ability to make novel biological insights. Here, we discuss the importance of interpretable ML, different strategies for interpreting ML models, and examples of how these strategies have been applied. Finally, we identify challenges and promising future directions for interpretable ML in genetics and genomics.",2020-06-01,6,768,69,341
14,32393820,Illuminating dendritic function with computational models,"Dendrites have always fascinated researchers: from the artistic drawings by Ramon y Cajal to the beautiful recordings of today, neuroscientists have been striving to unravel the mysteries of these structures. Theoretical work in the 1960s predicted important dendritic effects on neuronal processing, establishing computational modelling as a powerful technique for their investigation. Since then, modelling of dendrites has been instrumental in driving neuroscience research in a targeted manner, providing experimentally testable predictions that range from the subcellular level to the systems level, and their relevance extends to fields beyond neuroscience, such as machine learning and artificial intelligence. Validation of modelling predictions often requires - and drives - new technological advances, thus closing the loop with theory-driven experimentation that moves the field forward. This Review features the most important, to our understanding, contributions of modelling of dendritic computations, including those pending experimental verification, and highlights studies of successful interactions between the modelling and experimental neuroscience communities.",2020-06-01,3,1181,57,341
419,30378482,Advanced in Silico Methods for the Development of Anti- Leishmaniasis and Anti-Trypanosomiasis Agents,"Leishmaniasis and trypanosomiasis occur primarily in undeveloped countries and account for millions of deaths and disability-adjusted life years. Limited therapeutic options, high toxicity of chemotherapeutic drugs and the emergence of drug resistance associated with these diseases demand urgent development of novel therapeutic agents for the treatment of these dreadful diseases. In the last decades, different in silico methods have been successfully implemented for supporting the lengthy and expensive drug discovery process. In the current review, we discuss recent advances pertaining to in silico analyses towards lead identification, lead modification and target identification of antileishmaniasis and anti-trypanosomiasis agents. We describe recent applications of some important in silico approaches, such as 2D-QSAR, 3D-QSAR, pharmacophore mapping, molecular docking, and so forth, with the aim of understanding the utility of these techniques for the design of novel therapeutic anti-parasitic agents. This review focuses on: (a) advanced computational drug design options; (b) diverse methodologies - e.g.: use of machine learning tools, software solutions, and web-platforms; (c) recent applications and advances in the last five years; (d) experimental validations of in silico predictions; (e) virtual screening tools; and (f) rationale or justification for the selection of these in silico methods.",2020-06-01,2,1418,101,341
18,32389272,Unsupervised Machine Learning in Pathology: The Next Frontier,"Applications of artificial intelligence and particularly deep learning to aid pathologists in carrying out laborious and qualitative tasks in histopathologic image analysis have now become ubiquitous. We introduce and illustrate how unsupervised machine learning workflows can be deployed in existing pathology workflows to begin learning autonomously through exploration and without the need for extensive direction. Although still in its infancy, this type of machine learning, which more closely mirrors human intelligence, stands to add another exciting layer of innovation to computational pathology and accelerate the transition to autonomous pathologic tissue analysis.",2020-06-01,1,676,61,341
1562,32156226,A Brief Survey of Machine Learning Methods in Identification of Mitochondria Proteins in Malaria Parasite,"The number of human deaths caused by malaria is increasing day-by-day. In fact, the mitochondrial proteins of the malaria parasite play vital roles in the organism. For developing effective drugs and vaccines against infection, it is necessary to accurately identify mitochondrial proteins of the malaria parasite. Although precise details for the mitochondrial proteins can be provided by biochemical experiments, they are expensive and time-consuming. In this review, we summarized the machine learning-based methods for mitochondrial proteins identification in the malaria parasite and compared the construction strategies of these computational methods. Finally, we also discussed the future development of mitochondrial proteins recognition with algorithms.",2020-06-01,0,762,105,341
1559,32162711,Fracture risk assessment and clinical decision making for patients with metastatic bone disease,"Metastatic breast, prostate, lung, and other cancers often affect bone, causing pain, increasing fracture risk, and decreasing function. Management of metastatic bone disease (MBD) is clinically challenging when there is potential but uncertain risk of pathological fracture. Management of MBD has become a major focus within orthopedic oncology with respect to fracture and impending fracture care. If impending skeletal-related events (SREs), particularly pathologic fracture, could be predicted, increasing evidence suggests that prophylactic surgical treatment improves patient outcomes. However, current fracture risk assessment and radiographic metrics do not have high accuracy and have not been combined with relevant patient survival tools. This review first explores the prevalence, incidence, and morbidity of MBD and associated SREs for different cancer types. Strengths and limitations of current fracture risk scoring systems for spinal stability and long bone fracture are highlighted. More recent computed tomography (CT)-based structural rigidity analysis (CTRA) and finite element (FE) analysis methods offer advantages of increased specificity (true negative rate), but are limited in availability. Other fracture prediction approaches including parametric response mapping and positron emission tomography/computed tomography measures show early promise. Substantial new information to inform clinical decision-making includes measures of survival, clinical benefits, and economic analysis of prophylactic treatment compared to after-fracture stabilization. Areas of future research include use of big data and machine learning to predict SREs, greater access and refinement of CTRA/FE approaches, combination of clinical survival prediction tools with radiographically based fracture risk assessment, and net benefit analysis for fracture risk assessment and prophylactic treatment.",2020-06-01,2,1903,95,341
1584,32124415,Human Papillomavirus Testing in Head and Neck Squamous Cell Carcinoma in 2020: Where Are We Now and Where Are We Going?,"High risk human papillomavirus (HPV) has transformed head and neck oncology in the past several decades. Now that we have recognized that HPV-positive oropharyngeal squamous cell carcinoma (OPSCC) is a unique cancer type with distinct clinicopathologic features and favorable prognosis, it has become essential to test patients in routine practice. We have progressed greatly in our knowledge of this disease and gone, over the past two to three decades, from doing testing in highly variable amounts and methods to, now, with the help of national and international guidelines and patient staging requirements, to a situation where almost all patients with OPSCC are getting accurate classification through at least p16 immunohistochemistry. However, we are still struggling with how to accurately test specimens from cervical lymph nodes, and, in particular, on fine needle aspiration. In addition, many patients with non-oropharyngeal SCC are getting clinically unnecessary p16 and/or HPV-specific testing. The trends suggest progressive improvement in practices, but many practical questions still remain. On the horizon are myriad non-tissue-based tests, such as HPV serology and plasma DNA, DNA-based testing of fine needle aspirate fluid, computerized analysis of digitized pathology and radiology images, and machine learning from clinical and pathologic features, that may render pathologists largely obsolete for establishing HPV status for our patients' tumors. This review takes a brief look back in time to where we have been, then characterizes current practices in 2020 and lingering questions, and, finally, looks ahead into the possible future of HPV testing in patients with head and neck SCC.",2020-06-01,3,1710,119,341
1557,32164853,Monitoring performance of professional and occupational operators,"The human capacity to simultaneously perform several tasks depends on the quantity and the mode of mentally processing the information imposed by the tasks. Since operational environments are highly dynamic, priorities across tasks will be expected to change as the mission evolves, thus the capability to reallocate the mental resources dynamically depending on such changes is very important. The resources required in very complex situations, such as air traffic management (ATM), can exceed the user's available resources leading to increased workload and performance impairments. In this regard, the availability of information concerning the workload experienced by the operators while dealing with tasks will be fundamental for both warning them when overload conditions are approaching and improving interactions with the system. The idea of our work was to use neurophysiologic data collected from professional air traffic controllers (ATCOs) to provide additional information to standard measures with which to assess the ATCOs' expertise and a machine learning electroencephalography-based index to evaluate their mental workload during the execution of ATC tasks. The results showed that the proposed method was able to track the workload alongside the execution of the realistic ATM scenario, and provide added values to objectively assess the expertise of the ATCOs.",2020-06-01,1,1380,65,341
34,32359808,"Machine learning, the kidney, and genotype-phenotype analysis","With biomedical research transitioning into data-rich science, machine learning provides a powerful toolkit for extracting knowledge from large-scale biological data sets. The increasing availability of comprehensive kidney omics compendia (transcriptomics, proteomics, metabolomics, and genome sequencing), as well as other data modalities such as electronic health records, digital nephropathology repositories, and radiology renal images, makes machine learning approaches increasingly essential for analyzing human kidney data sets. Here, we discuss how machine learning approaches can be applied to the study of kidney disease, with a particular focus on how they can be used for understanding the relationship between genotype and phenotype.",2020-06-01,2,747,61,341
37,32357816,Computational Approaches for the Design of (Mutant-)Selective Tyrosine Kinase Inhibitors: State-of-the-Art and Future Prospects,"Kinases remain one of the major attractive therapeutic targets for a large number of indications such as cancer, rheumatoid arthritis, cardiac failure and many others. Design and development of kinase inhibitors (ATP-competitive, allosteric or covalent) is a clinically validated and successful strategy in the pharmaceutical industry. The perks come with limitations, particularly the development of resistance to highly potent and selective inhibitors. When this happens, the cycle needs to be repeated, i.e., the design and development of kinase inhibitors active against the mutated forms. The complexity of tumor milieu makes it awfully difficult for these molecularly-targeted therapies to work. Every year newer and better versions of these agents are introduced in the clinic. Several computational approaches such as structure-, ligand-based or hybrid ones continue to live up to their potential in discovering novel kinase inhibitors. New schools of thought in this area continue to emerge, e.g., development of dual-target kinase inhibitors. But there are fundamental issues with this approach. It is indeed difficult to selectively optimize binding at two entirely different or related kinases. In addition to the conventional strategies, modern technologies (machine learning, deep learning, artificial intelligence, etc.) started yielding the results and building success stories. Computational tools invariably played a critical role in catalysing the phenomenal progress in kinase drug discovery field. The present review summarized the progress in utilizing computational methods and tools for discovering (mutant-)selective tyrosine kinase inhibitor drugs in the last three years (2017-2019). Representative investigations have been discussed, while others are merely listed. The author believes that the enthusiastic reader will be inspired to dig out the cited literature extensively to appreciate the progress made so far and the future prospects of the field.",2020-06-01,1,1981,127,341
38,32356548,QSAR without borders,"Prediction of chemical bioactivity and physical properties has been one of the most important applications of statistical and more recently, machine learning and artificial intelligence methods in chemical sciences. This field of research, broadly known as quantitative structure-activity relationships (QSAR) modeling, has developed many important algorithms and has found a broad range of applications in physical organic and medicinal chemistry in the past 55+ years. This Perspective summarizes recent technological advances in QSAR modeling but it also highlights the applicability of algorithms, modeling methods, and validation practices developed in QSAR to a wide range of research areas outside of traditional QSAR boundaries including synthesis planning, nanotechnology, materials science, biomaterials, and clinical informatics. As modern research methods generate rapidly increasing amounts of data, the knowledge of robust data-driven modelling methods professed within the QSAR field can become essential for scientists working both within and outside of chemical research. We hope that this contribution highlighting the generalizable components of QSAR modeling will serve to address this challenge.",2020-06-01,23,1216,20,341
1556,32164861,Merging brain-computer interface and functional electrical stimulation technologies for movement restoration,"BCI (brain-computer interface) and functional electrical stimulation (FES) technologies have advanced significantly over the last several decades. Recent efforts have involved the integration of these technologies with the goal of restoring functional movement in paralyzed patients. Implantable BCIs have provided neural recordings with increased spatial resolution and have been combined with sophisticated neural decoding algorithms and increasingly capable FES systems to advance efforts toward this goal. This chapter reviews historical developments that have occurred as the exciting fields of BCI and FES have evolved and now overlapped to allow new breakthroughs in medicine, targeting restoration of movement and lost function in users with disabilities.",2020-06-01,1,763,108,341
19,32388726,Predicting Patient-Centered Outcomes from Spine Surgery Using Risk Assessment Tools: a Systematic Review,"Purpose of review:                    The purpose of this systematic review is to evaluate the current literature in patients undergoing spine surgery in the cervical, thoracic, and lumbar spine to determine the available risk assessment tools to predict the patient-centered outcomes of pain, disability, physical function, quality of life, psychological disposition, and return to work after surgery.              Recent findings:                    Risk assessment tools can assist surgeons and other healthcare providers in identifying the benefit-risk ratio of surgical candidates. These tools gather demographic, medical history, and other pertinent patient-reported measures to calculate a probability utilizing regression or machine learning statistical foundations. Currently, much is still unknown about the use of these tools to predict quality of life, disability, and other factors following spine surgery. A systematic review was conducted using PRISMA guidelines that identified risk assessment tools that utilized patient-reported outcome measures as part of the calculation. From 8128 identified studies, 13 articles met inclusion criteria and were accepted into this review. The range of c-index values reported in the studies was between 0.63 and 0.84, indicating fair to excellent model performance. Post-surgical patient-reported outcomes were identified in the following categories (n = total number of predictive models): return to work (n = 3), pain (n = 9), physical functioning and disability (n = 5), quality of life (QOL) (n = 6), and psychosocial disposition (n = 2). Our review has synthesized the available evidence on risk assessment tools for predicting patient-centered outcomes in patients undergoing spine surgery and described their findings and clinical utility.",2020-06-01,0,1800,104,341
522,31023632,"Image reconstruction: Part 1 - understanding filtered back projection, noise and image acquisition","Image reconstruction is an increasingly complex field in CT. Iterative Reconstruction (IR) is at present an adjunct to standard Filtered Back Projection (FBP) reconstruction, but could become a replacement for it. Due to its potential for scanning at lower radiation doses, IR has received a lot of attention in the medical literature and all vendors offer commercial solutions. Its use in cardiovascular CT has been driven in part due to concerns about radiation dose and image quality. This paper is the first manuscript of a pair. It aims to review the basic principles of CT scanning, to describe image reconstruction using Filtered Back Projection, and to identify the physical processes that contribute to image noise which IR may be able to compensate for. The aim is to enable cardiovascular imagers to understand what happens to the raw data prior to the reconstruction process so they may have a better appreciation of the strengths and weaknesses of the various reconstruction techniques available. The second manuscript of this pair will discuss the various vendor permutations of IR in more detail, including the most recent machine learning based offerings, and critically appraise the current clinical research available on the various IR techniques used in cardiovascular CT.",2020-06-01,3,1291,98,341
1874,32676206,Artificial Intelligence (AI) and Cardiovascular Diseases: An Unexpected Alliance,"Cardiovascular disease (CVD), despite the significant advances in the diagnosis and treatments, still represents the leading cause of morbidity and mortality worldwide. In order to improve and optimize CVD outcomes, artificial intelligence techniques have the potential to radically change the way we practice cardiology, especially in imaging, offering us novel tools to interpret data and make clinical decisions. AI techniques such as machine learning and deep learning can also improve medical knowledge due to the increase of the volume and complexity of the data, unlocking clinically relevant information. Likewise, the use of emerging communication and information technologies is becoming pivotal to create a pervasive healthcare service through which elderly and chronic disease patients can receive medical care at their home, reducing hospitalizations and improving quality of life. The aim of this review is to describe the contemporary state of artificial intelligence and digital health applied to cardiovascular medicine as well as to provide physicians with their potential not only in cardiac imaging but most of all in clinical practice.",2020-06-01,0,1156,80,341
753,33063054,A Study on Fight Against COVID-19 from Latest Technological Intervention,"Uncontrolled spread of pandemic COVID-19 in India and across the globe over several months, created an impact as never before any pandemic would have created. This certainly demands a technological intervention from all possibility to overcome the situation and lead a normal life as early as possible. AI/Machine learning responds to the situation, through inspecting different aspects of the pandemic. This paper analyses and studies those aspects, (I) Quarantine and statistical aspect: Quarantine potentially affected candidates (person who is in touch, travel history) through Data analytics/Machine learning. (II) Diagnosis and Treatment aspect: Early detection and fast treatment will save lives. Diagnosis using deep learning assists radiologist from saving their effort and time to a greater extent and arrives faster conclusion. (III) Prevention aspect: Monitoring and enforce social distancing through visual social distancing using deep learning and Computer vision.",2020-06-01,0,978,72,341
1498,32253623,"Machine Learning in Dermatology: Current Applications, Opportunities, and Limitations","Machine learning (ML) has the potential to improve the dermatologist's practice from diagnosis to personalized treatment. Recent advancements in access to large datasets (e.g., electronic medical records, image databases, omics), faster computing, and cheaper data storage have encouraged the development of ML algorithms with human-like intelligence in dermatology. This article is an overview of the basics of ML, current applications of ML, and potential limitations and considerations for further development of ML. We have identified five current areas of applications for ML in dermatology: (1) disease classification using clinical images; (2) disease classification using dermatopathology images; (3) assessment of skin diseases using mobile applications and personal monitoring devices; (4) facilitating large-scale epidemiology research; and (5) precision medicine. The purpose of this review is to provide a guide for dermatologists to help demystify the fundamentals of ML and its wide range of applications in order to better evaluate its potential opportunities and challenges.",2020-06-01,4,1091,85,341
932,31962244,Incorporating biological structure into machine learning models in biomedicine,"In biomedical applications of machine learning, relevant information often has a rich structure that is not easily encoded as real-valued predictors. Examples of such data include DNA or RNA sequences, gene sets or pathways, gene interaction or coexpression networks, ontologies, and phylogenetic trees. We highlight recent examples of machine learning models that use structure to constrain model architecture or incorporate structured data into model training. For machine learning in biomedicine, where sample size is limited and model interpretability is crucial, incorporating prior knowledge in the form of structured data can be particularly useful. The area of research would benefit from performant open source implementations and independent benchmarking efforts.",2020-06-01,2,773,78,341
560,30968771,The Computational Models of Drug-target Interaction Prediction,"The identification of Drug-Target Interactions (DTIs) is an important process in drug discovery and medical research. However, the tradition experimental methods for DTIs identification are still time consuming, extremely expensive and challenging. In the past ten years, various computational methods have been developed to identify potential DTIs. In this paper, the identification methods of DTIs are summarized. What's more, several state-of-the-art computational methods are mainly introduced, containing network-based method and machine learning-based method. In particular, for machine learning-based methods, including the supervised and semisupervised models, have essential differences in the approach of negative samples. Although these effective computational models in identification of DTIs have achieved significant improvements, network-based and machine learning-based methods have their disadvantages, respectively. These computational methods are evaluated on four benchmark data sets via values of Area Under the Precision Recall curve (AUPR).",2020-06-01,2,1063,62,341
944,31951873,Artificial intelligence in multiparametric prostate cancer imaging with focus on deep-learning methods,"Prostate cancer represents today the most typical example of a pathology whose diagnosis requires multiparametric imaging, a strategy where multiple imaging techniques are combined to reach an acceptable diagnostic performance. However, the reviewing, weighing and coupling of multiple images not only places additional burden on the radiologist, it also complicates the reviewing process. Prostate cancer imaging has therefore been an important target for the development of computer-aided diagnostic (CAD) tools. In this survey, we discuss the advances in CAD for prostate cancer over the last decades with special attention to the deep-learning techniques that have been designed in the last few years. Moreover, we elaborate and compare the methods employed to deliver the CAD output to the operator for further medical decision making.",2020-06-01,4,840,102,341
945,31951162,A Review of Recent Developments and Progress in Computational Drug Repositioning,"Computational drug repositioning is an efficient approach towards discovering new indications for existing drugs. In recent years, with the accumulation of online health-related information and the extensive use of biomedical databases, computational drug repositioning approaches have achieved significant progress in drug discovery. In this review, we summarize recent advancements in drug repositioning. Firstly, we explicitly demonstrated the available data source information which is conducive to identifying novel indications. Furthermore, we provide a summary of the commonly used computing approaches. For each method, we briefly described techniques, case studies, and evaluation criteria. Finally, we discuss the limitations of the existing computing approaches.",2020-06-01,1,773,80,341
2263,31108201,"Single-cell approaches to cell competition: High-throughput imaging, machine learning and simulations","Cell competition is a quality control mechanism in tissues that results in the elimination of less fit cells. Over the past decade, the phenomenon of cell competition has been identified in many physiological and pathological contexts, driven either by biochemical signaling or by mechanical forces within the tissue. In both cases, competition has generally been characterized based on the elimination of loser cells at the population level, but significantly less attention has been focused on determining how single-cell dynamics and interactions regulate population-wide changes. In this review, we describe quantitative strategies and outline the outstanding challenges in understanding the single cell rules governing tissue-scale competition dynamics. We propose quantitative metrics to characterize single cell behaviors in competition and use them to distinguish the types and outcomes of competition. We describe how such metrics can be measured experimentally using a novel combination of high-throughput imaging and machine learning algorithms. We outline the experimental challenges to quantify cell fate dynamics with high-statistical precision, and describe the utility of computational modeling in testing hypotheses not easily accessible in experiments. In particular, cell-based modeling approaches that combine mechanical interaction of cells with decision-making rules for cell fate choices provide a powerful framework to understand and reverse-engineer the diverse rules of cell competition.",2020-06-01,0,1513,101,341
959,31923803,Power dynamics in intergroup relations,"Power and intergroup relations are complex, multilevel, and dynamic. Using Power Basis Theory, we explain our criteria for deciding whether theory or research addresses intergroup power dynamics: it must (a) address power and not authority or other topics, (b) involve attempted or real change regarding groups and power, or the prevention of change, (c) involve protracted interactions among multiple actors through more than one channel, (d) involve more than one level of social organization (e.g. person, group, superordinate group). We organize our 10-year review by these criteria. Research meeting all our criteria is rare. We explain relevant new theory and new research tools, including multi-level modelling, multi-player games, agent-based models, big data, and machine-learning, that can help fill the gap.",2020-06-01,0,818,38,341
1489,32580330,Gait Analysis in Parkinson's Disease: An Overview of the Most Accurate Markers for Diagnosis and Symptoms Monitoring,"The aim of this review is to summarize that most relevant technologies used to evaluate gait features and the associated algorithms that have shown promise to aid diagnosis and symptom monitoring in Parkinson's disease (PD) patients. We searched PubMed for studies published between 1 January 2005, and 30 August 2019 on gait analysis in PD. We selected studies that have either used technologies to distinguish PD patients from healthy subjects or stratified PD patients according to motor status or disease stages. Only those studies that reported at least 80% sensitivity and specificity were included. Gait analysis algorithms used for diagnosis showed a balanced accuracy range of 83.5-100%, sensitivity of 83.3-100% and specificity of 82-100%. For motor status discrimination the gait analysis algorithms showed a balanced accuracy range of 90.8-100%, sensitivity of 92.5-100% and specificity of 88-100%. Despite a large number of studies on the topic of objective gait analysis in PD, only a limited number of studies reported algorithms that were accurate enough deemed to be useful for diagnosis and symptoms monitoring. In addition, none of the reported algorithms and technologies has been validated in large scale, independent studies.",2020-06-01,3,1247,116,341
1487,32581758,"Current Status, Challenges, and Possible Solutions of EEG-Based Brain-Computer Interface: A Comprehensive Review","Brain-Computer Interface (BCI), in essence, aims at controlling different assistive devices through the utilization of brain waves. It is worth noting that the application of BCI is not limited to medical applications, and hence, the research in this field has gained due attention. Moreover, the significant number of related publications over the past two decades further indicates the consistent improvements and breakthroughs that have been made in this particular field. Nonetheless, it is also worth mentioning that with these improvements, new challenges are constantly discovered. This article provides a comprehensive review of the state-of-the-art of a complete BCI system. First, a brief overview of electroencephalogram (EEG)-based BCI systems is given. Secondly, a considerable number of popular BCI applications are reviewed in terms of electrophysiological control signals, feature extraction, classification algorithms, and performance evaluation metrics. Finally, the challenges to the recent BCI systems are discussed, and possible solutions to mitigate the issues are recommended.",2020-06-01,7,1099,112,341
964,31912547,"NMR signal processing, prediction, and structure verification with machine learning techniques","Machine learning (ML) methods have been present in the field of NMR since decades, but it has experienced a tremendous growth in the last few years, especially thanks to the emergence of deep learning (DL) techniques taking advantage of the increased amounts of data and available computer power. These algorithms are successfully employed for classification, regression, clustering, or dimensionality reduction tasks of large data sets and have been intensively applied in different areas of NMR including metabonomics, clinical diagnosis, or relaxometry. In this article, we concentrate on the various applications of ML/DL in the areas of NMR signal processing and analysis of small molecules, including automatic structure verification and prediction of NMR observables in solution.",2020-06-01,1,786,94,341
965,31910421,Machine Learning in Fetal Cardiology: What to Expect,"In fetal cardiology, imaging (especially echocardiography) has demonstrated to help in the diagnosis and monitoring of fetuses with a compromised cardiovascular system potentially associated with several fetal conditions. Different ultrasound approaches are currently used to evaluate fetal cardiac structure and function, including conventional 2-D imaging and M-mode and tissue Doppler imaging among others. However, assessment of the fetal heart is still challenging mainly due to involuntary movements of the fetus, the small size of the heart, and the lack of expertise in fetal echocardiography of some sonographers. Therefore, the use of new technologies to improve the primary acquired images, to help extract measurements, or to aid in the diagnosis of cardiac abnormalities is of great importance for optimal assessment of the fetal heart. Machine leaning (ML) is a computer science discipline focused on teaching a computer to perform tasks with specific goals without explicitly programming the rules on how to perform this task. In this review we provide a brief overview on the potential of ML techniques to improve the evaluation of fetal cardiac function by optimizing image acquisition and quantification/segmentation, as well as aid in improving the prenatal diagnoses of fetal cardiac remodeling and abnormalities.",2020-06-01,2,1333,52,341
967,31907710,Machine learning applications in imaging analysis for patients with pituitary tumors: a review of the current literature and future directions,"Purpose:                    To provide an overview of fundamental concepts in machine learning (ML), review the literature on ML applications in imaging analysis of pituitary tumors for the last 10 years, and highlight the future directions on potential applications of ML for pituitary tumor patients.              Method:                    We presented an overview of the fundamental concepts in ML, its various stages used in healthcare, and highlighted the key components typically present in an imaging-based tumor analysis pipeline. A search was conducted across four databases (PubMed, Ovid, Embase, and Google Scholar) to gather research articles from the past 10 years (2009-2019) involving imaging related to pituitary tumor and ML. We grouped the studies by imaging modalities and analyzed the ML tasks in terms of the data inputs, reference standards, methodologies, and limitations.              Results:                    Of the 16 studies included in our analysis, 10 appeared in 2018-2019. Most of the studies utilized retrospective data and followed a semi-automatic ML pipeline. The studies included use of magnetic resonance imaging (MRI), facial photographs, surgical microscopic video, spectrometry, and spectroscopy imaging. The objectives of the studies covered 14 distinct applications and majority of the studies addressed a binary classification problem. Only five of the 11 MRI-based studies had an external validation or a holdout set to test the performance of a final trained model.              Conclusion:                    Through our concise evaluation and comparison of the studies using the concepts presented, we highlight future directions so that potential ML applications using different imaging modalities can be developed to benefit the clinical care of pituitary tumor patients.",2020-06-01,1,1824,142,341
1486,32582539,Machine Learning-Based Models for Prediction of Toxicity Outcomes in Radiotherapy,"In order to limit radiotherapy (RT)-related side effects, effective toxicity prediction and assessment schemes are essential. In recent years, the growing interest toward artificial intelligence and machine learning (ML) within the science community has led to the implementation of innovative tools in RT. Several researchers have demonstrated the high performance of ML-based models in predicting toxicity, but the application of these approaches in clinics is still lagging, partly due to their low interpretability. Therefore, an overview of contemporary research is needed in order to familiarize practitioners with common methods and strategies. Here, we present a review of ML-based models for predicting and classifying RT-induced complications from both a methodological and a clinical standpoint, focusing on the type of features considered, the ML methods used, and the main results achieved. Our work overviews published research in multiple cancer sites, including brain, breast, esophagus, gynecological, head and neck, liver, lung, and prostate cancers. The aim is to define the current state of the art and main achievements within the field for both researchers and clinicians.",2020-06-01,1,1194,81,341
990,31880240,Computational Models for Self-Interacting Proteins Prediction,"Self-Interacting Proteins (SIPs), whose two or more copies can interact with each other, have significant roles in cellular functions and evolution of Protein Interaction Networks (PINs). Knowing whether a protein can act on itself is important to understand its functions. Previous studies on SIPs have focused on their structures and functions, while their whole properties are less emphasized. Not surprisingly, identifying SIPs is one of the most important works in biomedical research, which will help to understanding the function and mechanism of proteins. It is worth noting that high throughput methods can be used for SIPs prediction, but can be costly, time consuming and challenging. Therefore, it is urgent to design computational models for the identification of SIPs. In this review, the concept and function of SIPs were introduced in detail. We further introduced SIPs data and some excellent computational models that have been designed for SIPs prediction. Specially, the most existing approaches were developed based on machine learning through carrying out different extract feature methods. Finally, we discussed several difficult problems in developing computational models for SIPs prediction.",2020-06-01,0,1217,61,341
1443,32626645,A systematic review on spatial crime forecasting,"Background:                    Predictive policing and crime analytics with a spatiotemporal focus get increasing attention among a variety of scientific communities and are already being implemented as effective policing tools. The goal of this paper is to provide an overview and evaluation of the state of the art in spatial crime forecasting focusing on study design and technical aspects.              Methods:                    We follow the PRISMA guidelines for reporting this systematic literature review and we analyse 32 papers from 2000 to 2018 that were selected from 786 papers that entered the screening phase and a total of 193 papers that went through the eligibility phase. The eligibility phase included several criteria that were grouped into: (a) the publication type, (b) relevance to research scope, and (c) study characteristics.              Results:                    The most predominant type of forecasting inference is the hotspots (i.e. binary classification) method. Traditional machine learning methods were mostly used, but also kernel density estimation based approaches, and less frequently point process and deep learning approaches. The top measures of evaluation performance are the Prediction Accuracy, followed by the Prediction Accuracy Index, and the F1-Score. Finally, the most common validation approach was the train-test split while other approaches include the cross-validation, the leave one out, and the rolling horizon.              Limitations:                    Current studies often lack a clear reporting of study experiments, feature engineering procedures, and are using inconsistent terminology to address similar problems.              Conclusions:                    There is a remarkable growth in spatial crime forecasting studies as a result of interdisciplinary technical work done by scholars of various backgrounds. These studies address the societal need to understand and combat crime as well as the law enforcement interest in almost real-time prediction.              Implications:                    Although we identified several opportunities and strengths there are also some weaknesses and threats for which we provide suggestions. Future studies should not neglect the juxtaposition of (existing) algorithms, of which the number is constantly increasing (we enlisted 66). To allow comparison and reproducibility of studies we outline the need for a protocol or standardization of spatial forecasting approaches and suggest the reporting of a study's key data items.",2020-06-01,1,2543,48,341
1008,31845078,Landscape of HIV Implementation Research Funded by the National Institutes of Health: A Mapping Review of Project Abstracts,"In 2019, the requisite biomedical and behavioral interventions to eliminate new HIV infections exist. ""Ending the HIV Epidemic"" now becomes primarily a challenge of will and implementation. This review maps the extent to which implementation research (IR) has been integrated into HIV research by reviewing the recent funding portfolio of the NIH. We searched NIH RePORTER for HIV and IR-related research projects funded from January 2013 to March 2018. The 4629 unique studies identified were screened using machine learning and manual methods. 216 abstracts met the eligibility criteria of HIV and IR. Key study characteristics were then abstracted. NIH currently funds HIV studies that are either formally IR (n = 109) or preparatory for IR (n = 107). Few (13%) projects mentioned a guiding implementation model, theory, or framework, and only 56% of all studies explicitly mentioned measuring an implementation outcome. Considering the study aims along an IR continuum, 18 (8%) studies examined barriers and facilitators, 43 (20%) developed implementation strategies, 46 (21%) piloted strategies, 73 (34%) tested a single strategy, and 35 (16%) compared strategies. A higher proportion of formal IR projects involved established interventions (e.g., integrated services) compared to newer interventions (e.g., pre-exposure prophylaxis). Prioritizing HIV-related IR in NIH and other federal funding opportunity announcements and expanded training in implementation science could have a substantial impact on ending the HIV pandemic. This review serves as a baseline by which to compare funding patterns and the sophistication of IR in HIV research over time.",2020-06-01,5,1661,123,341
1227,32108409,Will machine learning applied to neuroimaging in bipolar disorder help the clinician? A critical review and methodological suggestions,"Objectives:                    The existence of anatomofunctional brain abnormalities in bipolar disorder (BD) is now well established by magnetic resonance imaging (MRI) studies. To create diagnostic and prognostic tools, as well as identifying biologically valid subtypes of BD, research has recently turned towards the use of machine learning (ML) techniques. We assessed both supervised ML and unsupervised ML studies in BD to evaluate their robustness, reproducibility and the potential need for improvement.              Method:                    We systematically searched for studies using ML algorithms based on MRI data of patients with BD until February 2019.              Result:                    We identified 47 studies, 45 using supervised ML techniques and 2 including unsupervised ML analyses. Among supervised studies, 43 focused on diagnostic classification. The reported accuracies for classification of BD ranged between (a) 57% and 100%, for BD vs healthy controls; (b) 49.5% and 93.1% for BD vs patients with major depressive disorder; and (c) 50% and 96.2% for BD vs patients with schizophrenia. Reported accuracies for discriminating subjects genetically at risk for BD (either from control or from patients with BD) ranged between 64.3% and 88.93%.              Conclusions:                    Although there are strong methodological limitations in previous studies and an important need for replication in large multicentric samples, the conclusions of our review bring hope of future computer-aided diagnosis of BD and pave the way for other applications, such as treatment response prediction. To reinforce the reliability of future results we provide methodological suggestions for good practice in conducting and reporting MRI-based ML studies in BD.",2020-06-01,4,1785,134,341
2016,32849924,"Biobanks in the era of big data: objectives, challenges, perspectives, and innovations for predictive, preventive, and personalised medicine","Biobanking is entering the new era-era of big data. New technologies, techniques, and knowledge opened the potential of the whole domain of biobanking. Biobanks collect, analyse, store, and share the samples and associated data. Both samples and especially associated data are growing enormously, and new innovative approaches are required to handle samples and to utilize the potential of biobanking data. The data reached the quantity and quality of big data, and the scientists are facing the questions how to use them more efficiently, both retrospectively and prospectively with the aim to discover new preventive methods, optimize treatment, and follow up and to optimize healthcare processes. Biobanking in the era of big data contribute to the development of predictive, preventive, and personalised medicine, for every patient providing the right treatment at the right time. Biobanking in the era of big data contributes to the paradigm shift towards personalising of healthcare.",2020-06-01,2,989,140,341
849,31352462,New Technologies for Outcome Measures in Retinal Disease: Review from the European Vision Institute Special Interest Focus Group,"Novel diagnostic tools to measure retinal function and structure are rapidly being developed and introduced into clinical use. Opportunities exist to use these informative and robust measures as endpoints for clinical trials to determine efficacy and to monitor safety of therapeutic interventions. In order to inform researchers and clinician-scientists about these new diagnostic tools, a workshop was organized by the European Vision Institute. Invited speakers highlighted the recent advances in state-of-the-art technologies for outcome measures in the field of retina. This review highlights the workshop's presentations in the context of published literature.",2020-06-01,1,666,128,341
943,31952684,Prediction and targeting of GPCR oligomer interfaces,"GPCR oligomerization has emerged as a hot topic in the GPCR field in the last years. Receptors that are part of these oligomers can influence each other's function, although it is not yet entirely understood how these interactions work. The existence of such a highly complex network of interactions between GPCRs generates the possibility of alternative targets for new therapeutic approaches. However, challenges still exist in the characterization of these complexes, especially at the interface level. Different experimental approaches, such as FRET or BRET, are usually combined to study GPCR oligomer interactions. Computational methods have been applied as a useful tool for retrieving information from GPCR sequences and the few X-ray-resolved oligomeric structures that are accessible, as well as for predicting new and trustworthy GPCR oligomeric interfaces. Machine-learning (ML) approaches have recently helped with some hindrances of other methods. By joining and evaluating multiple structure-, sequence- and co-evolution-based features on the same algorithm, it is possible to dilute the issues of particular structures and residues that arise from the experimental methodology into all-encompassing algorithms capable of accurately predict GPCR-GPCR interfaces. All these methods used as a single or a combined approach provide useful information about GPCR oligomerization and its role in GPCR function and dynamics. Altogether, we present experimental, computational and machine-learning methods used to study oligomers interfaces, as well as strategies that have been used to target these dynamic complexes.",2020-06-01,1,1626,52,341
814,32983950,Artificial Intelligence and Computational Approaches for Epilepsy,"Studies on treatment of epilepsy have been actively conducted in multiple avenues, but there are limitations in improving its efficacy due to between-subject variability in which treatment outcomes vary from patient to patient. Accordingly, there is a growing interest in precision medicine that provides accurate diagnosis for seizure types and optimal treatment for an individual epilepsy patient. Among these approaches, computational studies making this feasible are rapidly progressing in particular and have been widely applied in epilepsy. These computational studies are being conducted in two main streams: 1) artificial intelligence-based studies implementing computational machines with specific functions, such as automatic diagnosis and prognosis prediction for an individual patient, using machine learning techniques based on large amounts of data obtained from multiple patients and 2) patient-specific modeling-based studies implementing biophysical in-silico platforms to understand pathological mechanisms and derive the optimal treatment for each patient by reproducing the brain network dynamics of the particular patient per se based on individual patient's data. These computational approaches are important as it can integrate multiple types of data acquired from patients and analysis results into a single platform. If these kinds of methods are efficiently operated, it would suggest a novel paradigm for precision medicine.",2020-06-01,2,1451,65,341
1515,32239350,Artificial Intelligence in Hematology: Current Challenges and Opportunities,"Purpose of review:                    Artificial intelligence (AI), and in particular its subcategory machine learning, is finding an increasing number of applications in medicine, driven in large part by an abundance of data and powerful, accessible tools that have made AI accessible to a larger circle of investigators.              Recent findings:                    AI has been employed in the analysis of hematopathological, radiographic, laboratory, genomic, pharmacological, and chemical data to better inform diagnosis, prognosis, treatment planning, and foundational knowledge related to benign and malignant hematology. As more widespread implementation of clinical AI nears, attention has also turned to the effects this will have on other areas in medicine. AI offers many promising tools to clinicians broadly, and specifically in the practice of hematology. Ongoing research into its various applications will likely result in an increasing utilization of AI by a broader swath of clinicians.",2020-06-01,2,1008,75,341
1522,32228387,Artificial Intelligence in Clinical Neuroscience: Methodological and Ethical Challenges,"Clinical neuroscience is increasingly relying on the collection of large volumes of differently structured data and the use of intelligent algorithms for data analytics. In parallel, the ubiquitous collection of unconventional data sources (e.g. mobile health, digital phenotyping, consumer neurotechnology) is increasing the variety of data points. Big data analytics and approaches to Artificial Intelligence (AI) such as advanced machine learning are showing great potential to make sense of these larger and heterogeneous data flows. AI provides great opportunities for making new discoveries about the brain, improving current preventative and diagnostic models in both neurology and psychiatry and developing more effective assistive neurotechnologies. Concurrently, it raises many new methodological and ethical challenges. Given their transformative nature, it is still largely unclear how AI-driven approaches to the study of the human brain will meet adequate standards of scientific validity and affect normative instruments in neuroethics and research ethics. This manuscript provides an overview of current AI-driven approaches to clinical neuroscience and an assessment of the associated key methodological and ethical challenges. In particular, it will discuss what ethical principles are primarily affected by AI approaches to human neuroscience, and what normative safeguards should be enforced in this domain.",2020-06-01,0,1427,87,341
762,33042325,Management of Acute Pulmonary Embolism,"Purpose of the review:                    Over 100,000 cardiovascular-related deaths annually are caused by acute pulmonary embolism (PE). While anticoagulation has historically been the foundation for treatment of PE, this review highlights the recent rapid expansion in the interventional strategies for this condition.              Recent findings:                    At the time of diagnosis, appropriate risk stratification helps to accurately identify patients who may be candidates for advanced therapeutic interventions. While systemic thrombolytics (ST) is the mostly commonly utilized intervention for high-risk PE, the risk profile of ST for intermediate-risk PE limits its use. Assessment of an individualized patient risk profile, often via a multidisciplinary pulmonary response team (PERT) model, there are various interventional strategies to consider for PE management. Novel therapeutic options include catheter-directed thrombolysis, catheter-based embolectomy, or mechanical circulatory support for certain high-risk PE patients. Current data has established safety and efficacy for catheter-based treatment of PE based on surrogate outcome measures. However, there is limited long-term data or prospective comparisons between treatment modalities and ST. While PE diagnosis has improved with modern cross-sectional imaging, there is interest in improved diagnostic models for PE that incorporate artificial intelligence and machine learning techniques.              Summary:                    In patients with acute pulmonary embolism, after appropriate risk stratification, some intermediate and high-risk patients should be considered for interventional-based treatment for PE.",2020-06-01,0,1701,38,341
2021,32833571,How Will Machine Learning Inform the Clinical Care of Atrial Fibrillation?,"Machine learning applications in cardiology have rapidly evolved in the past decade. With the availability of machine learning tools coupled with vast data sources, the management of atrial fibrillation (AF), a common chronic disease with significant associated morbidity and socioeconomic impact, is undergoing a knowledge and practice transformation in the increasingly complex healthcare environment. Among other advances, deep-learning machine learning methods, including convolutional neural networks, have enabled the development of AF screening pathways using the ubiquitous 12-lead ECG to detect asymptomatic paroxysmal AF in at-risk populations (such as those with cryptogenic stroke), the refinement of AF and stroke prediction schemes through comprehensive digital phenotyping using structured and unstructured data abstraction from the electronic health record or wearable monitoring technologies, and the optimization of treatment strategies, ranging from stroke prophylaxis to monitoring of antiarrhythmic drug (AAD) therapy. Although the clinical and population-wide impact of these tools continues to be elucidated, such transformative progress does not come without challenges, such as the concerns about adopting black box technologies, assessing input data quality for training such models, and the risk of perpetuating rather than alleviating health disparities. This review critically appraises the advances of machine learning related to the care of AF thus far, their potential future directions, and its potential limitations and challenges.",2020-06-01,0,1565,74,341
1503,32250332,Artificial neural networks in neurorehabilitation: A scoping review,"Background:                    Advances in medical technology produce highly complex datasets in neurorehabilitation clinics and research laboratories. Artificial neural networks (ANNs) have been utilized to analyze big and complex datasets in various fields, but the use of ANNs in neurorehabilitation is limited.              Objective:                    To explore the current use of ANNs in neurorehabilitation.              Methods:                    PubMed, CINAHL, and Web of Science were used for the literature search. Studies in the scoping review (1) utilized ANNs, (2) examined populations with neurological conditions, and (3) focused on rehabilitation outcomes. The initial search identified 1,136 articles. A total of 19 articles were included.              Results:                    ANNs were used for prediction of functional outcomes and mortality (n = 11) and classification of motor symptoms and cognitive status (n = 8). Most ANN-based models outperformed regression or other machine learning models (n = 11) and showed accurate performance (n = 6; no comparison with other models) in predicting clinical outcomes and accurately classifying different neurological impairments.              Conclusions:                    This scoping review provides encouraging evidence to use ANNs for clinical decision-making of complex datasets in neurorehabilitation. However, more research is needed to establish the clinical utility of ANNs in diagnosing, monitoring, and rehabilitation of individuals with neurological conditions.",2020-06-01,2,1547,67,341
1820,32765399,The Role of Magnetic Resonance Imaging for the Diagnosis of Atypical Parkinsonism,"The diagnosis of Parkinson's disease and atypical Parkinsonism remains clinically difficult, especially at the early stage of the disease, since there is a significant overlap of symptoms. Multimodal MRI has significantly improved diagnostic accuracy and understanding of the pathophysiology of Parkinsonian disorders. Structural and quantitative MRI sequences provide biomarkers sensitive to different tissue properties that detect abnormalities specific to each disease and contribute to the diagnosis. Machine learning techniques using these MRI biomarkers can effectively differentiate atypical Parkinsonian syndromes. Such approaches could be implemented in a clinical environment and improve the management of Parkinsonian patients. This review presents different structural and quantitative MRI techniques, their contribution to the differential diagnosis of atypical Parkinsonian disorders and their interest for individual-level diagnosis.",2020-07-01,1,948,81,311
1835,32733918,"Allosteric Regulation at the Crossroads of New Technologies: Multiscale Modeling, Networks, and Machine Learning","Allosteric regulation is a common mechanism employed by complex biomolecular systems for regulation of activity and adaptability in the cellular environment, serving as an effective molecular tool for cellular communication. As an intrinsic but elusive property, allostery is a ubiquitous phenomenon where binding or disturbing of a distal site in a protein can functionally control its activity and is considered as the ""second secret of life."" The fundamental biological importance and complexity of these processes require a multi-faceted platform of synergistically integrated approaches for prediction and characterization of allosteric functional states, atomistic reconstruction of allosteric regulatory mechanisms and discovery of allosteric modulators. The unifying theme and overarching goal of allosteric regulation studies in recent years have been integration between emerging experiment and computational approaches and technologies to advance quantitative characterization of allosteric mechanisms in proteins. Despite significant advances, the quantitative characterization and reliable prediction of functional allosteric states, interactions, and mechanisms continue to present highly challenging problems in the field. In this review, we discuss simulation-based multiscale approaches, experiment-informed Markovian models, and network modeling of allostery and information-theoretical approaches that can describe the thermodynamics and hierarchy allosteric states and the molecular basis of allosteric mechanisms. The wealth of structural and functional information along with diversity and complexity of allosteric mechanisms in therapeutically important protein families have provided a well-suited platform for development of data-driven research strategies. Data-centric integration of chemistry, biology and computer science using artificial intelligence technologies has gained a significant momentum and at the forefront of many cross-disciplinary efforts. We discuss new developments in the machine learning field and the emergence of deep learning and deep reinforcement learning applications in modeling of molecular mechanisms and allosteric proteins. The experiment-guided integrated approaches empowered by recent advances in multiscale modeling, network science, and machine learning can lead to more reliable prediction of allosteric regulatory mechanisms and discovery of allosteric modulators for therapeutically important protein targets.",2020-07-01,1,2477,112,311
1819,32765975,Machine learning: A powerful tool for gene function prediction in plants,"Recent advances in sequencing and informatic technologies have led to a deluge of publicly available genomic data. While it is now relatively easy to sequence, assemble, and identify genic regions in diploid plant genomes, functional annotation of these genes is still a challenge. Over the past decade, there has been a steady increase in studies utilizing machine learning algorithms for various aspects of functional prediction, because these algorithms are able to integrate large amounts of heterogeneous data and detect patterns inconspicuous through rule-based approaches. The goal of this review is to introduce experimental plant biologists to machine learning, by describing how it is currently being used in gene function prediction to gain novel biological insights. In this review, we discuss specific applications of machine learning in identifying structural features in sequenced genomes, predicting interactions between different cellular components, and predicting gene function and organismal phenotypes. Finally, we also propose strategies for stimulating functional discovery using machine learning-based approaches in plants.",2020-07-01,3,1147,72,311
1818,32766229,"Automation, Monitoring, and Standardization of Cell Product Manufacturing","Although regenerative medicine products are at the forefront of scientific research, technological innovation, and clinical translation, their reproducibility and large-scale production are compromised by automation, monitoring, and standardization issues. To overcome these limitations, new technologies at software (e.g., algorithms and artificial intelligence models, combined with imaging software and machine learning techniques) and hardware (e.g., automated liquid handling, automated cell expansion bioreactor systems, automated colony-forming unit counting and characterization units, and scalable cell culture plates) level are under intense investigation. Automation, monitoring and standardization should be considered at the early stages of the developmental cycle of cell products to deliver more robust and effective therapies and treatment plans to the bedside, reducing healthcare expenditure and improving services and patient care.",2020-07-01,4,950,73,311
1483,32587159,Insights into the growing popularity of artificial intelligence in ophthalmology,"Artificial intelligence (AI) in healthcare is the use of computer-algorithms in analyzing complex medical data to detect associations and provide diagnostic support outputs. AI and deep learning (DL) find obvious applications in fields like ophthalmology wherein huge amount of image-based data need to be analyzed; however, the outcomes related to image recognition are reasonably well-defined. AI and DL have found important roles in ophthalmology in early screening and detection of conditions such as diabetic retinopathy (DR), age-related macular degeneration (ARMD), retinopathy of prematurity (ROP), glaucoma, and other ocular disorders, being successful inroads as far as early screening and diagnosis are concerned and appear promising with advantages of high-screening accuracy, consistency, and scalability. AI algorithms need equally skilled manpower, trained optometrists/ophthalmologists (annotators) to provide accurate ground truth for training the images. The basis of diagnoses made by AI algorithms is mechanical, and some amount of human intervention is necessary for further interpretations. This review was conducted after tracing the history of AI in ophthalmology across multiple research databases and aims to summarise the journey of AI in ophthalmology so far, making a close observation of most of the crucial studies conducted. This article further aims to highlight the potential impact of AI in ophthalmology, the pitfalls, and how to optimally use it to the maximum benefits of the ophthalmologists, the healthcare systems and the patients, alike.",2020-07-01,3,1579,80,311
1865,32699250,Causality matters in medical imaging,"Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies.",2020-07-01,3,679,36,311
1842,32728829,The Role of Artificial Intelligence in Echocardiography,"Purpose of review:                    Echocardiography is an indispensable tool in diagnostic cardiology and is fundamental to clinical care. Significant advances in cardiovascular imaging technology paralleled by rapid growth in electronic medical records, miniaturized devices, real-time monitoring, and wearable devices using body sensor network technology have led to the development of complex data.              Recent findings:                    The intricate nature of these data can be overwhelming and exceed the capabilities of current statistical software. Machine learning (ML), a branch of artificial intelligence (AI), can help health care providers navigate through this complex labyrinth of information and unravel hidden discoveries. Furthermore, ML algorithms can help automate several tasks in echocardiography and clinical care. ML can serve as a valuable diagnostic tool for physicians in the field of echocardiography. In addition, it can help expand the capabilities of research and discover alternative pathways in medical management. In this review article, we describe the role of AI and ML in echocardiography.",2020-07-01,3,1139,55,311
1816,32767227,Emerging Clinical Technology: Application of Machine Learning to Chronic Pain Assessments Based on Emotional Body Maps,"Depression and anxiety co-occur with chronic pain, and all three are thought to be caused by dysregulation of shared brain systems related to emotional processing associated with body sensations. Understanding the connection between emotional states, pain, and bodily sensations may help understand chronic pain conditions. We developed a mobile platform for measuring pain, emotions, and associated bodily feelings in chronic pain patients in their daily life conditions. Sixty-five chronic back pain patients reported the intensity of their pain, 11 emotional states, and the corresponding body locations. These variables were used to predict pain 2 weeks later. Applying machine learning, we developed two predictive models of future pain, emphasizing interpretability. One model excluded pain-related features as predictors of future pain, and the other included pain-related predictors. The best predictors of future pain were interactive effects of (a) body maps of fatigue with negative affect and (b) positive affect with past pain. Our findings emphasize the contribution of emotions, especially emotional experience felt in the body, to understanding chronic pain above and beyond the mere tracking of pain levels. The results may contribute to the generation of a novel artificial intelligence framework to help in the development of better diagnostic and therapeutic approaches to chronic pain.",2020-07-01,1,1406,118,311
1796,32793285,A Review on the Challenges in Indian Genomics Research for Variant Identification and Interpretation,"Today, genomic data holds great potential to improve healthcare strategies across various dimensions - be it disease prevention, enhanced diagnosis, or optimized treatment. The biggest hurdle faced by the medical and research community in India is the lack of genotype-phenotype correlations for Indians at a population-wide and an individual level. This leads to inefficient translation of genomic information during clinical decision making. Population-wide sequencing projects for Indian genomes help overcome hurdles and enable us to unearth and validate the genetic markers for different health conditions. Machine learning algorithms are essential to analyze huge amounts of genotype data in synergy with gene expression, demographic, clinical, and pathological data. Predictive models developed through these algorithms help in classifying the individuals into different risk groups, so that preventive measures and personalized therapies can be designed. They also help in identifying the impact of each genetic marker with the associated condition, from a clinical perspective. In India, genome sequencing technologies have now become more accessible to the general population. However, information on variants associated with several major diseases is not available in publicly-accessible databases. Creating a centralized database of variants facilitates early detection and mitigation of health risks in individuals. In this article, we discuss the challenges faced by genetic researchers and genomic testing facilities in India, in terms of dearth of public databases, people with knowledge on machine learning algorithms, computational resources and awareness in the medical community in interpreting genetic variants. Potential solutions to enhance genomic research in India, are also discussed.",2020-07-01,0,1810,100,311
1579,32128929,Artificial intelligence as the next step towards precision pathology,"Pathology is the cornerstone of cancer care. The need for accuracy in histopathologic diagnosis of cancer is increasing as personalized cancer therapy requires accurate biomarker assessment. The appearance of digital image analysis holds promise to improve both the volume and precision of histomorphological evaluation. Recently, machine learning, and particularly deep learning, has enabled rapid advances in computational pathology. The integration of machine learning into routine care will be a milestone for the healthcare sector in the next decade, and histopathology is right at the centre of this revolution. Examples of potential high-value machine learning applications include both model-based assessment of routine diagnostic features in pathology, and the ability to extract and identify novel features that provide insights into a disease. Recent groundbreaking results have demonstrated that applications of machine learning methods in pathology significantly improves metastases detection in lymph nodes, Ki67 scoring in breast cancer, Gleason grading in prostate cancer and tumour-infiltrating lymphocyte (TIL) scoring in melanoma. Furthermore, deep learning models have also been demonstrated to be able to predict status of some molecular markers in lung, prostate, gastric and colorectal cancer based on standard HE slides. Moreover, prognostic (survival outcomes) deep neural network models based on digitized HE slides have been demonstrated in several diseases, including lung cancer, melanoma and glioma. In this review, we aim to present and summarize the latest developments in digital image analysis and in the application of artificial intelligence in diagnostic pathology.",2020-07-01,13,1702,68,311
1845,32726074,Leveraging Population Genomics for Individualized Correction of the Hallmarks of Alpha-1 Antitrypsin Deficiency,"Deep medicine is rapidly moving towards a high-definition approach for therapeutic management of the patient as an individual given the rapid progress of genome sequencing technologies and machine learning algorithms. While considered a monogenic disease, alpha-1 antitrypsin (AAT) deficiency (AATD) patients present with complex and variable phenotypes we refer to as the ""hallmarks of AATD"" that involve distinct molecular mechanisms in the liver, plasma and lung tissues, likely due to both coding and non-coding variation as well as genetic and environmental modifiers in different individuals. Herein, we briefly review the current therapeutic strategies for the management of AATD. To embrace genetic diversity in the management of AATD, we provide an overview of the disease phenotypes of AATD patients harboring different AAT variants. Linking genotypic diversity to phenotypic diversity illustrates the potential for sequence-specific regions of AAT protein fold design to play very different roles during nascent synthesis in the liver and/or function in post-liver plasma and lung environments. We illustrate how to manage diversity with recently developed machine learning (ML) approaches that bridge sequence-to-function-to-structure knowledge gaps based on the principle of spatial covariance (SCV). SCV relationships provide a deep understanding of the genotype to phenotype transformation initiated by AAT variation in the population to address the role of genetic and environmental modifiers in the individual. Embracing the complexity of AATD in the population is critical for risk management and therapeutic intervention to generate a high definition medicine approach for the patient.",2020-07-01,0,1704,111,311
1520,32231270,Memory devices and applications for in-memory computing,"Traditional von Neumann computing systems involve separate processing and memory units. However, data movement is costly in terms of time and energy and this problem is aggravated by the recent explosive growth in highly data-centric applications related to artificial intelligence. This calls for a radical departure from the traditional systems and one such non-von Neumann computational approach is in-memory computing. Hereby certain computational tasks are performed in place in the memory itself by exploiting the physical attributes of the memory devices. Both charge-based and resistance-based memory devices are being explored for in-memory computing. In this Review, we provide a broad overview of the key computational primitives enabled by these memory devices as well as their applications spanning scientific computing, signal processing, optimization, machine learning, deep learning and stochastic computing.",2020-07-01,13,924,55,311
1482,32589980,Machine learning in haematological malignancies,"Machine learning is a branch of computer science and statistics that generates predictive or descriptive models by learning from training data rather than by being rigidly programmed. It has attracted substantial attention for its many applications in medicine, both as a catalyst for research and as a means of improving clinical care across the cycle of diagnosis, prognosis, and treatment of disease. These applications include the management of haematological malignancy, in which machine learning has created inroads in pathology, radiology, genomics, and the analysis of electronic health record data. As computational power becomes cheaper and the tools for implementing machine learning become increasingly democratised, it is likely to become increasingly integrated into the research and practice landscape of haematology. As such, machine learning merits understanding and attention from researchers and clinicians alike. This narrative Review describes important concepts in machine learning for unfamiliar readers, details machine learning's current applications in haematological malignancy, and summarises important concepts for clinicians to be aware of when appraising research that uses machine learning.",2020-07-01,5,1222,47,311
1860,32706670,Reinforcement Learning for Clinical Decision Support in Critical Care: Comprehensive Review,"Background:                    Decision support systems based on reinforcement learning (RL) have been implemented to facilitate the delivery of personalized care. This paper aimed to provide a comprehensive review of RL applications in the critical care setting.              Objective:                    This review aimed to survey the literature on RL applications for clinical decision support in critical care and to provide insight into the challenges of applying various RL models.              Methods:                    We performed an extensive search of the following databases: PubMed, Google Scholar, Institute of Electrical and Electronics Engineers (IEEE), ScienceDirect, Web of Science, Medical Literature Analysis and Retrieval System Online (MEDLINE), and Excerpta Medica Database (EMBASE). Studies published over the past 10 years (2010-2019) that have applied RL for critical care were included.              Results:                    We included 21 papers and found that RL has been used to optimize the choice of medications, drug dosing, and timing of interventions and to target personalized laboratory values. We further compared and contrasted the design of the RL models and the evaluation metrics for each application.              Conclusions:                    RL has great potential for enhancing decision making in critical care. Challenges regarding RL system design, evaluation metrics, and model choice exist. More importantly, further work is required to validate RL in authentic clinical environments.",2020-07-01,0,1543,91,311
1500,32251707,Machine Learning in oncology: A clinical appraisal,"Machine learning (ML) is a branch of artificial intelligence centered on algorithms which do not need explicit prior programming to function but automatically learn from available data, creating decision models to complete tasks. ML-based tools have numerous promising applications in several fields of medicine. Its use has grown following the increased availability of patient data due to technological advances such as digital health records and high-volume information extraction from medical images. Multiple ML algorithms have been proposed for applications in oncology. For instance, they have been employed for oncological risk assessment, automated segmentation, lesion detection, characterization, grading and staging, prediction of prognosis and therapy response. In the near future, ML could become essential part of every step of oncological screening strategies and patients' management thus leading to precision medicine.",2020-07-01,10,936,50,311
1790,32802272,Methods for sequence and structural analysis of B and T cell receptor repertoires,"B cell receptors (BCRs) and T cell receptors (TCRs) make up an essential network of defense molecules that, collectively, can distinguish self from non-self and facilitate destruction of antigen-bearing cells such as pathogens or tumors. The analysis of BCR and TCR repertoires plays an important role in both basic immunology as well as in biotechnology. Because the repertoires are highly diverse, specialized software methods are needed to extract meaningful information from BCR and TCR sequence data. Here, we review recent developments in bioinformatics tools for analysis of BCR and TCR repertoires, with an emphasis on those that incorporate structural features. After describing the recent sequencing technologies for immune receptor repertoires, we survey structural modeling methods for BCR and TCRs, along with methods for clustering such models. We review downstream analyses, including BCR and TCR epitope prediction, antibody-antigen docking and TCR-peptide-MHC Modeling. We also briefly discuss molecular dynamics in this context.",2020-07-01,2,1046,81,311
1826,32754606,Artificial Intelligence in Cutaneous Oncology,"Skin cancer, previously known to be a common disease in Western countries, is becoming more common in Asian countries. Skin cancer differs from other carcinomas in that it is visible to our eyes. Although skin biopsy is essential for the diagnosis of skin cancer, decisions regarding whether or not to conduct a biopsy are made by an experienced dermatologist. From this perspective, it is easy to obtain and store photos using a smartphone, and artificial intelligence technologies developed to analyze these photos can represent a useful tool to complement the dermatologist's knowledge. In addition, the universal use of dermoscopy, which allows for non-invasive inspection of the upper dermal level of skin lesions with a usual 10-fold magnification, adds to the image storage and analysis techniques, foreshadowing breakthroughs in skin cancer diagnosis. Current problems include the inaccuracy of the available technology and resulting legal liabilities. This paper presents a comprehensive review of the clinical applications of artificial intelligence and a discussion on how it can be implemented in the field of cutaneous oncology.",2020-07-01,0,1141,45,311
1851,32715331,The automaton as a surgeon: the future of artificial intelligence in emergency and general surgery,"Background:                    Artificial intelligence (AI) is a field involving computational simulation of human intelligence processes; these applications of deep learning could have implications in the specialty of emergency surgery (ES). ES is a rapidly advancing area, and this review will outline the most recent advances.              Methods:                    A literature search encompassing the uses of AI in surgery was conducted across large databases (Pubmed, OVID, SCOPUS). Two doctors (LR, CH) both collated relevant papers and appraised them. Papers included were published within the last 5 years, and a ""snowball effect"" used to collate further relevant literature.              Results:                    AI has been shown to provide value in predicting surgical outcomes and giving personalised patient risks based on inputted data. Further to this, image recognition technology within AI has showed success in fracture identification and breast cancer diagnosis. Regarding theatre presence, supervised robots have carried out suturing and anastomosis of bowel in controlled environments to a high standard.              Conclusion:                    AI has potential for integration across surgical services, from diagnosis to treatment, and aiding the surgeon in key decision-making for risks per patient. Fully automated surgery may be the future, but at present, AI needs human supervision.",2020-07-01,1,1419,98,311
1740,31588687,The Role of Protein Engineering in Biomedical Applications of Mammalian Synthetic Biology,"Engineered proteins with enhanced or altered functionality, generated for example by mutation or domain fusion, are at the core of nearly all synthetic biology endeavors in the context of precision medicine, also known as personalized medicine. From designer receptors sensing elevated blood markers to effectors rerouting signaling pathways to synthetic transcription factors and the customized therapeutics they regulate, engineered proteins play a crucial role at every step of novel therapeutic approaches using synthetic biology. Here, recent developments in protein engineering aided by advances in directed evolution, de novo design, and machine learning are discussed. Building on clinical successes already achieved with chimeric antigen receptor (CAR-) T cells and other cell-based therapies, these developments are expected to further enhance the capabilities of mammalian synthetic biology in biomedical and other applications.",2020-07-01,0,939,89,311
1810,32774791,Limitations and challenges in protein stability prediction upon genome variations: towards future applications in precision medicine,"Protein stability predictions are becoming essential in medicine to develop novel immunotherapeutic agents and for drug discovery. Despite the large number of computational approaches for predicting the protein stability upon mutation, there are still critical unsolved problems: 1) the limited number of thermodynamic measurements for proteins provided by current databases; 2) the large intrinsic variability of G values due to different experimental conditions; 3) biases in the development of predictive methods caused by ignoring the anti-symmetry of G values between mutant and native protein forms; 4) over-optimistic prediction performance, due to sequence similarity between proteins used in training and test datasets. Here, we review these issues, highlighting new challenges required to improve current tools and to achieve more reliable predictions. In addition, we provide a perspective of how these methods will be beneficial for designing novel precision medicine approaches for several genetic disorders caused by mutations, such as cancer and neurodegenerative diseases.",2020-07-01,4,1092,132,311
1850,32715351,Digital Pharmaceutical Sciences,"Artificial intelligence (AI) and machine learning, in particular, have gained significant interest in many fields, including pharmaceutical sciences. The enormous growth of data from several sources, the recent advances in various analytical tools, and the continuous developments in machine learning algorithms have resulted in a rapid increase in new machine learning applications in different areas of pharmaceutical sciences. This review summarizes the past, present, and potential future impacts of machine learning technologies on different areas of pharmaceutical sciences, including drug design and discovery, preformulation, and formulation. The machine learning methods commonly used in pharmaceutical sciences are discussed, with a specific emphasis on artificial neural networks due to their capability to model the nonlinear relationships that are commonly encountered in pharmaceutical research. AI and machine learning technologies in common day-to-day pharma needs as well as industrial and regulatory insights are reviewed. Beyond traditional potentials of implementing digital technologies using machine learning in the development of more efficient, fast, and economical solutions in pharmaceutical sciences are also discussed.",2020-07-01,4,1246,31,311
1849,32715371,Detecting Seizures and Epileptiform Abnormalities in Acute Brain Injury,"Purpose of review:                    Acute brain injury (ABI) is a broad category of pathologies, including traumatic brain injury, and is commonly complicated by seizures. Electroencephalogram (EEG) studies are used to detect seizures or other epileptiform patterns. This review seeks to clarify EEG findings relevant to ABI, explore practical barriers limiting EEG implementation, discuss strategies to leverage EEG monitoring in various clinical settings, and suggest an approach to utilize EEG for triage.              Recent findings:                    Current literature suggests there is an increased morbidity and mortality risk associated with seizures or patterns on the ictal-interictal continuum (IIC) due to ABI. Further, increased use of EEG is associated with better clinical outcomes. However, there are many logistical barriers to successful EEG implementation that prohibit its ubiquitous use. Solutions to these limitations include the use of rapid EEG systems, non-expert EEG analysis, machine learning algorithms, and the incorporation of EEG data into prognostic models.",2020-07-01,1,1094,71,311
1857,32707839,Biological and Medical Importance of Cellular Heterogeneity Deciphered by Single-Cell RNA Sequencing,"The present review discusses recent progress in single-cell RNA sequencing (scRNA-seq), which can describe cellular heterogeneity in various organs, bodily fluids, and pathologies (e.g., cancer and Alzheimer's disease). We outline scRNA-seq techniques that are suitable for investigating cellular heterogeneity that is present in cell populations with very high resolution of the transcriptomic landscape. We summarize scRNA-seq findings and applications of this technology to identify cell types, activity, and other features that are important for the function of different bodily organs. We discuss future directions for scRNA-seq techniques that can link gene expression, protein expression, cellular function, and their roles in pathology. We speculate on how the field could develop beyond its present limitations (e.g., performing scRNA-seq in situ and in vivo). Finally, we discuss the integration of machine learning and artificial intelligence with cutting-edge scRNA-seq technology, which could provide a strong basis for designing precision medicine and targeted therapy in the future.",2020-07-01,1,1097,100,311
1847,32722605,"A Comparative Systematic Literature Review on Knee Bone Reports from MRI, X-rays and CT Scans Using Deep Learning and Machine Learning Methodologies","The purpose of this research was to provide a ""systematic literature review"" of knee bone reports that are obtained by MRI, CT scans, and X-rays by using deep learning and machine learning techniques by comparing different approaches-to perform a comprehensive study on the deep learning and machine learning methodologies to diagnose knee bone diseases by detecting symptoms from X-ray, CT scan, and MRI images. This study will help those researchers who want to conduct research in the knee bone field. A comparative systematic literature review was conducted for the accomplishment of our work. A total of 32 papers were reviewed in this research. Six papers consist of X-rays of knee bone with deep learning methodologies, five papers cover the MRI of knee bone using deep learning approaches, and another five papers cover CT scans of knee bone with deep learning techniques. Another 16 papers cover the machine learning techniques for evaluating CT scans, X-rays, and MRIs of knee bone. This research compares the deep learning methodologies for CT scan, MRI, and X-ray reports on knee bone, comparing the accuracy of each technique, which can be used for future development. In the future, this research will be enhanced by comparing X-ray, CT-scan, and MRI reports of knee bone with information retrieval and big data techniques. The results show that deep learning techniques are best for X-ray, MRI, and CT scan images of the knee bone to diagnose diseases.",2020-07-01,2,1467,148,311
1506,32249351,Radiomics in gliomas: clinical implications of computational modeling and fractal-based analysis,"Radiomics is an emerging field that involves extraction and quantification of features from medical images. These data can be mined through computational analysis and models to identify predictive image biomarkers that characterize intra-tumoral dynamics throughout the course of treatment. This is particularly difficult in gliomas, where heterogeneity has been well established at a molecular level as well as visually in conventional imaging. Thus, acquiring clinically useful features remains difficult due to temporal variations in tumor dynamics. Identifying surrogate biomarkers through radiomics may provide a non-invasive means of characterizing biologic activities of gliomas. We present an extensive literature review of radiomics-based analysis, with a particular focus on computational modeling, machine learning, and fractal-based analysis in improving differential diagnosis and predicting clinical outcomes. Novel strategies in extracting quantitative features, segmentation methods, and their clinical applications are producing promising results. Moreover, we provide a detailed summary of the morphometric parameters that have so far been proposed as a means of quantifying imaging characteristics of gliomas. Newly emerging radiomic techniques via machine learning and fractal-based analyses holds considerable potential for improving diagnostic and prognostic accuracy of gliomas. Key points Radiomic features can be mined through computational analysis to produce quantitative imaging biomarkers that characterize intra-tumoral dynamics throughout the course of treatment. Surrogate image biomarkers identified through radiomics could enable a non-invasive means of characterizing biologic activities of gliomas. With novel analytic algorithms, quantification of morphological or sub-regional tumor features to predict survival outcomes is producing promising results. Quantifying intra-tumoral heterogeneity may improve grading and molecular sub-classifications of gliomas. Computational fractal-based analysis of gliomas allows geometrical evaluation of tumor irregularities and complexity, leading to novel techniques for tumor segmentation, grading, and therapeutic monitoring.",2020-07-01,3,2208,96,311
1858,32706710,Toward a Taxonomy for Analyzing the Heart Rate as a Physiological Indicator of Posttraumatic Stress Disorder: Systematic Review and Development of a Framework,"Background:                    Posttraumatic stress disorder (PTSD) is a prevalent psychiatric condition that is associated with symptoms such as hyperarousal and overreactions. Treatments for PTSD are limited to medications and in-session therapies. Assessing the way the heart responds to PTSD has shown promise in detecting and understanding the onset of symptoms.              Objective:                    This study aimed to extract statistical and mathematical approaches that researchers can use to analyze heart rate (HR) data to understand PTSD.              Methods:                    A scoping literature review was conducted to extract HR models. A total of 5 databases including Medical Literature Analysis and Retrieval System Online (Medline) OVID, Medline EBSCO, Cumulative Index to Nursing and Allied Health Literature (CINAHL) EBSCO, Excerpta Medica Database (Embase) Ovid, and Google Scholar were searched. Non-English language studies, as well as studies that did not analyze human data, were excluded. A total of 54 studies that met the inclusion criteria were included in this review.              Results:                    We identified 4 categories of models: descriptive time-independent output, descriptive and time-dependent output, predictive and time-independent output, and predictive and time-dependent output. Descriptive and time-independent output models include analysis of variance and first-order exponential; the descriptive time-dependent output model includes a classical time series analysis and mixed regression. Predictive time-independent output models include machine learning methods and analysis of the HR-based fluctuation-dissipation method. Finally, predictive time-dependent output models include the time-variant method and nonlinear dynamic modeling.              Conclusions:                    All of the identified modeling categories have relevance in PTSD, although the modeling selection is dependent on the specific goals of the study. Descriptive models are well-founded for the inference of PTSD. However, there is a need for additional studies in this area that explore a broader set of predictive models and other factors (eg, activity level) that have not been analyzed with descriptive models.",2020-07-01,0,2263,158,311
1512,32244257,Recent advances in computational methods for measurement of dendritic spines imaged by light microscopy,"Dendritic spines are small protrusions that receive most of the excitatory inputs to the pyramidal neurons in the neocortex and the hippocampus. Excitatory neural circuits in the neocortex and hippocampus are important for experience-dependent changes in brain functions, including postnatal sensory refinement and memory formation. Several lines of evidence indicate that synaptic efficacy is correlated with spine size and structure. Hence, precise and accurate measurement of spine morphology is important for evaluation of neural circuit function and plasticity. Recent advances in light microscopy and image analysis techniques have opened the way toward a full description of spine nanostructure. In addition, large datasets of spine nanostructure can be effectively analyzed using machine learning techniques and other mathematical approaches, and recent advances in super-resolution imaging allow researchers to analyze spine structure at an unprecedented level of precision. This review summarizes computational methods that can effectively identify, segment and quantitate dendritic spines in either 2D or 3D imaging. Nanoscale analysis of spine structure and dynamics, combined with new mathematical approaches, will facilitate our understanding of spine functions in physiological and pathological conditions.",2020-07-01,5,1321,103,311
1859,32706688,Role of Artificial Intelligence in Patient Safety Outcomes: Systematic Literature Review,"Background:                    Artificial intelligence (AI) provides opportunities to identify the health risks of patients and thus influence patient safety outcomes.              Objective:                    The purpose of this systematic literature review was to identify and analyze quantitative studies utilizing or integrating AI to address and report clinical-level patient safety outcomes.              Methods:                    We restricted our search to the PubMed, PubMed Central, and Web of Science databases to retrieve research articles published in English between January 2009 and August 2019. We focused on quantitative studies that reported positive, negative, or intermediate changes in patient safety outcomes using AI apps, specifically those based on machine-learning algorithms and natural language processing. Quantitative studies reporting only AI performance but not its influence on patient safety outcomes were excluded from further review.              Results:                    We identified 53 eligible studies, which were summarized concerning their patient safety subcategories, the most frequently used AI, and reported performance metrics. Recognized safety subcategories were clinical alarms (n=9; mainly based on decision tree models), clinical reports (n=21; based on support vector machine models), and drug safety (n=23; mainly based on decision tree models). Analysis of these 53 studies also identified two essential findings: (1) the lack of a standardized benchmark and (2) heterogeneity in AI reporting.              Conclusions:                    This systematic review indicates that AI-enabled decision support systems, when implemented correctly, can aid in enhancing patient safety by improving error detection, patient stratification, and drug management. Future work is still needed for robust validation of these systems in prospective and real-world clinical environments to understand how well AI can predict safety outcomes in health care settings.",2020-07-01,3,2011,88,311
1497,32255249,Harnessing data science to advance radiation oncology,"Radiation oncology, a major treatment modality in the care of patients with malignant disease, is a technology- and computer-intensive medical specialty. As such, it should lend itself ideally to data science methods, where computer science, statistics, and clinical knowledge are combined to advance state-of-the-art care. Nevertheless, data science methods in radiation oncology research are still in their infancy and successful applications leading to improved patient care remain scarce. Here, we discuss data interoperability issues within and across organizational boundaries that hamper the introduction of big data and data science techniques in radiation oncology. At the semantic level, creating common underlying models and codification of the data, including the use of data elements with standardized definitions, an ontology, remains a work in progress. Methodological issues in data science and in the use of large population-based health data registries are identified. We show that data science methods and big data cannot replace randomized clinical trials in comparative effectiveness research by reviewing a series of instances where the outcomes of big data analyses and randomized trials are at odds. We also discuss the modern wave of machine learning and artificial intelligence as represented by deep learning and convolutional neural networks. Finally, we identify promising research avenues and remain optimistic that the data sources in radiation oncology can be linked to yield important insights in the near future. We argue that data science will be a valuable complement to, but not a replacement of, the traditional hypothesis-driven translational research chain and the randomized clinical trials that form the backbone of evidence-based medicine.",2020-07-01,2,1782,53,311
1546,32187757,"Discovering Intermetallics Through Synthesis, Computation, and Data-Driven Analysis","Intermetallics adopt an array of crystal structures, boast diverse chemical compositions, and possess exotic physical properties that have led to a wide range of applications from the biomedical to aerospace industries. Despite a long history of intermetallic synthesis and crystal structure analysis, identifying new intermetallic phases has remained challenging due to the prolonged nature of experimental phase space searching or the need for fortuitous discovery. In this Minireview, new approaches that build on the traditional methods for materials synthesis and characterization are discussed with a specific focus on realizing novel intermetallics. Indeed, advances in the computational modeling of solids using density functional theory in combination with structure prediction algorithms have led to new high-pressure phases, functional intermetallics, and aided experimental efforts. Furthermore, the advent of data-centered methodologies has provided new opportunities to rapidly predict crystal structures, physical properties, and the existence of unknown compounds. Describing the research results for each of these examples in depth while also highlighting the numerous opportunities to merge traditional intermetallic synthesis and characterization with computation and informatics provides insight that is essential to advance the discovery of metal-rich solids.",2020-07-01,0,1380,83,311
1501,32250986,Can machine-learning methods really help predict suicide?,"Purpose of review:                    In recent years there has been interest in the use of machine learning in suicide research in reaction to the failure of traditional statistical methods to produce clinically useful models of future suicide. The current review summarizes recent prediction studies in the suicide literature including those using machine learning approaches to understand what value these novel approaches add.              Recent findings:                    Studies using machine learning to predict suicide deaths report area under the curve that are only modestly greater than, and sensitivities that are equal to, those reported in studies using more conventional predictive methods. Positive predictive value remains around 1% among the cohort studies with a base rate that was not inflated by case-control methodology.              Summary:                    Machine learning or artificial intelligence may afford opportunities in mental health research and in the clinical care of suicidal patients. However, application of such techniques should be carefully considered to avoid repeating the mistakes of existing methodologies. Prediction studies using machine-learning methods have yet to make a major contribution to our understanding of the field and are unproven as clinically useful tools.",2020-07-01,3,1325,57,311
1539,32201044,Disentangling Heterogeneity in Alzheimer's Disease and Related Dementias Using Data-Driven Methods,"Brain aging is a complex process that includes atrophy, vascular injury, and a variety of age-associated neurodegenerative pathologies, together determining an individual's course of cognitive decline. While Alzheimer's disease and related dementias contribute to the heterogeneity of brain aging, these conditions themselves are also heterogeneous in their clinical presentation, progression, and pattern of neural injury. We reviewed studies that leveraged data-driven approaches to examining heterogeneity in Alzheimer's disease and related dementias, with a principal focus on neuroimaging studies exploring subtypes of regional neurodegeneration patterns. Over the past decade, the steadily increasing wealth of clinical, neuroimaging, and molecular biomarker information collected within large-scale observational cohort studies has allowed for a richer understanding of the variability of disease expression within the aging and Alzheimer's disease and related dementias continuum. Moreover, the availability of these large-scale datasets has supported the development and increasing application of clustering techniques for studying disease heterogeneity in a data-driven manner. In particular, data-driven studies have led to new discoveries of previously unappreciated disease subtypes characterized by distinct neuroimaging patterns of regional neurodegeneration, which are paralleled by heterogeneous profiles of pathological, clinical, and molecular biomarker characteristics. Incorporating these findings into novel frameworks for more differentiated disease stratification holds great promise for improving individualized diagnosis and prognosis of expected clinical progression, and provides opportunities for development of precision medicine approaches for therapeutic intervention. We conclude with an account of the principal challenges associated with data-driven heterogeneity analyses and outline avenues for future developments in the field.",2020-07-01,7,1965,98,311
1406,32665978,Artificial intelligence to improve back pain outcomes and lessons learnt from clinical classification approaches: three systematic reviews,"Artificial intelligence and machine learning (AI/ML) could enhance the ability to detect patterns of clinical characteristics in low-back pain (LBP) and guide treatment. We conducted three systematic reviews to address the following aims: (a) review the status of AI/ML research in LBP, (b) compare its status to that of two established LBP classification systems (STarT Back, McKenzie). AI/ML in LBP is in its infancy: 45 of 48 studies assessed sample sizes <1000 people, 19 of 48 studies used 5 parameters in models, 13 of 48 studies applied multiple models and attained high accuracy, 25 of 48 studies assessed the binary classification of LBP versus no-LBP only. Beyond the 48 studies using AI/ML for LBP classification, no studies examined use of AI/ML in prognosis prediction of specific sub-groups, and AI/ML techniques are yet to be implemented in guiding LBP treatment. In contrast, the STarT Back tool has been assessed for internal consistency, test-retest reliability, validity, pain and disability prognosis, and influence on pain and disability treatment outcomes. McKenzie has been assessed for inter- and intra-tester reliability, prognosis, and impact on pain and disability outcomes relative to other treatments. For AI/ML methods to contribute to the refinement of LBP (sub-)classification and guide treatment allocation, large data sets containing known and exploratory clinical features should be examined. There is also a need to establish reliability, validity, and prognostic capacity of AI/ML techniques in LBP as well as its ability to inform treatment allocation for improved patient outcomes and/or reduced healthcare costs.",2020-07-01,2,1653,138,311
2668,32437744,Artificial intelligence and neuropsychological measures: The case of Alzheimer's disease,"One of the current challenges in the field of Alzheimer's disease (AD) is to identify patients with mild cognitive impairment (MCI) that will convert to AD. Artificial intelligence, in particular machine learning (ML), has established as one of more powerful approach to extract reliable predictors and to automatically classify different AD phenotypes. It is time to accelerate the translation of this knowledge in clinical practice, mainly by using low-cost features originating from the neuropsychological assessment. We performed a meta-analysis to assess the contribution of ML and neuropsychological measures for the automated classification of MCI patients and the prediction of their conversion to AD. The pooled sensitivity and specificity of patients' classifications was obtained by means of a quantitative bivariate random-effect meta-analytic approach. Although a high heterogeneity was observed, the results of meta-analysis show that ML applied to neuropsychological measures can lead to a successful automatic classification, being more specific as screening rather than prognosis tool. Relevant categories of neuropsychological tests can be extracted by ML that maximize the classification accuracy.",2020-07-01,0,1216,88,311
75,32311376,Computational predictive approaches for interaction and structure of aptamers,"Aptamers are short single-strand sequences that can bind to their specific targets with high affinity and specificity. Usually, aptamers are selected experimentally via systematic evolution of ligands by exponential enrichment (SELEX), an evolutionary process that consists of multiple cycles of selection and amplification. The SELEX process is expensive, time-consuming, and its success rates are relatively low. To overcome these difficulties, in recent years, several computational techniques have been developed in aptamer sciences that bring together different disciplines and branches of technologies. In this paper, a complementary review on computational predictive approaches of the aptamer has been organized. Generally, the computational prediction approaches of aptamer have been proposed to carry out in two main categories: interaction-based prediction and structure-based predictions. Furthermore, the available software packages and toolkits in this scope were reviewed. The aim of describing computational methods and tools in aptamer science is that aptamer scientists might take advantage of these computational techniques to develop more accurate and more sensitive aptamers.",2020-07-01,3,1196,77,311
2666,32439080,Risk Stratification Strategies for Colorectal Cancer Screening: From Logistic Regression to Artificial Intelligence,"Risk stratification is a system by which clinically meaningful separation of risk is achieved in a group of otherwise similar persons. Although parametric logistic regression dominates risk prediction, use of nonparametric and semiparametric methods, including artificial neural networks, is increasing. These statistical-learning and machine-learning methods, along with simple rules, are collectively referred to as ""artificial intelligence"" (AI). AI requires knowledge of study validity, understanding of model metrics, and determination of whether and to what extent the model can and should be applied to the patient or population under consideration. Further investigation is needed, especially in model validation and impact assessment.",2020-07-01,0,743,115,311
888,31282778,Iterative processes: a review of semi-supervised machine learning in rehabilitation science,"Purpose: To define semi-supervised machine learning (SSML) and explore current and potential applications of this analytic strategy in rehabilitation research.Method: We conducted a scoping review using PubMed, GoogleScholar and Medline. Studies were included if they: (1) described a semi-supervised approach to apply machine learning algorithms during data analysis and (2) examined constructs encompassed by the International Classification of Functioning, Disability and Health (ICF). The first two authors reviewed identified articles and recorded study and participant characteristics. The ICF domain used in each study was also identified.Results: After combining information from the eight studies, we established that SSML was a feasible approach for analysis of complex data in rehabilitation research. We also determined that semi-supervised approaches may be more accurate than supervised machine learning approaches.Conclusions: A semi-supervised approach to machine learning has potential to enhance our understanding of complex data sets in rehabilitation science. SSML mirrors the iterative process of rehabilitation, making this approach ideal for calibrating devices, classifying activities or identifying just-in-time interventions. Rehabilitation scientists who are interested in conducting SSML should collaborate with data scientists to advance the application of this approach within our field.Implications for rehabilitationSemi-supervised machine learning applications may be a feasible approach for analyses of complex data sets in rehabilitation research.Semi-supervised machine learning approaches uses a combination of labelled and unlabelled data to produce accurate predictive models, thereby requiring less user-input data than other machine learning approaches (i.e., supervised, unsupervised), reducing resource cost and user-burden.Semi-supervised machine learning is an iterative process that, when applied to rehabilitation assessment and outcomes, could produce accurate personalized models for treatment.Rehabilitation researchers and data scientists should collaborate to implement semi-supervised machine learning approaches in rehabilitation research, optimizing the power of large datasets that are becoming more readily available within the field (e.g., EEG signals, sensors, smarthomes).",2020-07-01,0,2332,91,311
1407,32664491,Prevention of Prosthetic Joint Infection: From Traditional Approaches towards Quality Improvement and Data Mining,"A projected increased use of total joint arthroplasties will naturally result in a related increase in the number of prosthetic joint infections (PJIs). Suppression of the local peri-implant immune response counters efforts to eradicate bacteria, allowing the formation of biofilms and compromising preventive measures taken in the operating room. For these reasons, the prevention of PJI should focus concurrently on the following targets: (i) identifying at-risk patients; (ii) reducing ""bacterial load"" perioperatively; (iii) creating an antibacterial/antibiofilm environment at the site of surgery; and (iv) stimulating the local immune response. Despite considerable recent progress made in experimental and clinical research, a large discrepancy persists between proposed and clinically implemented preventative strategies. The ultimate anti-infective strategy lies in an optimal combination of all preventative approaches into a single ""clinical pack"", applied rigorously in all settings involving prosthetic joint implantation. In addition, ""anti-infective"" implants might be a choice in patients who have an increased risk for PJI. However, further progress in the prevention of PJI is not imaginable without a close commitment to using quality improvement tools in combination with continual data mining, reflecting the efficacy of the preventative strategy in a particular clinical setting.",2020-07-01,2,1401,113,311
79,32304429,Machine-learning approaches to substance-abuse research: emerging trends and their implications,"Purpose of review:                    To provide an accessible overview of some of the most recent trends in the application of machine learning to the field of substance use disorders and their implications for future research and practice.              Recent findings:                    Machine-learning (ML) techniques have recently been applied to substance use disorder (SUD) data for multiple predictive applications including detecting current abuse, assessing future risk and predicting treatment success. These models cover a wide range of machine-learning techniques and data types including physiological measures, longitudinal surveys, treatment outcomes, national surveys, medical records and social media.              Summary:                    The application of machine-learning models to substance use disorder data shows significant promise, with some use cases and data types showing high predictive accuracy, particularly for models of physiological and behavioral measures for predicting current substance use, portending potential clinical diagnostic applications; however, these results are uneven, with some models performing poorly or at chance, a limitation likely reflecting insufficient data and/or weak validation methods. The field will likely benefit from larger and more multimodal datasets, greater standardization of data recording and rigorous testing protocols as well as greater use of modern deep neural network models applied to multimodal unstructured datasets.",2020-07-01,0,1505,95,311
1405,32668634,Gallium Nitride (GaN) Nanostructures and Their Gas Sensing Properties: A Review,"In the last two decades, GaN nanostructures of various forms like nanowires (NWs), nanotubes (NTs), nanofibers (NFs), nanoparticles (NPs) and nanonetworks (NNs) have been reported for gas sensing applications. In this paper, we have reviewed our group's work and the works published by other groups on the advances in GaN nanostructures-based sensors for detection of gases such as hydrogen (H2), alcohols (R-OH), methane (CH4), benzene and its derivatives, nitric oxide (NO), nitrogen dioxide (NO2), sulfur-dioxide (SO2), ammonia (NH3), hydrogen sulfide (H2S) and carbon dioxide (CO2). The important sensing performance parameters like limit of detection, response/recovery time and operating temperature for different type of sensors have been summarized and tabulated to provide a thorough performance comparison. A novel metric, the product of response time and limit of detection, has been established, to quantify and compare the overall sensing performance of GaN nanostructure-based devices reported so far. According to this metric, it was found that the InGaN/GaN NW-based sensor exhibits superior overall sensing performance for H2 gas sensing, whereas the GaN/(TiO2-Pt) nanowire-nanoclusters (NWNCs)-based sensor is better for ethanol sensing. The GaN/TiO2 NWNC-based sensor is also well suited for TNT sensing. This paper has also reviewed density-functional theory (DFT)-based first principle studies on the interaction between gas molecules and GaN. The implementation of machine learning algorithms on GaN nanostructured sensors and sensor array has been analyzed as well. Finally, gas sensing mechanism on GaN nanostructure-based sensors at room temperature has been discussed.",2020-07-01,3,1694,79,311
93,32274856,Using Artificial Intelligence for Predicting Survival of Individual Grafts in Liver Transplantation: A Systematic Review,"The demand for liver transplantation far outstrips the supply of deceased donor organs, and so, listing and allocation decisions aim to maximize utility. Most existing methods for predicting transplant outcomes use basic methods, such as regression modeling, but newer artificial intelligence (AI) techniques have the potential to improve predictive accuracy. The aim was to perform a systematic review of studies predicting graft outcomes following deceased donor liver transplantation using AI techniques and to compare these findings to linear regression and standard predictive modeling: donor risk index (DRI), Model for End-Stage Liver Disease (MELD), and Survival Outcome Following Liver Transplantation (SOFT). After reviewing available article databases, a total of 52 articles were reviewed for inclusion. Of these articles, 9 met the inclusion criteria, which reported outcomes from 18,771 liver transplants. Artificial neural networks (ANNs) were the most commonly used methodology, being reported in 7 studies. Only 2 studies directly compared machine learning (ML) techniques to liver scoring modalities (i.e., DRI, SOFT, and balance of risk [BAR]). Both studies showed better prediction of individual organ survival with the optimal ANN model, reporting an area under the receiver operating characteristic curve (AUROC) 0.82 compared with BAR (0.62) and SOFT (0.57), and the other ANN model gave an AUC ROC of 0.84 compared with a DRI (0.68) and SOFT (0.64). AI techniques can provide high accuracy in predicting graft survival based on donors and recipient variables. When compared with the standard techniques, AI methods are dynamic and are able to be trained and validated within every population. However, the high accuracy of AI may come at a cost of losing explainability (to patients and clinicians) on how the technology works.",2020-07-01,2,1851,120,311
94,32272169,"Biohorology and biomarkers of aging: Current state-of-the-art, challenges and opportunities","The aging process results in multiple traceable footprints, which can be quantified and used to estimate an organism's age. Examples of such aging biomarkers include epigenetic changes, telomere attrition, and alterations in gene expression and metabolite concentrations. More than a dozen aging clocks use molecular features to predict an organism's age, each of them utilizing different data types and training procedures. Here, we offer a detailed comparison of existing mouse and human aging clocks, discuss their technological limitations and the underlying machine learning algorithms. We also discuss promising future directions of research in biohorology - the science of measuring the passage of time in living systems. Overall, we expect deep learning, deep neural networks and generative approaches to be the next power tools in this timely and actively developing field.",2020-07-01,6,882,91,311
97,32269329,Next-generation robotics in gastrointestinal surgery,"The global numbers of robotic gastrointestinal surgeries are increasing. However, the evidence base for robotic gastrointestinal surgery does not yet support its widespread adoption or justify its cost. The reasons for its continued popularity are complex, but a notable driver is the push for innovation - robotic surgery is seen as a compelling solution for delivering on the promise of minimally invasive precision surgery - and a changing commercial landscape delivers the promise of increased affordability. Novel systems will leverage the robot as a data-driven platform, integrating advances in imaging, artificial intelligence and machine learning for decision support. However, if this vision is to be realized, lessons must be heeded from current clinical trials and translational strategies, which have failed to demonstrate patient benefit. In this Perspective, we critically appraise current research to define the principles on which the next generation of gastrointestinal robotics trials should be based. We also discuss the emerging commercial landscape and define existing and new technologies.",2020-07-01,2,1112,52,311
98,32266827,The State of Data Science in Genomic Nursing,"Nurse scientists are generating, acquiring, distributing, processing, storing, and analyzing greater volumes of complex omics data than ever before. To take full advantage of big omics data, to address core biological questions, and to enhance patient care, however, genomic nurse scientists must embrace data science. Intended for readership with limited but expanding data science knowledge and skills, this article aims to provide a brief overview of the state of data science in genomic nursing. Our goal is to introduce key data science concepts to genomic nurses who participate at any stage of the data science lifecycle, from research patient recruitment to data wrangling, preprocessing, and analysis to implementation in clinical practice to policy creation. We address three major components in this review: (1) fundamental terminology for the field of genomic nursing data science, (2) current genomic nursing data science research exemplars, and (3) the spectrum of genomic nursing data science roles as well as education pathways and training opportunities. Links to helpful resources are included throughout the article.",2020-07-01,0,1135,44,311
2652,32475607,Artificial intelligence and machine learning in nephropathology,"Artificial intelligence (AI) for the purpose of this review is an umbrella term for technologies emulating a nephropathologist's ability to extract information on diagnosis, prognosis, and therapy responsiveness from native or transplant kidney biopsies. Although AI can be used to analyze a wide variety of biopsy-related data, this review focuses on whole slide images traditionally used in nephropathology. AI applications in nephropathology have recently become available through several advancing technologies, including (i) widespread introduction of glass slide scanners, (ii) data servers in pathology departments worldwide, and (iii) through greatly improved computer hardware to enable AI training. In this review, we explain how AI can enhance the reproducibility of nephropathology results for certain parameters in the context of precision medicine using advanced architectures, such as convolutional neural networks, that are currently the state of the art in machine learning software for this task. Because AI applications in nephropathology are still in their infancy, we show the power and potential of AI applications mostly in the example of oncopathology. Moreover, we discuss the technological obstacles as well as the current stakeholder and regulatory concerns about developing AI applications in nephropathology from the perspective of nephropathologists and the wider nephrology community. We expect the gradual introduction of these technologies into routine diagnostics and research for selective tasks, suggesting that this technology will enhance the performance of nephropathologists rather than making them redundant.",2020-07-01,3,1649,63,311
1400,32671229,Harnessing repeated measurements of predictor variables for clinical risk prediction: a review of existing methods,"Background:                    Clinical prediction models (CPMs) predict the risk of health outcomes for individual patients. The majority of existing CPMs only harness cross-sectional patient information. Incorporating repeated measurements, such as those stored in electronic health records, into CPMs may provide an opportunity to enhance their performance. However, the number and complexity of methodological approaches available could make it difficult for researchers to explore this opportunity. Our objective was to review the literature and summarise existing approaches for harnessing repeated measurements of predictor variables in CPMs, primarily to make this field more accessible for applied researchers.              Methods:                    MEDLINE, Embase and Web of Science were searched for articles reporting the development of a multivariable CPM for individual-level prediction of future binary or time-to-event outcomes and modelling repeated measurements of at least one predictor. Information was extracted on the following: the methodology used, its specific aim, reported advantages and limitations, and software available to apply the method.              Results:                    The search revealed 217 relevant articles. Seven methodological frameworks were identified: time-dependent covariate modelling, generalised estimating equations, landmark analysis, two-stage modelling, joint-modelling, trajectory classification and machine learning. Each of these frameworks satisfies at least one of three aims: to better represent the predictor-outcome relationship over time, to infer a covariate value at a pre-specified time and to account for the effect of covariate change.              Conclusions:                    The applicability of identified methods depends on the motivation for including longitudinal information and the method's compatibility with the clinical context and available patient data, for both model development and risk estimation in practice.",2020-07-01,1,2008,114,311
2039,33778721,Precision Digital Oncology: Emerging Role of Radiomics-based Biomarkers and Artificial Intelligence for Advanced Imaging and Characterization of Brain Tumors,"Advances in computerized image analysis and the use of artificial intelligence-based approaches for image-based analysis and construction of prediction algorithms represent a new era for noninvasive biomarker discovery. In recent literature, it has become apparent that radiologic images can serve as mineable databases that contain large amounts of quantitative features with potential clinical significance. Extraction and analysis of these quantitative features is commonly referred to as texture or radiomic analysis. Numerous studies have demonstrated applications for texture and radiomic characterization methods for assessing brain tumors to improve noninvasive predictions of tumor histologic characteristics, molecular profile, distinction of treatment-related changes, and prediction of patient survival. In this review, the current use and future potential of texture or radiomic-based approaches with machine learning for brain tumor image analysis and prediction algorithm construction will be discussed. This technology has the potential to advance the value of diagnostic imaging by extracting currently unused information on medical scans that enables more precise, personalized therapy; however, significant barriers must be overcome if this technology is to be successfully implemented on a wide scale for routine use in the clinical setting. Keywords: Adults and Pediatrics, Brain/Brain Stem, CNS, Computer Aided Diagnosis (CAD), Computer Applications-General (Informatics), Image Postprocessing, Informatics, Neural Networks, Neuro-Oncology, Oncology, Treatment Effects, Tumor Response Supplemental material is available for this article.  RSNA, 2020.",2020-07-01,0,1673,157,311
2036,33948244,Lessons and tips for designing a machine learning study using EHR data,"Machine learning (ML) provides the ability to examine massive datasets and uncover patterns within data without relying on a priori assumptions such as specific variable associations, linearity in relationships, or prespecified statistical interactions. However, the application of ML to healthcare data has been met with mixed results, especially when using administrative datasets such as the electronic health record. The black box nature of many ML algorithms contributes to an erroneous assumption that these algorithms can overcome major data issues inherent in large administrative healthcare data. As with other research endeavors, good data and analytic design is crucial to ML-based studies. In this paper, we will provide an overview of common misconceptions for ML, the corresponding truths, and suggestions for incorporating these methods into healthcare research while maintaining a sound study design.",2020-07-01,0,916,70,311
2034,32821854,Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities,"The machine learning community has become alert to the ways that predictive algorithms can inadvertently introduce unfairness in decision-making. Herein, we discuss how concepts of algorithmic fairness might apply in healthcare, where predictive algorithms are being increasingly used to support decision-making. Central to our discussion is the distinction between algorithmic fairness and algorithmic bias. Fairness concerns apply specifically when algorithms are used to support polar decisions (i.e., where one pole of prediction leads to decisions that are generally more desired than the other), such as when predictions are used to allocate scarce health care resources to a group of patients that could all benefit. We review different fairness criteria and demonstrate their mutual incompatibility. Even when models are used to balance benefits-harms to make optimal decisions for individuals (i.e., for non-polar decisions)-and fairness concerns are not germane-model, data or sampling issues can lead to biased predictions that support decisions that are differentially harmful/beneficial across groups. We review these potential sources of bias, and also discuss ways to diagnose and remedy algorithmic bias. We note that remedies for algorithmic fairness may be more problematic, since we lack agreed upon definitions of fairness. Finally, we propose a provisional framework for the evaluation of clinical prediction models offered for further elaboration and refinement. Given the proliferation of prediction models used to guide clinical decisions, developing consensus for how these concerns can be addressed should be prioritized.",2020-07-01,0,1647,127,311
2033,32821856,Digitizing clinical trials,"Clinical trials are a fundamental tool used to evaluate the efficacy and safety of new drugs and medical devices and other health system interventions. The traditional clinical trials system acts as a quality funnel for the development and implementation of new drugs, devices and health system interventions. The concept of a ""digital clinical trial"" involves leveraging digital technology to improve participant access, engagement, trial-related measurements, and/or interventions, enable concealed randomized intervention allocation, and has the potential to transform clinical trials and to lower their cost. In April 2019, the US National Institutes of Health (NIH) and the National Science Foundation (NSF) held a workshop bringing together experts in clinical trials, digital technology, and digital analytics to discuss strategies to implement the use of digital technologies in clinical trials while considering potential challenges. This position paper builds on this workshop to describe the current state of the art for digital clinical trials including (1) defining and outlining the composition and elements of digital trials; (2) describing recruitment and retention using digital technology; (3) outlining data collection elements including mobile health, wearable technologies, application programming interfaces (APIs), digital transmission of data, and consideration of regulatory oversight and guidance for data security, privacy, and remotely provided informed consent; (4) elucidating digital analytics and data science approaches leveraging artificial intelligence and machine learning algorithms; and (5) setting future priorities and strategies that should be addressed to successfully harness digital methods and the myriad benefits of such technologies for clinical research.",2020-07-01,3,1802,26,311
1236,32088032,Oocyte and embryo evaluation by AI and multi-spectral auto-fluorescence imaging: Livestock embryology needs to catch-up to clinical practice,"A highly accurate 'non-invasive quantitative embryo assessment for pregnancy' (NQEAP) technique that determines embryo quality has been an elusive goal. If developed, NQEAP would transform the selection of embryos from both Multiple Ovulation and Embryo Transfer (MOET), and even more so, in vitro produced (IVP) embryos for livestock breeding. The area where this concept is already having impact is in the field of clinical embryology, where great strides have been taken in the application of morphokinetics and artificial intelligence (AI); while both are already in practice, rigorous and robust evidence of efficacy is still required. Even the translation of advances in the qualitative scoring of human IVF embryos have yet to be translated to the livestock IVP industry, which remains dependent on the MOET-standardised 3-point scoring system. Furthermore, there are new ways to interrogate the biochemistry of individual embryonic cells by using new, light-based methodologies, such as FLIM and hyperspectral microscopy. Combinations of these technologies, in particular combining new imaging systems with AI, will lead to very accurate NQEAP predictive tools, improving embryo selection and recipient pregnancy success.",2020-07-01,1,1229,140,311
1069,31456318,Imaging signatures of glioblastoma molecular characteristics: A radiogenomics review,"Over the past few decades, the advent and development of genomic assessment methods and computational approaches have raised the hopes for identifying therapeutic targets that may aid in the treatment of glioblastoma. However, the targeted therapies have barely been successful in their effort to cure glioblastoma patients, leaving them with a grim prognosis. Glioblastoma exhibits high heterogeneity, both spatially and temporally. The existence of different genetic subpopulations in glioblastoma allows this tumor to adapt itself to environmental forces. Therefore, patients with glioblastoma respond poorly to the prescribed therapies, as treatments are directed towards the whole tumor and not to the specific genetic subregions. Genomic alterations within the tumor develop distinct radiographic phenotypes. In this regard, MRI plays a key role in characterizing molecular signatures of glioblastoma, based on regional variations and phenotypic presentation of the tumor. Radiogenomics has emerged as a (relatively) new field of research to explore the connections between genetic alterations and imaging features. Radiogenomics offers numerous advantages, including noninvasive and global assessment of the tumor and its response to therapies. In this review, we summarize the potential role of radiogenomic techniques to stratify patients according to their specific tumor characteristics with the goal of designing patient-specific therapies. Level of Evidence: 5 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;52:54-69.",2020-07-01,7,1540,84,311
1014,31833636,Density Functional Theory as a Data Science,"The development of density functional theory (DFT) functionals and physical corrections are reviewed focusing on the physical meanings and the semiempirical parameters from the viewpoint of data science. This review shows that DFT exchange-correlation functionals have been developed under many strict physical conditions with minimizing the number of the semiempirical parameters, except for some recent functionals. Major physical corrections for exchange-correlation function- als are also shown to have clear physical meanings independent of the functionals, though they inevitably require minimum semiempirical parameters dependent on the functionals combined. We, therefore, interpret that DFT functionals with physical corrections are the most sophisticated target functions that are physically legitimated, even from the viewpoint of data science.",2020-07-01,0,855,43,311
1994,32903956,Computer-Aided Diagnosis Systems in Diagnosing Malignant Thyroid Nodules on Ultrasonography: A Systematic Review and Meta-Analysis,"Background:                    Computer-aided diagnosis (CAD) systems are being applied to the ultrasonographic diagnosis of malignant thyroid nodules, but it remains controversial whether the systems add any accuracy for radiologists.              Objective:                    To determine the accuracy of CAD systems in diagnosing malignant thyroid nodules.              Methods:                    PubMed, EMBASE, and the Cochrane Library were searched for studies on the diagnostic performance of CAD systems. The diagnostic performance was assessed by pooled sensitivity and specificity, and their accuracy was compared with that of radiologists. The present systematic review was registered in PROSPERO (CRD42019134460).              Results:                    Nineteen studies with 4,781 thyroid nodules were included. Both the classic machine learning- and the deep learning-based CAD system had good performance in diagnosing malignant thyroid nodules (classic machine learning: sensitivity 0.86 [95% CI 0.79-0.92], specificity 0.85 [95% CI 0.77-0.91], diagnostic odds ratio (DOR) 37.41 [95% CI 24.91-56.20]; deep learning: sensitivity 0.89 [95% CI 0.81-0.93], specificity 0.84 [95% CI 0.75-0.90], DOR 40.87 [95% CI 18.13-92.13]). The diagnostic performance of the deep learning-based CAD system was comparable to that of the radiologists (sensitivity 0.87 [95% CI 0.78-0.93] vs. 0.87 [95% CI 0.85-0.89], specificity 0.85 [95% CI 0.76-0.91] vs. 0.87 [95% CI 0.81-0.91], DOR 40.12 [95% CI 15.58-103.33] vs. DOR 44.88 [95% CI 30.71-65.57]).              Conclusions:                    The CAD systems demonstrated good performance in diagnosing malignant thyroid nodules. However, experienced radiologists may still have an advantage over CAD systems during real-time diagnosis.",2020-07-01,2,1788,130,311
963,31917651,Are We Meeting the Promise of Endotypes and Precision Medicine in Asthma?,"While the term asthma has long been known to describe heterogeneous groupings of patients, only recently have data evolved which enable a molecular understanding of the clinical differences. The evolution of transcriptomics (and other 'omics platforms) and improved statistical analyses in combination with large clinical cohorts opened the door for molecular characterization of pathobiologic processes associated with a range of asthma patients. When linked with data from animal models and clinical trials of targeted biologic therapies, emerging distinctions arose between patients with and without elevations in type 2 immune and inflammatory pathways, leading to the confirmation of a broad categorization of type 2-Hi asthma. Differences in the ratios, sources, and location of type 2 cytokines and their relation to additional immune pathway activation appear to distinguish several different (sub)molecular phenotypes, and perhaps endotypes of type 2-Hi asthma, which respond differently to broad and targeted anti-inflammatory therapies. Asthma in the absence of type 2 inflammation is much less well defined, without clear biomarkers, but is generally linked with poor responses to corticosteroids. Integration of ""big data"" from large cohorts, over time, using machine learning approaches, combined with validation and iterative learning in animal (and human) model systems is needed to identify the biomarkers and tightly defined molecular phenotypes/endotypes required to fulfill the promise of precision medicine.",2020-07-01,5,1528,73,311
960,31922268,Machine intelligence in peptide therapeutics: A next-generation tool for rapid disease screening,"Discovery and development of biopeptides are time-consuming, laborious, and dependent on various factors. Data-driven computational methods, especially machine learning (ML) approach, can rapidly and efficiently predict the utility of therapeutic peptides. ML methods offer an array of tools that can accelerate and enhance decision making and discovery for well-defined queries with ample and sophisticated data quality. Various ML approaches, such as support vector machines, random forest, extremely randomized tree, and more recently deep learning methods, are useful in peptide-based drug discovery. These approaches leverage the peptide data sets, created via high-throughput sequencing and computational methods, and enable the prediction of functional peptides with increased levels of accuracy. The use of ML approaches in the development of peptide-based therapeutics is relatively recent; however, these techniques are already revolutionizing protein research by unraveling their novel therapeutic peptide functions. In this review, we discuss several ML-based state-of-the-art peptide-prediction tools and compare these methods in terms of their algorithms, feature encodings, prediction scores, evaluation methodologies, and software utilities. We also assessed the prediction performance of these methods using well-constructed independent data sets. In addition, we discuss the common pitfalls and challenges of using ML approaches for peptide therapeutics. Overall, we show that using ML models in peptide research can streamline the development of targeted peptide therapies.",2020-07-01,28,1592,96,311
2680,32418724,Emergence of New Disease: How Can Artificial Intelligence Help?,"Emergence of new disease remains a critical parameter in human health and society. Advances in artificial intelligence (AI) allow for rapid processing and analysis of massive and complex data. In this forum article, the recent applications across disease prediction and drug development in relation to the COVID-19 pandemic are reviewed.",2020-07-01,4,337,63,311
1418,32648130,Heart Failure with Preserved Ejection Fraction-a Concise Review,"Purpose of review:                    Heart failure with preserved ejection fraction (HFpEF) is a relatively new disease entity used in medical terminology; however, both the number of patients and its clinical significance are growing. HFpEF used to be seen as a mild condition; however, the symptoms and quality of life of the patients are comparable to those with reduced ejection fraction. The disease is much more complex than previously thought. In this article, information surrounding the etiology, diagnosis, prognosis, and possible therapeutic options of HFpEF are reviewed and summarized.              Recent findings:                    It has recently been proposed that heart failure (HF) is rather a heterogeneous syndrome with a spectrum of overlapping and distinct characteristics. HFpEF itself can be distilled into different phenotypes based on the underlying biology. The etiological factors of HFpEF are unclear; however, systemic low-grade inflammation and microvascular damage as a consequence of comorbidities associated with endothelial dysfunction, oxidative stress, myocardial remodeling, and fibrosis are considered to play a crucial role in the pathogenesis of a disease. The H2FPEF score and the HFpEF nomogram are recently validated highly sensitive tools employed for risk assessment of subclinical heart failure. Despite numerous studies, there is still no evidence-based pharmacotherapy for HFpEF and the mortality and morbidity associated with HFpEF remain high. A better understanding of the etiological factors, the impact of comorbidities, the phenotypes of the disease, and implementation of machine learning algorithms may play a key role in the development of future therapeutic strategies.",2020-07-01,2,1731,63,311
2019,32836901,Time-series forecasting of Bitcoin prices using high-dimensional features: a machine learning approach,"Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65% accuracy for next-day forecast and scoring from 62 to 64% accuracy for seventh-ninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44%, while it varies from 2.88 to 4.10% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.",2020-07-01,1,1287,102,311
1419,32648059,Radiomics in Echocardiography: Deep Learning and Echocardiographic Analysis,"Purpose of review:                    Recent development in artificial intelligence (AI) for cardiovascular imaging analysis, involving deep learning, is the start of a new phase in the research field. We review the current state of AI in cardiovascular field and discuss about its potential to improve clinical workflows and accuracy of diagnosis.              Recent findings:                    In the AI cardiovascular imaging field, there are many applications involving efficient image reconstruction, patient triage, and support for clinical decisions. These tools have a role to support repetitive clinical tasks. Although they will be powerful in some situations, these applications may have new potential in the hands of echo cardiologists, assisting but not replacing the human observer. We believe AI has the potential to improve the quality of echocardiography. Someday AI may be incorporated into the daily clinical setting, being an instrumental tool for cardiologists dealing with cardiovascular diseases.",2020-07-01,0,1021,75,311
2189,31665258,Differential epigenetic factors in the prediction of cardiovascular risk in diabetic patients,"Hyperglycaemia can strongly alter the epigenetic signatures in many types of human vascular cells providing persistent perturbations of protein-protein interactions both in micro- and macro-domains. The establishment of these epigenetic changes may precede cardiovascular (CV) complications and help us to predict vascular lesions in diabetic patients. Importantly, these epigenetic marks may be transmitted across several generations (transgenerational effect) and increase the individual risk of disease. Aberrant DNA methylation and imbalance of histone modifications, mainly acetylation and methylation of H3, represent key determinants of vascular lesions and, thus, putative useful biomarkers for prevention and diagnosis of CV risk in diabetics. Moreover, a differential expression of some micro-RNAs (miRNAs), mainly miR-126, may be a useful prognostic biomarker for atherosclerosis development in asymptomatic subjects. Recently, also environmental-induced chemical perturbations in mRNA (epitranscriptome), mainly the N6-methyladenosine, have been associated with obesity and diabetes. Importantly, reversal of epigenetic changes by modulation of lifestyle and use of metformin, statins, fenofibrate, and apabetalone may offer useful therapeutic options to prevent or delay CV events in diabetics increasing the opportunity for personalized therapy. Network medicine is a promising molecular-bioinformatic approach to identify the signalling pathways underlying the pathogenesis of CV lesions in diabetic patients. Moreover, machine learning tools combined with tomography are advancing the individualized assessment of CV risk in these patients. We remark the need for combining epigenetics and advanced bioinformatic platforms to improve the prediction of vascular lesions in diabetics increasing the opportunity for CV precision medicine.",2020-07-01,8,1851,93,311
72,32315260,Artificial Intelligence in Dentistry: Chances and Challenges,"The term ""artificial intelligence"" (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which ""learns"" intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (""explainable AI""). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.",2020-07-01,4,2184,60,311
2607,32558902,Automated video monitoring of insect pollinators in the field,"Ecosystems are at increasing risk from the global pollination crisis. Gaining better knowledge about pollinators and their interactions with plants is an urgent need. However, conventional methods of manually recording pollinator activity in the field can be time- and cost-consuming in terms of labour. Field-deployable video recording systems have become more common in ecological studies as they enable the capture of plant-insect interactions in fine detail. Standard video recording can be effective, although there are issues with hardware reliability under field-conditions (e.g. weatherproofing), and reviewing raw video manually is a time-consuming task. Automated video monitoring systems based on motion detection partly overcome these issues by only recording when activity occurs hence reducing the time needed to review footage during post-processing. Another advantage of these systems is that the hardware has relatively low power requirements. A few systems have been tested in the field which permit the collection of large datasets. Compared with other systems, automated monitoring allows vast increases in sampling at broad spatiotemporal scales. Some tools such as post-recording computer vision software and data-import scripts exist, further reducing users' time spent processing and analysing the data. Integrated computer vision and automated species recognition using machine learning models have great potential to further the study of pollinators in the field. Together, it is predicted that future advances in technology-based field monitoring methods will contribute significantly to understanding the causes underpinning pollinator declines and, hence, developing effective solutions for dealing with this global challenge.",2020-07-01,1,1755,61,311
2613,32541594,Explainable AI (xAI) for Anatomic Pathology,"Pathologists are adopting whole slide images (WSIs) for diagnosis, thanks to recent FDA approval of WSI systems as class II medical devices. In response to new market forces and recent technology advances outside of pathology, a new field of computational pathology has emerged that applies artificial intelligence (AI) and machine learning algorithms to WSIs. Computational pathology has great potential for augmenting pathologists' accuracy and efficiency, but there are important concerns regarding trust of AI due to the opaque, black-box nature of most AI algorithms. In addition, there is a lack of consensus on how pathologists should incorporate computational pathology systems into their workflow. To address these concerns, building computational pathology systems with explainable AI (xAI) mechanisms is a powerful and transparent alternative to black-box AI models. xAI can reveal underlying causes for its decisions; this is intended to promote safety and reliability of AI for critical tasks such as pathology diagnosis. This article outlines xAI enabled applications in anatomic pathology workflow that improves efficiency and accuracy of the practice. In addition, we describe HistoMapr-Breast, an initial xAI enabled software application for breast core biopsies. HistoMapr-Breast automatically previews breast core WSIs and recognizes the regions of interest to rapidly present the key diagnostic areas in an interactive and explainable manner. We anticipate xAI will ultimately serve pathologists as an interactive computational guide for computer-assisted primary diagnosis.",2020-07-01,2,1594,43,311
2620,32525020,A survey on single and multi omics data mining methods in cancer data classification,"Data analytics is routinely used to support biomedical research in all areas, with particular focus on the most relevant clinical conditions, such as cancer. Bioinformatics approaches, in particular, have been used to characterize the molecular aspects of diseases. In recent years, numerous studies have been performed on cancer based upon single and multi-omics data. For example, Single-omics-based studies have employed a diverse set of data, such as gene expression, DNA methylation, or miRNA, to name only a few instances. Despite that, a significant part of literature reports studies on gene expression with microarray datasets. Single-omics data have high numbers of attributes and very low sample counts. This characteristic makes them paradigmatic of an under-sampled, small-n large-p machine learning problem. An important goal of single-omics data analysis is to find the most relevant genes, in terms of their potential use in clinics and research, in the batch of available data. This problem has been addressed in gene selection as one of the pre-processing steps in data mining. An analysis that use only one type of data (single-omics) often miss the complexity of the landscape of molecular phenomena underlying the disease. As a result, they provide limited and sometimes poorly reliable information about the disease mechanisms. Therefore, in recent years, researchers have been eager to build models that are more complex, obtaining more reliable results using multi-omics data. However, to achieve this, the most important challenge is data integration. In this paper, we provide a comprehensive overview of the challenges in single and multi-omics data analysis of cancer data, focusing on gene selection and data integration methods.",2020-07-01,0,1758,84,311
33,32361380,Application of artificial intelligence in cardiac CT: From basics to clinical practice,"Research into the possibilities of AI in cardiac CT has been growing rapidly in the last decade. With the rise of publicly available databases and AI algorithms, many researchers and clinicians have started investigations into the use of AI in the clinical workflow. This review is a comprehensive overview on the types of tasks and applications in which AI can aid the clinician in cardiac CT, and can be used as a primer for medical researchers starting in the field of AI. The applications of AI algorithms are explained and recent examples in cardiac CT of these algorithms are further elaborated on. The critical factors for implementation in the future are discussed.",2020-07-01,1,673,86,311
2184,31675466,"Simulation vs. Understanding: A Tension, in Quantum Chemistry and Beyond. Part A. Stage Setting","We begin our tripartite Essay with a triangle of understanding, theory and simulation. Sketching the intimate tie between explanation and teaching, we also point to the emotional impact of understanding. As we trace the development of theory in chemistry, Dirac's characterization of what is known and what is needed for theoretical chemistry comes up, as does the role of prediction, and Thom's phrase ""To predict is not to explain."" We give a typology of models, and then describe, no doubt inadequately, machine learning and neural networks. In the second part, we leave philosophy, beginning by describing Roald's being beaten by simulation. This leads us to artificial intelligence (AI), Searle's Chinese room, and Strevens' account of what a go-playing program knows. Back to our terrain-we ask ""Quantum Chemistry,  ca. 2020?"" Then move to examples of AI affecting social matters, ranging from trivial to scary. We argue that moral decisions are hardly to be left to a computer. At this point, we try to pull the reader up, giving the opposing view of an optimistic, limitless future a voice. But we don't do justice to that view-how could we? We return to questioning the ascetic dimension of scientists, their romance with black boxes. Onward: In the 3rd part of this Essay, we work our way up from pessimism. We trace (another triangle!) the special interests of experimentalists, who want the theory we love, and reliable numbers as well. We detail in our own science instances where theory gave us real joy. Two more examples-on magnetic coupling in inorganic diradicals, and the way to think about alkali metal halides, show us the way to integrate simulation with theory. Back and forth is how it should be-between painfully-obtained, intriguing numbers, begging for interpretation, in turn requiring new concepts, new models, new theoretically grounded tools of computation. Through such iterations understanding is formed. As our tripartite Essay ends, we outline a future of consilience, with a role both for fact-seekers, and searchers for understanding. Chemistry's streak of creation provides in that conjoined future a passage to art and to perceiving, as we argue we must, the sacred in science.",2020-07-01,0,2217,95,311
1432,32635375,When I Look into Your Eyes: A Survey on Computer Vision Contributions for Human Gaze Estimation and Tracking,"The automatic detection of eye positions, their temporal consistency, and their mapping into a line of sight in the real world (to find where a person is looking at) is reported in the scientific literature as gaze tracking. This has become a very hot topic in the field of computer vision during the last decades, with a surprising and continuously growing number of application fields. A very long journey has been made from the first pioneering works, and this continuous search for more accurate solutions process has been further boosted in the last decade when deep neural networks have revolutionized the whole machine learning area, and gaze tracking as well. In this arena, it is being increasingly useful to find guidance through survey/review articles collecting most relevant works and putting clear pros and cons of existing techniques, also by introducing a precise taxonomy. This kind of manuscripts allows researchers and technicians to choose the better way to move towards their application or scientific goals. In the literature, there exist holistic and specifically technological survey documents (even if not updated), but, unfortunately, there is not an overview discussing how the great advancements in computer vision have impacted gaze tracking. Thus, this work represents an attempt to fill this gap, also introducing a wider point of view that brings to a new taxonomy (extending the consolidated ones) by considering gaze tracking as a more exhaustive task that aims at estimating gaze target from different perspectives: from the eye of the beholder (first-person view), from an external camera framing the beholder's, from a third-person view looking at the scene where the beholder is placed in, and from an external view independent from the beholder.",2020-07-01,2,1784,108,311
2154,31748800,Promises and Perils of Artificial Intelligence in Neurosurgery,"Artificial intelligence (AI)-facilitated clinical automation is expected to become increasingly prevalent in the near future. AI techniques may permit rapid and detailed analysis of the large quantities of clinical data generated in modern healthcare settings, at a level that is otherwise impossible by humans. Subsequently, AI may enhance clinical practice by pushing the limits of diagnostics, clinical decision making, and prognostication. Moreover, if combined with surgical robotics and other surgical adjuncts such as image guidance, AI may find its way into the operating room and permit more accurate interventions, with fewer errors. Despite the considerable hype surrounding the impending medical AI revolution, little has been written about potential downsides to increasing clinical automation. These may include both direct and indirect consequences. Directly, faulty, inadequately trained, or poorly understood algorithms may produce erroneous results, which may have wide-scale impact. Indirectly, increasing use of automation may exacerbate de-skilling of human physicians due to over-reliance, poor understanding, overconfidence, and lack of necessary vigilance of an automated clinical workflow. Many of these negative phenomena have already been witnessed in other industries that have already undergone, or are undergoing ""automation revolutions,"" namely commercial aviation and the automotive industry. This narrative review explores the potential benefits and consequences of the anticipated medical AI revolution from a neurosurgical perspective.",2020-07-01,0,1570,62,311
2066,33431024,"A review of computational drug repositioning: strategies, approaches, opportunities, challenges, and directions","Drug repositioning is the process of identifying novel therapeutic potentials for existing drugs and discovering therapies for untreated diseases. Drug repositioning, therefore, plays an important role in optimizing the pre-clinical process of developing novel drugs by saving time and cost compared to the traditional de novo drug discovery processes. Since drug repositioning relies on data for existing drugs and diseases the enormous growth of publicly available large-scale biological, biomedical, and electronic health-related data along with the high-performance computing capabilities have accelerated the development of computational drug repositioning approaches. Multidisciplinary researchers and scientists have carried out numerous attempts, with different degrees of efficiency and success, to computationally study the potential of repositioning drugs to identify alternative drug indications. This study reviews recent advancements in the field of computational drug repositioning. First, we highlight different drug repositioning strategies and provide an overview of frequently used resources. Second, we summarize computational approaches that are extensively used in drug repositioning studies. Third, we present different computing and experimental models to validate computational methods. Fourth, we address prospective opportunities, including a few target areas. Finally, we discuss challenges and limitations encountered in computational drug repositioning and conclude with an outline of further research directions.",2020-07-01,4,1543,111,311
1437,32632670,Artificial Intelligence and Myocardial Contrast Enhancement Pattern,"Purpose of review:                    Machine learning (ML) and deep learning (DL) are two important categories of AI algorithms. Nowadays, AI technology has been gradually applied to cardiac magnetic resonance imaging (CMRI), covering the fields of myocardial contrast enhancement (MCE) pattern and automatic ventricular segmentation. This paper mainly discusses the relationship between machine learning and deep learning based on AI and pattern of MCE in CMRI.              Recent findings:                    It found that some histogram and GLCM parameters in ML algorithm had significant statistical differences in diagnosis of cardiomyopathy and differentiation of fibrosis and normal myocardial tissue. In the DL algorithm, there was no significant difference between CNN and observers in measuring myocardial fibrosis. The rapid development of texture parameter analysis methods would promote the medical imaging based on AI into a new era. Histogram and GLCM parameters are the research hotspot of unsupervised learning of MCE images. CNN has a great advantage in automatically identifying and quantifying myocardial fibrosis reflected by LGE images.",2020-07-01,0,1160,67,311
63,32324388,Statistics in Proteomics: A Meta-analysis of 100 Proteomics Papers Published in 2019,"We randomly selected 100 journal articles published in five proteomics journals in 2019 and manually examined each of them against a set of 13 criteria concerning the statistical analyses used, all of which were based on items mentioned in the journals' instructions to authors. This included questions such as whether a pilot study was conducted and whether false discovery rate calculation was employed at either the quantitation or identification stage. These data were then transformed to binary inputs, analyzed via machine learning algorithms, and classified accordingly, with the aim of determining if clusters of data existed for specific journals or if certain statistical measures correlated with each other. We applied a variety of classification methods including principal component analysis decomposition, agglomerative clustering, and multinomial and Bernoulli nave Bayes classification and found that none of these could readily determine journal identity given extracted statistical features. Logistic regression was useful in determining high correlative potential between statistical features such as false discovery rate criteria and multiple testing corrections methods, but was similarly ineffective at determining correlations between statistical features and specific journals. This meta-analysis highlights that there is a very wide variety of approaches being used in statistical analysis of proteomics data, many of which do not conform to published journal guidelines, and that contrary to implicit assumptions in the field there are no clear correlations between statistical methods and specific journals.",2020-07-01,0,1635,84,311
1438,32631256,MASS: predict the global qualities of individual protein models using random forests and novel statistical potentials,"Background:                    Protein model quality assessment (QA) is an essential procedure in protein structure prediction. QA methods can predict the qualities of protein models and identify good models from decoys. Clustering-based methods need a certain number of models as input. However, if a pool of models are not available, methods that only need a single model as input are indispensable.              Results:                    We developed MASS, a QA method to predict the global qualities of individual protein models using random forests and various novel energy functions. We designed six novel energy functions or statistical potentials that can capture the structural characteristics of a protein model, which can also be used in other protein-related bioinformatics research. MASS potentials demonstrated higher importance than the energy functions of RWplus, GOAP, DFIRE and Rosetta when the scores they generated are used as machine learning features. MASS outperforms almost all of the four CASP11 top-performing single-model methods for global quality assessment in terms of all of the four evaluation criteria officially used by CASP, which measure the abilities to assign relative and absolute scores, identify the best model from decoys, and distinguish between good and bad models. MASS has also achieved comparable performances with the leading QA methods in CASP12 and CASP13.              Conclusions:                    MASS and the source code for all MASS potentials are publicly available at http://dna.cs.miami.edu/MASS/ .",2020-07-01,0,1560,117,311
1420,32647932,Machine Learning and Coronary Artery Calcium Scoring,"Purpose of review:                    To summarize current artificial intelligence (AI)-based applications for coronary artery calcium scoring (CACS) and their potential clinical impact.              Recent findings:                    Recent evolution of AI-based technologies in medical imaging has accelerated progress in CACS performed in diverse types of CT examinations, providing promising results for future clinical application in this field. CACS plays a key role in risk stratification of coronary artery disease (CAD) and patient management. Recent emergence of AI algorithms, particularly deep learning (DL)-based applications, have provided considerable progress in CACS. Many investigations have focused on the clinical role of DL models in CACS and showed excellent agreement between those algorithms and manual scoring, not only in dedicated coronary calcium CT but also in coronary CT angiography (CCTA), low-dose chest CT, and standard chest CT. Therefore, the potential of AI-based CACS may become more influential in the future.",2020-07-01,0,1049,52,311
815,32983527,Applications of Machine Learning in Cardiac Electrophysiology,"Artificial intelligence through machine learning (ML) methods is becoming prevalent throughout the world, with increasing adoption in healthcare. Improvements in technology have allowed early applications of machine learning to assist physician efficiency and diagnostic accuracy. In electrophysiology, ML has applications for use in every stage of patient care. However, its use is still in infancy. This article will introduce the potential of ML, before discussing the concept of big data and its pitfalls. The authors review some common ML methods including supervised and unsupervised learning, then examine applications in cardiac electrophysiology. This will focus on surface electrocardiography, intracardiac mapping and cardiac implantable electronic devices. Finally, the article concludes with an overview of how ML may impact on electrophysiology in the future.",2020-08-01,1,873,61,280
2,32413821,A review of modern technologies for tackling COVID-19 pandemic,"Objective:                    Science and technology sector constituting of data science, machine learning and artificial intelligence are contributing towards COVID-19. The aim of the present study is to discuss the various aspects of modern technology used to fight against COVID-19 crisis at different scales, including medical image processing, disease tracking, prediction outcomes, computational biology and medicines.              Methods:                    A progressive search of the database related to modern technology towards COVID-19 is made. Further, a brief review is done on the extracted information by assessing the various aspects of modern technologies for tackling COVID-19 pandemic.              Results:                    We provide a window of thoughts on review of the technology advances used to decrease and smother the substantial impact of the outburst. Though different studies relating to modern technology towards COVID-19 have come up, yet there are still constrained applications and contributions of technology in this fight.              Conclusions:                    On-going progress in the modern technology has contributed in improving people's lives and hence there is a solid conviction that validated research plans including artificial intelligence will be of significant advantage in helping people to fight this infection.",2020-08-01,23,1373,62,280
799,33005629,Systems Biology of Gastric Cancer: Perspectives on the Omics-Based Diagnosis and Treatment,"Gastric cancer is the fifth most diagnosed cancer in the world, affecting more than a million people and causing nearly 783,000 deaths each year. The prognosis of advanced gastric cancer remains extremely poor despite the use of surgery and adjuvant therapy. Therefore, understanding the mechanism of gastric cancer development, and the discovery of novel diagnostic biomarkers and therapeutics are major goals in gastric cancer research. Here, we review recent progress in application of omics technologies in gastric cancer research, with special focus on the utilization of systems biology approaches to integrate multi-omics data. In addition, the association between gastrointestinal microbiota and gastric cancer are discussed, which may offer insights in exploring the novel microbiota-targeted therapeutics. Finally, the application of data-driven systems biology and machine learning approaches could provide a predictive understanding of gastric cancer, and pave the way to the development of novel biomarkers and rational design of cancer therapeutics.",2020-08-01,1,1063,90,280
821,32974382,The Role of Machine Learning in Spine Surgery: The Future Is Now,"The recent influx of machine learning centered investigations in the spine surgery literature has led to increased enthusiasm as to the prospect of using artificial intelligence to create clinical decision support tools, optimize postoperative outcomes, and improve technologies used in the operating room. However, the methodology underlying machine learning in spine research is often overlooked as the subject matter is quite novel and may be foreign to practicing spine surgeons. Improper application of machine learning is a significant bioethics challenge, given the potential consequences of over- or underestimating the results of such studies for clinical decision-making processes. Proper peer review of these publications requires a baseline familiarity of the language associated with machine learning, and how it differs from classical statistical analyses. This narrative review first introduces the overall field of machine learning and its role in artificial intelligence, and defines basic terminology. In addition, common modalities for applying machine learning, including classification and regression decision trees, support vector machines, and artificial neural networks are examined in the context of examples gathered from the spine literature. Lastly, the ethical challenges associated with adapting machine learning for research related to patient care, as well as future perspectives on the potential use of machine learning in spine surgery, are discussed specifically.",2020-08-01,0,1498,64,280
822,32973876,Infrared Spectrometry as a High-Throughput Phenotyping Technology to Predict Complex Traits in Livestock Systems,"High-throughput phenotyping technologies are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. Collecting such individual-level information can generate novel traits and potentially improve animal selection and management decisions in livestock operations. One of the most relevant tools used in the dairy and beef industry to predict complex traits is infrared spectrometry, which is based on the analysis of the interaction between electromagnetic radiation and matter. The infrared electromagnetic radiation spans an enormous range of wavelengths and frequencies known as the electromagnetic spectrum. The spectrum is divided into different regions, with near- and mid-infrared regions being the main spectral regions used in livestock applications. The advantage of using infrared spectrometry includes speed, non-destructive measurement, and great potential for on-line analysis. This paper aims to review the use of mid- and near-infrared spectrometry techniques as tools to predict complex dairy and beef phenotypes, such as milk composition, feed efficiency, methane emission, fertility, energy balance, health status, and meat quality traits. Although several research studies have used these technologies to predict a wide range of phenotypes, most of them are based on Partial Least Squares (PLS) and did not considered other machine learning (ML) techniques to improve prediction quality. Therefore, we will discuss the role of analytical methods employed on spectral data to improve the predictive ability for complex traits in livestock operations. Furthermore, we will discuss different approaches to reduce data dimensionality and the impact of validation strategies on predictive quality.",2020-08-01,0,1795,112,280
793,33013638,Intracranial Pressure Monitoring Signals After Traumatic Brain Injury: A Narrative Overview and Conceptual Data Science Framework,"Continuous intracranial pressure (ICP) monitoring is a cornerstone of neurocritical care after severe brain injuries such as traumatic brain injury and acts as a biomarker of secondary brain injury. With the rapid development of artificial intelligent (AI) approaches to data analysis, the acquisition, storage, real-time analysis, and interpretation of physiological signal data can bring insights to the field of neurocritical care bioinformatics. We review the existing literature on the quantification and analysis of the ICP waveform and present an integrated framework to incorporate signal processing tools, advanced statistical methods, and machine learning techniques in order to comprehensively understand the ICP signal and its clinical importance. Our goals were to identify the strengths and pitfalls of existing methods for data cleaning, information extraction, and application. In particular, we describe the use of ICP signal analytics to detect intracranial hypertension and to predict both short-term intracranial hypertension and long-term clinical outcome. We provide a well-organized roadmap for future researchers based on existing literature and a computational approach to clinically-relevant biomedical signal data.",2020-08-01,0,1241,129,280
806,32994889,Artificial intelligence (AI) and big data in cancer and precision oncology,"Artificial intelligence (AI) and machine learning have significantly influenced many facets of the healthcare sector. Advancement in technology has paved the way for analysis of big datasets in a cost- and time-effective manner. Clinical oncology and research are reaping the benefits of AI. The burden of cancer is a global phenomenon. Efforts to reduce mortality rates requires early diagnosis for effective therapeutic interventions. However, metastatic and recurrent cancers evolve and acquire drug resistance. It is imperative to detect novel biomarkers that induce drug resistance and identify therapeutic targets to enhance treatment regimes. The introduction of the next generation sequencing (NGS) platforms address these demands, has revolutionised the future of precision oncology. NGS offers several clinical applications that are important for risk predictor, early detection of disease, diagnosis by sequencing and medical imaging, accurate prognosis, biomarker identification and identification of therapeutic targets for novel drug discovery. NGS generates large datasets that demand specialised bioinformatics resources to analyse the data that is relevant and clinically significant. Through these applications of AI, cancer diagnostics and prognostic prediction are enhanced with NGS and medical imaging that delivers high resolution images. Regardless of the improvements in technology, AI has some challenges and limitations, and the clinical application of NGS remains to be validated. By continuing to enhance the progression of innovation and technology, the future of AI and precision oncology show great promise.",2020-08-01,2,1638,74,280
783,33029222,"Recent advances and future directions for uterine diseases diagnosis, pathogenesis, and management in dairy cows","Researchers, veterinarians, and farmers' pursuit of a consistent diagnosis, treatment, and prevention of uterine diseases remains challenging. The diagnosis and treatment of metritis is inconsistent, a concerning situation when considered the global threat of antimicrobial resistance dissemination. Endometritis is an insidious disease absent on routine health programs in many dairy farms and from pharmaceutical therapeutics arsenal in places like the US market. Conversely, a multitude of studies advanced the understanding of how uterine diseases compromise oocyte, follicle, and embryo development, and the uterine environment having long-lasting effects on fertility. The field of uterine disease microbiome also experienced tremendous progress and created opportunities for the development of novel preventives to improve the management of uterine diseases. Activity monitors, biomarkers, genomic selection, and machine learning predictive models are other innovative developments that have been explored in recent years to help mitigate the negative impacts of uterine diseases. Albeit novel tools such as vaccines for metritis, immune modulators, probiotics, genomic selection, and selective antimicrobial therapy are promising, further research is warranted to implement these technologies in a systematic and cost-effective manner.",2020-08-01,0,1343,112,280
31,32364202,Engineering biosynthetic enzymes for industrial natural product synthesis,"Covering: 2000 to 2020 Natural products and their derivatives are commercially important medicines, agrochemicals, flavors, fragrances, and food ingredients. Industrial strategies to produce these structurally complex molecules encompass varied combinations of chemical synthesis, biocatalysis, and extraction from natural sources. Interest in engineering natural product biosynthesis began with the advent of genetic tools for pathway discovery. Genes and strains can now readily be synthesized, mutated, recombined, and sequenced. Enzyme engineering has succeeded commercially due to the development of genetic methods, analytical technologies, and machine learning algorithms. Today, engineered biosynthetic enzymes from organisms spanning the tree of life are used industrially to produce diverse molecules. These biocatalytic processes include single enzymatic steps, multienzyme cascades, and engineered native and heterologous microbial strains. This review will describe how biosynthetic enzymes have been engineered to enable commercial and near-commercial syntheses of natural products and their analogs.",2020-08-01,6,1114,73,280
25,32374075,Towards a brain-based predictome of mental illness,"Neuroimaging-based approaches have been extensively applied to study mental illness in recent years and have deepened our understanding of both cognitively healthy and disordered brain structure and function. Recent advancements in machine learning techniques have shown promising outcomes for individualized prediction and characterization of patients with psychiatric disorders. Studies have utilized features from a variety of neuroimaging modalities, including structural, functional, and diffusion magnetic resonance imaging data, as well as jointly estimated features from multiple modalities, to assess patients with heterogeneous mental disorders, such as schizophrenia and autism. We use the term ""predictome"" to describe the use of multivariate brain network features from one or more neuroimaging modalities to predict mental illness. In the predictome, multiple brain network-based features (either from the same modality or multiple modalities) are incorporated into a predictive model to jointly estimate features that are unique to a disorder and predict subjects accordingly. To date, more than 650 studies have been published on subject-level prediction focusing on psychiatric disorders. We have surveyed about 250 studies including schizophrenia, major depression, bipolar disorder, autism spectrum disorder, attention-deficit hyperactivity disorder, obsessive-compulsive disorder, social anxiety disorder, posttraumatic stress disorder, and substance dependence. In this review, we present a comprehensive review of recent neuroimaging-based predictomic approaches, current trends, and common shortcomings and share our vision for future directions.",2020-08-01,3,1669,50,280
1537,32201388,Sport and exercise genomics: the FIMS 2019 consensus statement update,"Rapid advances in technologies in the field of genomics such as high throughput DNA sequencing, big data processing by machine learning algorithms and gene-editing techniques are expected to make precision medicine and gene-therapy a greater reality. However, this development will raise many important new issues, including ethical, moral, social and privacy issues. The field of exercise genomics has also advanced by incorporating these innovative technologies. There is therefore an urgent need for guiding references for sport and exercise genomics to allow the necessary advancements in this field of sport and exercise medicine, while protecting athletes from any invasion of privacy and misuse of their genomic information. Here, we update a previous consensus and develop a guiding reference for sport and exercise genomics based on a SWOT (Strengths, Weaknesses, Opportunities and Threats) analysis. This SWOT analysis and the developed guiding reference highlight the need for scientists/clinicians to be well-versed in ethics and data protection policy to advance sport and exercise genomics without compromising the privacy of athletes and the efforts of international sports federations. Conducting research based on the present guiding reference will mitigate to a great extent the risks brought about by inappropriate use of genomic information and allow further development of sport and exercise genomics in accordance with best ethical standards and international data protection principles and policies. This guiding reference should regularly be updated on the basis of new information emerging from the area of sport and exercise medicine as well as from the developments and challenges in genomics of health and disease in general in order to best protect the athletes, patients and all other relevant stakeholders.",2020-08-01,5,1837,69,280
1544,32193643,Diagnostic accuracy and potential covariates for machine learning to identify IDH mutations in glioma patients: evidence from a meta-analysis,"Objectives:                    To assess the diagnostic accuracy of machine learning (ML) in predicting isocitrate dehydrogenase (IDH) mutations in patients with glioma and to identify potential covariates that could influence the diagnostic performance of ML.              Methods:                    A systematic search of PubMed, Web of Science, and the Cochrane library up to 1 August 2019 was conducted to collect all the articles investigating the diagnostic performance of ML for prediction of IDH mutation in glioma. The search strategy combined synonyms for 'machine learning', 'glioma', and 'IDH'. Pooled sensitivity, specificity, and their 95% confidence intervals (CIs) were calculated, and the area under the receiver operating characteristic curve (AUC) was obtained.              Results:                    Nine original articles assessing a total of 996 patients with glioma were included. Among these studies, five divided the participants into training and validation sets, while the remaining four studies only had a training set. The AUC of ML for predicting IDH mutation in the training and validation sets was 93% (95% CI 91-95%) and 89% (95% CI 86-92%), respectively. The pooled sensitivity and specificity were, respectively, 87% (95% CI 82-91%) and 88% (95% CI 83-92%) in the training set and 87% (95% CI 76-93%) and 90% (95% CI 72-97%) in the validation set. In subgroup analyses in the training set, the combined use of clinical and imaging features with ML yielded higher sensitivity (90% vs. 83%) and specificity (90% vs. 82%) than the use of imaging features alone. In addition, ML performed better for high-grade gliomas than for low-grade gliomas, and ML that used conventional MRI sequences demonstrated higher specificity for predicting IDH mutation than ML using conventional and advanced MRI sequences.              Conclusions:                    ML demonstrated an excellent diagnostic performance in predicting IDH mutation of glioma. Clinical information, MRI sequences, and glioma grade were the main factors influencing diagnostic specificity.              Key points:                     Machine learning demonstrated an excellent diagnostic performance for prediction of IDH mutation in glioma (the pooled sensitivity and specificity were 88% and 87%, respectively).  Machine learning that used conventional MRI sequences demonstrated higher specificity in predicting IDH mutation than that based on conventional and advanced MRI sequences (89% vs. 85%).  Integration of clinical and imaging features in machine learning yielded a higher sensitivity (90% vs. 83%) and specificity (90% vs. 82%) than that achieved by using imaging features alone.",2020-08-01,0,2693,141,280
41,32353725,Sustainable soil use and management: An interdisciplinary and systematic approach,"Soil is a key component of Earth's critical zone. It provides essential services for agricultural production, plant growth, animal habitation, biodiversity, carbon sequestration and environmental quality, which are crucial for achieving the United Nations' Sustainable Development Goals (SDGs). However, soil degradation has occurred in many places throughout the world due to factors such as soil pollution, erosion, salinization, and acidification. In order to achieve the SDGs by the target date of 2030, soils may need to be used and managed in a manner that is more sustainable than is currently practiced. Here we show that research in the field of sustainable soil use and management should prioritize the multifunctional value of soil health and address interdisciplinary linkages with major issues such as biodiversity and climate change. As soil is the largest terrestrial carbon pool, as well as a significant contributor of greenhouse gases, much progress can be made toward curtailing the climate crisis by sustainable soil management practices. One identified option is to increase soil organic carbon levels, especially with recalcitrant forms of carbon (e.g., biochar application). In general, soil health is primarily determined by the actions of the farming community. Therefore, information management and knowledge sharing are necessary to improve the sustainable behavior of practitioners and end-users. Scientists and policy makers are important actors in this social learning process, not only to disseminate evidence-based scientific knowledge, but also in generating new knowledge in close collaboration with farmers. While governmental funding for soil data collection has been generally decreasing, newly available 5G telecommunications, big data and machine learning based data collection and analytical tools are maturing. Interdisciplinary studies that incorporate such advances may lead to the formation of innovative sustainable soil use and management strategies that are aimed toward optimizing soil health and achieving the SDGs.",2020-08-01,1,2064,81,280
90,32279157,Enabling pregnant women and their physicians to make informed medication decisions using artificial intelligence,"The role of artificial intelligence (AI) in healthcare for pregnant women. To assess the role of AI in women's health, discover gaps, and discuss the future of AI in maternal health. A systematic review of English articles using EMBASE, PubMed, and SCOPUS. Search terms included pregnancy and AI. Research articles and book chapters were included, while conference papers, editorials and notes were excluded from the review. Included papers focused on pregnancy and AI methods, and pertained to pharmacologic interventions. We identified 376 distinct studies from our queries. A final set of 31 papers were included for the review. Included papers represented a variety of pregnancy concerns and multidisciplinary applications of AI. Few studies relate to pregnancy, AI, and pharmacologics and therefore, we review carefully those studies. External validation of models and techniques described in the studies is limited, impeding on generalizability of the studies. Our review describes how AI has been applied to address maternal health, throughout the pregnancy process: preconception, prenatal, perinatal, and postnatal health concerns. However, there is a lack of research applying AI methods to understand how pharmacologic treatments affect pregnancy. We identify three areas where AI methods could be used to improve our understanding of pharmacological effects of pregnancy, including: (a) obtaining sound and reliable data from clinical records (15 studies), (b) designing optimized animal experiments to validate specific hypotheses (1 study) to (c) implementing decision support systems that inform decision-making (11 studies). The largest literature gap that we identified is with regards to using AI methods to optimize translational studies between animals and humans for pregnancy-related drug exposures.",2020-08-01,3,1821,112,280
53,32337704,Importance of protein Ser/Thr/Tyr phosphorylation for bacterial pathogenesis,"Protein phosphorylation regulates a large variety of biological processes in all living cells. In pathogenic bacteria, the study of serine, threonine, and tyrosine (Ser/Thr/Tyr) phosphorylation has shed light on the course of infectious diseases, from adherence to host cells to pathogen virulence, replication, and persistence. Mass spectrometry (MS)-based phosphoproteomics has provided global maps of Ser/Thr/Tyr phosphosites in bacterial pathogens. Despite recent developments, a quantitative and dynamic view of phosphorylation events that occur during bacterial pathogenesis is currently lacking. Temporal, spatial, and subpopulation resolution of phosphorylation data is required to identify key regulatory nodes underlying bacterial pathogenesis. Herein, we discuss how technological improvements in sample handling, MS instrumentation, data processing, and machine learning should improve bacterial phosphoproteomic datasets and the information extracted from them. Such information is expected to significantly extend the current knowledge of Ser/Thr/Tyr phosphorylation in pathogenic bacteria and should ultimately contribute to the design of novel strategies to combat bacterial infections.",2020-08-01,1,1202,76,280
78,32305024,Artificial Intelligence (AI) applications for COVID-19 pandemic,"Background and aims:                    Healthcare delivery requires the support of new technologies like Artificial Intelligence (AI), Internet of Things (IoT), Big Data and Machine Learning to fight and look ahead against the new diseases. We aim to review the role of AI as a decisive technology to analyze, prepare us for prevention and fight with COVID-19 (Coronavirus) and other pandemics.              Methods:                    The rapid review of the literature is done on the database of Pubmed, Scopus and Google Scholar using the keyword of COVID-19 or Coronavirus and Artificial Intelligence or AI. Collected the latest information regarding AI for COVID-19, then analyzed the same to identify its possible application for this disease.              Results:                    We have identified seven significant applications of AI for COVID-19 pandemic. This technology plays an important role to detect the cluster of cases and to predict where this virus will affect in future by collecting and analyzing all previous data.              Conclusions:                    Healthcare organizations are in an urgent need for decision-making technologies to handle this virus and help them in getting proper suggestions in real-time to avoid its spread. AI works in a proficient way to mimic like human intelligence. It may also play a vital role in understanding and suggesting the development of a vaccine for COVID-19. This result-driven technology is used for proper screening, analyzing, prediction and tracking of current patients and likely future patients. The significant applications are applied to tracks data of confirmed, recovered and death cases.",2020-08-01,84,1674,63,280
59,32325045,Artificial Intelligence: The Future for Diabetes Care,"Artificial intelligence (AI) is a fast-growing field and its applications to diabetes, a global pandemic, can reform the approach to diagnosis and management of this chronic condition. Principles of machine learning have been used to build algorithms to support predictive models for the risk of developing diabetes or its consequent complications. Digital therapeutics have proven to be an established intervention for lifestyle therapy in the management of diabetes. Patients are increasingly being empowered for self-management of diabetes, and both patients and health care professionals are benefitting from clinical decision support. AI allows a continuous and burden-free remote monitoring of the patient's symptoms and biomarkers. Further, social media and online communities enhance patient engagement in diabetes care. Technical advances have helped to optimize resource use in diabetes. Together, these intelligent technical reforms have produced better glycemic control with reductions in fasting and postprandial glucose levels, glucose excursions, and glycosylated hemoglobin. AI will introduce a paradigm shift in diabetes care from conventional management strategies to building targeted data-driven precision care.",2020-08-01,2,1231,53,280
77,32305218,Individualized Diagnostic and Prognostic Models for Patients With Psychosis Risk Syndromes: A Meta-analytic View on the State of the Art,"Background:                    The clinical high risk (CHR) paradigm has facilitated research into the underpinnings of help-seeking individuals at risk for developing psychosis, aiming at predicting and possibly preventing transition to the overt disorder. Statistical methods such as machine learning and Cox regression have provided the methodological basis for this research by enabling the construction of diagnostic models (i.e., distinguishing CHR individuals from healthy individuals) and prognostic models (i.e., predicting a future outcome) based on different data modalities, including clinical, neurocognitive, and neurobiological data. However, their translation to clinical practice is still hindered by the high heterogeneity of both CHR populations and methodologies applied.              Methods:                    We systematically reviewed the literature on diagnostic and prognostic models built on Cox regression and machine learning. Furthermore, we conducted a meta-analysis on prediction performances investigating heterogeneity of methodological approaches and data modality.              Results:                    A total of 44 articles were included, covering 3707 individuals for prognostic studies and 1052 individuals for diagnostic studies (572 CHR patients and 480 healthy control subjects). CHR patients could be classified against healthy control subjects with 78% sensitivity and 77% specificity. Across prognostic models, sensitivity reached 67% and specificity reached 78%. Machine learning models outperformed those applying Cox regression by 10% sensitivity. There was a publication bias for prognostic studies yet no other moderator effects.              Conclusions:                    Our results may be driven by substantial clinical and methodological heterogeneity currently affecting several aspects of the CHR field and limiting the clinical implementability of the proposed models. We discuss conceptual and methodological harmonization strategies to facilitate more reliable and generalizable models for future clinical practice.",2020-08-01,2,2081,136,280
71,32315515,Continuous glucose monitoring for hypoglycaemia in children: Perspectives in 2020,"Hypoglycaemia in children is a major risk factor for adverse neurodevelopment with rates as high as 50% in hyperinsulinaemic hypoglycaemia (HH). A key part of management relies upon timely identification and treatment of hypoglycaemia. The current standard of care for glucose monitoring is by infrequent fingerprick plasma glucose testing but this carries a high risk of missed hypoglycaemia identification. High-frequency Continuous Glucose Monitoring (CGM) offers an attractive alternative for glucose trend monitoring and glycaemic phenotyping but its utility remains largely unestablished in disorders of hypoglycaemia. Attempts to determine accuracy through correlation with plasma glucose measurements using conventional methods such as Mean Absolute Relative Difference (MARD) overestimate accuracy at hypoglycaemia. The inaccuracy of CGM in true hypoglycaemia is amplified by calibration algorithms that prioritize hyperglycaemia over hypoglycaemia with minimal objective evidence of efficacy in HH. Conversely, alternative algorithm design has significant potential for predicting hypoglycaemia to prevent neuroglycopaenia and consequent brain dysfunction in childhood disorders. Delays in the detection of hypoglycaemia, alarm fatigue, device calibration and current high cost are all barriers to the wider adoption of CGM in disorders of hypoglycaemia. However, machine learning, artificial intelligence and other computer-generated algorithms now offer significant potential for further improvement in CGM device technology and widespread application in childhood hypoglycaemia.",2020-08-01,0,1591,81,280
76,32305325,Toward a Revised Nosology for Attention-Deficit/Hyperactivity Disorder Heterogeneity,"Attention-deficit/hyperactivity disorder (ADHD) is among the many syndromes in the psychiatric nosology for which etiological signal and clinical prediction are weak. Reducing phenotypic and mechanistic heterogeneity should be useful to arrive at stronger etiological and clinical prediction signals. We discuss key conceptual and methodological issues, highlighting the role of dimensional features aligned with Research Domain Criteria and cognitive, personality, and temperament theory as well as neurobiology. We describe several avenues of work in this area, utilizing different statistical, computational, and machine learning approaches to resolve heterogeneity in ADHD. We offer methodological and conceptual recommendations. Methodologically, we propose that an integrated approach utilizing theory and advanced computational logic to address targeted questions, with consideration of developmental context, can render the heterogeneity problem tractable for ADHD. Conceptually, we conclude that the field is on the cusp of justifying an emotionally dysregulated subprofile in ADHD that may be useful for clinical prediction and treatment testing. Cognitive profiles, while more nascent, may be useful for clinical prediction and treatment assignment in different ways depending on developmental stage. Targeting these psychological profiles for neurobiological and etiological study to capture different pathophysiological routes remains a near-term opportunity. Subtypes are likely to be multifactorial, cut across multiple dimensions, and depend on the research or clinical outcomes of interest for their ultimate selection. In this context parallel profiles based on cognition, emotion, and specific neural signatures appear to be on the horizon, each with somewhat different utilities. Efforts to integrate such cross-cutting profiles within a conceptual dysregulation framework are well underway.",2020-08-01,2,1911,84,280
22,32380203,Towards the quantum-enabled technologies for development of drugs or delivery systems,"Enormous advances in technology and science have provided outstanding innovations including the development of quantum computers (QCs) capable of performing various tasks much more efficiently and quickly than the classical computers. Integrating and analyzing gigantic amounts of data, ultra-rapid calculations, solving intractable problems, secure communications, providing novel insights into the material design or biosystems, advanced simulations, rapid genome analysis and sequencing, early cancer detection, identifying novel drug applications, accelerated discovery of new molecules, targets, or theranostic agents and evaluation of their behaviors, and acquiring a deeper knowledge about the complex data patterns, formation of proteins, or mechanism of disease progression and evolution by QCs may indeed revolutionize conventional technologies and strategies. Application of quantum computing and machine learning for accelerated analysis of the biological or medical data, uncovering the mechanisms of chemical reactions or action of drug candidates, and creation of patient-specific treatment strategies using genomics data can result in the development of more effective and less toxic drugs or personalized therapy. This article highlights the importance of QCs in designing drugs and delivery systems, limitations, and possible solutions.",2020-08-01,0,1354,85,280
1224,32111636,Big data in IBD: big progress for clinical practice,"IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research.",2020-08-01,4,1040,51,280
902,31267627,The Crucial Role of Methodology Development in Directed Evolution of Selective Enzymes,"Directed evolution of stereo-, regio-, and chemoselective enzymes constitutes a unique way to generate biocatalysts for synthetically interesting transformations in organic chemistry and biotechnology. In order for this protein engineering technique to be efficient, fast, and reliable, and also of relevance to synthetic organic chemistry, methodology development was and still is necessary. Following a description of early key contributions, this review focuses on recent developments. It includes optimization of molecular biological methods for gene mutagenesis and the design of efficient strategies for their application, resulting in notable reduction of the screening effort (bottleneck of directed evolution). When aiming for laboratory evolution of selectivity and activity, second-generation versions of Combinatorial Active-Site Saturation Test (CAST) and Iterative Saturation Mutagenesis (ISM), both involving saturation mutagenesis (SM) at sites lining the binding pocket, have emerged as preferred approaches, aided by in silico methods such as machine learning. The recently proposed Focused Rational Iterative Site-specific Mutagenesis (FRISM) constitutes a fusion of rational design and directed evolution. On-chip solid-phase chemical gene synthesis for rapid library construction enhances library quality notably by eliminating undesired amino acid bias, the future of directed evolution?",2020-08-01,21,1409,86,280
1409,32657885,Machine learning for classification and prediction of brain diseases: recent advances and upcoming challenges,"Purpose of review:                    Machine learning is an artificial intelligence technique that allows computers to perform a task without being explicitly programmed. Machine learning can be used to assist diagnosis and prognosis of brain disorders. Although the earliest articles date from more than ten years ago, research increases at a very fast pace.              Recent findings:                    Recent works using machine learning for diagnosis have moved from classification of a given disease versus controls to differential diagnosis. Intense research has been devoted to the prediction of the future patient state. Although a lot of earlier works focused on neuroimaging as data source, the current trend is on the integration of multimodal data. In terms of targeted diseases, dementia remains dominant but approaches have been developed for a wide variety of neurological and psychiatric diseases.              Summary:                    Machine learning is extremely promising for assisting diagnosis and prognosis in brain disorders. Nevertheless, we argue that key challenges remain to be addressed by the community for bringing these tools in clinical routine: good practices regarding validation and reproducible research need to be more widely adopted; extensive generalization studies are required; interpretable models are needed to overcome the limitations of black-box approaches.",2020-08-01,2,1412,109,280
1459,32610220,Blurred lines: integrating emerging technologies to advance plant biosecurity,"Plant diseases threaten global food security and biodiversity. Rapid dispersal of pathogens particularly via human means has accelerated in recent years. Timely detection of plant pathogens is essential to limit their spread. At the same time, international regulations must keep abreast of advances in plant disease diagnostics. In this review we describe recent progress in developing modern plant disease diagnostics based on detection of pathogen components, high-throughput image analysis, remote sensing, and machine learning. We discuss how different diagnostic approaches can be integrated in detection frameworks that can work at different scales and account for sampling biases. Lastly, we briefly discuss the requirements to apply these advances under regulatory settings to improve biosecurity measures globally.",2020-08-01,1,824,77,280
2645,32487887,Artificial intelligence in transplantation (machine-learning classifiers and transplant oncology),"Purpose of review:                    To highlight recent efforts in the development and implementation of machine learning in transplant oncology - a field that uses liver transplantation for the treatment of hepatobiliary malignancies - and particularly in hepatocellular carcinoma, the most commonly treated diagnosis in transplant oncology.              Recent findings:                    The development of machine learning has occurred within three domains related to hepatocellular carcinoma: identification of key clinicopathological variables, genomics, and image processing.              Summary:                    Machine-learning classifiers can be effectively applied for more accurate clinical prediction and handling of data, such as genetics and imaging in transplant oncology. This has allowed for the identification of factors that most significantly influence recurrence and survival in disease, such as hepatocellular carcinoma, and thus help in prognosticating patients who may benefit from a liver transplant. Although progress has been made in using these methods to analyse clinicopathological information, genomic profiles, and image processed data (both histopathological and radiomic), future progress relies on integrating data across these domains.",2020-08-01,0,1279,97,280
2644,32487891,Machine-learning algorithms for predicting results in liver transplantation: the problem of donor-recipient matching,"Purpose of review:                    Classifiers based on artificial intelligence can be useful to solve decision problems related to the inclusion or removal of possible liver transplant candidates, and assisting in the heterogeneous field of donor-recipient (D-R) matching.              Recent findings:                    Artificial intelligence models can show a great advantage by being able to handle a multitude of variables, be objective and help in cases of similar probabilities. In the field of liver transplantation, the most commonly used classifiers have been artificial neural networks (ANNs) and random forest classifiers. ANNs are excellent tools for finding patterns which are far too complex for a clinician and are capable of generating near-perfect predictions on the data on which they are fit, yielding excellent prediction capabilities reaching 95% for 3 months graft survival. On the other hand, RF can overcome ANNs in some of their limitations, mainly because of the lack of information on the variables they provide. Random forest algorithms may allow for improved confidence with the use of marginal organs and better outcome after transplantation.              Summary:                    ANNs and random forest can handle a multitude of structured and unstructured parameters, and establish non explicit relationships among risk factors of clinical relevance.",2020-08-01,0,1391,116,280
1424,32645448,Machine learning and AI-based approaches for bioactive ligand discovery and GPCR-ligand recognition,"In the last decade, machine learning and artificial intelligence applications have received a significant boost in performance and attention in both academic research and industry. The success behind most of the recent state-of-the-art methods can be attributed to the latest developments in deep learning. When applied to various scientific domains that are concerned with the processing of non-tabular data, for example, image or text, deep learning has been shown to outperform not only conventional machine learning but also highly specialized tools developed by domain experts. This review aims to summarize AI-based research for GPCR bioactive ligand discovery with a particular focus on the most recent achievements and research trends. To make this article accessible to a broad audience of computational scientists, we provide instructive explanations of the underlying methodology, including overviews of the most commonly used deep learning architectures and feature representations of molecular data. We highlight the latest AI-based research that has led to the successful discovery of GPCR bioactive ligands. However, an equal focus of this review is on the discussion of machine learning-based technology that has been applied to ligand discovery in general and has the potential to pave the way for successful GPCR bioactive ligand discovery in the future. This review concludes with a brief outlook highlighting the recent research trends in deep learning, such as active learning and semi-supervised learning, which have great potential for advancing bioactive ligand discovery.",2020-08-01,0,1596,99,280
1455,32617080,Artificial intelligence in radiotherapy,"Artificial intelligence (AI) has already been implemented widely in the medical field in the recent years. This paper first reviews the background of AI and radiotherapy. Then it explores the basic concepts of different AI algorithms and machine learning methods, such as neural networks, that are available to us today and how they are being implemented in radiotherapy and diagnostic processes, such as medical imaging, treatment planning, patient simulation, quality assurance and radiation dose delivery. It also explores the ongoing research on AI methods that are to be implemented in radiotherapy in the future. The review shows very promising progress and future for AI to be widely used in various areas of radiotherapy. However, basing on various concerns such as availability and security of using big data, and further work on polishing and testing AI algorithms, it is found that we may not ready to use AI primarily in radiotherapy at the moment.",2020-08-01,2,960,39,280
1425,32644061,AI on a chip,"Artificial intelligence (AI) has dramatically changed the landscape of science, industry, defence, and medicine in the last several years. Supported by considerably enhanced computational power and cloud storage, the field of AI has shifted from mostly theoretical studies in the discipline of computer science to diverse real-life applications such as drug design, material discovery, speech recognition, self-driving cars, advertising, finance, medical imaging, and astronomical observation, where AI-produced outcomes have been proven to be comparable or even superior to the performance of human experts. In these applications, what is essentially important for the development of AI is the data needed for machine learning. Despite its prominent importance, the very first process of the AI development, namely data collection and data preparation, is typically the most laborious task and is often a limiting factor of constructing functional AI algorithms. Lab-on-a-chip technology, in particular microfluidics, is a powerful platform for both the construction and implementation of AI in a large-scale, cost-effective, high-throughput, automated, and multiplexed manner, thereby overcoming the above bottleneck. On this platform, high-throughput imaging is a critical tool as it can generate high-content information (e.g., size, shape, structure, composition, interaction) of objects on a large scale. High-throughput imaging can also be paired with sorting and DNA/RNA sequencing to conduct a massive survey of phenotype-genotype relations whose data is too complex to analyze with traditional computational tools, but is analyzable with the power of AI. In addition to its function as a data provider, lab-on-a-chip technology can also be employed to implement the developed AI for accurate identification, characterization, classification, and prediction of objects in mixed, heterogeneous, or unknown samples. In this review article, motivated by the excellent synergy between AI and lab-on-a-chip technology, we outline fundamental elements, recent advances, future challenges, and emerging opportunities of AI with lab-on-a-chip technology or ""AI on a chip"" for short.",2020-08-01,4,2183,12,280
2633,32511198,Applying Artificial Intelligence to Mitigate Effects of Patient Motion or Other Complicating Factors on Image Quality,"Artificial intelligence, particularly deep learning, offers several possibilities to improve the quality or speed of image acquisition in magnetic resonance imaging (MRI). In this article, we briefly review basic machine learning concepts and discuss commonly used neural network architectures for image-to-image translation. Recent examples in the literature describing application of machine learning techniques to clinical MR image acquisition or postprocessing are discussed. Machine learning can contribute to better image quality by improving spatial resolution, reducing image noise, and removing undesired motion or other artifacts. As patients occasionally are unable to tolerate lengthy acquisition times or gadolinium agents, machine learning can potentially assist MRI workflow and patient comfort by facilitating faster acquisitions or reducing exogenous contrast dosage. Although artificial intelligence approaches often have limitations, such as problems with generalizability or explainability, there is potential for these techniques to improve diagnostic utility, throughput, and patient experience in clinical MRI practice.",2020-08-01,2,1142,117,280
2621,32520786,Precision transplant pathology,"Purpose of review:                    Transplant pathology contributes substantially to personalized treatment of organ allograft recipients. Rapidly advancing next-generation human leukocyte antigen (HLA) sequencing and pathology are enhancing the abilities to improve donor/recipient matching and allograft monitoring.              Recent findings:                    The present review summarizes the workflow of a prototypical patient through a pathology practice, highlighting histocompatibility assessment and pathologic review of tissues as areas that are evolving to incorporate next-generation technologies while emphasizing critical needs of the field.              Summary:                    Successful organ transplantation starts with the most precise pratical donor-recipient histocompatibility matching. Next-generation sequencing provides the highest resolution donor-recipient matching and enables eplet mismatch scores and more precise monitoring of donor-specific antibodies (DSAs) that may arise after transplant. Multiplex labeling combined with hand-crafted machine learning is transforming traditional histopathology. The combination of traditional blood/body fluid laboratory tests, eplet and DSA analysis, traditional and next-generation histopathology, and -omics-based platforms enables risk stratification and identification of early subclinical molecular-based changes that precede a decline in allograft function. Needs include software integration of data derived from diverse platforms that can render the most accurate assessment of allograft health and needs for immunosuppression adjustments.",2020-08-01,0,1628,30,280
2617,32529700,Global Leadership Initiative on Malnutrition (GLIM): Guidance on Validation of the Operational Criteria for the Diagnosis of Protein-Energy Malnutrition in Adults,"Background:                    The Global Leadership Initiative on Malnutrition (GLIM) created a consensus-based framework consisting of phenotypic and etiologic criteria to record the occurrence of malnutrition in adults. This is a minimum set of practicable indicators for use in characterizing a patient/client as malnourished, considering the global variations in screening and nutrition assessment, and to be used across different healthcare settings. As with other consensus-based frameworks for diagnosing disease states, these operational criteria require validation and reliability testing, as they are currently based solely on expert opinion.              Methods:                    Several forms of validation and reliability are reviewed in the context of GLIM, providing guidance on how to conduct retrospective and prospective studies for criterion and construct validity.              Results:                    There are some aspects of GLIM that require refinement; research using large databases can be employed to reach this goal. Machine learning is also introduced as a potential method to support identification of the best cut points and combinations of indicators for use with the different forms of malnutrition, which the GLIM criteria were created to denote. It is noted as well that validation and reliability testing need to occur in a variety of sectors and populations and with diverse persons using GLIM criteria.              Conclusion:                    The guidance presented supports the conduct and publication of quality validation and reliability studies for GLIM.",2020-08-01,1,1608,162,280
1450,32618714,Machine learning methods in organ transplantation,"Purpose of review:                    Machine learning techniques play an important role in organ transplantation. Analysing the main tasks for which they are being applied, together with the advantages and disadvantages of their use, can be of crucial interest for clinical practitioners.              Recent findings:                    In the last 10 years, there has been an explosion of interest in the application of machine-learning techniques to organ transplantation. Several approaches have been proposed in the literature aiming to find universal models by considering multicenter cohorts or from different countries. Moreover, recently, deep learning has also been applied demonstrating a notable ability when dealing with a vast amount of information.              Summary:                    Organ transplantation can benefit from machine learning in such a way to improve the current procedures for donor--recipient matching or to improve standard scores. However, a correct preprocessing is needed to provide consistent and high quality databases for machine-learning algorithms, aiming to robust and fair approaches to support expert decision-making systems.",2020-08-01,0,1175,49,280
2606,32559068,Machine Learning Force Fields and Coarse-Grained Variables in Molecular Dynamics: Application to Materials and Biological Systems,"Machine learning encompasses tools and algorithms that are now becoming popular in almost all scientific and technological fields. This is true for molecular dynamics as well, where machine learning offers promises of extracting valuable information from the enormous amounts of data generated by simulation of complex systems. We provide here a review of our current understanding of goals, benefits, and limitations of machine learning techniques for computational studies on atomistic systems, focusing on the construction of empirical force fields from ab initio databases and the determination of reaction coordinates for free energy computation and enhanced sampling.",2020-08-01,2,673,129,280
2603,32561299,Image-based high-content screening in drug discovery,"While target-based drug discovery strategies rely on the precise knowledge of the identity and function of the drug targets, phenotypic drug discovery (PDD) approaches allow the identification of novel drugs based on knowledge of a distinct phenotype. Image-based high-content screening (HCS) is a potent PDD strategy that characterizes small-molecule effects through the quantification of features that depict cellular changes among or within cell populations, thereby generating valuable data sets for subsequent data analysis. However, these data can be complex, making image analysis from large HCS campaigns challenging. Technological advances in image acquisition, processing, and analysis as well as machine-learning (ML) approaches for the analysis of multidimensional data sets have rendered HCS as a viable technology for small-molecule drug discovery. Here, we discuss HCS concepts, current workflows as well as opportunities and challenges of image-based phenotypic screening and data analysis.",2020-08-01,0,1006,52,280
2602,32561444,Recent advances of HCI in decision-making tasks for optimized clinical workflows and precision medicine,"The ever-increasing amount of biomedical data is enabling new large-scale studies, even though ad hoc computational solutions are required. The most recent Machine Learning (ML) and Artificial Intelligence (AI) techniques have been achieving outstanding performance and an important impact in clinical research, aiming at precision medicine, as well as improving healthcare workflows. However, the inherent heterogeneity and uncertainty in the healthcare information sources pose new compelling challenges for clinicians in their decision-making tasks. Only the proper combination of AI and human intelligence capabilities, by explicitly taking into account effective and safe interaction paradigms, can permit the delivery of care that outperforms what either can do separately. Therefore, Human-Computer Interaction (HCI) plays a crucial role in the design of software oriented to decision-making in medicine. In this work, we systematically review and discuss several research fields strictly linked to HCI and clinical decision-making, by subdividing the articles into six themes, namely: Interfaces, Visualization, Electronic Health Records, Devices, Usability, and Clinical Decision Support Systems. However, these articles typically present overlaps among the themes, revealing that HCI inter-connects multiple topics. With the goal of focusing on HCI and design aspects, the articles under consideration were grouped into four clusters. The advances in AI can effectively support the physicians' cognitive processes, which certainly play a central role in decision-making tasks because the human mental behavior cannot be completely emulated and captured; the human mind might solve a complex problem even without a statistically significant amount of data by relying upon domain knowledge. For this reason, technology must focus on interactive solutions for supporting the physicians effectively in their daily activities, by exploiting their unique knowledge and evidence-based reasoning, as well as improving the various aspects highlighted in this review.",2020-08-01,2,2067,103,280
2596,32567390,In silico models for genotoxicity and drug regulation,"Introduction:                    Whereas in the past, (Q)SAR methods have been largely used to support the design of new drugs, in the last few decades, there has been a new interest in its applications for the assessment of drug safety. In particular, the ICH M7 guideline has introduced the concept that (Q)SAR predictions for the Ames mutagenicity of drug impurities can be used for regulatory purposes.              Areas covered:                    This review introduces the ICH M7 conceptual framework and illustrates the most updated evaluations of the in silico approaches for the prediction of genotoxicity. The strengths and weaknesses of the state-of-the-art are presented and future perspectives are discussed.              Expert opinion:                    Given the growing recognition of (Q)SAR approaches, more investment will be devoted to its improvement. The major areas of research should be the expansion and curation of the experimental training sets, with particular attention to the portions of chemical space which are poorly represented. New modeling methodologies (e.g. machine-learning methods) may support this effort, particularly for treating proprietary data without disclosure. Research on new integrative approaches for regulatory decisions will also be important.",2020-08-01,1,1300,53,280
1447,32622173,How to do things with (thousands of) words: Computational approaches to discourse analysis in Alzheimer's disease,"Natural Language Processing (NLP) is an ever-growing field of computational science that aims to model natural human language. Combined with advances in machine learning, which learns patterns in data, it offers practical capabilities including automated language analysis. These approaches have garnered interest from clinical researchers seeking to understand the breakdown of language due to pathological changes in the brain, offering fast, replicable and objective methods. The study of Alzheimer's disease (AD), and preclinical Mild Cognitive Impairment (MCI), suggests that changes in discourse (connected speech or writing) may be key to early detection of disease. There is currently no disease-modifying treatment for AD, the leading cause of dementia in people over the age of 65, but detection of those at risk of developing the disease could help with the identification and testing of medications which can take effect before the underlying pathology has irreversibly spread. We outline important components of natural language, as well as NLP tools and approaches with which they can be extracted, analysed and used for disease identification and risk prediction. We review literature using these tools to model discourse across the spectrum of AD, including the contribution of machine learning approaches and Automatic Speech Recognition (ASR). We conclude that NLP and machine learning techniques are starting to greatly enhance research in the field, with measurable and quantifiable language components showing promise for early detection of disease, but there remain research and practical challenges for clinical implementation of these approaches. Challenges discussed include the availability of large and diverse datasets, ethics of data collection and sharing, diagnostic specificity and clinical acceptability.",2020-08-01,0,1837,113,280
2665,32446113,The emerging roles of artificial intelligence in cancer drug development and precision therapy,"Artificial intelligence (AI) has strong logical reasoning ability and independent learning ability, which can simulate the thinking process of the human brain. AI technologies such as machine learning can profoundly optimize the existing mode of anticancer drug research. But at present AI also has its relative limitation. In this paper, the development of artificial intelligence technology such as deep learning and machine learning in anticancer drug research is reviewed. At the same time, we look forward to the future of AI.",2020-08-01,4,531,94,280
828,32968541,Listening forward: approaching marine biodiversity assessments using acoustic methods,"Ecosystems and the communities they support are changing at alarmingly rapid rates. Tracking species diversity is vital to managing these stressed habitats. Yet, quantifying and monitoring biodiversity is often challenging, especially in ocean habitats. Given that many animals make sounds, these cues travel efficiently under water, and emerging technologies are increasingly cost-effective, passive acoustics (a long-standing ocean observation method) is now a potential means of quantifying and monitoring marine biodiversity. Properly applying acoustics for biodiversity assessments is vital. Our goal here is to provide a timely consideration of emerging methods using passive acoustics to measure marine biodiversity. We provide a summary of the brief history of using passive acoustics to assess marine biodiversity and community structure, a critical assessment of the challenges faced, and outline recommended practices and considerations for acoustic biodiversity measurements. We focused on temperate and tropical seas, where much of the acoustic biodiversity work has been conducted. Overall, we suggest a cautious approach to applying current acoustic indices to assess marine biodiversity. Key needs are preliminary data and sampling sufficiently to capture the patterns and variability of a habitat. Yet with new analytical tools including source separation and supervised machine learning, there is substantial promise in marine acoustic diversity assessment methods.",2020-08-01,1,1483,85,280
2667,32437828,Predicting treatment effects in unipolar depression: A meta-review,"There is increasing interest in clinical prediction models in psychiatry, which focus on developing multivariate algorithms to guide personalized diagnostic or management decisions. The main target of these models is the prediction of treatment response to different antidepressant therapies. This is because the ability to predict response based on patients' personal data may allow clinicians to make improved treatment decisions, and to provide more efficacious or more tolerable medications to the right patient. We searched the literature for systematic reviews about treatment prediction in the context of existing treatment modalities for adult unipolar depression, until July 2019. Treatment effect is defined broadly to include efficacy, safety, tolerability and acceptability outcomes. We first focused on the identification of individual predictor variables that might predict treatment response, and second, we considered multivariate clinical prediction models. Our meta-review included a total of 10 systematic reviews; seven (from 2014 to 2018) focusing on individual predictor variables and three focusing on clinical prediction models. These identified a number of sociodemographic, phenomenological, clinical, neuroimaging, remote monitoring, genetic and serum marker variables as possible predictor variables for treatment response, alongside statistical and machine-learning approaches to clinical prediction model development. Effect sizes for individual predictor variables were generally small and clinical prediction models had generally not been validated in external populations. There is a need for rigorous model validation in large external data-sets to prove the clinical utility of models. We also discuss potential future avenues in the field of personalized psychiatry, particularly the combination of multiple sources of data and the emerging field of artificial intelligence and digital mental health to identify new individual predictor variables.",2020-08-01,4,1983,66,280
2671,32434014,Evolution of Minimally Invasive Lumbar Spine Surgery,"Spine surgery has evolved over centuries from first being practiced with Hippocratic boards and ladders to now being able to treat spinal pathologies with minimal tissue invasion. With the advent of new imaging and surgical technologies, spine surgeries can now be performed minimally invasively with smaller incisions, less blood loss, quicker return to daily activities, and increased visualization. Modern minimally invasive procedures include percutaneous pedicle screw fixation techniques and minimally invasive lateral approach for lumbar interbody fusion (i.e., minimally invasive transforaminal lumbar interbody fusion, extreme lateral interbody fusion, oblique lateral interbody fusion) and midline lumbar fusion with cortical bone trajectory screws. Just as evolutions in surgical techniques have helped revolutionize the field of spine surgery, imaging technologies have also contributed significantly. The advent of computer image guidance has allowed spine surgeons to advance their ability to refine surgical techniques, increase the accuracy of spinal hardware placement, and reduce radiation exposure to the operating room staff. As the field of spine surgery looks to the future, many novel technologies are on the horizon, including robotic spine surgery, artificial intelligence, and machine learning to help improve preoperative planning, improve surgical execution, and optimize patient selection to ensure improved postoperative outcomes and patient satisfaction. As more spine surgeons begin incorporating these novel minimally invasive techniques into practice, the field of minimally invasive spine surgery will continue to innovate and evolve over the coming years.",2020-08-01,0,1691,52,280
955,31932230,Toward Addiction Prediction: An Overview of Cross-Validated Predictive Modeling Findings and Considerations for Future Neuroimaging Research,"Substance use is a leading cause of disability and death worldwide. Despite the existence of evidence-based treatments, clinical outcomes are highly variable across individuals, and relapse rates following treatment remain high. Within this context, methods to identify individuals at particular risk for unsuccessful treatment (i.e., limited within-treatment abstinence), or for relapse following treatment, are needed to improve outcomes. Cumulatively, the literature generally supports the hypothesis that individual differences in brain function and structure are linked to differences in treatment outcomes, although anatomical loci and directions of associations have differed across studies. However, this work has almost entirely used methods that may overfit the data, leading to inflated effect size estimates and reduced likelihood of reproducibility in novel clinical samples. In contrast, cross-validated predictive modeling (i.e., machine learning) approaches are designed to overcome limitations of traditional approaches by focusing on individual differences and generalization to novel subjects (i.e., cross-validation), thereby increasing the likelihood of replication and potential translation to novel clinical settings. Here, we review recent studies using these approaches to generate brain-behavior models of treatment outcomes in addictions and provide recommendations for further work using these methods.",2020-08-01,4,1430,140,280
969,31902580,Discovery and Validation of Prediction Algorithms for Psychosis in Youths at Clinical High Risk,"In the past 2 to 3 decades, clinicians have used the clinical high risk for psychosis (CHR-P) paradigm to better understand factors that contribute to the onset of psychotic disorders. While this paradigm is useful to identify individuals at risk, the CHR-P criteria are not sufficient to predict outcomes from the CHR-P population. Because approximately 25% of the CHR-P population will ultimately convert to psychosis, more precise methods of prediction are needed to account for heterogeneity in both risk factors and outcomes in the CHR-P population. To this end, several groups in recent years have used data-driven approaches to refine predictive algorithms to predict both conversion to psychosis and functional outcomes. These models have generally used either clinical and behavioral data, including demographics and measures of symptom severity, neurocognitive functioning, and social functioning, or neuroimaging data, including structural and functional measures, to predict conversion to psychosis in CHR-P samples. This review focuses on the empirical models that have been derived within each of these lines of research and evaluates the performance and methodology of these models. This review also serves to inform best practices for data-driven approaches and directions moving forward to improve our prediction of psychotic disorders and associated outcomes. Because sample size is still the most critical consideration in the current models, we urge that algorithms to predict conversion be conducted using multisite data in order to obtain the power necessary to conclusively determine predictive accuracy without overfitting.",2020-08-01,4,1647,95,280
1484,32584780,Approaches Based on Artificial Intelligence and the Internet of Intelligent Things to Prevent the Spread of COVID-19: Scoping Review,"Background:                    Artificial intelligence (AI) and the Internet of Intelligent Things (IIoT) are promising technologies to prevent the concerningly rapid spread of coronavirus disease (COVID-19) and to maximize safety during the pandemic. With the exponential increase in the number of COVID-19 patients, it is highly possible that physicians and health care workers will not be able to treat all cases. Thus, computer scientists can contribute to the fight against COVID-19 by introducing more intelligent solutions to achieve rapid control of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes the disease.              Objective:                    The objectives of this review were to analyze the current literature, discuss the applicability of reported ideas for using AI to prevent and control COVID-19, and build a comprehensive view of how current systems may be useful in particular areas. This may be of great help to many health care administrators, computer scientists, and policy makers worldwide.              Methods:                    We conducted an electronic search of articles in the MEDLINE, Google Scholar, Embase, and Web of Knowledge databases to formulate a comprehensive review that summarizes different categories of the most recently reported AI-based approaches to prevent and control the spread of COVID-19.              Results:                    Our search identified the 10 most recent AI approaches that were suggested to provide the best solutions for maximizing safety and preventing the spread of COVID-19. These approaches included detection of suspected cases, large-scale screening, monitoring, interactions with experimental therapies, pneumonia screening, use of the IIoT for data and information gathering and integration, resource allocation, predictions, modeling and simulation, and robotics for medical quarantine.              Conclusions:                    We found few or almost no studies regarding the use of AI to examine COVID-19 interactions with experimental therapies, the use of AI for resource allocation to COVID-19 patients, or the use of AI and the IIoT for COVID-19 data and information gathering/integration. Moreover, the adoption of other approaches, including use of AI for COVID-19 prediction, use of AI for COVID-19 modeling and simulation, and use of AI robotics for medical quarantine, should be further emphasized by researchers because these important approaches lack sufficient numbers of studies. Therefore, we recommend that computer scientists focus on these approaches, which are still not being adequately addressed.",2020-08-01,15,2646,132,280
1441,32628863,Artificial Intelligence and Machine Learning in Arrhythmias and Cardiac Electrophysiology,"Artificial intelligence (AI) and machine learning (ML) in medicine are currently areas of intense exploration, showing potential to automate human tasks and even perform tasks beyond human capabilities. Literacy and understanding of AI/ML methods are becoming increasingly important to researchers and clinicians. The first objective of this review is to provide the novice reader with literacy of AI/ML methods and provide a foundation for how one might conduct an ML study. We provide a technical overview of some of the most commonly used terms, techniques, and challenges in AI/ML studies, with reference to recent studies in cardiac electrophysiology to illustrate key points. The second objective of this review is to use examples from recent literature to discuss how AI and ML are changing clinical practice and research in cardiac electrophysiology, with emphasis on disease detection and diagnosis, prediction of patient outcomes, and novel characterization of disease. The final objective is to highlight important considerations and challenges for appropriate validation, adoption, and deployment of AI technologies into clinical practice.",2020-08-01,2,1151,89,280
1228,32105592,New criteria and new methodological tools for devising criteria sets of inflammatory rheumatic diseases,"Rheumatologists use classification criteria to separate patients with inflammatory rheumatic diseases (IRD). They change over time, and the concepts of the diseases also change. The paradigm is currently moving as the goal of classification in the future will be more to select which patients may be relevant for a specific treatment rather than to describe their characteristics. Therefore, the challenge will be to reclassify multifactorial diseases on the basis of their biological mechanisms rather than their clinical phenotype. Currently, various projects are trying to reclassify diseases using bioinformatics approaches and in the near future the use of advanced machine learning algorithms with large omics datasets could lead to new classification models not only based on a clinical phenotype but also on complex biological profile and common sensitivity to targeted treatment. These models would highlight common biological pathways between patients classified in the same cluster and provide a deep understanding of the mechanisms involved in the patient's clinical phenotype. Such approaches would ultimately lead to classification models that rely more on biological causes than on symptoms. This overview on current classification of subgroups of IRD summarises the classification criteria that we use routinely, and how we will classify IRD in the future using bioinformatics and artificial intelligence techniques.",2020-08-01,0,1432,103,280
1240,32085921,Data-Driven Diagnostics and the Potential of Mobile Artificial Intelligence for Digital Therapeutic Phenotyping in Computational Psychiatry,"Data science and digital technologies have the potential to transform diagnostic classification. Digital technologies enable the collection of big data, and advances in machine learning and artificial intelligence enable scalable, rapid, and automated classification of medical conditions. In this review, we summarize and categorize various data-driven methods for diagnostic classification. In particular, we focus on autism as an example of a challenging disorder due to its highly heterogeneous nature. We begin by describing the frontier of data science methods for the neuropsychiatry of autism. We discuss early signs of autism as defined by existing pen-and-paper-based diagnostic instruments and describe data-driven feature selection techniques for determining the behaviors that are most salient for distinguishing children with autism from neurologically typical children. We then describe data-driven detection techniques, particularly computer vision and eye tracking, that provide a means of quantifying behavioral differences between cases and controls. We also describe methods of preserving the privacy of collected videos and prior efforts of incorporating humans in the diagnostic loop. Finally, we summarize existing digital therapeutic interventions that allow for data capture and longitudinal outcome tracking as the diagnosis moves along a positive trajectory. Digital phenotyping of autism is paving the way for quantitative psychiatry more broadly and will set the stage for more scalable, accessible, and precise diagnostic techniques in the field.",2020-08-01,8,1576,139,280
1278,32035758,Creating Artificial Images for Radiology Applications Using Generative Adversarial Networks (GANs) - A Systematic Review,"Rationale and objectives:                    Generative adversarial networks (GANs) are deep learning models aimed at generating fake realistic looking images. These novel models made a great impact on the computer vision field. Our study aims to review the literature on GANs applications in radiology.              Materials and methods:                    This systematic review followed the PRISMA guidelines. Electronic datasets were searched for studies describing applications of GANs in radiology. We included studies published up-to September 2019.              Results:                    Data were extracted from 33 studies published between 2017 and 2019. Eighteen studies focused on CT images generation, ten on MRI, three on PET/MRI and PET/CT, one on ultrasound and one on X-ray. Applications in radiology included image reconstruction and denoising for dose and scan time reduction (fourteen studies), data augmentation (six studies), transfer between modalities (eight studies) and image segmentation (five studies). All studies reported that generated images improved the performance of the developed algorithms.              Conclusion:                    GANs are increasingly studied for various radiology applications. They enable the creation of new data, which can be used to improve clinical care, education and research.",2020-08-01,4,1346,120,280
1315,31982357,Machine Learning With Neuroimaging: Evaluating Its Applications in Psychiatry,"Psychiatric disorders are complex, involving heterogeneous symptomatology and neurobiology that rarely involves the disruption of single, isolated brain structures. In an attempt to better describe and understand the complexities of psychiatric disorders, investigators have increasingly applied multivariate pattern classification approaches to neuroimaging data and in particular supervised machine learning methods. However, supervised machine learning approaches also come with unique challenges and trade-offs, requiring additional study design and interpretation considerations. The goal of this review is to provide a set of best practices for evaluating machine learning applications to psychiatric disorders. We discuss how to evaluate two common efforts: 1) making predictions that have the potential to aid in diagnosis, prognosis, and treatment and 2) interrogating the complex neurophysiological mechanisms underlying psychopathology. We focus here on machine learning as applied to functional connectivity with magnetic resonance imaging, as an example to ground discussion. We argue that for machine learning classification to have translational utility for individual-level predictions, investigators must ensure that the classification is clinically informative, independent of confounding variables, and appropriately assessed for both performance and generalizability. We contend that shedding light on the complex mechanisms underlying psychiatric disorders will require consideration of the unique utility, interpretability, and reliability of the neuroimaging features (e.g., regions, networks, connections) identified from machine learning approaches. Finally, we discuss how the rise of large, multisite, publicly available datasets may contribute to the utility of machine learning approaches in psychiatry.",2020-08-01,5,1832,77,280
1368,33127296,Transforming vaccine development,"The urgency to develop vaccines against Covid-19 is putting pressure on the long and expensive development timelines that are normally required for development of lifesaving vaccines. There is a unique opportunity to take advantage of new technologies, the smart and flexible design of clinical trials, and evolving regulatory science to speed up vaccine development against Covid-19 and transform vaccine development altogether.",2020-08-01,1,429,32,280
1398,32673788,Extraction of temporal relations from clinical free text: A systematic review of current approaches,"Background:                    Temporal relations between clinical events play an important role in clinical assessment and decision making. Extracting such relations from free text data is a challenging task because it lies on between medical natural language processing, temporal representation and temporal reasoning.              Objectives:                    To survey existing methods for extracting temporal relations (TLINKs) between events from clinical free text in English; to establish the state-of-the-art in this field; and to identify outstanding methodological challenges.              Methods:                    A systematic search in PubMed and the DBLP computer science bibliography was conducted for studies published between January 2006 and December 2018. The relevant studies were identified by examining the titles and abstracts. Then, the full text of selected studies was analyzed in depth and information were collected on TLINK tasks, TLINK types, data sources, features selection, methods used, and reported performance.              Results:                    A total of 2834 publications were identified for title and abstract screening. Of these publications, 51 studies were selected. Thirty-two studies used machine learning approaches, 15 studies used a hybrid approaches, and only four studies used a rule-based approach. The majority of studies use publicly available corpora: THYME (28 studies) and the i2b2 corpus (17 studies).              Conclusion:                    The performance of TLINK extraction methods ranges widely depending on relation types and events (e.g. from 32% to 87% F-score for identifying relations between clinical events and document creation time). A small set of TLINKs (before, after, overlap and contains) has been widely studied with relatively good performance, whereas other types of TLINK (e.g., started by, finished by, precedes) are rarely studied and remain challenging. Machine learning classifiers (such as Support Vector Machine and Conditional Random Fields) and Deep Neural Networks were among the best performing methods for extracting TLINKs, but nearly all the work has been carried out and tested on two publicly available corpora only. The field would benefit from the availability of more publicly available, high-quality, annotated clinical text corpora.",2020-08-01,0,2347,99,280
1399,32672961,Transfer Learning for Drug Discovery,"The data sets available to train models for in silico drug discovery efforts are often small. Indeed, the sparse availability of labeled data is a major barrier to artificial-intelligence-assisted drug discovery. One solution to this problem is to develop algorithms that can cope with relatively heterogeneous and scarce data. Transfer learning is a type of machine learning that can leverage existing, generalizable knowledge from other related tasks to enable learning of a separate task with a small set of data. Deep transfer learning is the most commonly used type of transfer learning in the field of drug discovery. This Perspective provides an overview of transfer learning and related applications to drug discovery to date. Furthermore, it provides outlooks on the future development of transfer learning for drug discovery.",2020-08-01,0,835,36,280
1475,32599515,Challenges and opportunities with CRISPR activation in bacteria for data-driven metabolic engineering,"Creating CRISPR gene activation (CRISPRa) technologies in industrially promising bacteria could be transformative for accelerating data-driven metabolic engineering and strain design. CRISPRa has been widely used in eukaryotes, but applications in bacterial systems have remained limited. Recent work shows that multiple features of bacterial promoters impose stringent requirements on CRISPRa-mediated gene activation. However, by systematically defining rules for effective bacterial CRISPRa sites and developing new approaches for encoding complex functions in engineered guide RNAs, there are now clear routes to generalize synthetic gene regulation in bacteria. When combined with multi-omics data collection and machine learning, the full development of bacterial CRISPRa will dramatically improve the ability to rapidly engineer bacteria for bioproduction through accelerated design-build-test-learn cycles.",2020-08-01,0,914,101,280
1474,32600633,Dual Energy Computed Tomography in Head and Neck Imaging: Pushing the Envelope,"Multiple applications of dual energy computed tomography (DECT) have been described for the evaluation of disorders in the head and neck, especially in oncology. We review the body of evidence suggesting advantages of DECT for the evaluation of the neck compared with conventional single energy computed tomography scans, but the full potential of DECT is still to be realized. There is early evidence suggesting significant advantages of DECT for the extraction of quantitative biomarkers using radiomics and machine learning, representing a new horizon that may enable this technology to reach its full potential.",2020-08-01,0,615,78,280
1473,32600636,Artificial Intelligence in Head and Neck Imaging: A Glimpse into the Future,"Artificial intelligence, specifically machine learning and deep learning, is a rapidly developing field in imaging sciences with the potential to improve the efficiency and effectiveness of radiologists. This review covers common technical terms and basic concepts in imaging artificial intelligence and briefly reviews the application of these techniques to general imaging as well as head and neck imaging. Artificial intelligence has the potential to contribute improvements to all areas of patient care, including image acquisition, processing, segmentation, automated detection of findings, integration of clinical information, quality improvement, and research. Numerous challenges remain, however, before widespread imaging clinical adoption and integration occur.",2020-08-01,0,771,75,280
1472,32600638,Common Data Elements in Head and Neck Radiology Reporting,"Radiologists must convert the complex information in head and neck imaging into text reports that can be understood and used by clinicians, patients, and fellow radiologists for patient care, research, and quality initiatives. Common data elements in reporting, through use of defined questions with constrained answers and terminology, allow radiologists to incorporate best practice standards and improve communication of information regardless of individual reporting style. Use of common data elements for head and neck reporting has the potential to improve outcomes, reduce errors, and transition data consumption not only for humans but future machine learning systems.",2020-08-01,0,676,57,280
1471,32600802,Theory and practical use of Bayesian methods in interpreting clinical trial data: a narrative review,"The critical reading of scientific articles is necessary for the daily practice of evidence-based medicine. Rigorous comprehension of statistical methods is essential, as reflected by the extensive use of statistics in the biomedical literature. In contrast to the customary frequentist approach, which never uses or gives the probability of a hypothesis, Bayesian theory uses probabilities for both hypotheses and data. This statistical approach is increasingly used for analyses of clinical trial data and for applied machine learning. The aim of this review is to compare general Bayesian concepts with frequentist methods to facilitate a better understanding of Bayesian theory for readers who are not familiar with this approach. The review is intended to be used in combination with a checklist we have devised for reading reports analysed by Bayesian methods. We compare and contrast the different approaches of Bayesian vs frequentist statistical methods by considering data from a clinical trial that lends itself to this comparative approach.",2020-08-01,1,1052,100,280
1404,32669685,Applications of machine learning to diagnosis and treatment of neurodegenerative diseases,"Globally, there is a huge unmet need for effective treatments for neurodegenerative diseases. The complexity of the molecular mechanisms underlying neuronal degeneration and the heterogeneity of the patient population present massive challenges to the development of early diagnostic tools and effective treatments for these diseases. Machine learning, a subfield of artificial intelligence, is enabling scientists, clinicians and patients to address some of these challenges. In this Review, we discuss how machine learning can aid early diagnosis and interpretation of medical images as well as the discovery and development of new therapies. A unifying theme of the different applications of machine learning is the integration of multiple high-dimensional sources of data, which all provide a different view on disease, and the automated derivation of actionable insights.",2020-08-01,4,876,89,280
1468,32602650,A Cancer Biologist's Primer on Machine Learning Applications in High-Dimensional Cytometry,"The application of machine learning and artificial intelligence to high-dimensional cytometry data sets has increasingly become a staple of bioinformatic data analysis over the past decade. This is especially true in the field of cancer biology, where protocols for collecting multiparameter single-cell data in a high-throughput fashion are rapidly developed. As the use of machine learning methodology in cytometry becomes increasingly common, there is a need for cancer biologists to understand the basic theory and applications of a variety of algorithmic tools for analyzing and interpreting cytometry data. We introduce the reader to several keystone machine learning-based analytic approaches with an emphasis on defining key terms and introducing a conceptual framework for making translational or clinically relevant discoveries. The target audience consists of cancer cell biologists and physician-scientists interested in applying these tools to their own data, but who may have limited training in bioinformatics.  2020 International Society for Advancement of Cytometry.",2020-08-01,5,1084,90,280
1448,32621201,Current applications of artificial intelligence for intraoperative decision support in surgery,"Research into medical artificial intelligence (AI) has made significant advances in recent years, including surgical applications. This scoping review investigated AI-based decision support systems targeted at the intraoperative phase of surgery and found a wide range of technological approaches applied across several surgical specialties. Within the twenty-one (n = 21) included papers, three main categories of motivations were identified for developing such technologies: (1) augmenting the information available to surgeons, (2) accelerating intraoperative pathology, and (3) recommending surgical steps. While many of the proposals hold promise for improving patient outcomes, important methodological shortcomings were observed in most of the reviewed papers that made it difficult to assess the clinical significance of the reported performance statistics. Despite limitations, the current state of this field suggests that a number of opportunities exist for future researchers and clinicians to work on AI for surgical decision support with exciting implications for improving surgical care.",2020-08-01,1,1102,94,280
1492,32265157,Radiomics: A primer for the radiation oncologist,"Purpose:                    Radiomics are a set of methods used to leverage medical imaging and extract quantitative features that can characterize a patient's phenotype. All modalities can be used with several different software packages. Specific informatics methods can then be used to create meaningful predictive models. In this review, we will explain the major steps of a radiomics analysis pipeline and then present the studies published in the context of radiation therapy.              Methods:                    A literature review was performed on Medline using the search engine PubMed. The search strategy included the search terms ""radiotherapy"", ""radiation oncology"" and ""radiomics"". The search was conducted in July 2019 and reference lists of selected articles were hand searched for relevance to this review.              Results:                    A typical radiomics workflow always includes five steps: imaging and segmenting, data curation and preparation, feature extraction, exploration and selection and finally modeling. In radiation oncology, radiomics studies have been published to explore different clinical outcome in lung (n=5), head and neck (n=5), esophageal (n=3), rectal (n=3), pancreatic (n=2) cancer and brain metastases (n=2). The quality of these retrospective studies is heterogeneous and their results have not been translated to the clinic.              Conclusion:                    Radiomics has a great potential to predict clinical outcome and better personalize treatment. But the field is still young and constantly evolving. Improvement in bias reduction techniques and multicenter studies will hopefully allow more robust and generalizable models.",2020-08-01,2,1702,48,280
1582,32127291,Data-Driven Approaches to Neuroimaging Analysis to Enhance Psychiatric Diagnosis and Therapy,"Combining advanced neuroimaging with novel computational methods in network science and machine learning has led to increasingly meaningful descriptions of structure and function in both the normal and the abnormal brain, thereby contributing significantly to our understanding of psychiatric disorders as circuit dysfunctions. Despite its marked potential for psychiatric care, this approach has not yet extended beyond the research setting to any clinically useful applications. Here we review current developments in the study of neuroimaging data using network models and machine learning methods, with a focus on their promise in offering a framework for clinical translation. We discuss 3 potential contributions of these methods to psychiatric care: 1) a better understanding of psychopathology beyond current diagnostic boundaries; 2) individualized prediction of treatment response and prognosis; and 3) formal theories to guide the development of novel interventions. Finally, we highlight current obstacles and sketch a forward-looking perspective of how the application of machine learning and network modeling methods should proceed to accelerate their potential transformation of clinically useful tools.",2020-08-01,2,1218,92,280
1813,32770165,Digital technologies in the public-health response to COVID-19,"Digital technologies are being harnessed to support the public-health response to COVID-19 worldwide, including population surveillance, case identification, contact tracing and evaluation of interventions on the basis of mobility data and communication with the public. These rapid responses leverage billions of mobile phones, large online datasets, connected devices, relatively low-cost computing resources and advances in machine learning and natural language processing. This Review aims to capture the breadth of digital innovations for the public-health response to COVID-19 worldwide and their limitations, and barriers to their implementation, including legal, ethical and privacy barriers, as well as organizational and workforce barriers. The future of public health is likely to become increasingly digital, and we review the need for the alignment of international strategies for the regulation, evaluation and use of digital technologies to strengthen pandemic management, and future preparedness for COVID-19 and other infectious diseases.",2020-08-01,42,1055,62,280
2025,32824342,Machine Learning Models to Predict Childhood and Adolescent Obesity: A Review,"The prevalence of childhood and adolescence overweight an obesity is raising at an alarming rate in many countries. This poses a serious threat to the current and near-future health systems, given the association of these conditions with different comorbidities (cardiovascular diseases, type II diabetes, and metabolic syndrome) and even death. In order to design appropriate strategies for its prevention, as well as understand its origins, the development of predictive models for childhood/adolescent overweight/obesity and related outcomes is of extreme value. Obesity has a complex etiology, and in the case of childhood and adolescence obesity, this etiology includes also specific factors like (pre)-gestational ones; weaning; and the huge anthropometric, metabolic, and hormonal changes that during this period the body suffers. In this way, Machine Learning models are becoming extremely useful tools in this area, given their excellent predictive power; ability to model complex, nonlinear relationships between variables; and capacity to deal with high-dimensional data typical in this area. This is especially important given the recent appearance of large repositories of Electronic Health Records (EHR) that allow the development of models using datasets with many instances and predictor variables, from which Deep Learning variants can generate extremely accurate predictions. In the current work, the area of Machine Learning models to predict childhood and adolescent obesity and related outcomes is comprehensively and critically reviewed, including the latest ones using Deep Learning with EHR. These models are compared with the traditional statistical ones that used mainly logistic regression. The main features and applications appearing from these models are described, and the future opportunities are discussed.",2020-08-01,1,1839,77,280
1817,32767134,Predicting Absenteeism and Temporary Disability Using Machine Learning: a Systematic Review and Analysis,"The main objective of this paper is to present a systematic analysis and review of the state of the art regarding the prediction of absenteeism and temporary incapacity using machine learning techniques. Moreover, the main contribution of this research is to reveal the most successful prediction models available in the literature. A systematic review of research papers published from 2010 to the present, related to the prediction of temporary disability and absenteeism in available in different research databases, is presented in this paper. The review focuses primarily on scientific databases such as Google Scholar, Science Direct, IEEE Xplore, Web of Science, and ResearchGate. A total of 58 articles were obtained from which, after removing duplicates and applying the search criteria, 18 have been included in the review. In total, 44% of the articles were published in 2019, representing a significant growth in scientific work regarding these indicators. This study also evidenced the interest of several countries. In addition, 56% of the articles were found to base their study on regression methods, 33% in classification, and 11% in grouping. After this systematic review, the efficiency and usefulness of artificial neural networks in predicting absenteeism and temporary incapacity are demonstrated. The studies regarding absenteeism and temporary disability at work are mainly conducted in Brazil and India, which are responsible for 44% of the analyzed papers followed by Saudi Arabia, and Australia which represented 22%. ANNs are the most used method in both classification and regression models representing 83% and 80% of the analyzed works, respectively. Only 10% of the literature use SVM, which is the less used method in regression models. Moreover, Nave Bayes is the less used method in classification models representing 17%.",2020-08-01,0,1858,104,280
1821,32763886,Conversational Agents in Health Care: Scoping Review and Conceptual Analysis,"Background:                    Conversational agents, also known as chatbots, are computer programs designed to simulate human text or verbal conversations. They are increasingly used in a range of fields, including health care. By enabling better accessibility, personalization, and efficiency, conversational agents have the potential to improve patient care.              Objective:                    This study aimed to review the current applications, gaps, and challenges in the literature on conversational agents in health care and provide recommendations for their future research, design, and application.              Methods:                    We performed a scoping review. A broad literature search was performed in MEDLINE (Medical Literature Analysis and Retrieval System Online; Ovid), EMBASE (Excerpta Medica database; Ovid), PubMed, Scopus, and Cochrane Central with the search terms ""conversational agents,"" ""conversational AI,"" ""chatbots,"" and associated synonyms. We also searched the gray literature using sources such as the OCLC (Online Computer Library Center) WorldCat database and ResearchGate in April 2019. Reference lists of relevant articles were checked for further articles. Screening and data extraction were performed in parallel by 2 reviewers. The included evidence was analyzed narratively by employing the principles of thematic analysis.              Results:                    The literature search yielded 47 study reports (45 articles and 2 ongoing clinical trials) that matched the inclusion criteria. The identified conversational agents were largely delivered via smartphone apps (n=23) and used free text only as the main input (n=19) and output (n=30) modality. Case studies describing chatbot development (n=18) were the most prevalent, and only 11 randomized controlled trials were identified. The 3 most commonly reported conversational agent applications in the literature were treatment and monitoring, health care service support, and patient education.              Conclusions:                    The literature on conversational agents in health care is largely descriptive and aimed at treatment and monitoring and health service support. It mostly reports on text-based, artificial intelligence-driven, and smartphone app-delivered conversational agents. There is an urgent need for a robust evaluation of diverse health care conversational agents' formats, focusing on their acceptability, safety, and effectiveness.",2020-08-01,7,2480,76,280
1823,32763220,Modern diagnostic technologies for HIV,"Novel diagnostic technologies, including nanotechnology, microfluidics, -omics science, next-generation sequencing, genomics big data, and machine learning, could contribute to meeting the UNAIDS 95-95-95 targets to end the HIV epidemic by 2030. Novel technologies include multiplexed technologies (including biomarker-based point-of-care tests and molecular platform technologies), biomarker-based combination antibody and antigen technologies, dried-blood-spot testing, and self-testing. Although biomarker-based rapid tests, in particular antibody-based tests, have dominated HIV diagnostics since the development of the first HIV test in the mid-1980s, targets such as nucleic acids and genes are now used in nanomedicine, biosensors, microfluidics, and -omics to enable early diagnosis of HIV. These novel technologies show promise as they are associated with ease of use, high diagnostic accuracy, rapid detection, and the ability to detect HIV-specific markers. Additional clinical and implementation research is needed to generate evidence for use of novel technologies and a public health approach will be required to address clinical and operational challenges to optimise their global deployment.",2020-08-01,0,1207,38,280
1568,32141423,Review: Synergy between mechanistic modelling and data-driven models for modern animal production systems in the era of big data,"Mechanistic models (MMs) have served as causal pathway analysis and 'decision-support' tools within animal production systems for decades. Such models quantitatively define how a biological system works based on causal relationships and use that cumulative biological knowledge to generate predictions and recommendations (in practice) and generate/evaluate hypotheses (in research). Their limitations revolve around obtaining sufficiently accurate inputs, user training and accuracy/precision of predictions on-farm. The new wave in digitalization technologies may negate some of these challenges. New data-driven (DD) modelling methods such as machine learning (ML) and deep learning (DL) examine patterns in data to produce accurate predictions (forecasting, classification of animals, etc.). The deluge of sensor data and new self-learning modelling techniques may address some of the limitations of traditional MM approaches - access to input data (e.g. sensors) and on-farm calibration. However, most of these new methods lack transparency in the reasoning behind predictions, in contrast to MM that have historically been used to translate knowledge into wisdom. The objective of this paper is to propose means to hybridize these two seemingly divergent methodologies to advance the models we use in animal production systems and support movement towards truly knowledge-based precision agriculture. In order to identify potential niches for models in animal production of the future, a cross-species (dairy, swine and poultry) examination of the current state of the art in MM and new DD methodologies (ML, DL analytics) is undertaken. We hypothesize that there are several ways via which synergy may be achieved to advance both our predictive capabilities and system understanding, being: (1) building and utilizing data streams (e.g. intake, rumination behaviour, rumen sensors, activity sensors, environmental sensors, cameras and near IR) to apply MM in real-time and/or with new resolution and capabilities; (2) hybridization of MM and DD approaches where, for example, a ML framework is augmented by MM-generated parameters or predicted outcomes and (3) hybridization of the MM and DD approaches, where biological bounds are placed on parameters within a MM framework, and the DD system parameterizes the MM for individual animals, farms or other such clusters of data. As animal systems modellers, we should expand our toolbox to explore new DD approaches and big data to find opportunities to increase understanding of biological systems, find new patterns in data and move the field towards intelligent, knowledge-based precision agriculture systems.",2020-08-01,1,2667,128,280
2024,32825520,"The Role of EEG in the Diagnosis, Prognosis and Clinical Correlations of Dementia with Lewy Bodies-A Systematic Review","Despite improvements in diagnostic criteria for dementia with Lewy bodies (DLB), the ability to discriminate DLB from Alzheimer's disease (AD) and other dementias remains suboptimal. Electroencephalography (EEG) is currently a supportive biomarker in the diagnosis of DLB. We performed a systematic review to better clarify the diagnostic and prognostic role of EEG in DLB and define the clinical correlates of various EEG features described in DLB. MEDLINE, EMBASE, and PsycINFO were searched using search strategies for relevant articles up to 6 August 2020. We included 43 studies comparing EEG in DLB with other diagnoses, 42 of them included a comparison of DLB with AD, 10 studies compared DLB with Parkinson's disease dementia, and 6 studies compared DLB with other dementias. The studies were visual EEG assessment (6), quantitative EEG (35) and event-related potential studies (2). The most consistent observation was the slowing of the dominant EEG rhythm (<8 Hz) assessed visually or through quantitative EEG, which was observed in ~90% of patients with DLB and only ~10% of patients with AD. Other findings based on qualitative rating, spectral power analyses, connectivity, microstate and machine learning algorithms were largely heterogenous due to differences in study design, EEG acquisition, preprocessing and analysis. EEG protocols should be standardized to allow replication and validation of promising EEG features as potential biomarkers in DLB.",2020-08-01,1,1467,118,280
2022,32827173,Low-Field MRI of Stroke: Challenges and Opportunities,"Stroke is a leading cause of death and disability worldwide. The reasons for increased stroke burden in developing countries are inadequately controlled risk factors resulting from poor public awareness and inadequate infrastructure. Computed tomography and MRI are common neuroimaging modalities used to assess stroke with diffusion-weighted MRI, in particular, being the recommended choice for acute stroke imaging. However, access to these imaging modalities is primarily restricted to major cities and high-income groups. In the case of stroke, the time-window of treatment to limit the damage is of a few hours and needs a point-of-care diagnosis. A low-cost MR system typically achieved at the ultra-low- and very-low-field would meet the need for a geographically accessible and portable solution. We review studies focused on accessible stroke imaging and recent developments in MR methodologies, including hardware, to image at low fields. We hypothesize that in the absence of a formal, rapid stroke triaging system, the value of timely on-site delivery of the scanner to the stroke patient can be significant. To this end, we discuss multiple recent hardware and methods developments in the low-field regime. Our review suggests a compelling need to explore further the trade-offs between high signal, contrast, and accessibility at low fields in low-income communities. LEVEL OF EVIDENCE: 4 TECHNICAL EFFICACY STAGE: 6.",2020-08-01,0,1431,53,280
1831,32740678,Utility of Artificial Intelligence Amidst the COVID 19 Pandemic: A Review,"The term machine learning refers to a collection of tools used for identifying patterns in data. As opposed to traditional methods of pattern identification, machine learning tools relies on artificial intelligence to map out patters from large amounts of data, can self-improve as and when new data becomes available and is quicker in accomplishing these tasks. This review describes various techniques of machine learning that have been used in the past in the prediction, detection and management of infectious diseases, and how these tools are being brought into the battle against COVID-19. In addition, we also discuss their applications in various stages of the pandemic, the advantages, disadvantages and possible pit falls.",2020-08-01,2,732,73,280
1833,32738777,Machine learning in quantitative PET: A review of attenuation correction and low-count image reconstruction methods,"The rapid expansion of machine learning is offering a new wave of opportunities for nuclear medicine. This paper reviews applications of machine learning for the study of attenuation correction (AC) and low-count image reconstruction in quantitative positron emission tomography (PET). Specifically, we present the developments of machine learning methodology, ranging from random forest and dictionary learning to the latest convolutional neural network-based architectures. For application in PET attenuation correction, two general strategies are reviewed: 1) generating synthetic CT from MR or non-AC PET for the purposes of PET AC, and 2) direct conversion from non-AC PET to AC PET. For low-count PET reconstruction, recent deep learning-based studies and the potential advantages over conventional machine learning-based methods are presented and discussed. In each application, the proposed methods, study designs and performance of published studies are listed and compared with a brief discussion. Finally, the overall contributions and remaining challenges are summarized.",2020-08-01,3,1083,115,280
1967,32933862,Big Data Solutions for Controversies in Breast Cancer Treatment,"The digital world of data is expanding with an annual growth rate of 40%, and health care is among the fastest growing sector of the digital world with an annual growth rate of 48%. Rapid growth in technology has augmented data generation; for example, electronic health records produce huge amounts of patient-level data, whereas national registries capture information on numerous factors affecting health care delivery and patient outcomes. This big data can be utilized to improve health care outcomes. This review discusses relevant applications in breast cancer treatment.",2020-08-01,0,578,63,280
1840,32728877,Artificial intelligence in radiotherapy: a technological review,"Radiation therapy (RT) is widely used to treat cancer. Technological advances in RT have occurred in the past 30 years. These advances, such as three-dimensional image guidance, intensity modulation, and robotics, created challenges and opportunities for the next breakthrough, in which artificial intelligence (AI) will possibly play important roles. AI will replace certain repetitive and labor-intensive tasks and improve the accuracy and consistency of others, particularly those with increased complexity because of technological advances. The improvement in efficiency and consistency is important to manage the increasing cancer patient burden to the society. Furthermore, AI may provide new functionalities that facilitate satisfactory RT. The functionalities include superior images for real-time intervention and adaptive and personalized RT. AI may effectively synthesize and analyze big data for such purposes. This review describes the RT workflow and identifies areas, including imaging, treatment planning, quality assurance, and outcome prediction, that benefit from AI. This review primarily focuses on deep-learning techniques, although conventional machine-learning techniques are also mentioned.",2020-08-01,0,1215,63,280
2026,32823322,From Patient Engagement to Precision Oncology: Leveraging Informatics to Advance Cancer Care,"Objectives:                    Conduct a survey of the literature for advancements in cancer informatics over the last three years in three specific areas where there has been unprecedented growth: 1) digital health; 2) machine learning; and 3) precision oncology. We also highlight the ethical implications and future opportunities within each area.              Methods:                    A search was conducted over a three-year period in two electronic databases (PubMed, Google Scholar) to identify peer-reviewed articles and conference proceedings. Search terms included variations of the following: neoplasms[MeSH], informatics[MeSH], cancer, oncology, clinical cancer informatics, medical cancer informatics. The search returned too many articles for practical review (23,994 from PubMed and 23,100 from Google Scholar). Thus, we conducted searches of key PubMed-indexed informatics journals and proceedings. We further limited our search to manuscripts that demonstrated a clear focus on clinical or translational cancer informatics. Manuscripts were then selected based on their methodological rigor, scientific impact, innovation, and contribution towards cancer informatics as a field or on their impact on cancer care and research.              Results:                    Key developments and opportunities in cancer informatics research in the areas of digital health, machine learning, and precision oncology were summarized.              Conclusion:                    While there are numerous innovations in the field of cancer informatics to advance prevention and clinical care, considerable challenges remain related to data sharing and privacy, digital accessibility, and algorithm biases and interpretation. The implementation and application of these findings in cancer care necessitates further consideration and research.",2020-08-01,1,1848,92,280
1973,32922515,Using artificial intelligence for improving stroke diagnosis in emergency departments: a practical framework,"Stroke is the fifth leading cause of death in the United States and a major cause of severe disability worldwide. Yet, recognizing the signs of stroke in an acute setting is still challenging and leads to loss of opportunity to intervene, given the narrow therapeutic window. A decision support system using artificial intelligence (AI) and clinical data from electronic health records combined with patients' presenting symptoms can be designed to support emergency department providers in stroke diagnosis and subsequently reduce the treatment delay. In this article, we present a practical framework to develop a decision support system using AI by reflecting on the various stages, which could eventually improve patient care and outcome. We also discuss the technical, operational, and ethical challenges of the process.",2020-08-01,1,825,108,280
1843,32728518,Smart diagnostics devices through artificial intelligence and mechanobiological approaches,"The present work illustrates the promising intervention of smart diagnostics devices through artificial intelligence (AI) and mechanobiological approaches in health care practices. The artificial intelligence and mechanobiological approaches in diagnostics widen the scope for point of care techniques for the timely revealing of diseases by understanding the biomechanical properties of the tissue of interest. Smart diagnostic device senses the physical parameters due to change in mechanical, biological, and luidic properties of the cells and to control these changes, supply the necessary drugs immediately using AI techniques. The latest techniques like sweat diagnostics to measure the overall health, Photoplethysmography (PPG) for real-time monitoring of pulse waveform by capturing the reflected signal due to blood pulsation), Micro-electromechanical systems (MEMS) and Nano-electromechanical systems (NEMS) smart devices to detect disease at its early stage, lab-on-chip and organ-on-chip technologies, Ambulatory Circadian Monitoring device (ACM), a wrist-worn device for Parkinson's disease have been discussed. The recent and futuristic smart diagnostics tool/techniques like emotion recognition by applying machine learning algorithms, atomic force microscopy that measures the fibrinogen and erythrocytes binding force, smartphone-based retinal image analyser system, image-based computational modeling for various neurological disorders, cardiovascular diseases, tuberculosis, predicting and preventing of Zika virus, optimal drugs and doses for HIV using AI, etc. have been reviewed. The objective of this review is to examine smart diagnostics devices based on artificial intelligence and mechanobiological approaches, with their medical applications in healthcare. This review determines that smart diagnostics devices have potential applications in healthcare, but more research work will be essential for prospective accomplishments of this technology.",2020-08-01,0,1975,90,280
2015,32852654,AI (Artificial Intelligence) and Hypertension Research,"Purpose of review:                    This review a highlights that to use artificial intelligence (AI) tools effectively for hypertension research, a new foundation to further understand the biology of hypertension needs to occur by leveraging genome and RNA sequencing technology and derived tools on a broad scale in hypertension.              Recent findings:                    For the last few years, progress in research and management of essential hypertension has been stagnating while at the same time, the sequencing of the human genome has been generating many new research tools and opportunities to investigate the biology of hypertension. Cancer research has applied modern tools derived from DNA and RNA sequencing on a large scale, enabling the improved understanding of cancer biology and leading to many clinical applications. Compared with cancer, studies in hypertension, using whole genome, exome, or RNA sequencing tools, total less than 2% of the number cancer studies. While true, sequencing the genome of cancer tissue has provided cancer research an advantage, DNA and RNA sequencing derived tools can also be used in hypertension to generate new understanding how complex protein network, in non-cancer tissue, adapts and learns to be effective when for example, somatic mutations or environmental inputs change the gene expression profiles at different network nodes. The amount of data and differences in clinical condition classification at the individual sample level might be of such magnitude to overwhelm and stretch comprehension. Here is the opportunity to use AI tools for the analysis of data streams derived from DNA and RNA sequencing tools combined with clinical data to generate new hypotheses leading to the discovery of mechanisms and potential target molecules from which drugs or treatments can be developed and tested. Basic and clinical research taking advantage of new gene sequencing-based tools, to uncover mechanisms how complex protein networks regulate blood pressure in health and disease, will be critical to lift hypertension research and management from its stagnation. The use of AI analytic tools will help leverage such insights. However, applying AI tools to vast amounts of data that certainly exist in hypertension, without taking advantage of new gene sequencing-based research tools, will generate questionable results and will miss many new potential molecular targets and possibly treatments. Without such approaches, the vision of precision medicine for hypertension will be hard to accomplish and most likely not occur in the near future.",2020-08-01,0,2609,54,280
2014,32854412,Wearable Activity Trackers in the Management of Rheumatic Diseases: Where Are We in 2020?,"In healthcare, physical activity can be monitored in two ways: self-monitoring by the patient himself or external monitoring by health professionals. Regarding self-monitoring, wearable activity trackers allow automated passive data collection that educate and motivate patients. Wearing an activity tracker can improve walking time by around 1500 steps per day. However, there are concerns about measurement accuracy (e.g., lack of a common validation protocol or measurement discrepancies between different devices). For external monitoring, many innovative electronic tools are currently used in rheumatology to help support physician time management, to reduce the burden on clinic time, and to prioritize patients who may need further attention. In inflammatory arthritis, such as rheumatoid arthritis, regular monitoring of patients to detect disease flares improves outcomes. In a pilot study applying machine learning to activity tracker steps, we showed that physical activity was strongly linked to disease flares and that patterns of physical activity could be used to predict flares with great accuracy, with a sensitivity and specificity above 95%. Thus, automatic monitoring of steps may lead to improved disease control through potential early identification of disease flares. However, activity trackers have some limitations when applied to rheumatic patients, such as tracker adherence, lack of clarity on long-term effectiveness, or the potential multiplicity of trackers.",2020-08-01,0,1491,89,280
2012,32862251,Deep Learning in Radiation Oncology Treatment Planning for Prostate Cancer: A Systematic Review,"Radiation oncology for prostate cancer is important as it can decrease the morbidity and mortality associated with this disease. Planning for this modality of treatment is both fundamental, time-consuming and prone to human-errors, leading to potentially avoidable delays in start of treatment. A fundamental step in radiotherapy planning is contouring of radiation targets, where medical specialists contouring, i.e., segment, the boundaries of the structures to be irradiated. Automating this step can potentially lead to faster treatment planning without a decrease in quality, while increasing time available to physicians and also more consistent treatment results. This can be framed as an image segmentation task, which has been studied for many decades in the fields of Computer Vision and Machine Learning. With the advent of Deep Learning, there have been many proposals for different network architectures achieving high performance levels. In this review, we searched the literature for those methods and describe them briefly, grouping those based on Computed Tomography (CT) or Magnetic Resonance Imaging (MRI). This is a booming field, evidenced by the date of the publications found. However, most publications use data from a very limited number of patients, which presents an obstacle to deep learning models training. Although the performance of the models has achieved very satisfactory results, there is still room for improvement, and there is arguably a long way before these models can be used safely and effectively in clinical practice.",2020-08-01,0,1562,95,280
2010,32864782,AI-Enhanced Diagnosis of Challenging Lesions in Breast MRI: A Methodology and Application Primer,"Computer-aided diagnosis (CAD) systems have become an important tool in the assessment of breast tumors with magnetic resonance imaging (MRI). CAD systems can be used for the detection and diagnosis of breast tumors as a ""second opinion"" review complementing the radiologist's review. CAD systems have many common parts, such as image preprocessing, tumor feature extraction, and data classification that are mostly based on machine-learning (ML) techniques. In this review article, we describe applications of ML-based CAD systems in MRI covering the detection of diagnostically challenging lesions of the breast such as nonmass enhancing (NME) lesions, and furthermore discuss how multiparametric MRI and radiomics can be applied to the study of NME, including prediction of response to neoadjuvant chemotherapy (NAC). Since ML has been widely used in the medical imaging community, we provide an overview about the state-of-the-art and novel techniques applied as classifiers to CAD systems. The differences in the CAD systems in MRI of the breast for several standard and novel applications for NME are explained in detail to provide important examples, illustrating: 1) CAD for detection and diagnosis, 2) CAD in multiparametric imaging, 3) CAD in NAC, and 4) breast cancer radiomics. We aim to provide a comparison between these CAD applications and to illustrate a global view on intelligent CAD systems based on machine and deep learning in MRI of the breast. LEVEL OF EVIDENCE: 2 TECHNICAL EFFICACY STAGE: 2.",2020-08-01,0,1517,96,280
2009,32866134,Machine learning (ML) for the diagnosis of autism spectrum disorder (ASD) using brain imaging,"Autism spectrum disorder (ASD) is a neurodevelopmental incurable disorder with a long diagnostic period encountered in the early years of life. If diagnosed early, the negative effects of this disease can be reduced by starting special education early. Machine learning (ML), an increasingly ubiquitous technology, can be applied for the early diagnosis of ASD. The aim of this study is to examine and provide a comprehensive state-of-the-art review of ML research for the diagnosis of ASD based on (a) structural magnetic resonance image (MRI), (b) functional MRI and (c) hybrid imaging techniques over the past decade. The accuracy of the studies with a large number of participants is in general lower than those with fewer participants leading to the conclusion that further large-scale studies are needed. An examination of the age of the participants shows that the accuracy of the automated diagnosis of ASD is higher at a younger age range. ML technology is expected to contribute significantly to the early and rapid diagnosis of ASD in the coming years and become available to clinicians in the near future. This review is aimed to facilitate that.",2020-08-01,1,1158,93,280
2005,32872562,Proteomics and Metabolomics Approaches towards a Functional Insight onto AUTISM Spectrum Disorders: Phenotype Stratification and Biomarker Discovery,"Autism spectrum disorders (ASDs) are neurodevelopmental disorders characterized by behavioral alterations and currently affect about 1% of children. Significant genetic factors and mechanisms underline the causation of ASD. Indeed, many affected individuals are diagnosed with chromosomal abnormalities, submicroscopic deletions or duplications, single-gene disorders or variants. However, a range of metabolic abnormalities has been highlighted in many patients, by identifying biofluid metabolome and proteome profiles potentially usable as ASD biomarkers. Indeed, next-generation sequencing and other omics platforms, including proteomics and metabolomics, have uncovered early age disease biomarkers which may lead to novel diagnostic tools and treatment targets that may vary from patient to patient depending on the specific genomic and other omics findings. The progressive identification of new proteins and metabolites acting as biomarker candidates, combined with patient genetic and clinical data and environmental factors, including microbiota, would bring us towards advanced clinical decision support systems (CDSSs) assisted by machine learning models for advanced ASD-personalized medicine. Herein, we will discuss novel computational solutions to evaluate new proteome and metabolome ASD biomarker candidates, in terms of their recurrence in the reviewed literature and laboratory medicine feasibility. Moreover, the way to exploit CDSS, performed by artificial intelligence, is presented as an effective tool to integrate omics data to electronic health/medical records (EHR/EMR), hopefully acting as added value in the near future for the clinical management of ASD.",2020-08-01,5,1685,148,280
2002,32873989,Evaluation-oriented exploration of photo energy conversion systems: from fundamental optoelectronics and material screening to the combination with data science,"Light is a form of energy that can be converted to electric and chemical energies. Thus, organic photovoltaics (OPVs), perovskite solar cells (PSCs), photocatalysts, and photodetectors have evolved as scientific and commercial enterprises. However, the complex photochemical reactions and multicomponent materials involved in these systems have hampered rapid progress in their fundamental understanding and material design. This review showcases the evaluation-oriented exploration of photo energy conversion materials by using electrodeless time-resolved microwave conductivity (TRMC) and materials informatics (MI). TRMC with its unique options (excitation sources, environmental control, frequency modulation, etc.) provides not only accelerated experimental screening of OPV and PSC materials but also a versatile route toward shedding light on their charge carrier dynamics. Furthermore, MI powered by machine learning is shown to allow extremely high-throughput exploration in the large molecular space, which is compatible with experimental screening and combinatorial synthesis.",2020-08-01,0,1087,160,280
2001,32884217,Current understanding of the metabolism of micronutrients in chronic alcoholic liver disease,"Alcoholic liver disease (ALD) remains an important health problem worldwide. Perturbation of micronutrients has been broadly reported to be a common characteristic in patients with ALD, given the fact that micronutrients often act as composition or coenzymes of many biochemical enzymes responsible for the inflammatory response, oxidative stress, and cell proliferation. Mapping the metabolic pattern and the function of these micronutrients is a prerequisite before targeted intervention can be delivered in clinical practice. Recent years have registered a significant improvement in our understanding of the role of micronutrients on the pathogenesis and progression of ALD. However, how and to what extent these micronutrients are involved in the pathophysiology of ALD remains largely unknown. In the current study, we provide a review of recent studies that investigated the imbalance of micronutrients in patients with ALD with a focus on zinc, iron, copper, magnesium, selenium, vitamin D and vitamin E, and determine how disturbances in micronutrients relates to the pathophysiology of ALD. Overall, zinc, selenium, vitamin D, and vitamin E uniformly exhibited a deficiency, and iron demonstrated an elevated trend. While for copper, both an elevation and deficiency were observed from existing literature. More importantly, we also highlight several challenges in terms of low sample size, study design discrepancies, sample heterogeneity across studies, and the use of machine learning approaches.",2020-08-01,0,1509,92,280
1995,32903110,Using Bayesian networks to clarify interpretation of exposure-response regression coefficients: blood lead-mortality association as an example,"We examine how Bayesian network (BN) learning and analysis methods can help to meet several methodological challenges that arise in interpreting significant regression coefficients in exposure-response regression modeling. As a motivating example, we consider the challenge of interpreting positive regression coefficients for blood lead level (BLL) as a predictor of mortality risk for nonsmoking men. We first note that practices such as dichotomizing or categorizing continuous confounders (e.g. income), omitting potentially important socioeconomic confounders (e.g. education), and assuming specific parametric regression model forms leave unclear to what extent a positive regression coefficient reflects these modeling choices, rather than a direct dependence of mortality risk on exposure. Therefore, significant exposure-response coefficients in parametric regression models do not necessarily reveal the extent to which reducing exposure-related variables (e.g. BLL) alone, while leaving fixed other correlates of exposure and mortality risks (e.g. education, income, etc.) would reduce adverse outcome risks (e.g. mortality risks). We then consider how BN structure-learning and inference algorithms and nonparametric estimation methods (partial dependence plots) can be used to clarify dependencies between variables, variable selection, confounding, and quantification of joint effects of multiple factors on risk, including possible high-order interactions and nonlinearities. We conclude that these details must be carefully modeled to determine whether a data set provides evidence that exposure itself directly affects risks; and that BN and nonparametric effect estimation and uncertainty quantification methods can complement regression modeling and help to improve the scientific basis for risk management decisions and policy-making by addressing these issues.",2020-08-01,0,1881,142,280
1868,32694345,Smartphone-Based Fundus Imaging-Where Are We Now?,"With the advent of smartphone-based fundus imaging (SBFI), a low-cost alternative to conventional digital fundus photography has become available. SBFI allows for a mobile fundus examination, is applicable both with and without pupil dilation, comes with built-in connectivity and post-processing capabilities, and is relatively easy to master. Furthermore, it is delegable to paramedical staff/technicians and, hence, suitable for telemedicine. Against this background a variety of SBFI applications have become available including screening for diabetic retinopathy, glaucoma, and retinopathy of prematurity and its applications in emergency medicine and pediatrics. In addition, SBFI is convenient for teaching purposes and might serve as a surrogate for direct ophthalmoscopy. First wide-field montage techniques are available and the combination of SBFI with machine learning algorithms for image analyses is promising. In conclusion, SBFI has the potential to make fundus examinations and screenings for patients particularly in low- and middle-income settings more accessible and, therefore, aid tackling the burden of diabetic retinopathy, glaucoma, and retinopathy of prematurity screening. However, image quality for SBFI varies substantially and a reference standard for grading appears prudent. In addition, there is a strong need for comparison of different SBFI approaches in terms of applicability to disease screening and cost-effectiveness.",2020-08-01,1,1457,49,280
1993,32904333,Conceptualising Artificial Intelligence as a Digital Healthcare Innovation: An Introductory Review,"Artificial intelligence (AI) is widely recognised as a transformative innovation and is already proving capable of outperforming human clinicians in the diagnosis of specific medical conditions, especially in image analysis within dermatology and radiology. These abilities are enhanced by the capacity of AI systems to learn from patient records, genomic information and real-time patient data. Uses of AI range from integrating with robotics to creating training material for clinicians. Whilst AI research is mounting, less attention has been paid to the practical implications on healthcare services and potential barriers to implementation. AI is recognised as a ""Software as a Medical Device (SaMD)"" and is increasingly becoming a topic of interest for regulators. Unless the introduction of AI is carefully considered and gradual, there are risks of automation bias, overdependence and long-term staffing problems. This is in addition to already well-documented generic risks associated with AI, such as data privacy, algorithmic biases and corrigibility. AI is able to potentiate innovations which preceded it, using Internet of Things, digitisation of patient records and genetic data as data sources. These synergies are important in both realising the potential of AI and utilising the potential of the data. As machine learning systems begin to cross-examine an array of databases, we must ensure that clinicians retain autonomy over the diagnostic process and understand the algorithmic processes generating diagnoses. This review uses established management literature to explore artificial intelligence as a digital healthcare innovation and highlight potential risks and opportunities.",2020-08-01,1,1701,98,280
1841,32728875,Deep learning in digital pathology image analysis: a survey,"Deep learning (DL) has achieved state-of-the-art performance in many digital pathology analysis tasks. Traditional methods usually require hand-crafted domain-specific features, and DL methods can learn representations without manually designed features. In terms of feature extraction, DL approaches are less labor intensive compared with conventional machine learning methods. In this paper, we comprehensively summarize recent DL-based image analysis studies in histopathology, including different tasks (e.g., classification, semantic segmentation, detection, and instance segmentation) and various applications (e.g., stain normalization, cell/gland/region structure analysis). DL methods can provide consistent and accurate outcomes. DL is a promising tool to assist pathologists in clinical diagnosis.",2020-08-01,1,808,59,280
2027,32823318,Medical Information Extraction in the Age of Deep Learning,"Objectives:                    We survey recent developments in medical Information Extraction (IE) as reported in the literature from the past three years. Our focus is on the fundamental methodological paradigm shift from standard Machine Learning (ML) techniques to Deep Neural Networks (DNNs). We describe applications of this new paradigm concentrating on two basic IE tasks, named entity recognition and relation extraction, for two selected semantic classes-diseases and drugs (or medications)-and relations between them.              Methods:                    For the time period from 2017 to early 2020, we searched for relevant publications from three major scientific communities: medicine and medical informatics, natural language processing, as well as neural networks and artificial intelligence.              Results:                    In the past decade, the field of Natural Language Processing (NLP) has undergone a profound methodological shift from symbolic to distributed representations based on the paradigm of Deep Learning (DL). Meanwhile, this trend is, although with some delay, also reflected in the medical NLP community. In the reporting period, overwhelming experimental evidence has been gathered, as illustrated in this survey for medical IE, that DL-based approaches outperform non-DL ones by often large margins. Still, small-sized and access-limited corpora create intrinsic problems for data-greedy DL as do special linguistic phenomena of medical sublanguages that have to be overcome by adaptive learning strategies.              Conclusions:                    The paradigm shift from (feature-engineered) ML to DNNs changes the fundamental methodological rules of the game for medical NLP. This change is by no means restricted to medical IE but should also deeply influence other areas of medical informatics, either NLP- or non-NLP-based.",2020-08-01,2,1884,58,280
1987,32908491,Machine Learning in Neuroimaging: A New Approach to Understand Acupuncture for Neuroplasticity,"The effects of acupuncture facilitating neural plasticity for treating diseases have been identified by clinical and experimental studies. In the last two decades, the application of neuroimaging techniques in acupuncture research provided visualized evidence for acupuncture promoting neuroplasticity. Recently, the integration of machine learning (ML) and neuroimaging techniques becomes a focus in neuroscience and brings a new and promising approach to understand the facilitation of acupuncture on neuroplasticity at the individual level. This review is aimed at providing an overview of this rapidly growing field by introducing the commonly used ML algorithms in neuroimaging studies briefly and analyzing the characteristics of the acupuncture studies based on ML and neuroimaging, so as to provide references for future research.",2020-08-01,0,838,94,280
1799,32785796,"Radiomics in medical imaging-""how-to"" guide and critical reflection","Radiomics is a quantitative approach to medical imaging, which aims at enhancing the existing data available to clinicians by means of advanced mathematical analysis. Through mathematical extraction of the spatial distribution of signal intensities and pixel interrelationships, radiomics quantifies textural information by using analysis methods from the field of artificial intelligence. Various studies from different fields in imaging have been published so far, highlighting the potential of radiomics to enhance clinical decision-making. However, the field faces several important challenges, which are mainly caused by the various technical factors influencing the extracted radiomic features.The aim of the present review is twofold: first, we present the typical workflow of a radiomics analysis and deliver a practical ""how-to"" guide for a typical radiomics analysis. Second, we discuss the current limitations of radiomics, suggest potential improvements, and summarize relevant literature on the subject.",2020-08-01,10,1016,67,280
1794,32797983,Use of Machine Learning and Artificial Intelligence to Drive Personalized Medicine Approaches for Spine Care,"Personalized medicine is a new paradigm of healthcare in which interventions are based on individual patient characteristics rather than on ""one-size-fits-all"" guidelines. As epidemiological datasets continue to burgeon in size and complexity, powerful methods such as statistical machine learning and artificial intelligence (AI) become necessary to interpret and develop prognostic models from underlying data. Through such analysis, machine learning can be used to facilitate personalized medicine via its precise predictions. Additionally, other AI tools, such as natural language processing and computer vision, can play an instrumental part in personalizing the care provided to patients with spine disease. In the present report, we discuss the current strides made in incorporating AI into research on spine disease, especially traumatic spinal cord injury and degenerative spine disease. We describe studies using AI to build accurate prognostic models, extract important information from medical reports via natural language processing, and evaluate functional status in a granular manner using computer vision. Through a case illustration, we have demonstrated how these breakthroughs can facilitate an increased role for more personalized medicine and, thus, change the landscape of spine care.",2020-08-01,0,1306,108,280
1795,32797044,Machine Learning based histology phenotyping to investigate the epidemiologic and genetic basis of adipocyte morphology and cardiometabolic traits,"Genetic studies have recently highlighted the importance of fat distribution, as well as overall adiposity, in the pathogenesis of obesity-associated diseases. Using a large study (n = 1,288) from 4 independent cohorts, we aimed to investigate the relationship between mean adipocyte area and obesity-related traits, and identify genetic factors associated with adipocyte cell size. To perform the first large-scale study of automatic adipocyte phenotyping using both histological and genetic data, we developed a deep learning-based method, the Adipocyte U-Net, to rapidly derive mean adipocyte area estimates from histology images. We validate our method using three state-of-the-art approaches; CellProfiler, Adiposoft and floating adipocytes fractions, all run blindly on two external cohorts. We observe high concordance between our method and the state-of-the-art approaches (Adipocyte U-net vs. CellProfiler: R2visceral = 0.94, P < 2.2  10-16, R2subcutaneous = 0.91, P < 2.2  10-16), and faster run times (10,000 images: 6mins vs 3.5hrs). We applied the Adipocyte U-Net to 4 cohorts with histology, genetic, and phenotypic data (total N = 820). After meta-analysis, we found that mean adipocyte area positively correlated with body mass index (BMI) (Psubq = 8.13  10-69, subq = 0.45; Pvisc = 2.5  10-55, visc = 0.49; average R2 across cohorts = 0.49) and that adipocytes in subcutaneous depots are larger than their visceral counterparts (Pmeta = 9.8  10-7). Lastly, we performed the largest GWAS and subsequent meta-analysis of mean adipocyte area and intra-individual adipocyte variation (N = 820). Despite having twice the number of samples than any similar study, we found no genome-wide significant associations, suggesting that larger sample sizes and a homogenous collection of adipose tissue are likely needed to identify robust genetic associations.",2020-08-01,0,1872,146,280
1807,32778984,Challenge-Enabled Machine Learning to Drug-Response Prediction,"In recent decades, the advancement of computational algorithms and the availability of big data have enabled artificial intelligence (AI) to dramatically improve predictive performance in nearly all research areas. Specifically, machine learning (ML) techniques, a major branch of AI, have been widely used in many tasks of drug discovery and development, including predicting treatment effects, identifying target genes and functional pathways, as well as selecting potential biomarkers. However, in practice, blindly applying ML methods may lead to common pitfalls, including overfitting and lack of generalizability. Therefore, how to improve the robustness and prediction accuracy of ML methods has become a crucial problem for researchers. In this review, we summarize the application of ML models to drug discovery by introducing the top-performing methods developed from large-scale drug-related data challenges in recent years.",2020-08-01,0,935,62,280
2030,32823311,Design and Use of Semantic Resources: Findings from the Section on Knowledge Representation and Management of the 2020 International Medical Informatics Association Yearbook,"Objective:                    To select, present, and summarize the best papers in the field of Knowledge Representation and Management (KRM) published in 2019.              Methods:                    A comprehensive and standardized review of the biomedical informatics literature was performed to select the most interesting papers of KRM published in 2019, based on PubMed and ISI Web Of Knowledge queries.              Results:                    Four best papers were selected among 1,189 publications retrieved, following the usual International Medical Informatics Association Yearbook reviewing process. In 2019, research areas covered by pre-selected papers were represented by the design of semantic resources (methods, visualization, curation) and the application of semantic representations for the integration/enrichment of biomedical data. Besides new ontologies and sound methodological guidance to rethink knowledge bases design, we observed large scale applications, promising results for phenotypes characterization, semantic-aware machine learning solutions for biomedical data analysis, and semantic provenance information representations for scientific reproducibility evaluation.              Conclusion:                    In the KRM selection for 2019, research on knowledge representation demonstrated significant contributions both in the design and in the application of semantic resources. Semantic representations serve a great variety of applications across many medical domains, with actionable results.",2020-08-01,0,1535,173,280
1791,32802024,Generative Adversarial Network Technologies and Applications in Computer Vision,"Computer vision is one of the hottest research fields in deep learning. The emergence of generative adversarial networks (GANs) provides a new method and model for computer vision. The idea of GANs using the game training method is superior to traditional machine learning algorithms in terms of feature learning and image generation. GANs are widely used not only in image generation and style transfer but also in the text, voice, video processing, and other fields. However, there are still some problems with GANs, such as model collapse and uncontrollable training. This paper deeply reviews the theoretical basis of GANs and surveys some recently developed GAN models, in comparison with traditional GAN models. The applications of GANs in computer vision include data enhancement, domain transfer, high-quality sample generation, and image restoration. The latest research progress of GANs in artificial intelligence (AI) based security attack and defense is introduced. The future development of GANs in computer vision is also discussed at the end of the paper with possible applications of AI in computer vision.",2020-08-01,1,1122,79,280
1789,32803154,Artificial Intelligence: A Primer for Breast Imaging Radiologists,"Artificial intelligence (AI) is a branch of computer science dedicated to developing computer algorithms that emulate intelligent human behavior. Subfields of AI include machine learning and deep learning. Advances in AI technologies have led to techniques that could increase breast cancer detection, improve clinical efficiency in breast imaging practices, and guide decision-making regarding screening and prevention strategies. This article reviews key terminology and concepts, discusses common AI models and methods to validate and evaluate these models, describes emerging AI applications in breast imaging, and outlines challenges and future directions. Familiarity with AI terminology, concepts, methods, and applications is essential for breast imaging radiologists to critically evaluate these emerging technologies, recognize their strengths and limitations, and ultimately ensure optimal patient care.",2020-08-01,0,914,65,280
2029,32823315,Contributions from the 2019 Literature on Bioinformatics and Translational Informatics,"Objectives:                    Summarize recent research and select the best papers published in 2019 in the field of Bioinformatics and Translational Informatics (BTI) for the corresponding section of the International Medical Informatics Association Yearbook.              Methods:                    A literature review was performed for retrieving from PubMed papers indexed with keywords and free terms related to BTI. Independent review allowed the section editors to select a list of 15 candidate best papers which were subsequently peer-reviewed. A final consensus meeting gathering the whole Yearbook editorial committee was organized to finally decide on the selection of the best papers.              Results:                    Among the 931 retrieved papers covering the various subareas of BTI, the review process selected four best papers. The first paper presents a logical modeling of cancer pathways. Using their tools, the authors are able to identify two known behaviours of tumors. The second paper describes a deep-learning approach to predicting resistance to antibiotics in Mycobacterium tuberculosis. The authors of the third paper introduce a Genomic Global Positioning System (GPS) enabling comparison of genomic data with other individuals or genomics databases while preserving privacy. The fourth paper presents a multi-omics and temporal sequence-based approach to provide a better understanding of the sequence of events leading to Alzheimer's Disease.              Conclusions:                    Thanks to the normalization of open data and open science practices, research in BTI continues to develop and mature. Noteworthy achievements are sophisticated applications of leading edge machine-learning methods dedicated to personalized medicine.",2020-08-01,0,1779,86,280
2044,33733182,Artificial Intelligence for COVID-19 Drug Discovery and Vaccine Development,"SARS-COV-2 has roused the scientific community with a call to action to combat the growing pandemic. At the time of this writing, there are as yet no novel antiviral agents or approved vaccines available for deployment as a frontline defense. Understanding the pathobiology of COVID-19 could aid scientists in their discovery of potent antivirals by elucidating unexplored viral pathways. One method for accomplishing this is the leveraging of computational methods to discover new candidate drugs and vaccines in silico. In the last decade, machine learning-based models, trained on specific biomolecules, have offered inexpensive and rapid implementation methods for the discovery of effective viral therapies. Given a target biomolecule, these models are capable of predicting inhibitor candidates in a structural-based manner. If enough data are presented to a model, it can aid the search for a drug or vaccine candidate by identifying patterns within the data. In this review, we focus on the recent advances of COVID-19 drug and vaccine development using artificial intelligence and the potential of intelligent training for the discovery of COVID-19 therapeutics. To facilitate applications of deep learning for SARS-COV-2, we highlight multiple molecular targets of COVID-19, inhibition of which may increase patient survival. Moreover, we present CoronaDB-AI, a dataset of compounds, peptides, and epitopes discovered either in silico or in vitro that can be potentially used for training models in order to extract COVID-19 treatment. The information and datasets provided in this review can be used to train deep learning-based models and accelerate the discovery of effective viral therapies.",2020-08-01,3,1705,75,280
1800,32784652,Practice of Simulation and Life Cycle Assessment in Tribology-A Review,"To simulate today's complex tribo-contact scenarios, a methodological breakdown of a complex design problem into simpler sub-problems is essential to achieve acceptable simulation outcomes. This also helps to manage iterative, hierarchical systems within given computational power. In this paper, the authors reviewed recent trends of simulation practices in tribology to model tribo-contact scenario and life cycle assessment (LCA) with the help of simulation. With the advancement of modern computers and computing power, increasing effort has been given towards simulation, which not only saves time and resources but also provides meaningful results. Having said that, like every other technique, simulation has some inherent limitations which need to be considered during practice. Keeping this in mind, the pros and cons of both physical experiments and simulation approaches are reviewed together with their interdependency and how one approach can benefit the other. Various simulation techniques are outlined with a focus on machine learning which will dominate simulation approaches in the future. In addition, simulation of tribo-contacts across different length scales and lubrication conditions is discussed in detail. An extension of the simulation approach, together with experimental data, can lead towards LCA of components which will provide us with a better understanding of the efficient usage of limited resources and conservation of both energy and resources.",2020-08-01,1,1481,70,280
1801,32784445,Computational Methods for Predicting Functions at the mRNA Isoform Level,"Multiple mRNA isoforms of the same gene are produced via alternative splicing, a biological mechanism that regulates protein diversity while maintaining genome size. Alternatively spliced mRNA isoforms of the same gene may sometimes have very similar sequence, but they can have significantly diverse effects on cellular function and regulation. The products of alternative splicing have important and diverse functional roles, such as response to environmental stress, regulation of gene expression, human heritable, and plant diseases. The mRNA isoforms of the same gene can have dramatically different functions. Despite the functional importance of mRNA isoforms, very little has been done to annotate their functions. The recent years have however seen the development of several computational methods aimed at predicting mRNA isoform level biological functions. These methods use a wide array of proteo-genomic data to develop machine learning-based mRNA isoform function prediction tools. In this review, we discuss the computational methods developed for predicting the biological function at the individual mRNA isoform level.",2020-08-01,1,1135,72,280
2031,32823307,"Notable Papers and Trends from 2019 in Sensors, Signals, and Imaging Informatics","Objective:                    To highlight noteworthy papers that are representative of 2019 developments in the fields of sensors, signals, and imaging informatics.              Method:                    A broad literature search was conducted in January 2020 using PubMed. Separate predefined queries were created for sensors/signals and imaging informatics using a combination of Medical Subject Heading (MeSH) terms and keywords. Section editors reviewed the titles and abstracts of both sets of results. Papers were assessed on a three-point Likert scale by two co-editors, rated from 3 (do not include) to 1 (should be included). Papers with an average score of 2 or less were then read by all three section editors, and the group nominated top papers based on consensus. These candidate best papers were then rated by at least six external reviewers.              Results:                    The query related to signals and sensors returned a set of 255 papers from 140 unique journals. The imaging informatics query returned a set of 3,262 papers from 870 unique journals. Based on titles and abstracts, the section co-editors jointly filtered the list down to 50 papers from which 15 candidate best papers were nominated after discussion. A composite rating after review determined four papers which were then approved by consensus of the International Medical Informatics Association (IMIA) Yearbook editorial board. These best papers represent different international groups and journals.              Conclusions:                    The four best papers represent state-of-the-art approaches for processing, combining, and analyzing heterogeneous sensor and imaging data. These papers demonstrate the use of advanced machine learning techniques to improve comparisons between images acquired at different time points, fuse information from multiple sensors, and translate images from one modality to another.",2020-08-01,0,1922,80,280
1748,31580992,Machine learning applications in systems metabolic engineering,"Systems metabolic engineering allows efficient development of high performing microbial strains for the sustainable production of chemicals and materials. In recent years, increasing availability of bio big data, for example, omics data, has led to active application of machine learning techniques across various stages of systems metabolic engineering, including host strain selection, metabolic pathway reconstruction, metabolic flux optimization, and fermentation. In this paper, recent contributions of machine learning approaches to each major step of systems metabolic engineering are discussed. As the use of machine learning in systems metabolic engineering will become more widespread in accordance with the ever-increasing volume of bio big data, future prospects are also provided for the successful applications of machine learning.",2020-08-01,6,845,62,280
2102,31812921,Recent advances on constraint-based models by integrating machine learning,"Research that meaningfully integrates constraint-based modeling with machine learning is at its infancy but holds much promise. Here, we consider where machine learning has been implemented within the constraint-based modeling reconstruction framework and highlight the need to develop approaches that can identify meaningful features from large-scale data and connect them to biological mechanisms to establish causality to connect genotype to phenotype. We motivate the construction of iterative integrative schemes where machine learning can fine-tune the input constraints in a constraint-based model or contrarily, constraint-based model simulation results are analyzed by machine learning and reconciled with experimental data. This can iteratively refine a constraint-based model until there is consistency between experimental data, machine learning results, and constraint-based model simulations.",2020-08-01,3,906,74,280
1806,32781665,Promoter Architecture and Promoter Engineering in Saccharomyces cerevisiae,"Promoters play an essential role in the regulation of gene expression for fine-tuning genetic circuits and metabolic pathways in Saccharomyces cerevisiae (S. cerevisiae). However, native promoters in S. cerevisiae have several limitations which hinder their applications in metabolic engineering. These limitations include an inadequate number of well-characterized promoters, poor dynamic range, and insufficient orthogonality to endogenous regulations. Therefore, it is necessary to perform promoter engineering to create synthetic promoters with better properties. Here, we review recent advances related to promoter architecture, promoter engineering and synthetic promoter applications in S. cerevisiae. We also provide a perspective of future directions in this field with an emphasis on the recent advances of machine learning based promoter designs.",2020-08-01,3,857,74,280
2028,32823316,Review of Clinical Research Informatics,"Objectives:                    Clinical Research Informatics (CRI) declares its scope in its name, but its content, both in terms of the clinical research it supports-and sometimes initiates-and the methods it has developed over time, reach much further than the name suggests. The goal of this review is to celebrate the extraordinary diversity of activity and of results, not as a prize-giving pageant, but in recognition of the field, the community that both serves and is sustained by it, and of its interdisciplinarity and its international dimension.              Methods:                    Beyond personal awareness of a range of work commensurate with the author's own research, it is clear that, even with a thorough literature search, a comprehensive review is impossible. Moreover, the field has grown and subdivided to an extent that makes it very hard for one individual to be familiar with every branch or with more than a few branches in any depth. A literature survey was conducted that focused on informatics-related terms in the general biomedical and healthcare literature, and specific concerns (""artificial intelligence"", ""data models"", ""analytics"", etc.) in the biomedical informatics (BMI) literature. In addition to a selection from the results from these searches, suggestive references within them were also considered.              Results:                    The substantive sections of the paper-Artificial Intelligence, Machine Learning, and ""Big Data"" Analytics; Common Data Models, Data Quality, and Standards; Phenotyping and Cohort Discovery; Privacy: Deidentification, Distributed Computation, Blockchain; Causal Inference and Real-World Evidence-provide broad coverage of these active research areas, with, no doubt, a bias towards this reviewer's interests and preferences, landing on a number of papers that stood out in one way or another, or, alternatively, exemplified a particular line of work.              Conclusions:                    CRI is thriving, not only in the familiar major centers of research, but more widely, throughout the world. This is not to pretend that the distribution is uniform, but to highlight the potential for this domain to play a prominent role in supporting progress in medicine, healthcare, and wellbeing everywhere. We conclude with the observation that CRI and its practitioners would make apt stewards of the new medical knowledge that their methods will bring forward.",2020-08-01,0,2449,39,280
1690,31655339,A quantitative lens on anaerobic life: leveraging the state-of-the-art fluxomics approach to explore clostridial metabolism,"Quantitative understanding of clostridial metabolism is of longstanding interest due to the importance of Clostridia as model anaerobes, biotechnology workhorses, and contributors to evolutionary history and ecosystem. Current computational methods such as flux balance analysis-based construction of clostridial metabolism in genome scale provide a fundamental framework for metabolic analysis. However, this method alone is inadequate to characterize cellular metabolic activity. Experiment-driven approaches including isotope tracer-based fluxomics in association with genetic and biochemical methods are needed to gain a more comprehensive understanding. Here we focus on typical examples where these integrated approaches have contributed to the identification of new metabolic pathways and quantification of metabolic fluxes in Clostridia. We also highlight the opportunities and challenges of cutting-edge fluxomics approaches such as machine learning modeling, deuterium tracer approach, and high throughput flux phenotyping in exploring clostridial metabolism with respect to inorganic carbon utilization, redox cofactor interconversion, and other key metabolic features.",2020-08-01,0,1180,123,280
1805,32781750,Antioxidant Potential of Psychotropic Drugs: From Clinical Evidence to In Vitro and In Vivo Assessment and toward a New Challenge for in Silico Molecular Design,"Due to high oxygen consumption, the brain is particularly vulnerable to oxidative stress, which is considered an important element in the etiopathogenesis of several mental disorders, including schizophrenia, depression and dependencies. Despite the fact that it is not established yet whether oxidative stress is a cause or a consequence of clinic manifestations, the intake of antioxidant supplements in combination with the psychotropic therapy constitutes a valuable solution in patients' treatment. Anyway, some drugs possess antioxidant capacity themselves and this aspect is discussed in this review, focusing on antipsychotics and antidepressants. In the context of a collection of clinical observations, in vitro and in vivo results are critically reported, often highlighting controversial aspects. Finally, a new challenge is discussed, i.e., the possibility of assessing in silico the antioxidant potential of these drugs, exploiting computational chemistry methodologies and machine learning. Despite the physiological environment being incredibly complex and the detection of meaningful oxidative stress biomarkers being all but an easy task, a rigorous and systematic analysis of the structural and reactivity properties of antioxidant drugs seems to be a promising route to better interpret therapeutic outcomes and provide elements for the rational design of novel drugs.",2020-08-01,3,1388,160,280
2185,31675462,"Simulation vs. Understanding: A Tension, in Quantum Chemistry and Beyond. Part B. The March of Simulation, for Better or Worse","In the second part of this Essay, we leave philosophy, and begin by describing Roald's being trashed by simulation. This leads us to a general sketch of artificial intelligence (AI), Searle's Chinese room, and Strevens' account of what a go-playing program knows. Back to our terrain-we ask ""Quantum Chemistry,  ca. 2020?"" Then we move to examples of Big Data, machine learning and neural networks in action, first in chemistry and then affecting social matters, trivial to scary. We argue that moral decisions are hardly to be left to a computer. And that posited causes, even if recognized as provisional, represent a much deeper level of understanding than correlations. At this point, we try to pull the reader up, giving voice to the opposing view of an optimistic, limitless future. But we don't do justice to that view-how could we, older mammals on the way to extinction that we are? We try. But then we return to fuss, questioning the ascetic dimension of scientists, their romance with black boxes. And argue for a science of many tongues.",2020-08-01,0,1050,126,280
1981,32916652,"To operate, or not to operate? Narrative review of the role of survival predictors in patient selection for operative management of patients with metastatic spine disease","Accurate prediction of patient survival is an essential component of the preoperative evaluation of patients with spinal metastases. Over the past quarter of a century, a number of predictors have been developed, although none have been accurate enough to be instituted as a staple of clinical practice. However, recently more comprehensive survival calculators have been published that make use of larger data sets and machine learning to predict postoperative survival among patients with spine metastases. Given the glut of calculators that have been published, the authors sought to perform a narrative review of the current literature, highlighting existing calculators along with the strengths and weaknesses of each. In doing so, they identify two ""generations"" of scoring systems-a first generation based on a priori factor weighting and a second generation comprising predictive tools that are developed using advanced statistical modeling and are focused on clinical deployment. In spite of recent advances, the authors found that most predictors have only a moderate ability to explain variation in patient survival. Second-generation models have a greater prognostic accuracy relative to first-generation scoring systems, but most still require external validation. Given this, it seems that there are two outstanding goals for these survival predictors, foremost being external validation of current calculators in multicenter prospective cohorts, as the majority have been developed from, and internally validated within, the same single-institution data sets. Lastly, current predictors should be modified to incorporate advances in targeted systemic therapy and radiotherapy, which have been heretofore largely ignored.",2020-09-01,0,1735,170,249
1978,32917886,Designing and understanding light-harvesting devices with machine learning,"Understanding the fundamental processes of light-harvesting is crucial to the development of clean energy materials and devices. Biological organisms have evolved complex metabolic mechanisms to efficiently convert sunlight into chemical energy. Unraveling the secrets of this conversion has inspired the design of clean energy technologies, including solar cells and photocatalytic water splitting. Describing the emergence of macroscopic properties from microscopic processes poses the challenge to bridge length and time scales of several orders of magnitude. Machine learning experiences increased popularity as a tool to bridge the gap between multi-level theoretical models and Edisonian trial-and-error approaches. Machine learning offers opportunities to gain detailed scientific insights into the underlying principles governing light-harvesting phenomena and can accelerate the fabrication of light-harvesting devices.",2020-09-01,1,928,74,249
2643,32495652,Machine Learning in Cardiology-Ensuring Clinical Impact Lives Up to the Hype,"Despite substantial advances in the study, treatment, and prevention of cardiovascular disease, numerous challenges relating to optimally screening, diagnosing, and managing patients remain. Simultaneous improvements in computing power, data storage, and data analytics have led to the development of new techniques to address these challenges. One powerful tool to this end is machine learning (ML), which aims to algorithmically identify and represent structure within data. Machine learning's ability to efficiently analyze large and highly complex data sets make it a desirable investigative approach in modern biomedical research. Despite this potential and enormous public and private sector investment, few prospective studies have demonstrated improved clinical outcomes from this technology. This is particularly true in cardiology, despite its emphasis on objective, data-driven results. This threatens to stifle ML's growth and use in mainstream medicine. We outline the current state of ML in cardiology and outline methods through which impactful and sustainable ML research can occur. Following these steps can ensure ML reaches its potential as a transformative technology in medicine.",2020-09-01,1,1200,76,249
2662,32452701,An up-to-date overview of computational polypharmacology in modern drug discovery,"Introduction:                    In recent years, computational polypharmacology has gained significant attention to study the promiscuous nature of drugs. Despite tremendous challenges, community-wide efforts have led to a variety of novel approaches for predicting drug polypharmacology. In particular, some rapid advances using machine learning and artificial intelligence have been reported with great success.              Areas covered:                    In this article, the authors provide a comprehensive update on the current state-of-the-art polypharmacology approaches and their applications, focusing on those reports published after our 2017 review article. The authors particularly discuss some novel, groundbreaking concepts, and methods that have been developed recently and applied to drug polypharmacology studies.              Expert opinion:                    Polypharmacology is evolving and novel concepts are being introduced to counter the current challenges in the field. However, major hurdles remain including incompleteness of high-quality experimental data, lack of in vitro and in vivo assays to characterize multi-targeting agents, shortage of robust computational methods, and challenges to identify the best target combinations and design effective multi-targeting agents. Fortunately, numerous national/international efforts including multi-omics and artificial intelligence initiatives as well as most recent collaborations on addressing the COVID-19 pandemic have shown significant promise to propel the field of polypharmacology forward.",2020-09-01,1,1577,81,249
1416,32652309,Bayer's in silico ADMET platform: a journey of machine learning over the past two decades,"Over the past two decades, an in silico absorption, distribution, metabolism, and excretion (ADMET) platform has been created at Bayer Pharma with the goal to generate models for a variety of pharmacokinetic and physicochemical endpoints in early drug discovery. These tools are accessible to all scientists within the company and can be a useful in assisting with the selection and design of novel leads, as well as the process of lead optimization. Here. we discuss the development of machine-learning (ML) approaches with special emphasis on data, descriptors, and algorithms. We show that high company internal data quality and tailored descriptors, as well as a thorough understanding of the experimental endpoints, are essential to the utility of our models. We discuss the recent impact of deep neural networks and show selected application examples.",2020-09-01,2,857,89,249
2639,32500471,Ethical Considerations of Using Machine Learning for Decision Support in Occupational Health: An Example Involving Periodic Workers' Health Assessments,"Purpose Computer algorithms and Machine Learning (ML) will be integrated into clinical decision support within occupational health care. This will change the interaction between health care professionals and their clients, with unknown consequences. The aim of this study was to explore ethical considerations and potential consequences of using ML based decision support tools (DSTs) in the context of occupational health. Methods We conducted an ethical deliberation. This was supported by a narrative literature review of publications about ML and DSTs in occupational health and by an assessment of the potential impact of ML-DSTs according to frameworks from medical ethics and philosophy of technology. We introduce a hypothetical clinical scenario from a workers' health assessment to reflect on biomedical ethical principles: respect for autonomy, beneficence, non-maleficence and justice. Results Respect for autonomy is affected by uncertainty about what future consequences the worker is consenting to as a result of the fluctuating nature of ML-DSTs and validity evidence used to inform the worker. A beneficent advisory process is influenced because the three elements of evidence based practice are affected through use of a ML-DST. The principle of non-maleficence is challenged by the balance between group-level benefits and individual harm, the vulnerability of the worker in the occupational context, and the possibility of function creep. Justice might be empowered when the ML-DST is valid, but profiling and discrimination are potential risks. Conclusions Implications of ethical considerations have been described for the socially responsible design of ML-DSTs. Three recommendations were provided to minimize undesirable adverse effects of the development and implementation of ML-DSTs.",2020-09-01,0,1810,151,249
1422,32647898,Rage Against the Machine: Advancing the study of aggression ethology via machine learning,"Rationale:                    Aggression, comorbid with neuropsychiatric disorders, exhibits with diverse clinical presentations and places a significant burden on patients, caregivers, and society. This diversity is observed because aggression is a complex behavior that can be ethologically demarcated as either appetitive (rewarding) or reactive (defensive), each with its own behavioral characteristics, functionality, and neural basis that may transition from adaptive to maladaptive depending on genetic and environmental factors. There has been a recent surge in the development of preclinical animal models for studying appetitive aggression-related behaviors and identifying the neural mechanisms guiding their progression and expression. However, adoption of these procedures is often impeded by the arduous task of manually scoring complex social interactions. Manual observations are generally susceptible to observer drift, long analysis times, and poor inter-rater reliability, and are further incompatible with the sampling frequencies required of modern neuroscience methods.              Objectives:                    In this review, we discuss recent advances in the preclinical study of appetitive aggression in mice, paired with our perspective on the potential for machine learning techniques in producing automated, robust scoring of aggressive social behavior. We discuss critical considerations for implementing valid computer classifications within behavioral pharmacological studies.              Key results:                    Open-source automated classification platforms can match or exceed the performance of human observers while removing the confounds of observer drift, bias, and inter-rater reliability. Furthermore, unsupervised approaches can identify previously uncharacterized aggression-related behavioral repertoires in model species.              Discussion and conclusions:                    Advances in open-source computational approaches hold promise for overcoming current manual annotation caveats while also introducing and generalizing computational neuroethology to the greater behavioral neuroscience community. We propose that currently available open-source approaches are sufficient for overcoming the main limitations preventing wide adoption of machine learning within the context of preclinical aggression behavioral research.",2020-09-01,1,2387,89,249
1976,32920050,MPTherm-Pred: Analysis and Prediction of Thermal Stability Changes upon Mutations in Transmembrane Proteins,"The stability of membrane proteins differs from globular proteins due to the presence of nonpolar membrane-spanning regions. Using a dataset of 929 membrane protein mutations whose effects on thermal stability (Tm) were experimentally determined, we found that the average Tm due to 190 stabilizing and 232 destabilizing mutations occurring in membrane-spanning regions are 2.43  3.1 and - 5.48  5.5 C, respectively. The Tm values for mutations occurring in solvent-exposed regions are 2.56  2.82 and - 6.8  7.2 C. We have systematically analyzed the factors influencing the stability of mutants and observed that changes in hydrophobicity, number of contacts between C atoms and frequency of aliphatic residues are important determinants of the stability change induced by mutations occurring in membrane-spanning regions. We have developed structure- and sequence-based machine learning predictors of Tm due to mutations specifically for membrane proteins. They showed a correlation and mean absolute error (MAE) of 0.72 and 2.85 C, respectively, between experimental and predicted Tm for mutations in membrane-spanning regions on 10-fold group-wise cross-validation. The average correlation and MAE for mutations in aqueous regions are 0.73 and 3.7 C, respectively. These MAE values are about 50% lower than standard deviations from the mean Tm values. The reliability of the method was affirmed on a test set of mutations occurring in evolutionary independent protein sequences. The developed MPTherm-pred server for predicting thermal stability changes upon mutations in membrane proteins is available at https://web.iitm.ac.in/bioinfo2/mpthermpred/. Our results provide insights into factors influencing the stability of membrane proteins and can aid in designing mutants that are more resistant to thermal stress.",2020-09-01,1,1835,107,249
1982,32915503,Advances in optical gastrointestinal endoscopy: a technical review,"Optical endoscopy is the primary diagnostic and therapeutic tool for management of gastrointestinal (GI) malignancies. Most GI neoplasms arise from precancerous lesions; thus, technical innovations to improve detection and diagnosis of precancerous lesions and early cancers play a pivotal role in improving outcomes. Over the last few decades, the field of GI endoscopy has witnessed enormous and focused efforts to develop and translate accurate, user-friendly, and minimally invasive optical imaging modalities. From a technical point of view, a wide range of novel optical techniques is now available to probe different aspects of light-tissue interaction at macroscopic and microscopic scales, complementing white light endoscopy. Most of these new modalities have been successfully validated and translated to routine clinical practice. Herein, we provide a technical review of the current status of existing and promising new optical endoscopic imaging technologies for GI cancer screening and surveillance. We summarize the underlying principles of light-tissue interaction, the imaging performance at different scales, and highlight what is known about clinical applicability and effectiveness. Furthermore, we discuss recent discovery and translation of novel molecular probes that have shown promise to augment endoscopists' ability to diagnose GI lesions with high specificity. We also review and discuss the role and potential clinical integration of artificial intelligence-based algorithms to provide decision support in real time. Finally, we provide perspectives on future technology development and its potential to transform endoscopic GI cancer detection and diagnosis.",2020-09-01,0,1689,66,249
1980,32916943,Research Progress of Automated Visual Surface Defect Detection for Industrial Metal Planar Materials,"The computer-vision-based surface defect detection of metal planar materials is a research hotspot in the field of metallurgical industry. The high standard of planar surface quality in the metal manufacturing industry requires that the performance of an automated visual inspection system and its algorithms are constantly improved. This paper attempts to present a comprehensive survey on both two-dimensional and three-dimensional surface defect detection technologies based on reviewing over 160 publications for some typical metal planar material products of steel, aluminum, copper plates and strips. According to the algorithm properties as well as the image features, the existing two-dimensional methodologies are categorized into four groups: statistical, spectral, model, and machine learning-based methods. On the basis of three-dimensional data acquisition, the three-dimensional technologies are divided into stereoscopic vision, photometric stereo, laser scanner, and structured light measurement methods. These classical algorithms and emerging methods are introduced, analyzed, and compared in this review. Finally, the remaining challenges and future research trends of visual defect detection are discussed and forecasted at an abstract level.",2020-09-01,0,1262,100,249
1952,32961749,Comprehensive Survey and Comparative Assessment of RNA-Binding Residue Predictions with Analysis by RNA Type,"With close to 30 sequence-based predictors of RNA-binding residues (RBRs), this comparative survey aims to help with understanding and selection of the appropriate tools. We discuss past reviews on this topic, survey a comprehensive collection of predictors, and comparatively assess six representative methods. We provide a novel and well-designed benchmark dataset and we are the first to report and compare protein-level and datasets-level results, and to contextualize performance to specific types of RNAs. The methods considered here are well-cited and rely on machine learning algorithms on occasion combined with homology-based prediction. Empirical tests reveal that they provide relatively accurate predictions. Virtually all methods perform well for the proteins that interact with rRNAs, some generate accurate predictions for mRNAs, snRNA, SRP and IRES, while proteins that bind tRNAs are predicted poorly. Moreover, except for DRNApred, they confuse DNA and RNA-binding residues. None of the six methods consistently outperforms the others when tested on individual proteins. This variable and complementary protein-level performance suggests that users should not rely on applying just the single best dataset-level predictor. We recommend that future work should focus on the development of approaches that facilitate protein-level selection of accurate predictors and the consensus-based prediction of RBRs.",2020-09-01,0,1424,108,249
1968,32931932,Deep learning of brain magnetic resonance images: A brief review,"Magnetic resonance imaging (MRI) is one of the most popular techniques in brain science and is important for understanding brain function and neuropsychiatric disorders. However, the processing and analysis of MRI is not a trivial task with lots of challenges. Recently, deep learning has shown superior performance over traditional machine learning approaches in image analysis. In this survey, we give a brief review of the recent popular deep learning approaches and their applications in brain MRI analysis. Furthermore, popular brain MRI databases and deep learning tools are also introduced. The strength and weaknesses of different approaches are addressed, and challenges as well as future directions are also discussed.",2020-09-01,0,728,64,249
1971,32928475,Constructing bi-plots for random forest: Tutorial,"Current technological developments have allowed for a significant increase and availability of data. Consequently, this has opened enormous opportunities for the machine learning and data science field, translating into the development of new algorithms in a wide range of applications in medical, biomedical, daily-life, and national security areas. Ensemble techniques are among the pillars of the machine learning field, and they can be defined as approaches in which multiple, complex, independent/uncorrelated, predictive models are subsequently combined by either averaging or voting to yield a higher model performance. Random forest (RF), a popular ensemble method, has been successfully applied in various domains due to its ability to build predictive models with high certainty and little necessity of model optimization. RF provides both a predictive model and an estimation of the variable importance. However, the estimation of the variable importance is based on thousands of trees, and therefore, it does not specify which variable is important for which sample group. The present study demonstrates an approach based on the pseudo-sample principle that allows for construction of bi-plots (i.e. spin plots) associated with RF models. The pseudo-sample principle for RF. is explained and demonstrated by using two simulated datasets, and three different types of real data, which include political sciences, food chemistry and the human microbiome data. The pseudo-sample bi-plots, associated with RF and its unsupervised version, allow for a versatile visualization of multivariate models, and the variable importance and the relation among them.",2020-09-01,0,1663,49,249
2687,32417653,Synthetic macromolecules as therapeutics that overcome resistance in cancer and microbial infection,"Synthetic macromolecular antimicrobials have shown efficacy in the treatment of multidrug resistant (MDR) pathogens. These synthetic macromolecules, inspired by Nature's antimicrobial peptides (AMPs), mitigate resistance by disrupting microbial cell membrane or targeting multiple intracellular proteins or genes. Unlike AMPs, these polymers are less prone to degradation by proteases and are easier to synthesize on a large scale. Recently, various studies have revealed that cancer cell membrane, like that of microbes, is negatively charged, and AMPs can be used as anticancer agents. Nevertheless, efforts in developing polymers as anticancer agents has remained limited. This review highlights the recent advancement in the development of synthetic biodegradable antimicrobial polymers (e.g. polycarbonates, polyesters and polypeptides) and anticancer macromolecules including peptides and polymers. Additionally, strategies to improve their in vivo bioavailability and selectivity towards bacteria and cancer cells are examined. Lastly, future perspectives, including use of artificial intelligence or machine learning, in the development of antimicrobial and anticancer macromolecules are discussed.",2020-09-01,3,1206,99,249
1396,33071738,Heart Rate Variability in the Perinatal Period: A Critical and Conceptual Review,"Neonatal intensive care units (NICUs) greatly expand the use of technology. There is a need to accurately diagnose discomfort, pain, and complications, such as sepsis, mainly before they occur. While specific treatments are possible, they are often time-consuming, invasive, or painful, with detrimental effects for the development of the infant. In the last 40 years, heart rate variability (HRV) has emerged as a non-invasive measurement to monitor newborns and infants, but it still is underused. Hence, the present paper aims to review the utility of HRV in neonatology and the instruments available to assess it, showing how HRV could be an innovative tool in the years to come. When continuously monitored, HRV could help assess the baby's overall wellbeing and neurological development to detect stress-/pain-related behaviors or pathological conditions, such as respiratory distress syndrome and hyperbilirubinemia, to address when to perform procedures to reduce the baby's stress/pain and interventions, such as therapeutic hypothermia, and to avoid severe complications, such as sepsis and necrotizing enterocolitis, thus reducing mortality. Based on literature and previous experiences, the first step to efficiently introduce HRV in the NICUs could consist in a monitoring system that uses photoplethysmography, which is low-cost and non-invasive, and displays one or a few metrics with good clinical utility. However, to fully harness HRV clinical potential and to greatly improve neonatal care, the monitoring systems will have to rely on modern bioinformatics (machine learning and artificial intelligence algorithms), which could easily integrate infant's HRV metrics, vital signs, and especially past history, thus elaborating models capable to efficiently monitor and predict the infant's clinical conditions. For this reason, hospitals and institutions will have to establish tight collaborations between the obstetric, neonatal, and pediatric departments: this way, healthcare would truly improve in every stage of the perinatal period (from conception to the first years of life), since information about patients' health would flow freely among different professionals, and high-quality research could be performed integrating the data recorded in those departments.",2020-09-01,2,2289,80,249
1956,32957598,AI Approaches Towards Prechtl's Assessment of General Movements: A Systematic Literature Review,"General movements (GMs) are spontaneous movements of infants up to five months post-term involving the whole body varying in sequence, speed, and amplitude. The assessment of GMs has shown its importance for identifying infants at risk for neuromotor deficits, especially for the detection of cerebral palsy. As the assessment is based on videos of the infant that are rated by trained professionals, the method is time-consuming and expensive. Therefore, approaches based on Artificial Intelligence have gained significantly increased attention in the last years. In this article, we systematically analyze and discuss the main design features of all existing technological approaches seeking to transfer the Prechtl's assessment of general movements from an individual visual perception to computer-based analysis. After identifying their shared shortcomings, we explain the methodological reasons for their limited practical performance and classification rates. As a conclusion of our literature study, we conceptually propose a methodological solution to the defined problem based on the groundbreaking innovation in the area of Deep Learning.",2020-09-01,2,1148,95,249
1386,33093807,Evolution of Sequence-based Bioinformatics Tools for Protein-protein Interaction Prediction,"Protein-protein interactions (PPIs) are the physical connections between two or more proteins via electrostatic forces or hydrophobic effects. Identification of the PPIs is pivotal, which contributes to many biological processes including protein function, disease incidence, and therapy design. The experimental identification of PPIs via high-throughput technology is time-consuming and expensive. Bioinformatics approaches are expected to solve such restrictions. In this review, our main goal is to provide an inclusive view of the existing sequence-based computational prediction of PPIs. Initially, we briefly introduce the currently available PPI databases and then review the state-of-the-art bioinformatics approaches, working principles, and their performances. Finally, we discuss the caveats and future perspective of the next generation algorithms for the prediction of PPIs.",2020-09-01,2,888,91,249
1951,32961909,Data-Driven Molecular Dynamics: A Multifaceted Challenge,"The big data concept is currently revolutionizing several fields of science including drug discovery and development. While opening up new perspectives for better drug design and related strategies, big data analysis strongly challenges our current ability to manage and exploit an extraordinarily large and possibly diverse amount of information. The recent renewal of machine learning (ML)-based algorithms is key in providing the proper framework for addressing this issue. In this respect, the impact on the exploitation of molecular dynamics (MD) simulations, which have recently reached mainstream status in computational drug discovery, can be remarkable. Here, we review the recent progress in the use of ML methods coupled to biomolecular simulations with potentially relevant implications for drug design. Specifically, we show how different ML-based strategies can be applied to the outcome of MD simulations for gaining knowledge and enhancing sampling. Finally, we discuss how intrinsic limitations of MD in accurately modeling biomolecular systems can be alleviated by including information coming from experimental data.",2020-09-01,1,1135,56,249
1380,33101385,Predicting Thermal Adaptation by Looking Into Populations' Genomic Past,"Molecular evolution offers an insightful theory to interpret the genomic consequences of thermal adaptation to previous events of climate change beyond range shifts. However, disentangling often mixed footprints of selective and demographic processes from those due to lineage sorting, recombination rate variation, and genomic constrains is not trivial. Therefore, here we condense current and historical population genomic tools to study thermal adaptation and outline key developments (genomic prediction, machine learning) that might assist their utilization for improving forecasts of populations' responses to thermal variation. We start by summarizing how recent thermal-driven selective and demographic responses can be inferred by coalescent methods and in turn how quantitative genetic theory offers suitable multi-trait predictions over a few generations via the breeder's equation. We later assume that enough generations have passed as to display genomic signatures of divergent selection to thermal variation and describe how these footprints can be reconstructed using genome-wide association and selection scans or, alternatively, may be used for forward prediction over multiple generations under an infinitesimal genomic prediction model. Finally, we move deeper in time to comprehend the genomic consequences of thermal shifts at an evolutionary time scale by relying on phylogeographic approaches that allow for reticulate evolution and ecological parapatric speciation, and end by envisioning the potential of modern machine learning techniques to better inform long-term predictions. We conclude that foreseeing future thermal adaptive responses requires bridging the multiple spatial scales of historical and predictive environmental change research under modern cohesive approaches such as genomic prediction and machine learning frameworks.",2020-09-01,3,1865,71,249
1950,32962078,Review: Application and Prospective Discussion of Machine Learning for the Management of Dairy Farms,"Dairy farmers use herd management systems, behavioral sensors, feeding lists, breeding schedules, and health records to document herd characteristics. Consequently, large amounts of dairy data are becoming available. However, a lack of data integration makes it difficult for farmers to analyze the data on their dairy farm, which indicates that these data are currently not being used to their full potential. Hence, multiple issues in dairy farming such as low longevity, poor performance, and health issues remain. We aimed to evaluate whether machine learning (ML) methods can solve some of these existing issues in dairy farming. This review summarizes peer-reviewed ML papers published in the dairy sector between 2015 and 2020. Ultimately, 97 papers from the subdomains of management, physiology, reproduction, behavior analysis, and feeding were considered in this review. The results confirm that ML algorithms have become common tools in most areas of dairy research, particularly to predict data. Despite the quantity of research available, most tested algorithms have not performed sufficiently for a reliable implementation in practice. This may be due to poor training data. The availability of data resources from multiple farms covering longer periods would be useful to improve prediction accuracies. In conclusion, ML is a promising tool in dairy research, which could be used to develop and improve decision support for farmers. As the cow is a multifactorial system, ML algorithms could analyze integrated data sources that describe and ultimately allow managing cows according to all relevant influencing factors. However, both the integration of multiple data sources and the obtainability of public data currently remain challenging.",2020-09-01,0,1756,100,249
1975,32921304,Review of medical image recognition technologies to detect melanomas using neural networks,"Background:                    Melanoma is one of the most aggressive types of cancer that has become a world-class problem. According to the World Health Organization estimates, 132,000 cases of the disease and 66,000 deaths from malignant melanoma and other forms of skin cancer are reported annually worldwide ( https://apps.who.int/gho/data/?theme=main ) and those numbers continue to grow. In our opinion, due to the increasing incidence of the disease, it is necessary to find new, easy to use and sensitive methods for the early diagnosis of melanoma in a large number of people around the world. Over the last decade, neural networks show highly sensitive, specific, and accurate results.              Objective:                    This study presents a review of PubMed papers including requests melanoma neural network and melanoma neural network dermatoscopy. We review recent researches and discuss their opportunities acceptable in clinical practice.              Methods:                    We searched the PubMed database for systematic reviews and original research papers on the requests melanoma neural network and melanoma neural network dermatoscopy published in English. Only papers that reported results, progress and outcomes are included in this review.              Results:                    We found 11 papers that match our requests that observed convolutional and deep-learning neural networks combined with fuzzy clustering or World Cup Optimization algorithms in analyzing dermatoscopic images. All of them require an ABCD (asymmetry, border, color, and differential structures) algorithm and its derivates (in combination with ABCD algorithm or separately). Also, they require a large dataset of dermatoscopic images and optimized estimation parameters to provide high specificity, accuracy and sensitivity.              Conclusions:                    According to the analyzed papers, neural networks show higher specificity, accuracy and sensitivity than dermatologists. Neural networks are able to evaluate features that might be unavailable to the naked human eye. Despite that, we need more datasets to confirm those statements. Nowadays machine learning becomes a helpful tool in early diagnosing skin diseases, especially melanoma.",2020-09-01,2,2280,90,249
1986,32911665,"Comparison of Conventional Statistical Methods with Machine Learning in Medicine: Diagnosis, Drug Development, and Treatment","Futurists have anticipated that novel autonomous technologies, embedded with machine learning (ML), will substantially influence healthcare. ML is focused on making predictions as accurate as possible, while traditional statistical models are aimed at inferring relationships between variables. The benefits of ML comprise flexibility and scalability compared with conventional statistical approaches, which makes it deployable for several tasks, such as diagnosis and classification, and survival predictions. However, much of ML-based analysis remains scattered, lacking a cohesive structure. There is a need to evaluate and compare the performance of well-developed conventional statistical methods and ML on patient outcomes, such as survival, response to treatment, and patient-reported outcomes (PROs). In this article, we compare the usefulness and limitations of traditional statistical methods and ML, when applied to the medical field. Traditional statistical methods seem to be more useful when the number of cases largely exceeds the number of variables under study and a priori knowledge on the topic under study is substantial such as in public health. ML could be more suited in highly innovative fields with a huge bulk of data, such as omics, radiodiagnostics, drug development, and personalized treatment. Integration of the two approaches should be preferred over a unidirectional choice of either approach.",2020-09-01,1,1426,124,249
1339,33173516,Selective Review of Neuroimaging Findings in Youth at Clinical High Risk for Psychosis: On the Path to Biomarkers for Conversion,"First episode psychosis (FEP), and subsequent diagnosis of schizophrenia or schizoaffective disorder, predominantly occurs during late adolescence, is accompanied by a significant decline in function and represents a traumatic experience for patients and families alike. Prior to first episode psychosis, most patients experience a prodromal period of 1-2 years, during which symptoms first appear and then progress. During that time period, subjects are referred to as being at Clinical High Risk (CHR), as a prodromal period can only be designated in hindsight in those who convert. The clinical high-risk period represents a critical window during which interventions may be targeted to slow or prevent conversion to psychosis. However, only one third of subjects at clinical high risk will convert to psychosis and receive a formal diagnosis of a primary psychotic disorder. Therefore, in order for targeted interventions to be developed and applied, predicting who among this population will convert is of critical importance. To date, a variety of neuroimaging modalities have identified numerous differences between CHR subjects and healthy controls. However, complicating attempts at predicting conversion are increasingly recognized co-morbidities, such as major depressive disorder, in a significant number of CHR subjects. The result of this is that phenotypes discovered between CHR subjects and healthy controls are likely non-specific to psychosis and generalized for major mental illness. In this paper, we selectively review evidence for neuroimaging phenotypes in CHR subjects who later converted to psychosis. We then evaluate the recent landscape of machine learning as it relates to neuroimaging phenotypes in predicting conversion to psychosis.",2020-09-01,0,1765,128,249
1959,32952403,"Artificial Intelligence, Machine Learning, and Cardiovascular Disease","Artificial intelligence (AI)-based applications have found widespread applications in many fields of science, technology, and medicine. The use of enhanced computing power of machines in clinical medicine and diagnostics has been under exploration since the 1960s. More recently, with the advent of advances in computing, algorithms enabling machine learning, especially deep learning networks that mimic the human brain in function, there has been renewed interest to use them in clinical medicine. In cardiovascular medicine, AI-based systems have found new applications in cardiovascular imaging, cardiovascular risk prediction, and newer drug targets. This article aims to describe different AI applications including machine learning and deep learning and their applications in cardiovascular medicine. AI-based applications have enhanced our understanding of different phenotypes of heart failure and congenital heart disease. These applications have led to newer treatment strategies for different types of cardiovascular diseases, newer approach to cardiovascular drug therapy and postmarketing survey of prescription drugs. However, there are several challenges in the clinical use of AI-based applications and interpretation of the results including data privacy, poorly selected/outdated data, selection bias, and unintentional continuance of historical biases/stereotypes in the data which can lead to erroneous conclusions. Still, AI is a transformative technology and has immense potential in health care.",2020-09-01,0,1519,69,249
1945,33205132,Generative Adversarial Networks in Digital Pathology: A Survey on Trends and Future Potential,"Image analysis in the field of digital pathology has recently gained increased popularity. The use of high-quality whole-slide scanners enables the fast acquisition of large amounts of image data, showing extensive context and microscopic detail at the same time. Simultaneously, novel machine-learning algorithms have boosted the performance of image analysis approaches. In this paper, we focus on a particularly powerful class of architectures, the so-called generative adversarial networks (GANs) applied to histological image data. Besides improving performance, GANs also enable previously intractable application scenarios in this field. However, GANs could exhibit a potential for introducing bias. Hereby, we summarize the recent state-of-the-art developments in a generalizing notation, present the main applications of GANs, and give an outlook of some chosen promising approaches and their possible future applications. In addition, we identify currently unavailable methods with potential for future applications.",2020-09-01,0,1026,93,249
1963,32944070,Machine intelligence for nerve conduit design and production,"Nerve guidance conduits (NGCs) have emerged from recent advances within tissue engineering as a promising alternative to autografts for peripheral nerve repair. NGCs are tubular structures with engineered biomaterials, which guide axonal regeneration from the injured proximal nerve to the distal stump. NGC design can synergistically combine multiple properties to enhance proliferation of stem and neuronal cells, improve nerve migration, attenuate inflammation and reduce scar tissue formation. The aim of most laboratories fabricating NGCs is the development of an automated process that incorporates patient-specific features and complex tissue blueprints (e.g. neurovascular conduit) that serve as the basis for more complicated muscular and skin grafts. One of the major limitations for tissue engineering is lack of guidance for generating tissue blueprints and the absence of streamlined manufacturing processes. With the rapid expansion of machine intelligence, high dimensional image analysis, and computational scaffold design, optimized tissue templates for 3D bioprinting (3DBP) are feasible. In this review, we examine the translational challenges to peripheral nerve regeneration and where machine intelligence can innovate bottlenecks in neural tissue engineering.",2020-09-01,0,1281,60,249
1936,33235943,Risk-stratified and stepped models of care for back pain and osteoarthritis: are we heading towards a common model?,"The overall quality of care for musculoskeletal pain conditions is suboptimal, partly due to a considerable evidence-practice gap. In osteoarthritis and low back pain, structured models of care exist to help overcome that challenge. In osteoarthritis, focus is on stepped care models, where treatment decisions are guided by response to treatment, and increasingly comprehensive interventions are only offered to people with inadequate response to more simple care. In low back pain, the most widely known approach is based on risk stratification, where patients with higher predicted risk of poor outcome are offered more comprehensive care. For both conditions, the recommended interventions and models of care share many commonalities and there is no evidence that one model of care is more effective than the other. Limitations of existing models of care include a lack of integrated information on social factors, comorbid conditions, and previous treatment experience, and they do not support an interplay between health care, self-management, and community-based activities. Moving forwards, a common model across musculoskeletal conditions seems realistic, which points to an opportunity for reducing the complexity of implementation. We foresee this development will use big data sources and machine-learning methods to combine stepped and risk-stratified care and to integrate self-management support and patient-centred care to a greater extent in future models of care.",2020-09-01,0,1481,115,249
1966,32936088,Artificial Intelligence for the Prediction of Helicobacter Pylori Infection in Endoscopic Images: Systematic Review and Meta-Analysis Of Diagnostic Test Accuracy,"Background:                    Helicobacter pylori plays a central role in the development of gastric cancer, and prediction of H pylori infection by visual inspection of the gastric mucosa is an important function of endoscopy. However, there are currently no established methods of optical diagnosis of H pylori infection using endoscopic images. Definitive diagnosis requires endoscopic biopsy. Artificial intelligence (AI) has been increasingly adopted in clinical practice, especially for image recognition and classification.              Objective:                    This study aimed to evaluate the diagnostic test accuracy of AI for the prediction of H pylori infection using endoscopic images.              Methods:                    Two independent evaluators searched core databases. The inclusion criteria included studies with endoscopic images of H pylori infection and with application of AI for the prediction of H pylori infection presenting diagnostic performance. Systematic review and diagnostic test accuracy meta-analysis were performed.              Results:                    Ultimately, 8 studies were identified. Pooled sensitivity, specificity, diagnostic odds ratio, and area under the curve of AI for the prediction of H pylori infection were 0.87 (95% CI 0.72-0.94), 0.86 (95% CI 0.77-0.92), 40 (95% CI 15-112), and 0.92 (95% CI 0.90-0.94), respectively, in the 1719 patients (385 patients with H pylori infection vs 1334 controls). Meta-regression showed methodological quality and included the number of patients in each study for the purpose of heterogeneity. There was no evidence of publication bias. The accuracy of the AI algorithm reached 82% for discrimination between noninfected images and posteradication images.              Conclusions:                    An AI algorithm is a reliable tool for endoscopic diagnosis of H pylori infection. The limitations of lacking external validation performance and being conducted only in Asia should be overcome.              Trial registration:                    PROSPERO CRD42020175957; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=175957.",2020-09-01,1,2147,161,249
1408,32663517,Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling,"Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.",2020-09-01,2,798,100,249
1356,33145087,Radiogenomics of lung cancer,"Machine learning (ML) and artificial intelligence (AI) are aiding in improving sensitivity and specificity of diagnostic imaging. The rapid adoption of these advanced ML algorithms is transforming imaging analysis; taking us from noninvasive detection of pathology to noninvasive precise diagnosis of the pathology by identifying whether detected abnormality is a secondary to infection, inflammation and/or neoplasm. This is led to the emergence of ""Radiobiogenomics""; referring to the concept of identifying biologic (genomic, proteomic) alterations in the detected lesion. Radiobiogenomic involves image segmentation, feature extraction, and ML model to predict underlying tumor genotype and clinical outcomes. Lung cancer is the most common cause of cancer related death worldwide. There are several histologic subtypes of lung cancer, e.g., small cell lung cancer (SCLC), non-small cell lung cancer (NSCLC) (adenocarcinoma, squamous cell carcinoma). These variable histologic subtypes not only appear different at microscopic level, but these also differ at genetic and transcription level. This intrinsic heterogeneity reveals itself as different morphologic appearances on diagnostic imaging, such as CT, PET/CT and MRI. Traditional evaluation of imaging findings of lung cancer is limited to morphologic characteristics, such as lesion size, margins, density. Radiomics takes image analysis a step further by looking at imaging phenotype with higher order statistics in efforts to quantify intralesional heterogeneity. This heterogeneity, in turn, can be potentially used to extract intralesional genomic and proteomic data. This review aims to highlight novel concepts in ML and AI and their potential applications in identifying radiobiogenomics of lung cancer.",2020-09-01,0,1771,28,249
1957,32955675,Technological advancements and opportunities in Neuromarketing: a systematic review,"Neuromarketing has become an academic and commercial area of interest, as the advancements in neural recording techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response of consumers to the marketing stimuli. This article presents the very first systematic review of the technological advancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a total of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic or empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both product and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band signals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha asymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional magnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to its low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, skin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical studies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component analysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and classification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) have performed with the highest average accuracy among other machine learning algorithms used in these literatures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarketing for making novel contributions.",2020-09-01,0,1973,83,249
829,32967993,Preparing to adapt is key for Olympic curling robots,Continued advances in machine learning could enable robots to solve tasks on a human level and adapt to changing conditions.,2020-09-01,0,124,52,249
1989,32906819,3D Deep Learning on Medical Images: A Review,"The rapid advancements in machine learning, graphics processing technologies and the availability of medical imaging data have led to a rapid increase in the use of deep learning models in the medical domain. This was exacerbated by the rapid advancements in convolutional neural network (CNN) based architectures, which were adopted by the medical imaging community to assist clinicians in disease diagnosis. Since the grand success of AlexNet in 2012, CNNs have been increasingly used in medical image analysis to improve the efficiency of human clinicians. In recent years, three-dimensional (3D) CNNs have been employed for the analysis of medical images. In this paper, we trace the history of how the 3D CNN was developed from its machine learning roots, we provide a brief mathematical description of 3D CNN and provide the preprocessing steps required for medical images before feeding them to 3D CNNs. We review the significant research in the field of 3D medical imaging analysis using 3D CNNs (and its variants) in different medical areas such as classification, segmentation, detection and localization. We conclude by discussing the challenges associated with the use of 3D CNNs in the medical imaging domain (and the use of deep learning models in general) and possible future trends in the field.",2020-09-01,4,1311,44,249
791,33015010,Review on the Application of Machine Learning Algorithms in the Sequence Data Mining of DNA,"Deoxyribonucleic acid (DNA) is a biological macromolecule. Its main function is information storage. At present, the advancement of sequencing technology had caused DNA sequence data to grow at an explosive rate, which has also pushed the study of DNA sequences in the wave of big data. Moreover, machine learning is a powerful technique for analyzing largescale data and learns spontaneously to gain knowledge. It has been widely used in DNA sequence data analysis and obtained a lot of research achievements. Firstly, the review introduces the development process of sequencing technology, expounds on the concept of DNA sequence data structure and sequence similarity. Then we analyze the basic process of data mining, summary several major machine learning algorithms, and put forward the challenges faced by machine learning algorithms in the mining of biological sequence data and possible solutions in the future. Then we review four typical applications of machine learning in DNA sequence data: DNA sequence alignment, DNA sequence classification, DNA sequence clustering, and DNA pattern mining. We analyze their corresponding biological application background and significance, and systematically summarized the development and potential problems in the field of DNA sequence data mining in recent years. Finally, we summarize the content of the review and look into the future of some research directions for the next step.",2020-09-01,0,1435,91,249
790,33015372,The future of digital health with federated learning,"Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how federated learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.",2020-09-01,11,755,52,249
786,33024393,Artificial intelligence in gastric cancer: Application and future perspectives,"Gastric cancer is the fourth leading cause of cancer-related mortality across the globe, with a 5-year survival rate of less than 40%. In recent years, several applications of artificial intelligence (AI) have emerged in the gastric cancer field based on its efficient computational power and learning capacities, such as image-based diagnosis and prognosis prediction. AI-assisted diagnosis includes pathology, endoscopy, and computerized tomography, while researchers in the prognosis circle focus on recurrence, metastasis, and survival prediction. In this review, a comprehensive literature search was performed on articles published up to April 2020 from the databases of PubMed, Embase, Web of Science, and the Cochrane Library. Thereby the current status of AI-applications was systematically summarized in gastric cancer. Moreover, future directions that target this field were also analyzed to overcome the risk of overfitting AI models and enhance their accuracy as well as the applicability in clinical practice.",2020-09-01,1,1023,78,249
782,33033576,Mitochondria under the spotlight: On the implications of mitochondrial dysfunction and its connectivity to neuropsychiatric disorders,"Neuropsychiatric disorders (NPDs) such as bipolar disorder (BD), schizophrenia (SZ) and mood disorder (MD) are hard to manage due to overlapping symptoms and lack of biomarkers. Risk alleles of BD/SZ/MD are emerging, with evidence suggesting mitochondrial (mt) dysfunction as a critical factor for disease onset and progression. Mood stabilizing treatments for these disorders are scarce, revealing the need for biomarker discovery and artificial intelligence approaches to design synthetically accessible novel therapeutics. Here, we show mt involvement in NPDs by associating 245 mt proteins to BD/SZ/MD, with 7 common players in these disease categories. Analysis of over 650 publications suggests that 245 NPD-linked mt proteins are associated with 800 other mt proteins, with mt impairment likely to rewire these interactions. High dosage of mood stabilizers is known to alleviate manic episodes, but which compounds target mt pathways is another gap in the field that we address through mood stabilizer-gene interaction analysis of 37 prescriptions and over-the-counter psychotropic treatments, which we have refined to 15 mood-stabilizing agents. We show 26 of the 245 NPD-linked mt proteins are uniquely or commonly targeted by one or more of these mood stabilizers. Further, induced pluripotent stem cell-derived patient neurons and three-dimensional human brain organoids as reliable BD/SZ/MD models are outlined, along with multiomics methods and machine learning-based decision making tools for biomarker discovery, which remains a bottleneck for precision psychiatry medicine.",2020-09-01,1,1589,133,249
778,33038981,"Acute myeloid leukemia and artificial intelligence, algorithms and new scores","Artificial intelligence, and more narrowly machine-learning, is beginning to expand humanity's capacity to analyze increasingly large and complex datasets. Advances in computer hardware and software have led to breakthroughs in multiple sectors of our society, including a burgeoning role in medical research and clinical practice. As the volume of medical data grows at an apparently exponential rate, particularly since the human genome project laid the foundation for modern genetic inquiry, informatics tools like machine learning are becoming crucial in analyzing these data to provide meaningful tools for diagnostic, prognostic, and therapeutic purposes. Within medicine, hematologic diseases can be particularly challenging to understand and treat given the increasingly complex and intercalated genetic, epigenetic, immunologic, and regulatory pathways that must be understood to optimize patient outcomes. In acute myeloid leukemia (AML), new developments in machine learning algorithms have enabled a deeper understanding of disease biology and the development of better prognostic and predictive tools. Ongoing work in the field brings these developments incrementally closer to clinical implementation.",2020-09-01,1,1215,77,249
765,33039027,Commentary on a combined approach to the problem of developing biomarkers for the prediction of spontaneous preterm labor that leads to preterm birth,"Introduction:                    Globally, preterm birth has replaced congenital malformation as the major cause of perinatal mortality and morbidity. The reduced rate of congenital malformation was not achieved through a single biophysical or biochemical marker at a specific gestational age, but rather through a combination of clinical, biophysical and biochemical markers at different gestational ages. Since the aetiology of spontaneous preterm birth is also multifactorial, it is unlikely that a single biomarker test, at a specific gestational age will emerge as the definitive predictive test.              Methods:                    The Biomarkers Group of PREBIC, comprising clinicians, basic scientists and other experts in the field, with a particular interest in preterm birth have produced this commentary with short, medium and long-term aims: i) to alert clinicians to the advances that are being made in the prediction of spontaneous preterm birth; ii) to encourage clinicians and scientists to continue their efforts in this field, and not to be disheartened or nihilistic because of a perceived lack of progress and iii) to enable development of novel interventions that can reduce the mortality and morbidity associated with preterm birth.              Results:                    Using language that we hope is clear to practising clinicians, we have identified 11 Sections in which there exists the potential, feasibility and capability of technologies for candidate biomarkers in the prediction of spontaneous preterm birth and how current limitations to this research might be circumvented.              Discussion:                    The combination of biophysical, biochemical, immunological, microbiological, fetal cell, exosomal, or cell free RNA at different gestational ages, integrated as part of a multivariable predictor model may be necessary to advance our attempts to predict sPTL and PTB. This will require systems biological data using ""omics"" data and artificial intelligence/machine learning to manage the data appropriately. The ultimate goal is to reduce the mortality and morbidity associated with preterm birth.",2020-09-01,0,2156,149,249
792,33014031,EEG-Based Emotion Recognition: A State-of-the-Art Review of Current Trends and Opportunities,"Emotions are fundamental for human beings and play an important role in human cognition. Emotion is commonly associated with logical decision making, perception, human interaction, and to a certain extent, human intelligence itself. With the growing interest of the research community towards establishing some meaningful ""emotional"" interactions between humans and computers, the need for reliable and deployable solutions for the identification of human emotional states is required. Recent developments in using electroencephalography (EEG) for emotion recognition have garnered strong interest from the research community as the latest developments in consumer-grade wearable EEG solutions can provide a cheap, portable, and simple solution for identifying emotions. Since the last comprehensive review was conducted back from the years 2009 to 2016, this paper will update on the current progress of emotion recognition using EEG signals from 2016 to 2019. The focus on this state-of-the-art review focuses on the elements of emotion stimuli type and presentation approach, study size, EEG hardware, machine learning classifiers, and classification approach. From this state-of-the-art review, we suggest several future research opportunities including proposing a different approach in presenting the stimuli in the form of virtual reality (VR). To this end, an additional section devoted specifically to reviewing only VR studies within this research domain is presented as the motivation for this proposed new approach using VR as the stimuli presentation device. This review paper is intended to be useful for the research community working on emotion recognition using EEG signals as well as for those who are venturing into this field of research.",2020-09-01,3,1758,92,249
761,33042915,Predicting Apnoeic Events in Preterm Infants,"Apnoea, a pause in respiration, is almost ubiquitous in preterm infants born before completing 30 weeks gestation. Apnoea often begets hypoxemia and/or bradycardia, and has the potential to result in adverse neurodevelopmental consequences. Our current inability to predict apnoeic events in preterm infants requires apnoea to first be detected by monitoring device/s in order to trigger an intervention by bedside (medical or nursing) staff. Such a reactive management approach is laborious, and makes the consequences of apnoeic events inevitable. Recent technological advances and improved signal processing have allowed the possibility of developing prediction models for apnoeic events in preterm infants. However, the development of such models has numerous challenges and is only starting to show potential. This paper identifies requisite components and current gaps in developing prediction models for apnoeic events, and reviews previous studies on predicting apnoeic events in preterm infants.",2020-09-01,1,1004,44,249
749,33070881,Tele-robotics and artificial-intelligence in stroke care,"In the last forty years, the field of medicine has experienced dramatic shifts in technology-enhanced surgical procedures - from its initial use in 1985 for neurosurgical biopsies to current implementation of systems such as magnetic-guided catheters for endovascular procedures. Systems such as the Niobe Magnetic Navigation system and CorPath GRX have allowed for utilization of a fully integrated surgical robotic systems for perioperative manipulation, as well as tele-controlled manipulation systems for telemedicine. These robotic systems hold tremendous potential for future implementation in cerebrovascular procedures, but lack of relevant clinical experience and uncharted ethical and legal territory for real-life tele-robotics have stalled their adoption for neurovascular surgery, and might present significant challenges for future development and widespread implementation. Yet, the promise that these technologies hold for dramatically improving the quality and accessibility of cerebrovascular procedures such as thrombectomy for acute stroke, drives the research and development of surgical robotics. These technologies, coupled with artificial intelligence (AI) capabilities such as machine learning, deep-learning, and outcome-based analyses and modifications, have the capability to uncover new dimensions within the realm of cerebrovascular surgery.",2020-09-01,1,1371,56,249
2041,33733216,Integration of AI and Machine Learning in Radiotherapy QA,"The use of machine learning and other sophisticated models to aid in prediction and decision making has become widely popular across a breadth of disciplines. Within the greater diagnostic radiology, radiation oncology, and medical physics communities promising work is being performed in tissue classification and cancer staging, outcome prediction, automated segmentation, treatment planning, and quality assurance as well as other areas. In this article, machine learning approaches are explored, highlighting specific applications in machine and patient-specific quality assurance (QA). Machine learning can analyze multiple elements of a delivery system on its performance over time including the multileaf collimator (MLC), imaging system, mechanical and dosimetric parameters. Virtual Intensity-Modulated Radiation Therapy (IMRT) QA can predict passing rates using different measurement techniques, different treatment planning systems, and different treatment delivery machines across multiple institutions. Prediction of QA passing rates and other metrics can have profound implications on the current IMRT process. Here we cover general concepts of machine learning in dosimetry and various methods used in virtual IMRT QA, as well as their clinical applications.",2020-09-01,0,1273,57,249
2043,33733193,The Next Generation of Medical Decision Support: A Roadmap Toward Transparent Expert Companions,"Increasing quality and performance of artificial intelligence (AI) in general and machine learning (ML) in particular is followed by a wider use of these approaches in everyday life. As part of this development, ML classifiers have also gained more importance for diagnosing diseases within biomedical engineering and medical sciences. However, many of those ubiquitous high-performing ML algorithms reveal a black-box-nature, leading to opaque and incomprehensible systems that complicate human interpretations of single predictions or the whole prediction process. This puts up a serious challenge on human decision makers to develop trust, which is much needed in life-changing decision tasks. This paper is designed to answer the question how expert companion systems for decision support can be designed to be interpretable and therefore transparent and comprehensible for humans. On the other hand, an approach for interactive ML as well as human-in-the-loop-learning is demonstrated in order to integrate human expert knowledge into ML models so that humans and machines act as companions within a critical decision task. We especially address the problem of Semantic Alignment between ML classifiers and its human users as a prerequisite for semantically relevant and useful explanations as well as interactions. Our roadmap paper presents and discusses an interdisciplinary yet integrated Comprehensible Artificial Intelligence (cAI)-transition-framework with regard to the task of medical diagnosis. We explain and integrate relevant concepts and research areas to provide the reader with a hands-on-cookbook for achieving the transition from opaque black-box models to interactive, transparent, comprehensible and trustworthy systems. To make our approach tangible, we present suitable state of the art methods with regard to the medical domain and include a realization concept of our framework. The emphasis is on the concept of Mutual Explanations (ME) that we introduce as a dialog-based, incremental process in order to provide human ML users with trust, but also with stronger participation within the learning process.",2020-09-01,0,2136,95,249
2049,33693398,Considerations for a More Ethical Approach to Data in AI: On Data Representation and Infrastructure,"Data shapes the development of Artificial Intelligence (AI) as we currently know it, and for many years centralized networking infrastructures have dominated both the sourcing and subsequent use of such data. Research suggests that centralized approaches result in poor representation, and as AI is now integrated more in daily life, there is a need for efforts to improve on this. The AI research community has begun to explore managing data infrastructures more democratically, finding that decentralized networking allows for more transparency which can alleviate core ethical concerns, such as selection-bias. With this in mind, herein, we present a mini-survey framed around data representation and data infrastructures in AI. We outline four key considerations (auditing, benchmarking, confidence and trust, explainability and interpretability) as they pertain to data-driven AI, and propose that reflection of them, along with improved interdisciplinary discussion may aid the mitigation of data-based AI ethical concerns, and ultimately improve individual wellbeing when interacting with AI.",2020-09-01,0,1099,99,249
56,32333084,"Circulating tumor cells as Trojan Horse for understanding, preventing, and treating cancer: a critical appraisal","Circulating tumor cells (CTCs) are regarded as harbingers of metastases. Their ability to predict response to therapy, relapse, and resistance to treatment has proposed their value as putative diagnostic and prognostic indicators. CTCs represent one of the zeniths of cancer evolution in terms of cell survival; however, the triggers of CTC generation, the identification of potentially metastatic CTCs, and the mechanisms contributing to their heterogeneity and aggressiveness represent issues not yet fully deciphered. Thus, prior to enabling liquid biopsy applications to reach clinical prime time, understanding how the above mechanistic information can be applied to improve treatment decisions is a key challenge. Here, we provide our perspective on how CTCs can provide mechanistic insights into tumor pathogenesis, as well as on CTC clinical value. In doing so, we aim to (a) describe how CTCs disseminate from the primary tumor, and their link to epithelial-mesenchymal transition (EMT); (b) trace the route of CTCs through the circulation, focusing on tumor self-seeding and the possibility of tertiary metastasis; (c) describe possible mechanisms underlying the enhanced metastatic potential of CTCs; (d) discuss how CTC could provide further information on the tissue of origin, especially in cancer of unknown primary origin. We also provide a comprehensive review of meta-analyses assessing the prognostic significance of CTCs, to highlight the emerging role of CTCs in clinical oncology. We also explore how cell-free circulating tumor DNA (ctDNA) analysis, using a combination of genomic and phylogenetic analysis, can offer insights into CTC biology, including our understanding of CTC heterogeneity and tumor evolution. Last, we discuss emerging technologies, such as high-throughput quantitative imaging, radiogenomics, machine learning approaches, and the emerging breath biopsy. These technologies could compliment CTC and ctDNA analyses, and they collectively represent major future steps in cancer detection, monitoring, and management.",2020-09-01,3,2059,112,249
20,32387803,Understanding artificial intelligence based radiology studies: What is overfitting?,"Artificial intelligence (AI) is a broad umbrella term used to encompass a wide variety of subfields dedicated to creating algorithms to perform tasks that mimic human intelligence. As AI development grows closer to clinical integration, radiologists will need to become familiar with the principles of artificial intelligence to properly evaluate and use this powerful tool. This series aims to explain certain basic concepts of artificial intelligence, and their applications in medical imaging starting with a concept of overfitting.",2020-09-01,7,535,83,249
750,33070540,Usefulness of machine learning in COVID-19 for the detection and prognosis of cardiovascular complications,"Since January 2020, coronavirus disease 2019 (COVID-19) has rapidly become a global concern, and its cardiovascular manifestations have highlighted the need for fast, sensitive and specific tools for early identification and risk stratification. Machine learning is a software solution with the ability to analyze large amounts of data and make predictions without prior programming. When faced with new problems with unique challenges as evident in the COVID-19 pandemic, machine learning can offer solutions that are not apparent on the surface by sifting quickly through massive quantities of data and making associations that may have been missed. Artificial intelligence is a broad term that encompasses different tools, including various types of machine learning and deep learning. Here, we review several cardiovascular applications of machine learning and artificial intelligence and their potential applications to cardiovascular diagnosis, prognosis, and therapy in COVID-19 infection.",2020-09-01,0,996,106,249
803,32998213,Frontiers of Robotic Gastroscopy: A Comprehensive Review of Robotic Gastroscopes and Technologies,"Upper gastrointestinal (UGI) tract pathology is common worldwide. With recent advancements in robotics, innovative diagnostic and treatment devices have been developed and several translational attempts made. This review paper aims to provide a highly pictorial critical review of robotic gastroscopes, so that clinicians and researchers can obtain a swift and comprehensive overview of key technologies and challenges. Therefore, the paper presents robotic gastroscopes, either commercial or at a progressed technology readiness level. Among them, we show tethered and wireless gastroscopes, as well as devices aimed for UGI surgery. The technological features of these instruments, as well as their clinical adoption and performance, are described and compared. Although the existing endoscopic devices have thus far provided substantial improvements in the effectiveness of diagnosis and treatment, there are certain aspects that represent unwavering predicaments of the current gastroenterology practice. A detailed list includes difficulties and risks, such as transmission of communicable diseases (e.g., COVID-19) due to the doctor-patient proximity, unchanged learning curves, variable detection rates, procedure-related adverse events, endoscopists' and nurses' burnouts, limited human and/or material resources, and patients' preferences to choose non-invasive options that further interfere with the successful implementation and adoption of routine screening. The combination of robotics and artificial intelligence, as well as remote telehealth endoscopy services, are also discussed, as viable solutions to improve existing platforms for diagnosis and treatment are emerging.",2020-09-01,2,1689,97,249
807,32994452,Machine learning prediction in cardiovascular diseases: a meta-analysis,"Several machine learning (ML) algorithms have been increasingly utilized for cardiovascular disease prediction. We aim to assess and summarize the overall predictive ability of ML algorithms in cardiovascular diseases. A comprehensive search strategy was designed and executed within the MEDLINE, Embase, and Scopus databases from database inception through March 15, 2019. The primary outcome was a composite of the predictive ability of ML algorithms of coronary artery disease, heart failure, stroke, and cardiac arrhythmias. Of 344 total studies identified, 103 cohorts, with a total of 3,377,318 individuals, met our inclusion criteria. For the prediction of coronary artery disease, boosting algorithms had a pooled area under the curve (AUC) of 0.88 (95% CI 0.84-0.91), and custom-built algorithms had a pooled AUC of 0.93 (95% CI 0.85-0.97). For the prediction of stroke, support vector machine (SVM) algorithms had a pooled AUC of 0.92 (95% CI 0.81-0.97), boosting algorithms had a pooled AUC of 0.91 (95% CI 0.81-0.96), and convolutional neural network (CNN) algorithms had a pooled AUC of 0.90 (95% CI 0.83-0.95). Although inadequate studies for each algorithm for meta-analytic methodology for both heart failure and cardiac arrhythmias because the confidence intervals overlap between different methods, showing no difference, SVM may outperform other algorithms in these areas. The predictive ability of ML algorithms in cardiovascular diseases is promising, particularly SVM and boosting algorithms. However, there is heterogeneity among ML algorithms in terms of multiple parameters. This information may assist clinicians in how to interpret data and implement optimal algorithms for their dataset.",2020-09-01,1,1715,71,249
808,32994368,"Big data, machine learning and artificial intelligence: a neurologist's guide","Modern clinical practice requires the integration and interpretation of ever-expanding volumes of clinical data. There is, therefore, an imperative to develop efficient ways to process and understand these large amounts of data. Neurologists work to understand the function of biological neural networks, but artificial neural networks and other forms of machine learning algorithm are likely to be increasingly encountered in clinical practice. As their use increases, clinicians will need to understand the basic principles and common types of algorithm. We aim to provide a coherent introduction to this jargon-heavy subject and equip neurologists with the tools to understand, critically appraise and apply insights from this burgeoning field.",2020-09-01,2,747,77,249
1070,31454239,Programmable One-Pot Synthesis of Oligosaccharides,"Carbohydrates make up one of the four major classes of biomolecules, often conjugated with proteins as glycoproteins or with lipids as glycolipids, and participate in many important biochemical functions in living species. However, glycoproteins or glycolipids often exist as mixtures, and as a consequence, it is difficult to isolate individual glycoproteins or glycolipids as pure forms to understand the role carbohydrates play in the glycoconjugate. Currently, the only feasible way to obtain pure glycoconjugates is through synthesis, and of the many methods developed for the synthesis of oligosaccharides, those with automatic and programmable potential are considered to be more effective for addressing the issues of carbohydrate diversity and related functions. In this Perspective, we describe how data science, including algorithm and machine learning, can be used to assist the chemical synthesis of oligosaccharide in a programmable and one-pot manner and how the programmable method can be used to accelerate the construction of diverse oligosaccharides to facilitate our understanding of glycosylation in biology.",2020-09-01,1,1129,50,249
2000,32888472,Molecular profiling of neuroendocrine tumours to predict response and toxicity to peptide receptor radionuclide therapy,"Peptide receptor radionuclide therapy (PRRT) is a type of radiotherapy that targets peptide receptors and is typically used for neuroendocrine tumours (NETs). Some of the key challenges in its use are the prediction of efficacy and toxicity, patient selection, and response optimisation. In this Review, we assess current knowledge on the molecular profile of NETs and the strategies and tools used to predict, monitor, and assess the toxicity of PRRT. The few mutations in tumour genes that can be evaluated (eg, ATM and DAXX) are limited to pancreatic NETs and are most likely not informative. Assays that are transcriptomic or based on genes are effective in the prediction of radiotherapy response in other cancers. A blood-based assay for eight genes (the PRRT prediction quotient [PPQ]) has an overall accuracy of 95% for predicting responses to PRRT in NETs. No molecular markers exist that can predict the toxicity of PRRT. Candidate molecular targets include seven single nucleotide polymorphisms (SNPs) that are susceptible to radiation. Transcriptomic evaluations of blood and a combination of gene expression and specific SNPs, assessed by machine learning with algorithms that are tumour-specific, might yield molecular tools to enhance the efficacy and safety of PRRT.",2020-09-01,7,1282,119,249
935,31960407,What is AI? Applications of artificial intelligence to dermatology,"In the past, the skills required to make an accurate dermatological diagnosis have required exposure to thousands of patients over many years. However, in recent years, artificial intelligence (AI) has made enormous advances, particularly in the area of image classification. This has led computer scientists to apply these techniques to develop algorithms that are able to recognize skin lesions, particularly melanoma. Since 2017, there have been numerous studies assessing the accuracy of algorithms, with some reporting that the accuracy matches or surpasses that of a dermatologist. While the principles underlying these methods are relatively straightforward, it can be challenging for the practising dermatologist to make sense of a plethora of unfamiliar terms in this domain. Here we explain the concepts of AI, machine learning, neural networks and deep learning, and explore the principles of how these tasks are accomplished. We critically evaluate the studies that have assessed the efficacy of these methods and discuss limitations and potential ethical issues. The burden of skin cancer is growing within the Western world, with major implications for both population skin health and the provision of dermatology services. AI has the potential to assist in the diagnosis of skin lesions and may have particular value at the interface between primary and secondary care. The emerging technology represents an exciting opportunity for dermatologists, who are the individuals best informed to explore the utility of this powerful novel diagnostic tool, and facilitate its safe and ethical implementation within healthcare systems.",2020-09-01,6,1642,66,249
2011,32864600,The myth of generalisability in clinical research and machine learning in health care,An emphasis on overly broad notions of generalisability as it pertains to applications of machine learning in health care can overlook situations in which machine learning might provide clinical utility. We believe that this narrow focus on generalisability should be replaced with wider considerations for the ultimate goal of building machine learning systems that are useful at the bedside.,2020-09-01,7,393,85,249
831,32964109,Mining genetic and transcriptomic data using machine learning approaches in Parkinson's disease,"High-throughput techniques have generated abundant genetic and transcriptomic data of Parkinson's disease (PD) patients but data analysis approaches such as traditional statistical methods have not provided much in the way of insightful integrated analysis or interpretation of the data. As an advanced computational approach, machine learning, which enables people to identify complex patterns and insight from data, has consequently been harnessed to analyze and interpret large, highly complex genetic and transcriptomic data toward a better understanding of PD. In particular, machine learning models have been developed to integrate patient genotype data alone or combined with demographic, clinical, neuroimaging, and other information, for PD outcome study. They have also been used to identify biomarkers of PD based on transcriptomic data, e.g., gene expression profiles from microarrays. This study overviews the relevant literature on using machine learning models for genetic and transcriptomic data analysis in PD, points out remaining challenges, and suggests future directions accordingly. Undoubtedly, the use of machine learning is amplifying PD genetic and transcriptomic achievements for accelerating the study of PD. Existing studies have demonstrated the great potential of machine learning in discovering hidden patterns within genetic or transcriptomic information and thus revealing clues underpinning pathology and pathogenesis. Moving forward, by addressing the remaining challenges, machine learning may advance our ability to precisely diagnose, prognose, and treat PD.",2020-09-01,0,1597,95,249
830,32967266,Applications of Genome-Wide Screening and Systems Biology Approaches in Drug Repositioning,"Modern drug discovery through de novo drug discovery entails high financial costs, low success rates, and lengthy trial periods. Drug repositioning presents a suitable approach for overcoming these issues by re-evaluating biological targets and modes of action of approved drugs. Coupling high-throughput technologies with genome-wide essentiality screens, network analysis, genome-scale metabolic modeling, and machine learning techniques enables the proposal of new drug-target signatures and uncovers unanticipated modes of action for available drugs. Here, we discuss the current issues associated with drug repositioning in light of curated high-throughput multi-omic databases, genome-wide screening technologies, and their application in systems biology/medicine approaches.",2020-09-01,1,781,90,249
2632,32512174,New methodologies in ageing research,"Ageing is arguably the most complex phenotype that occurs in humans. To understand and treat ageing as well as associated diseases, highly specialised technologies are emerging that reveal critical insight into the underlying mechanisms and provide new hope for previously untreated diseases. Herein, we describe the latest developments in cutting edge technologies applied across the field of ageing research. We cover emerging model organisms, high-throughput methodologies and machine-driven approaches. In all, this review will give you a glimpse of what will be pushing the field onwards and upwards.",2020-09-01,1,605,36,249
827,32970997,Artificial Neural Networks for Neuroscientists: A Primer,"Artificial neural networks (ANNs) are essential tools in machine learning that have drawn increasing attention in neuroscience. Besides offering powerful techniques for data analysis, ANNs provide a new approach for neuroscientists to build models for complex behaviors, heterogeneous neural activity, and circuit connectivity, as well as to explore optimization in neural systems, in ways that traditional models are not designed for. In this pedagogical Primer, we introduce ANNs and demonstrate how they have been fruitfully deployed to study neuroscientific questions. We first discuss basic concepts and methods of ANNs. Then, with a focus on bringing this mathematical framework closer to neurobiology, we detail how to customize the analysis, structure, and learning of ANNs to better address a wide range of challenges in brain research. To help readers garner hands-on experience, this Primer is accompanied with tutorial-style code in PyTorch and Jupyter Notebook, covering major topics.",2020-09-01,4,997,56,249
825,32971981,Application of Artificial Intelligence in Early Diagnosis of Spontaneous Preterm Labor and Birth,"This study reviews the current status and future prospective of knowledge on the use of artificial intelligence for the prediction of spontaneous preterm labor and birth (""preterm birth"" hereafter). The summary of review suggests that different machine learning approaches would be optimal for different types of data regarding the prediction of preterm birth: the artificial neural network, logistic regression and/or the random forest for numeric data; the support vector machine for electrohysterogram data; the recurrent neural network for text data; and the convolutional neural network for image data. The ranges of performance measures were 0.79-0.94 for accuracy, 0.22-0.97 for sensitivity, 0.86-1.00 for specificity, and 0.54-0.83 for the area under the receiver operating characteristic curve. The following maternal variables were reported to be major determinants of preterm birth: delivery and pregestational body mass index, age, parity, predelivery systolic and diastolic blood pressure, twins, below high school graduation, infant sex, prior preterm birth, progesterone medication history, upper gastrointestinal tract symptom, gastroesophageal reflux disease, Helicobacter pylori, urban region, calcium channel blocker medication history, gestational diabetes mellitus, prior cone biopsy, cervical length, myomas and adenomyosis, insurance, marriage, religion, systemic lupus erythematosus, hydroxychloroquine sulfate, and increased cerebrospinal fluid and reduced cortical folding due to impaired brain growth.",2020-09-01,3,1528,96,249
824,32972036,An Introduction to Probabilistic Record Linkage with a Focus on Linkage Processing for WTC Registries,"Since its post-World War II inception, the science of record linkage has grown exponentially and is used across industrial, governmental, and academic agencies. The academic fields that rely on record linkage are diverse, ranging from history to public health to demography. In this paper, we introduce the different types of data linkage and give a historical context to their development. We then introduce the three types of underlying models for probabilistic record linkage: Fellegi-Sunter-based methods, machine learning methods, and Bayesian methods. Practical considerations, such as data standardization and privacy concerns, are then discussed. Finally, recommendations are given for organizations developing or maintaining record linkage programs, with an emphasis on organizations measuring long-term complications of disasters, such as 9/11.",2020-09-01,0,854,101,249
823,32972444,Diagnosis of Rare Diseases: a scoping review of clinical decision support systems,"Background:                    Rare Diseases (RDs), which are defined as diseases affecting no more than 5 out of 10,000 people, are often severe, chronic and life-threatening. A main problem is the delay in diagnosing RDs. Clinical decision support systems (CDSSs) for RDs are software systems to support clinicians in the diagnosis of patients with RDs. Due to their clinical importance, we conducted a scoping review to determine which CDSSs are available to support the diagnosis of RDs patients, whether the CDSSs are available to be used by clinicians and which functionalities and data are used to provide decision support.              Methods:                    We searched PubMed for CDSSs in RDs published between December 16, 2008 and December 16, 2018. Only English articles, original peer reviewed journals and conference papers describing a clinical prototype or a routine use of CDSSs were included. For data charting, we used the data items ""Objective and background of the publication/project"", ""System or project name"", ""Functionality"", ""Type of clinical data"", ""Rare Diseases covered"", ""Development status"", ""System availability"", ""Data entry and integration"", ""Last software update"" and ""Clinical usage"".              Results:                    The search identified 636 articles. After title and abstracting screening, as well as assessing the eligibility criteria for full-text screening, 22 articles describing 19 different CDSSs were identified. Three types of CDSSs were classified: ""Analysis or comparison of genetic and phenotypic data,"" ""machine learning"" and ""information retrieval"". Twelve of nineteen CDSSs use phenotypic and genetic data, followed by clinical data, literature databases and patient questionnaires. Fourteen of nineteen CDSSs are fully developed systems and therefore publicly available. Data can be entered or uploaded manually in six CDSSs, whereas for four CDSSs no information for data integration was available. Only seven CDSSs allow further ways of data integration. thirteen CDSS do not provide information about clinical usage.              Conclusions:                    Different CDSS for various purposes are available, yet clinicians have to determine which is best for their patient. To allow a more precise usage, future research has to focus on CDSSs RDs data integration, clinical usage and updating clinical knowledge. It remains interesting which of the CDSSs will be used and maintained in the future.",2020-09-01,1,2473,81,249
2023,32825928,The future of pathology is digital,"Information, archives, and intelligent artificial systems are part of everyday life in modern medicine. They already support medical staff by mapping their workflows with shared availability of cases' referral information, as needed for example, by the pathologist, and this support will be increased in the future even more. In radiology, established standards define information models, data transmission mechanisms, and workflows. Other disciplines, such as pathology, cardiology, and radiation therapy, now define further demands in addition to these established standards. Pathology may have the highest technical demands on the systems, with very complex workflows, and the digitization of slides generating enormous amounts of data up to Gigabytes per biopsy. This requires enormous amounts of data to be generated per biopsy, up to the gigabyte range. Digital pathology allows a change from classical histopathological diagnosis with microscopes and glass slides to virtual microscopy on the computer, with multiple tools using artificial intelligence and machine learning to support pathologists in their future work.",2020-09-01,1,1126,34,249
811,32989410,Leveraging Computational Modeling to Understand Infectious Diseases,"Purpose of review:                    Computational and mathematical modeling have become a critical part of understanding in-host infectious disease dynamics and predicting effective treatments. In this review, we discuss recent findings pertaining to the biological mechanisms underlying infectious diseases, including etiology, pathogenesis, and the cellular interactions with infectious agents. We present advances in modeling techniques that have led to fundamental disease discoveries and impacted clinical translation.              Recent findings:                    Combining mechanistic models and machine learning algorithms has led to improvements in the treatment of Shigella and tuberculosis through the development of novel compounds. Modeling of the epidemic dynamics of malaria at the within-host and between-host level has afforded the development of more effective vaccination and antimalarial therapies. Similarly, in-host and host-host models have supported the development of new HIV treatment modalities and an improved understanding of the immune involvement in influenza. In addition, large-scale transmission models of SARS-CoV-2 have furthered the understanding of coronavirus disease and allowed for rapid policy implementations on travel restrictions and contract tracing apps.              Summary:                    Computational modeling is now more than ever at the forefront of infectious disease research due to the COVID-19 pandemic. This review highlights how infectious diseases can be better understood by connecting scientists from medicine and molecular biology with those in computer science and applied mathematics.",2020-09-01,1,1659,67,249
810,32993652,Using machine learning of clinical data to diagnose COVID-19: a systematic review and meta-analysis,"Background:                    The recent Coronavirus Disease 2019 (COVID-19) pandemic has placed severe stress on healthcare systems worldwide, which is amplified by the critical shortage of COVID-19 tests.              Methods:                    In this study, we propose to generate a more accurate diagnosis model of COVID-19 based on patient symptoms and routine test results by applying machine learning to reanalyzing COVID-19 data from 151 published studies. We aim to investigate correlations between clinical variables, cluster COVID-19 patients into subtypes, and generate a computational classification model for discriminating between COVID-19 patients and influenza patients based on clinical variables alone.              Results:                    We discovered several novel associations between clinical variables, including correlations between being male and having higher levels of serum lymphocytes and neutrophils. We found that COVID-19 patients could be clustered into subtypes based on serum levels of immune cells, gender, and reported symptoms. Finally, we trained an XGBoost model to achieve a sensitivity of 92.5% and a specificity of 97.9% in discriminating COVID-19 patients from influenza patients.              Conclusions:                    We demonstrated that computational methods trained on large clinical datasets could yield ever more accurate COVID-19 diagnostic models to mitigate the impact of lack of testing. We also presented previously unknown COVID-19 clinical variable correlations and clinical subgroups.",2020-09-01,5,1558,99,249
809,32994289,The Gut Microbiome and Individual-Specific Responses to Diet,"Nutritional content and timing are increasingly appreciated to constitute important human variables collectively impacting all aspects of human physiology and disease. However, person-specific mechanisms driving nutritional impacts on the human host remain incompletely understood, while current dietary recommendations remain empirical and nonpersonalized. Precision nutrition aims to harness individualized bodies of data, including the human gut microbiome, in predicting person-specific physiological responses (such as glycemic responses) to food. With these advances notwithstanding, many unknowns remain, including the long-term efficacy of such interventions in delaying or reversing human metabolic disease, mechanisms driving these dietary effects, and the extent of the contribution of the gut microbiome to these processes. We summarize these conceptual advances, while highlighting challenges and means of addressing them in the next decade of study of precision medicine, toward generation of insights that may help to evolve precision nutrition as an effective future tool in a variety of ""multifactorial"" human disorders.",2020-09-01,2,1137,60,249
1988,32908264,Illuminating the dark spaces of healthcare with ambient intelligence,"Advances in machine learning and contactless sensors have given rise to ambient intelligence-physical spaces that are sensitive and responsive to the presence of humans. Here we review how this technology could improve our understanding of the metaphorically dark, unobserved spaces of healthcare. In hospital spaces, early applications could soon enable more efficient clinical workflows and improved patient safety in intensive care units and operating rooms. In daily living spaces, ambient intelligence could prolong the independence of older individuals and improve the management of individuals with a chronic disease by understanding everyday behaviour. Similar to other technologies, transformation into clinical applications at scale must overcome challenges such as rigorous clinical validation, appropriate data privacy and model transparency. Thoughtful use of this technology would enable us to understand the complex interplay between the physical environment and health-critical human behaviours.",2020-09-01,4,1011,68,249
2627,32515148,Radiomics in liver diseases: Current progress and future opportunities,"Liver diseases, a wide spectrum of pathologies from inflammation to neoplasm, have become an increasingly significant health problem worldwide. Noninvasive imaging plays a critical role in the clinical workflow of liver diseases, but conventional imaging assessment may provide limited information. Accurate detection, characterization and monitoring remain challenging. With progress in quantitative imaging analysis techniques, radiomics emerged as an efficient tool that shows promise to aid in personalized diagnosis and treatment decision-making. Radiomics could reflect the heterogeneity of liver lesions via extracting high-throughput and high-dimensional features from multi-modality imaging. Machine learning algorithms are then used to construct clinical target-oriented imaging biomarkers to assist disease management. Here, we review the methodological process in liver disease radiomics studies in a stepwise fashion from data acquisition and curation, region of interest segmentation, liver-specific feature extraction, to task-oriented modelling. Furthermore, the applications of radiomics in liver diseases are outlined in aspects of diagnosis and staging, evaluation of liver tumour biological behaviours, and prognosis according to different disease type. Finally, we discuss the current limitations of radiomics in liver disease studies and explore its future opportunities.",2020-09-01,1,1393,70,249
2156,31746088,Role of Breast MRI in the Evaluation and Detection of DCIS: Opportunities and Challenges,"Historically, breast magnetic resonance imaging (MRI) was not considered an effective modality in the evaluation of ductal carcinoma in situ (DCIS). Over the past decade this has changed, with studies demonstrating that MRI is the most sensitive imaging tool for detection of all grades of DCIS. It has been suggested that not only is breast MRI the most sensitive imaging tool for detection but it may also detect the most clinically relevant DCIS lesions. The role and outcomes of MRI in the preoperative setting for patients with DCIS remains controversial; however, several studies have shown benefit in the preoperative evaluation of extent of disease as well as predicting an underlying invasive component. The most common presentation of DCIS on MRI is nonmass enhancement (NME) in a linear or segmental distribution pattern. Maximizing breast MRI spatial resolution is therefore beneficial, given the frequent presentation of DCIS as NME on MRI. Emerging MRI techniques, such as diffusion-weighted imaging (DWI), have shown promising potential to discriminate DCIS from benign and invasive lesions. Future opportunities including advanced imaging visual techniques, radiomics/radiogenomics, and machine learning / artificial intelligence may also be applicable to the detection and treatment of DCIS. Level of Evidence: 3 Technical Efficacy Stage: 3 J. Magn. Reson. Imaging 2019. J. Magn. Reson. Imaging 2020;52:697-709.",2020-09-01,2,1428,88,249
1870,32694266,Artificial intelligence for retinopathy of prematurity,"Purpose of review:                    In this article, we review the current state of artificial intelligence applications in retinopathy of prematurity (ROP) and provide insight on challenges as well as strategies for bringing these algorithms to the bedside.              Recent findings:                    In the past few years, there has been a dramatic shift from machine learning approaches based on feature extraction to 'deep' convolutional neural networks for artificial intelligence applications. Several artificial intelligence for ROP approaches have demonstrated adequate proof-of-concept performance in research studies. The next steps are to determine whether these algorithms are robust to variable clinical and technical parameters in practice. Integration of artificial intelligence into ROP screening and treatment is limited by generalizability of the algorithms to maintain performance on unseen data and integration of artificial intelligence technology into new or existing clinical workflows.              Summary:                    Real-world implementation of artificial intelligence for ROP diagnosis will require massive efforts targeted at developing standards for data acquisition, true external validation, and demonstration of feasibility. We must now focus on ethical, technical, clinical, regulatory, and financial considerations to bring this technology to the infant bedside to realize the promise offered by this technology to reduce preventable blindness from ROP.",2020-09-01,0,1504,54,249
1871,32692273,The recent application of 3D-QSAR and docking studies to novel HIV-protease inhibitor drug discovery,"Introduction:                    Despite the availability of FDA approved inhibitors of HIV protease, numerous efforts are still ongoing to achieve 'near-perfect' drugs devoid of characteristic adverse side effects, toxicities, and mutational resistance. While experimental methods have been plagued with huge consumption of time and resources, there has been an incessant shift towards the use of computational simulations in HIV protease inhibitor drug discovery.              Areas covered:                    Herein, the authors review the numerous applications of 3D-QSAR modeling methods over recent years relative to the design of new HIV protease inhibitors from a series of experimentally derived compounds. Also, the augmentative contributions of molecular docking are discussed.              Expert opinion:                    Efforts to optimize 3D QSAR and molecular docking for HIV-1 drug discovery are ongoing, which could further incorporate inhibitor motions at the active site using molecular dynamics parameters. Also, highly predictive machine learning algorithms such as random forest, K-means, decision trees, linear regression, hierarchical clustering, and Bayesian classifiers could be employed.",2020-09-01,0,1219,100,249
1442,32627972,Artificial Intelligence and Machine Learning in Computational Nanotoxicology: Unlocking and Empowering Nanomedicine,"Advances in nanomedicine, coupled with novel methods of creating advanced materials at the nanoscale, have opened new perspectives for the development of healthcare and medical products. Special attention must be paid toward safe design approaches for nanomaterial-based products. Recently, artificial intelligence (AI) and machine learning (ML) gifted the computational tool for enhancing and improving the simulation and modeling process for nanotoxicology and nanotherapeutics. In particular, the correlation of in vitro generated pharmacokinetics and pharmacodynamics to in vivo application scenarios is an important step toward the development of safe nanomedicinal products. This review portrays how in vitro and in vivo datasets are used in in silico models to unlock and empower nanomedicine. Physiologically based pharmacokinetic (PBPK) modeling and absorption, distribution, metabolism, and excretion (ADME)-based in silico methods along with dosimetry models as a focus area for nanomedicine are mainly described. The computational OMICS, colloidal particle determination, and algorithms to establish dosimetry for inhalation toxicology, and quantitative structure-activity relationships at nanoscale (nano-QSAR) are revisited. The challenges and opportunities facing the blind spots in nanotoxicology in this computationally dominated era are highlighted as the future to accelerate nanomedicine clinical translation.",2020-09-01,5,1429,115,249
1491,32578340,Microdissection-An Essential Prerequisite for Spatial Cancer Omics,"The problem with cancer tissue is that its intratumoral heterogeneity and its complexity is extremely high as cells possess, depending on their location and function, different mutations, different mRNA expression and the highest intricacy in the protein pattern. Prior to genomic and proteomic analyses, it is therefore indispensable to identify the exact part of the tissue or even the exact cell. Laser-based microdissection is a tried and tested technique able to produce pure and well-defined cell material for further analysis with proteomic and genomic techniques. It sheds light on the heterogeneity of cancer or other complex diseases and enables the identification of biomarkers. This review aims to raise awareness for the reconsideration of laser-based microdissection and seeks to present current state-of-the-art combinations with omic techniques.",2020-09-01,0,861,66,249
1490,32579720,Synthesizing Systems Biology Knowledge from Omics Using Genome-Scale Models,"Omic technologies have enabled the complete readout of the molecular state of a cell at different biological scales. In principle, the combination of multiple omic data types can provide an integrated view of the entire biological system. This integration requires appropriate models in a systems biology approach. Here, genome-scale models (GEMs) are focused upon as one computational systems biology approach for interpreting and integrating multi-omic data. GEMs convert the reactions (related to metabolism, transcription, and translation) that occur in an organism to a mathematical formulation that can be modeled using optimization principles. A variety of genome-scale modeling methods used to interpret multiple omic data types, including genomics, transcriptomics, proteomics, metabolomics, and meta-omics are reviewed. The ability to interpret omics in the context of biological systems has yielded important findings for human health, environmental biotechnology, bioenergy, and metabolic engineering. The authors find that concurrent with advancements in omic technologies, genome-scale modeling methods are also expanding to enable better interpretation of omic data. Therefore, continued synthesis of valuable knowledge, through the integration of omic data with GEMs, are expected.",2020-09-01,1,1297,75,249
1802,32783626,"Functional cardiac CT-Going beyond Anatomical Evaluation of Coronary Artery Disease with Cine CT, CT-FFR, CT Perfusion and Machine Learning","The aim of this review is to provide an overview of different functional cardiac CT techniques which can be used to supplement assessment of the coronary arteries to establish the significance of coronary artery stenoses. We focus on cine-CT, CT-FFR, CT-myocardial perfusion and how developments in machine learning can supplement these techniques.",2020-09-01,1,348,139,249
1434,32634438,Update on therapeutic approaches and emerging therapies for SARS-CoV-2 virus,"The global pandemic of coronavirus disease 2019 (COVID-19), caused by novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has resulted in over 7,273,958 cases with almost over 413,372 deaths worldwide as per the WHO situational report 143 on COVID-19. There are no known treatment regimens with proven efficacy and vaccines thus far, posing an unprecedented challenge to identify effective drugs and vaccines for prevention and treatment. The urgency for its prevention and cure has resulted in an increased number of proposed treatment options. The high rate and volume of emerging clinical trials on therapies for COVID-19 need to be compared and evaluated to provide scientific evidence for effective medical options. Other emerging non-conventional drug discovery techniques such as bioinformatics and cheminformatics, structure-based drug design, network-based methods for prediction of drug-target interactions, artificial intelligence (AI) and machine learning (ML) and phage technique could provide alternative routes to discovering potent Anti-SARS-CoV2 drugs. While drugs are being repurposed and discovered for COVID-19, novel drug delivery systems will be paramount for efficient delivery and avoidance of possible drug resistance. This review describes the proposed drug targets for therapy, and outcomes of clinical trials that have been reported. It also identifies the adopted treatment modalities that are showing promise, and those that have failed as drug candidates. It further highlights various emerging therapies and future strategies for the treatment of COVID-19 and delivery of Anti-SARS-CoV2 drugs.",2020-09-01,18,1641,76,249
2592,32578067,Machine Learning Within Studies of Early-Life Environmental Exposures and Child Health: Review of the Current Literature and Discussion of Next Steps,"Purpose of review:                    The goal of this article is to review the use of machine learning (ML) within studies of environmental exposures and children's health, identify common themes across studies, and provide recommendations to advance their use in research and practice.              Recent findings:                    We identified 42 articles reporting upon the use of ML within studies of environmental exposures and children's health between 2017 and 2019. The common themes among the articles were analysis of mixture data, exposure prediction, disease prediction and forecasting, analysis of complex data, and causal inference. With the increasing complexity of environmental health data, we anticipate greater use of ML to address the challenges that cannot be handled by traditional analytics. In order for these methods to beneficially impact public health, the ML techniques we use need to be appropriate for our study questions, rigorously evaluated and reported in a way that can be critically assessed by the scientific community.",2020-09-01,0,1061,149,249
1864,32702587,The ethics of AI in health care: A mapping review,"This article presents a mapping review of the literature concerning the ethics of artificial intelligence (AI) in health care. The goal of this review is to summarise current debates and identify open questions for future research. Five literature databases were searched to support the following research question: how can the primary ethical risks presented by AI-health be categorised, and what issues must policymakers, regulators and developers consider in order to be 'ethically mindful? A series of screening stages were carried out-for example, removing articles that focused on digital health in general (e.g. data sharing, data access, data privacy, surveillance/nudging, consent, ownership of health data, evidence of efficacy)-yielding a total of 156 papers that were included in the review. We find that ethical issues can be (a) epistemic, related to misguided, inconclusive or inscrutable evidence; (b) normative, related to unfair outcomes and transformative effectives; or (c) related to traceability. We further find that these ethical issues arise at six levels of abstraction: individual, interpersonal, group, institutional, and societal or sectoral. Finally, we outline a number of considerations for policymakers and regulators, mapping these to existing literature, and categorising each as epistemic, normative or traceability-related and at the relevant level of abstraction. Our goal is to inform policymakers, regulators and developers of what they must consider if they are to enable health and care systems to capitalise on the dual advantage of ethical AI; maximising the opportunities to cut costs, improve care, and improve the efficiency of health and care systems, whilst proactively avoiding the potential harms. We argue that if action is not swiftly taken in this regard, a new 'AI winter' could occur due to chilling effects related to a loss of public trust in the benefits of AI for health care.",2020-09-01,3,1936,49,249
2614,32537842,Role of Artificial Intelligence and Machine Learning in Nanosafety,"Robotics and automation provide potentially paradigm shifting improvements in the way materials are synthesized and characterized, generating large, complex data sets that are ideal for modeling and analysis by modern machine learning (ML) methods. Nanomaterials have not yet fully captured the benefits of automation, so lag behind in the application of ML methods of data analysis. Here, some key developments in, and roadblocks to the application of ML methods are reviewed to model and predict potentially adverse biological and environmental effects of nanomaterials. This work focuses on the diverse ways a range of ML algorithms are applied to understand and predict nanomaterials properties, provides examples of the application of traditional ML and deep learning methods to nanosafety, and provides context and future perspectives on developments that are likely to occur, or need to occur in the near future that allow artificial intelligence to make a deeper contribution to nanosafety.",2020-09-01,3,998,66,249
1485,32583141,Computer-aided diagnosis systems for osteoporosis detection: a comprehensive survey,"Computer-aided diagnosis (CAD) has revolutionized the field of medical diagnosis. They assist in improving the treatment potentials and intensify the survival frequency by early diagnosing the diseases in an efficient, timely, and cost-effective way. The automatic segmentation has led the radiologist to successfully segment the region of interest to improve the diagnosis of diseases from medical images which is not so efficiently possible by manual segmentation. The aim of this paper is to survey the vision-based CAD systems especially focusing on the segmentation techniques for the pathological bone disease known as osteoporosis. Osteoporosis is the state of the bones where the mineral density of bones decreases and they become porous, making the bones easily susceptible to fractures by small injury or a fall. The article covers the image acquisition techniques for acquiring the medical images for osteoporosis diagnosis. The article also discusses the advanced machine learning paradigms employed in segmentation for osteoporosis disease. Other image processing steps in osteoporosis like feature extraction and classification are also briefly described. Finally, the paper gives the future directions to improve the osteoporosis diagnosis and presents the proposed architecture. Graphical abstract.",2020-09-01,2,1314,83,249
1815,32768446,Clinical concept extraction: A methodology review,"Background:                    Concept extraction, a subdomain of natural language processing (NLP) with a focus on extracting concepts of interest, has been adopted to computationally extract clinical information from text for a wide range of applications ranging from clinical decision support to care quality improvement.              Objectives:                    In this literature review, we provide a methodology review of clinical concept extraction, aiming to catalog development processes, available methods and tools, and specific considerations when developing clinical concept extraction applications.              Methods:                    Based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, a literature search was conducted for retrieving EHR-based information extraction articles written in English and published from January 2009 through June 2019 from Ovid MEDLINE In-Process & Other Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and the ACM Digital Library.              Results:                    A total of 6,686 publications were retrieved. After title and abstract screening, 228 publications were selected. The methods used for developing clinical concept extraction applications were discussed in this review.",2020-09-01,2,1316,49,249
1798,32792129,Radiomics and Artificial Intelligence for Renal Mass Characterization,"Radiomics allows for high throughput extraction of quantitative data from images. This is an area of active research as groups try to capture and quantify imaging parameters and convert these into descriptive phenotypes of organs or tumors. Texture analysis is one radiomics tool that extracts information about heterogeneity within a given region of interest. This is used with or without associated machine learning classifiers or a deep learning approach is applied to similar types of data. These tools have shown utility in characterizing renal masses, renal cell carcinoma, and assessing response to targeted therapeutic agents in metastatic renal cell carcinoma.",2020-09-01,3,669,69,249
1499,32253595,Intelligent Robotics Incorporating Machine Learning Algorithms for Improving Functional Capacity Evaluation and Occupational Rehabilitation,"Introduction Occupational rehabilitation often involves functional capacity evaluations (FCE) that use simulated work tasks to assess work ability. Currently, there exists no single, streamlined solution to simulate all or a large number of standard work tasks. Such a system would improve FCE and functional rehabilitation through simulating reaching maneuvers and more dexterous functional tasks that are typical of workplace activities. This paper reviews efforts to develop robotic FCE solutions that incorporate machine learning algorithms. Methods We reviewed the literature regarding rehabilitation robotics, with an emphasis on novel techniques incorporating robotics and machine learning into FCE. Results Rehabilitation robotics aims to improve the assessment and rehabilitation of injured workers by providing methods for easily simulating workplace tasks using intelligent robotic systems. Machine learning-based approaches combine the benefits of robotic systems with the expertise and experience of human therapists. These innovations have the potential to improve the quantification of function as well as learn the haptic interactions provided by therapists to assist patients during assessment and rehabilitation. This is done by allowing a robot to learn based on a therapist's motions (""demonstrations"") what the desired workplace activity (""task"") is and how to recreate it for a worker with an injury (""patient""). Through Telerehabilitation and internet connectivity, these robotic assessment techniques can be used over a distance to reach rural and remote locations. Conclusions While the research is in the early stages, robotics with integrated machine learning algorithms have great potential for improving traditional FCE practice.",2020-09-01,2,1758,139,249
1541,32196135,Progress in Computational and Machine-Learning Methods for Heterogeneous Small-Molecule Activation,"The chemical conversion of small molecules such as H2 , H2 O, O2 , N2 , CO2 , and CH4 to energy and chemicals is critical for a sustainable energy future. However, the high chemical stability of these molecules poses grand challenges to the practical implementation of these processes. In this regard, computational approaches such as density functional theory, microkinetic modeling, data science, and machine learning have guided the rational design of catalysts by elucidating mechanistic insights, identifying active sites, and predicting catalytic activity. Here, the theory and methodologies for heterogeneous catalysis and their applications for small-molecule activation are reviewed. An overview of fundamental theory and key computational methods for designing catalysts, including the emerging data science techniques in particular, is given. Applications of these methods for finding efficient heterogeneous catalysts for the activation of the aforementioned small molecules are then surveyed. Finally, promising directions of the computational catalysis field for further outlooks are discussed, focusing on the challenges and opportunities for new methods.",2020-09-01,1,1170,98,249
1797,32792177,The Pediatric Difficult Airway: Updates and Innovations,"Children have unique characteristics that make them particularly vulnerable to perioperative adverse events. Skilled airway management is a cornerstone of high-quality anesthetic management. The use of hybrid airway techniques is a critical tool for the pediatric anesthesiologist. Point-of-care ultrasonography has an expanding role in airway management, from preoperative assessment of airway pathology and gastric contents to confirmation of tracheal intubation and identification of the cricothyroid membrane. The exciting fields of 3-dimensional printing, artificial intelligence, and machine learning are areas of innovation that will transform pediatric difficult airway management in years to come.",2020-09-01,0,706,55,249
2615,32536431,Artificial Intelligence and Echocardiography: A Primer for Cardiac Sonographers,"Artificial intelligence (AI) is emerging as a key component in diagnostic medical imaging, including echocardiography. AI with deep learning has already been used with automated view labeling, measurements, and interpretation. As the development and use of AI in echocardiography increase, potential concerns may be raised by cardiac sonographers and the profession. This report, from a sonographer's perspective, focuses on defining AI, the basics of the technology, identifying some current applications of AI, and how the use of AI may improve patient care in the future.",2020-09-01,3,574,79,249
1560,32162398,Improvement of oral cancer screening quality and reach: The promise of artificial intelligence,"Oral cancer is easily detectable by physical (self) examination. However, many cases of oral cancer are detected late, which causes unnecessary morbidity and mortality. Screening of high-risk populations seems beneficial, but these populations are commonly located in regions with limited access to health care. The advent of information technology and its modern derivative artificial intelligence (AI) promises to improve oral cancer screening but to date, few efforts have been made to apply these techniques and relatively little research has been conducted to retrieve meaningful information from AI data. In this paper, we discuss the promise of AI to improve the quality and reach of oral cancer screening and its potential effect on improving mortality and unequal access to health care around the world.",2020-09-01,1,812,94,249
1852,32713447,The Future of Cardiac Ultrasound in the Neonatal Intensive Care Unit,"Cardiac ultrasound is increasingly used to guide hemodynamic decision making in the neonatal intensive care unit (NICU). This article focuses on likely future progress in training, accreditation, digital connectivity, miniaturization, and modality development. Many documents have been published internationally to guide cardiac ultrasound training, accreditation, and implementation in the NICU, but challenges remain in providing assessments of hemodynamic status without risking missed structural diagnoses. Advances in simulation training and digital connectivity provide an opportunity to standardize approaches across institutions and continents. Development of machine learning and ultrasound modalities in turn provide huge scope for improving robustness and completeness of assessment.",2020-09-01,0,794,68,249
1853,32713443,Machine Learning to Support Hemodynamic Intervention in the Neonatal Intensive Care Unit,"Hemodynamic support in neonatal intensive care is directed at maintaining cardiovascular wellbeing. At present, monitoring of vital signs plays an essential role in augmenting care in a reactive manner. By applying machine learning techniques, a model can be trained to learn patterns in time series data, allowing the detection of adverse outcomes before they become clinically apparent. In this review we provide an overview of the different machine learning techniques that have been used to develop models in hemodynamic care for newborn infants. We focus on their potential benefits, research pitfalls, and challenges related to their implementation in clinical care.",2020-09-01,0,672,88,249
1854,32713088,"Flying blind, or just flying under the radar? The underappreciated power of de novo methods of mass spectrometric peptide identification","Mass spectrometry-based proteomics is a popular and powerful method for precise and highly multiplexed protein identification. The most common method of analyzing untargeted proteomics data is called database searching, where the database is simply a collection of protein sequences from the target organism, derived from genome sequencing. Experimental peptide tandem mass spectra are compared to simplified models of theoretical spectra calculated from the translated genomic sequences. However, in several interesting application areas, such as forensics, archaeology, venomics, and others, a genome sequence may not be available, or the correct genome sequence to use is not known. In these cases, de novo peptide identification can play an important role. De novo methods infer peptide sequence directly from the tandem mass spectrum without reference to a sequence database, usually using graph-based or machine learning algorithms. In this review, we provide a basic overview of de novo peptide identification methods and applications, briefly covering de novo algorithms and tools, and focusing in more depth on recent applications from venomics, metaproteomics, forensics, and characterization of antibody drugs.",2020-09-01,0,1221,136,249
1824,32762970,EHR Integration of PAP Devices in Sleep Medicine Implementation in the Clinical Setting,Positive airway pressure (PAP) therapy integration is a component of electronic health record (EHR) sleep medicine optimization. EHR optimization facilitates telehealth in continuous care population health. A coordinated care plan can leverage early telehealth interventions.,2020-09-01,0,275,87,249
1721,31617274,Flexible Piezoelectric Acoustic Sensors and Machine Learning for Speech Processing,"Flexible piezoelectric acoustic sensors have been developed to generate multiple sound signals with high sensitivity, shifting the paradigm of future voice technologies. Speech recognition based on advanced acoustic sensors and optimized machine learning software will play an innovative interface for artificial intelligence (AI) services. Collaboration and novel approaches between both smart sensors and speech algorithms should be attempted to realize a hyperconnected society, which can offer personalized services such as biometric authentication, AI secretaries, and home appliances. Here, representative developments in speech recognition are reviewed in terms of flexible piezoelectric materials, self-powered sensors, machine learning algorithms, and speaker recognition.",2020-09-01,3,781,82,249
1830,32745966,COPD phenotypes and machine learning cluster analysis: A systematic review and future research agenda,"Chronic Obstructive Pulmonary Disease (COPD) is a highly heterogeneous condition projected to become the third leading cause of death worldwide by 2030. To better characterize this condition, clinicians have classified patients sharing certain symptomatic characteristics, such as symptom intensity and history of exacerbations, into distinct phenotypes. In recent years, the growing use of machine learning algorithms, and cluster analysis in particular, has promised to advance this classification through the integration of additional patient characteristics, including comorbidities, biomarkers, and genomic information. This combination would allow researchers to more reliably identify new COPD phenotypes, as well as better characterize existing ones, with the aim of improving diagnosis and developing novel treatments. Here, we systematically review the last decade of research progress, which uses cluster analysis to identify COPD phenotypes. Collectively, we provide a systematized account of the extant evidence, describe the strengths and weaknesses of the main methods used, identify gaps in the literature, and suggest recommendations for future research.",2020-09-01,1,1171,101,249
1792,32800297,Supervised Machine Learning: A Brief Primer,"Machine learning is increasingly used in mental health research and has the potential to advance our understanding of how to characterize, predict, and treat mental disorders and associated adverse health outcomes (e.g., suicidal behavior). Machine learning offers new tools to overcome challenges for which traditional statistical methods are not well-suited. This paper provides an overview of machine learning with a specific focus on supervised learning (i.e., methods that are designed to predict or classify an outcome of interest). Several common supervised learning methods are described, along with applied examples from the published literature. We also provide an overview of supervised learning model building, validation, and performance evaluation. Finally, challenges in creating robust and generalizable machine learning algorithms are discussed.",2020-09-01,2,862,43,249
1855,32709998,Digital health technologies: opportunities and challenges in rheumatology,"The past decade in rheumatology has seen tremendous innovation in digital health technologies, including the electronic health record, virtual visits, mobile health, wearable technology, digital therapeutics, artificial intelligence and machine learning. The increased availability of these technologies offers opportunities for improving important aspects of rheumatology, including access, outcomes, adherence and research. However, despite its growth in some areas, particularly with non-health-care consumers, digital health technology has not substantially changed the delivery of rheumatology care. This Review discusses key barriers and opportunities to improve application of digital health technologies in rheumatology. Key topics include smart design, voice enablement and the integration of electronic patient-reported outcomes. Smart design involves active engagement with the end users of the technologies, including patients and clinicians through focus groups, user testing sessions and prototype review. Voice enablement using voice assistants could be critical for enabling patients with hand arthritis to effectively use smartphone apps and might facilitate patient engagement with many technologies. Tracking many rheumatic diseases requires frequent monitoring of patient-reported outcomes. Current practice only collects this information sporadically, and rarely between visits. Digital health technology could enable patient-reported outcomes to inform appropriate timing of face-to-face visits and enable improved application of treat-to-target strategies. However, best practice standards for digital health technologies do not yet exist. To achieve the potential of digital health technology in rheumatology, rheumatology professionals will need to be more engaged upstream in the technology design process and provide leadership to effectively incorporate the new tools into clinical care.",2020-09-01,3,1915,73,249
2008,32866909,Machine learning techniques for detecting electrode misplacement and interchanges when recording ECGs: A systematic review and meta-analysis,"Introduction:                    Electrode misplacement and interchange errors are known problems when recording the 12lead electrocardiogram (ECG). Automatic detection of these errors could play an important role for improving clinical decision making and outcomes in cardiac care. The objectives of this systematic review and meta-analysis is to 1) study the impact of electrode misplacement on ECG signals and ECG interpretation, 2) to determine the most challenging electrode misplacements to detect using machine learning (ML), 3) to analyse the ML performance of algorithms that detect electrode misplacement or interchange according to sensitivity and specificity and 4) to identify the most commonly used ML technique for detecting electrode misplacement/interchange. This review analysed the current literature regarding electrode misplacement/interchange recognition accuracy using machine learning techniques.              Method:                    A search of three online databases including IEEE, PubMed and ScienceDirect identified 228 articles, while 3 articles were included from additional sources from co-authors. According to the eligibility criteria, 14 articles were selected. The selected articles were considered for qualitative analysis and meta-analysis.              Results:                    The articles showed the effect of lead interchange on ECG morphology and as a consequence on patient diagnoses. Statistical analysis of the included articles found that machine learning performance is high in detecting electrode misplacement/interchange except left arm/left leg interchange.              Conclusion:                    This review emphasises the importance of detecting electrode misplacement detection in ECG diagnosis and the effects on decision making. Machine learning shows promise in detecting lead misplacement/interchange and highlights an opportunity for developing and operationalising deep learning algorithms such as convolutional neural network (CNN) to detect electrode misplacement/interchange.",2020-10-01,0,2050,140,219
1999,32888634,Identification of Patients with Heart Failure in Large Datasets,"Large registries, administrative data, and the electronic health record (EHR) offer opportunities to identify patients with heart failure, which can be used for research purposes, process improvement, and optimal care delivery. Identification of cases is challenging because of the heterogeneous nature of the disease, which encompasses various phenotypes that may respond differently to treatment. The increasing availability of both structured and unstructured data in the EHR has expanded opportunities for cohort construction. This article reviews the current literature on approaches to identification of heart failure, and looks toward the future of machine learning, big data, and phenomapping.",2020-10-01,0,701,63,219
2004,32873407,Cancer Nanomedicines in an Evolving Oncology Landscape,"Nanomedicine represents an important class of cancer therapy. Clinical translation of cancer nanomedicine has significantly reduced the toxicity and adverse consequences of standard-of-care chemotherapy. Recent advances in new cancer treatment modalities (e.g., gene and immune therapies) are profoundly changing the oncology landscape, bringing with them new requirements and challenges for next-generation cancer nanomedicines. We present an overview of cancer nanomedicines in four emerging oncology-associated fields: (i) gene therapy, (ii) immunotherapy, (iii) extracellular vesicle (EV) therapy, and (iv) machine learning-assisted therapy. We discuss the incorporation of nanomedicine into these emerging disciplines, present prominent examples, and evaluate their advantages and challenges. Finally, we discuss future opportunities for next-generation cancer nanomedicines.",2020-10-01,4,880,54,219
1998,32888635,Predicting High-Risk Patients and High-Risk Outcomes in Heart Failure,"Identifying patients with heart failure at high risk for poor outcomes is important for patient care, resource allocation, and process improvement. Although numerous risk models exist to predict mortality, hospitalization, and patient-reported health status, they are infrequently used for several reasons, including modest performance, lack of evidence to support routine clinical use, and barriers to implementation. Artificial intelligence has the potential to enhance the performance of risk prediction models, but has its own limitations and remains unproved.",2020-10-01,1,564,69,219
1997,32891632,Sequence-enabled community-based microbial source tracking in surface waters using machine learning classification: A review,"The development of Microbial Source Tracking (MST) technologies was borne out of necessity. This was largely due to the: 1) inadequacies of the fecal indicator bacterial paradigm, 2) fact that many fecal bacteria can survive and often grow in the environment, 3) inability of traditional microbiological methods to attribute source, 4) lack of correspondence between numbers of fecal indicator bacteria in waterways and many human pathogens, and 5) source allocation requirements and load determinations needed for total maximum daily loads. The MST tools have changed over time, evolving from culture-dependent to culture-independent molecular analyses. More recently, MST tools based on microbial community analyses, mainly DNA sequencing-based approaches, have been developed in an attempt to overcome some of these issues. These approaches generate large data sets and require the use of sophisticated machine learning algorithms to allocate potential host sources to contaminated waterways. In this review we discuss the origins and needs for community-based MST methods, as well as elaborate on the Bayesian algorithm-based program SourceTracker, which is increasingly being used for the determination of sources of fecal contamination of waterways.",2020-10-01,1,1255,124,219
2003,32873936,Beyond dichotomies in reinforcement learning,"Reinforcement learning (RL) is a framework of particular importance to psychology, neuroscience and machine learning. Interactions between these fields, as promoted through the common hub of RL, has facilitated paradigm shifts that relate multiple levels of analysis in a singular framework (for example, relating dopamine function to a computationally defined RL signal). Recently, more sophisticated RL algorithms have been proposed to better account for human learning, and in particular its oft-documented reliance on two separable systems: a model-based (MB) system and a model-free (MF) system. However, along with many benefits, this dichotomous lens can distort questions, and may contribute to an unnecessarily narrow perspective on learning and decision-making. Here, we outline some of the consequences that come from overconfidently mapping algorithms, such as MB versus MF RL, with putative cognitive processes. We argue that the field is well positioned to move beyond simplistic dichotomies, and we propose a means of refocusing research questions towards the rich and complex components that comprise learning and decision-making.",2020-10-01,0,1146,44,219
1451,32618431,Advances in accelerometry for cardiovascular patients: a systematic review with practical recommendations,"Aims:                    Accelerometers are becoming increasingly commonplace for assessing physical activity; however, their use in patients with cardiovascular diseases is relatively substandard. We aimed to systematically review the methods used for collecting and processing accelerometer data in cardiology, using the example of heart failure, and to provide practical recommendations on how to improve objective physical activity assessment in patients with cardiovascular diseases by using accelerometers.              Methods and results:                    Four electronic databases were searched up to September 2019 for observational, interventional, and validation studies using accelerometers to assess physical activity in patients with heart failure. Study and population characteristics, details of accelerometry data collection and processing, and description of physical activity metrics were extracted from the eligible studies and synthesized. To assess the quality and completeness of accelerometer reporting, the studies were scored using 12 items on data collection and processing, such as the placement of accelerometer, days of data collected, and criteria for non-wear of the accelerometer. In 60 eligible studies with 3500 patients (of those, 536 were heart failure with preserved ejection fraction patients), a wide variety of accelerometer brands (n = 27) and models (n = 46) were used, with Actigraph being the most frequent (n = 12), followed by Fitbit (n = 5). The accelerometer was usually worn on the hip (n = 32), and the most prevalent wear period was 7 days (n = 22). The median wear time required for a valid day was 600 min, and between two and five valid days was required for a patient to be included in the analysis. The most common measures of physical activity were steps (n = 20), activity counts (n = 15), and time spent in moderate-to-vigorous physical activity (n = 14). Only three studies validated accelerometers in a heart failure population, showing that their accuracy deteriorates at slower speeds. Studies failed to report between one and six (median 4) of the 12 scored items, with non-wear time criteria and valid day definition being the most underreported items.              Conclusions:                    The use of accelerometers in cardiology lacks consistency and reporting on data collection, and processing methods need to be improved. Furthermore, calculating metrics based on raw acceleration and machine learning techniques is lacking, opening the opportunity for future exploration. Therefore, we encourage researchers and clinicians to improve the quality and transparency of data collection and processing by following our proposed practical recommendations for using accelerometers in patients with cardiovascular diseases, which are outlined in the article.",2020-10-01,3,2832,105,219
1449,32621068,A mapping study of ensemble classification methods in lung cancer decision support systems,"Achieving a high level of classification accuracy in medical datasets is a capital need for researchers to provide effective decision systems to assist doctors in work. In many domains of artificial intelligence, ensemble classification methods are able to improve the performance of single classifiers. This paper reports the state of the art of ensemble classification methods in lung cancer detection. We have performed a systematic mapping study to identify the most interesting papers concerning this topic. A total of 65 papers published between 2000 and 2018 were selected after an automatic search in four digital libraries and a careful selection process. As a result, it was observed that diagnosis was the task most commonly studied; homogeneous ensembles and decision trees were the most frequently adopted for constructing ensembles; and the majority voting rule was the predominant combination rule. Few studies considered the parameter tuning of the techniques used. These findings open several perspectives for researchers to enhance lung cancer research by addressing the identified gaps, such as investigating different classification methods, proposing other heterogeneous ensemble methods, and using new combination rules. Graphical abstract Main features of the mapping study performed in ensemble classification methods applied on lung cancer decision support systems.",2020-10-01,0,1390,90,219
2412,30834534,Machine learning in plant-pathogen interactions: empowering biological predictions from field scale to genome scale,"Machine learning (ML) encompasses statistical methods that learn to identify patterns in complex datasets. Here, I review application areas in plant-pathogen interactions that have recently benefited from ML, such as disease monitoring, the discovery of gene regulatory networks, genomic selection for disease resistance and prediction of pathogen effectors. However, achieving robust performance from ML is not trivial and requires knowledge of both the methodology and the biology. I discuss common pitfalls and challenges in using ML approaches. Finally, I highlight future opportunities for ML as a tool for dissecting plant-pathogen interactions using high-throughput data, for example, through integration of diverse data sources and the analysis with higher resolution, such as from individual cells or on elaborate spatial and temporal scales.",2020-10-01,7,851,115,219
1992,32905155,Artificial intelligence and deep learning in ophthalmology - present and future (Review),"Since its introduction in 1959, artificial intelligence technology has evolved rapidly and helped benefit research, industries and medicine. Deep learning, as a process of artificial intelligence (AI) is used in ophthalmology for data analysis, segmentation, automated diagnosis and possible outcome predictions. The association of deep learning and optical coherence tomography (OCT) technologies has proven reliable for the detection of retinal diseases and improving the diagnostic performance of the eye's posterior segment diseases. This review explored the possibility of implementing and using AI in establishing the diagnosis of retinal disorders. The benefits and limitations of AI in the field of retinal disease medical management were investigated by analyzing the most recent literature data. Furthermore, the future trends of AI involvement in ophthalmology were analyzed, as AI will be part of the decision-making regarding the scientific investigation, diagnosis and therapeutic management.",2020-10-01,1,1006,88,219
1252,32061798,"Machine learning in infection management using routine electronic health records: tools, techniques, and reporting of future technologies","Background:                    Machine learning (ML) is increasingly being used in many areas of health care. Its use in infection management is catching up as identified in a recent review in this journal. We present here a complementary review to this work.              Objectives:                    To support clinicians and researchers in navigating through the methodological aspects of ML approaches in the field of infection management.              Sources:                    A Medline search was performed with the keywords artificial intelligence, machine learning, infection, and infectious disease for the years 2014-2019. Studies using routinely available electronic hospital record data from an inpatient setting with a focus on bacterial and fungal infections were included.              Content:                    Fifty-two studies were included and divided into six groups based on their focus. These studies covered detection/prediction of sepsis (n = 19), hospital-acquired infections (n = 11), surgical site infections and other postoperative infections (n = 11), microbiological test results (n = 4), infections in general (n = 2), musculoskeletal infections (n = 2), and other topics (urinary tract infections, deep fungal infections, antimicrobial prescriptions; n = 1 each). In total, 35 different ML techniques were used. Logistic regression was applied in 18 studies followed by random forest, support vector machines, and artificial neural networks in 18, 12, and seven studies, respectively. Overall, the studies were very heterogeneous in their approach and their reporting. Detailed information on data handling and software code was often missing. Validation on new datasets and/or in other institutions was rarely done. Clinical studies on the impact of ML in infection management were lacking.              Implications:                    Promising approaches for ML use in infectious diseases were identified. But building trust in these new technologies will require improved reporting. Explainability and interpretability of the models used were rarely addressed and should be further explored. Independent model validation and clinical studies evaluating the added value of ML approaches are needed.",2020-10-01,6,2243,137,219
1872,32691219,A machine learning approach for mortality prediction only using non-invasive parameters,"At present, the traditional scoring methods generally utilize laboratory measurements to predict mortality. It results in difficulties of early mortality prediction in the rural areas lack of professional laboratorians and medical laboratory equipment. To improve the efficiency, accuracy, and applicability of mortality prediction in the remote areas, a novel mortality prediction method based on machine learning algorithms is proposed, which only uses non-invasive parameters readily available from ordinary monitors and manual measurement. A new feature selection method based on the Bayes error rate is developed to select valuable features. Based on non-invasive parameters, four machine learning models were trained for early mortality prediction. The subjects contained in this study suffered from general critical diseases including but not limited to cancer, bone fracture, and diarrhea. Comparison tests among five traditional scoring methods and these four machine learning models with and without laboratory measurement variables are performed. Only using the non-invasive parameters, the LightGBM algorithms have an excellent performance with the largest accuracy of 0.797 and AUC of 0.879. There is no apparent difference between the mortality prediction performance with and without laboratory measurement variables for the four machine learning methods. After reducing the number of feature variables to no more than 50, the machine learning models still outperform the traditional scoring systems, with AUC higher than 0.83. The machine learning approaches only using non-invasive parameters achieved an excellent mortality prediction performance and can equal those using extra laboratory measurements, which can be applied in rural areas and remote battlefield for mortality risk evaluation. Graphical abstract.",2020-10-01,0,1831,87,219
1358,33142863,Federated Learning in Smart City Sensing: Challenges and Opportunities,"Smart Cities sensing is an emerging paradigm to facilitate the transition into smart city services. The advent of the Internet of Things (IoT) and the widespread use of mobile devices with computing and sensing capabilities has motivated applications that require data acquisition at a societal scale. These valuable data can be leveraged to train advanced Artificial Intelligence (AI) models that serve various smart services that benefit society in all aspects. Despite their effectiveness, legacy data acquisition models backed with centralized Machine Learning models entail security and privacy concerns, and lead to less participation in large-scale sensing and data provision for smart city services. To overcome these challenges, Federated Learning is a novel concept that can serve as a solution to the privacy and security issues encountered within the process of data collection. This survey article presents an overview of smart city sensing and its current challenges followed by the potential of Federated Learning in addressing those challenges. A comprehensive discussion of the state-of-the-art methods for Federated Learning is provided along with an in-depth discussion on the applicability of Federated Learning in smart city sensing; clear insights on open issues, challenges, and opportunities in this field are provided as guidance for the researchers studying this subject matter.",2020-10-01,1,1404,70,219
1357,33143137,A Review of the Important Role of CYP2D6 in Pharmacogenomics,"Cytochrome P450 2D6 (CYP2D6) is a critical pharmacogene involved in the metabolism of ~20% of commonly used drugs across a broad spectrum of medical disciplines including psychiatry, pain management, oncology and cardiology. Nevertheless, CYP2D6 is highly polymorphic with single-nucleotide polymorphisms, small insertions/deletions and larger structural variants including multiplications, deletions, tandem arrangements, and hybridisations with non-functional CYP2D7 pseudogenes. The frequency of these variants differs across populations, and they significantly influence the drug-metabolising enzymatic function of CYP2D6. Importantly, altered CYP2D6 function has been associated with both adverse drug reactions and reduced drug efficacy, and there is growing recognition of the clinical and economic burdens associated with suboptimal drug utilisation. To date, pharmacogenomic clinical guidelines for at least 48 CYP2D6-substrate drugs have been developed by prominent pharmacogenomics societies, which contain therapeutic recommendations based on CYP2D6-predicted categories of metaboliser phenotype. Novel algorithms to interpret CYP2D6 function from sequencing data that consider structural variants, and machine learning approaches to characterise the functional impact of novel variants, are being developed. However, CYP2D6 genotyping is yet to be implemented broadly into clinical practice, and so further effort and initiatives are required to overcome the implementation challenges and deliver the potential benefits to the bedside.",2020-10-01,3,1548,60,219
1352,33154949,Artificial Intelligence (AI)-Based Systems Biology Approaches in Multi-Omics Data Analysis of Cancer,"Cancer is the manifestation of abnormalities of different physiological processes involving genes, DNAs, RNAs, proteins, and other biomolecules whose profiles are reflected in different omics data types. As these bio-entities are very much correlated, integrative analysis of different types of omics data, multi-omics data, is required to understanding the disease from the tumorigenesis to the disease progression. Artificial intelligence (AI), specifically machine learning algorithms, has the ability to make decisive interpretation of ""big""-sized complex data and, hence, appears as the most effective tool for the analysis and understanding of multi-omics data for patient-specific observations. In this review, we have discussed about the recent outcomes of employing AI in multi-omics data analysis of different types of cancer. Based on the research trends and significance in patient treatment, we have primarily focused on the AI-based analysis for determining cancer subtypes, disease prognosis, and therapeutic targets. We have also discussed about AI analysis of some non-canonical types of omics data as they have the capability of playing the determiner role in cancer patient care. Additionally, we have briefly discussed about the data repositories because of their pivotal role in multi-omics data storing, processing, and analysis.",2020-10-01,1,1351,100,219
1345,33162885,Classifying Intracortical Brain-Machine Interface Signal Disruptions Based on System Performance and Applicable Compensatory Strategies: A Review,"Brain-machine interfaces (BMIs) record and translate neural activity into a control signal for assistive or other devices. Intracortical microelectrode arrays (MEAs) enable high degree-of-freedom BMI control for complex tasks by providing fine-resolution neural recording. However, chronically implanted MEAs are subject to a dynamic in vivo environment where transient or systematic disruptions can interfere with neural recording and degrade BMI performance. Typically, neural implant failure modes have been categorized as biological, material, or mechanical. While this categorization provides insight into a disruption's causal etiology, it is less helpful for understanding degree of impact on BMI function or possible strategies for compensation. Therefore, we propose a complementary classification framework for intracortical recording disruptions that is based on duration of impact on BMI performance and requirement for and responsiveness to interventions: (1) Transient disruptions interfere with recordings on the time scale of minutes to hours and can resolve spontaneously; (2) Reversible disruptions cause persistent interference in recordings but the root cause can be remedied by an appropriate intervention; (3) Irreversible compensable disruptions cause persistent or progressive decline in signal quality, but their effects on BMI performance can be mitigated algorithmically; and (4) Irreversible non-compensable disruptions cause permanent signal loss that is not amenable to remediation or compensation. This conceptualization of intracortical BMI disruption types is useful for highlighting specific areas for potential hardware improvements and also identifying opportunities for algorithmic interventions. We review recording disruptions that have been reported for MEAs and demonstrate how biological, material, and mechanical mechanisms of disruption can be further categorized according to their impact on signal characteristics. Then we discuss potential compensatory protocols for each of the proposed disruption classes. Specifically, transient disruptions may be minimized by using robust neural decoder features, data augmentation methods, adaptive machine learning models, and specialized signal referencing techniques. Statistical Process Control methods can identify reparable disruptions for rapid intervention. In-vivo diagnostics such as impedance spectroscopy can inform neural feature selection and decoding models to compensate for irreversible disruptions. Additional compensatory strategies for irreversible disruptions include information salvage techniques, data augmentation during decoder training, and adaptive decoding methods to down-weight damaged channels.",2020-10-01,0,2712,145,219
1343,33163152,Data mining in Raman imaging in a cellular biological system,"The distribution and dynamics of biomolecules in the cell is of critical interest in biological research. Raman imaging techniques have expanded our knowledge of cellular biological systems significantly. The technological developments that have led to the optimization of Raman instrumentation have helped to improve the speed of the measurement and the sensitivity. As well as instrumental developments, data mining plays a significant role in revealing the complicated chemical information contained within the spectral data. A number of data mining methods have been applied to extract the spectral information and translate them into biological information. Single-cell visualization, cell classification and biomolecular/drug quantification have all been achieved by the application of data mining to Raman imaging data. Herein we summarize the framework for Raman imaging data analysis, which involves preprocessing, pattern recognition and validation. There are multiple methods developed for each stage of analysis. The characteristics of these methods are described in relation to their application in Raman imaging of the cell. Furthermore, we summarize the software that can facilitate the implementation of these methods. Through its careful selection and application, data mining can act as an essential tool in the exploration of information-rich Raman spectral data.",2020-10-01,0,1382,60,219
1337,33178623,"Diagnostic Methods, Clinical Guidelines, and Antibiotic Treatment for Group A Streptococcal Pharyngitis: A Narrative Review","The most common bacterial cause of pharyngitis is infection by Group A -hemolytic streptococcus (GABHS), commonly known as strep throat. 5-15% of adults and 15-35% of children in the United States with pharyngitis have a GABHS infection. The symptoms of GABHS overlap with non-GABHS and viral causes of acute pharyngitis, complicating the problem of diagnosis. A careful physical examination and patient history is the starting point for diagnosing GABHS. After a physical examination and patient history is completed, five types of diagnostic methods can be used to ascertain the presence of a GABHS infection: clinical scoring systems, rapid antigen detection tests, throat culture, nucleic acid amplification tests, and machine learning and artificial intelligence. Clinical guidelines developed by professional associations can help medical professionals choose among available techniques to diagnose strep throat. However, guidelines for diagnosing GABHS created by the American and European professional associations vary significantly, and there is substantial evidence that most physicians do not follow any published guidelines. Treatment for GABHS using analgesics, antipyretics, and antibiotics seeks to provide symptom relief, shorten the duration of illness, prevent nonsuppurative and suppurative complications, and decrease the risk of contagion, while minimizing the unnecessary use of antibiotics. There is broad agreement that antibiotics with narrow spectrums of activity are appropriate for treating strep throat. But whether and when patients should be treated with antibiotics for GABHS remains a controversial question. There is no clearly superior management strategy for strep throat, as significant controversy exists regarding the best methods to diagnose GABHS and under what conditions antibiotics should be prescribed.",2020-10-01,0,1849,123,219
1330,33192435,Neurorobots as a Means Toward Neuroethology and Explainable AI,"Understanding why deep neural networks and machine learning algorithms act as they do is a difficult endeavor. Neuroscientists are faced with similar problems. One way biologists address this issue is by closely observing behavior while recording neurons or manipulating brain circuits. This has been called neuroethology. In a similar way, neurorobotics can be used to explain how neural network activity leads to behavior. In real world settings, neurorobots have been shown to perform behaviors analogous to animals. Moreover, a neuroroboticist has total control over the network, and by analyzing different neural groups or studying the effect of network perturbations (e.g., simulated lesions), they may be able to explain how the robot's behavior arises from artificial brain activity. In this paper, we review neurorobot experiments by focusing on how the robot's behavior leads to a qualitative and quantitative explanation of neural activity, and vice versa, that is, how neural activity leads to behavior. We suggest that using neurorobots as a form of computational neuroethology can be a powerful methodology for understanding neuroscience, as well as for artificial intelligence and machine learning.",2020-10-01,0,1213,62,219
1329,33192663,Illuminating the Black Box: Interpreting Deep Neural Network Models for Psychiatric Research,"Psychiatric research is often confronted with complex abstractions and dynamics that are not readily accessible or well-defined to our perception and measurements, making data-driven methods an appealing approach. Deep neural networks (DNNs) are capable of automatically learning abstractions in the data that can be entirely novel and have demonstrated superior performance over classical machine learning models across a range of tasks and, therefore, serve as a promising tool for making new discoveries in psychiatry. A key concern for the wider application of DNNs is their reputation as a ""black box"" approach-i.e., they are said to lack transparency or interpretability of how input data are transformed to model outputs. In fact, several existing and emerging tools are providing improvements in interpretability. However, most reviews of interpretability for DNNs focus on theoretical and/or engineering perspectives. This article reviews approaches to DNN interpretability issues that may be relevant to their application in psychiatric research and practice. It describes a framework for understanding these methods, reviews the conceptual basis of specific methods and their potential limitations, and discusses prospects for their implementation and future directions.",2020-10-01,0,1281,92,219
1328,33193532,Modern Strategies to Assess and Breed Forest Tree Adaptation to Changing Climate,"Studying the genetics of adaptation to new environments in ecologically and industrially important tree species is currently a major research line in the fields of plant science and genetic improvement for tolerance to abiotic stress. Specifically, exploring the genomic basis of local adaptation is imperative for assessing the conditions under which trees will successfully adapt in situ to global climate change. However, this knowledge has scarcely been used in conservation and forest tree improvement because woody perennials face major research limitations such as their outcrossing reproductive systems, long juvenile phase, and huge genome sizes. Therefore, in this review we discuss predictive genomic approaches that promise increasing adaptive selection accuracy and shortening generation intervals. They may also assist the detection of novel allelic variants from tree germplasm, and disclose the genomic potential of adaptation to different environments. For instance, natural populations of tree species invite using tools from the population genomics field to study the signatures of local adaptation. Conventional genetic markers and whole genome sequencing both help identifying genes and markers that diverge between local populations more than expected under neutrality, and that exhibit unique signatures of diversity indicative of ""selective sweeps."" Ultimately, these efforts inform the conservation and breeding status capable of pivoting forest health, ecosystem services, and sustainable production. Key long-term perspectives include understanding how trees' phylogeographic history may affect the adaptive relevant genetic variation available for adaptation to environmental change. Encouraging ""big data"" approaches (machine learning-ML) capable of comprehensively merging heterogeneous genomic and ecological datasets is becoming imperative, too.",2020-10-01,3,1877,80,219
1327,33193667,A Review of Integrative Imputation for Multi-Omics Datasets,"Multi-omics studies, which explore the interactions between multiple types of biological factors, have significant advantages over single-omics analysis for their ability to provide a more holistic view of biological processes, uncover the causal and functional mechanisms for complex diseases, and facilitate new discoveries in precision medicine. However, omics datasets often contain missing values, and in multi-omics study designs it is common for individuals to be represented for some omics layers but not all. Since most statistical analyses cannot be applied directly to the incomplete datasets, imputation is typically performed to infer the missing values. Integrative imputation techniques which make use of the correlations and shared information among multi-omics datasets are expected to outperform approaches that rely on single-omics information alone, resulting in more accurate results for the subsequent downstream analyses. In this review, we provide an overview of the currently available imputation methods for handling missing values in bioinformatics data with an emphasis on multi-omics imputation. In addition, we also provide a perspective on how deep learning methods might be developed for the integrative imputation of multi-omics datasets.",2020-10-01,2,1271,59,219
1326,33194711,A Review on Application of Deep Learning Algorithms in External Beam Radiotherapy Automated Treatment Planning,"Treatment planning plays an important role in the process of radiotherapy (RT). The quality of the treatment plan directly and significantly affects patient treatment outcomes. In the past decades, technological advances in computer and software have promoted the development of RT treatment planning systems with sophisticated dose calculation and optimization algorithms. Treatment planners now have greater flexibility in designing highly complex RT treatment plans in order to mitigate the damage to healthy tissues better while maximizing radiation dose to tumor targets. Nevertheless, treatment planning is still largely a time-inefficient and labor-intensive process in current clinical practice. Artificial intelligence, including machine learning (ML) and deep learning (DL), has been recently used to automate RT treatment planning and has gained enormous attention in the RT community due to its great promises in improving treatment planning quality and efficiency. In this article, we reviewed the historical advancement, strengths, and weaknesses of various DL-based automated RT treatment planning techniques. We have also discussed the challenges, issues, and potential research directions of DL-based automated RT treatment planning techniques.",2020-10-01,0,1261,110,219
1311,31989390,Computer-Aided Histopathological Image Analysis Techniques for Automated Nuclear Atypia Scoring of Breast Cancer: a Review,"Breast cancer is the most common type of malignancy diagnosed in women. Through early detection and diagnosis, there is a great chance of recovery and thereby reduce the mortality rate. Many preliminary tests like non-invasive radiological diagnosis using ultrasound, mammography, and MRI are widely used for the diagnosis of breast cancer. However, histopathological analysis of breast biopsy specimen is inevitable and is considered to be the golden standard for the affirmation of cancer. With the advancements in the digital computing capabilities, memory capacity, and imaging modalities, the development of computer-aided powerful analytical techniques for histopathological data has increased dramatically. These automated techniques help to alleviate the laborious work of the pathologist and to improve the reproducibility and reliability of the interpretation. This paper reviews and summarizes digital image computational algorithms applied on histopathological breast cancer images for nuclear atypia scoring and explores the future possibilities. The algorithms for nuclear pleomorphism scoring of breast cancer can be widely grouped into two categories: handcrafted feature-based and learned feature-based. Handcrafted feature-based algorithms mainly include the computational steps like pre-processing the images, segmenting the nuclei, extracting unique features, feature selection, and machine learning-based classification. However, most of the recent algorithms are based on learned features, that extract high-level abstractions directly from the histopathological images utilizing deep learning techniques. In this paper, we discuss the various algorithms applied for the nuclear pleomorphism scoring of breast cancer, discourse the challenges to be dealt with, and outline the importance of benchmark datasets. A comparative analysis of some prominent works on breast cancer nuclear atypia scoring is done using a benchmark dataset which enables to quantitatively measure and compare the different features and algorithms used for breast cancer grading. Results show that improvements are still required, to have an automated cancer grading system suitable for clinical applications.",2020-10-01,3,2205,122,219
1962,32946413,Artificial Intelligence for COVID-19: Rapid Review,"Background:                    COVID-19 was first discovered in December 2019 and has since evolved into a pandemic.              Objective:                    To address this global health crisis, artificial intelligence (AI) has been deployed at various levels of the health care system. However, AI has both potential benefits and limitations. We therefore conducted a review of AI applications for COVID-19.              Methods:                    We performed an extensive search of the PubMed and EMBASE databases for COVID-19-related English-language studies published between December 1, 2019, and March 31, 2020. We supplemented the database search with reference list checks. A thematic analysis and narrative review of AI applications for COVID-19 was conducted.              Results:                    In total, 11 papers were included for review. AI was applied to COVID-19 in four areas: diagnosis, public health, clinical decision making, and therapeutics. We identified several limitations including insufficient data, omission of multimodal methods of AI-based assessment, delay in realization of benefits, poor internal/external validation, inability to be used by laypersons, inability to be used in resource-poor settings, presence of ethical pitfalls, and presence of legal barriers. AI could potentially be explored in four other areas: surveillance, combination with big data, operation of other core clinical services, and management of patients with COVID-19.              Conclusions:                    In view of the continuing increase in the number of cases, and given that multiple waves of infections may occur, there is a need for effective methods to help control the COVID-19 pandemic. Despite its shortcomings, AI holds the potential to greatly augment existing human efforts, which may otherwise be overwhelmed by high patient numbers.",2020-10-01,4,1874,50,219
1965,32936770,Identification of Risk Factors and Symptoms of COVID-19: Analysis of Biomedical Literature and Social Media Data,"Background:                    In December 2019, the COVID-19 outbreak started in China and rapidly spread around the world. Lack of a vaccine or optimized intervention raised the importance of characterizing risk factors and symptoms for the early identification and successful treatment of patients with COVID-19.              Objective:                    This study aims to investigate and analyze biomedical literature and public social media data to understand the association of risk factors and symptoms with the various outcomes observed in patients with COVID-19.              Methods:                    Through semantic analysis, we collected 45 retrospective cohort studies, which evaluated 303 clinical and demographic variables across 13 different outcomes of patients with COVID-19, and 84,140 Twitter posts from 1036 COVID-19-positive users. Machine learning tools to extract biomedical information were introduced to identify mentions of uncommon or novel symptoms in tweets. We then examined and compared two data sets to expand our landscape of risk factors and symptoms related to COVID-19.              Results:                    From the biomedical literature, approximately 90% of clinical and demographic variables showed inconsistent associations with COVID-19 outcomes. Consensus analysis identified 72 risk factors that were specifically associated with individual outcomes. From the social media data, 51 symptoms were characterized and analyzed. By comparing social media data with biomedical literature, we identified 25 novel symptoms that were specifically mentioned in tweets but have been not previously well characterized. Furthermore, there were certain combinations of symptoms that were frequently mentioned together in social media.              Conclusions:                    Identified outcome-specific risk factors, symptoms, and combinations of symptoms may serve as surrogate indicators to identify patients with COVID-19 and predict their clinical outcomes in order to provide appropriate treatments.",2020-10-01,3,2048,112,219
1433,32634717,Application of Artificial Intelligence in COVID-19 drug repurposing,"Background and aim:                    COVID-19 outbreak has created havoc and a quick cure for the disease will be a therapeutic medicine that has usage history in patients to resolve the current pandemic. With technological advancements in Artificial Intelligence (AI) coupled with increased computational power, the AI-empowered drug repurposing can prove beneficial in the COVID-19 scenario.              Methods:                    The recent literature is studied and analyzed from various sources such as Scopus, Google Scholar, PubMed, and IEEE Xplore databases. The search terms used are 'COVID-19', ' AI ', and 'Drug Repurposing'.              Results:                    AI is implemented in the field design through the generation of the learning-prediction model and performs a quick virtual screening to accurately display the output. With a drug-repositioning strategy, AI can quickly detect drugs that can fight against emerging diseases such as COVID-19. This technology has the potential to improve the drug discovery, planning, treatment, and reported outcomes of the COVID-19 patient, being an evidence-based medical tool.              Conclusions:                    Thus, there are chances that the application of the AI approach in drug discovery is feasible. With prior usage experiences in patients, few of the old drugs, if shown active against SARS-CoV-2, can be readily applied to treat the COVID-19 patients. With the collaboration of AI with pharmacology, the efficiency of drug repurposing can improve significantly.",2020-10-01,13,1547,67,219
1256,32057708,Accelerating the future of cardiac CT: Social media as sine qua non?,"The vision for the Journal of Cardiovascular Computed Tomography's social media efforts is to amplify the impact of the Journal while driving engagement, increasing journal visibility and disseminating content to new audiences globally. Serving as ""the front door"" to the Journal, this digital evolution represents an important step forward for a field in which advancements in hardware, image processing and clinical evidence have evolved rapidly. However, is social media the panem et circenses of cardiovascular computed tomography (CT), that of superficial appeasement, or of sine qua non; an essential ingredient to the acceleration of the Journal and of the field of cardiovascular CT? This paper aims to present the initial impact of social media within a dedicated cardiovascular CT journal.",2020-10-01,3,799,68,219
1253,32061795,Machine learning in the clinical microbiology laboratory: has the time come for routine practice?,"Background:                    Machine learning (ML) allows the analysis of complex and large data sets and has the potential to improve health care. The clinical microbiology laboratory, at the interface of clinical practice and diagnostics, is of special interest for the development of ML systems.              Aims:                    This narrative review aims to explore the current use of ML In clinical microbiology.              Sources:                    References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, arXiV, ACM Digital Library and IEEE Xplore Digital Library up to November 2019.              Content:                    We found 97 ML systems aiming to assist clinical microbiologists. Overall, 82 ML systems (85%) targeted bacterial infections, 11 (11%) parasitic infections, nine (9%) viral infections and three (3%) fungal infections. Forty ML systems (41%) focused on microorganism detection, identification and quantification, 36 (37%) evaluated antimicrobial susceptibility, and 21 (22%) targeted the diagnosis, disease classification and prediction of clinical outcomes. The ML systems used very diverse data sources: 21 (22%) used genomic data of microorganisms, 19 (20%) microbiota data obtained by metagenomic sequencing, 19 (20%) analysed microscopic images, 17 (18%) spectroscopy data, eight (8%) targeted gene sequencing, six (6%) volatile organic compounds, four (4%) photographs of bacterial colonies, four (4%) transcriptome data, three (3%) protein structure, and three (3%) clinical data. Most systems used data from high-income countries (n = 71, 73%) but a significant number used data from low- and middle-income countries (n = 36, 37%). Performance measures were reported for the 97 ML systems, but no article described their use in clinical practice or reported impact on processes or clinical outcomes.              Implications:                    In clinical microbiology, ML has been used with various data sources and diverse practical applications. The evaluation and implementation processes represent the main gap in existing ML systems, requiring a focus on their interpretability and potential integration into real-world settings.",2020-10-01,7,2243,97,219
1848,32717108,Network-based approaches for understanding gene regulation and function in plants,Expression reprogramming directed by transcription factors is a primary gene regulation underlying most aspects of the biology of any organism. Our views of how gene regulation is coordinated are dramatically changing thanks to the advent and constant improvement of high-throughput profiling and transcriptional network inference methods: from activities of individual genes to functional interactions across genes. These technical and analytical advances can reveal the topology of transcriptional networks in which hundreds of genes are hierarchically regulated by multiple transcription factors at systems level. Here we review the state of the art of experimental and computational methods used in plant biology research to obtain large-scale datasets and model transcriptional networks. Examples of direct use of these network models and perspectives on their limitations and future directions are also discussed.,2020-10-01,1,919,81,219
1984,32912543,Machine Learning for Brain Stroke: A Review,"Machine Learning (ML) delivers an accurate and quick prediction outcome and it has become a powerful tool in health settings, offering personalized clinical care for stroke patients. An application of ML and Deep Learning in health care is growing however, some research areas do not catch enough attention for scientific investigation though there is real need of research. Therefore, the aim of this work is to classify state-of-arts on ML techniques for brain stroke into 4 categories based on their functionalities or similarity, and then review studies of each category systematically. A total of 39 studies were identified from the results of ScienceDirect web scientific database on ML for brain stroke from the year 2007 to 2019. Support Vector Machine (SVM) is obtained as optimal models in 10 studies for stroke problems. Besides, maximum studies are found in stroke diagnosis although number for stroke treatment is least thus, it identifies a research gap for further investigation. Similarly, CT images are a frequently used dataset in stroke. Finally SVM and Random Forests are efficient techniques used under each category. The present study showcases the contribution of various ML approaches applied to brain stroke.",2020-10-01,1,1233,43,219
1990,32906056,A review of Cloud computing technologies for comprehensive microRNA analyses,"Cloud computing revolutionized many fields that require ample computational power. Cloud platforms may also provide huge support for microRNA analysis mainly through disclosing scalable resources of different types. In Clouds, these resources are available as services, which simplifies their allocation and releasing. This feature is especially useful during the analysis of large volumes of data, like the one produced by next generation sequencing experiments, which require not only extended storage space but also a distributed computing environment. In this paper, we show which of the Cloud properties and service models can be especially beneficial for microRNA analysis. We also explain the most useful services of the Cloud (including storage space, computational power, web application hosting, machine learning models, and Big Data frameworks) that can be used for microRNA analysis. At the same time, we review several solutions for microRNA and show that the utilization of the Cloud in this field is still weak, but can increase in the future when the awareness of their applicability grows.",2020-10-01,2,1106,76,219
898,31271668,"Medication management needs information and communications technology-based approaches, including telehealth and artificial intelligence","Life expectancy is rising in most parts of the world as is the prevalence of chronic diseases. Suboptimal adherence to long-term medications is still rather the norm than the exception, although it is well known that suboptimal adherence compromises the therapeutic effectiveness. Information and communications technology provides new concepts for improving adherence to medications. These so-called telehealth concepts or services help to implement closed-loop healthcare paradigms and to establish collaborative care networks involving all stakeholders relevant to optimising the overall medication therapy. Together with data from Electronic Health Records and Electronic Medical Records, these networks pave the way to data-driven decision support systems. Recent advances in machine learning, predictive analytics, and artificial intelligence allow further steps towards fully autonomous telehealth systems. This might bring advances in the future: disburden healthcare professionals from repetitive tasks, enable them to timely react to critical situations, and offer a comprehensive overview of the patients' medication status. Advanced analytics can help to assess whether patients have taken their medications as prescribed, to improve adherence via automatic reminders. Ultimately, all relevant data sources need to be collated into a basis for data-driven methods, with the goal to assist healthcare professionals in guiding patients to obtain the best possible health status, with a reasonable resource utilisation and a risk-adjusted safety and privacy approach. This paper summarises the state-of-the-art of telehealth and artificial intelligence applications in medication management. It focuses on 3 major aspects: latest technologies, current applications, and patient related issues.",2020-10-01,6,1802,136,219
1832,32740044,Use of artificial intelligence and machine learning for estimating malignancy risk of thyroid nodules,"Purpose of review:                    Current methods for thyroid nodule risk stratification are subjective, and artificial intelligence algorithms have been used to overcome this shortcoming. In this review, we summarize recent developments in the application of artificial intelligence algorithms for estimating the risks of malignancy in a thyroid nodule.              Recent findings:                    Artificial intelligence have been used to predict malignancy in thyroid nodules using ultrasound images, cytopathology images, and molecular markers. Recent clinical trials have shown that artificial intelligence model's performance matched that of experienced radiologists and pathologists. Explainable artificial intelligence models are being developed to avoid the black box problem. Risk stratification algorithms using artificial intelligence for thyroid nodules are now commercially available in many countries.              Summary:                    Artificial intelligence models could become a useful tool in a thyroidolgist's armamentarium as a decision support tool. Increased adoption of this emerging technology will depend upon increased awareness of the potential benefits and pitfalls in using artificial intelligence.",2020-10-01,0,1244,101,219
1460,32607906,MR Image-Based Attenuation Correction of Brain PET Imaging: Review of Literature on Machine Learning Approaches for Segmentation,"Recent emerging hybrid technology of positron emission tomography/magnetic resonance (PET/MR) imaging has generated a great need for an accurate MR image-based PET attenuation correction. MR image segmentation, as a robust and simple method for PET attenuation correction, has been clinically adopted in commercial PET/MR scanners. The general approach in this method is to segment the MR image into different tissue types, each assigned an attenuation constant as in an X-ray CT image. Machine learning techniques such as clustering, classification and deep networks are extensively used for brain MR image segmentation. However, only limited work has been reported on using deep learning in brain PET attenuation correction. In addition, there is a lack of clinical evaluation of machine learning methods in this application. The aim of this review is to study the use of machine learning methods for MR image segmentation and its application in attenuation correction for PET brain imaging. Furthermore, challenges and future opportunities in MR image-based PET attenuation correction are discussed.",2020-10-01,0,1102,128,219
1803,32783560,"Machine Learning in Radiomic Renal Mass Characterization: Fundamentals, Applications, Challenges, and Future Directions","OBJECTIVE. The purpose of this study is to provide an overview of the traditional machine learning (ML)-based and deep learning-based radiomic approaches, with focus placed on renal mass characterization. CONCLUSION. ML currently has a very low barrier to entry into general medical practice because of the availability of many open-source, free, and easy-to-use toolboxes. Therefore, it should not be surprising to see its related applications in renal mass characterization. A wider picture of the previous works might be beneficial to move this field forward.",2020-10-01,0,562,119,219
1528,32217160,Machine learning for microbial identification and antimicrobial susceptibility testing on MALDI-TOF mass spectra: a systematic review,"Background:                    The matrix assisted laser desorption/ionization and time-of-flight mass spectrometry (MALDI-TOF MS) technology has revolutionized the field of microbiology by facilitating precise and rapid species identification. Recently, machine learning techniques have been leveraged to maximally exploit the information contained in MALDI-TOF MS, with the ultimate goal to refine species identification and streamline antimicrobial resistance determination.              Objectives:                    The aim was to systematically review and evaluate studies employing machine learning for the analysis of MALDI-TOF mass spectra.              Data sources:                    Using PubMed/Medline, Scopus and Web of Science, we searched the existing literature for machine learning-supported applications of MALDI-TOF mass spectra for microbial species and antimicrobial susceptibility identification.              Study eligibility criteria:                    Original research studies using machine learning to exploit MALDI-TOF mass spectra for microbial specie and antimicrobial susceptibility identification were included. Studies focusing on single proteins and peptides, case studies and review articles were excluded.              Methods:                    A systematic review according to the PRISMA guidelines was performed and a quality assessment of the machine learning models conducted.              Results:                    From the 36 studies that met our inclusion criteria, 27 employed machine learning for species identification and nine for antimicrobial susceptibility testing. Support Vector Machines, Genetic Algorithms, Artificial Neural Networks and Quick Classifiers were the most frequently used machine learning algorithms. The quality of the studies ranged between poor and very good. The majority of the studies reported how to interpret the predictors (88.89%) and suggested possible clinical applications of the developed algorithm (100%), but only four studies (11.11%) validated machine learning algorithms on external datasets.              Conclusions:                    A growing number of studies utilize machine learning to optimize the analysis of MALDI-TOF mass spectra. This review, however, demonstrates that there are certain shortcomings of current machine learning-supported approaches that have to be addressed to make them widely available and incorporated them in the clinical routine.",2020-10-01,6,2462,133,219
758,33059367,Mapping scientific landscapes in UMLS research: a scientometric review,"Objective:                    The Unified Medical Language System (UMLS) is 1 of the most successful, collaborative efforts of terminology resource development in biomedicine. The present study aims to 1) survey historical footprints, emerging technologies, and the existing challenges in the use of UMLS resources and tools, and 2) present potential future directions.              Materials and methods:                    We collected 10 469 bibliographic records published between 1986 and 2019, using a Web of Science database. graph analysis, data visualization, and text mining to analyze domain-level citations, subject categories, keyword co-occurrence and bursts, document co-citation networks, and landmark papers.              Results:                    The findings show that the development of UMLS resources and tools have been led by interdisciplinary collaboration among medicine, biology, and computer science. Efforts encompassing multiple disciplines, such as medical informatics, biochemical sciences, and genetics, were the driving forces behind the domain's growth. The following topics were found to be the dominant research themes from the early phases to mid-phases: 1) development and extension of ontologies and 2) enhancing the integrity and accessibility of these resources. Knowledge discovery using machine learning and natural language processing and applications in broader contexts such as drug safety surveillance have recently been receiving increasing attention.              Discussion:                    Our analysis confirms that while reaching its scientific maturity, UMLS research aims to boundary-span to more variety in the biomedical context. We also made some recommendations for editorship and authorship in the domain.              Conclusion:                    The present study provides a systematic approach to map the intellectual growth of science, as well as a self-explanatory bibliometric profile of the published UMLS literature. It also suggests potential future directions. Using the findings of this study, the scientific community can better align the studies within the emerging agenda and current challenges.",2020-10-01,1,2176,70,219
754,33062908,"Quantification and classification of potassium and calcium disorders with the electrocardiogram: What do clinical studies, modeling, and reconstruction tell us?","Diseases caused by alterations of ionic concentrations are frequently observed challenges and play an important role in clinical practice. The clinically established method for the diagnosis of electrolyte concentration imbalance is blood tests. A rapid and non-invasive point-of-care method is yet needed. The electrocardiogram (ECG) could meet this need and becomes an established diagnostic tool allowing home monitoring of the electrolyte concentration also by wearable devices. In this review, we present the current state of potassium and calcium concentration monitoring using the ECG and summarize results from previous work. Selected clinical studies are presented, supporting or questioning the use of the ECG for the monitoring of electrolyte concentration imbalances. Differences in the findings from automatic monitoring studies are discussed, and current studies utilizing machine learning are presented demonstrating the potential of the deep learning approach. Furthermore, we demonstrate the potential of computational modeling approaches to gain insight into the mechanisms of relevant clinical findings and as a tool to obtain synthetic data for methodical improvements in monitoring approaches.",2020-10-01,0,1214,160,219
1529,32215849,The Role and Promise of Artificial Intelligence in Medical Toxicology,"Artificial intelligence (AI) refers to machines or software that process information and interact with the world as understanding beings. Examples of AI in medicine include the automated reading of chest X-rays and the detection of heart dysrhythmias from wearables. A key promise of AI is its potential to apply logical reasoning at the scale of data too vast for the human mind to comprehend. This scaling up of logical reasoning may allow clinicians to bring the entire breadth of current medical knowledge to bear on each patient in real time. It may also unearth otherwise unreachable knowledge in the attempt to integrate knowledge and research across disciplines. In this review, we discuss two complementary aspects of artificial intelligence: deep learning and knowledge representation. Deep learning recognizes and predicts patterns. Knowledge representation structures and interprets those patterns or predictions. We frame this review around how deep learning and knowledge representation might expand the reach of Poison Control Centers and enhance syndromic surveillance from social media.",2020-10-01,1,1103,69,219
752,33066636,Statistical and Machine-Learning Analyses in Nutritional Genomics Studies,"Nutritional compounds may have an influence on different OMICs levels, including genomics, epigenomics, transcriptomics, proteomics, metabolomics, and metagenomics. The integration of OMICs data is challenging but may provide new knowledge to explain the mechanisms involved in the metabolism of nutrients and diseases. Traditional statistical analyses play an important role in description and data association; however, these statistical procedures are not sufficiently enough powered to interpret the large integrated multiple OMICs (multi-OMICS) datasets. Machine learning (ML) approaches can play a major role in the interpretation of multi-OMICS in nutrition research. Specifically, ML can be used for data mining, sample clustering, and classification to produce predictive models and algorithms for integration of multi-OMICs in response to dietary intake. The objective of this review was to investigate the strategies used for the analysis of multi-OMICs data in nutrition studies. Sixteen recent studies aimed to understand the association between dietary intake and multi-OMICs data are summarized. Multivariate analysis in multi-OMICs nutrition studies is used more commonly for analyses. Overall, as nutrition research incorporated multi-OMICs data, the use of novel approaches of analysis such as ML needs to complement the traditional statistical analyses to fully explain the impact of nutrition on health and disease.",2020-10-01,1,1435,73,219
1531,32213317,Image analysis and artificial intelligence in infectious disease diagnostics,"Background:                    Microbiologists are valued for their time-honed skills in image analysis, including identification of pathogens and inflammatory context in Gram stains, ova and parasite preparations, blood smears and histopathologic slides. They also must classify colony growth on a variety of agar plates for triage and assessment. Recent advances in image analysis, in particular application of artificial intelligence (AI), have the potential to automate these processes and support more timely and accurate diagnoses.              Objectives:                    To review current AI-based image analysis as applied to clinical microbiology; and to discuss future trends in the field.              Sources:                    Material sourced for this review included peer-reviewed literature annotated in the PubMed or Google Scholar databases and preprint articles from bioRxiv. Articles describing use of AI for analysis of images used in infectious disease diagnostics were reviewed.              Content:                    We describe application of machine learning towards analysis of different types of microbiologic image data. Specifically, we outline progress in smear and plate interpretation as well as the potential for AI diagnostic applications in the clinical microbiology laboratory.              Implications:                    Combined with automation, we predict that AI algorithms will be used in the future to prescreen and preclassify image data, thereby increasing productivity and enabling more accurate diagnoses through collaboration between the AI and the microbiologist. Once developed, image-based AI analysis is inexpensive and amenable to local and remote diagnostic use.",2020-10-01,2,1725,76,219
2042,33733202,"Artificial Intelligence, Big Data, and mHealth: The Frontiers of the Prevention of Violence Against Children","Violence against children is a global public health threat of considerable concern. At least half of all children worldwide experience violence every year; globally, the total number of children between the ages of 2 and 17 years who have experienced violence in any given year is one billion. Based on a review of the literature, we argue that there is substantial potential for AI (and associated machine learning and big data), and mHealth approaches to be utilized to prevent and address violence at a large scale. This potential is particularly marked in low- and middle-income countries (LMIC), although whether it could translate into effective solutions at scale remains unclear. We discuss possible entry points for Artificial Intelligence (AI), big data, and mHealth approaches to violence prevention, linking these to the World Health Organization's seven INSPIRE strategies. However, such work should be approached with caution. We highlight clear directions for future work in technology-based and technology-enabled violence prevention. We argue that there is a need for good agent-based models at the level of entire cities where and when violence can occur, where local response systems are. Yet, there is a need to develop common, reliable, and valid population- and individual/family-level data on predictors of violence. These indicators could be integrated into routine health or other information systems and become the basis of Al algorithms for violence prevention and response systems. Further, data on individual help-seeking behavior, risk factors for child maltreatment, and other information which could help us to identify the parameters required to understand what happens to cause, and in response to violence, are needed. To respond to ethical issues engendered by these kinds of interventions, there must be concerted, meaningful efforts to develop participatory and user-led work in the AI space, to ensure that the privacy and profiling concerns outlined above are addressed explicitly going forward. Finally, we make the case that developing AI and other technological infrastructure will require substantial investment, particularly in LMIC.",2020-10-01,0,2178,108,219
1804,32783147,Big Data Approaches in Heart Failure Research,"Purpose of review:                    The goal of this review is to summarize the state of big data analyses in the study of heart failure (HF). We discuss the use of big data in the HF space, focusing on ""omics"" and clinical data. We address some limitations of this data, as well as their future potential.              Recent findings:                    Omics are providing insight into plasmal and myocardial molecular profiles in HF patients. The introduction of single cell and spatial technologies is a major advance that will reshape our understanding of cell heterogeneity and function as well as tissue architecture. Clinical data analysis focuses on HF phenotyping and prognostic modeling. Big data approaches are increasingly common in HF research. The use of methods designed for big data, such as machine learning, may help elucidate the biology underlying HF. However, important challenges remain in the translation of this knowledge into improvements in clinical care.",2020-10-01,0,985,45,219
1788,32816167,Scalable Signature-Based Molecular Diagnostics Through On-chip Biomarker Profiling Coupled with Machine Learning,"Molecular diagnostics have traditionally relied on discrete biological substances as diagnostic markers. In recent years however, advances in on-chip biomarker screening technologies and data analytics have enabled signature-based diagnostics. Such diagnostics aim to utilize unique combinations of multiple biomarkers or diagnostic 'fingerprints' rather than discrete analyte measurements. This approach has shown to improve both diagnostic accuracy and diagnostic specificity. In this review, signature-based diagnostics enabled by microfluidic and micro-/nano- technologies will be reviewed with a focus on device design and data analysis pipelines and methodologies. With increasing amounts of data available from microfluidic biomarker screening, isolation, and detection platforms, advanced data handling and analytics approaches can be employed. Thus, current data analysis approaches including machine learning and recent advances with image processing, along with potential future directions will be explored. Lastly, the needs and gaps in current literature will be elucidated to inform future efforts towards development of molecular diagnostics and biomarker screening technologies.",2020-10-01,2,1194,112,219
80,32302888,Modeling regulatory networks using machine learning for systems metabolic engineering,"Systems metabolic engineering attempts to engineer a production host's biological network to overproduce valuable chemicals and materials in a sustainable manner. In contrast to genome-scale metabolic models that are well established, regulatory network models have not been sufficiently considered in systems metabolic engineering despite their importance and recent notable advances. In this paper, recent studies on inferring and characterizing regulatory networks at both transcriptional and translational levels are reviewed. The recent studies discussed herein suggest that their corresponding computational methods and models can be effectively applied to optimize a production host's regulatory networks for the enhanced biological production. For the successful application of regulatory network models, datasets on biological sequence-phenotype relationship need to be more generated.",2020-10-01,1,894,85,219
2941,29859198,Non-invasive biomarkers of fetal brain development reflecting prenatal stress: An integrative multi-scale multi-species perspective on data collection and analysis,"Prenatal stress (PS) impacts early postnatal behavioural and cognitive development. This process of 'fetal programming' is mediated by the effects of the prenatal experience on the developing hypothalamic-pituitary-adrenal (HPA) axis and autonomic nervous system (ANS). We derive a multi-scale multi-species approach to devising preclinical and clinical studies to identify early non-invasively available pre- and postnatal biomarkers of PS. The multiple scales include brain epigenome, metabolome, microbiome and the ANS activity gauged via an array of advanced non-invasively obtainable properties of fetal heart rate fluctuations. The proposed framework has the potential to reveal mechanistic links between maternal stress during pregnancy and changes across these physiological scales. Such biomarkers may hence be useful as early and non-invasive predictors of neurodevelopmental trajectories influenced by the PS as well as follow-up indicators of success of therapeutic interventions to correct such altered neurodevelopmental trajectories. PS studies must be conducted on multiple scales derived from concerted observations in multiple animal models and human cohorts performed in an interactive and iterative manner and deploying machine learning for data synthesis, identification and validation of the best non-invasive detection and follow-up biomarkers, a prerequisite for designing effective therapeutic interventions.",2020-10-01,6,1433,163,219
46,32348823,Technological advances for the detection of melanoma: Advances in diagnostic techniques,"Managing the balance between accurately identifying early stage melanomas while avoiding obtaining biopsy specimens of benign lesions (ie, overbiopsy) is the major challenge of melanoma detection. Decision making can be especially difficult in patients with extensive atypical nevi. Recognizing that the primary screening modality for melanoma is subjective examination, studies have shown a tendency toward overbiopsy. Even low-risk routine surgical procedures are associated with morbidity, mounting health care costs, and patient anxiety. Recent advancements in noninvasive diagnostic modalities have helped improve diagnostic accuracy, especially when managing melanocytic lesions of uncertain diagnosis. Breakthroughs in artificial intelligence have also shown exciting potential in changing the landscape of melanoma detection. In the first article in this continuing medical education series, we review novel diagnostic technologies, such as automated 2- and 3-dimensional total body imaging with sequential digital dermoscopic imaging, reflectance confocal microscopy, and electrical impedance spectroscopy, and we explore the logistics and implications of potentially integrating artificial intelligence into existing melanoma management paradigms.",2020-10-01,1,1257,87,219
29,32367456,Radiomics and deep learning in lung cancer,"Lung malignancies have been extensively characterized through radiomics and deep learning. By providing a three-dimensional characterization of the lesion, models based on radiomic features from computed tomography (CT) and positron-emission tomography (PET) have been developed to detect nodules, distinguish malignant from benign lesions, characterize their histology, stage, and genotype. Deep learning models have been applied to automatically segment organs at risk in lung cancer radiotherapy, stratify patients according to the risk for local and distant recurrence, and identify patients candidate for molecular targeted therapy and immunotherapy. Moreover, radiomics has also been applied successfully to predict side effects such as radiation- and immunotherapy-induced pneumonitis and differentiate lung injury from recurrence. Radiomics could also untap the potential for further use of the cone beam CT acquired for treatment image guidance, four-dimensional CT, and dose-volume data from radiotherapy treatment plans. Radiomics is expected to increasingly affect the clinical practice of treatment of lung tumors, optimizing the end-to-end diagnosis-treatment-follow-up chain. The main goal of this article is to provide an update on the current status of lung cancer radiomics.",2020-10-01,3,1292,42,219
1583,32127174,Opportunities for machine learning to improve surgical ward safety,"Background:                    Delayed recognition of decompensation and failure-to-rescue on surgical wards are major sources of preventable harm. This review assimilates and critically evaluates available evidence and identifies opportunities to improve surgical ward safety.              Data sources:                    Fifty-eight articles from Cochrane Library, EMBASE, and PubMed databases were included.              Conclusions:                    Only 15-20% of patients suffering ward arrest survive. In most cases, subtle signs of instability often occur prior to critical illness and arrest, and underlying pathology is reversible. Coarse risk assessments lead to under-triage of high-risk patients to wards, where surveillance for complications depends on time-consuming manual review of health records, infrequent patient assessments, prediction models that lack accuracy and autonomy, and biased, error-prone decision-making. Streaming electronic heath record data, wearable continuous monitors, and recent advances in deep learning and reinforcement learning can promote efficient and accurate risk assessments, earlier recognition of instability, and better decisions regarding diagnosis and treatment of reversible underlying pathology.",2020-10-01,0,1255,66,219
2089,33372625,A review of optical chemical structure recognition tools,"Structural information about chemical compounds is typically conveyed as 2D images of molecular structures in scientific documents. Unfortunately, these depictions are not a machine-readable representation of the molecules. With a backlog of decades of chemical literature in printed form not properly represented in open-access databases, there is a high demand for the translation of graphical molecular depictions into machine-readable formats. This translation process is known as Optical Chemical Structure Recognition (OCSR). Today, we are looking back on nearly three decades of development in this demanding research field. Most OCSR methods follow a rule-based approach where the key step of vectorization of the depiction is followed by the interpretation of vectors and nodes as bonds and atoms. Opposed to that, some of the latest approaches are based on deep neural networks (DNN). This review provides an overview of all methods and tools that have been published in the field of OCSR. Additionally, a small benchmark study was performed with the available open-source OCSR tools in order to examine their performance.",2020-10-01,1,1132,56,219
13,32394100,Applications of radiomics and machine learning for radiotherapy of malignant brain tumors,"Background:                    Magnetic resonance imaging (MRI) and amino acid positron-emission tomography (PET) of the brain contain a vast amount of structural and functional information that can be analyzed by machine learning algorithms and radiomics for the use of radiotherapy in patients with malignant brain tumors.              Methods:                    This study is based on comprehensive literature research on machine learning and radiomics analyses in neuroimaging and their potential application for radiotherapy in patients with malignant glioma or brain metastases.              Results:                    Feature-based radiomics and deep learning-based machine learning methods can be used to improve brain tumor diagnostics and automate various steps of radiotherapy planning. In glioma patients, important applications are the determination of WHO grade and molecular markers for integrated diagnosis in patients not eligible for biopsy or resection, automatic image segmentation for target volume planning, prediction of the location of tumor recurrence, and differentiation of pseudoprogression from actual tumor progression. In patients with brain metastases, radiomics is applied for additional detection of smaller brain metastases, accurate segmentation of multiple larger metastases, prediction of local response after radiosurgery, and differentiation of radiation injury from local brain metastasis relapse. Importantly, high diagnostic accuracies of 80-90% can be achieved by most approaches, despite a large variety in terms of applied imaging techniques and computational methods.              Conclusion:                    Clinical application of automated image analyses based on radiomics and artificial intelligence has a great potential for improving radiotherapy in patients with malignant brain tumors. However, a common problem associated with these techniques is the large variability and the lack of standardization of the methods applied.",2020-10-01,2,1986,89,219
2136,31776737,Artificial intelligence and robotics: a combination that is changing the operating room,"Purpose:                    The aim of the current narrative review was to summarize the available evidence in the literature on artificial intelligence (AI) methods that have been applied during robotic surgery.              Methods:                    A narrative review of the literature was performed on MEDLINE/Pubmed and Scopus database on the topics of artificial intelligence, autonomous surgery, machine learning, robotic surgery, and surgical navigation, focusing on articles published between January 2015 and June 2019. All available evidences were analyzed and summarized herein after an interactive peer-review process of the panel.              Literature review:                    The preliminary results of the implementation of AI in clinical setting are encouraging. By providing a readout of the full telemetry and a sophisticated viewing console, robot-assisted surgery can be used to study and refine the application of AI in surgical practice. Machine learning approaches strengthen the feedback regarding surgical skills acquisition, efficiency of the surgical process, surgical guidance and prediction of postoperative outcomes. Tension-sensors on the robotic arms and the integration of augmented reality methods can help enhance the surgical experience and monitor organ movements.              Conclusions:                    The use of AI in robotic surgery is expected to have a significant impact on future surgical training as well as enhance the surgical experience during a procedure. Both aim to realize precision surgery and thus to increase the quality of the surgical care. Implementation of AI in master-slave robotic surgery may allow for the careful, step-by-step consideration of autonomous robotic surgery.",2020-10-01,4,1750,87,219
85,32294230,Leading a Digital Transformation in the Pharmaceutical Industry: Reimagining the Way We Work in Global Drug Development,"We are experiencing seminal times in computing that seem to define a fourth industrial revolution. This may fundamentally change the way we live, work, and relate to one another. Embracing data and digital information is a top priority for most industries these days, and Life Sciences is no exception. The pharmaceutical industry in particular is fundamentally a data-driven business. Inspired by a desire to ""Go Big on Data,"" we developed a strategic roadmap defining a digital transformation to reimagine the way we work in Novartis Global Drug Development, leveraging data science to generate and inject actionable insights into our best practices. We launched a program called Nerve Live, and built a state-of-the-art data and analytics platform to harness past and present operational data, providing access to decades of drug development ""experience"" buried across multiple sources. The platform enabled the systematic application of machine learning and predictive analytics to generate ""intelligence"": new insights across multiple functional areas. To action the insights and create ""value,"" we crafted skillfully designed end-user applications for domain experts to plan, track, predict, compare and monitor domain activities, optimize costs, and maximize quality. Today, the Nerve Live program enables insights-driven decision making at scale, unlocking productivity, and providing transparency across the Novartis Global Drug Development organization and beyond. We identified three main drivers making the Nerve Live program successful and enabling the associated digital transformation to flourish. We discuss the challenges, highlight the benefits, and see the importance of leading the way to become future proof.",2020-10-01,1,1729,119,219
896,31276247,Machine learning in breast MRI,"Machine-learning techniques have led to remarkable advances in data extraction and analysis of medical imaging. Applications of machine learning to breast MRI continue to expand rapidly as increasingly accurate 3D breast and lesion segmentation allows the combination of radiologist-level interpretation (eg, BI-RADS lexicon), data from advanced multiparametric imaging techniques, and patient-level data such as genetic risk markers. Advances in breast MRI feature extraction have led to rapid dataset analysis, which offers promise in large pooled multiinstitutional data analysis. The object of this review is to provide an overview of machine-learning and deep-learning techniques for breast MRI, including supervised and unsupervised methods, anatomic breast segmentation, and lesion segmentation. Finally, it explores the role of machine learning, current limitations, and future applications to texture analysis, radiomics, and radiogenomics. Level of Evidence: 3 Technical Efficacy Stage: 2 J. Magn. Reson. Imaging 2019. J. Magn. Reson. Imaging 2020;52:998-1018.",2020-10-01,6,1070,30,219
779,33037949,Machine Learning in Electrocardiography and Echocardiography: Technological Advances in Clinical Cardiology,"Purpose of review:                    Electrocardiography (ECG) and echocardiography are the most widely used diagnostic tools in clinical cardiology. This review focuses on recent advancements in applying machine learning (ML) in ECG and echocardiography and potential synergistic ML integration of ECG and echocardiography.              Recent findings:                    ML algorithms have been used in ECG for technical quality assurance, arrhythmia identification, and prognostic predictions, and in echocardiography to recognize image views, quantify measurements, and identify pathologic patterns. Synergistic application of ML in ECG and echocardiograph has demonstrated the potential to optimize therapeutic response, improve risk stratification, and generate new disease classification. There is mounting evidence that ML potentially outperforms in disease diagnoses and outcome prediction with ECG and echocardiography when compared with trained healthcare professionals. The applications of ML in ECG and echocardiography are playing increasingly greater roles in medical research and clinical practice, particularly for their contributions to developing novel diagnostic/prognostic prediction models. The automation in data acquisition, processing, and interpretation help streamline the workflows of ECG and echocardiography in contemporary cardiology practice.",2020-10-01,0,1376,107,219
1808,32777930,Artificial Intelligence in plastic surgery: What is it? Where are we now? What is on the horizon?,"Introduction:                    An increasing quantity of data is required to guide precision medicine and advance future healthcare practices, but current analytical methods often become overwhelmed. Artificial intelligence (AI) provides a promising solution. Plastic surgery is an innovative surgical specialty expected to implement AI into current and future practices. It is important for all plastic surgeons to understand how AI may affect current and future practice, and to recognise its potential limitations.              Methods:                    Peer-reviewed published literature and online content were comprehensively reviewed. We report current applications of AI in plastic surgery and possible future applications based on published literature and continuing scientific studies, and detail its potential limitations and ethical considerations.              Findings:                    Current machine learning models using convolutional neural networks can evaluate breast mammography and differentiate benign and malignant tumours as accurately as specialist doctors, and motion sensor surgical instruments can collate real-time data to advise intraoperative technical adjustments. Centralised big data portals are expected to collate large datasets to accelerate understanding of disease pathogeneses and best practices. Information obtained using computer vision could guide intraoperative surgical decisions in unprecedented detail and semi-autonomous surgical systems guided by AI algorithms may enable improved surgical outcomes in low- and middle-income countries. Surgeons must collaborate with computer scientists to ensure that AI algorithms inform clinically relevant health objectives and are interpretable. Ethical concerns such as systematic biases causing non-representative conclusions for under-represented patient groups, patient confidentiality and the limitations of AI based on the quality of data input suggests that AI will accompany the plastic surgeon, rather than replace them.",2020-10-01,0,2025,97,219
1466,32603804,Digital microbiology,"Background:                    Digitalization and artificial intelligence have an important impact on the way microbiology laboratories will work in the near future. Opportunities and challenges lie ahead to digitalize the microbiological workflows. Making efficient use of big data, machine learning, and artificial intelligence in clinical microbiology requires a profound understanding of data handling aspects.              Objective:                    This review article summarizes the most important concepts of digital microbiology. The article gives microbiologists, clinicians and data scientists a viewpoint and practical examples along the diagnostic process.              Sources:                    We used peer-reviewed literature identified by a PubMed search for digitalization, machine learning, artificial intelligence and microbiology.              Content:                    We describe the opportunities and challenges of digitalization in microbiological diagnostic processes with various examples. We also provide in this context key aspects of data structure and interoperability, as well as legal aspects. Finally, we outline the way for applications in a modern microbiology laboratory.              Implications:                    We predict that digitalization and the usage of machine learning will have a profound impact on the daily routine of laboratory staff. Along the analytical process, the most important steps should be identified, where digital technologies can be applied and provide a benefit. The education of all staff involved should be adapted to prepare for the advances in digital microbiology.",2020-10-01,0,1645,20,219
2018,32837980,Data-driven modeling of COVID-19-Lessons learned,"Understanding the outbreak dynamics of COVID-19 through the lens of mathematical models is an elusive but significant goal. Within only half a year, the COVID-19 pandemic has resulted in more than 19 million reported cases across 188 countries with more than 700,000 deaths worldwide. Unlike any other disease in history, COVID-19 has generated an unprecedented volume of data, well documented, continuously updated, and broadly available to the general public. Yet, the precise role of mathematical modeling in providing quantitative insight into the COVID-19 pandemic remains a topic of ongoing debate. Here we discuss the lessons learned from six month of modeling COVID-19. We highlight the early success of classical models for infectious diseases and show why these models fail to predict the current outbreak dynamics of COVID-19. We illustrate how data-driven modeling can integrate classical epidemiology modeling and machine learning to infer critical disease parameters-in real time-from reported case data to make informed predictions and guide political decision making. We critically discuss questions that these models can and cannot answer and showcase controversial decisions around the early outbreak dynamics, outbreak control, and exit strategies. We anticipate that this summary will stimulate discussion within the modeling community and help provide guidelines for robust mathematical models to understand and manage the COVID-19 pandemic. EML webinar speakers, videos, and overviews are updated at https://imechanica.org/node/24098.",2020-10-01,4,1556,48,219
1838,32729465,Nano-enabled sensing approaches for pathogenic bacterial detection,"Infectious diseases caused by pathogenic bacteria, especially antibiotic-resistant bacteria, are one of the biggest threats to global health. To date, bacterial contamination is detected using conventional culturing techniques, which are highly dependent on expert users, limited by the processing time and on-site availability. Hence, real-time and continuous monitoring of pathogen levels is required to obtain valuable information that could assist health agencies in guiding prevention and containment of pathogen-related outbreaks. Nanotechnology-based smart sensors are opening new avenues for early and rapid detection of such pathogens at the patient's point-of-care. Nanomaterials can play an essential role in bacterial sensing owing to their unique optical, magnetic, and electrical properties. Carbon nanoparticles, metallic nanoparticles, metal oxide nanoparticles, and various types of nanocomposites are examples of smart nanomaterials that have drawn intense attention in the field of microbial detection. These approaches, together with the advent of modern technologies and coupled with machine learning and wireless communication, represent the future trend in the diagnosis of infectious diseases. This review provides an overview of the recent advancements in the successful harnessing of different nanoparticles for bacterial detection. In the beginning, we have introduced the fundamental concepts and mechanisms behind the design and strategies of the nanoparticles-based diagnostic platform. Representative research efforts are highlighted for in vitro and in vivo detection of bacteria. A comprehensive discussion is then presented to cover the most commonly adopted techniques for bacterial identification, including some seminal studies to detect bacteria at the single-cell level. Finally, we discuss the current challenges and a prospective outlook on the field, together with the recommended solutions.",2020-10-01,4,1933,66,219
1469,32602066,Prognostic factors of biochemical remission after transsphenoidal surgery for acromegaly: a structured review,"Purpose:                    Biochemical control is the main determinant of survival, clinical manifestations and comorbidities in acromegaly. Transsphenoidal selective adenomectomy (TSA) is the initial treatment of choice with reported biochemical remission rates varying between 32 and 85%. Understanding the limiting factors is essential for identification of patients who require medical treatment.              Methods:                    We reviewed the English literature published in Medline/Pubmed until Dec 31, 2019 to identify eligible studies that described outcomes of TSA as primary therapy and performed analyses to determine the main predictors of remission.              Results:                    Most publications reported single-institution, retrospective studies. The following preoperative parameters were consistently associated with lower remission rates: cavernous sinus invasion by imaging, larger tumor size and higher GH levels. Young age and preoperative IGF-1 levels were predictive in some studies. When controlled for covariates, the best single preoperative predictor was cavernous sinus invasion, followed by preoperative GH levels. Conversely, low GH level in the first few days postoperatively was a robust predictor of durable remission. The influence of tumor histology (sparsely granular pattern, co-expression of prolactin and proliferation markers) on surgical remission remains to be established. Few studies developed predictive models that yielded much higher predictive values than individual parameters.              Conclusion:                    Surgical outcome prognostication systems could be further generated by machine learning algorithms in order to support development and implementation of personalized care in patients with acromegaly.",2020-10-01,2,1793,109,219
1837,32729531,Artificial intelligence biosensors: Challenges and prospects,"Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.",2020-10-01,5,1081,60,219
1364,33134890,Integrating Machine Learning with Human Knowledge,"Machine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.",2020-10-01,2,1021,49,219
2020,32834612,Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: A review,"Background and objective:                    During the recent global urgency, scientists, clinicians, and healthcare experts around the globe keep on searching for a new technology to support in tackling the Covid-19 pandemic. The evidence of Machine Learning (ML) and Artificial Intelligence (AI) application on the previous epidemic encourage researchers by giving a new angle to fight against the novel Coronavirus outbreak. This paper aims to comprehensively review the role of AI and ML as one significant method in the arena of screening, predicting, forecasting, contact tracing, and drug development for SARS-CoV-2 and its related epidemic.              Method:                    A selective assessment of information on the research article was executed on the databases related to the application of ML and AI technology on Covid-19. Rapid and critical analysis of the three crucial parameters, i.e., abstract, methodology, and the conclusion was done to relate to the model's possibilities for tackling the SARS-CoV-2 epidemic.              Result:                    This paper addresses on recent studies that apply ML and AI technology towards augmenting the researchers on multiple angles. It also addresses a few errors and challenges while using such algorithms in real-world problems. The paper also discusses suggestions conveying researchers on model design, medical experts, and policymakers in the current situation while tackling the Covid-19 pandemic and ahead.              Conclusion:                    The ongoing development in AI and ML has significantly improved treatment, medication, screening, prediction, forecasting, contact tracing, and drug/vaccine development process for the Covid-19 pandemic and reduce the human intervention in medical practice. However, most of the models are not deployed enough to show their real-world operation, but they are still up to the mark to tackle the SARS-CoV-2 epidemic.",2020-10-01,36,1946,105,219
818,32979542,Artificial intelligence in celiac disease,"Celiac disease (CD) has been on the rise in the world and a large part of it remains undiagnosed. Novel methods are required to address the gaps in prompt detection and management. Artificial intelligence (AI) has seen an exponential surge in the last decade worldwide. With the advent of big data and powerful computational ability, we now have self-driving cars and smart devices in our daily lives. Huge databases in the form of electronic medical records and images have rendered healthcare a lucrative sector where AI can prove revolutionary. It is being used extensively to overcome the barriers in clinical workflows. From the perspective of a disease, it can be deployed in multiple steps i.e. screening tools, diagnosis, developing novel therapeutic agents, proposing management plans, and defining prognostic indicators, etc. We review the areas where it may augment physicians in the delivery of better healthcare by summarizing current literature on the use of AI in healthcare using CD as a model. We further outline major barriers to its large-scale implementations and prospects from the healthcare point of view.",2020-10-01,0,1128,41,219
1829,32749209,Cardiovascular CT and MRI in 2019: Review of Key Articles,"Cardiac imaging is becoming commonplace throughout radiology practices and is increasingly important in large-cohort prospective cardiovascular trials and in statements and guidelines. In this review, the authors summarize some of the most important imaging findings relevant to clinical practice in the past year. Key coronary CT angiography studies have included rigorous meta-analysis of its diagnostic accuracy, prognostic implications of adverse coronary plaque features, and sex differences. The value of CT for catheter-delivered valve implantation (eg, transcatheter aortic and mitral valve replacements) was further elucidated in large-cohort outcome trials. Hypertrophic cardiomyopathy registries have revealed distinct clinical and MRI phenotypes, highlighting different underlying causes, while others clarified the prognostic usefulness of MRI in hypertrophic cardiomyopathy and Fabry disease. Artificial intelligence and/or machine learning was applied to many aspects of cardiovascular imaging, while evidence of the benefits of both adenosine stress perfusion cardiac MRI and coronary CT angiography-derived fractional flow reserve from real-world trials has increased. Studies on vaping and vascular endothelial function and the whole-body MRI depiction of metabolic syndrome consequences were also noteworthy. Although this review focuses on Radiology articles, key articles from high-impact clinical journals are also included. Although not possible to detail all articles because of space limitations, the authors attempted to highlight those with the most pragmatic and scientific value.",2020-10-01,0,1608,57,219
800,33004526,"Artificial intelligence in pulmonary medicine: computer vision, predictive model and COVID-19","Artificial intelligence (AI) is transforming healthcare delivery. The digital revolution in medicine and healthcare information is prompting a staggering growth of data intertwined with elements from many digital sources such as genomics, medical imaging and electronic health records. Such massive growth has sparked the development of an increasing number of AI-based applications that can be deployed in clinical practice. Pulmonary specialists who are familiar with the principles of AI and its applications will be empowered and prepared to seize future practice and research opportunities. The goal of this review is to provide pulmonary specialists and other readers with information pertinent to the use of AI in pulmonary medicine. First, we describe the concept of AI and some of the requisites of machine learning and deep learning. Next, we review some of the literature relevant to the use of computer vision in medical imaging, predictive modelling with machine learning, and the use of AI for battling the novel severe acute respiratory syndrome-coronavirus-2 pandemic. We close our review with a discussion of limitations and challenges pertaining to the further incorporation of AI into clinical pulmonary practice.",2020-10-01,2,1232,93,219
798,33006425,Machine Learning in Meningioma MRI: Past to Present. A Narrative Review,"Meningioma is one of the most frequent primary central nervous system tumors. While magnetic resonance imaging (MRI), is the standard radiologic technique for provisional diagnosis and surveillance of meningioma, it nevertheless lacks the prima facie capacity in determining meningioma biological aggressiveness, growth, and recurrence potential. An increasing body of evidence highlights the potential of machine learning and radiomics in improving the consistency and productivity and in providing novel diagnostic, treatment, and prognostic modalities in neuroncology imaging. The aim of the present article is to review the evolution and progress of approaches utilizing machine learning in meningioma MRI-based sementation, diagnosis, grading, and prognosis. We provide a historical perspective on original research on meningioma spanning over two decades and highlight recent studies indicating the feasibility of pertinent approaches, including deep learning in addressing several clinically challenging aspects. We indicate the limitations of previous research designs and resources and propose future directions by highlighting areas of research that remain largely unexplored. LEVEL OF EVIDENCE: 5 TECHNICAL EFFICACY STAGE: 2.",2020-10-01,0,1236,71,219
795,33010852,The 21st Annual Feigenbaum Lecture: Beyond Artificial: Echocardiography from Elegant Images to Analytic Intelligence,"Echocardiography has always been a journey from scientific observation to clinical application. Whether in theranostics, understanding the performance of the systemic right ventricle, or uncovering the predictive power of echocardiographic data in congenital heart disease, the author's experiences highlight how echocardiographers at the frontier of scientific inquiry making observations today are inundated with data. It becomes apparent that new clinical applications, if they are to be successful, depend more than ever on effective management of the information we collect. In light of this realization, the 21st Feigenbaum lecture explores analytic intelligence-one path echocardiography might now take on its march from observation to application.",2020-10-01,0,755,116,219
1811,32771905,Sequencing enabling design and learning in synthetic biology,"The ability to read and quantify nucleic acids such as DNA and RNA using sequencing technologies has revolutionized our understanding of life. With the emergence of synthetic biology, these tools are now being put to work in new ways - enabling de novo biological design. Here, we show how sequencing is supporting the creation of a new wave of biological parts and systems, as well as providing the vast data sets needed for the machine learning of design rules for predictive bioengineering. However, we believe this is only the tip of the iceberg and end by providing an outlook on recent advances that will likely broaden the role of sequencing in synthetic biology and its deployment in real-world environments.",2020-10-01,0,716,60,219
789,33019765,Review of the State of the Art of Deep Learning for Plant Diseases: A Broad Analysis and Discussion,"Deep learning (DL) represents the golden era in the machine learning (ML) domain, and it has gradually become the leading approach in many fields. It is currently playing a vital role in the early detection and classification of plant diseases. The use of ML techniques in this field is viewed as having brought considerable improvement in cultivation productivity sectors, particularly with the recent emergence of DL, which seems to have increased accuracy levels. Recently, many DL architectures have been implemented accompanying visualisation techniques that are essential for determining symptoms and classifying plant diseases. This review investigates and analyses the most recent methods, developed over three years leading up to 2020, for training, augmentation, feature fusion and extraction, recognising and counting crops, and detecting plant diseases, including how these methods can be harnessed to feed deep classifiers and their effects on classifier accuracy.",2020-10-01,2,977,99,219
787,33022947,"Bone Age Assessment Empowered with Deep Learning: A Survey, Open Research Challenges and Future Directions","Deep learning is a quite useful and proliferating technique of machine learning. Various applications, such as medical images analysis, medical images processing, text understanding, and speech recognition, have been using deep learning, and it has been providing rather promising results. Both supervised and unsupervised approaches are being used to extract and learn features as well as for the multi-level representation of pattern recognition and classification. Hence, the way of prediction, recognition, and diagnosis in various domains of healthcare including the abdomen, lung cancer, brain tumor, skeletal bone age assessment, and so on, have been transformed and improved significantly by deep learning. By considering a wide range of deep-learning applications, the main aim of this paper is to present a detailed survey on emerging research of deep-learning models for bone age assessment (e.g., segmentation, prediction, and classification). An enormous number of scientific research publications related to bone age assessment using deep learning are explored, studied, and presented in this survey. Furthermore, the emerging trends of this research domain have been analyzed and discussed. Finally, a critical discussion section on the limitations of deep-learning models has been presented. Open research challenges and future directions in this promising area have been included as well.",2020-10-01,0,1405,106,219
784,33028042,Motion Capture Technology in Industrial Applications: A Systematic Review,"The rapid technological advancements of Industry 4.0 have opened up new vectors for novel industrial processes that require advanced sensing solutions for their realization. Motion capture (MoCap) sensors, such as visual cameras and inertial measurement units (IMUs), are frequently adopted in industrial settings to support solutions in robotics, additive manufacturing, teleworking and human safety. This review synthesizes and evaluates studies investigating the use of MoCap technologies in industry-related research. A search was performed in the Embase, Scopus, Web of Science and Google Scholar. Only studies in English, from 2015 onwards, on primary and secondary industrial applications were considered. The quality of the articles was appraised with the AXIS tool. Studies were categorized based on type of used sensors, beneficiary industry sector, and type of application. Study characteristics, key methods and findings were also summarized. In total, 1682 records were identified, and 59 were included in this review. Twenty-one and 38 studies were assessed as being prone to medium and low risks of bias, respectively. Camera-based sensors and IMUs were used in 40% and 70% of the studies, respectively. Construction (30.5%), robotics (15.3%) and automotive (10.2%) were the most researched industry sectors, whilst health and safety (64.4%) and the improvement of industrial processes or products (17%) were the most targeted applications. Inertial sensors were the first choice for industrial MoCap applications. Camera-based MoCap systems performed better in robotic applications, but camera obstructions caused by workers and machinery was the most challenging issue. Advancements in machine learning algorithms have been shown to increase the capabilities of MoCap systems in applications such as activity and fatigue detection as well as tool condition monitoring and object recognition.",2020-10-01,1,1908,73,219
2240,31144777,Operational framework and training standard requirements for AI-empowered robotic surgery,"Background:                    For autonomous robot-delivered surgeries to ever become a feasible option, we recommend the combination of human-centered artificial intelligence (AI) and transparent machine learning (ML), with integrated Gross anatomy models. This can be supplemented with medical imaging data of cadavers for performance evaluation.              Methods:                    We reviewed technological advances and state-of-the-art documented developments. We undertook a literature search on surgical robotics and skills, tracing agent studies, relevant frameworks, and standards for AI. This embraced transparency aspects of AI.              Conclusion:                    We recommend ""a procedure/skill template"" for teaching AI that can be used by a surgeon. Similar existing methodologies show that when such a metric-based approach is used for training surgeons, cardiologists, and anesthetists, it results in a >40% error reduction in objectively assessed intraoperative procedures. The integration of Explainable AI and ML, and novel tissue characterization sensorics to tele-operated robotic-assisted procedures with medical imaged cadavers, provides robotic guidance and refines tissue classifications at a molecular level.",2020-10-01,1,1249,89,219
804,32996518,Towards development of a novel universal medical diagnostic method: Raman spectroscopy and machine learning,"Many problems exist within the myriad of currently employed screening and diagnostic methods. Further, an incredibly wide variety of procedures are used to identify an even greater number of diseases which exist in the world. There is a definite unmet clinical need to improve diagnostic capabilities of these procedures, including improving test sensitivity and specificity, objectivity and definitiveness, and reducing cost and invasiveness of the test, with an interest in replacing multiple diagnostic methods with one powerful tool. There has been a recent surge in the literature which focuses on utilizing Raman spectroscopy in combination with machine learning analyses to improve diagnostic measures for identifying an assortment of diseases, including cancers, viral and bacterial infections, neurodegenerative and autoimmune disorders, and more. This review highlights the work accomplished since 2018 which focuses on using Raman spectroscopy and machine learning to address the need for better screening and medical diagnostics in all areas of disease. A critical evaluation considers both the benefits and obstacles of utilizing the method for universal diagnostics. It is clear based on the evidence provided herein Raman spectroscopy in combination with machine learning provides the first glimmer of hope for the development of an accurate, inexpensive, fast, and non-invasive method for universal medical diagnostics.",2020-10-01,0,1435,107,219
1365,33133423,Deep metabolome: Applications of deep learning in metabolomics,"In the past few years, deep learning has been successfully applied to various omics data. However, the applications of deep learning in metabolomics are still relatively low compared to others omics. Currently, data pre-processing using convolutional neural network architecture appears to benefit the most from deep learning. Compound/structure identification and quantification using artificial neural network/deep learning performed relatively better than traditional machine learning techniques, whereas only marginally better results are observed in biological interpretations. Before deep learning can be effectively applied to metabolomics, several challenges should be addressed, including metabolome-specific deep learning architectures, dimensionality problems, and model evaluation regimes.",2020-10-01,1,801,62,219
1344,33162926,Machine Learning Applications in the Neuro ICU: A Solution to Big Data Mayhem?,"The neurological ICU (neuro ICU) often suffers from significant limitations due to scarce resource availability for their neurocritical care patients. Neuro ICU patients require frequent neurological evaluations, continuous monitoring of various physiological parameters, frequent imaging, and routine lab testing. This amasses large amounts of data specific to each patient. Neuro ICU teams are often overburdened by the resulting complexity of data for each patient. Machine Learning algorithms (ML), are uniquely capable of interpreting high-dimensional datasets that are too difficult for humans to comprehend. Therefore, the application of ML in the neuro ICU could alleviate the burden of analyzing big datasets for each patient. This review serves to (1) briefly summarize ML and compare the different types of MLs, (2) review recent ML applications to improve neuro ICU management and (3) describe the future implications of ML to neuro ICU management.",2020-10-01,0,960,78,219
1893,33299440,Artificial intelligence (AI) and interventional radiotherapy (brachytherapy): state of art and future perspectives,"Purpose:                    Artificial intelligence (AI) plays a central role in building decision supporting systems (DSS), and its application in healthcare is rapidly increasing. The aim of this study was to define the role of AI in healthcare, with main focus on radiation oncology (RO) and interventional radiotherapy (IRT, brachytherapy).              Artificial intelligence in interventional radiation therapy:                    AI in RO has a large impact in providing clinical decision support, data mining and advanced imaging analysis, automating repetitive tasks, optimizing time, and modelling patients and physicians' behaviors in heterogeneous contexts. Implementing AI and automation in RO and IRT can successfully facilitate all the steps of treatment workflow, such as patient consultation, target volume delineation, treatment planning, and treatment delivery.              Conclusions:                    AI may contribute to improve clinical outcomes through the application of predictive models and DSS optimization. This approach could lead to reducing time-consuming repetitive tasks, healthcare costs, and improving treatment quality assurance and patient's assistance in IRT.",2020-10-01,4,1203,114,219
1423,32647386,"Gut microbiome, big data and machine learning to promote precision medicine for cancer","The gut microbiome has been implicated in cancer in several ways, as specific microbial signatures are known to promote cancer development and influence safety, tolerability and efficacy of therapies. The 'omics' technologies used for microbiome analysis continuously evolve and, although much of the research is still at an early stage, large-scale datasets of ever increasing size and complexity are being produced. However, there are varying levels of difficulty in realizing the full potential of these new tools, which limit our ability to critically analyse much of the available data. In this Perspective, we provide a brief overview on the role of gut microbiome in cancer and focus on the need, role and limitations of a machine learning-driven approach to analyse large amounts of complex health-care information in the era of big data. We also discuss the potential application of microbiome-based big data aimed at promoting precision medicine in cancer.",2020-10-01,14,966,86,219
1899,33294134,Combining structure and genomics to understand antimicrobial resistance,"Antimicrobials against bacterial, viral and parasitic pathogens have transformed human and animal health. Nevertheless, their widespread use (and misuse) has led to the emergence of antimicrobial resistance (AMR) which poses a potentially catastrophic threat to public health and animal husbandry. There are several routes, both intrinsic and acquired, by which AMR can develop. One major route is through non-synonymous single nucleotide polymorphisms (nsSNPs) in coding regions. Large scale genomic studies using high-throughput sequencing data have provided powerful new ways to rapidly detect and respond to such genetic mutations linked to AMR. However, these studies are limited in their mechanistic insight. Computational tools can rapidly and inexpensively evaluate the effect of mutations on protein function and evolution. Subsequent insights can then inform experimental studies, and direct existing or new computational methods. Here we review a range of sequence and structure-based computational tools, focussing on tools successfully used to investigate mutational effect on drug targets in clinically important pathogens, particularly Mycobacterium tuberculosis. Combining genomic results with the biophysical effects of mutations can help reveal the molecular basis and consequences of resistance development. Furthermore, we summarise how the application of such a mechanistic understanding of drug resistance can be applied to limit the impact of AMR.",2020-10-01,1,1470,71,219
1391,33083571,Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines,"Advancements in deep learning techniques carry the potential to make significant contributions to healthcare, particularly in fields that utilize medical imaging for diagnosis, prognosis, and treatment decisions. The current state-of-the-art deep learning models for radiology applications consider only pixel-value information without data informing clinical context. Yet in practice, pertinent and accurate non-imaging data based on the clinical history and laboratory data enable physicians to interpret imaging findings in the appropriate clinical context, leading to a higher diagnostic accuracy, informative clinical decision making, and improved patient outcomes. To achieve a similar goal using deep learning, medical imaging pixel-based models must also achieve the capability to process contextual data from electronic health records (EHR) in addition to pixel data. In this paper, we describe different data fusion techniques that can be applied to combine medical imaging with EHR, and systematically review medical data fusion literature published between 2012 and 2020. We conducted a systematic search on PubMed and Scopus for original research articles leveraging deep learning for fusion of multimodality data. In total, we screened 985 studies and extracted data from 17 papers. By means of this systematic review, we present current knowledge, summarize important results and provide implementation guidelines to serve as a reference for researchers interested in the application of multimodal fusion in medical imaging.",2020-10-01,2,1539,126,219
1390,33086465,"Source tracking of antibiotic resistance genes in the environment - Challenges, progress, and prospects","Antibiotic resistance has become a global public health concern, rendering common infections untreatable. Given the widespread occurrence, increasing attention is being turned toward environmental pathways that potentially contribute to antibiotic resistance gene (ARG) dissemination outside the clinical realm. Studies during the past decade have clearly proved the increased ARG pollution trend along with gradient of anthropogenic interference, mainly through marker-ARG detection by PCR-based approaches. However, accurate source-tracking has been always confounded by various factors in previous studies, such as autochthonous ARG level, spatiotemporal variability and environmental resistome complexity, as well as inherent method limitation. The rapidly developed metagenomics profiles ARG occurrence within the sample-wide genomic context, opening a new avenue for source tracking of environmental ARG pollution. Coupling with machine-learning classification, it has been demonstrated the potential of metagenomic ARG profiles in unambiguously assigning source contribution. Through identifying indicator ARG and recovering ARG-host genomes, metagenomics-based analysis will further increase the resolution and accuracy of source tracking. In this review, challenges and progresses in source-tracking studies on environmental ARG pollution will be discussed, with specific focus on recent metagenomics-guide approaches. We propose an integrative metagenomics-based framework, in which coordinated efforts on experimental design and metagenomic analysis will assist in realizing the ultimate goal of robust source-tracking in environmental ARG pollution.",2020-10-01,1,1661,103,219
1385,33094213,Artificial intelligence for brain diseases: A systematic review,"Artificial intelligence (AI) is a major branch of computer science that is fruitfully used for analyzing complex medical data and extracting meaningful relationships in datasets, for several clinical aims. Specifically, in the brain care domain, several innovative approaches have achieved remarkable results and open new perspectives in terms of diagnosis, planning, and outcome prediction. In this work, we present an overview of different artificial intelligent techniques used in the brain care domain, along with a review of important clinical applications. A systematic and careful literature search in major databases such as Pubmed, Scopus, and Web of Science was carried out using ""artificial intelligence"" and ""brain"" as main keywords. Further references were integrated by cross-referencing from key articles. 155 studies out of 2696 were identified, which actually made use of AI algorithms for different purposes (diagnosis, surgical treatment, intra-operative assistance, and postoperative assessment). Artificial neural networks have risen to prominent positions among the most widely used analytical tools. Classic machine learning approaches such as support vector machine and random forest are still widely used. Task-specific algorithms are designed for solving specific problems. Brain images are one of the most used data types. AI has the possibility to improve clinicians' decision-making ability in neuroscience applications. However, major issues still need to be addressed for a better practical use of AI in the brain. To this aim, it is important to both gather comprehensive data and build explainable AI algorithms.",2020-10-01,0,1645,63,219
2664,32449232,"The use of artificial intelligence, machine learning and deep learning in oncologic histopathology","Background:                    Recently, there has been a momentous drive to apply advanced artificial intelligence (AI) technologies to diagnostic medicine. The introduction of AI has provided vast new opportunities to improve health care and has introduced a new wave of heightened precision in oncologic pathology. The impact of AI on oncologic pathology has now become apparent, and its use with respect to oral oncology is still in the nascent stage.              Discussion:                    A foundational overview of AI classification systems used in medicine and a review of common terminology used in machine learning and computational pathology will be presented. This paper provides a focused review on the recent advances in AI and deep learning in oncologic histopathology and oral oncology. In addition, specific emphasis on recent studies that have applied these technologies to oral cancer prognostication will also be discussed.              Conclusion:                    Machine and deep learning methods designed to enhance prognostication of oral cancer have been proposed with much of the work focused on prediction models on patient survival and locoregional recurrences in patients with oral squamous cell carcinomas (OSCC). Few studies have explored machine learning methods on OSCC digital histopathologic images. It is evident that further research at the whole slide image level is needed and future collaborations with computer scientists may progress the field of oral oncology.",2020-10-01,0,1511,98,219
1389,33088156,Application of artificial intelligence in the diagnosis and treatment of hepatocellular carcinoma: A review,"Although artificial intelligence (AI) was initially developed many years ago, it has experienced spectacular advances over the last 10 years for application in the field of medicine, and is now used for diagnostic, therapeutic and prognostic purposes in almost all fields. Its application in the area of hepatology is especially relevant for the study of hepatocellular carcinoma (HCC), as this is a very common tumor, with particular radiological characteristics that allow its diagnosis without the need for a histological study. However, the interpretation and analysis of the resulting images is not always easy, in addition to which the images vary during the course of the disease, and prognosis and treatment response can be conditioned by multiple factors. The vast amount of data available lend themselves to study and analysis by AI in its various branches, such as deep-learning (DL) and machine learning (ML), which play a fundamental role in decision-making as well as overcoming the constraints involved in human evaluation. ML is a form of AI based on automated learning from a set of previously provided data and training in algorithms to organize and recognize patterns. DL is a more extensive form of learning that attempts to simulate the working of the human brain, using a lot more data and more complex algorithms. This review specifies the type of AI used by the various authors. However, well-designed prospective studies are needed in order to avoid as far as possible any bias that may later affect the interpretability of the images and thereby limit the acceptance and application of these models in clinical practice. In addition, professionals now need to understand the true usefulness of these techniques, as well as their associated strengths and limitations.",2020-10-01,0,1792,107,219
1374,33117669,Application of Radiomics for the Prediction of Radiation-Induced Toxicity in the IMRT Era: Current State-of-the-Art,"Normal tissue complication probability (NTCP) models that were formulated in the Quantitative Analyses of Normal Tissue Effects in the Clinic (QUANTEC) are one of the pillars in support of everyday's clinical radiation oncology. Because of steady therapeutic refinements and the availability of cutting-edge technical solutions, the ceiling of organs-at-risk-sparing has been reached for photon-based intensity modulated radiotherapy (IMRT). The possibility to capture heterogeneity of patients and tissues in the prediction of toxicity is still an unmet need in modern radiation therapy. Potentially, a major step towards a wider therapeutic index could be obtained from refined assessment of radiation-induced morbidity at an individual level. The rising integration of quantitative imaging and machine learning applications into radiation oncology workflow offers an unprecedented opportunity to further explore the biologic interplay underlying the normal tissue response to radiation. Based on these premises, in this review we focused on the current-state-of-the-art on the use of radiomics for the prediction of toxicity in the field of head and neck, lung, breast and prostate radiotherapy.",2020-10-01,1,1198,115,219
1375,33117259,Role of Artificial Intelligence in TeleStroke: An Overview,"Teleneurology has provided access to neurological expertise and state-of-the-art stroke care where previously they have been inaccessible. The use of Artificial Intelligence with machine learning to assist telestroke care can be revolutionary. This includes more rapid and more reliable diagnosis through imaging analysis as well as prediction of hospital course and 3-month prognosis. Intelligent Electronic Medical Records can search free text and provide decision assistance by analyzing patient charts. Speech recognition has advanced enough to be reliable and highly convenient. Smart contextually aware communication and alert programs can enhance efficiency of patient flow and improve outcomes. Automated data collection and analysis can make quality improvement and research projects quicker and much less burdensome. Despite current challenges, these synergistic technologies hold immense promise in enhancing the clinician experience, helping to reduce physician burnout while improving patient health outcomes at a lower cost. This brief overview discusses the multifaceted potential of AI use in telestroke.",2020-10-01,0,1120,58,219
1376,33117114,A Survey on Deep Learning for Neuroimaging-Based Brain Disorder Analysis,"Deep learning has recently been used for the analysis of neuroimages, such as structural magnetic resonance imaging (MRI), functional MRI, and positron emission tomography (PET), and it has achieved significant performance improvements over traditional machine learning in computer-aided diagnosis of brain disorders. This paper reviews the applications of deep learning methods for neuroimaging-based brain disorder analysis. We first provide a comprehensive overview of deep learning techniques and popular network architectures by introducing various types of deep neural networks and recent developments. We then review deep learning methods for computer-aided analysis of four typical brain disorders, including Alzheimer's disease, Parkinson's disease, Autism spectrum disorder, and Schizophrenia, where the first two diseases are neurodegenerative disorders and the last two are neurodevelopmental and psychiatric disorders, respectively. More importantly, we discuss the limitations of existing studies and present possible future directions.",2020-10-01,1,1050,72,219
1377,33114254,An Optimal Time for Treatment-Predicting Circadian Time by Machine Learning and Mathematical Modelling,"Tailoring medical interventions to a particular patient and pathology has been termed personalized medicine. The outcome of cancer treatments is improved when the intervention is timed in accordance with the patient's internal time. Yet, one challenge of personalized medicine is how to consider the biological time of the patient. Prerequisite for this so-called chronotherapy is an accurate characterization of the internal circadian time of the patient. As an alternative to time-consuming measurements in a sleep-laboratory, recent studies in chronobiology predict circadian time by applying machine learning approaches and mathematical modelling to easier accessible observables such as gene expression. Embedding these results into the mathematical dynamics between clock and cancer in mammals, we review the precision of predictions and the potential usage with respect to cancer treatment and discuss whether the patient's internal time and circadian observables, may provide an additional indication for individualized treatment timing. Besides the health improvement, timing treatment may imply financial advantages, by ameliorating side effects of treatments, thus reducing costs. Summarizing the advances of recent years, this review brings together the current clinical standard for measuring biological time, the general assessment of circadian rhythmicity, the usage of rhythmic variables to predict biological time and models of circadian rhythmicity.",2020-10-01,1,1467,102,219
1378,33110340,A comprehensive survey of AI-enabled phishing attacks detection techniques,"In recent times, a phishing attack has become one of the most prominent attacks faced by internet users, governments, and service-providing organizations. In a phishing attack, the attacker(s) collects the client's sensitive data (i.e., user account login details, credit/debit card numbers, etc.) by using spoofed emails or fake websites. Phishing websites are common entry points of online social engineering attacks, including numerous frauds on the websites. In such types of attacks, the attacker(s) create website pages by copying the behavior of legitimate websites and sends URL(s) to the targeted victims through spam messages, texts, or social networking. To provide a thorough understanding of phishing attack(s), this paper provides a literature review of Artificial Intelligence (AI) techniques: Machine Learning, Deep Learning, Hybrid Learning, and Scenario-based techniques for phishing attack detection. This paper also presents the comparison of different studies detecting the phishing attack for each AI technique and examines the qualities and shortcomings of these methodologies. Furthermore, this paper provides a comprehensive set of current challenges of phishing attacks and future research direction in this domain.",2020-10-01,1,1241,74,219
1379,33106154,Literature mining for context-specific molecular relations using multimodal representations (COMMODAR),"Biological contextual information helps understand various phenomena occurring in the biological systems consisting of complex molecular relations. The construction of context-specific relational resources vastly relies on laborious manual extraction from unstructured literature. In this paper, we propose COMMODAR, a machine learning-based literature mining framework for context-specific molecular relations using multimodal representations. The main idea of COMMODAR is the feature augmentation by the cooperation of multimodal representations for relation extraction. We leveraged biomedical domain knowledge as well as canonical linguistic information for more comprehensive representations of textual sources. The models based on multiple modalities outperformed those solely based on the linguistic modality. We applied COMMODAR to the 14 million PubMed abstracts and extracted 9214 context-specific molecular relations. All corpora, extracted data, evaluation results, and the implementation code are downloadable at https://github.com/jae-hyun-lee/commodar . CCS CONCEPTS:  Computing methodologies~Information extraction  Computing methodologies~Neural networks  Applied computing~Biological networks.",2020-10-01,0,1214,102,219
1948,33204501,Artificial intelligence in orthopaedics: false hope or not? A narrative review along the line of Gartner's hype cycle,"Artificial Intelligence (AI) in general, and Machine Learning (ML)-based applications in particular, have the potential to change the scope of healthcare, including orthopaedic surgery.The greatest benefit of ML is in its ability to learn from real-world clinical use and experience, and thereby its capability to improve its own performance.Many successful applications are known in orthopaedics, but have yet to be adopted and evaluated for accuracy and efficacy in patients' care and doctors' workflows.The recent hype around AI triggered hope for development of better risk stratification tools to personalize orthopaedics in all subsequent steps of care, from diagnosis to treatment.Computer vision applications for fracture recognition show promising results to support decision-making, overcome bias, process high-volume workloads without fatigue, and hold the promise of even outperforming doctors in certain tasks.In the near future, AI-derived applications are very likely to assist orthopaedic surgeons rather than replace us. 'If the computer takes over the simple stuff, doctors will have more time again to practice the art of medicine'.76 Cite this article: EFORT Open Rev 2020;5:593-603. DOI: 10.1302/2058-5241.5.190092.",2020-10-01,0,1236,117,219
1381,33098140,The application of artificial intelligence for the diagnosis and treatment of liver diseases,"Modern medical care produces large volumes of multi-modal patient data, which many clinicians struggle to process and synthesize into actionable knowledge. In recent years, artificial intelligence (AI) has emerged as an effective tool in this regard. The field of hepatology is no exception, with a growing number of studies published that apply artificial intelligence techniques to the diagnosis and treatment of liver diseases. These have included Machine Learning algorithms (such as regression models, Bayesian networks, and support vector machines) to predict disease progression, the presence of complications, and mortality; Deep Learning algorithms to enable rapid, automated interpretation of radiologic and pathologic images; and Natural Language Processing to extract clinically meaningful concepts from vast quantities of unstructured data in Electronic Health Records. This review article will provide a comprehensive overview of hepatology-focused AI research, discuss some of the barriers to clinical implementation and adoption, and suggest future directions for the field.",2020-10-01,0,1090,92,219
1382,33096595,Digital Twin Coaching for Physical Activities: A Survey,"Digital Twin technology has been rising in popularity thanks to the popularity of machine learning in the last decade. As the life expectancy of people around the world is increasing, so is the focus on physical activity to remain healthy especially in the current times where people are staying sedentary while in quarantine. This article aims to provide a survey on the field of Digital Twin technology focusing on machine learning and coaching techniques as they have not been explored yet. We also define what Digital Twin Coaching is and categorize the work done so far in terms of the objective of the physical activity. We also list common Digital Twin Coaching characteristics found in the articles reviewed in terms of concepts such as interactivity, privacy and security and also detail future perspectives in multimodal interaction and standardization, to name a few, that can guide researchers if they choose to work in this field. Finally, we provide a diagram for the Digital Twin Ecosystem showing the interaction between relevant entities and the information flow as well as an idealization of an ideal Digital Twin Ecosystem for team sports' athlete tracking.",2020-10-01,0,1176,55,219
1383,33094899,Machine Learning in the Prediction of Medical Inpatient Length of Stay,"Background:                    Length of stay (LOS) estimates are important for patients, doctors and hospital administrators. However, making accurate estimates of LOS can be difficult for medical patients.              Aims:                    This review was conducted with the aim of identifying and assessing previous studies on the application of machine learning to the prediction of total hospital inpatient LOS for medical patients.              Methods:                    A review of machine learning in the prediction of total hospital LOS for medical inpatients was conducted using the databases PubMed, EMBASE and Web of Science.              Results:                    Of the 673 publications returned by the initial search, 21 articles met inclusion criteria. Of these articles the most commonly represented medical specialty was cardiology. Studies were also identified that had specifically evaluated machine learning LOS prediction in patients with diabetes and tuberculosis. The performance of the machine learning models in the identified studies varied significantly depending on factors including differing input datasets and different LOS thresholds and outcome metrics. Common methodological shortcomings included a lack of reporting of patient demographics and lack of reporting of clinical details of included patients.              Conclusions:                    The variable performance reported by the studies identified in this review supports the need for further research of the utility of machine learning in the prediction of total inpatient LOS in medical patients. Future studies should follow and report a more standardised methodology to better assess performance and to allow replication and validation. In particular, prospective validation studies and studies assessing the clinical impact of such machine learning models would be beneficial. This article is protected by copyright. All rights reserved.",2020-10-01,0,1947,70,219
1932,33240726,Is Artificial Intelligence the New Friend for Radiologists? A Review Article,"Artificial intelligence (AI) is a path-breaking advancement for many industries, including the health care sector. The expeditious development of information technology and data processing has led to the formation of recent tools known as artificial intelligence. Radiology has been a portal for medical technological advancements, and AI will likely be no dissimilar. Radiology is the platform for many technological advances in the medical field; AI can undoubtedly impact every step of a radiologist's workflow. AI can simplify every activity like ordering and scheduling, protocoling and acquisition, image interpretation, reporting, communication, and billing. AI has eminent potential to augment efficiency and accuracy throughout radiology, but it also possesses inherent drawbacks and biases. We collected studies that were published in the past five years using PubMed as our database. We chose studies that were relevant to artificial intelligence in radiology. We mainly focused on the overview of AI in radiology, components included in the functioning of AI, AI assisting in the radiologists' workflow, ethical aspects of AI, challenges, and biases that AI experiencing together with some clinical applications of AI. Of all 33 studies, we found 15 articles discussed the overview and components of AI, five articles about AI affecting radiologist's workflow, five articles related to challenges and biases in AI, two articles discussed ethical aspects of AI, and six articles about practical implications of AI. We found out that the application of AI could make time-dependent tasks that can be performed effortlessly, permitting radiologists more time and opportunities to engage in patient care via increased time for consultation and development in imaging and extracting useful data from those images. AI could only be an aid to radiologists but will not replace a radiologist. Radiologists who use AI to their benefit, rather than to avoid it out of fear, might supersede those radiologists who do not. Substantial research should be done regarding the practical implications of AI algorithms for residents curriculum and the benefits of AI in radiology.",2020-10-01,0,2174,76,219
1933,33240470,The era of big data: Genome-scale modelling meets machine learning,"With omics data being generated at an unprecedented rate, genome-scale modelling has become pivotal in its organisation and analysis. However, machine learning methods have been gaining ground in cases where knowledge is insufficient to represent the mechanisms underlying such data or as a means for data curation prior to attempting mechanistic modelling. We discuss the latest advances in genome-scale modelling and the development of optimisation algorithms for network and error reduction, intracellular constraining and applications to strain design. We further review applications of supervised and unsupervised machine learning methods to omics datasets from microbial and mammalian cell systems and present efforts to harness the potential of both modelling approaches through hybrid modelling.",2020-10-01,1,803,66,219
1421,32647917,"Radiomics in radiation oncology-basics, methods, and limitations","Over the past years, the quantity and complexity of imaging data available for the clinical management of patients with solid tumors has increased substantially. Without the support of methods from the field of artificial intelligence (AI) and machine learning, a complete evaluation of the available image information is hardly feasible in clinical routine. Especially in radiotherapy planning, manual detection and segmentation of lesions is laborious, time consuming, and shows significant variability among observers. Here, AI already offers techniques to support radiation oncologists, whereby ultimately, the productivity and the quality are increased, potentially leading to an improved patient outcome. Besides detection and segmentation of lesions, AI allows the extraction of a vast number of quantitative imaging features from structural or functional imaging data that are typically not accessible by means of human perception. These features can be used alone or in combination with other clinical parameters to generate mathematical models that allow, for example, prediction of the response to radiotherapy. Within the large field of AI, radiomics is the subdiscipline that deals with the extraction of quantitative image features as well as the generation of predictive or prognostic mathematical models. This review gives an overview of the basics, methods, and limitations of radiomics, with a focus on patients with brain tumors treated by radiation therapy.",2020-10-01,1,1477,64,219
1393,33073257,Statistical Hypothesis Testing versus Machine Learning Binary Classification: Distinctions and Guidelines,"Making binary decisions is a common data analytical task in scientific research and industrial applications. In data sciences, there are two related but distinct strategies: hypothesis testing and binary classification. In practice, how to choose between these two strategies can be unclear and rather confusing. Here, we summarize key distinctions between these two strategies in three aspects and list five practical guidelines for data analysts to choose the appropriate strategy for specific analysis needs. We demonstrate the use of those guidelines in a cancer driver gene prediction example.",2020-10-01,0,598,105,219
1371,33120974,A Systematic Review of Machine Learning Techniques in Hematopoietic Stem Cell Transplantation (HSCT),"Machine learning techniques are widely used nowadays in the healthcare domain for the diagnosis, prognosis, and treatment of diseases. These techniques have applications in the field of hematopoietic cell transplantation (HCT), which is a potentially curative therapy for hematological malignancies. Herein, a systematic review of the application of machine learning (ML) techniques in the HCT setting was conducted. We examined the type of data streams included, specific ML techniques used, and type of clinical outcomes measured. A systematic review of English articles using PubMed, Scopus, Web of Science, and IEEE Xplore databases was performed. Search terms included ""hematopoietic cell transplantation (HCT),"" ""autologous HCT,"" ""allogeneic HCT,"" ""machine learning,"" and ""artificial intelligence."" Only full-text studies reported between January 2015 and July 2020 were included. Data were extracted by two authors using predefined data fields. Following PRISMA guidelines, a total of 242 studies were identified, of which 27 studies met the inclusion criteria. These studies were sub-categorized into three broad topics and the type of ML techniques used included ensemble learning (63%), regression (44%), Bayesian learning (30%), and support vector machine (30%). The majority of studies examined models to predict HCT outcomes (e.g., survival, relapse, graft-versus-host disease). Clinical and genetic data were the most commonly used predictors in the modeling process. Overall, this review provided a systematic review of ML techniques applied in the context of HCT. The evidence is not sufficiently robust to determine the optimal ML technique to use in the HCT setting and/or what minimal data variables are required.",2020-10-01,1,1732,100,219
1941,33215079,Use of machine learning in geriatric clinical care for chronic diseases: a systematic literature review,"Objectives:                    Geriatric clinical care is a multidisciplinary assessment designed to evaluate older patients' (age 65 years and above) functional ability, physical health, and cognitive well-being. The majority of these patients suffer from multiple chronic conditions and require special attention. Recently, hospitals utilize various artificial intelligence (AI) systems to improve care for elderly patients. The purpose of this systematic literature review is to understand the current use of AI systems, particularly machine learning (ML), in geriatric clinical care for chronic diseases.              Materials and methods:                    We restricted our search to eight databases, namely PubMed, WorldCat, MEDLINE, ProQuest, ScienceDirect, SpringerLink, Wiley, and ERIC, to analyze research articles published in English between January 2010 and June 2019. We focused on studies that used ML algorithms in the care of geriatrics patients with chronic conditions.              Results:                    We identified 35 eligible studies and classified in three groups: psychological disorder (n = 22), eye diseases (n = 6), and others (n = 7). This review identified the lack of standardized ML evaluation metrics and the need for data governance specific to health care applications.              Conclusion:                    More studies and ML standardization tailored to health care applications are required to confirm whether ML could aid in improving geriatric clinical care.",2020-10-01,1,1513,103,219
1427,32639061,"Modeling, instrumentation, automation, and optimization of water resource recovery facilities (2019) DIRECT","A review of the literature published in 2019 on topics relating to water resource recovery facilities (WRRFs) in the areas of modeling, automation, measurement and sensors, and optimization of wastewater treatment (or water resource reclamation) is presented.",2020-10-01,0,259,107,219
1906,33286958,An Appraisal of Incremental Learning Methods,"As a special case of machine learning, incremental learning can acquire useful knowledge from incoming data continuously while it does not need to access the original data. It is expected to have the ability of memorization and it is regarded as one of the ultimate goals of artificial intelligence technology. However, incremental learning remains a long term challenge. Modern deep neural network models achieve outstanding performance on stationary data distributions with batch training. This restriction leads to catastrophic forgetting for incremental learning scenarios since the distribution of incoming data is unknown and has a highly different probability from the old data. Therefore, a model must be both plastic to acquire new knowledge and stable to consolidate existing knowledge. This review aims to draw a systematic review of the state of the art of incremental learning methods. Published reports are selected from Web of Science, IEEEXplore, and DBLP databases up to May 2020. Each paper is reviewed according to the types: architectural strategy, regularization strategy and rehearsal and pseudo-rehearsal strategy. We compare and discuss different methods. Moreover, the development trend and research focus are given. It is concluded that incremental learning is still a hot research area and will be for a long period. More attention should be paid to the exploration of both biological systems and computational models.",2020-10-01,0,1445,44,219
2623,32518043,Towards a Digital Bioprocess Replica: Computational Approaches in Biopharmaceutical Development and Manufacturing,"Quantitative unit operation models for the optimization and refinement of modern late-stage biopharmaceutical drug manufacturing processes have recently attracted increasing attention. The supplementary benefits of these models include increased process robustness and control in combination with a more stringent design of the bioprocess due to a reduced number of exploratory experiments. In addition to unit operations, further efforts also focus on digital bioprocess replicas, which are straightforward combinations of unit operation and process models from inoculum to the fill and finish phase. In this review, we shed more light on digital bioprocess replicas in addition to standard unit operation models and discuss their strengths and weaknesses. We comment on the current usage of these approaches for late stage processes and outline the associated benefits, challenges and limitations.",2020-10-01,0,899,113,219
1370,33120976,Automatic Gene Function Prediction in the 2020's,"The current rate at which new DNA and protein sequences are being generated is too fast to experimentally discover the functions of those sequences, emphasizing the need for accurate Automatic Function Prediction (AFP) methods. AFP has been an active and growing research field for decades and has made considerable progress in that time. However, it is certainly not solved. In this paper, we describe challenges that the AFP field still has to overcome in the future to increase its applicability. The challenges we consider are how to: (1) include condition-specific functional annotation, (2) predict functions for non-model species, (3) include new informative data sources, (4) deal with the biases of Gene Ontology (GO) annotations, and (5) maximally exploit the GO to obtain performance gains. We also provide recommendations for addressing those challenges, by adapting (1) the way we represent proteins and genes, (2) the way we represent gene functions, and (3) the algorithms that perform the prediction from gene to function. Together, we show that AFP is still a vibrant research area that can benefit from continuing advances in machine learning with which AFP in the 2020s can again take a large step forward reinforcing the power of computational biology.",2020-10-01,0,1272,48,219
2608,32552005,The application of machine learning techniques to innovative antibacterial discovery and development,"Introduction:                    After the initial wave of antibiotic discovery, few novel classes of antibiotics have emerged, with the latest dating back to the 1980's. Furthermore, the pace of antibiotic drug discovery is unable to keep up with the increasing prevalence of antibiotic drug resistance. However, the increasing amount of available data promotes the use of machine learning techniques (MLT) in drug discovery projects (e.g. construction of regression/classification models and ranking/virtual screening of compounds).              Areas covered:                    In this review, the authors cover some of the applications of MLT in medicinal chemistry, focusing on the development of new antibiotics, the prediction of resistance and its mechanisms. The aim of this review is to illustrate the main advantages and disadvantages and the major trends from studies over the past 5 years.              Expert opinion:                    The application of MLT to antibacterial drug discovery can aid the selection of new and potent lead compounds, with desirable pharmacokinetic and toxic profiles for further optimization. The increasing volume of available data along with the constant improvement in computational power and algorithms has meant that we are experiencing a transition in the way we face modern issues such as drug resistance, where our decisions are data-driven and experiments can be focused by data-suggested hypotheses.",2020-10-01,2,1455,100,219
1394,33073256,"High Tech, High Risk: Tech Ethics Lessons for the COVID-19 Pandemic Response","The COVID-19 pandemic has, in a matter of a few short months, drastically reshaped society around the world. Because of the growing perception of machine learning as a technology capable of addressing large problems at scale, machine learning applications have been seen as desirable interventions in mitigating the risks of the pandemic disease. However, machine learning, like many tools of technocratic governance, is deeply implicated in the social production and distribution of risk and the role of machine learning in the production of risk must be considered as engineers and other technologists develop tools for the current crisis. This paper describes the coupling of machine learning and the social production of risk, generally, and in pandemic responses specifically. It goes on to describe the role of risk management in the effort to institutionalize ethics in the technology industry and how such efforts can benefit from a deeper understanding of the social production of risk through machine learning.",2020-10-01,1,1020,76,219
1944,33205139,Artificial Intelligence Meets Citizen Science to Supercharge Ecological Monitoring,"The development and uptake of citizen science and artificial intelligence (AI) techniques for ecological monitoring is increasing rapidly. Citizen science and AI allow scientists to create and process larger volumes of data than possible with conventional methods. However, managers of large ecological monitoring projects have little guidance on whether citizen science, AI, or both, best suit their resource capacity and objectives. To highlight the benefits of integrating the two techniques and guide future implementation by managers, we explore the opportunities, challenges, and complementarities of using citizen science and AI for ecological monitoring. We identify project attributes to consider when implementing these techniques and suggest that financial resources, engagement, participant training, technical expertise, and subject charisma and identification are important project considerations. Ultimately, we highlight that integration can supercharge outcomes for ecological monitoring, enhancing cost-efficiency, accuracy, and multi-sector engagement.",2020-10-01,0,1071,82,219
2677,32424281,Integrated multi-omics approaches to improve classification of chronic kidney disease,"Chronic kidney diseases (CKDs) are currently classified according to their clinical features, associated comorbidities and pattern of injury on biopsy. Even within a given classification, considerable variation exists in disease presentation, progression and response to therapy, highlighting heterogeneity in the underlying biological mechanisms. As a result, patients and clinicians experience uncertainty when considering optimal treatment approaches and risk projection. Technological advances now enable large-scale datasets, including DNA and RNA sequence data, proteomics and metabolomics data, to be captured from individuals and groups of patients along the genotype-phenotype continuum of CKD. The ability to combine these high-dimensional datasets, in which the number of variables exceeds the number of clinical outcome observations, using computational approaches such as machine learning, provides an opportunity to re-classify patients into molecularly defined subgroups that better reflect underlying disease mechanisms. Patients with CKD are uniquely poised to benefit from these integrative, multi-omics approaches since the kidney biopsy, blood and urine samples used to generate these different types of molecular data are frequently obtained during routine clinical care. The ultimate goal of developing an integrated molecular classification is to improve diagnostic classification, risk stratification and assignment of molecular, disease-specific therapies to improve the care of patients with CKD.",2020-11-01,5,1522,85,188
1809,32776259,Neoadjuvant therapy in pancreatic cancer: what is the true oncological benefit?,"Background:                    Neoadjuvant therapies (neoTx) have revolutionized the treatment of borderline resectable (BR) and locally advanced (LA) pancreatic cancer (PCa) by significantly increasing the rate of R0 resections, which remains the only curative strategy for these patients. However, there is still room for improvement of neoTx in PCa.              Purpose:                    Here, we aimed to critically analyze the benefits of neoTx in LA and BR PCa and its potential use on patients with resectable PCa. We also explored the feasibility of arterial resection (AR) to increase surgical radicality and the incorporation of immunotherapy to optimize neoadjuvant approaches in PCa.              Conclusion:                    For early stage, i.e., resectable, PCa, there is not enough scientific evidence for routinely recommending neoTx. For LA and BR PCa, optimization of neoadjuvant therapy necessitates more sophisticated complex surgical resections, machine learning and radiomic approaches, integration of immunotherapy due to the high antigen load, standardized histopathological assessment, and improved multidisciplinary communication.",2020-11-01,0,1162,79,188
1926,33245914,"Challenges in the Development, Deployment, and Regulation of Artificial Intelligence in Anatomic Pathology","Significant advances in artificial intelligence (AI), deep learning, and other machine-learning approaches have been made in recent years, with applications found in almost every industry, including health care. AI has proved to be capable of completing a spectrum of mundane to complex medically oriented tasks previously performed only by boarded physicians, most recently assisting with the detection of cancers difficult to find on histopathology slides. Although computers will not replace pathologists any time soon, properly designed AI-based tools hold great potential for increasing workflow efficiency and diagnostic accuracy in the practice of pathology. Recent trends, such as data augmentation, crowdsourcing for generating annotated data sets, and unsupervised learning with molecular and/or clinical outcomes versus human diagnoses as a source of ground truth, are eliminating the direct role of pathologists in algorithm development. Proper integration of AI-based systems into anatomic-pathology practice will necessarily require fully digital imaging platforms, an overhaul of legacy information-technology infrastructures, modification of laboratory/pathologist workflows, appropriate reimbursement/cost-offsetting models, and ultimately, the active participation of pathologists to encourage buy-in and oversight. Regulations tailored to the nature and limitations of AI are currently in development and, when instituted, are expected to promote safe and effective use. This review addresses the challenges in AI development, deployment, and regulation to be overcome prior to its widespread adoption in anatomic pathology.",2020-11-01,1,1643,106,188
1415,32652384,Predicting the rate constants of semivolatile organic compounds with hydroxyl radicals and ozone in indoor air,"Semivolatile organic compounds (SVOCs) in air can react with hydroxyl radicals (OH), nitrate radicals (NO3) and ozone (O3). Two questions regarding SVOC reactivity with OH, NO3 and O3 in the gas and particle phases remain to be addressed: according to the existing measurements in the literature, which are the most reactive SVOCs in air, and how can the SVOC reactivity in the gas and particle phases be predicted? In the present study, a literature review of the second-order rate constant (k) was carried out to determine the SVOC reactivity with OH, NO3 and O3 in the gas and particle phases in ambient and indoor air at room temperature. Measured k values were available in the literature for 90 polycyclic aromatic hydrocarbons (PAHs), polychlorinated biphenyls (PCBs), organophosphates, dioxins, di(2-ethylhexyl)phthalate (DEHP) and pesticides including pyrifenox, carbamates and terbuthylazine. PAHs and organophosphates were found to be more reactive than dioxins and PCBs. Based on the obtained data, quantitative structure-activity relationship (QSAR) models were developed to predict the k value using quantum chemical, molecular, physical property and environmental descriptors. Eight linear and nonlinear statistical models were employed, including regression models, bagging, random forest and gradient boosting. QSAR models were developed for SVOC/OH reactions in the gas and particle phases and SVOC/O3 reactions in the particle phase. Models for SVOC/NO3 and SVOC/O3 reactions in the gas phase could not be developed due to the lack of measured k values for model training. The least absolute shrinkage and selection operator (LASSO) regression and random forest models were identified as the most effective models for SVOC reactivity prediction according to a comparison of model performance metrics.",2020-11-01,0,1819,110,188
1897,33294867,Model-Driven Decision Making in Multiple Sclerosis Research: Existing Works and Latest Trends,"Multiple sclerosis (MS) is a neurological disorder that strikes the central nervous system. Due to the complexity of this disease, healthcare sectors are increasingly in need of shared clinical decision-making tools to provide practitioners with insightful knowledge and information about MS. These tools ought to be comprehensible by both technical and non-technical healthcare audiences. To aid this cause, this literature review analyzes the state-of-the-art decision support systems (DSSs) in MS research with a special focus on model-driven decision-making processes. The review clusters common methodologies used to support the decision-making process in classifying, diagnosing, predicting, and treating MS. This work observes that the majority of the investigated DSSs rely on knowledge-based and machine learning (ML) approaches, so the utilization of ontology and ML in the MS domain is observed to extend the scope of this review. Finally, this review summarizes the state-of-the-art DSSs, discusses the methods that have commonalities, and addresses the future work of applying DSS technologies in the MS field.",2020-11-01,0,1123,93,188
794,33012648,Effectiveness of Radiofrequency Ablation in the Treatment of Painful Osseous Metastases: A Correlation Meta-Analysis with Machine Learning Cluster Identification,"A systematic review and meta-analysis of pain response after radiofrequency (RF) ablation over time for osseous metastases was conducted in 2019. Analysis used a random-effects model with GOSH plots and meta-regression. Fourteen studies comprising 426 patients, most with recalcitrant pain, were identified. Median pain reduction after RF ablation was 67% over median follow-up of 24 weeks (R2 = -.66, 95% confidence interval -0.76 to -0.55, I2 = 71.24%, fail-safe N = 875) with 44% pain reduction within 1 week. A low-heterogeneity subgroup was identified with median pain reduction after RF ablation of 70% over 12 weeks (R2 = -.75, 95% confidence interval -0.80 to -0.70, I2 = 2.66%, fail-safe N = 910). Addition of cementoplasty after RF ablation did not significantly affect pain scores. Primary tumor type and tumor size did not significantly affect pain scores. A particular, positive association between pain after RF ablation and axial tumors was identified, implying possible increased palliative effects for RF ablation on axial over appendicular lesions. RF ablation is a useful palliative therapy for osseous metastases, particularly in patients with recalcitrant pain.",2020-11-01,0,1182,161,188
1924,33247802,"Magnetic resonance imaging for chronic pain: diagnosis, manipulation, and biomarkers","Pain is a multidimensional subjective experience with biological, psychological, and social factors. Whereas acute pain can be a warning signal for the body to avoid excessive injury, long-term and ongoing pain may be developed as chronic pain. There are more than 100 million people in China living with chronic pain, which has raised a huge socioeconomic burden. Studying the mechanisms of pain and developing effective analgesia approaches are important for basic and clinical research. Recently, with the development of brain imaging and data analytical approaches, the neural mechanisms of chronic pain have been widely studied. In the first part of this review, we briefly introduced the magnetic resonance imaging and conventional analytical approaches for brain imaging data. Then, we reviewed brain alterations caused by several chronic pain disorders, including localized and widespread primary pain, primary headaches and orofacial pain, musculoskeletal pain, and neuropathic pain, and present meta-analytical results to show brain regions associated with the pathophysiology of chronic pain. Next, we reviewed brain changes induced by pain interventions, such as pharmacotherapy, neuromodulation, and acupuncture. Lastly, we reviewed emerging studies that combined advanced machine learning and neuroimaging techniques to identify diagnostic, prognostic, and predictive biomarkers in chronic pain patients.",2020-11-01,4,1418,84,188
1943,33207552,Nonlinear Optical Characterization of 2D Materials,"Characterizing the physical and chemical properties of two-dimensional (2D) materials is of great significance for performance analysis and functional device applications. As a powerful characterization method, nonlinear optics (NLO) spectroscopy has been widely used in the characterization of 2D materials. Here, we summarize the research progress of NLO in 2D materials characterization. First, we introduce the principles of NLO and common detection methods. Second, we introduce the recent research progress on the NLO characterization of several important properties of 2D materials, including the number of layers, crystal orientation, crystal phase, defects, chemical specificity, strain, chemical dynamics, and ultrafast dynamics of excitons and phonons, aiming to provide a comprehensive review on laser-based characterization for exploring 2D material properties. Finally, the future development trends, challenges of advanced equipment construction, and issues of signal modulation are discussed. In particular, we also discuss the machine learning and stimulated Raman scattering (SRS) technologies which are expected to provide promising opportunities for 2D material characterization.",2020-11-01,0,1199,50,188
797,33009150,VR and machine learning: novel pathways in surgical hands-on training,"Purpose of review:                    Surgical training has dramatically changed over the last decade. It has become not only the way to prepare surgeons for their everyday work, but also a way to certify their skills thus increasing patient safety. This article reviews advances in the use of machine learning and artificial intelligence applied to virtual reality based surgical training over the last 5 years.              Recent findings:                    Eight articles have been published which met the inclusion criteria. This included six articles about the use of machine learning and artificial intelligence for assessment purposes and two articles about the possibility of teaching applications, including one review and one original research article. All the research articles pointed out the importance of machine learning and artificial intelligence for the stratification of trainees, based on their performance on basic tasks or procedures simulated in a virtual reality environment.              Summary:                    Machine learning and artificial intelligence are designed to analyse data and use them to take decisions that typically require human intelligence. Evidence in literature is still scarce about this technology applied to virtual reality and existing manuscripts are mainly focused on its potential to stratify surgical performance and provide synthetic feedbacks about it. In consideration of the exponential growth of computer calculation capabilities, it is possible to expect a parallel increase of research about this topic within the next few years.",2020-11-01,0,1596,69,188
1942,33210822,"Worldwide occurrence of haemoplasmas in wildlife: Insights into the patterns of infection, transmission, pathology and zoonotic potential","Haemotropic mycoplasmas (haemoplasmas) have increasingly attracted the attention of wildlife disease researchers due to a combination of wide host range, high prevalence and genetic diversity. A systematic review identified 75 articles that investigated haemoplasma infection in wildlife by molecular methods (chiefly targeting partial 16S rRNA gene sequences), which included 131 host genera across six orders. Studies were less common in the Eastern Hemisphere (especially Africa and Asia) and more frequent in the Artiodactyla and Carnivora. Meta-analysis showed that infection prevalence did not vary by geographic region nor host order, but wild hosts showed significantly higher prevalence than captive hosts. Using a taxonomically flexible machine learning algorithm, we also found vampire bats and cervids to have greater prevalence, whereas mink, a subclade of vesper bats, and true foxes all had lower prevalence compared to the remaining sampled mammal phylogeny. Haemoplasma genotype and nucleotide diversity varied little among wild mammals but were marginally lower in primates and bats. Coinfection with more than one haemoplasma species or genotype was always confirmed when assessed. Risk factors of infection identified were sociality, age, males and high trophic levels, and both prevalence and diversity were often higher in undisturbed environments. Haemoplasmas likely use different and concurrent transmission routes and typically display enzootic dynamics when wild populations are studied longitudinally. Haemoplasma pathology is poorly known in wildlife but appears subclinical. Candidatus Mycoplasma haematohominis, which causes disease in humans, probably has it natural host in bats. Haemoplasmas can serve as a model system in ecological and evolutionary studies, and future research on these pathogens in wildlife must focus on increasing the geographic range and taxa of studies and elucidating pathology, transmission and zoonotic potential. To facilitate such work, we recommend using universal PCR primers or NGS protocols to detect novel haemoplasmas and other genetic markers to differentiate among species and infer cross-species transmission.",2020-11-01,1,2181,137,188
1928,33243455,Machine Learning and Improved Quality Metrics in Acute Intracranial Hemorrhage by Noncontrast Computed Tomography,"Objective:                    The timely reporting of critical results in radiology is paramount to improved patient outcomes. Artificial intelligence has the ability to improve quality by optimizing clinical radiology workflows. We sought to determine the impact of a United States Food and Drug Administration-approved machine learning (ML) algorithm, meant to mark computed tomography (CT) head examinations pending interpretation as higher probability for intracranial hemorrhage (ICH), on metrics across our healthcare system. We hypothesized that ML is associated with a reduction in report turnaround time (RTAT) and length of stay (LOS) in emergency department (ED) and inpatient populations.              Materials and methods:                    An ML algorithm was incorporated across CT scanners at imaging sites in January 2018. RTAT and LOS were derived for reports and patients between July 2017 and December 2017 prior to implementation of ML and compared to those between January 2018 and June 2018 after implementation of ML. A total of 25,658 and 24,996 ED and inpatient cases were evaluated across the entire healthcare system before and after ML, respectively.              Results:                    RTAT decreased from 75 to 69 minutes (P <0.001) at all facilities in the healthcare system. At the level 1 trauma center specifically, RTAT decreased from 67 to 59 minutes (P <0.001). ED LOS decreased from 471 to 425 minutes (P <0.001) for patients without ICH, and from 527 to 491 minutes for those with ICH (P = 0.456). Inpatient LOS decreased from 18.4 to 15.8 days for those without ICH (P = 0.001) and 18.1 to 15.8 days for those with ICH (P = 0.02).              Conclusion:                    We demonstrated that utilization of ML was associated with a statistically significant decrease in RTAT. There was also a significant decrease in LOS for ED patients without ICH, but not for ED patients with ICH. Further evaluation of the impact of such tools on patient care and outcomes is needed.",2020-11-01,0,2022,113,188
1814,32768685,From genetics to epigenetics to unravel the etiology of adolescent idiopathic scoliosis,"Scoliosis is defined as the three-dimensional (3D) structural deformity of the spine with a radiological lateral Cobb angle (a measure of spinal curvature) of 10 that can be caused by congenital, developmental or degenerative problems. However, those cases whose etiology is still unknown, and affect healthy children and adolescents during growth, are the commonest form of spinal deformity, known as adolescent idiopathic scoliosis (AIS). In AIS management, early diagnosis and the accurate prediction of curve progression are most important because they can decrease negative long-term effects of AIS treatment, such as unnecessary bracing, frequent exposure to radiation, as well as saving the high costs of AIS treatment. Despite efforts made to identify a method or technique capable of predicting AIS progression, this challenge still remains unresolved. Genetics and epigenetics, and the application of machine learning and artificial intelligence technologies, open up new avenues to not only clarify AIS etiology, but to also identify potential biomarkers that can substantially improve the clinical management of these patients. This review presents the most relevant biomarkers to help explain the etiopathogenesis of AIS and provide new potential biomarkers to be validated in large clinical trials so they can be finally implemented into clinical settings.",2020-11-01,0,1372,87,188
2017,32848206,Digital pathology and computational image analysis in nephropathology,"The emergence of digital pathology - an image-based environment for the acquisition, management and interpretation of pathology information supported by computational techniques for data extraction and analysis - is changing the pathology ecosystem. In particular, by virtue of our new-found ability to generate and curate digital libraries, the field of machine vision can now be effectively applied to histopathological subject matter by individuals who do not have deep expertise in machine vision techniques. Although these novel approaches have already advanced the detection, classification, and prognostication of diseases in the fields of radiology and oncology, renal pathology is just entering the digital era, with the establishment of consortia and digital pathology repositories for the collection, analysis and integration of pathology data with other domains. The development of machine-learning approaches for the extraction of information from image data, allows for tissue interrogation in a way that was not previously possible. The application of these novel tools are placing pathology centre stage in the process of defining new, integrated, biologically and clinically homogeneous disease categories, to identify patients at risk of progression, and shifting current paradigms for the treatment and prevention of kidney diseases.",2020-11-01,1,1352,69,188
1940,33217963,"Digital Pathology: Advantages, Limitations and Emerging Perspectives","Digital pathology is on the verge of becoming a mainstream option for routine diagnostics. Faster whole slide image scanning has paved the way for this development, but implementation on a large scale is challenging on technical, logistical, and financial levels. Comparative studies have published reassuring data on safety and feasibility, but implementation experiences highlight the need for training and the knowledge of pitfalls. Up to half of the pathologists are reluctant to sign out reports on only digital slides and are concerned about reporting without the tool that has represented their profession since its beginning. Guidelines by international pathology organizations aim to safeguard histology in the digital realm, from image acquisition over the setup of work-stations to long-term image archiving, but must be considered a starting point only. Cost-efficiency analyses and occupational health issues need to be addressed comprehensively. Image analysis is blended into the traditional work-flow, and the approval of artificial intelligence for routine diagnostics starts to challenge human evaluation as the gold standard. Here we discuss experiences from past digital pathology implementations, future possibilities through the addition of artificial intelligence, technical and occupational health challenges, and possible changes to the pathologist's profession.",2020-11-01,1,1387,68,188
1896,33294870,Extensions of the External Validation for Checking Learned Model Interpretability and Generalizability,"We discuss the validation of machine learning models, which is standard practice in determining model efficacy and generalizability. We argue that internal validation approaches, such as cross-validation and bootstrap, cannot guarantee the quality of a machine learning model due to potentially biased training data and the complexity of the validation procedure itself. For better evaluating the generalization ability of a learned model, we suggest leveraging on external data sources from elsewhere as validation datasets, namely external validation. Due to the lack of research attractions on external validation, especially a well-structured and comprehensive study, we discuss the necessity for external validation and propose two extensions of the external validation approach that may help reveal the true domain-relevant model from a candidate set. Moreover, we also suggest a procedure to check whether a set of validation datasets is valid and introduce statistical reference points for detecting external data problems.",2020-11-01,0,1031,102,188
1930,33242635,Artificial intelligence in cardiology,"This review examines the current state and application of artificial intelligence (AI) and machine learning (ML) in cardiovascular medicine. AI is changing the clinical practice of medicine in other specialties. With progress continuing in this emerging technology, the impact for cardiovascular medicine is highlighted to provide insight for the practicing clinician and to identify potential patient benefits.",2020-11-01,1,411,37,188
1939,33222032,Modeling of dynamical systems through deep learning,"This review presents a modern perspective on dynamical systems in the context of current goals and open challenges. In particular, our review focuses on the key challenges of discovering dynamics from data and finding data-driven representations that make nonlinear systems amenable to linear analysis. We explore various challenges in modern dynamical systems, along with emerging techniques in data science and machine learning to tackle them. The two chief challenges are (1) nonlinear dynamics and (2) unknown or partially known dynamics. Machine learning is providing new and powerful techniques for both challenges. Dimensionality reduction methods are used for projecting dynamical methods in reduced form, and these methods perform computational efficiency on real-world data. Data-driven models drive to discover the governing equations and give laws of physics. The identification of dynamical systems through deep learning techniques succeeds in inferring physical systems. Machine learning provides advanced new and powerful algorithms for nonlinear dynamics. Advanced deep learning methods like autoencoders, recurrent neural networks, convolutional neural networks, and reinforcement learning are used in modeling of dynamical systems.",2020-11-01,1,1249,51,188
812,32987202,A comprehensive review of deep learning in colon cancer,"Deep learning has emerged as a leading machine learning tool in object detection and has attracted attention with its achievements in progressing medical image analysis. Convolutional Neural Networks (CNNs) are the most preferred method of deep learning algorithms for this purpose and they have an essential role in the detection and potential early diagnosis of colon cancer. In this article, we hope to bring a perspective to progress in this area by reviewing deep learning practices for colon cancer analysis. This study first presents an overview of popular deep learning architectures used in colon cancer analysis. After that, all studies related to colon cancer analysis are collected under the field of colon cancer and deep learning, then they are divided into five categories that are detection, classification, segmentation, survival prediction, and inflammatory bowel diseases. Then, the studies collected under each category are summarized in detail and listed. We conclude our work with a summary of recent deep learning practices for colon cancer analysis, a critical discussion of the challenges faced, and suggestions for future research. This study differs from other studies by including 135 recent academic papers, separating colon cancer into five different classes, and providing a comprehensive structure. We hope that this study is beneficial to researchers interested in using deep learning techniques for the diagnosis of colon cancer.",2020-11-01,2,1463,55,188
1938,33224516,"Suicidal thoughts, suicidal behaviours and self-harm in daily life: A systematic review of ecological momentary assessment studies","Background:                    Ecological Momentary Assessments (EMA) offer an approach to understand the daily risk factors of suicide and self-harm of individuals through the use of self-monitoring techniques using mobile technologies.              Objectives:                    This systematic review aimed to examine the results of studies on suicidality risk factors and self-harm that used Ecological Momentary Assessments.              Methods:                    Pubmed and PsycINFO databases were searched up to April 2020. Bibliographies of eligible studies were hand-searched, and 744 abstracts were screened and double-coded for inclusion.              Results:                    The 49 studies using EMA included in the review found associations between daily affect, rumination and interpersonal interactions and daily non-suicidal self-injury (NSSI). Studies also found associations between daily negative affect and positive affect, social support, sleep, and emotions and a person's history of suicide and self-harm. Associations between daily suicide thoughts and self-harm, and psychopathology factors measured at baseline were also observed.              Conclusions:                    Research using EMA has the potential to offer clinicians the ability to understand the daily predictors, or risk factors, of suicide and self-harm. However, there are no clear reporting standards for EMA studies on risk factors for suicide. Further research should utilise longitudinal study designs, harmonise datasets and use machine learning techniques to identify patterns of proximal risk factors for suicide behaviours.",2020-11-01,1,1634,130,188
1937,33232845,Fermented food products in the era of globalization: tradition meets biotechnology innovations,"Omics tools offer the opportunity to characterize and trace traditional and industrial fermented foods. Bioinformatics, through machine learning, and other advanced statistical approaches, are able to disentangle fermentation processes and to predict the evolution and metabolic outcomes of a food microbial ecosystem. By assembling microbial artificial consortia, the biotechnological advances will also be able to enhance the nutritional value and organoleptics characteristics of fermented food, preserving, at the same time, the potential of autochthonous microbial consortia and metabolic pathways, which are difficult to reproduce. Preserving the traditional methods contributes to protecting the hidden value of local biodiversity, and exploits its potential in industrial processes with the final aim of guaranteeing food security and safety, even in developing countries.",2020-11-01,0,880,94,188
813,32984921,Application of artificial intelligence models and optimization algorithms in plant cell and tissue culture,"Artificial intelligence (AI) models and optimization algorithms (OA) are broadly employed in different fields of technology and science and have recently been applied to improve different stages of plant tissue culture. The usefulness of the application of AI-OA has been demonstrated in the prediction and optimization of length and number of microshoots or roots, biomass in plant cell cultures or hairy root culture, and optimization of environmental conditions to achieve maximum productivity and efficiency, as well as classification of microshoots and somatic embryos. Despite its potential, the use of AI and OA in this field has been limited due to complex definition terms and computational algorithms. Therefore, a systematic review to unravel modeling and optimizing methods is important for plant researchers and has been acknowledged in this study. First, the main steps for AI-OA development (from data selection to evaluation of prediction and classification models), as well as several AI models such as artificial neural networks (ANNs), neurofuzzy logic, support vector machines (SVMs), decision trees, random forest (FR), and genetic algorithms (GA), have been represented. Then, the application of AI-OA models in different steps of plant tissue culture has been discussed and highlighted. This review also points out limitations in the application of AI-OA in different plant tissue culture processes and provides a new view for future study objectives. KEY POINTS:  Artificial intelligence models and optimization algorithms can be considered a novel and reliable computational method in plant tissue culture.  This review provides the main steps and concepts for model development.  The application of machine learning algorithms in different steps of plant tissue culture has been discussed and highlighted.",2020-11-01,9,1834,106,188
1934,33240317,Current Trends in Experimental and Computational Approaches to Combat Antimicrobial Resistance,"A multitude of factors, such as drug misuse, lack of strong regulatory measures, improper sewage disposal, and low-quality medicine and medications, have been attributed to the emergence of drug resistant microbes. The emergence and outbreaks of multidrug resistance to last-line antibiotics has become quite common. This is further fueled by the slow rate of drug development and the lack of effective resistome surveillance systems. In this review, we provide insights into the recent advances made in computational approaches for the surveillance of antibiotic resistomes, as well as experimental formulation of combinatorial drugs. We explore the multiple roles of antibiotics in nature and the current status of combinatorial and adjuvant-based antibiotic treatments with nanoparticles, phytochemical, and other non-antibiotics based on synergetic effects. Furthermore, advancements in machine learning algorithms could also be applied to combat the spread of antibiotic resistance. Development of resistance to new antibiotics is quite rapid. Hence, we review the recent literature on discoveries of novel antibiotic resistant genes though shotgun and expression-based metagenomics. To decelerate the spread of antibiotic resistant genes, surveillance of the resistome is of utmost importance. Therefore, we discuss integrative applications of whole-genome sequencing and metagenomics together with machine learning models as a means for state-of-the-art surveillance of the antibiotic resistome. We further explore the interactions and negative effects between antibiotics and microbiomes upon drug administration.",2020-11-01,0,1621,94,188
1931,33242537,Automation and data-driven design of polymer therapeutics,"Polymers are uniquely suited for drug delivery and biomaterial applications due to tunable structural parameters such as length, composition, architecture, and valency. To facilitate designs, researchers may explore combinatorial libraries in a high throughput fashion to correlate structure to function. However, traditional polymerization reactions including controlled living radical polymerization (CLRP) and ring-opening polymerization (ROP) require inert reaction conditions and extensive expertise to implement. With the advent of air-tolerance and automation, several polymerization techniques are now compatible with well plates and can be carried out at the benchtop, making high throughput synthesis and high throughput screening (HTS) possible. To avoid HTS pitfalls often described as ""fishing expeditions,"" it is crucial to employ intelligent and big data approaches to maximize experimental efficiency. This is where the disruptive technologies of machine learning (ML) and artificial intelligence (AI) will likely play a role. In fact, ML and AI are already impacting small molecule drug discovery and showing signs of emerging in drug delivery. In this review, we present state-of-the-art research in drug delivery, gene delivery, antimicrobial polymers, and bioactive polymers alongside data-driven developments in drug design and organic synthesis. From this insight, important lessons are revealed for the polymer therapeutics community including the value of a closed loop design-build-test-learn workflow. This is an exciting time as researchers will gain the ability to fully explore the polymer structural landscape and establish quantitative structure-property relationships (QSPRs) with biological significance.",2020-11-01,0,1737,57,188
1927,33245088,"The application potential of machine learning and genomics for understanding natural product diversity, chemistry, and therapeutic translatability","Covering: up to the end of 2020. The machine learning field can be defined as the study and application of algorithms that perform classification and prediction tasks through pattern recognition instead of explicitly defined rules. Among other areas, machine learning has excelled in natural language processing. As such methods have excelled at understanding written languages (e.g. English), they are also being applied to biological problems to better understand the ""genomic language"". In this review we focus on recent advances in applying machine learning to natural products and genomics, and how those advances are improving our understanding of natural product biology, chemistry, and drug discovery. We discuss machine learning applications in genome mining (identifying biosynthetic signatures in genomic data), predictions of what structures will be created from those genomic signatures, and the types of activity we might expect from those molecules. We further explore the application of these approaches to data derived from complex microbiomes, with a focus on the human microbiome. We also review challenges in leveraging machine learning approaches in the field, and how the availability of other ""omics"" data layers provides value. Finally, we provide insights into the challenges associated with interpreting machine learning models and the underlying biology and promises of applying machine learning to natural product drug discovery. We believe that the application of machine learning methods to natural product research is poised to accelerate the identification of new molecular entities that may be used to treat a variety of disease indications.",2020-11-01,0,1674,146,188
1922,33253326,Transcriptional insights into pathogenesis of cutaneous systemic sclerosis using pathway driven meta-analysis assisted by machine learning methods,"Pathophysiology of systemic sclerosis (SSc, Scleroderma), an autoimmune rheumatic disease, comprises of mechanisms that drive vasculopathy, inflammation and fibrosis. Understanding of the disease and associated clinical heterogeneity has advanced considerably in the past decade, highlighting the necessity of more specific targeted therapy. While many of the recent trials in SSc failed to meet the primary end points that predominantly relied on changes in modified Rodnan skin scores (MRSS), sub-group analysis, especially those focused on the basal skin transcriptomic data have provided insights into patient subsets that respond to therapies. These findings suggest that deeper understanding of the molecular changes in pathways is very important to define disease drivers in various patient subgroups. In view of these challenges, we performed meta-analysis on 9 public available SSc microarray studies using a novel pathway pivoted approach combining consensus clustering and machine learning assisted feature selection. Selected pathway modules were further explored through cluster specific topological network analysis in search of novel therapeutic concepts. In addition, we went beyond previously described SSc class divisions of 3 clusters (e.g. inflammation, fibro-proliferative, normal-like) and expanded into a much finer stratification in order to profile SSc patients more accurately. Our analysis unveiled an important 80 pathway signatures that differentiated SSc patients into 8 unique subtypes. The 5 pathway modules derived from such signature successfully defined the 8 SSc subsets and were validated by in-silico cellular deconvolution analysis. Myeloid cells and fibroblasts involvement in different clusters were confirmed and linked to corresponding pathway activities. Collectively, our findings revealed more complex disease subtypes in SSc; Key gene mediators such as IL6, FGFR1, TLR7, PLCG2, IRK2 identified by network analysis underscored the scientific rationale for exploring additional targets in treatment of SSc.",2020-11-01,0,2051,146,188
1916,33260343,Autonomous Corrosion Assessment of Reinforced Concrete Structures: Feasibility Study,"In this work, technological feasibility of autonomous corrosion assessment of reinforced concrete structures is studied. Corrosion of reinforcement bars (rebar), induced by carbonation or chloride penetration, is one of the leading causes for deterioration of concrete structures throughout the globe. Continuous nondestructive in-service monitoring of carbonation through pH and chloride ion (Cl-) concentration in concrete is indispensable for early detection of corrosion and making appropriate decisions, which ultimately make the lifecycle management of RC structures optimal from resources and safety perspectives. Critical state-of-the-art review of pH and Cl- sensors revealed that the majority of the sensors have high sensitivity, reliability, and stability in concrete environment, though the experiments were carried out for relatively short periods. Among the reviewed works, only three attempted to monitor Cl- wirelessly, albeit over a very short range. As part of the feasibility study, this work recommends the use of internet of things (IoT) and machine learning for autonomous corrosion condition assessment of RC structures.",2020-11-01,0,1144,84,188
1915,33260412,Proximal Methods for Plant Stress Detection Using Optical Sensors and Machine Learning,"Plant stresses have been monitored using the imaging or spectrometry of plant leaves in the visible (red-green-blue or RGB), near-infrared (NIR), infrared (IR), and ultraviolet (UV) wavebands, often augmented by fluorescence imaging or fluorescence spectrometry. Imaging at multiple specific wavelengths (multi-spectral imaging) or across a wide range of wavelengths (hyperspectral imaging) can provide exceptional information on plant stress and subsequent diseases. Digital cameras, thermal cameras, and optical filters have become available at a low cost in recent years, while hyperspectral cameras have become increasingly more compact and portable. Furthermore, smartphone cameras have dramatically improved in quality, making them a viable option for rapid, on-site stress detection. Due to these developments in imaging technology, plant stresses can be monitored more easily using handheld and field-deployable methods. Recent advances in machine learning algorithms have allowed for images and spectra to be analyzed and classified in a fully automated and reproducible manner, without the need for complicated image or spectrum analysis methods. This review will highlight recent advances in portable (including smartphone-based) detection methods for biotic and abiotic stresses, discuss data processing and machine learning techniques that can produce results for stress identification and classification, and suggest future directions towards the successful translation of these methods into practical use.",2020-11-01,0,1520,86,188
1914,33260881,A Customizable Analysis Flow in Integrative Multi-Omics,"The number of researchers using multi-omics is growing. Though still expensive, every year it is cheaper to perform multi-omic studies, often exponentially so. In addition to its increasing accessibility, multi-omics reveals a view of systems biology to an unprecedented depth. Thus, multi-omics can be used to answer a broad range of biological questions in finer resolution than previous methods. We used six omic measurements-four nucleic acid (i.e., genomic, epigenomic, transcriptomics, and metagenomic) and two mass spectrometry (proteomics and metabolomics) based-to highlight an analysis workflow on this type of data, which is often vast. This workflow is not exhaustive of all the omic measurements or analysis methods, but it will provide an experienced or even a novice multi-omic researcher with the tools necessary to analyze their data. This review begins with analyzing a single ome and study design, and then synthesizes best practices in data integration techniques that include machine learning. Furthermore, we delineate methods to validate findings from multi-omic integration. Ultimately, multi-omic integration offers a window into the complexity of molecular interactions and a comprehensive view of systems biology.",2020-11-01,0,1240,55,188
1913,33266128,"Deep Learning in LncRNAome: Contribution, Challenges, and Perspectives","Long non-coding RNAs (lncRNA), the pervasively transcribed part of the mammalian genome, have played a significant role in changing our protein-centric view of genomes. The abundance of lncRNAs and their diverse roles across cell types have opened numerous avenues for the research community regarding lncRNAome. To discover and understand lncRNAome, many sophisticated computational techniques have been leveraged. Recently, deep learning (DL)-based modeling techniques have been successfully used in genomics due to their capacity to handle large amounts of data and produce relatively better results than traditional machine learning (ML) models. DL-based modeling techniques have now become a choice for many modeling tasks in the field of lncRNAome as well. In this review article, we summarized the contribution of DL-based methods in nine different lncRNAome research areas. We also outlined DL-based techniques leveraged in lncRNAome, highlighting the challenges computational scientists face while developing DL-based models for lncRNAome. To the best of our knowledge, this is the first review article that summarizes the role of DL-based techniques in multiple areas of lncRNAome.",2020-11-01,1,1191,70,188
2032,32822545,What's New for Clinical Whole-body MRI (WB-MRI) in the 21st Century,"Whole-body MRI (WB-MRI) has evolved since its first introduction in the 1970s as an imaging technique to detect and survey disease across multiple sites and organ systems in the body. The development of diffusion-weighted MRI (DWI) has added a new dimension to the implementation of WB-MRI on modern scanners, offering excellent lesion-to-background contrast, while achieving acceptable spatial resolution to detect focal lesions 5 to 10 mm in size. MRI hardware and software advances have reduced acquisition times, with studies taking 40-50 min to complete.The rising awareness of medical radiation exposure coupled with the advantages of MRI has resulted in increased utilization of WB-MRI in oncology, paediatrics, rheumatological and musculoskeletal conditions and more recently in population screening. There is recognition that WB-MRI can be used to track disease evolution and monitor response heterogeneity in patients with cancer. There are also opportunities to combine WB-MRI with molecular imaging on PET-MRI systems to harness the strengths of hybrid imaging. The advent of artificial intelligence and machine learning will shorten image acquisition times and image analyses, making the technique more competitive against other imaging technologies.",2020-11-01,3,1263,67,188
2038,33785181,The prediction of preeclampsia: the way forward,"Despite intensive investigation, we still cannot adequately predict, treat, or prevent preeclampsia. We have gained awareness that preeclampsia is a syndrome not a disease and is heterogeneous in its presentation and pathophysiology, which may indicate differing underlying phenotypes, and that the impact extends beyond pregnancy per se. Effects on the fetus and mother extend many years after pregnancy, as evidenced by fetal programming of adult disease and increased risk of the development of maternal cardiovascular disease. The increased occurrence of preeclampsia in women with preexisting risk factors suggests that the stress of pregnancy may expose subclinical vascular disease as opposed to preeclampsia damaging the vasculature. The heterogeneity of preeclampsia has blighted efforts to predict preeclampsia early in gestation and has thwarted success in attempts at therapy with treatments, such as low-dose aspirin or global antioxidants. There is a critical need to identify the phenotypes to enable their specific prediction and treatment. Such studies require considerably larger collections of patients than employed in past and current studies. This does not necessarily imply much larger patient numbers in single studies but can be facilitated by the ability to easily combine many smaller studies. This can be accomplished by agreeing on a priori standardized and harmonized clinical data and biospecimen collection across new studies. Such standards are being established by international groups of investigators. Leadership by international organizations, perhaps adopting a carrot and stick approach, to overcome investigator, institutional and funder reticence toward data sharing is required to ensure adoption of such standards. Future studies should include women in both low- and high-resource settings and employ social media and novel methods for data collection and analysis, including machine learning and artificial intelligence. The goal is to identify the pathophysiology underlying differing preeclampsia phenotypes, their successful prediction with the design, and the implementation of phenotype-specific therapies.",2020-11-01,0,2156,47,188
2040,33733218,Prognostics and Health Management of Industrial Assets: Current Progress and Road Ahead,"Prognostic and Health Management (PHM) systems are some of the main protagonists of the Industry 4.0 revolution. Efficiently detecting whether an industrial component has deviated from its normal operating condition or predicting when a fault will occur are the main challenges these systems aim at addressing. Efficient PHM methods promise to decrease the probability of extreme failure events, thus improving the safety level of industrial machines. Furthermore, they could potentially drastically reduce the often conspicuous costs associated with scheduled maintenance operations. The increasing availability of data and the stunning progress of Machine Learning (ML) and Deep Learning (DL) techniques over the last decade represent two strong motivating factors for the development of data-driven PHM systems. On the other hand, the black-box nature of DL models significantly hinders their level of interpretability, de facto limiting their application to real-world scenarios. In this work, we explore the intersection of Artificial Intelligence (AI) methods and PHM applications. We present a thorough review of existing works both in the contexts of fault diagnosis and fault prognosis, highlighting the benefits and the drawbacks introduced by the adoption of AI techniques. Our goal is to highlight potentially fruitful research directions along with characterizing the main challenges that need to be addressed in order to realize the promises of AI-based PHM systems.",2020-11-01,0,1480,87,188
1917,33256133,What Can Machine Learning Approaches in Genomics Tell Us about the Molecular Basis of Amyotrophic Lateral Sclerosis?,"Amyotrophic Lateral Sclerosis (ALS) is the most common late-onset motor neuron disorder, but our current knowledge of the molecular mechanisms and pathways underlying this disease remain elusive. This review (1) systematically identifies machine learning studies aimed at the understanding of the genetic architecture of ALS, (2) outlines the main challenges faced and compares the different approaches that have been used to confront them, and (3) compares the experimental designs and results produced by those approaches and describes their reproducibility in terms of biological results and the performances of the machine learning models. The majority of the collected studies incorporated prior knowledge of ALS into their feature selection approaches, and trained their machine learning models using genomic data combined with other types of mined knowledge including functional associations, protein-protein interactions, disease/tissue-specific information, epigenetic data, and known ALS phenotype-genotype associations. The importance of incorporating gene-gene interactions and cis-regulatory elements into the experimental design of future ALS machine learning studies is highlighted. Lastly, it is suggested that future advances in the genomic and machine learning fields will bring about a better understanding of ALS genetic architecture, and enable improved personalized approaches to this and other devastating and complex diseases.",2020-11-01,2,1450,116,188
1793,32799353,Research challenges and opportunities for using big data in global change biology,"Global change biology has been entering a big data era due to the vast increase in availability of both environmental and biological data. Big data refers to large data volume, complex data sets, and multiple data sources. The recent use of such big data is improving our understanding of interactions between biological systems and global environmental changes. In this review, we first explore how big data has been analyzed to identify the general patterns of biological responses to global changes at scales from gene to ecosystem. After that, we investigate how observational networks and space-based big data have facilitated the discovery of emergent mechanisms and phenomena on the regional and global scales. Then, we evaluate the predictions of terrestrial biosphere under global changes by big modeling data. Finally, we introduce some methods to extract knowledge from big data, such as meta-analysis, machine learning, traceability analysis, and data assimilation. The big data has opened new research opportunities, especially for developing new data-driven theories for improving biological predictions in Earth system models, tracing global change impacts across different organismic levels, and constructing cyberinfrastructure tools to accelerate the pace of model-data integrations. These efforts will uncork the bottleneck of using big data to understand biological responses and adaptations to future global changes.",2020-11-01,0,1437,81,188
2048,33693419,Machine Learning Methods to Predict Acute Respiratory Failure and Acute Respiratory Distress Syndrome,"Acute respiratory failure (ARF) is a common problem in medicine that utilizes significant healthcare resources and is associated with high morbidity and mortality. Classification of acute respiratory failure is complicated, and it is often determined by the level of mechanical support that is required, or the discrepancy between oxygen supply and uptake. These phenotypes make acute respiratory failure a continuum of syndromes, rather than one homogenous disease process. Early recognition of the risk factors for new or worsening acute respiratory failure may prevent that process from occurring. Predictive analytical methods using machine learning leverage clinical data to provide an early warning for impending acute respiratory failure or its sequelae. The aims of this review are to summarize the current literature on ARF prediction, to describe accepted procedures and common machine learning tools for predictive tasks through the lens of ARF prediction, and to demonstrate the challenges and potential solutions for ARF prediction that can improve patient outcomes.",2020-11-01,0,1079,101,188
2050,33659949,New techniques for studying neurodevelopment,"The extraordinary diversity, variability, and complexity of cell types in the vertebrate brain is overwhelming and far exceeds that of any other organ. This complexity is the result of multiple cell divisions and intricate gene regulation and cell movements that take place during embryonic development. Understanding the cellular and molecular mechanisms underlying these complicated developmental processes requires the ability to obtain a complete registry of interconnected events often taking place far apart from each other. To assist with this challenging task, developmental neuroscientists take advantage of a broad set of methods and technologies, often adopted from other fields of research. Here, we review some of the methods developed in recent years whose use has rapidly spread for application in the field of developmental neuroscience. We also provide several considerations regarding the promise that these techniques hold for the near future and share some ideas on how existing methods from other research fields could help with the analysis of how neural circuits emerge.",2020-11-01,0,1093,44,188
1909,33282401,Putting artificial intelligence (AI) on the spot: machine learning evaluation of pulmonary nodules,"Lung cancer remains the leading cause of cancer related death world-wide despite advances in treatment. This largely relates to the fact that many of these patients already have advanced diseases at the time of initial diagnosis. As most lung cancers present as nodules initially, an accurate classification of pulmonary nodules as early lung cancers is critical to reducing lung cancer morbidity and mortality. There have been significant recent advances in artificial intelligence (AI) for lung nodule evaluation. Deep learning (DL) and convolutional neural networks (CNNs) have shown promising results in pulmonary nodule detection and have also excelled in segmentation and classification of pulmonary nodules. This review aims to provide an overview of progress that has been made in AI recently for pulmonary nodule detection and characterization with the ultimate goal of lung cancer prediction and classification while outlining some of the pitfalls and challenges that remain to bring such advancements to routine clinical use.",2020-11-01,0,1036,98,188
16,32393540,Machine learning in GI endoscopy: practical guidance in how to interpret a novel field,"There has been a vast increase in GI literature focused on the use of machine learning in endoscopy. The relative novelty of this field poses a challenge for reviewers and readers of GI journals. To appreciate scientific quality and novelty of machine learning studies, understanding of the technical basis and commonly used techniques is required. Clinicians often lack this technical background, while machine learning experts may be unfamiliar with clinical relevance and implications for daily practice. Therefore, there is an increasing need for a multidisciplinary, international evaluation on how to perform high-quality machine learning research in endoscopy. This review aims to provide guidance for readers and reviewers of peer-reviewed GI journals to allow critical appraisal of the most relevant quality requirements of machine learning studies. The paper provides an overview of common trends and their potential pitfalls and proposes comprehensive quality requirements in six overarching themes: terminology, data, algorithm description, experimental setup, interpretation of results and machine learning in clinical practice.",2020-11-01,4,1141,86,188
2622,32520404,Comprehensive data integration-Toward a more personalized assessment of diastolic function,"Background and aim:                    The main challenge of assessing diastolic function is the balance between clinical utility, in the sense of usability and time-efficiency, and overall applicability, in the sense of precision for the patient under investigation. In this review, we aim to explore the challenges of integrating data in the assessment of diastolic function and discuss the perspectives of a more comprehensive data integration approach.              Methods:                    Review of traditional and novel approaches regarding data integration in the assessment of diastolic function.              Results:                    Comprehensive data integration can lead to improved understanding of disease phenotypes and better relation of these phenotypes to underlying pathophysiological processes-which may help affirm diagnostic reasoning, guide treatment options, and reduce limitations related to previously unaddressed confounders. The optimal assessment of diastolic function should ideally integrate all relevant clinical information with all available structural and functional whole cardiac cycle echocardiographic data-envisioning a personalized approach to patient care, a high-reaching future goal in medicine.              Conclusion:                    Complete data integration seems to be a long-lasting goal, the way forward in diastology, and machine learning seems to be one of the tools suited for the challenge. With perpetual evidence that traditional approaches to complex problems may not the optimal solution, there is room for a steady and cautious, and inherently very exciting paradigm shift toward novel diagnostic tools and workflows to reach a more personalized, comprehensive, and integrated assessment of cardiac function.",2020-11-01,0,1778,90,188
2095,33344334,Artificial Intelligence in Dermatology: A Practical Introduction to a Paradigm Shift,"Artificial Intelligence (AI) has surpassed dermatologists in skin cancer detection, but dermatology still lags behind radiology in its broader adoption. Building and using AI applications are becoming increasingly accessible. However, complex use cases may still require specialized expertise for design and deployment. AI has many applications in dermatology ranging from fundamental research, diagnostics, therapeutics, and cosmetic dermatology. The lack of standardization of images and privacy concerns are the foremost challenges stifling AI adoption. Dermatologists have a significant role to play in standardized data collection, curating data for machine learning, clinically validating AI solutions, and ultimately adopting this paradigm shift that is changing the way we practice.",2020-11-01,0,790,84,188
2047,33693420,Securing Machine Learning in the Cloud: A Systematic Review of Cloud Machine Learning Security,"With the advances in machine learning (ML) and deep learning (DL) techniques, and the potency of cloud computing in offering services efficiently and cost-effectively, Machine Learning as a Service (MLaaS) cloud platforms have become popular. In addition, there is increasing adoption of third-party cloud services for outsourcing training of DL models, which requires substantial costly computational resources (e.g., high-performance graphics processing units (GPUs)). Such widespread usage of cloud-hosted ML/DL services opens a wide range of attack surfaces for adversaries to exploit the ML/DL system to achieve malicious goals. In this article, we conduct a systematic evaluation of literature of cloud-hosted ML/DL models along both the important dimensions-attacks and defenses-related to their security. Our systematic review identified a total of 31 related articles out of which 19 focused on attack, six focused on defense, and six focused on both attack and defense. Our evaluation reveals that there is an increasing interest from the research community on the perspective of attacking and defending different attacks on Machine Learning as a Service platforms. In addition, we identify the limitations and pitfalls of the analyzed articles and highlight open research issues that require further investigation.",2020-11-01,0,1325,94,188
755,33059823,Multimodality Imaging and Artificial Intelligence for Tumor Characterization: Current Status and Future Perspective,"Research in medical imaging has yet to do to achieve precision oncology. Over the past 30 years, only the simplest imaging biomarkers (RECIST, SUV,) have become widespread clinical tools. This may be due to our inability to accurately characterize tumors and monitor intratumoral changes in imaging. Artificial intelligence, through machine learning and deep learning, opens a new path in medical research because it can bring together a large amount of heterogeneous data into the same analysis to reach a single outcome. Supervised or unsupervised learning may lead to new paradigms by identifying unrevealed structural patterns across data. Deep learning will provide human-free, undefined upstream, reproducible, and automated quantitative imaging biomarkers. Since tumor phenotype is driven by its genotype and thus indirectly defines tumoral progression, tumor characterization using machine learning and deep learning algorithms will allow us to monitor molecular expression noninvasively, anticipate therapeutic failure, and lead therapeutic management. To follow this path, quality standards have to be set: standardization of imaging acquisition as it has been done in the field of biology, transparency of the model development as it should be reproducible by different institutions, validation, and testing through a high-quality process using large and complex open databases and better interpretability of these algorithms.",2020-11-01,1,1438,115,188
756,33059822,A Role for FDG PET Radiomics in Personalized Medicine?,"Radiomics describes the extraction of multiple features from medical images, including molecular imaging modalities, that with bioinformatic approaches, provide additional clinically relevant information that may be invisible to the human eye. This information may complement standard radiological interpretation with data that may better characterize a disease or that may provide predictive or prognostic information. Progressing from predefined image features, often describing heterogeneity of voxel intensities within a volume of interest, there is increasing use of machine learning to classify disease characteristics and deep learning methods based on artificial neural networks that can learn features without a priori definition and without the need for preprocessing of images. There have been advances in standardization and harmonization of methods to a level that should support multicenter studies. However, in this relatively early phase of research in the field, there are limited aspects that have been adopted into routine practice. Most of the reports in the molecular imaging field describe radiomic approaches in cancer using 18F-fluorodeoxyglucose positron emission tomography (18F-FDG-PET). In this review, we will describe radiomics in molecular imaging and summarize the pertinent literature in lung cancer where reports are most prevalent and mature.",2020-11-01,1,1377,54,188
2647,32484920,Noninvasive detection of focal seizures in ambulatory patients,"Reliably detecting focal seizures without secondary generalization during daily life activities, chronically, using convenient portable or wearable devices, would offer patients with active epilepsy a number of potential benefits, such as providing more reliable seizure count to optimize treatment and seizure forecasting, and triggering alarms to promote safeguarding interventions. However, no generic solution is currently available to reach these objectives. A number of biosignals are sensitive to specific forms of focal seizures, in particular heart rate and its variability for seizures affecting the neurovegetative system, and accelerometry for those responsible for prominent motor activity. However, most studies demonstrate high rates of false detection or poor sensitivity, with only a minority of patients benefiting from acceptable levels of accuracy. To tackle this challenging issue, several lines of technological progress are envisioned, including multimodal biosensing with cross-modal analytics, a combination of embedded and distributed self-aware machine learning, and ultra-low-power design to enable appropriate autonomy of such sophisticated portable solutions.",2020-11-01,0,1189,62,188
1920,33255668,From Early Morphometrics to Machine Learning-What Future for Cardiovascular Imaging of the Pulmonary Circulation?,"Imaging plays a cardinal role in the diagnosis and management of diseases of the pulmonary circulation. Behind the picture itself, every digital image contains a wealth of quantitative data, which are hardly analysed in current routine clinical practice and this is now being transformed by radiomics. Mathematical analyses of these data using novel techniques, such as vascular morphometry (including vascular tortuosity and vascular volumes), blood flow imaging (including quantitative lung perfusion and computational flow dynamics), and artificial intelligence, are opening a window on the complex pathophysiology and structure-function relationships of pulmonary vascular diseases. They have the potential to make dramatic alterations to how clinicians investigate the pulmonary circulation, with the consequences of more rapid diagnosis and a reduction in the need for invasive procedures in the future. Applied to multimodality imaging, they can provide new information to improve disease characterization and increase diagnostic accuracy. These new technologies may be used as sophisticated biomarkers for risk prediction modelling of prognosis and for optimising the long-term management of pulmonary circulatory diseases. These innovative techniques will require evaluation in clinical trials and may in themselves serve as successful surrogate end points in trials in the years to come.",2020-11-01,0,1397,113,188
777,33038991,Brief History of Artificial Intelligence,"This article reviews the history of artificial intelligence and introduces the reader to major events that prompted interest in the field, as well as pitfalls and challenges that have slowed its development. The purpose of this article is to provide a high-level historical perspective on the development of the field over the past decades, highlighting the potential of the field for transforming health care, but also the importance of setting realistic expectations for artificial intelligence applications to avoid repeating historical cyclical trends and a third ""artificial intelligence winter.""",2020-11-01,0,601,40,188
775,33038993,Overview of Machine Learning: Part 2: Deep Learning for Medical Image Analysis,"Deep learning has contributed to solving complex problems in science and engineering. This article provides the fundamental background required to understand and develop deep learning models for medical imaging applications. The authors review the main deep learning architectures such as multilayer perceptron, convolutional neural networks, autoencoders, recurrent neural networks, and generative adversarial neural networks. They also discuss the strategies for training deep learning models when the available datasets are imbalanced or of limited size and conclude with a discussion of the obstacles and challenges hindering the deployment of deep learning solutions in clinical settings.",2020-11-01,0,693,78,188
774,33038994,Machine Learning Algorithm Validation: From Essentials to Advanced Applications and Implications for Regulatory Certification and Deployment,"The deployment of machine learning (ML) models in the health care domain can increase the speed and accuracy of diagnosis and improve treatment planning and patient care. Translating academic research to applications that are deployable in clinical settings requires the ability to generalize and high reproducibility, which are contingent on a rigorous and sound methodology for the development and evaluation of ML models. This article describes the fundamental concepts and processes for ML model evaluation and highlights common workflows. It concludes with a discussion of the requirements for the deployment of ML models in clinical settings.",2020-11-01,0,648,140,188
773,33038995,Review of Natural Language Processing in Radiology,"Natural language processing (NLP) is an interdisciplinary field, combining linguistics, computer science, and artificial intelligence to enable machines to read and understand human language for meaningful purposes. Recent advancements in deep learning have begun to offer significant improvements in NLP task performance. These techniques have the potential to create new automated tools that could improve clinical workflows and unlock unstructured textual information contained in radiology and clinical reports for the development of radiology and clinical artificial intelligence applications. These applications will combine the appropriate application of classic linguistic and NLP preprocessing techniques, modern NLP techniques, and modern deep learning techniques.",2020-11-01,0,774,50,188
772,33038996,An East Coast Perspective on Artificial Intelligence and Machine Learning: Part 1: Hemorrhagic Stroke Imaging and Triage,"Hemorrhagic stroke is a medical emergency. Artificial intelligence techniques and algorithms may be used to automatically detect and quantitate intracranial hemorrhage in a semiautomated fashion. This article reviews the use of deep learning convolutional neural networks for managing hemorrhagic stroke. Such a capability may be used to alert appropriate care teams, make decisions about patient transport from a primary care center to a comprehensive stroke center, and assist in treatment selection. This article reviews artificial intelligence algorithms for intracranial hemorrhage detection, quantification, and prognostication. Multiple algorithms currently being explored are described and illustrated with the help of examples.",2020-11-01,0,736,120,188
771,33038997,An East Coast Perspective on Artificial Intelligence and Machine Learning: Part 2: Ischemic Stroke Imaging and Triage,"Acute ischemic stroke constitutes approximately 85% of strokes. Most strokes occur in community settings; thus, automatic algorithms techniques are attractive for managing these cases. This article reviews the use of deep learning convolutional neural networks in the management of ischemic stroke. Artificial intelligence-based algorithms may be used in patient triage to detect and sound the alarm based on early imaging, alert care teams, and assist in treatment selection. This article reviews algorithms for artificial intelligence techniques that may be used to detect and localize acute ischemic stroke. We describe artificial intelligence algorithms for these tasks and illustrate them with examples.",2020-11-01,0,708,117,188
770,33038998,Artificial Intelligence and Stroke Imaging: A West Coast Perspective,"Artificial intelligence (AI) advancements have significant implications for medical imaging. Stroke is the leading cause of disability and the fifth leading cause of death in the United States. AI applications for stroke imaging are a topic of intense research. AI techniques are well-suited for dealing with vast amounts of stroke imaging data and a large number of multidisciplinary approaches used in classification, risk assessment, segmentation tasks, diagnosis, prognosis, and even prediction of therapy responses. This article addresses this topic and seeks to present an overview of machine learning and/or deep learning applied to stroke imaging.",2020-11-01,0,655,68,188
769,33038999,Updates on Deep Learning and Glioma: Use of Convolutional Neural Networks to Image Glioma Heterogeneity,"Deep learning represents end-to-end machine learning in which feature selection from images and classification happen concurrently. This articles provides updates on how deep learning is being applied to the study of glioma and its genetic heterogeneity. Deep learning algorithms can detect patterns in routine and advanced MR imaging that elude the eyes of neuroradiologists and make predictions about glioma genetics, which impact diagnosis, treatment response, patient management, and long-term survival. The success of these deep learning initiatives may enhance the performance of neuroradiologists and add greater value to patient care by expediting treatment.",2020-11-01,0,666,103,188
768,33039001,Machine Learning Applications for Head and Neck Imaging,The head and neck (HN) consists of a large number of vital anatomic structures within a compact area. Imaging plays a central role in the diagnosis and management of major disorders affecting the HN. This article reviews the recent applications of machine learning (ML) in HN imaging with a focus on deep learning approaches. It categorizes ML applications in HN imaging into deep learning and traditional ML applications and provides examples of each category. It also discusses the main challenges facing the successful deployment of ML-based applications in the clinical setting and provides suggestions for addressing these challenges.,2020-11-01,0,639,55,188
767,33039002,"Artificial Intelligence Applications for Workflow, Process Optimization and Predictive Analytics","There is great potential for artificial intelligence (AI) applications, especially machine learning and natural language processing, in medical imaging. Much attention has been garnered by the image analysis tasks for diagnostic decision support and precision medicine, but there are many other potential applications of AI in radiology and have potential to enhance all levels of the radiology workflow and practice, including workflow optimization and support for interpretation tasks, quality and safety, and operational efficiency. This article reviews the important potential applications of informatics and AI related to process improvement and operations in the radiology department.",2020-11-01,0,690,96,188
766,33039003,Overview of Machine Learning Part 1: Fundamentals and Classic Approaches,The extensive body of research and advances in machine learning (ML) and the availability of a large volume of patient data make ML a powerful tool for producing models with the potential for widespread deployment in clinical settings. This article provides an overview of the classic supervised and unsupervised ML methods as well as fundamental concepts required for understanding how to develop generalizable and high-performing ML applications. It also describes the important steps for developing a ML model and how decisions made in these steps affect model performance and ability to generalize.,2020-11-01,1,602,72,188
1919,33255705,"Augmented Realities, Artificial Intelligence, and Machine Learning: Clinical Implications and How Technology Is Shaping the Future of Medicine","Technology has been integrated into every facet of human life, and whether it is completely advantageous remains unknown, but one thing is for sure; we are dependent on technology. Medical advances from the integration of artificial intelligence, machine learning, and augmented realities are widespread and have helped countless patients. Much of the advanced technology utilized by medical providers today has been borrowed and extrapolated from other industries. There remains no great collaboration between providers and engineers, which may be why medicine is only in its infancy of innovation with regards to advanced technologic integration. The purpose of this narrative review is to highlight the different technologies currently being utilized in a variety of medical specialties. Furthermore, we hope that by bringing attention to one shortcoming of the medical community, we may inspire future innovators to seek collaboration outside of the purely medical community for the betterment of all patients seeking care.",2020-11-01,3,1027,142,188
1918,33256107,Application of Artificial Intelligence Technology in Oncology: Towards the Establishment of Precision Medicine,"In recent years, advances in artificial intelligence (AI) technology have led to the rapid clinical implementation of devices with AI technology in the medical field. More than 60 AI-equipped medical devices have already been approved by the Food and Drug Administration (FDA) in the United States, and the active introduction of AI technology is considered to be an inevitable trend in the future of medicine. In the field of oncology, clinical applications of medical devices using AI technology are already underway, mainly in radiology, and AI technology is expected to be positioned as an important core technology. In particular, ""precision medicine,"" a medical treatment that selects the most appropriate treatment for each patient based on a vast amount of medical data such as genome information, has become a worldwide trend; AI technology is expected to be utilized in the process of extracting truly useful information from a large amount of medical data and applying it to diagnosis and treatment. In this review, we would like to introduce the history of AI technology and the current state of medical AI, especially in the oncology field, as well as discuss the possibilities and challenges of AI technology in the medical field.",2020-11-01,7,1244,110,188
1898,33294256,Application of artificial intelligence to the diagnosis and therapy of colorectal cancer,"Artificial intelligence (AI) is a relatively new branch of computer science involving many disciplines and technologies, including robotics, speech recognition, natural language and image recognition or processing, and machine learning. Recently, AI has been widely applied in the medical field. The effective combination of AI and big data can provide convenient and efficient medical services for patients. Colorectal cancer (CRC) is a common type of gastrointestinal cancer. The early diagnosis and treatment of CRC are key factors affecting its prognosis. This review summarizes the research progress and clinical application value of AI in the investigation, early diagnosis, treatment, and prognosis of CRC, to provide a comprehensive theoretical basis for AI as a promising diagnostic and treatment tool for CRC.",2020-11-01,0,819,88,188
2650,32479253,Artificial intelligence (AI) in urology-Current use and future directions: An iTRUE study,"Objective:                    Artificial intelligence (AI) is used in various urological conditions such as urolithiasis, pediatric urology, urogynecology, benign prostate hyperplasia (BPH), renal transplant, and uro-oncology. The various models of AI and its application in urology subspecialties are reviewed and discussed.              Material and methods:                    Search strategy was adapted to identify and review the literature pertaining to the application of AI in urology using the keywords ""urology,"" ""artificial intelligence,"" ""machine learning,"" ""deep learning,"" ""artificial neural networks,"" ""computer vision,"" and ""natural language processing"" were included and categorized. Review articles, editorial comments, and non-urologic studies were excluded.              Results:                    The article reviewed 47 articles that reported characteristics and implementation of AI in urological cancer. In all cases with benign conditions, artificial intelligence was used to predict outcomes of the surgical procedure. In urolithiasis, it was used to predict stone composition, whereas in pediatric urology and BPH, it was applied to predict the severity of condition. In cases with malignant conditions, it was applied to predict the treatment response, survival, prognosis, and recurrence on the basis of the genomic and biomarker studies. These results were also found to be statistically better than routine approaches. Application of radiomics in classification and nuclear grading of renal masses, cystoscopic diagnosis of bladder cancers, predicting Gleason score, and magnetic resonance imaging with computer-assisted diagnosis for prostate cancers are few applications of AI that have been studied extensively.              Conclusions:                    In the near future, we will see a shift in the clinical paradigm as AI applications will find their place in the guidelines and revolutionize the decision-making process.",2020-11-01,5,1962,89,188
776,33038992,Knowledge Based Versus Data Based: A Historical Perspective on a Continuum of Methodologies for Medical Image Analysis,"The advent of big data and deep learning algorithms has promoted a major shift toward data-driven methods in medical image analysis recently. However, the medical image analysis field has a long and rich history inclusive of both knowledge-driven and data-driven methodologies. In the present article, we provide a historical review of an illustrative sample of medical image analysis methods and locate them along a knowledge-driven versus data-driven continuum. In doing so, we highlight the historical importance as well as current-day relevance of more traditional, knowledge-based artificial intelligence approaches and their complementarity with fully data-driven techniques such as deep learning.",2020-11-01,0,703,118,188
1479,32594605,Cardiac mechanics in heart failure with preserved ejection fraction,"Heart failure with preserved ejection fraction (HFpEF) is a complex clinical entity associated with significant morbidity and mortality. Common comorbidities including hypertension, coronary artery disease, diabetes, chronic kidney disease, obesity, and increasing age predispose to preclinical diastolic dysfunction that often progresses to frank HFpEF. Clinical HFpEF is typically associated with some degree of diastolic dysfunction, but can occur in the absence of many conventional diastolic dysfunction indices. The exact biologic links between risk factors, structural changes, and clinical manifestations are not clearly apparent. Innovative approaches including deformation imaging have enabled deeper understanding of HFpEF cardiac mechanics beyond conventional metrics. Furthermore, predictive analytics through data-driven platforms have allowed for a deeper understanding of HFpEF phenotypes. This review focuses on the changes in cardiac mechanics that occur through preclinical myocardial dysfunction to clinically apparent HFpEF.",2020-11-01,2,1045,67,188
2007,32871186,Connecting the dots: Advances in modern metabolomics and its application in yeast system,"History of metabolism originates with yeast making bread. In fact, study based on ""Yeast"" was so crucial in the development of the field of biochemistry that the word ""enzyme"" is derived from the Greek word meaning leavened (yeast). Yeast has always been a point of interest as a eukaryotic model system to demonstrate the metabolites and their function. In recent times their metabolites are widely studied to predict their role in various pathways. Many traditional and analytical techniques have been employed, but its study through metabolomics is of recent interest in research. The present review focuses on details about yeast metabolomics based on preliminary research on various analytical techniques along with computational approaches. The review also aimed to highlight machine learning and various inceptions of yeast metabolomics.",2020-11-01,0,844,88,188
1979,32917408,Circuit-Based Biomarkers for Mood and Anxiety Disorders,"Mood and anxiety disorders are complex heterogeneous syndromes that manifest in dysfunctions across multiple brain regions, cell types, and circuits. Biomarkers using brain-wide activity patterns in humans have proven useful in distinguishing between disorder subtypes and identifying effective treatments. In order to improve biomarker identification, it is crucial to understand the basic circuitry underpinning brain-wide activity patterns. Leveraging a large repertoire of techniques, animal studies have examined roles of specific cell types and circuits in driving maladaptive behavior. Recent advances in multiregion recording techniques, data-driven analysis approaches, and machine-learning-based behavioral analysis tools can further push the boundary of animal studies and bridge the gap with human studies, to assess how brain-wide activity patterns encode and drive emotional behavior. Together, these efforts will allow identifying more precise biomarkers to enhance diagnosis and treatment.",2020-11-01,1,1005,55,188
1878,33330127,Mini Review: Clinical Routine Microbiology in the Era of Automation and Digital Health,"Clinical microbiology laboratories are the first line to combat and handle infectious diseases and antibiotic resistance, including newly emerging ones. Although most clinical laboratories still rely on conventional methods, a cascade of technological changes, driven by digital imaging and high-throughput sequencing, will revolutionize the management of clinical diagnostics for direct detection of bacteria and swift antimicrobial susceptibility testing. Importantly, such technological advancements occur in the golden age of machine learning where computers are no longer acting passively in data mining, but once trained, can also help physicians in making decisions for diagnostics and optimal treatment administration. The further potential of physically integrating new technologies in an automation chain, combined to machine-learning-based software for data analyses, is seducing and would indeed lead to a faster management in infectious diseases. However, if, from one side, technological advancement would achieve a better performance than conventional methods, on the other side, this evolution challenges clinicians in terms of data interpretation and impacts the entire hospital personnel organization and management. In this mini review, we discuss such technological achievements offering practical examples of their operability but also their limitations and potential issues that their implementation could rise in clinical microbiology laboratories.",2020-11-01,0,1471,86,188
1355,33147836,Review of Laser Raman Spectroscopy for Surgical Breast Cancer Detection: Stochastic Backpropagation Neural Networks,"Laser Raman spectroscopy (LRS) is a highly specific biomolecular technique which has been shown to have the ability to distinguish malignant and normal breast tissue. This paper discusses significant advancements in the use of LRS in surgical breast cancer diagnosis, with an emphasis on statistical and machine learning strategies employed for precise, transparent and real-time analysis of Raman spectra. When combined with a variety of ""machine learning"" techniques LRS has been increasingly employed in oncogenic diagnostics. This paper proposes that the majority of these algorithms fail to provide the two most critical pieces of information required by the practicing surgeon: a probability that the classification of a tissue is correct, and, more importantly, the expected error in that probability. Stochastic backpropagation artificial neural networks inherently provide both pieces of information for each and every tissue site examined by LRS. If the networks are trained using both human experts and an unsupervised classification algorithm as gold standards, rapid progress can be made understanding what additional contextual data is needed to improve network classification performance. Our patients expect us to not simply have an opinion about their tumor, but to know how certain we are that we are correct. Stochastic networks can provide that information.",2020-11-01,0,1377,115,188
1887,33311940,Emerging use of artificial intelligence in inflammatory bowel disease,"Inflammatory bowel disease (IBD) is a complex, immune-mediated gastrointestinal disorder with ill-defined etiology, multifaceted diagnostic criteria, and unpredictable treatment response. Innovations in IBD diagnostics, including developments in genomic sequencing and molecular analytics, have generated tremendous interest in leveraging these large data platforms into clinically meaningful tools. Artificial intelligence, through machine learning facilitates the interpretation of large arrays of data, and may provide insight to improving IBD outcomes. While potential applications of machine learning models are vast, further research is needed to generate standardized models that can be adapted to target IBD populations.",2020-11-01,1,728,69,188
1435,32633615,Probing the characteristics and biofunctional effects of disease-affected cells and drug response via machine learning applications,"Drug-induced transformations in disease characteristics at the cellular and molecular level offers the opportunity to predict and evaluate the efficacy of pharmaceutical ingredients whilst enabling the optimal design of new and improved drugs with enhanced pharmacokinetics and pharmacodynamics. Machine learning is a promising in-silico tool used to simulate cells with specific disease properties and to determine their response toward drug uptake. Differences in the properties of normal and infected cells, including biophysical, biochemical and physiological characteristics, plays a key role in developing fundamental cellular probing platforms for machine learning applications. Cellular features can be extracted periodically from both the drug treated, infected, and normal cells via image segmentations in order to probe dynamic differences in cell behavior. Cellular segmentation can be evaluated to reflect the levels of drug effect on a distinct cell or group of cells via probability scoring. This article provides an account for the use of machine learning methods to probe differences in the biophysical, biochemical and physiological characteristics of infected cells in response to pharmacokinetics uptake of drug ingredients for application in cancer, diabetes and neurodegenerative disease therapies.",2020-11-01,2,1320,131,188
1972,32925312,Machine learning in the optimization of robotics in the operative field,"Purpose of review:                    The increasing use of robotics in urologic surgery facilitates collection of 'big data'. Machine learning enables computers to infer patterns from large datasets. This review aims to highlight recent findings and applications of machine learning in robotic-assisted urologic surgery.              Recent findings:                    Machine learning has been used in surgical performance assessment and skill training, surgical candidate selection, and autonomous surgery. Autonomous segmentation and classification of surgical data have been explored, which serves as the stepping-stone for providing real-time surgical assessment and ultimately, improve surgical safety and quality. Predictive machine learning models have been created to guide appropriate surgical candidate selection, whereas intraoperative machine learning algorithms have been designed to provide 3-D augmented reality and real-time surgical margin checks. Reinforcement-learning strategies have been utilized in autonomous robotic surgery, and the combination of expert demonstrations and trial-and-error learning by the robot itself is a promising approach towards autonomy.              Summary:                    Robot-assisted urologic surgery coupled with machine learning is a burgeoning area of study that demonstrates exciting potential. However, further validation and clinical trials are required to ensure the safety and efficacy of incorporating machine learning into surgical practice.",2020-11-01,0,1511,71,188
1969,32931022,"Clinical trial design: Past, present, and future in the context of big data and precision medicine","Clinical trials are fundamental for advances in cancer treatment. The traditional framework of phase 1 to 3 trials is designed for incremental advances between regimens. However, our ability to understand and treat cancer has evolved with the increase in drugs targeting an expanding array of therapeutic targets, the development of progressively comprehensive data sets, and emerging computational analytics, all of which are reshaping our treatment strategies. A more robust linkage between drugs and underlying cancer biology is blurring historical lines that define trials on the basis of cancer type. The complexity of the molecular basis of cancer, coupled with manifold variations in clinical status, is driving the individually tailored use of combinations of precision targeted drugs. This approach is spawning a new era of clinical trial types. Although most care is delivered in a community setting, large centers support real-time multi-omic analytics and their integrated interpretation by using machine learning in the context of real-world data sets. Coupling the analytic capabilities of large centers to the tailored delivery of therapy in the community is forging a paradigm that is optimizing service for patients. Understanding the importance of these evolving trends across the health care spectrum will affect our treatment of cancer in the future and is the focus of this review.",2020-11-01,1,1402,98,188
1879,33329090,Reviewing a Decade of Research Into Suicide and Related Behaviour Using the South London and Maudsley NHS Foundation Trust Clinical Record Interactive Search (CRIS) System,"Suicide is a serious public health issue worldwide, yet current clinical methods for assessing a person's risk of taking their own life remain unreliable and new methods for assessing suicide risk are being explored. The widespread adoption of electronic health records (EHRs) has opened up new possibilities for epidemiological studies of suicide and related behaviour amongst those receiving healthcare. These types of records capture valuable information entered by healthcare practitioners at the point of care. However, much recent work has relied heavily on the structured data of EHRs, whilst much of the important information about a patient's care pathway is recorded in the unstructured text of clinical notes. Accessing and structuring text data for use in clinical research, and particularly for suicide and self-harm research, is a significant challenge that is increasingly being addressed using methods from the fields of natural language processing (NLP) and machine learning (ML). In this review, we provide an overview of the range of suicide-related studies that have been carried out using the Clinical Records Interactive Search (CRIS): a database for epidemiological and clinical research that contains de-identified EHRs from the South London and Maudsley NHS Foundation Trust. We highlight the variety of clinical research questions, cohorts and techniques that have been explored for suicide and related behaviour research using CRIS, including the development of NLP and ML approaches. We demonstrate how EHR data provides comprehensive material to study prevalence of suicide and self-harm in clinical populations. Structured data alone is insufficient and NLP methods are needed to more accurately identify relevant information from EHR data. We also show how the text in clinical notes provide signals for ML approaches to suicide risk assessment. We envision increased progress in the decades to come, particularly in externally validating findings across multiple sites and countries, both in terms of clinical evidence and in terms of NLP and machine learning method transferability.",2020-11-01,0,2115,171,188
1964,32941248,Novel classifications for systemic sclerosis: challenging historical subsets to unlock new doors,"Purpose of review:                    Systemic sclerosis (SSc) is a severe rheumatic disease characterized by a considerable heterogeneity in clinical presentations and pathophysiological mechanisms. This variability has a substantial impact on morbidity and mortality and limits the generalizability of clinical trial results. This review aims to highlight recent studies that have proposed new innovative approaches to decipher this heterogeneity, in particular, by attempting to optimize disease classification.              Recent findings:                    The historical dichotomy limited/diffuse subsets based on cutaneous involvement has been challenged by studies highlighting an underestimated heterogeneity between these two subtypes and showing that presence of organ damage and autoantibody profiles markedly influenced survival beyond skin extension. Advanced computational methods using unsupervised machine learning analyses of clinical variables and/or high-throughput omics technologies, clinical variables trajectories modelling overtime or radiomics have provided significant insights on key pathogenic processes that could help defining new subgroups beyond the diffuse/limited subsets.              Summary:                    We can anticipate that a future classification of SSc patients will integrate innovative approaches encompassing clinical phenotypes, variables trajectories, serological features and innovative omics molecular signatures. It nevertheless seems crucial to also pursue the implementation and standardization of readily available and easy to use tools that can be used in clinical practice.",2020-11-01,1,1638,96,188
1960,32950874,Use and performance of machine learning models for type 2 diabetes prediction in community settings: A systematic review and meta-analysis,"Objective:                    We aimed to identify machine learning (ML) models for type 2 diabetes (T2DM) prediction in community settings and determine their predictive performance.              Method:                    Systematic review of ML predictive modelling studies in 13 databases since 2009 was conducted. Primary outcomes included metrics of discrimination, calibration, and classification. Secondary outcomes included important variables, level of validation, and intended use of models. Meta-analysis of c-indices, subgroup analyses, meta-regression, publication bias assessments and sensitivity analyses were conducted.              Results:                    Twenty-three studies (40 prediction models) were included. Studies with high-, moderate-, and low- risk of bias were 3, 14, and 6 respectively. All studies conducted internal validation whereas none conducted external validation of their models. Twenty studies provided classification metrics to varying extents whereas only 7 studies performed model calibration. Eighteen studies reported information on both the variables used for model development and the feature importance. Twelve studies highlighted potential applicability of their models for T2DM screening. Meta-analysis produced a good pooled c-index (0.812). Sources of heterogeneity were identified through subgroup analyses and meta-regression. Issues pertaining to methodological quality and reporting were observed.              Conclusions:                    We found evidence of good performance of ML models for T2DM prediction in the community. Improvements to methodology, reporting and validation are needed before they can be used at scale.",2020-11-01,1,1691,138,188
1324,33198233,Machine Learning Methods in Drug Discovery,"The advancements of information technology and related processing techniques have created a fertile base for progress in many scientific fields and industries. In the fields of drug discovery and development, machine learning techniques have been used for the development of novel drug candidates. The methods for designing drug targets and novel drug discovery now routinely combine machine learning and deep learning algorithms to enhance the efficiency, efficacy, and quality of developed outputs. The generation and incorporation of big data, through technologies such as high-throughput screening and high through-put computational analysis of databases used for both lead and target discovery, has increased the reliability of the machine learning and deep learning incorporated techniques. The use of these virtual screening and encompassing online information has also been highlighted in developing lead synthesis pathways. In this review, machine learning and deep learning algorithms utilized in drug discovery and associated techniques will be discussed. The applications that produce promising results and methods will be reviewed.",2020-11-01,0,1144,42,188
1325,33196915,Review of computational neuroaesthetics: bridging the gap between neuroaesthetics and computer science,"The mystery of aesthetics attracts scientists from various research fields. The topic of aesthetics, in combination with other disciplines such as neuroscience and computer science, has brought out the burgeoning fields of neuroaesthetics and computational aesthetics within less than two decades. Despite profound findings are carried out by experimental approaches in neuroaesthetics and by machine learning algorithms in computational neuroaesthetics, these two fields cannot be easily combined to benefit from each other and findings from each field are isolated. Computational neuroaesthetics, which inherits computational approaches from computational aesthetics and experimental approaches from neuroaesthetics, seems to be promising to bridge the gap between neuroaesthetics and computational aesthetics. Here, we review theoretical models and neuroimaging findings about brain activity in neuroaesthetics. Then machine learning algorithms and computational models in computational aesthetics are enumerated. Finally, we introduce studies in computational neuroaesthetics which combine computational models with neuroimaging data to analyze brain connectivity during aesthetic appreciation or give a prediction on aesthetic preference. This paper outlines the rich potential for computational neuroaesthetics to take advantages from both neuroaesthetics and computational aesthetics. We conclude by discussing some of the challenges and potential prospects in computational neuroaesthetics, and highlight issues for future consideration.",2020-11-01,0,1545,102,188
1332,33185950,Analytics with artificial intelligence to advance the treatment of acute respiratory distress syndrome,"Artificial intelligence (AI) has found its way into clinical studies in the era of big data. Acute respiratory distress syndrome (ARDS) or acute lung injury (ALI) is a clinical syndrome that encompasses a heterogeneous population. Management of such heterogeneous patient population is a big challenge for clinicians. With accumulating ALI datasets being publicly available, more knowledge could be discovered with sophisticated analytics. We reviewed literatures with big data analytics to understand the role of AI for improving the caring of patients with ALI/ARDS. Many studies have utilized the electronic medical records (EMR) data for the identification and prognostication of ARDS patients. As increasing number of ARDS clinical trials data is open to public, secondary analysis on these combined datasets provide a powerful way of finding solution to clinical questions with a new perspective. AI techniques such as Classification and Regression Tree (CART) and artificial neural networks (ANN) have also been successfully used in the investigation of ARDS problems. Individualized treatment of ARDS could be implemented with a support from AI as we are now able to classify ARDS into many subphenotypes by unsupervised machine learning algorithms. Interestingly, these subphenotypes show different responses to a certain intervention. However, current analytics involving ARDS have not fully incorporated information from omics such as transcriptome, proteomics, daily activities and environmental conditions. AI technology is assisting us to interpret complex data of ARDS patients and enable us to further improve the management of ARDS patients in future with individual treatment plans.",2020-11-01,4,1700,102,188
1439,32631041,Recent Advances in the Application of Artificial Intelligence in Otorhinolaryngology-Head and Neck Surgery,"This study presents an up-to-date survey of the use of artificial intelligence (AI) in the field of otorhinolaryngology, considering opportunities, research challenges, and research directions. We searched PubMed, the Cochrane Central Register of Controlled Trials, Embase, and the Web of Science. We initially retrieved 458 articles. The exclusion of non-English publications and duplicates yielded a total of 90 remaining studies. These 90 studies were divided into those analyzing medical images, voice, medical devices, and clinical diagnoses and treatments. Most studies (42.2%, 38/90) used AI for image-based analysis, followed by clinical diagnoses and treatments (24 studies). Each of the remaining two subcategories included 14 studies. Machine learning and deep learning have been extensively applied in the field of otorhinolaryngology. However, the performance of AI models varies and research challenges remain.",2020-11-01,0,924,106,188
1958,32954550,"High Throughput Methods in the Synthesis, Characterization, and Optimization of Porous Materials","Porous materials are widely employed in a large range of applications, in particular, for storage, separation, and catalysis of fine chemicals. Synthesis, characterization, and pre- and post-synthetic computer simulations are mostly carried out in a piecemeal and ad hoc manner. Whilst high throughput approaches have been used for more than 30 years in the porous material fields, routine integration of experimental and computational processes is only now becoming more established. Herein, important developments are highlighted and emerging challenges for the community identified, including the need to work toward more integrated workflows.",2020-11-01,0,646,96,188
1335,33182638,Wearable Sensors Incorporating Compensatory Reserve Measurement for Advancing Physiological Monitoring in Critically Injured Trauma Patients,"Vital signs historically served as the primary method to triage patients and resources for trauma and emergency care, but have failed to provide clinically-meaningful predictive information about patient clinical status. In this review, a framework is presented that focuses on potential wearable sensor technologies that can harness necessary electronic physiological signal integration with a current state-of-the-art predictive machine-learning algorithm that provides early clinical assessment of hypovolemia status to impact patient outcome. The ability to study the physiology of hemorrhage using a human model of progressive central hypovolemia led to the development of a novel machine-learning algorithm known as the compensatory reserve measurement (CRM). Greater sensitivity, specificity, and diagnostic accuracy to detect hemorrhage and onset of decompensated shock has been demonstrated by the CRM when compared to all standard vital signs and hemodynamic variables. The development of CRM revealed that continuous measurements of changes in arterial waveform features represented the most integrated signal of physiological compensation for conditions of reduced systemic oxygen delivery. In this review, detailed analysis of sensor technologies that include photoplethysmography, tonometry, ultrasound-based blood pressure, and cardiogenic vibration are identified as potential candidates for harnessing arterial waveform analog features required for real-time calculation of CRM. The integration of wearable sensors with the CRM algorithm provides a potentially powerful medical monitoring advancement to save civilian and military lives in emergency medical settings.",2020-11-01,0,1684,140,188
1949,33202354,Unlocking the next generation of phage therapy: the key is in the receptors,"Phage therapy, the clinical use of viruses that kill bacteria, is a promising strategy in the fight against antimicrobial resistance. Before administration, phages undergo a careful examination of their safety and interactions with target bacteria. This characterization seldom includes identifying the receptor on the bacterial surface involved in phage adsorption. In this perspective article, we propose that understanding the function and location of these phage receptors can open the door to improved and innovative ways to use phage therapy. With knowledge of phage receptors, we can design intelligent phage cocktails, discover new phage-derived antimicrobials, and steer the evolution of phage-resistance towards clinically exploitable phenotypes. In an effort to jump-start this initiative, we recommend priority groups of hosts and phages. Finally, we review modern approaches for the identification of phage receptors, including molecular platforms for high-throughput mutagenesis, synthetic biology, and machine learning.",2020-11-01,0,1034,75,188
1338,33176311,Recent Advances in Medical Image Processing,"Background:                    Application and development of the artificial intelligence technology have generated a profound impact in the field of medical imaging. It helps medical personnel to make an early and more accurate diagnosis. Recently, the deep convolution neural network is emerging as a principal machine learning method in computer vision and has received significant attention in medical imaging. Key Message: In this paper, we will review recent advances in artificial intelligence, machine learning, and deep convolution neural network, focusing on their applications in medical image processing. To illustrate with a concrete example, we discuss in detail the architecture of a convolution neural network through visualization to help understand its internal working mechanism.              Summary:                    This review discusses several open questions, current trends, and critical challenges faced by medical image processing and artificial intelligence technology.",2020-11-01,0,999,43,188
1340,33167561,Application of Transfer Learning in EEG Decoding Based on Brain-Computer Interfaces: A Review,"The algorithms of electroencephalography (EEG) decoding are mainly based on machine learning in current research. One of the main assumptions of machine learning is that training and test data belong to the same feature space and are subject to the same probability distribution. However, this may be violated in EEG processing. Variations across sessions/subjects result in a deviation of the feature distribution of EEG signals in the same task, which reduces the accuracy of the decoding model for mental tasks. Recently, transfer learning (TL) has shown great potential in processing EEG signals across sessions/subjects. In this work, we reviewed 80 related published studies from 2010 to 2020 about TL application for EEG decoding. Herein, we report what kind of TL methods have been used (e.g., instance knowledge, feature representation knowledge, and model parameter knowledge), describe which types of EEG paradigms have been analyzed, and summarize the datasets that have been used to evaluate performance. Moreover, we discuss the state-of-the-art and future development of TL for EEG decoding. The results show that TL can significantly improve the performance of decoding models across subjects/sessions and can reduce the calibration time of brain-computer interface (BCI) systems. This review summarizes the current practical suggestions and performance outcomes in the hope that it will provide guidance and help for EEG research in the future.",2020-11-01,2,1461,93,188
1341,33167558,Computational Diagnostic Techniques for Electrocardiogram Signal Analysis,"Cardiovascular diseases (CVDs), including asymptomatic myocardial ischemia, angina, myocardial infarction, and ischemic heart failure, are the leading cause of death globally. Early detection and treatment of CVDs significantly contribute to the prevention or delay of cardiovascular death. Electrocardiogram (ECG) records the electrical impulses generated by heart muscles, which reflect regular or irregular beating activity. Computer-aided techniques provide fast and accurate tools to identify CVDs using a patient's ECG signal, which have achieved great success in recent years. Latest computational diagnostic techniques based on ECG signals for estimating CVDs conditions are summarized here. The procedure of ECG signals analysis is discussed in several subsections, including data preprocessing, feature engineering, classification, and application. In particular, the End-to-End models integrate feature extraction and classification into learning algorithms, which not only greatly simplifies the process of data analysis, but also shows excellent accuracy and robustness. Portable devices enable users to monitor their cardiovascular status at any time, bringing new scenarios as well as challenges to the application of ECG algorithms. Computational diagnostic techniques for ECG signal analysis show great potential for helping health care professionals, and their application in daily life benefits both patients and sub-healthy people.",2020-11-01,0,1451,73,188
1342,33165729,Suicide Risk Assessment Using Machine Learning and Social Networks: a Scoping Review,"According to the World Health Organization (WHO) report in 2016, around 800,000 of individuals have committed suicide. Moreover, suicide is the second cause of unnatural death in people between 15 and 29 years. This paper reviews state of the art on the literature concerning the use of machine learning methods for suicide detection on social networks. Consequently, the objectives, data collection techniques, development process and the validation metrics used for suicide detection on social networks are analyzed. The authors conducted a scoping review using the methodology proposed by Arksey and O'Malley et al. and the PRISMA protocol was adopted to select the relevant studies. This scoping review aims to identify the machine learning techniques used to predict suicide risk based on information posted on social networks. The databases used are PubMed, Science Direct, IEEE Xplore and Web of Science. In total, 50% of the included studies (8/16) report explicitly the use of data mining techniques for feature extraction, feature detection or entity identification. The most commonly reported method was the Linguistic Inquiry and Word Count (4/8, 50%), followed by Latent Dirichlet Analysis, Latent Semantic Analysis, and Word2vec (2/8, 25%). Non-negative Matrix Factorization and Principal Component Analysis were used only in one of the included studies (12.5%). In total, 3 out of 8 research papers (37.5%) combined more than one of those techniques. Supported Vector Machine was implemented in 10 out of the 16 included studies (62.5%). Finally, 75% of the analyzed studies implement machine learning-based models using Python.",2020-11-01,1,1643,84,188
1372,33120319,Findings from machine learning in clinical medical imaging applications - Lessons for translation to the forensic setting,"Machine learning (ML) techniques are increasingly being used in clinical medical imaging to automate distinct processing tasks. In post-mortem forensic radiology, the use of these algorithms presents significant challenges due to variability in organ position, structural changes from decomposition, inconsistent body placement in the scanner, and the presence of foreign bodies. Existing ML approaches in clinical imaging can likely be transferred to the forensic setting with careful consideration to account for the increased variability and temporal factors that affect the data used to train these algorithms. Additional steps are required to deal with these issues, by incorporating the possible variability into the training data through data augmentation, or by using atlases as a pre-processing step to account for death-related factors. A key application of ML would be then to highlight anatomical and gross pathological features of interest, or present information to help optimally determine the cause of death. In this review, we highlight results and limitations of applications in clinical medical imaging that use ML to determine key implications for their application in the forensic setting.",2020-11-01,0,1210,121,188
1346,33160516,Predictive modeling in reproductive medicine: Where will the future of artificial intelligence research take us?,"Artificial intelligence (AI) systems have been proposed for reproductive medicine since 1997. Although AI is the main driver of emergent technologies in reproduction, such as robotics, Big Data, and internet of things, it will continue to be the engine for technological innovation for the foreseeable future. What does the future of AI research look like?",2020-11-01,0,356,112,188
1347,33160514,Evaluating predictive models in reproductive medicine,"Predictive modeling has become a distinct subdiscipline of reproductive medicine, and researchers and clinicians are just learning the skills and expertise to evaluate artificial intelligence (AI) studies. Diagnostic tests and model predictions are subject to evaluation. Their use offers potential for both harm and benefit in terms of diagnosis, treatment, and prognosis. The performance of AI models and their potential clinical utility hinge on the quality and size of the databases used, the types and distribution of data, and the particular AI method applied. Additionally, when images are involved, the method of capturing, preprocessing, and treatment and accurate labeling of images becomes an important component of AI modeling. Inconsistent image treatment or inaccurate labeling of images can lead to an inconsistent database, resulting in poor AI accuracy. We discuss the critical appraisal of AI models in reproductive medicine and convey the importance of transparency and standardization in reporting AI models so that the risk of bias and the potential clinical utility of AI can be assessed.",2020-11-01,0,1110,53,188
1348,33160513,Artificial intelligence in human in vitro fertilization and embryology,"Embryo evaluation and selection embody the aggregate manifestation of the entire in vitro fertilization (IVF) process. It aims to choose the ""best"" embryos from the larger cohort of fertilized oocytes, the majority of which will be determined to be not viable either as a result of abnormal development or due to chromosomal imbalances. Indeed, it is generally acknowledged that even after embryo selection based on morphology, time-lapse microscopic photography, or embryo biopsy with preimplantation genetic testing, implantation rates in the human are difficult to predict. Our pursuit of enhancing embryo evaluation and selection, as well as increasing live birth rates, will require the adoption of novel technologies. Recently, several artificial intelligence (AI)-based methods have emerged as objective, standardized, and efficient tools for evaluating human embryos. Moreover, AI-based methods can be implemented for other clinical aspects of IVF, such as assessing patient reproductive potential and individualizing gonadotropin stimulation protocols. As AI has the capability to analyze ""big"" data, the ultimate goal will be to apply AI tools to the analysis of all embryological, clinical, and genetic data in an effort to provide patient-tailored treatments. In this chapter, we present an overview of existing AI technologies in reproductive medicine and envision their potential future applications in the field.",2020-11-01,1,1427,70,188
1349,33160512,Precision medicine and artificial intelligence: overview and relevance to reproductive medicine,"Traditionally, new treatments have been developed for the population at large. Recently, large-scale genomic sequencing analyses have revealed tremendous genetic diversity between individuals. In diseases driven by genetic events such as cancer, genomic sequencing can unravel all the mutations that drive individual tumors. The ability to capture the genetic makeup of individual patients has led to the concept of precision medicine, a modern, technology-driven form of personalized medicine. Precision medicine matches each individual to the best treatment in a way that is tailored to his or her genetic uniqueness. To further personalize medicine, precision medicine increasingly incorporates and integrates data beyond genomics, such as epigenomics and metabolomics, as well as imaging. Increasingly, the robust use and integration of these modalities in precision medicine require the use of artificial intelligence and machine learning. This modern view of precision medicine, adopted early in certain areas of medicine such as cancer, has started to impact the field of reproductive medicine. Here we review the concepts and history of precision medicine and artificial intelligence, highlight their growing impact on reproductive medicine, and outline some of the challenges and limitations that these new fields have encountered in medicine.",2020-11-01,0,1352,95,188
1350,33156423,Survival prediction of glioblastoma patients-are we there yet? A systematic review of prognostic modeling for glioblastoma and its clinical potential,"Glioblastoma is associated with a poor prognosis. Even though survival statistics are well-described at the population level, it remains challenging to predict the prognosis of an individual patient despite the increasing number of prognostic models. The aim of this study is to systematically review the literature on prognostic modeling in glioblastoma patients. A systematic literature search was performed to identify all relevant studies that developed a prognostic model for predicting overall survival in glioblastoma patients following the PRISMA guidelines. Participants, type of input, algorithm type, validation, and testing procedures were reviewed per prognostic model. Among 595 citations, 27 studies were included for qualitative review. The included studies developed and evaluated a total of 59 models, of which only seven were externally validated in a different patient cohort. The predictive performance among these studies varied widely according to the AUC (0.58-0.98), accuracy (0.69-0.98), and C-index (0.66-0.70). Three studies deployed their model as an online prediction tool, all of which were based on a statistical algorithm. The increasing performance of survival prediction models will aid personalized clinical decision-making in glioblastoma patients. The scientific realm is gravitating towards the use of machine learning models developed on high-dimensional data, often with promising results. However, none of these models has been implemented into clinical care. To facilitate the clinical implementation of high-performing survival prediction models, future efforts should focus on harmonizing data acquisition methods, improving model interpretability, and externally validating these models in multicentered, prospective fashion.",2020-11-01,0,1771,149,188
1351,33156361,"Evolving robotic surgery training and improving patient safety, with the integration of novel technologies","Introduction:                    Robot-assisted surgery is becoming increasingly adopted by multiple surgical specialties. There is evidence of inherent risks of utilising new technologies that are unfamiliar early in the learning curve. The development of standardised and validated training programmes is crucial to deliver safe introduction. In this review, we aim to evaluate the current evidence and opportunities to integrate novel technologies into modern digitalised robotic training curricula.              Methods:                    A systematic literature review of the current evidence for novel technologies in surgical training was conducted online and relevant publications and information were identified. Evaluation was made on how these technologies could further enable digitalisation of training.              Results:                    Overall, the quality of available studies was found to be low with current available evidence consisting largely of expert opinion, consensus statements and small qualitative studies. The review identified that there are several novel technologies already being utilised in robotic surgery training. There is also a trend towards standardised validated robotic training curricula. Currently, the majority of the validated curricula do not incorporate novel technologies and training is delivered with more traditional methods that includes centralisation of training services with wet laboratories that have access to cadavers and dedicated training robots.              Conclusions:                    Improvements to training standards and understanding performance data have good potential to significantly lower complications in patients. Digitalisation automates data collection and brings data together for analysis. Machine learning has potential to develop automated performance feedback for trainees. Digitalised training aims to build on the current gold standards and to further improve the 'continuum of training' by integrating PBP training, 3D-printed models, telementoring, telemetry and machine learning.",2020-11-01,0,2079,106,188
1885,33319174,Emerging Materials for Neuromorphic Devices and Systems,"Neuromorphic devices and systems have attracted attention as next-generation computing due to their high efficiency in processing complex data. So far, they have been demonstrated using both machine-learning software and complementary metal-oxide-semiconductor-based hardware. However, these approaches have drawbacks in power consumption and learning speed. An energy-efficient neuromorphic computing system requires hardware that can mimic the functions of a brain. Therefore, various materials have been introduced for the development of neuromorphic devices. Here, recent advances in neuromorphic devices are reviewed. First, the functions of biological synapses and neurons are discussed. Also, deep neural networks and spiking neural networks are described. Then, the operation mechanism and the neuromorphic functions of emerging devices are reviewed. Finally, the challenges and prospects for developing neuromorphic devices that use emerging materials are discussed.",2020-11-01,0,975,55,188
1334,33182824,Evapotranspiration Estimation with Small UAVs in Precision Agriculture,"Estimating evapotranspiration (ET) has been one of the most critical research areas in agriculture because of water scarcity, the growing population, and climate change. The accurate estimation and mapping of ET are necessary for crop water management. Traditionally, researchers use water balance, soil moisture, weighing lysimeters, or an energy balance approach, such as Bowen ratio or eddy covariance towers to estimate ET. However, these ET methods are point-specific or area-weighted measurements and cannot be extended to a large scale. With the advent of satellite technology, remote sensing images became able to provide spatially distributed measurements. However, the spatial resolution of multispectral satellite images is in the range of meters, tens of meters, or hundreds of meters, which is often not enough for crops with clumped canopy structures, such as trees and vines. Unmanned aerial vehicles (UAVs) can mitigate these spatial and temporal limitations. Lightweight cameras and sensors can be mounted on the UAVs and take high-resolution images. Unlike satellite imagery, the spatial resolution of the UAV images can be at the centimeter-level. UAVs can also fly on-demand, which provides high temporal imagery. In this study, the authors examined different UAV-based approaches of ET estimation at first. Models and algorithms, such as mapping evapotranspiration at high resolution with internalized calibration (METRIC), the two-source energy balance (TSEB) model, and machine learning (ML) are analyzed and discussed herein. Second, challenges and opportunities for UAVs in ET estimation are also discussed, such as uncooled thermal camera calibration, UAV image collection, and image processing. Then, the authors share views on ET estimation with UAVs for future research and draw conclusive remarks.",2020-11-01,1,1827,70,188
1875,32674040,Use of artificial intelligence in diagnosis of head and neck precancerous and cancerous lesions: A systematic review,"This systematic review analyses and describes the application and diagnostic accuracy of Artificial Intelligence (AI) methods used for detection and grading of potentially malignant (pre-cancerous) and cancerous head and neck lesions using whole slide images (WSI) of human tissue slides. Electronic databases MEDLINE via OVID, Scopus and Web of Science were searched between October 2009 - April 2020. Tailored search-strings were developed using database-specific terms. Studies were selected using a strict inclusion criterion following PRISMA Guidelines. Risk of bias assessment was conducted using a tailored QUADAS-2 tool. Out of 315 records, 11 fulfilled the inclusion criteria. AI-based methods were employed for analysis of specific histological features for oral epithelial dysplasia (n = 1), oral submucous fibrosis (n = 5), oral squamous cell carcinoma (n = 4) and oropharyngeal squamous cell carcinoma (n = 1). A combination of heuristics, supervised and unsupervised learning methods were employed, including more than 10 different classification and segmentation techniques. Most studies used uni-centric datasets (range 40-270 images) comprising small sub-images within WSI with accuracy between 79 and 100%. This review provides early evidence to support the potential application of supervised machine learning methods as a diagnostic aid for some oral potentially malignant and malignant lesions; however, there is a paucity of evidence using AI for diagnosis of other head and neck pathologies. Overall, the quality of evidence is low, with most studies showing a high risk of bias which is likely to have overestimated accuracy rates. This review highlights the need for development of state-of-the-art deep learning techniques in future head and neck research.",2020-11-01,1,1782,116,188
1886,33313059,Leveraging Data Science for a Personalized Haemodialysis,"Background:                    The 2019 Science for Dialysis Meeting at Bellvitge University Hospital was devoted to the challenges and opportunities posed by the use of data science to facilitate precision and personalized medicine in nephrology, and to describe new approaches and technologies. The meeting included separate sections for issues in data collection and data analysis. As part of data collection, we presented the institutional ARGOS e-health project, which provides a common model for the standardization of clinical practice. We also pay specific attention to the way in which randomized controlled trials offer data that may be critical to decision-making in the real world. The opportunities of open source software (OSS) for data science in clinical practice were also discussed.              Summary:                    Precision medicine aims to provide the right treatment for the right patients at the right time and is deeply connected to data science. Dialysis patients are highly dependent on technology to live, and their treatment generates a huge volume of data that has to be analysed. Data science has emerged as a tool to provide an integrated approach to data collection, storage, cleaning, processing, analysis, and interpretation from potentially large volumes of information. This is meant to be a perspective article about data science based on the experience of the experts invited to the Science for Dialysis Meeting and provides an up-to-date perspective of the potential of data science in kidney disease and dialysis.              Key messages:                    Healthcare is quickly becoming data-dependent, and data science is a discipline that holds the promise of contributing to the development of personalized medicine, although nephrology still lags behind in this process. The key idea is to ensure that data will guide medical decisions based on individual patient characteristics rather than on averages over a whole population usually based on randomized controlled trials that excluded kidney disease patients. Furthermore, there is increasing interest in obtaining data about the effectiveness of available treatments in current patient care based on pragmatic clinical trials. The use of data science in this context is becoming increasingly feasible in part thanks to the swift developments in OSS.",2020-11-01,0,2359,56,188
1354,33151420,Denouements of machine learning and multimodal diagnostic classification of Alzheimer's disease,"Alzheimer's disease (AD) is the most common type of dementia. The exact cause and treatment of the disease are still unknown. Different neuroimaging modalities, such as magnetic resonance imaging (MRI), positron emission tomography, and single-photon emission computed tomography, have played a significant role in the study of AD. However, the effective diagnosis of AD, as well as mild cognitive impairment (MCI), has recently drawn large attention. Various technological advancements, such as robots, global positioning system technology, sensors, and machine learning (ML) algorithms, have helped improve the diagnostic process of AD. This study aimed to determine the influence of implementing different ML classifiers in MRI and analyze the use of support vector machines with various multimodal scans for classifying patients with AD/MCI and healthy controls. Conclusions have been drawn in terms of employing different classifier techniques and presenting the optimal multimodal paradigm for the classification of AD.",2020-11-01,1,1025,95,188
1362,33139605,Role of Artificial Intelligence in Fighting Antimicrobial Resistance in Pediatrics,"Artificial intelligence (AI) is a field of science and engineering concerned with the computational understanding of what is commonly called intelligent behavior. AI is extremely useful in many human activities including medicine. The aim of our narrative review is to show the potential role of AI in fighting antimicrobial resistance in pediatric patients. We searched for PubMed articles published from April 2010 to April 2020 containing the keywords ""artificial intelligence"", ""machine learning"", ""antimicrobial resistance"", ""antimicrobial stewardship"", ""pediatric"", and ""children"", and we described the different strategies for the application of AI in these fields. Literature analysis showed that the applications of AI in health care are potentially endless, contributing to a reduction in the development time of new antimicrobial agents, greater diagnostic and therapeutic appropriateness, and, simultaneously, a reduction in costs. Most of the proposed AI solutions for medicine are not intended to replace the doctor's opinion or expertise, but to provide a useful tool for easing their work. Considering pediatric infectious diseases, AI could play a primary role in fighting antibiotic resistance. In the pediatric field, a greater willingness to invest in this field could help antimicrobial stewardship reach levels of effectiveness that were unthinkable a few years ago.",2020-11-01,3,1388,82,188
1361,33139614,Machine Learning's Application in Deep Brain Stimulation for Parkinson's Disease: A Review,"Deep brain stimulation (DBS) is a surgical treatment for advanced Parkinson's disease (PD) that has undergone technological evolution that parallels an expansion in clinical phenotyping, neurophysiology, and neuroimaging of the disease state. Machine learning (ML) has been successfully used in a wide range of healthcare problems, including DBS. As computational power increases and more data become available, the application of ML in DBS is expected to grow. We review the literature of ML in DBS and discuss future opportunities for such applications. Specifically, we perform a comprehensive review of the literature from PubMed, the Institute for Scientific Information's Web of Science, Cochrane Database of Systematic Reviews, and Institute of Electrical and Electronics Engineers' (IEEE) Xplore Digital Library for ML applications in DBS. These studies are broadly placed in the following categories: (1) DBS candidate selection; (2) programming optimization; (3) surgical targeting; and (4) insights into DBS mechanisms. For each category, we provide and contextualize the current body of research and discuss potential future directions for the application of ML in DBS.",2020-11-01,0,1181,90,188
1889,33304463,The computational approaches of lncRNA identification based on coding potential: Status quo and challenges,"Long noncoding RNAs (lncRNAs) make up a large proportion of transcriptome in eukaryotes, and have been revealed with many regulatory functions in various biological processes. When studying lncRNAs, the first step is to accurately and specifically distinguish them from the colossal transcriptome data with complicated composition, which contains mRNAs, lncRNAs, small RNAs and their primary transcripts. In the face of such a huge and progressively expanding transcriptome data, the in-silico approaches provide a practicable scheme for effectively and rapidly filtering out lncRNA targets, using machine learning and probability statistics. In this review, we mainly discussed the characteristics of algorithms and features on currently developed approaches. We also outlined the traits of some state-of-the-art tools for ease of operation. Finally, we pointed out the underlying challenges in lncRNA identification with the advent of new experimental data.",2020-11-01,2,959,106,188
1360,33140591,Artificial Intelligence in Health Care: Current Applications and Issues,"In recent years, artificial intelligence (AI) technologies have greatly advanced and become a reality in many areas of our daily lives. In the health care field, numerous efforts are being made to implement the AI technology for practical medical treatments. With the rapid developments in machine learning algorithms and improvements in hardware performances, the AI technology is expected to play an important role in effectively analyzing and utilizing extensive amounts of health and medical data. However, the AI technology has various unique characteristics that are different from the existing health care technologies. Subsequently, there are a number of areas that need to be supplemented within the current health care system for the AI to be utilized more effectively and frequently in health care. In addition, the number of medical practitioners and public that accept AI in the health care is still low; moreover, there are various concerns regarding the safety and reliability of AI technology implementations. Therefore, this paper aims to introduce the current research and application status of AI technology in health care and discuss the issues that need to be resolved.",2020-11-01,0,1190,71,188
1954,32960410,Image-based state-of-the-art techniques for the identification and classification of brain diseases: a review,"Detection and classification methods have a vital and important role in identifying brain diseases. Timely detection and classification of brain diseases enable an accurate identification and effective management of brain impairment. Brain disorders are commonly most spreadable diseases and the diagnosing process is time-consuming and highly expensive. There is an utmost need to develop effective and advantageous methods for brain diseases detection and characterization. Magnetic resonance imaging (MRI), computed tomography (CT), and other various brain imaging scans are used to identify different brain diseases and disorders. Brain imaging scans are the efficient tool to understand the anatomical changes in brain in fast and accurate manner. These different brain imaging scans used with segmentation techniques and along with machine learning and deep learning techniques give maximum accuracy and efficiency. This paper focuses on different conventional approaches, machine learning and deep learning techniques used for the detection, and classification of brain diseases and abnormalities. This paper also summarizes the research gap and problems in the existing techniques used for detection and classification of brain disorders. Comparison and evaluation of different machine learning and deep learning techniques in terms of efficiency and accuracy are also highlighted in this paper. Furthermore, different brain diseases like leukoariaosis, Alzheimer's, Parkinson's, and Wilson's disorder are studied in the scope of machine learning and deep learning techniques.",2020-11-01,1,1584,109,188
1869,32694325,"Spinal Epidural Abscess: Diagnosis, Management, and Outcomes","An infection of the spinal epidural space, spinal epidural abscess (SEA) is a potentially devastating entity that is rising in incidence. Its insidious presentation, variable progression, and potential for precipitous neurologic decline make diagnosis and management of SEA challenging. Prompt diagnosis is key because treatment delay can lead to paralysis or death. Owing to the nonspecific symptoms and signs of SEA, misdiagnosis is alarmingly common. Risk factor assessment to determine the need for definitive MRI reduces diagnostic delays compared with relying on clinical or laboratory findings alone. Although decompression has long been considered the benchmark for SEA, considerable risk associated with spinal surgery is noted in an older cohort with multiple comorbidities. Nonoperative management may represent an alternative in select cases. Failure of nonoperative management is a feared outcome associated with motor deterioration and poor clinical outcomes. Recent studies have identified independent predictors of failure and residual neurologic dysfunction, recurrence, and mortality. Importantly, these studies provide tools that generate probabilities of these outcomes. Future directions of investigation should include external validation of existing algorithms through multi-institutional collaboration, prospective trials, and incorporation of powerful predictive statistics such as machine learning methods.",2020-11-01,0,1432,60,188
1359,33141088,Classification of Depression Through Resting-State Electroencephalogram as a Novel Practice in Psychiatry: Review,"Background:                    Machine learning applications in health care have increased considerably in the recent past, and this review focuses on an important application in psychiatry related to the detection of depression. Since the advent of computational psychiatry, research based on functional magnetic resonance imaging has yielded remarkable results, but these tools tend to be too expensive for everyday clinical use.              Objective:                    This review focuses on an affordable data-driven approach based on electroencephalographic recordings. Web-based applications via public or private cloud-based platforms would be a logical next step. We aim to compare several different approaches to the detection of depression from electroencephalographic recordings using various features and machine learning models.              Methods:                    To detect depression, we reviewed published detection studies based on resting-state electroencephalogram with final machine learning, and to predict therapy outcomes, we reviewed a set of interventional studies using some form of stimulation in their methodology.              Results:                    We reviewed 14 detection studies and 12 interventional studies published between 2008 and 2019. As direct comparison was not possible due to the large diversity of theoretical approaches and methods used, we compared them based on the steps in analysis and accuracies yielded. In addition, we compared possible drawbacks in terms of sample size, feature extraction, feature selection, classification, internal and external validation, and possible unwarranted optimism and reproducibility. In addition, we suggested desirable practices to avoid misinterpretation of results and optimism.              Conclusions:                    This review shows the need for larger data sets and more systematic procedures to improve the use of the solution for clinical diagnostics. Therefore, regulation of the pipeline and standard requirements for methodology used should become mandatory to increase the reliability and accuracy of the complete methodology for it to be translated to modern psychiatry.",2020-11-01,0,2188,113,188
1363,33136182,CT-based radiomics for differentiating renal tumours: a systematic review,"Purpose:                    Differentiating renal tumours into grades and tumour subtype from medical imaging is important for patient management; however, there is an element of subjectivity when performed qualitatively. Quantitative analysis such as radiomics may provide a more objective approach. The purpose of this article is to systematically review the literature on computed tomography (CT) radiomics for grading and differentiating renal tumour subtypes. An educational perspective will also be provided.              Methods:                    The Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist was followed. PubMed, Scopus and Web of Science were searched for relevant articles. The quality of each study was assessed using the Radiomic Quality Score (RQS).              Results:                    13 studies were found. The main outcomes were prediction of pathological grade and differentiating between renal tumour types, measured as area under the curve (AUC) for either the receiver operator curve or precision recall curve. Features extracted to predict pathological grade or tumour subtype included shape, intensity, texture and wavelet (a type of higher order feature). Four studies differentiated between low-grade and high-grade clear cell renal cell cancer (RCC) with good performance (AUC = 0.82-0.978). One other study differentiated low- and high-grade chromophobe with AUC = 0.84. Finally, eight studies used radiomics to differentiate between tumour types such as clear cell RCC, fat-poor angiomyolipoma, papillary RCC, chromophobe RCC and renal oncocytoma with high levels of performance (AUC 0.82-0.96).              Conclusion:                    Renal tumours can be pathologically classified using CT-based radiomics with good performance. The main radiomic feature used for tumour differentiation was texture. Fuhrman was the most common pathologic grading system used in the reviewed studies. Renal tumour grading studies should be extended beyond clear cell RCC and chromophobe RCC. Further research with larger prospective studies, performed in the clinical setting, across multiple institutions would help with clinical translation to the radiologist's workstation.",2020-11-01,2,2235,73,188
1890,33304450,Homology modeling in the time of collective and artificial intelligence,"Homology modeling is a method for building protein 3D structures using protein primary sequence and utilizing prior knowledge gained from structural similarities with other proteins. The homology modeling process is done in sequential steps where sequence/structure alignment is optimized, then a backbone is built and later, side-chains are added. Once the low-homology loops are modeled, the whole 3D structure is optimized and validated. In the past three decades, a few collective and collaborative initiatives allowed for continuous progress in both homology and ab initio modeling. Critical Assessment of protein Structure Prediction (CASP) is a worldwide community experiment that has historically recorded the progress in this field. Folding@Home and Rosetta@Home are examples of crowd-sourcing initiatives where the community is sharing computational resources, whereas RosettaCommons is an example of an initiative where a community is sharing a codebase for the development of computational algorithms. Foldit is another initiative where participants compete with each other in a protein folding video game to predict 3D structure. In the past few years, contact maps deep machine learning was introduced to the 3D structure prediction process, adding more information and increasing the accuracy of models significantly. In this review, we will take the reader in a journey of exploration from the beginnings to the most recent turnabouts, which have revolutionized the field of homology modeling. Moreover, we discuss the new trends emerging in this rapidly growing field.",2020-11-01,1,1585,71,188
1226,32109428,Imaging research in fibrotic lung disease; applying deep learning to unsolved problems,"Over the past decade, there has been a groundswell of research interest in computer-based methods for objectively quantifying fibrotic lung disease on high resolution CT of the chest. In the past 5 years, the arrival of deep learning-based image analysis has created exciting new opportunities for enhancing the understanding of, and the ability to interpret, fibrotic lung disease on CT. Specific unsolved problems for which computer-based imaging analysis might provide solutions include the development of reliable methods for assisting with diagnosis, detecting early disease, and predicting disease behaviour using baseline imaging data. However, to harness this technology, technical and societal challenges must be overcome. Large CT datasets will be needed to power the training of deep learning algorithms. Open science research and collaboration between academia and industry must be encouraged. Prospective clinical utility studies will be needed to test computer algorithm performance in real-world clinical settings and demonstrate patient benefit over current best practice. Finally, ethical standards, which ensure patient confidentiality and mitigate against biases in training datasets, that can be encoded in machine-learning systems will be needed as well as bespoke data governance and accountability frameworks to encourage buy-in from health-care professionals, patients, and the public.",2020-11-01,6,1409,86,188
1991,32905873,"Clinical applications of machine learning in the diagnosis, classification, and prediction of heart failure","Machine learning and artificial intelligence are generating significant attention in the scientific community and media. Such algorithms have great potential in medicine for personalizing and improving patient care, including in the diagnosis and management of heart failure. Many physicians are familiar with these terms and the excitement surrounding them, but many are unfamiliar with the basics of these algorithms and how they are applied to medicine. Within heart failure research, current applications of machine learning include creating new approaches to diagnosis, classifying patients into novel phenotypic groups, and improving prediction capabilities. In this paper, we provide an overview of machine learning targeted for the practicing clinician and evaluate current applications of machine learning in the diagnosis, classification, and prediction of heart failure.",2020-11-01,1,881,107,188
1953,32961306,Dementia medical screening using mobile applications: A systematic review with a new mapping model,"Early detection is the key to successfully tackling dementia, a neurocognitive condition common among the elderly. Therefore, screening using technological platforms such as mobile applications (apps) may provide an important opportunity to speed up the diagnosis process and improve accessibility. Due to the lack of research into dementia diagnosis and screening tools based on mobile apps, this systematic review aims to identify the available mobile-based dementia and mild cognitive impairment (MCI) apps using specific inclusion and exclusion criteria. More importantly, we critically analyse these tools in terms of their comprehensiveness, validity, performance, and the use of artificial intelligence (AI) techniques. The research findings suggest diagnosticians in a clinical setting use dementia screening apps such as ALZ and CognitiveExams since they cover most of the domains for the diagnosis of neurocognitive disorders. Further, apps such as Cognity and ACE-Mobile have great potential as they use machine learning (ML) and AI techniques, thus improving the accuracy of the outcome and the efficiency of the screening process. Lastly, there was overlapping among the dementia screening apps in terms of activities and questions they contain therefore mapping these apps to the designated cognitive domains is a challenging task, which has been done in this research.",2020-11-01,1,1383,98,188
1891,33303419,Applications of deep learning in dentistry,"Over the last few years, translational applications of so-called artificial intelligence in the field of medicine have garnered a significant amount of interest. The present article aims to review existing dental literature that has examined deep learning, a subset of machine learning that has demonstrated the highest performance when applied to image processing and that has been tested as a formidable diagnostic support tool through its automated analysis of radiographic/photographic images. Furthermore, the article will critically evaluate the literature to describe potential methodological weaknesses of the studies and the need for further development. This review includes 28 studies that have described the applications of deep learning in various fields of dentistry. Research into the applications of deep learning in dentistry contains claims of its high accuracy. Nonetheless, many of these studies have substantial limitations and methodological issues (e.g., examiner reliability, the number of images used for training/testing, the methods used for validation) that have significantly limited the external validity of their results. Therefore, future studies that acknowledge the methodological limitations of existing literature will help to establish a better understanding of the usefulness of applying deep learning in dentistry.",2020-11-01,0,1353,42,188
2096,33343782,Machine learning-enabled multiplexed microfluidic sensors,"High-throughput, cost-effective, and portable devices can enhance the performance of point-of-care tests. Such devices are able to acquire images from samples at a high rate in combination with microfluidic chips in point-of-care applications. However, interpreting and analyzing the large amount of acquired data is not only a labor-intensive and time-consuming process, but also prone to the bias of the user and low accuracy. Integrating machine learning (ML) with the image acquisition capability of smartphones as well as increasing computing power could address the need for high-throughput, accurate, and automatized detection, data processing, and quantification of results. Here, ML-supported diagnostic technologies are presented. These technologies include quantification of colorimetric tests, classification of biological samples (cells and sperms), soft sensors, assay type detection, and recognition of the fluid properties. Challenges regarding the implementation of ML methods, including the required number of data points, image acquisition prerequisites, and execution of data-limited experiments are also discussed.",2020-12-01,1,1135,57,158
2090,33371386,The Expanding Computational Toolbox for Engineering Microbial Phenotypes at the Genome Scale,"Microbial strains are being engineered for an increasingly diverse array of applications, from chemical production to human health. While traditional engineering disciplines are driven by predictive design tools, these tools have been difficult to build for biological design due to the complexity of biological systems and many unknowns of their quantitative behavior. However, due to many recent advances, the gap between design in biology and other engineering fields is closing. In this work, we discuss promising areas of development of computational tools for engineering microbial strains. We define five frontiers of active research: (1) Constraint-based modeling and metabolic network reconstruction, (2) Kinetics and thermodynamic modeling, (3) Protein structure analysis, (4) Genome sequence analysis, and (5) Regulatory network analysis. Experimental and machine learning drivers have enabled these methods to improve by leaps and bounds in both scope and accuracy. Modern strain design projects will require these tools to be comprehensively applied to the entire cell and efficiently integrated within a single workflow. We expect that these frontiers, enabled by the ongoing revolution of big data science, will drive forward more advanced and powerful strain engineering strategies.",2020-12-01,0,1298,92,158
1903,33291301,Diagnostic Utility of Genome-Wide DNA Methylation Analysis in Mendelian Neurodevelopmental Disorders,"Mendelian neurodevelopmental disorders customarily present with complex and overlapping symptoms, complicating the clinical diagnosis. Individuals with a growing number of the so-called rare disorders exhibit unique, disorder-specific DNA methylation patterns, consequent to the underlying gene defects. Besides providing insights to the pathophysiology and molecular biology of these disorders, we can use these epigenetic patterns as functional biomarkers for the screening and diagnosis of these conditions. This review summarizes our current understanding of DNA methylation episignatures in rare disorders and describes the underlying technology and analytical approaches. We discuss the computational parameters, including statistical and machine learning methods, used for the screening and classification of genetic variants of uncertain clinical significance. Describing the rationale and principles applied to the specific computational models that are used to develop and adapt the DNA methylation episignatures for the diagnosis of rare disorders, we highlight the opportunities and challenges in this emerging branch of diagnostic medicine.",2020-12-01,1,1153,100,158
2059,33485296,Machine learning as the new approach in understanding biomarkers of suicidal behavior,"In psychiatry, compared to other medical fields, the identification of biological markers that would complement current clinical interview, and enable more objective and faster clinical diagnosis, implement accurate monitoring of treatment response and remission, is grave. Current technological development enables analyses of various biological marks in high throughput scale at reasonable costs, and therefore 'omic' studies are entering the psychiatry research. However, big data demands a whole new plethora of skills in data processing, before clinically useful information can be extracted. So far the classical approach to data analysis did not really contribute to identification of biomarkers in psychiatry, but the extensive amounts of data might get to a higher level, if artificial intelligence in the shape of machine learning algorithms would be applied. Not many studies on machine learning in psychiatry have been published, but we can already see from that handful of studies that the potential to build a screening portfolio of biomarkers for different psychopathologies, including suicide, exists.",2020-12-01,0,1117,85,158
1905,33290527,Deep learning and generative methods in cheminformatics and chemical biology: navigating small molecule space intelligently,"The number of 'small' molecules that may be of interest to chemical biologists - chemical space - is enormous, but the fraction that have ever been made is tiny. Most strategies are discriminative, i.e. have involved 'forward' problems (have molecule, establish properties). However, we normally wish to solve the much harder generative or inverse problem (describe desired properties, find molecule). 'Deep' (machine) learning based on large-scale neural networks underpins technologies such as computer vision, natural language processing, driverless cars, and world-leading performance in games such as Go; it can also be applied to the solution of inverse problems in chemical biology. In particular, recent developments in deep learning admit the in silico generation of candidate molecular structures and the prediction of their properties, thereby allowing one to navigate (bio)chemical space intelligently. These methods are revolutionary but require an understanding of both (bio)chemistry and computer science to be exploited to best advantage. We give a high-level (non-mathematical) background to the deep learning revolution, and set out the crucial issue for chemical biology and informatics as a two-way mapping from the discrete nature of individual molecules to the continuous but high-dimensional latent representation that may best reflect chemical space. A variety of architectures can do this; we focus on a particular type known as variational autoencoders. We then provide some examples of recent successes of these kinds of approach, and a look towards the future.",2020-12-01,1,1588,123,158
2091,33364579,Machine learning in plant science and plant breeding,"Technological developments have revolutionized measurements on plant genotypes and phenotypes, leading to routine production of large, complex data sets. This has led to increased efforts to extract meaning from these measurements and to integrate various data sets. Concurrently, machine learning has rapidly evolved and is now widely applied in science in general and in plant genotyping and phenotyping in particular. Here, we review the application of machine learning in the context of plant science and plant breeding. We focus on analyses at different phenotype levels, from biochemical to yield, and in connecting genotypes to these. In this way, we illustrate how machine learning offers a suite of methods that enable researchers to find meaningful patterns in relevant plant data.",2020-12-01,1,791,52,158
2058,33490920,Reaction prediction via atomistic simulation: from quantum mechanics to machine learning,"It is an ultimate goal in chemistry to predict reaction without recourse to experiment. Reaction prediction is not just the reaction rate determination of known reactions but, more broadly, the reaction exploration to identify new reaction routes. This review briefly overviews the theory on chemical reaction and the current methods for computing/estimating reaction rate and exploring reaction space. We particularly focus on the atomistic simulation methods for reaction exploration, which are benefited significantly by recently emerged machine learning potentials. We elaborate the stochastic surface walking global pathway sampling based on the global neural network (SSW-NN) potential, developed in our group since 2013, which can explore complex reactions systems unbiasedly and automatedly. Two examples, molecular reaction and heterogeneous catalytic reactions, are presented to illustrate the current status for reaction prediction using SSW-NN.",2020-12-01,0,956,88,158
2057,33500754,Artificial Intelligence and Machine Learning in Cardiovascular Imaging,"Cardiovascular disease is the leading cause of mortality in Western countries and leads to a spectrum of complications that can complicate patient management. The emergence of artificial intelligence (AI) has garnered significant interest in many industries, and the field of cardiovascular imaging is no exception. Machine learning (ML) especially is showing significant promise in various diagnostic imaging modalities. As conventional statistics are reaching their apex in computational capabilities, ML can explore new possibilities and unravel hidden relationships. This will have a positive impact on diagnosis and prognosis for cardiovascular imaging. In this in-depth review, we highlight the role of AI and ML for various cardiovascular imaging modalities.",2020-12-01,0,765,70,158
2092,33362867,State of the Field in Multi-Omics Research: From Computational Needs to Data Mining and Sharing,"Multi-omics, variously called integrated omics, pan-omics, and trans-omics, aims to combine two or more omics data sets to aid in data analysis, visualization and interpretation to determine the mechanism of a biological process. Multi-omics efforts have taken center stage in biomedical research leading to the development of new insights into biological events and processes. However, the mushrooming of a myriad of tools, datasets, and approaches tends to inundate the literature and overwhelm researchers new to the field. The aims of this review are to provide an overview of the current state of the field, inform on available reliable resources, discuss the application of statistics and machine/deep learning in multi-omics analyses, discuss findable, accessible, interoperable, reusable (FAIR) research, and point to best practices in benchmarking. Thus, we provide guidance to interested users of the domain by addressing challenges of the underlying biology, giving an overview of the available toolset, addressing common pitfalls, and acknowledging current methods' limitations. We conclude with practical advice and recommendations on software engineering and reproducibility practices to share a comprehensive awareness with new researchers in multi-omics for end-to-end workflow.",2020-12-01,3,1294,95,158
2055,33501307,Emotion Recognition for Human-Robot Interaction: Recent Advances and Future Perspectives,"A fascinating challenge in the field of human-robot interaction is the possibility to endow robots with emotional intelligence in order to make the interaction more intuitive, genuine, and natural. To achieve this, a critical point is the capability of the robot to infer and interpret human emotions. Emotion recognition has been widely explored in the broader fields of human-machine interaction and affective computing. Here, we report recent advances in emotion recognition, with particular regard to the human-robot interaction context. Our aim is to review the state of the art of currently adopted emotional models, interaction modalities, and classification strategies and offer our point of view on future developments and critical issues. We focus on facial expressions, body poses and kinematics, voice, brain activity, and peripheral physiological responses, also providing a list of available datasets containing data from these modalities.",2020-12-01,0,953,88,158
1369,33126123,Towards a comprehensive pipeline to identify and functionally annotate long noncoding RNA (lncRNA),"Long noncoding RNAs (lncRNAs) are implicated in various genetic diseases and cancer, attributed to their critical role in gene regulation. They are a divergent group of RNAs and are easily differentiated from other types with unique characteristics, functions, and mechanisms of action. In this review, we provide a list of some of the prominent data repositories containing lncRNAs, their interactome, and predicted and validated disease associations. Next, we discuss various wet-lab experiments formulated to obtain the data for these repositories. We also provide a critical review of in silico methods available for the identification purpose and suggest techniques to further improve their performance. The bulk of the methods currently focus on distinguishing lncRNA transcripts from the coding ones. Functional annotation of these transcripts still remains a grey area and more efforts are needed in that space. Finally, we provide details of current progress, discuss impediments, and illustrate a roadmap for developing a generalized computational pipeline for comprehensive annotation of lncRNAs, which is essential to accelerate research in this area.",2020-12-01,2,1163,98,158
2054,33501338,A Brief Survey of Telerobotic Time Delay Mitigation,"There is a substantial number of telerobotics and teleoperation applications ranging from space operations, ground/aerial robotics, drive-by-wire systems to medical interventions. Major obstacles for such applications include latency, channel corruptions, and bandwidth which limit teleoperation efficacy. This survey reviews the time delay problem in teleoperation systems. We briefly review different solutions from early approaches which consist of control-theory-based models and user interface designs and focus on newer approaches developed since 2014. Future solutions to the time delay problem will likely be hybrid solutions which include modeling of user intent, prediction of robot movements, and time delay prediction all potentially using time series prediction methods. Hence, we examine methods that are primarily based on time series prediction. Recent prediction approaches take advantage of advances in nonlinear statistical models as well as machine learning and neural network techniques. We review Recurrent Neural Networks, Long Short-Term Memory, Sequence to Sequence, and Generative Adversarial Network models and examine each of these approaches for addressing time delay. As time delay is still an unsolved problem, we suggest some possible future research directions from information-theory-based modeling, which may lead to promising new approaches to advancing the field.",2020-12-01,0,1400,51,158
2052,33623737,Artificial Intelligence in COVID-19 Ultrastructure,"Artificial intelligence has found its way into numerous fields of medicine in the past decade, spurred by the availability of big data and powerful processors. For the COVID-19 pandemic, aside from predicting its onset, artificial intelligence has been used to track disease spread, detect pulmonary involvement in computed tomography scans, risk-stratify patients, and model virtual protein structure and potential therapeutic agents. This mini-review briefly discusses the potential applications of artificial intelligence in COVID-19 microscopy.",2020-12-01,0,548,50,158
2093,33362861,"Machine Learning Based Computational Gene Selection Models: A Survey, Performance Evaluation, Open Issues, and Future Research Directions","Gene Expression is the process of determining the physical characteristics of living beings by generating the necessary proteins. Gene Expression takes place in two steps, translation and transcription. It is the flow of information from DNA to RNA with enzymes' help, and the end product is proteins and other biochemical molecules. Many technologies can capture Gene Expression from the DNA or RNA. One such technique is Microarray DNA. Other than being expensive, the main issue with Microarray DNA is that it generates high-dimensional data with minimal sample size. The issue in handling such a heavyweight dataset is that the learning model will be over-fitted. This problem should be addressed by reducing the dimension of the data source to a considerable amount. In recent years, Machine Learning has gained popularity in the field of genomic studies. In the literature, many Machine Learning-based Gene Selection approaches have been discussed, which were proposed to improve dimensionality reduction precision. This paper does an extensive review of the various works done on Machine Learning-based gene selection in recent years, along with its performance analysis. The study categorizes various feature selection algorithms under Supervised, Unsupervised, and Semi-supervised learning. The works done in recent years to reduce the features for diagnosing tumors are discussed in detail. Furthermore, the performance of several discussed methods in the literature is analyzed. This study also lists out and briefly discusses the open issues in handling the high-dimension and less sample size data.",2020-12-01,0,1611,137,158
1904,33291266,The Utility of Deep Learning in Breast Ultrasonic Imaging: A Review,"Breast cancer is the most frequently diagnosed cancer in women; it poses a serious threat to women's health. Thus, early detection and proper treatment can improve patient prognosis. Breast ultrasound is one of the most commonly used modalities for diagnosing and detecting breast cancer in clinical practice. Deep learning technology has made significant progress in data extraction and analysis for medical images in recent years. Therefore, the use of deep learning for breast ultrasonic imaging in clinical practice is extremely important, as it saves time, reduces radiologist fatigue, and compensates for a lack of experience and skills in some cases. This review article discusses the basic technical knowledge and algorithms of deep learning for breast ultrasound and the application of deep learning technology in image classification, object detection, segmentation, and image synthesis. Finally, we discuss the current issues and future perspectives of deep learning technology in breast ultrasound.",2020-12-01,0,1010,67,158
2097,33340123,Promises and perils of artificial intelligence in dentistry,"Artificial intelligence (AI) is a subdiscipline of computer science that has made substantial progress in medicine and there is a growing body of AI research in dentistry. Dentists should have an understanding of the foundational concepts and the ability to critically evaluate dental research in AI. Machine learning (ML) is a subfield of AI that most dental AI research is dedicated to. The most prolific area of ML research is automated interpretation of dental imaging. Other areas include providing treatment recommendations, predicting future disease and treatment outcomes. The research impact is limited by small datasets that do not harness the positive correlation between very large datasets and ML performance. There is also a need to standardize research methodologies and utilize performance metrics that are appropriate for the clinical context. In addition to research challenges, this article discusses the ethical, legal and logistical considerations associated with implementation in clinical practice. This includes explainable AI, model bias, data privacy and security. The future implications of AI in dentistry involve a promise for a novel form of practicing dentistry however, the effect of AI on patient outcomes is yet to be determined.",2020-12-01,0,1263,59,158
2094,33362384,Evolving role of artificial intelligence in gastrointestinal endoscopy,"Artificial intelligence (AI) is a combination of different technologies that enable machines to sense, comprehend, and learn with human-like levels of intelligence. AI technology will eventually enhance human capability, provide machines genuine autonomy, and reduce errors, and increase productivity and efficiency. AI seems promising, and the field is full of invention, novel applications; however, the limitation of machine learning suggests a cautious optimism as the right strategy. AI is also becoming incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy for optimal patient care. AI using deep learning technology has been used to identify, differentiate catalog images in several medical fields including gastrointestinal endoscopy. The gastrointestinal endoscopy field involves endoscopic diagnoses and prognostication of various digestive diseases using image analysis with the help of various gastrointestinal endoscopic device systems. AI-based endoscopic systems can reliably detect and provide crucial information on gastrointestinal pathology based on their training and validation. These systems can make gastroenterology practice easier, faster, more reliable, and reduce inter-observer variability in the coming years. However, the thought that these systems will replace human decision making replace gastrointestinal endoscopists does not seem plausible in the near future. In this review, we discuss AI and associated various technological terminologies, evolving role in gastrointestinal endoscopy, and future possibilities.",2020-12-01,0,1601,70,158
1910,33270183,Use of Machine Learning Approaches in Clinical Epidemiological Research of Diabetes,"Purpose of review:                    Machine learning approaches-which seek to predict outcomes or classify patient features by recognizing patterns in large datasets-are increasingly applied to clinical epidemiology research on diabetes. Given its novelty and emergence in fields outside of biomedical research, machine learning terminology, techniques, and research findings may be unfamiliar to diabetes researchers. Our aim was to present the use of machine learning approaches in an approachable way, drawing from clinical epidemiological research in diabetes published from 1 Jan 2017 to 1 June 2020.              Recent findings:                    Machine learning approaches using tree-based learners-which produce decision trees to help guide clinical interventions-frequently have higher sensitivity and specificity than traditional regression models for risk prediction. Machine learning approaches using neural networking and ""deep learning"" can be applied to medical image data, particularly for the identification and staging of diabetic retinopathy and skin ulcers. Among the machine learning approaches reviewed, researchers identified new strategies to develop standard datasets for rigorous comparisons across older and newer approaches, methods to illustrate how a machine learner was treating underlying data, and approaches to improve the transparency of the machine learning process. Machine learning approaches have the potential to improve risk stratification and outcome prediction for clinical epidemiology applications. Achieving this potential would be facilitated by use of universal open-source datasets for fair comparisons. More work remains in the application of strategies to communicate how the machine learners are generating their predictions.",2020-12-01,0,1782,83,158
1765,31553509,Autonomous Discovery in the Chemical Sciences Part II: Outlook,"This two-part Review examines how automation has contributed to different aspects of discovery in the chemical sciences. In this second part, we reflect on a selection of exemplary studies. It is increasingly important to articulate what the role of automation and computation has been in the scientific process and how that has or has not accelerated discovery. One can argue that even the best automated systems have yet to ""discover"" despite being incredibly useful as laboratory assistants. We must carefully consider how they have been and can be applied to future problems of chemical discovery in order to effectively design and interact with future autonomous platforms. The majority of this Review defines a large set of open research directions, including improving our ability to work with complex data, build empirical models, automate both physical and computational experiments for validation, select experiments, and evaluate whether we are making progress towards the ultimate goal of autonomous discovery. Addressing these practical and methodological challenges will greatly advance the extent to which autonomous systems can make meaningful discoveries.",2020-12-01,7,1172,62,158
2112,31797321,Hybrid PET/MR imaging in myocardial inflammation post-myocardial infarction,"Hybrid PET/MR imaging is an emerging imaging modality combining positron emission tomography (PET) and magnetic resonance imaging (MRI) in the same system. Since the introduction of clinical PET/MRI in 2011, it has had some impact (e.g., imaging the components of inflammation in myocardial infarction), but its role could be much greater. Many opportunities remain unexplored and will be highlighted in this review. The inflammatory process post-myocardial infarction has many facets at a cellular level which may affect the outcome of the patient, specifically the effects on adverse left ventricular remodeling, and ultimately prognosis. The goal of inflammation imaging is to track the process non-invasively and quantitatively to determine the best therapeutic options for intervention and to monitor those therapies. While PET and MRI, acquired separately, can image aspects of inflammation, hybrid PET/MRI has the potential to advance imaging of myocardial inflammation. This review contains a description of hybrid PET/MRI, its application to inflammation imaging in myocardial infarction and the challenges, constraints, and opportunities in designing data collection protocols. Finally, this review explores opportunities in PET/MRI: improved registration, partial volume correction, machine learning, new approaches in the development of PET and MRI pulse sequences, and the use of novel injection strategies.",2020-12-01,5,1420,75,158
2051,33627972,Future of Health Services: The Role of Physicians in the Disruptive Era,"This article aimed to address the role of physicians in future health in the disruptive era. Physicians in this disruptive era must increase their capability and knowledge to compensate for this development. Advances in technology increase the impact on health care and the significance of disruption. Disruptive innovation encompasses several fields, such as physics, digital, and biology. Big data as one of the most important parts in clinical aspects encompass high-throughput cellular and protein-binding assays toward chemoinformatic-driven databases. Health status can be modified by changing epigenetic factor, such as lifestyle and environment. As a result, they affect human genetics and provide the insight of pathophysiology of disease, clinical treatment, and early preventive action. Disruptive innovations in health-care align with the development of artificial intelligence, machine learning, robotics, Internet of things, digitalization, and genomics. New paradigm shifting in physician-patient relationships is relevant to consumer health informatics.",2020-12-01,0,1069,71,158
1908,33284779,Artificial Intelligence in the Fight Against COVID-19: Scoping Review,"Background:                    In December 2019, COVID-19 broke out in Wuhan, China, leading to national and international disruptions in health care, business, education, transportation, and nearly every aspect of our daily lives. Artificial intelligence (AI) has been leveraged amid the COVID-19 pandemic; however, little is known about its use for supporting public health efforts.              Objective:                    This scoping review aims to explore how AI technology is being used during the COVID-19 pandemic, as reported in the literature. Thus, it is the first review that describes and summarizes features of the identified AI techniques and data sets used for their development and validation.              Methods:                    A scoping review was conducted following the guidelines of PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews). We searched the most commonly used electronic databases (eg, MEDLINE, EMBASE, and PsycInfo) between April 10 and 12, 2020. These terms were selected based on the target intervention (ie, AI) and the target disease (ie, COVID-19). Two reviewers independently conducted study selection and data extraction. A narrative approach was used to synthesize the extracted data.              Results:                    We considered 82 studies out of the 435 retrieved studies. The most common use of AI was diagnosing COVID-19 cases based on various indicators. AI was also employed in drug and vaccine discovery or repurposing and for assessing their safety. Further, the included studies used AI for forecasting the epidemic development of COVID-19 and predicting its potential hosts and reservoirs. Researchers used AI for patient outcome-related tasks such as assessing the severity of COVID-19, predicting mortality risk, its associated factors, and the length of hospital stay. AI was used for infodemiology to raise awareness to use water, sanitation, and hygiene. The most prominent AI technique used was convolutional neural network, followed by support vector machine.              Conclusions:                    The included studies showed that AI has the potential to fight against COVID-19. However, many of the proposed methods are not yet clinically accepted. Thus, the most rewarding research will be on methods promising value beyond COVID-19. More efforts are needed for developing standardized reporting protocols or guidelines for studies on AI.",2020-12-01,1,2483,69,158
2082,33375710,Privacy-Preserving Sensor-Based Continuous Authentication and User Profiling: A Review,"Ensuring the confidentiality of private data stored in our technological devices is a fundamental aspect for protecting our personal and professional information. Authentication procedures are among the main methods used to achieve this protection and, typically, are implemented only when accessing the device. Nevertheless, in many occasions it is necessary to carry out user authentication in a continuous manner to guarantee an allowed use of the device while protecting authentication data. In this work, we first review the state of the art of Continuous Authentication (CA), User Profiling (UP), and related biometric databases. Secondly, we summarize the privacy-preserving methods employed to protect the security of sensor-based data used to conduct user authentication, and some practical examples of their utilization. The analysis of the literature of these topics reveals the importance of sensor-based data to protect personal and professional information, as well as the need for exploring a combination of more biometric features with privacy-preserving approaches.",2020-12-01,0,1082,86,158
2083,33375658,Explainable AI: A Review of Machine Learning Interpretability Methods,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ""black box"" approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.",2020-12-01,1,1184,69,158
2080,33381570,Artificial Intelligence in Coronary Computed Tomography Angiography: From Anatomy to Prognosis,"Cardiac computed tomography angiography (CCTA) is widely used as a diagnostic tool for evaluation of coronary artery disease (CAD). Despite the excellent capability to rule-out CAD, CCTA may overestimate the degree of stenosis; furthermore, CCTA analysis can be time consuming, often requiring advanced postprocessing techniques. In consideration of the most recent ESC guidelines on CAD management, which will likely increase CCTA volume over the next years, new tools are necessary to shorten reporting time and improve the accuracy for the detection of ischemia-inducing coronary lesions. The application of artificial intelligence (AI) may provide a helpful tool in CCTA, improving the evaluation and quantification of coronary stenosis, plaque characterization, and assessment of myocardial ischemia. Furthermore, in comparison with existing risk scores, machine-learning algorithms can better predict the outcome utilizing both imaging findings and clinical parameters. Medical AI is moving from the research field to daily clinical practice, and with the increasing number of CCTA examinations, AI will be extensively utilized in cardiac imaging. This review is aimed at illustrating the state of the art in AI-based CCTA applications and future clinical scenarios.",2020-12-01,1,1272,94,158
2079,33383297,Omics-based strategies to discover novel classes of RiPP natural products,"Ribosomally synthesized and post-translationally modified peptides (RiPPs) form a highly diverse class of natural products, with various biotechnologically and clinically relevant activities. A recent increase in discoveries of novel RiPP classes suggests that currently known RiPPs constitute just the tip of the iceberg. Genome mining has been a driving force behind these discoveries, but remains challenging due to a lack of universal genetic markers for RiPP detection. In this review, we discuss how various genome mining methodologies contribute towards the discovery of novel RiPP classes. Some methods prioritize novel biosynthetic gene clusters (BGCs) based on shared modifications between RiPP classes. Other methods identify RiPP precursors using machine-learning classifiers. The integration of such methods as well as integration with other types of omics data in more comprehensive pipelines could help these tools reach their potential, and keep pushing the boundaries of the chemical diversity of this important class of molecules.",2020-12-01,0,1048,73,158
2078,33383831,Active and Passive Electro-Optical Sensors for Health Assessment in Food Crops,"In agriculture, early detection of plant stresses is advantageous in preventing crop yield losses. Remote sensors are increasingly being utilized for crop health monitoring, offering non-destructive, spatialized detection and the quantification of plant diseases at various levels of measurement. Advances in sensor technologies have promoted the development of novel techniques for precision agriculture. As in situ techniques are surpassed by multispectral imaging, refinement of hyperspectral imaging and the promising emergence of light detection and ranging (LIDAR), remote sensing will define the future of biotic and abiotic plant stress detection, crop yield estimation and product quality. The added value of LIDAR-based systems stems from their greater flexibility in capturing data, high rate of data delivery and suitability for a high level of automation while overcoming the shortcomings of passive systems limited by atmospheric conditions, changes in light, viewing angle and canopy structure. In particular, a multi-sensor systems approach and associated data fusion techniques (i.e., blending LIDAR with existing electro-optical sensors) offer increased accuracy in plant disease detection by focusing on traditional optimal estimation and the adoption of artificial intelligence techniques for spatially and temporally distributed big data. When applied across different platforms (handheld, ground-based, airborne, ground/aerial robotic vehicles or satellites), these electro-optical sensors offer new avenues to predict and react to plant stress and disease. This review examines the key sensor characteristics, platform integration options and data analysis techniques recently proposed in the field of precision agriculture and highlights the key challenges and benefits of each concept towards informing future research in this very important and rapidly growing field.",2020-12-01,0,1893,78,158
24,32378163,Big Data and Atrial Fibrillation: Current Understanding and New Opportunities,"Atrial fibrillation (AF) is the most common arrhythmia with diverse etiology that remarkably relates to high morbidity and mortality. With the advancements in intensive clinical and basic research, the understanding of electrophysiological and pathophysiological mechanism, as well as treatment of AF have made huge progress. However, many unresolved issues remain, including the core mechanisms and key intervention targets. Big data approach has produced new insights into the improvement of the situation. A large amount of data have been accumulated in the field of AF research, thus using the big data to achieve prevention and precise treatment of AF may be the direction of future development. In this review, we will discuss the current understanding of big data and explore the potential applications of big data in AF research, including predictive models of disease processes, disease heterogeneity, drug safety and development, precision medicine, and the potential source for big data acquisition. Grapical abstract.",2020-12-01,0,1029,77,158
2077,33383867,"Public Health Impact of Using Biosimilars, Is Automated Follow up Relevant?","Biologic reference drugs and their copies, biosimilars, have a complex structure. Biosimilars need to demonstrate their biosimilarity during development but unpredictable variations can remain, such as micro-heterogeneity. The healthcare community may raise questions regarding the clinical outcomes induced by this micro-heterogeneity. Indeed, unwanted immune reactions may be induced for numerous reasons, including product variations. However, it is challenging to assess these unwanted immune reactions because of the multiplicity of causes and potential delays before any reaction. Moreover, safety assessments as part of preclinical studies and clinical trials may be of limited value with respect to immunogenicity assessments because they are performed on a standardised population during a limited period. Real-life data could therefore supplement the assessments of clinical trials by including data on the real-life use of biosimilars, such as switches. Furthermore, real-life data also include any economic incentives to prescribe or use biosimilars. This article raises the question of relevance of automating real life data processing regarding Biosimilars. The objective is to initiate a discussion about different approaches involving Machine Learning. So, the discussion is established regarding implementation of Neural Network model to ensure safety of biosimilars subject to economic incentives. Nevertheless, the application of Machine Learning in the healthcare field raises ethical, legal and technical issues that require further discussion.",2020-12-01,0,1565,75,158
2084,33375609,Learning-Based Methods of Perception and Navigation for Ground Vehicles in Unstructured Environments: A Review,"The problem of autonomous navigation of a ground vehicle in unstructured environments is both challenging and crucial for the deployment of this type of vehicle in real-world applications. Several well-established communities in robotics research deal with these scenarios such as search and rescue robotics, planetary exploration, and agricultural robotics. Perception plays a crucial role in this context, since it provides the necessary information to make the vehicle aware of its own status and its surrounding environment. We present a review on the recent contributions in the robotics literature adopting learning-based methods to solve the problem of environment perception and interpretation with the final aim of the autonomous context-aware navigation of ground vehicles in unstructured environments. To the best of our knowledge, this is the first work providing such a review in this context.",2020-12-01,0,906,110,158
2076,33392139,Internet of Things and Artificial Intelligence in Healthcare During COVID-19 Pandemic-A South American Perspective,"The shudders of the COVID-19 pandemic have projected newer challenges in the healthcare domain across the world. In South American scenario, severe issues and difficulties have been noticed in areas like patient consultations, remote monitoring, medical resources, healthcare personnel etc. This work is aimed at providing a holistic view to the digital healthcare during the times of COVID-19 pandemic in South America. It includes different initiatives like mobile apps, web-platforms and intelligent analyses toward early detection and overall healthcare management. In addition to discussing briefly the key issues toward extensive implementation of eHealth paradigms, this work also sheds light on some key aspects of Artificial Intelligence and the Internet of Things along their potential applications like clinical decision support systems and predictive risk modeling, especially in the direction of combating the emergent challenges due to the COVID-19 pandemic.",2020-12-01,0,972,114,158
2075,33396434,Artificial Neural Network Algorithms for 3D Printing,"Additive manufacturing with an emphasis on 3D printing has recently become popular due to its exceptional advantages over conventional manufacturing processes. However, 3D printing process parameters are challenging to optimize, as they influence the properties and usage time of printed parts. Therefore, it is a complex task to develop a correlation between process parameters and printed parts' properties via traditional optimization methods. A machine-learning technique was recently validated to carry out intricate pattern identification and develop a deterministic relationship, eliminating the need to develop and solve physical models. In machine learning, artificial neural network (ANN) is the most widely utilized model, owing to its capability to solve large datasets and strong computational supremacy. This study compiles the advancement of ANN in several aspects of 3D printing. Challenges while applying ANN in 3D printing and their potential solutions are indicated. Finally, upcoming trends for the application of ANN in 3D printing are projected.",2020-12-01,0,1067,52,158
2074,33396740,Structural Aspects and Prediction of Calmodulin-Binding Proteins,"Calmodulin (CaM) is an important intracellular protein that binds Ca2+ and functions as a critical second messenger involved in numerous biological activities through extensive interactions with proteins and peptides. CaM's ability to adapt to binding targets with different structures is related to the flexible central helix separating the N- and C-terminal lobes, which allows for conformational changes between extended and collapsed forms of the protein. CaM-binding targets are most often identified using prediction algorithms that utilize sequence and structural data to predict regions of peptides and proteins that can interact with CaM. In this review, we provide an overview of different CaM-binding proteins, the motifs through which they interact with CaM, and shared properties that make them good binding partners for CaM. Additionally, we discuss the historical and current methods for predicting CaM binding, and the similarities and differences between these methods and their relative success at prediction. As new CaM-binding proteins are identified and classified, we will gain a broader understanding of the biological processes regulated through changes in Ca2+ concentration through interactions with CaM.",2020-12-01,0,1230,64,158
39,32355335,Autism spectrum heterogeneity: fact or artifact?,"The current diagnostic practices are linked to a 20-fold increase in the reported prevalence of ASD over the last 30 years. Fragmenting the autism phenotype into dimensional ""autistic traits"" results in the alleged recognition of autism-like symptoms in any psychiatric or neurodevelopemental condition and in individuals decreasingly distant from the typical population, and prematurely dismisses the relevance of a diagnostic threshold. Non-specific socio-communicative and repetitive DSM 5 criteria, combined with four quantitative specifiers as well as all their possible combinations, render limitless variety of presentations consistent with the categorical diagnosis of ASD. We propose several remedies to this problem: maintain a line of research on prototypical autism; limit the heterogeneity compatible with a categorical diagnosis to situations with a phenotypic overlap and a validated etiological link with prototypical autism; reintroduce the qualitative properties of autism presentations and of current dimensional specifiers, language, intelligence, comorbidity, and severity in the criteria used to diagnose autism in replacement of quantitative ""social"" and ""repetitive"" criteria; use these qualitative features combined with the clinical intuition of experts and machine-learning algorithms to differentiate coherent subgroups in today's autism spectrum; study these subgroups separately, and then compare them; and question the autistic nature of ""autistic traits"".",2020-12-01,15,1487,48,158
2073,33396754,Peptide-Based Nanomaterials for Tumor Immunotherapy,"With the increasing understanding of tumor immune circulation mechanisms, tumor immunotherapy including immune checkpoint blockade has become a research hotspot, which requires the development of more accurate and more efficient drugs with fewer side effects. In line with this requirement, peptides with good biocompatibility, targeting, and specificity become favorable theranostic reagents, and a series of promising candidates for tumor immunotherapy based on peptides have been developed. Additionally, the advantages of nanomaterials as drug carriers such as higher affinity have been demonstrated, providing possibilities of combination therapy. In this review, we summarize the development of peptide-based nanomaterials in tumor immunotherapy from the two aspects of functionalization and self-assembly. Furthermore, new methods for peptide screening, especially machine-learning-related strategies, is also a topic we were interested in, as this forms the basis for the construction of peptide-based platforms. Peptides provide broad prospects for tumor immunotherapy and we hope that this summary can provide insight into possible avenues for future exploration.",2020-12-01,1,1173,51,158
2072,33408594,Artificial Intelligence and its future potential in lung cancer screening,"Artificial intelligence (AI) simulates intelligent behavior as well as critical thinking comparable to a human being and can be used to analyze and interpret complex medical data. The application of AI in imaging diagnostics reduces the burden of radiologists and increases the sensitivity of lung cancer screening so that the morbidity and mortality associated with lung cancer can be decreased. In this article, we have tried to evaluate the role of artificial intelligence in lung cancer screening, as well as the future potential and efficiency of AI in the classification of nodules. The relevant studies between 2010-2020 were selected from the PubMed database after excluding animal studies and were analyzed for the contribution of AI. Techniques such as deep learning and machine learning allow automatic characterization and classification of nodules with high precision and promise an advanced lung cancer screening method in the future. Even though several combination models with high performance have been proposed, an effectively validated model for routine use still needs to be improvised. Combining the performance of artificial intelligence with a radiologist's expertise offers a successful outcome with higher accuracy. Thus, we can conclude that higher sensitivity, specificity, and accuracy of lung cancer screening and classification of nodules is possible through the integration of artificial intelligence and radiology. The validation of models and further research is to be carried out to determine the feasibility of this integration.",2020-12-01,0,1563,73,158
2085,33374969,Application of Biological Domain Knowledge Based Feature Selection on Gene Expression Data,"In the last two decades, there have been massive advancements in high throughput technologies, which resulted in the exponential growth of public repositories of gene expression datasets for various phenotypes. It is possible to unravel biomarkers by comparing the gene expression levels under different conditions, such as disease vs. control, treated vs. not treated, drug A vs. drug B, etc. This problem refers to a well-studied problem in the machine learning domain, i.e., the feature selection problem. In biological data analysis, most of the computational feature selection methodologies were taken from other fields, without considering the nature of the biological data. Thus, integrative approaches that utilize the biological knowledge while performing feature selection are necessary for this kind of data. The main idea behind the integrative gene selection process is to generate a ranked list of genes considering both the statistical metrics that are applied to the gene expression data, and the biological background information which is provided as external datasets. One of the main goals of this review is to explore the existing methods that integrate different types of information in order to improve the identification of the biomolecular signatures of diseases and the discovery of new potential targets for treatment. These integrative approaches are expected to aid the prediction, diagnosis, and treatment of diseases, as well as to enlighten us on disease state dynamics, mechanisms of their onset and progression. The integration of various types of biological information will necessitate the development of novel techniques for integration and data analysis. Another aim of this review is to boost the bioinformatics community to develop new approaches for searching and determining significant groups/clusters of features based on one or more biological grouping functions.",2020-12-01,0,1907,90,158
2071,33409270,Streamlining Natural Products Biomanufacturing With Omics and Machine Learning Driven Microbial Engineering,"Increasing demands for the supply of biopharmaceuticals have propelled the advancement of metabolic engineering and synthetic biology strategies for biomanufacturing of bioactive natural products. Using metabolically engineered microbes as the bioproduction hosts, a variety of natural products including terpenes, flavonoids, alkaloids, and cannabinoids have been synthesized through the construction and expression of known and newly found biosynthetic genes primarily from model and non-model plants. The employment of omics technology and machine learning (ML) platforms as high throughput analytical tools has been increasingly leveraged in promoting data-guided optimization of targeted biosynthetic pathways and enhancement of the microbial production capacity, thereby representing a critical debottlenecking approach in improving and streamlining natural products biomanufacturing. To this end, this mini review summarizes recent efforts that utilize omics platforms and ML tools in strain optimization and prototyping and discusses the beneficial uses of omics-enabled discovery of plant biosynthetic genes in the production of complex plant-based natural products by bioengineered microbes.",2020-12-01,0,1201,107,158
2069,33425253,Advances in integrative structural biology: Towards understanding protein complexes in their cellular context,"Microorganisms rely on protein interactions to transmit signals, react to stimuli, and grow. One of the best ways to understand these protein interactions is through structural characterization. However, in the past, structural knowledge was limited to stable, high-affinity complexes that could be crystallized. Recent developments in structural biology have revolutionized how protein interactions are characterized. The combination of multiple techniques, known as integrative structural biology, has provided insight into how large protein complexes interact in their native environment. In this mini-review, we describe the past, present, and potential future of integrative structural biology as a tool for characterizing protein interactions in their cellular context.",2020-12-01,0,775,109,158
2068,33425679,Review of deep learning for photoacoustic imaging,"Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging. The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.",2020-12-01,1,1325,49,158
2086,33374478,miRNA Targets: From Prediction Tools to Experimental Validation,"MicroRNAs (miRNAs) are post-transcriptional regulators of gene expression in both animals and plants. By pairing to microRNA responsive elements (mREs) on target mRNAs, miRNAs play gene-regulatory roles, producing remarkable changes in several physiological and pathological processes. Thus, the identification of miRNA-mRNA target interactions is fundamental for discovering the regulatory network governed by miRNAs. The best way to achieve this goal is usually by computational prediction followed by experimental validation of these miRNA-mRNA interactions. This review summarizes the key strategies for miRNA target identification. Several tools for computational analysis exist, each with different approaches to predict miRNA targets, and their number is constantly increasing. The major algorithms available for this aim, including Machine Learning methods, are discussed, to provide practical tips for familiarizing with their assumptions and understanding how to interpret the results. Then, all the experimental procedures for verifying the authenticity of the identified miRNA-mRNA target pairs are described, including High-Throughput technologies, in order to find the best approach for miRNA validation. For each strategy, strengths and weaknesses are discussed, to enable users to evaluate and select the right approach for their interests.",2020-12-01,0,1356,63,158
54,32336400,Neuroimaging-based Individualized Prediction of Cognition and Behavior for Mental Disorders and Health: Methods and Promises,"The neuroimaging community has witnessed a paradigm shift in biomarker discovery from using traditional univariate brain mapping approaches to multivariate predictive models, allowing the field to move toward a translational neuroscience era. Regression-based multivariate models (hereafter ""predictive modeling"") provide a powerful and widely used approach to predict human behavior with neuroimaging features. These studies maintain a focus on decoding individual differences in a continuously behavioral phenotype from neuroimaging data, opening up an exciting opportunity to describe the human brain at the single-subject level. In this survey, we provide an overview of recent studies that utilize machine learning approaches to identify neuroimaging predictors over the past decade. We first review regression-based approaches and highlight connectome-based predictive modeling, which has grown in popularity in recent years. Next, we systematically describe recent representative studies using these tools in the context of cognitive function, symptom severity, personality traits, and emotion processing. Finally, we highlight a few challenges related to combining multimodal data, longitudinal prediction, external validations, and the employment of deep learning methods that have emerged from our review of the existing literature, as well as present some promising and challenging future directions.",2020-12-01,5,1411,124,158
2067,33426010,The Role of Artificial Intelligence in Cardiovascular Imaging: State of the Art Review,"In this current digital landscape, artificial intelligence (AI) has established itself as a powerful tool in the commercial industry and is an evolving technology in healthcare. Cutting-edge imaging modalities outputting multi-dimensional data are becoming increasingly complex. In this era of data explosion, the field of cardiovascular imaging is undergoing a paradigm shift toward machine learning (ML) driven platforms. These diverse algorithms can seamlessly analyze information and automate a range of tasks. In this review article, we explore the role of ML in the field of cardiovascular imaging.",2020-12-01,2,604,86,158
1900,33293292,Neuronal differentiation strategies: insights from single-cell sequencing and machine learning,"Neuronal replacement therapies rely on the in vitro differentiation of specific cell types from embryonic or induced pluripotent stem cells, or on the direct reprogramming of differentiated adult cells via the expression of transcription factors or signaling molecules. The factors used to induce differentiation or reprogramming are often identified by informed guesses based on differential gene expression or known roles for these factors during development. Moreover, differentiation protocols usually result in partly differentiated cells or the production of a mix of cell types. In this Hypothesis article, we suggest that, to overcome these inefficiencies and improve neuronal differentiation protocols, we need to take into account the developmental history of the desired cell types. Specifically, we present a strategy that uses single-cell sequencing techniques combined with machine learning as a principled method to select a sequence of programming factors that are important not only in adult neurons but also during differentiation.",2020-12-01,1,1049,94,158
2087,33374270,"Driver Fatigue Detection Systems Using Multi-Sensors, Smartphone, and Cloud-Based Computing Platforms: A Comparative Analysis","Internet of things (IoT) cloud-based applications deliver advanced solutions for smart cities to decrease traffic accidents caused by driver fatigue while driving on the road. Environmental conditions or driver behavior can ultimately lead to serious roadside accidents. In recent years, the authors have developed many low-cost, computerized, driver fatigue detection systems (DFDs) to help drivers, by using multi-sensors, and mobile and cloud-based computing architecture. To promote safe driving, these are the most current emerging platforms that were introduced in the past. In this paper, we reviewed state-of-the-art approaches for predicting unsafe driving styles using three common IoT-based architectures. The novelty of this article is to show major differences among multi-sensors, smartphone-based, and cloud-based architectures in multimodal feature processing. We discussed all of the problems that machine learning techniques faced in recent years, particularly the deep learning (DL) model, to predict driver hypovigilance, especially in terms of these three IoT-based architectures. Moreover, we performed state-of-the-art comparisons by using driving simulators to incorporate multimodal features of the driver. We also mention online data sources in this article to test and train network architecture in the field of DFDs on public available multimodal datasets. These comparisons assist other authors to continue future research in this domain. To evaluate the performance, we mention the major problems in these three architectures to help researchers use the best IoT-based architecture for detecting DFDs in a real-time environment. Moreover, the important factors of Multi-Access Edge Computing (MEC) and 5th generation (5G) networks are analyzed in the context of deep learning architecture to improve the response time of DFD systems. Lastly, it is concluded that there is a research gap when it comes to implementing the DFD systems on MEC and 5G technologies by using multimodal features and DL architecture.",2020-12-01,0,2039,125,158
2088,33374181,The Role of Artificial Intelligence in Endoscopic Ultrasound for Pancreatic Disorders,"The use of artificial intelligence (AI) in various medical imaging applications has expanded remarkably, and several reports have focused on endoscopic ultrasound (EUS) images of the pancreas. This review briefly summarizes each report in order to help endoscopists better understand and utilize the potential of this rapidly developing AI, after a description of the fundamentals of the AI involved, as is necessary for understanding each study. At first, conventional computer-aided diagnosis (CAD) was used, which extracts and selects features from imaging data using various methods and introduces them into machine learning algorithms as inputs. Deep learning-based CAD utilizing convolutional neural networks has been used; in these approaches, the images themselves are used as inputs, and more information can be analyzed in less time and with higher accuracy. In the field of EUS imaging, although AI is still in its infancy, further research and development of AI applications is expected to contribute to the role of optical biopsy as an alternative to EUS-guided tissue sampling while also improving diagnostic accuracy through double reading with humans and contributing to EUS education.",2020-12-01,0,1201,85,158
1764,31553511,Autonomous Discovery in the Chemical Sciences Part I: Progress,"This two-part Review examines how automation has contributed to different aspects of discovery in the chemical sciences. In this first part, we describe a classification for discoveries of physical matter (molecules, materials, devices), processes, and models and how they are unified as search problems. We then introduce a set of questions and considerations relevant to assessing the extent of autonomy. Finally, we describe many case studies of discoveries accelerated by or resulting from computer assistance and automation from the domains of synthetic chemistry, drug discovery, inorganic chemistry, and materials science. These illustrate how rapid advancements in hardware automation and machine learning continue to transform the nature of experimentation and modeling. Part two reflects on these case studies and identifies a set of open challenges for the field.",2020-12-01,5,874,62,158
2063,33442551,Artificial Intelligence in Medicine: Chances and Challenges for Wide Clinical Adoption,"Background:                    Artificial intelligence (AI) applications that utilize machine learning are on the rise in clinical research and provide highly promising applications in specific use cases. However, wide clinical adoption remains far off. This review reflects on common barriers and current solution approaches.              Summary:                    Key challenges are abbreviated as the RISE criteria: Regulatory aspects, Interpretability, interoperability, and the need for Structured data and Evidence. As reoccurring barriers of AI adoption, these concepts are delineated and complemented by points to consider and possible solutions for effective and safe use of AI applications.              Key messages:                    There is a fraction of AI applications with proven clinical benefits and regulatory approval. Many new promising systems are the subject of current research but share common issues for wide clinical adoption. The RISE criteria can support preparation for challenges and pitfalls when designing or introducing AI applications into clinical practice.",2020-12-01,0,1097,86,158
2062,33447598,"Artificial Intelligence-Based Polyp Detection in Colonoscopy: Where Have We Been, Where Do We Stand, and Where Are We Headed?","Background:                    In the past, image-based computer-assisted diagnosis and detection systems have been driven mainly from the field of radiology, and more specifically mammography. Nevertheless, with the availability of large image data collections (known as the ""Big Data"" phenomenon) in correlation with developments from the domain of artificial intelligence (AI) and particularly so-called deep convolutional neural networks, computer-assisted detection of adenomas and polyps in real-time during screening colonoscopy has become feasible.              Summary:                    With respect to these developments, the scope of this contribution is to provide a brief overview about the evolution of AI-based detection of adenomas and polyps during colonoscopy of the past 35 years, starting with the age of ""handcrafted geometrical features"" together with simple classification schemes, over the development and use of ""texture-based features"" and machine learning approaches, and ending with current developments in the field of deep learning using convolutional neural networks. In parallel, the need and necessity of large-scale clinical data will be discussed in order to develop such methods, up to commercially available AI products for automated detection of polyps (adenoma and benign neoplastic lesions). Finally, a short view into the future is made regarding further possibilities of AI methods within colonoscopy.              Key messages:                    Research of image-based lesion detection in colonoscopy data has a 35-year-old history. Milestones such as the Paris nomenclature, texture features, big data, and deep learning were essential for the development and availability of commercial AI-based systems for polyp detection.",2020-12-01,0,1772,125,158
2060,33476063,How artificial intelligence may help the Covid-19 pandemic: Pitfalls and lessons for the future,"The clinical severity, rapid transmission and human losses due to coronavirus disease 2019 (Covid-19) have led the World Health Organization to declare it a pandemic. Traditional epidemiological tools are being significantly complemented by recent innovations especially using artificial intelligence (AI) and machine learning. AI-based model systems could improve pattern recognition of disease spread in populations and predictions of outbreaks in different geographical locations. A variable and a minimal amount of data are available for the signs and symptoms of Covid-19, allowing a composite of maximum likelihood algorithms to be employed to enhance the accuracy of disease diagnosis and to identify potential drugs. AI-based forecasting and predictions are expected to complement traditional approaches by helping public health officials to select better response and preparedness measures against Covid-19 cases. AI-based approaches have helped address the key issues but a significant impact on the global healthcare industry is yet to be achieved. The capability of AI to address the challenges may make it a key player in the operation of healthcare systems in future. Here, we present an overview of the prospective applications of the AI model systems in healthcare settings during the ongoing Covid-19 pandemic.",2020-12-01,0,1327,95,158
2061,33458608,Machine learning toward advanced energy storage devices and systems,"Technology advancement demands energy storage devices (ESD) and systems (ESS) with better performance, longer life, higher reliability, and smarter management strategy. Designing such systems involve a trade-off among a large set of parameters, whereas advanced control strategies need to rely on the instantaneous status of many indicators. Machine learning can dramatically accelerate calculations, capture complex mechanisms to improve the prediction accuracy, and make optimized decisions based on comprehensive status information. The computational efficiency makes it applicable for real-time management. This paper reviews recent progresses in this emerging area, especially new concepts, approaches, and applications of machine learning technologies for commonly used energy storage devices (including batteries, capacitors/supercapacitors, fuel cells, other ESDs) and systems (including battery ESS, hybrid ESS, grid and microgrid-containing energy storage units, pumped-storage system, thermal ESS). The perspective on future directions is also discussed.",2020-12-01,0,1065,67,158
1373,33120126,LMI-DForest: A deep forest model towards the prediction of lncRNA-miRNA interactions,"The interactions between miRNAs and long non-coding RNAs (lncRNAs) are subject to intensive recent studies due to its critical role in gene regulations. Computational prediction of lncRNA-miRNA interactions has become a popular alternative strategy to the experimental methods for identification of underlying interactions. It is desirable to develop the machine learning-based models for prediction of lncRNA-miRNA based on the experimentally validated interactions between lncRNAs and miRNAs. The accuracy and robustness of existing models based on machine learning techniques are subject to further improvement. Considering that the attributes of lncRNA and miRNA contribute key importance in the interaction between these two RNAs, a deep learning model, named LMI-DForest, is proposed here by combining the deep forest and autoencoder strategies. Systematic comparison on the experiment validated datasets for lncRNA-miRNA interaction datasets demonstrates that the proposed method consistently shows superior performance over the other machine learning models in the lncRNA-miRNA interaction prediction.",2020-12-01,1,1109,84,158
1876,33337337,Technical Aspects of Developing Chatbots for Medical Applications: Scoping Review,"Background:                    Chatbots are applications that can conduct natural language conversations with users. In the medical field, chatbots have been developed and used to serve different purposes. They provide patients with timely information that can be critical in some scenarios, such as access to mental health resources. Since the development of the first chatbot, ELIZA, in the late 1960s, much effort has followed to produce chatbots for various health purposes developed in different ways.              Objective:                    This study aimed to explore the technical aspects and development methodologies associated with chatbots used in the medical field to explain the best methods of development and support chatbot development researchers on their future work.              Methods:                    We searched for relevant articles in 8 literature databases (IEEE, ACM, Springer, ScienceDirect, Embase, MEDLINE, PsycINFO, and Google Scholar). We also performed forward and backward reference checking of the selected articles. Study selection was performed by one reviewer, and 50% of the selected studies were randomly checked by a second reviewer. A narrative approach was used for result synthesis. Chatbots were classified based on the different technical aspects of their development. The main chatbot components were identified in addition to the different techniques for implementing each module.              Results:                    The original search returned 2481 publications, of which we identified 45 studies that matched our inclusion and exclusion criteria. The most common language of communication between users and chatbots was English (n=23). We identified 4 main modules: text understanding module, dialog management module, database layer, and text generation module. The most common technique for developing text understanding and dialogue management is the pattern matching method (n=18 and n=25, respectively). The most common text generation is fixed output (n=36). Very few studies relied on generating original output. Most studies kept a medical knowledge base to be used by the chatbot for different purposes throughout the conversations. A few studies kept conversation scripts and collected user data and previous conversations.              Conclusions:                    Many chatbots have been developed for medical use, at an increasing rate. There is a recent, apparent shift in adopting machine learning-based approaches for developing chatbot systems. Further research can be conducted to link clinical outcomes to different chatbot development techniques and technical characteristics.",2020-12-01,0,2663,81,158
805,32996368,Exploring the Potential of Artificial Intelligence and Machine Learning to Combat COVID-19 and Existing Opportunities for LMIC: A Scoping Review,"Background:                    In the face of the current time-sensitive COVID-19 pandemic, the limited capacity of healthcare systems resulted in an emerging need to develop newer methods to control the spread of the pandemic. Artificial Intelligence (AI), and Machine Learning (ML) have a vast potential to exponentially optimize health care research. The use of AI-driven tools in LMIC can help in eradicating health inequalities and decrease the burden on health systems.              Methods:                    The literature search for this Scoping review was conducted through the PubMed database using keywords: COVID-19, Artificial Intelligence (AI), Machine Learning (ML), and Low Middle-Income Countries (LMIC). Forty-three articles were identified and screened for eligibility and 13 were included in the final review. All the items of this Scoping review are reported using guidelines for PRISMA extension for scoping reviews (PRISMA-ScR).              Results:                    Results were synthesized and reported under 4 themes. (a) The need of AI during this pandemic: AI can assist to increase the speed and accuracy of identification of cases and through data mining to deal with the health crisis efficiently, (b) Utility of AI in COVID-19 screening, contact tracing, and diagnosis: Efficacy for virus detection can a be increased by deploying the smart city data network using terminal tracking system along-with prediction of future outbreaks, (c) Use of AI in COVID-19 patient monitoring and drug development: A Deep learning system provides valuable information regarding protein structures associated with COVID-19 which could be utilized for vaccine formulation, and (d) AI beyond COVID-19 and opportunities for Low-Middle Income Countries (LMIC): There is a lack of financial, material, and human resources in LMIC, AI can minimize the workload on human labor and help in analyzing vast medical data, potentiating predictive and preventive healthcare.              Conclusion:                    AI-based tools can be a game-changer for diagnosis, treatment, and management of COVID-19 patients with the potential to reshape the future of healthcare in LMIC.",2020-12-01,1,2189,144,158
1888,33307272,Changing the landscape of tumor immunology: novel tools to examine T cell specificity,"Immunotherapy has established itself as a stalwart arm in patient care and with precision medicine forms the new paradigm in cancer treatment. T cells are an important group of immune cells capable of potent cancer immune surveillance and immunity. The advent of bioinformatics, particularly more recent advances incorporating algorithms employing machine learning, provide a seemingly limitless ability for T cell analysis and hypothesis generation. Such endeavors have become indispensable to research efforts accelerating and evolving to such an extent that there exists an appreciable gap between knowledge and proof of function and application. Exciting new technologies such as DNA barcoding, cytometry by time-of-flight (CyTOF), and peptide-exchangeable pHLA multimers inclusive of rare and difficult HLA alleles offer high-throughput cell-by-cell analytical capabilities. These outstanding recent contributions to T cell research will help close this gap and potentially bring practical benefit to patients.",2020-12-01,0,1015,85,158
1822,32763775,"Prediction on critically ill patients: The role of ""big data""","Accurate outcome prediction in Intensive Care Units (ICUs) would allow for better treatment planning, risk adjustment of study populations, and overall improvements in patient care. In the past, prognostic models have focused on mortality using simple ordinal severity of illness scores which could be tabulated manually by a human. With the improvements in computing power and proliferation of electronic medical records, entirely new approaches have become possible. Here we review the latest advances in outcome prediction, paying close attention to methods which are widely applicable and provide a high-level overview of the challenges the field currently faces.",2020-12-01,0,667,61,158
1366,33130528,Machine learning for suicidology: A practical review of exploratory and hypothesis-driven approaches,"Machine learning is being used to discover models to predict the progression from suicidal ideation to action in clinical populations. While quantifiable improvements in prediction accuracy have been achieved over theory-driven efforts, models discovered through machine learning continue to fall short of clinical relevance. Thus, the value of machine learning for reaching this objective is hotly contested. We agree that machine learning, treated as a ""black box"" approach antithetical to theory-building, will not discover clinically relevant models of suicide. However, such models may be developed through deliberate synthesis of data- and theory-driven approaches. By providing an accessible overview of essential concepts and common methods, we highlight how generalizable models and scientific insight may be obtained by incorporating prior knowledge and expectations to machine learning research, drawing examples from suicidology. We then discuss challenges investigators will face when using machine learning to discover models of low prevalence outcomes, such as suicide.",2020-12-01,0,1084,100,158
1873,32682191,Medical data science in rhinology: Background and implications for clinicians,"Background:                    An important challenge of big data is using complex information networks to provide useful clinical information. Recently, machine learning, and particularly deep learning, has enabled rapid advances in clinical practice. The application of artificial intelligence (AI) and machine learning (ML) in rhinology is an increasingly relevant topic.              Purpose:                    We review the literature and provide a detailed overview of the recent advances in AI and ML as applied to rhinology. Also, we discuss both the significant benefits of this work as well as the challenges in the implementation and acceptance of these methods for clinical purposes.              Methods:                    We aimed to identify and explain published studies on the use of AI and ML in rhinology based on PubMed, Scopus, and Google searches. The search string ""nasal OR respiratory AND artificial intelligence OR machine learning"" was used. Most of the studies covered areas of paranasal sinuses radiology, including allergic rhinitis, chronic rhinitis, computed tomography scans, and nasal cytology.              Results:                    Cluster analysis and convolutional neural networks (CNNs) were mainly used in studies related to rhinology. AI is increasingly affecting healthcare research, and ML technology has been used in studies of chronic rhinitis and allergic rhinitis, providing some exciting new research modalities.              Conclusion:                    AI is especially useful when there is no conclusive evidence to aid decision making. ML can help doctors make clinical decisions, but it does not entirely replace doctors. However, when critically evaluating studies using this technique, rhinologists must take into account the limitations of its applications and use.",2020-12-01,0,1827,77,158
1828,32750971,Machine Learning Techniques for Ophthalmic Data Processing: A Review,"Machine learning and especially deep learning techniques are dominating medical image and data analysis. This article reviews machine learning approaches proposed for diagnosing ophthalmic diseases during the last four years. Three diseases are addressed in this survey, namely diabetic retinopathy, age-related macular degeneration, and glaucoma. The review covers over 60 publications and 25 public datasets and challenges related to the detection, grading, and lesion segmentation of the three considered diseases. Each section provides a summary of the public datasets and challenges related to each pathology and the current methods that have been applied to the problem. Furthermore, the recent machine learning approaches used for retinal vessels segmentation, and methods of retinal layers and fluid segmentation are reviewed. Two main imaging modalities are considered in this survey, namely color fundus imaging, and optical coherence tomography. Machine learning approaches that use eye measurements and visual field data for glaucoma detection are also included in the survey. Finally, the authors provide their views, expectations and the limitations of the future of these techniques in the clinical practice.",2020-12-01,0,1223,68,158
1445,32623625,Ischemia and outcome prediction by cardiac CT based machine learning,"Cardiac CT using non-enhanced coronary artery calcium scoring (CACS) and coronary CT angiography (cCTA) has been proven to provide excellent evaluation of coronary artery disease (CAD) combining anatomical and morphological assessment of CAD for cardiovascular risk stratification and therapeutic decision-making, in addition to providing prognostic value for the occurrence of adverse cardiac outcome. In recent years, artificial intelligence (AI) and, in particular, the application of machine learning (ML) algorithms, have been promoted in cardiovascular CT imaging for improved decision pathways, risk stratification, and outcome prediction in a more objective, reproducible, and rational manner. AI is based on computer science and mathematics that are based on big data, high performance computational infrastructure, and applied algorithms. The application of ML in daily routine clinical practice may hold potential to improve imaging workflow and to promote better outcome prediction and more effective decision-making in patient management. Moreover, CT represents a field wherein ML may be particularly useful, such as CACS and cCTA. Thus, the purpose of this review is to give a short overview about the contemporary state of ML based algorithms in cardiac CT, as well as to provide clinicians with currently available scientific data on clinical validation and implementation of these algorithms for the prediction of ischemia-specific CAD and cardiovascular outcome.",2020-12-01,0,1481,68,158
1476,32596957,Magnetic resonance fingerprinting: from evolution to clinical applications,"In 2013, Magnetic Resonance Fingerprinting (MRF) emerged as a method for fast, quantitative Magnetic Resonance Imaging. This paper reviews the current status of MRF up to early 2020 and aims to highlight the advantages MRF can offer medical imaging professionals. By acquiring scan data as pseudorandom samples, MRF elicits a unique signal evolution, or 'fingerprint', from each tissue type. It matches 'randomised' free induction decay acquisitions against pre-computed simulated tissue responses to generate a set of quantitative images of T1 , T2 and proton density (PD) with co-registered voxels, rather than as traditional relative T1 - and T2 -weighted images. MRF numeric pixel values retain accuracy and reproducibility between 2% and 8%. MRF acquisition is robust to strong undersampling of k-space. Scan sequences have been optimised to suppress sub-sampling artefacts, while artificial intelligence and machine learning techniques have been employed to increase matching speed and precision. MRF promises improved patient comfort with reduced scan times and fewer image artefacts. Quantitative MRF data could be used to define population-wide numeric biomarkers that classify normal versus diseased tissue. Certification of clinical centres for MRF scan repeatability would permit numeric comparison of sequential images for any individual patient and the pooling of multiple patient images across large, cross-site imaging studies. MRF has to date shown promising results in early clinical trials, demonstrating reliable differentiation between malignant and benign prostate conditions, and normal and sclerotic hippocampal tissue. MRF is now undergoing small-scale trials at several sites across the world; moving it closer to routine clinical application.",2020-12-01,1,1769,74,158
1901,33291859,A Review and a Framework of Variables for Defining and Characterizing Tinnitus Subphenotypes,"Tinnitus patients can present with various characteristics, such as those related to the tinnitus perception, symptom severity, and pattern of comorbidities. It is speculated that this phenotypic heterogeneity is associated with differences in the underlying pathophysiology and personal reaction to the condition. However, there is as yet no established protocol for tinnitus profiling or subtyping, hindering progress in treatment development. This review summarizes data on variables that have been used in studies investigating phenotypic differences in subgroups of tinnitus, including variables used to both define and compare subgroups. A PubMed search led to the identification of 64 eligible articles. In most studies, variables for subgrouping were chosen by the researchers (hypothesis-driven approach). Other approaches included application of unsupervised machine-learning techniques for the definition of subgroups (data-driven), and subgroup definition based on the response to a tinnitus treatment (treatment response). A framework of 94 variable concepts was created to summarize variables used across all studies. Frequency statistics for the use of each variable concept are presented, demonstrating those most and least commonly assessed. This review highlights the high dimensionality of tinnitus heterogeneity. The framework of variables can contribute to the design of future studies, helping to decide on tinnitus assessment and subgrouping.",2020-12-01,0,1465,92,158
816,32981888,The use of artificial intelligence in computed tomography image reconstruction - A literature review,"Background and purpose:                    The use of AI in the process of CT image reconstruction may improve image quality of resultant images and therefore facilitate low-dose CT examinations.              Methods:                    Articles in this review were gathered from multiple databases (Google Scholar, Ovid and Monash University Library Database). A total of 17 articles regarding AI use in CT image reconstruction was reviewed, including 1 white paper from GE Healthcare.              Results:                    DLR algorithms performed better in terms of noise reduction abilities, and image quality preservation at low doses when compared to other reconstruction techniques.              Conclusion:                    Further research is required to discuss clinical application and diagnostic accuracy of DLR algorithms, but AI is a promising dose-reduction technique with future computational advances.",2020-12-01,0,923,100,158
817,32980438,Multivariate data analysis in cell gene therapy manufacturing,"The emergence of cell gene therapy (CGT) as a safe and efficacious treatment for numerous severe inherited and acquired human diseases has led to growing interest and investment in new CGT products. The most successful of these have been autologous viral vector-based treatments. The development of viral vector manufacturing processes and ex vivo patient cell processing capabilities is a pressing issue in the advancement of autologous viral vector-based CGT treatments. In viral vector production, scale-up is a critical task due to the limited scalability of traditional laboratory systems and the demand for high volumes of viral vector manufactured in accordance with current good manufacturing practice. Ex vivo cell processing methods require optimisation and automation before they can be scaled out, and several other manufacturing challenges are prevalent such as high levels of raw material and process variability, difficulty characterising complex materials, and a lack of knowledge of critical process parameters and their effect on critical quality attributes of the viral vector and cell drug products. Multivariate data analysis (MVDA) has been leveraged successfully in a variety of applications in the chemical and biochemical industries, including for tasks such as bioprocess monitoring, identification of critical process parameters and assessment of process variability and comparability during process development, scale-up and technology transfer. Henceforth, MVDA is reviewed here as a suitable tool for tackling some of the challenges faced in the development of CGT manufacturing processes. A summary of some key CGT manufacturing challenges is provided along with a review of MVDA applications to mammalian and microbial processes, and an exploration of the potential benefits, requirements and pre-requisites of MVDA applications in the development of CGT manufacturing processes.",2020-12-01,0,1911,61,158
801,33000556,Part 1: Artificial intelligence technology in surgery,"Artificial intelligence (AI) is one of the disruptive technologies of the fourth Industrial Revolution that is changing our work practices. This technology is in use in highly diverse industries including health care, defence, insurance and e-commerce. This review focuses on the relevance of AI to surgery. AI will aid surgeons with diagnostic decision-making, patient selection for surgery as well as improve patient pre- and post-operative care and management. Ethical considerations of AI with respect to patient rights and data privacy are highlighted. A further challenge is how best to present to national regulators a pragmatic way to assess AI as 'software as a medical device'. This relates to the ramifications for the adoption of AI technology in clinical practice, and its subsequent public funding support and reimbursement. It is evident that AI technology has important applications in surgery in the 21st century. The establishment of a key work programme in this area will be important if surgeons are to fully utilize AI in surgery.",2020-12-01,0,1051,53,158
1392,33080488,Effector Biology of Biotrophic Plant Fungal Pathogens: Current Advances and Future Prospects,"The interaction of fungal pathogens with their host requires a novel invading mechanism and the presence of various virulence-associated components responsible for promoting the infection. The small secretory proteins, explicitly known as effector proteins, are one of the prime mechanisms of host manipulation utilized by the pathogen to disarm the host. Several effector proteins are known to translocate from fungus to the plant cell for host manipulation. Many fungal effectors have been identified using genomic, transcriptomic, and bioinformatics approaches. Most of the effector proteins are devoid of any conserved signatures, and their prediction based on sequence homology is very challenging, therefore by combining the sequence consensus based upon machine learning features, multiple tools have also been developed for predicting apoplastic and cytoplasmic effectors. Various post-genomics approaches like transcriptomics of virulent isolates have also been utilized for identifying active consortia of effectors. Significant progress has been made in understanding biotrophic effectors; however, most of it is underway due to their complex interaction with host and complicated recognition and signaling networks. This review discusses advances, and challenges in effector identification and highlighted various features of the potential effector proteins and approaches for understanding their genetics and strategies for regulation.",2020-12-01,1,1448,92,158
820,32977139,Data-driven ICU management: Using Big Data and algorithms to improve outcomes,"The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term ""Big Data"" can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.",2020-12-01,1,1135,77,158
1895,33297436,A Review of Machine Learning Methods of Feature Selection and Classification for Autism Spectrum Disorder,"Autism Spectrum Disorder (ASD), according to DSM-5 in the American Psychiatric Association, is a neurodevelopmental disorder that includes deficits of social communication and social interaction with the presence of restricted and repetitive behaviors. Children with ASD have difficulties in joint attention and social reciprocity, using non-verbal and verbal behavior for communication. Due to these deficits, children with autism are often socially isolated. Researchers have emphasized the importance of early identification and early intervention to improve the level of functioning in language, communication, and well-being of children with autism. However, due to limited local assessment tools to diagnose these children, limited speech-language therapy services in rural areas, etc., these children do not get the rehabilitation they need until they get into compulsory schooling at the age of seven years old. Hence, efficient approaches towards early identification and intervention through speedy diagnostic procedures for ASD are required. In recent years, advanced technologies like machine learning have been used to analyze and investigate ASD to improve diagnostic accuracy, time, and quality without complexity. These machine learning methods include artificial neural networks, support vector machines, a priori algorithms, and decision trees, most of which have been applied to datasets connected with autism to construct predictive models. Meanwhile, the selection of features remains an essential task before developing a predictive model for ASD classification. This review mainly investigates and analyzes up-to-date studies on machine learning methods for feature selection and classification of ASD. We recommend methods to enhance machine learning's speedy execution for processing complex data for conceptualization and implementation in ASD diagnostic research. This study can significantly benefit future research in autism using a machine learning approach for feature selection, classification, and processing imbalanced data.",2020-12-01,1,2058,105,158
1996,32896771,Increasing metabolic pathway flux by using machine learning models,"Machine learning is transforming many industries through self-improving models that are fueled by big data and high computing power. The field of metabolic engineering, which uses cellular biochemical network to manufacture useful small molecules, has also witnessed the first wave of machine learning applications in the past five years, covering reaction route design, enzyme selection, pathway engineering and process optimization. This review focuses on pathway engineering, and uses a few recent studies to illustrate (1) how machine learning models can be useful in overcoming an evident rate-limiting step, and (2) how the models may be used to exhaustively search - or guide optimization algorithms to search - a large design space when the cellular regulation of the reaction network is more convoluted.",2020-12-01,0,812,66,158
1395,33072513,Systems biology approaches integrated with artificial intelligence for optimized metabolic engineering,"Metabolic engineering aims to maximize the production of bio-economically important substances (compounds, enzymes, or other proteins) through the optimization of the genetics, cellular processes and growth conditions of microorganisms. This requires detailed understanding of underlying metabolic pathways involved in the production of the targeted substances, and how the cellular processes or growth conditions are regulated by the engineering. To achieve this goal, a large system of experimental techniques, compound libraries, computational methods and data resources, including multi-omics data, are used. The recent advent of multi-omics systems biology approaches significantly impacted the field by opening new avenues to perform dynamic and large-scale analyses that deepen our knowledge on the manipulations. However, with the enormous transcriptomics, proteomics and metabolomics available, it is a daunting task to integrate the data for a more holistic understanding. Novel data mining and analytics approaches, including Artificial Intelligence (AI), can provide breakthroughs where traditional low-throughput experiment-alone methods cannot easily achieve. Here, we review the latest attempts of combining systems biology and AI in metabolic engineering research, and highlight how this alliance can help overcome the current challenges facing industrial biotechnology, especially for food-related substances and compounds using microorganisms.",2020-12-01,0,1461,102,158
1955,32959233,Popular Computational Tools Used for miRNA Prediction and Their Future Development Prospects,"MicroRNAs (miRNAs) are 19-24 nucleotide (nt)-long noncoding, single-stranded RNA molecules that play significant roles in regulating the gene expression, growth, and development of plants and animals. From the year that miRNAs were first discovered until the beginning of the twenty-first century, researchers used experimental methods such as cloning and sequencing to identify new miRNAs and their roles in the posttranscriptional regulation of protein synthesis. Later, in the early 2000s, informatics approaches to the discovery of new miRNAs began to be implemented. With increasing knowledge about miRNA, more efficient algorithms have been developed for computational miRNA prediction. The miRNA research community, hoping for greater coverage and faster results, has shifted from cumbersome and expensive traditional experimental approaches to computational approaches. These computational methods started with homology-based comparisons of known miRNAs with orthologs in the genomes of other species; this method could identify a known miRNA in new species. Second-generation sequencing and next-generation sequencing of mRNA at different developmental stages and in specific tissues, in combination with a better search and alignment algorithm, have accelerated the process of predicting novel miRNAs in a particular species. Using the accumulated annotated miRNA sequence information, researchers have been able to design ab initio algorithms for miRNA prediction independent of genome sequence knowledge. Here, the methods recently used for miRNA computational prediction are summarized and classified into the following four categories: homology-based, target-based, scoring-based, and machine-learning-based approaches. Finally, the future developmental directions of miRNA prediction methods are discussed.",2020-12-01,0,1821,92,158
1465,32604065,AI in mental health,"With the advent of digital approaches to mental health, modern artificial intelligence (AI), and machine learning in particular, is being used in the development of prediction, detection and treatment solutions for mental health care. In terms of treatment, AI is being incorporated into digital interventions, particularly web and smartphone apps, to enhance user experience and optimise personalised mental health care. In terms of prediction and detection, modern streams of abundant data mean that data-driven AI methods can be employed to develop prediction/detection models for mental health conditions. In particular, an individual's 'digital exhaust', the data gathered from their numerous personal digital device and social media interactions, can be mined for behavioural or mental health insights. Language, long considered a window into the human mind, can now be quantitatively harnessed as data with powerful computer-based natural language processing to also provide a method of inferring mental health. Furthermore, natural language processing can also be used to develop conversational agents used for therapeutic intervention.",2020-12-01,3,1144,19,158
1846,32725781,Cheminformatics in Natural Product-based Drug Discovery,"This review seeks to provide a timely survey of the scope and limitations of cheminformatics methods in natural product-based drug discovery. Following an overview of data resources of chemical, biological and structural information on natural products, we discuss, among other aspects, in silico methods for (i) data curation and natural products dereplication, (ii) analysis, visualization, navigation and comparison of the chemical space, (iii) quantification of natural product-likeness, (iv) prediction of the bioactivities (virtual screening, target prediction), ADME and safety profiles (toxicity) of natural products, (v) natural products-inspired de novo design and (vi) prediction of natural products prone to cause interference with biological assays. Among the many methods discussed are rule-based, similarity-based, shape-based, pharmacophore-based and network-based approaches, docking and machine learning methods.",2020-12-01,8,930,55,158
1461,32607629,Machine learning for the identification of clinically significant prostate cancer on MRI: a meta-analysis,"Objectives:                    The aim of this study was to systematically review the literature and perform a meta-analysis of machine learning (ML) diagnostic accuracy studies focused on clinically significant prostate cancer (csPCa) identification on MRI.              Methods:                    Multiple medical databases were systematically searched for studies on ML applications in csPCa identification up to July 31, 2019. Two reviewers screened all papers independently for eligibility. The area under the receiver operating characteristic curves (AUC) was pooled to quantify predictive accuracy. A random-effects model estimated overall effect size while statistical heterogeneity was assessed with the I2 value. A funnel plot was used to investigate publication bias. Subgroup analyses were performed based on reference standard (biopsy or radical prostatectomy) and ML type (deep and non-deep).              Results:                    After the final revision, 12 studies were included in the analysis. Statistical heterogeneity was high both in overall and in subgroup analyses. The overall pooled AUC for ML in csPCa identification was 0.86, with 0.81-0.91 95% confidence intervals (95%CI). The biopsy subgroup (n = 9) had a pooled AUC of 0.85 (95%CI = 0.79-0.91) while the radical prostatectomy one (n = 3) of 0.88 (95%CI = 0.76-0.99). Deep learning ML (n = 4) had a 0.78 AUC (95%CI = 0.69-0.86) while the remaining 8 had AUC = 0.90 (95%CI = 0.85-0.94).              Conclusions:                    ML pipelines using prostate MRI to identify csPCa showed good accuracy and should be further investigated, possibly with better standardisation in design and reporting of results.              Key points:                     Overall pooled AUC was 0.86 with 0.81-0.91 95% confidence intervals.  In the reference standard subgroup analysis, algorithm accuracy was similar with pooled AUCs of 0.85 (0.79-0.91 95% confidence intervals) and 0.88 (0.76-0.99 95% confidence intervals) for studies employing biopsies and radical prostatectomy, respectively.  Deep learning pipelines performed worse (AUC = 0.78, 0.69-0.86 95% confidence intervals) than other approaches (AUC = 0.90, 0.85-0.94 95% confidence intervals).",2020-12-01,1,2231,105,158
1452,32617720,From CT to artificial intelligence for complex assessment of plaque-associated risk,"The recent technological developments in the field of cardiac imaging have established coronary computed tomography angiography (CCTA) as a first-line diagnostic tool in patients with suspected coronary artery disease (CAD). CCTA offers robust information on the overall coronary circulation and luminal stenosis, also providing the ability to assess the composition, morphology, and vulnerability of atherosclerotic plaques. In addition, the perivascular adipose tissue (PVAT) has recently emerged as a marker of increased cardiovascular risk. The addition of PVAT quantification to standard CCTA imaging may provide the ability to extract information on local inflammation, for an individualized approach in coronary risk stratification. The development of image post-processing tools over the past several years allowed CCTA to provide a significant amount of data that can be incorporated into machine learning (ML) applications. ML algorithms that use radiomic features extracted from CCTA are still at an early stage. However, the recent development of artificial intelligence will probably bring major changes in the way we integrate clinical, biological, and imaging information, for a complex risk stratification and individualized therapeutic decision making in patients with CAD. This review aims to present the current evidence on the complex role of CCTA in the detection and quantification of vulnerable plaques and the associated coronary inflammation, also describing the most recent developments in the radiomics-based machine learning approach for complex assessment of plaque-associated risk.",2020-12-01,2,1611,83,158
2013,32859549,Brain Age Prediction Reveals Aberrant Brain White Matter in Schizophrenia and Bipolar Disorder: A Multisample Diffusion Tensor Imaging Study,"Background:                    Schizophrenia (SZ) and bipolar disorder (BD) share substantial neurodevelopmental components affecting brain maturation and architecture. This necessitates a dynamic lifespan perspective in which brain aberrations are inferred from deviations from expected lifespan trajectories. We applied machine learning to diffusion tensor imaging (DTI) indices of white matter structure and organization to estimate and compare brain age between patients with SZ, patients with BD, and healthy control (HC) subjects across 10 cohorts.              Methods:                    We trained 6 cross-validated models using different combinations of DTI data from 927 HC subjects (18-94 years of age) and applied the models to the test sets including 648 patients with SZ (18-66 years of age), 185 patients with BD (18-64 years of age), and 990 HC subjects (17-68 years of age), estimating the brain age for each participant. Group differences were assessed using linear models, accounting for age, sex, and scanner. A meta-analytic framework was applied to assess the heterogeneity and generalizability of the results.              Results:                    Tenfold cross-validation revealed high accuracy for all models. Compared with HC subjects, the model including all feature sets significantly overestimated the age of patients with SZ (Cohen's d = -0.29) and patients with BD (Cohen's d = 0.18), with similar effects for the other models. The meta-analysis converged on the same findings. Fractional anisotropy-based models showed larger group differences than the models based on other DTI-derived metrics.              Conclusions:                    Brain age prediction based on DTI provides informative and robust proxies for brain white matter integrity. Our results further suggest that white matter aberrations in SZ and BD primarily consist of anatomically distributed deviations from expected lifespan trajectories that generalize across cohorts and scanners.",2020-12-01,2,1993,140,158
1892,33302517,A Comprehensive Survey on Local Differential Privacy toward Data Statistics and Analysis,"Collecting and analyzing massive data generated from smart devices have become increasingly pervasive in crowdsensing, which are the building blocks for data-driven decision-making. However, extensive statistics and analysis of such data will seriously threaten the privacy of participating users. Local differential privacy (LDP) was proposed as an excellent and prevalent privacy model with distributed architecture, which can provide strong privacy guarantees for each user while collecting and analyzing data. LDP ensures that each user's data is locally perturbed first in the client-side and then sent to the server-side, thereby protecting data from privacy leaks on both the client-side and server-side. This survey presents a comprehensive and systematic overview of LDP with respect to privacy models, research tasks, enabling mechanisms, and various applications. Specifically, we first provide a theoretical summarization of LDP, including the LDP model, the variants of LDP, and the basic framework of LDP algorithms. Then, we investigate and compare the diverse LDP mechanisms for various data statistics and analysis tasks from the perspectives of frequency estimation, mean estimation, and machine learning. Furthermore, we also summarize practical LDP-based application scenarios. Finally, we outline several future research directions under LDP.",2020-12-01,0,1363,88,158
1894,33299275,Machine Learning and Artificial Intelligence in Surgical Fields,"Artificial intelligence (AI) and machine learning (ML) have the potential to improve multiple facets of medical practice, including diagnosis of disease, surgical training, clinical outcomes, and access to healthcare. There have been various applications of this technology to surgical fields. AI and ML have been used to evaluate a surgeon's technical skill. These technologies can detect instrument motion, recognize patterns in video recordings, and track the physical motion, eye movements, and cognitive function of the surgeon. These modalities also aid in the advancement of robotic surgical training. The da Vinci Standard Surgical System developed a recording and playback system to help trainees receive tactical feedback to acquire more precision when operating. ML has shown promise in recognizing and classifying complex patterns on diagnostic images and within pathologic tissue analysis. This allows for more accurate and efficient diagnosis and treatment. Artificial neural networks are able to analyze sets of symptoms in conjunction with labs, imaging, and exam findings to determine the likelihood of a diagnosis or outcome. Telemedicine is another use of ML and AI that uses technology such as voice recognition to deliver health care remotely. Limitations include the need for large data sets to program computers to create the algorithms. There is also the potential for misclassification of data points that do not follow the typical patterns learned by the machine. As more applications of AI and ML are developed for the surgical field, further studies are needed to determine feasibility, efficacy, and cost.",2020-12-01,0,1634,63,158
819,32978734,Lake water-level fluctuation forecasting using machine learning models: a systematic review,"Lake water-level fluctuation is a complex and dynamic process, characterized by high stochasticity and nonlinearity, and difficult to model and forecast. In recent years, applications of machine learning (ML) models have yielded substantial progress in forecasting lake water-level fluctuations. This paper presents a comprehensive review of the applications of ML models for modeling water-level dynamics in lakes. Among the many existing ML models, seven popular ML model types are reviewed: (1) artificial neural network (ANN); (2) support vector machine (SVM); (3) artificial neuro-fuzzy inference system (ANFIS); (4) hybrid models, such as hybrid wavelet-artificial neural network (WA-ANN) model, hybrid wavelet-artificial neuro-fuzzy inference system (WA-ANFIS) model, and hybrid wavelet-support vector machine (WA-SVM) model; (5) evolutionary models, such as gene expression programming (GEP) and genetic programming (GP); (6) extreme learning machine (ELM); and (7) deep learning (DL). Model inputs, data split, model performance criteria, and model inter-comparison as well as the associated issues are discussed. The advantages and limitations of the established ML models are also discussed. Some specific directions for future research are also offered. This review provides a new vision for hydrologists and water resources planners for sustainable management of lakes.",2020-12-01,0,1382,91,158
1353,33153682,"Informatics Approaches for Recognition, Management, and Prevention of Occupational Respiratory Disease","Computer and information systems can improve occupational respiratory disease prevention and surveillance by providing efficient resources for patients, workers, clinicians, and public health practitioners. Advances include interlinking electronic health records, autocoding surveillance data, clinical decision support systems, and social media applications for acquiring and disseminating information. Obstacles to advances include inflexible hierarchical coding schemes, inadequate occupational health electronic health record systems, and inadequate public focus on occupational respiratory disease. Potentially transformative approaches include machine learning, natural language processing, and improved ontologies.",2020-12-01,0,721,102,158
1812,32771864,Circadian clock effects on cellular proliferation: Insights from theory and experiments,"Oscillations of the cellular circadian clock have emerged as an important regulator of many physiological processes, both in health and in disease. One such process, cellular proliferation, is being increasingly recognized to be affected by the circadian clock. Here, we review how a combination of experimental and theoretical work has furthered our understanding of the way circadian clocks couple to the cell cycle and play a role in tissue homeostasis and cancer. Finally, we discuss recently introduced methods for modeling coupling of clocks based on techniques from survival analysis and machine learning and highlight their potential importance for future studies.",2020-12-01,2,672,87,158
1977,32920005,Overview of artificial intelligence-based applications in radiotherapy: Recommendations for implementation and quality assurance,"Artificial Intelligence (AI) is currently being introduced into different domains, including medicine. Specifically in radiation oncology, machine learning models allow automation and optimization of the workflow. A lack of knowledge and interpretation of these AI models can hold back wide-spread and full deployment into clinical practice. To facilitate the integration of AI models in the radiotherapy workflow, generally applicable recommendations on implementation and quality assurance (QA) of AI models are presented. For commonly used applications in radiotherapy such as auto-segmentation, automated treatment planning and synthetic computed tomography (sCT) the basic concepts are discussed in depth. Emphasis is put on the commissioning, implementation and case-specific and routine QA of AI models needed for a methodical introduction in clinical practice.",2020-12-01,3,868,128,158
2631,32513061,Artificial Intelligence Applications in Otology: A State of the Art Review,"Objective:                    Recent advances in artificial intelligence (AI) are driving innovative new health care solutions. We aim to review the state of the art of AI in otology and provide a discussion of work underway, current limitations, and future directions.              Data sources:                    Two comprehensive databases, MEDLINE and EMBASE, were mined using a directed search strategy to identify all articles that applied AI to otology.              Review methods:                    An initial abstract and title screening was completed. Exclusion criteria included nonavailable abstract and full text, language, and nonrelevance. References of included studies and relevant review articles were cross-checked to identify additional studies.              Conclusion:                    The database search identified 1374 articles. Abstract and title screening resulted in full-text retrieval of 96 articles. A total of N = 38 articles were retained. Applications of AI technologies involved the optimization of hearing aid technology (n = 5; 13% of all articles), speech enhancement technologies (n = 4; 11%), diagnosis and management of vestibular disorders (n = 11; 29%), prediction of sensorineural hearing loss outcomes (n = 9; 24%), interpretation of automatic brainstem responses (n = 5; 13%), and imaging modalities and image-processing techniques (n = 4; 10%). Publication counts of the included articles from each decade demonstrated a marked increase in interest in AI in recent years.              Implications for practice:                    This review highlights several applications of AI that otologists and otolaryngologists alike should be aware of given the possibility of implementation in mainstream clinical practice. Although there remain significant ethical and regulatory challenges, AI powered systems offer great potential to shape how healthcare systems of the future operate and clinicians are key stakeholders in this process.",2020-12-01,0,1985,74,158
1912,33269107,The Application of Artificial Intelligence in the Genetic Study of Alzheimer's Disease,"Alzheimer's disease (AD) is a neurodegenerative disease in which genetic factors contribute approximately 70% of etiological effects. Studies have found many significant genetic and environmental factors, but the pathogenesis of AD is still unclear. With the application of microarray and next-generation sequencing technologies, research using genetic data has shown explosive growth. In addition to conventional statistical methods for the processing of these data, artificial intelligence (AI) technology shows obvious advantages in analyzing such complex projects. This article first briefly reviews the application of AI technology in medicine and the current status of genetic research in AD. Then, a comprehensive review is focused on the application of AI in the genetic research of AD, including the diagnosis and prognosis of AD based on genetic data, the analysis of genetic variation, gene expression profile, gene-gene interaction in AD, and genetic analysis of AD based on a knowledge base. Although many studies have yielded some meaningful results, they are still in a preliminary stage. The main shortcomings include the limitations of the databases, failing to take advantage of AI to conduct a systematic biology analysis of multilevel databases, and lack of a theoretical framework for the analysis results. Finally, we outlook the direction of future development. It is crucial to develop high quality, comprehensive, large sample size, data sharing resources; a multi-level system biology AI analysis strategy is one of the development directions, and computational creativity may play a role in theory model building, verification, and designing new intervention protocols for AD.",2020-12-01,0,1703,86,158
748,33071481,A survey on artificial intelligence approaches in supporting frontline workers and decision makers for the COVID-19 pandemic,"While the world has experience with many different types of infectious diseases, the current crisis related to the spread of COVID-19 has challenged epidemiologists and public health experts alike, leading to a rapid search for, and development of, new and innovative solutions to combat its spread. The transmission of this virus has infected more than 18.92 million people as of August 6, 2020, with over half a million deaths across the globe; the World Health Organization (WHO) has declared this a global pandemic. A multidisciplinary approach needs to be followed for diagnosis, treatment and tracking, especially between medical and computer sciences, so, a common ground is available to facilitate the research work at a faster pace. With this in mind, this survey paper aimed to explore and understand how and which different technological tools and techniques have been used within the context of COVID-19. The primary contribution of this paper is in its collation of the current state-of-the-art technological approaches applied to the context of COVID-19, and doing this in a holistic way, covering multiple disciplines and different perspectives. The analysis is widened by investigating Artificial Intelligence (AI) approaches for the diagnosis, anticipate infection and mortality rate by tracing contacts and targeted drug designing. Moreover, the impact of different kinds of medical data used in diagnosis, prognosis and pandemic analysis is also provided. This review paper covers both medical and technological perspectives to facilitate the virologists, AI researchers and policymakers while in combating the COVID-19 outbreak.",2020-12-01,4,1648,124,158
1883,33325829,Paradigm Shift Toward Digital Neuropsychology and High-Dimensional Neuropsychological Assessments: Review,"Neuropsychologists in the digital age have increasing access to emerging technologies. The National Institutes of Health (NIH) initiatives for behavioral and social sciences have emphasized these developing scientific and technological potentials (eg, novel sensors) for augmented characterization of neurocognitive, behavioral, affective, and social processes. Perhaps these innovative technologies will lead to a paradigm shift from disintegrated and data-poor behavioral science to cohesive and data-rich science that permits improved translation from bench to bedside. The 4 main advances influencing the scientific priorities of a recent NIH Office of Behavioral and Social Sciences Research strategic plan include the following: integration of neuroscience into behavioral and social sciences, transformational advances in measurement science, digital intervention platforms, and large-scale population cohorts and data integration. This paper reviews these opportunities for novel brain-behavior characterizations. Emphasis is placed on the increasing concern of neuropsychology with these topics and the need for development in these areas to maintain relevance as a scientific discipline and advance scientific developments. Furthermore, the effects of such advancements necessitate discussion and modification of training as well as ethical and legal mandates for neuropsychological research and praxes.",2020-12-01,0,1413,105,158
751,33068919,Transboundary Pathogenic microRNA Analysis Framework for Crop Fungi Driven by Biological Big Data and Artificial Intelligence Model,"Plant fungal diseases have been affecting the world's agricultural production and economic levels for a long time, such as rice blast, gray tomato mold, potato late blight etc. Recent studies have shown that fungal pathogens transmit microRNA as an effector to host plants for infection. However, bioassay-based verification analysis is time-consuming and challenging, and it is difficult to analyze from a global perspective. With the accumulation of fungal and plant-related data, data analysis methods can be used to analyze pathogenic fungal microRNA further. Based on the microRNA expression data of fungal pathogens infecting plants before and after, this paper discusses the selection strategy of sample data, the extraction strategy of pathogenic fungal microRNA, the prediction strategy of a fungal pathogenic microRNA target gene, the bicluster-based fungal pathogenic microRNA functional analysis strategy and experimental verification methods. A general analysis pipeline based on machine learning and bicluster-based function module was proposed for plant-fungal pathogenic microRNA.The pipeline proposed in this paper is applied to the infection process of Magnaporthe oryzae and the infection process of potato late blight. It has been verified to prove the feasibility of the pipeline. It can be extended to other relevant crop pathogen research, providing a new idea for fungal research on plant diseases. It can be used as a reference for understanding the interaction between fungi and plants.",2020-12-01,0,1512,131,158
1882,33326953,Artificial Intelligence in Thyroid Fine Needle Aspiration Biopsies,"Background:                    From cell phones to aerospace, artificial intelligence (AI) has wide-reaching influence in the modern age. In this review, we discuss the application of AI solutions to an equally ubiquitous problem in cytopathology - thyroid fine needle aspiration biopsy (FNAB). Thyroid nodules are common in the general population, and FNAB is the sampling modality of choice. The resulting prevalence in the practicing pathologist's daily workload makes thyroid FNAB an appealing target for the application of AI solutions.              Summary:                    This review summarizes all available literature on the application of AI to thyroid cytopathology. We follow the evolution from morphometric analysis to convolutional neural networks. We explore the application of AI technology to different questions in thyroid cytopathology, including distinguishing papillary carcinoma from benign, distinguishing follicular adenoma from carcinoma and identifying non-invasive follicular thyroid neoplasm with papillary-like nuclear features by key words and phrases. Key Messages: The current literature shows promise towards the application of AI technology to thyroid fine needle aspiration biopsy. Much work is needed to define how this powerful technology will be of best use to the future of cytopathology practice.",2020-12-01,0,1340,66,158
1336,33180692,Can machine learning optimize the efficiency of the operating room in the era of COVID-19?,"The cancellation of large numbers of surgical procedures because of the coronavirus disease 2019 (COVID-19) pandemic has drastically extended wait lists and negatively affected patient care and experience. As many facilities resume clinical work owing to the currently low burden of disease in our community, we are faced with operative booking protocols and procedures that are not mathematically designed to optimize efficiency. Using a subset of artificial intelligence called ""machine learning,"" we have shown how the use of operating time can be optimized with a custom Python (a high-level programming language) script and an open source machine-learning algorithm, the ORTools software suite from the Google AI division of Alphabet Inc. This allowed the creation of customized models to optimize the efficiency of operating room booking times, which resulted in a reduction in nursing overtime of 21% - a theoretical cost savings of $469 000 over 3 years.",2020-12-01,0,962,90,158
759,33044239,Identification of acute kidney injury subphenotypes,"Purpose of review:                    AKI is a complex clinical syndrome with many causes and there is a broad range of clinical presentations that vary according to duration, severity and context. Established consensus definitions of AKI are nonspecific and limited to kidney function. This reduces treatment options to generic approaches rather than individualized, cause-based strategies that have limited both understanding and management of AKI.              Recent findings:                    The context and the temporal phase of kidney injury are critical features in the course of AKI and critical to timing-relevant intervention. These features are missing in generic definitions and terms used to describe AKI. Subphenotypes of AKI can be identified from novel damage biomarkers, from functional changes including creatinine trajectories, from the duration of change and from associated clinical characteristics and comorbidities. Subphenotype parameters can be combined in risk scores, or by association strategies ranging from a simple function-damage matrix to complex methods, such as machine learning. Examples of such strategies are reviewed along with tentative proposals for a revised nomenclature to facilitate description of AKI subphenotypes.              Summary:                    Appropriate intervention requires refinement of the nomenclature of AKI to identify subphenotypes that facilitate correctly timed and selectively targeted intervention.",2020-12-01,0,1475,51,158
760,33043272,Redesigning COVID-19 Care With Network Medicine and Machine Learning,"Emerging evidence regarding COVID-19 highlights the role of individual resistance and immune function in both susceptibility to infection and severity of disease. Multiple factors influence the response of the human host on exposure to viral pathogens. Influencing an individual's susceptibility to infection are such factors as nutritional status, physical and psychosocial stressors, obesity, protein-calorie malnutrition, emotional resilience, single-nucleotide polymorphisms, environmental toxins including air pollution and firsthand and secondhand tobacco smoke, sleep habits, sedentary lifestyle, drug-induced nutritional deficiencies and drug-induced immunomodulatory effects, and availability of nutrient-dense food and empty calories. This review examines the network of interacting cofactors that influence the host-pathogen relationship, which in turn determines one's susceptibility to viral infections like COVID-19. It then evaluates the role of machine learning, including predictive analytics and random forest modeling, to help clinicians assess patients' risk for development of active infection and to devise a comprehensive approach to prevention and treatment.",2020-12-01,1,1182,68,158
763,33040399,Digital cardiovascular care in COVID-19 pandemic: A potential alternative?,"Background:                    Cardiovascular patients are at increased risk of acquiring coronavirus disease 2019 (COVID-19) infection while their visit to healthcare facilities. There is a need for alternative tools for optimal monitoring and management of cardiovascular patients in the present pandemic situation. Digital health care may prove to be a new revolutionary tool to protect cardiovascular patients from coronavirus disease by avoiding routine visits to health care facilities that are already overwhelmed with COVID-19 patients.              Methods:                    To evaluate the role of digital health care in the present era of the COVID-19 pandemic, we have reviewed the published literature on digital health services providing cardiovascular care.              Results and conclusion:                    Digital health including telemedicine services, robotic telemedicine carts, use of artificial intelligence and machine learning, use of digital gadgets like smartwatches and web-based applications may be a safe alternative for the management of cardiovascular patients in the present pandemic situation.",2020-12-01,3,1134,74,158
764,33039428,The role of computational methods for automating and improving clinical target volume definition,"Treatment planning in radiotherapy distinguishes three target volume concepts: the gross tumor volume (GTV), the clinical target volume (CTV), and the planning target volume (PTV). Over time, GTV definition and PTV margins have improved through the development of novel imaging techniques and better image guidance, respectively. CTV definition is sometimes considered the weakest element in the planning process. CTV definition is particularly complex since the extension of microscopic disease cannot be seen using currently available in-vivo imaging techniques. Instead, CTV definition has to incorporate knowledge of the patterns of tumor progression. While CTV delineation has largely been considered the domain of radiation oncologists, this paper, arising from a 2019 ESTRO Physics research workshop, discusses the contributions that medical physics and computer science can make by developing computational methods to support CTV definition. First, we overview the role of image segmentation algorithms, which may in part automate CTV delineation through segmentation of lymph node stations or normal tissues representing anatomical boundaries of microscopic tumor progression. The recent success of deep convolutional neural networks has also enabled learning entire CTV delineations from examples. Second, we discuss the use of mathematical models of tumor progression for CTV definition, using as example the application of glioma growth models to facilitate GTV-to-CTV expansion for glioblastoma that is consistent with neuroanatomy. We further consider statistical machine learning models to quantify lymphatic metastatic progression of tumors, which may eventually improve elective CTV definition. Lastly, we discuss approaches to incorporate uncertainty in CTV definition into treatment plan optimization as well as general limitations of the CTV concept in the case of infiltrating tumors without natural boundaries.",2020-12-01,1,1932,96,158
1331,33186867,"Description, prediction and causation: Methodological challenges of studying child and adolescent development","Scientific research can be categorized into: a) descriptive research, with the main goal to summarize characteristics of a group (or person); b) predictive research, with the main goal to forecast future outcomes that can be used for screening, selection, or monitoring; and c) explanatory research, with the main goal to understand the underlying causal mechanism, which can then be used to develop interventions. Since each goal requires different research methods in terms of design, operationalization, model building and evaluation, it should form an important basis for decisions on how to set up and execute a study. To determine the extent to which developmental research is motivated by each goal and how this aligns with the research designs that are used, we evaluated 100 publications from the Consortium on Individual Development (CID). This analysis shows that the match between research goal and research design is not always optimal. We discuss alternative techniques, which are not yet part of the developmental scientist's standard toolbox, but that may help bridge some of the lurking gaps that developmental scientists encounter between their research design and their research goal. These include unsupervised and supervised machine learning, directed acyclical graphs, Mendelian randomization, and target trials.",2020-12-01,2,1334,109,158
1322,33202280,Partners for life: building microbial consortia for the future,"New technologies have allowed researchers to better design, build, and analyze complex consortia. These developments are fueling a wider implementation of consortium-based bioprocessing by leveraging synthetic biology, delivering on the field's multitudinous promises of higher efficiencies, superior resiliency, augmented capabilities, and modular bioprocessing. Here we chronicle current progress by presenting a range of screening, computational, and biomolecular tools enabling robust population control, efficient division of labor, and programmatic spatial organization; furthermore, we detail corresponding advancements in areas including machine learning, biocontainment, and standardization. Additionally, we show applications in myriad sectors, including medicine, energy and waste sustainability, chemical production, agriculture, and biosensors. Concluding remarks outline areas of growth that will promote the utilization of complex community structures across the biotechnology spectrum.",2020-12-01,0,1001,62,158
1877,33333829,A Systematic Review of Statistical and Machine Learning Methods for Electrical Power Forecasting with Reported MAPE Score,"Electric power forecasting plays a substantial role in the administration and balance of current power systems. For this reason, accurate predictions of service demands are needed to develop better programming for the generation and distribution of power and to reduce the risk of vulnerabilities in the integration of an electric power system. For the purposes of the current study, a systematic literature review was applied to identify the type of model that has the highest propensity to show precision in the context of electric power forecasting. The state-of-the-art model in accurate electric power forecasting was determined from the results reported in 257 accuracy tests from five geographic regions. Two classes of forecasting models were compared: classical statistical or mathematical (MSC) and machine learning (ML) models. Furthermore, the use of hybrid models that have made significant contributions to electric power forecasting is identified, and a case of study is applied to demonstrate its good performance when compared with traditional models. Among our main findings, we conclude that forecasting errors are minimized by reducing the time horizon, that ML models that consider various sources of exogenous variability tend to have better forecast accuracy, and finally, that the accuracy of the forecasting models has significantly increased over the last five years.",2020-12-01,1,1393,121,158
1881,33327400,Bird Eye View of Protein Subcellular Localization Prediction,"Proteins are made up of long chain of amino acids that perform a variety of functions in different organisms. The activity of the proteins is determined by the nucleotide sequence of their genes and by its 3D structure. In addition, it is essential for proteins to be destined to their specific locations or compartments to perform their structure and functions. The challenge of computational prediction of subcellular localization of proteins is addressed in various in silico methods. In this review, we reviewed the progress in this field and offered a bird eye view consisting of a comprehensive listing of tools, types of input features explored, machine learning approaches employed, and evaluation matrices applied. We hope the review will be useful for the researchers working in the field of protein localization predictions.",2020-12-01,1,835,60,158
2599,32563708,The compatibility of theoretical frameworks with machine learning analyses in psychological research,"Supervised machine learning has been increasingly used in psychology and psychiatry research. Machine learning offers an important advantage over traditional statistical analyses: statistical model training in example data to enhance predictions in external test data. Additional advantages include advanced, improved statistical algorithms, and empirical methods to select a smaller set of predictor variables. Yet machine learning researchers often use large numbers of predictor variables, without using theory to guide variable selection. Such approach leads to Type I error, spurious findings, and decreased generalizability. We discuss the importance of theory to the psychology field. We offer suggestions for using theory to drive variable selection and data analyses using machine learning in psychological research, including an example from the cyberpsychology field.",2020-12-01,0,878,100,158
1880,33328300,Soft sensors that can feel it all,"Soft materials and machine learning combine to enable a sensor that distinguishes bending, stretching, and compression.",2020-12-01,1,119,33,158
780,33035522,I tried a bunch of things: The dangers of unexpected overfitting in classification of brain data,"Machine learning has enhanced the abilities of neuroscientists to interpret information collected through EEG, fMRI, and MEG data. With these powerful techniques comes the danger of overfitting of hyperparameters which can render results invalid. We refer to this problem as 'overhyping' and show that it is pernicious despite commonly used precautions. Overhyping occurs when analysis decisions are made after observing analysis outcomes and can produce results that are partially or even completely spurious. It is commonly assumed that cross-validation is an effective protection against overfitting or overhyping, but this is not actually true. In this article, we show that spurious results can be obtained on random data by modifying hyperparameters in seemingly innocuous ways, despite the use of cross-validation. We recommend a number of techniques for limiting overhyping, such as lock boxes, blind analyses, pre-registrations, and nested cross-validation. These techniques, are common in other fields that use machine learning, including computer science and physics. Adopting similar safeguards is critical for ensuring the robustness of machine-learning techniques in the neurosciences.",2020-12-01,4,1199,96,158
1970,32930431,Rational Design of Semiconductor-Based Chemiresistors and their Libraries for Next-Generation Artificial Olfaction,"Artificial olfaction based on gas sensor arrays aims to substitute for, support, and surpass human olfaction. Like mammalian olfaction, a larger number of sensors and more signal processing are crucial for strengthening artificial olfaction. Due to rapid progress in computing capabilities and machine-learning algorithms, on-demand high-performance artificial olfaction that can eclipse human olfaction becomes inevitable once diverse and versatile gas sensing materials are provided. Here, rational strategies to design a myriad of different semiconductor-based chemiresistors and to grow gas sensing libraries enough to identify a wide range of odors and gases are reviewed, discussed, and suggested. Key approaches include the use of p-type oxide semiconductors, multinary perovskite and spinel oxides, carbon-based materials, metal chalcogenides, their heterostructures, as well as heterocomposites as distinctive sensing materials, the utilization of bilayer sensor design, the design of robust sensing materials, and the high-throughput screening of sensing materials. In addition, the state-of-the-art and key issues in the implementation of electronic noses are discussed. Finally, a perspective on chemiresistive sensing materials for next-generation artificial olfaction is provided.",2020-12-01,2,1294,114,158
1384,33094613,Cannabinoid Receptor Subtype 2 (CB2R) in a Multitarget Approach: Perspective of an Innovative Strategy in Cancer and Neurodegeneration,"The cannabinoid receptor subtype 2 (CB2R) represents an interesting and new therapeutic target for its involvement in the first steps of neurodegeneration as well as in cancer onset and progression. Several studies, focused on different types of tumors, report a promising anticancer activity induced by CB2R agonists due to their ability to reduce inflammation and cell proliferation. Moreover, in neuroinflammation, the stimulation of CB2R, overexpressed in microglial cells, exerts beneficial effects in neurodegenerative disorders. With the aim to overcome current treatment limitations, new drugs can be developed by specifically modulating, together with CB2R, other targets involved in such multifactorial disorders. Building on successful case studies of already developed multitarget strategies involving CB2R, in this Perspective we aim at prompting the scientific community to consider new promising target associations involving HDACs (histone deacetylases) and  receptors by employing modern approaches based on molecular hybridization, computational polypharmacology, and machine learning algorithms.",2020-12-01,0,1115,134,158
1257,32057617,Magnetic Resonance Texture Analysis in Alzheimer's disease,"Texture analysis is an emerging field that allows mathematical detection of changes in MRI signals that are not visible among image pixels. Alzheimer's disease, a progressive neurodegenerative disease, is the most common cause of dementia. Recently, multiple texture analysis studies in patients with Alzheimer's disease have been performed. This review summarizes the main contributors to Alzheimer's disease-associated cognitive decline, presents a brief overview of texture analysis, followed by review of various MR imaging texture analysis applications in Alzheimer's disease. We also discuss the current challenges for widespread clinical utilization. MR texture analysis could potentially be applied to develop neuroimaging biomarkers for use in Alzheimer's disease clinical trials and diagnosis.",2020-12-01,1,803,58,158
785,33027147,Artificial intelligence to guide management of acute kidney injury in the ICU: a narrative review,"Purpose of review:                    Acute kidney injury (AKI) frequently complicates hospital admission, especially in the ICU or after major surgery, and is associated with high morbidity and mortality. The risk of developing AKI depends on the presence of preexisting comorbidities and the cause of the current disease. Besides, many other parameters affect the kidney function, such as the state of other vital organs, the host response, and the initiated treatment. Advancements in the field of informatics have led to the opportunity to store and utilize the patient-related data to train and validate models to detect specific patterns and, as such, predict disease states or outcomes.              Recent findings:                    Machine-learning techniques have also been applied to predict AKI, as well as the patients' outcomes related to their AKI, such as mortality or the need for kidney replacement therapy. Several models have recently been developed, but only a few of them have been validated in external cohorts.              Summary:                    In this article, we provide an overview of the machine-learning prediction models for AKI and its outcomes in critically ill patients and individuals undergoing major surgery. We also discuss the pitfalls and the opportunities related to the implementation of these models in clinical practices.",2020-12-01,0,1373,97,158
1417,32652095,Addressing Reduced Laboratory-Based Pulmonary Function Testing During a Pandemic,"To reduce the spread of the severe acute respiratory syndrome coronavirus 2, many pulmonary function testing (PFT) laboratories have been closed or have significantly reduced their testing capacity. Because these mitigation strategies may be necessary for the next 6 to 18 months to prevent recurrent peaks in disease prevalence, fewer objective measurements of lung function will alter the diagnosis and care of patients with chronic respiratory diseases. PFT, which includes spirometry, lung volume, and diffusion capacity measurement, is essential to the diagnosis and management of patients with asthma, COPD, and other chronic lung conditions. Both traditional and innovative alternatives to conventional testing must now be explored. These may include peak expiratory flow devices, electronic portable spirometers, portable exhaled nitric oxide measurement, airwave oscillometry devices, and novel digital health tools such as smartphone microphone spirometers and mobile health technologies along with integration of machine learning approaches. The adoption of some novel approaches may not merely replace but could improve existing management strategies and alter common diagnostic paradigms. With these options comes important technical, privacy, ethical, financial, and medicolegal barriers that must be addressed. However, the coronavirus disease 19 pandemic also presents a unique opportunity to augment conventional testing by including innovative and emerging approaches to measuring lung function remotely in patients with respiratory disease. The benefits of such an approach have the potential to enhance respiratory care and empower patient self-management well beyond the current global pandemic.",2020-12-01,6,1716,80,158
1387,33092893,Distributional Reinforcement Learning in the Brain,"Learning about rewards and punishments is critical for survival. Classical studies have demonstrated an impressive correspondence between the firing of dopamine neurons in the mammalian midbrain and the reward prediction errors of reinforcement learning algorithms, which express the difference between actual reward and predicted mean reward. However, it may be advantageous to learn not only the mean but also the complete distribution of potential rewards. Recent advances in machine learning have revealed a biologically plausible set of algorithms for reconstructing this reward distribution from experience. Here, we review the mathematical foundations of these algorithms as well as initial evidence for their neurobiological implementation. We conclude by highlighting outstanding questions regarding the circuit computation and behavioral readout of these distributional codes.",2020-12-01,1,886,50,158
1925,33246838,Emerging use of machine learning and advanced technologies to assess red cell quality,"Improving blood product quality and patient outcomes is an accepted goal in transfusion medicine research. Thus, there is an urgent need to understand the potential adverse effects on red blood cells (RBCs) during pre-transfusion storage. Current assessment techniques of these degradation events, termed ""storage lesions"", are subjective, labor-intensive, and complex. Here we describe emerging technologies that assess the biochemical, biophysical, and morphological characteristics of RBC storage lesions. Of these emerging techniques, machine learning (ML) has shown potential to overcome the limitations of conventional RBC assessment methods. Our previous work has shown that neural networks can extract chronological progressions of morphological changes in RBCs during storage without human input. We hypothesize that, with broader training and testing of multivariate data (e.g., varying donor factors and manufacturing methods), ML can further our understanding of clinical transfusion outcomes in multiple patient groups.",2020-12-01,0,1032,85,158
1974,32921526,"Past, present and future EEG in the clinical workup of dementias","Electroencephalography (EEG), as non-invasive, global measure of neuronal activity, is a prime candidate functional marker of synapse dysfunction and loss in dementias. Nevertheless, EEG currently has no established role in the clinical workup of individual patients. This opinion paper presents our critical view on why EEG has so far failed to keep its promise, and where we believe EEG will be clinically useful for patients threatened with cognitive decline in the future. Individual EEGs are an integral outcome of many causally intermixing upstream factors contributing to dementia. Therefore, EEG cannot become a clinically useful ""simple"" stand-alone biomarker of some pathognomic accumulations of specific brain proteins, but rather offer unique opportunities for more comprehensive and richly faceted insights into the functional status of brain systems. EEG may thus remain an essential window into the brain when it comes to the at-risk and presymptomatic phases of dementias, where it can be uniquely informative about concepts such as burdens of plasticity and repair, cognitive reserve, and sleep. Jointly with rapid gains in usability, portability, machine learning, closed loop systems, and understanding of the role of EEG-based sleep stages for memory and brain repair, EEG may come to keep its initial promise after all.",2020-12-01,0,1340,64,158
1436,32632888,A-learning: A new formulation of associative learning theory,"We present a new mathematical formulation of associative learning focused on non-human animals, which we call A-learning. Building on current animal learning theory and machine learning, A-learning is composed of two learning equations, one for stimulus-response values and one for stimulus values (conditioned reinforcement). A third equation implements decision-making by mapping stimulus-response values to response probabilities. We show that A-learning can reproduce the main features of: instrumental acquisition, including the effects of signaled and unsignaled non-contingent reinforcement; Pavlovian acquisition, including higher-order conditioning, omission training, autoshaping, and differences in form between conditioned and unconditioned responses; acquisition of avoidance responses; acquisition and extinction of instrumental chains and Pavlovian higher-order conditioning; Pavlovian-to-instrumental transfer; Pavlovian and instrumental outcome revaluation effects, including insight into why these effects vary greatly with training procedures and with the proximity of a response to the reinforcer. We discuss the differences between current theory and A-learning, such as its lack of stimulus-stimulus and response-stimulus associations, and compare A-learning with other temporal-difference models from machine learning, such as Q-learning, SARSA, and the actor-critic model. We conclude that A-learning may offer a more convenient view of associative learning than current mathematical models, and point out areas that need further development.",2020-12-01,0,1566,60,158
1414,32655135,Future possibilities for artificial intelligence in the practical management of hypertension,"The use of artificial intelligence in numerous prediction and classification tasks, including clinical research and healthcare management, is becoming increasingly more common. This review describes the current status and a future possibility for artificial intelligence in blood pressure management, that is, the possibility of accurately predicting and estimating blood pressure using large-scale data, such as personal health records and electronic medical records. Individual blood pressure continuously changes because of lifestyle habits and the environment. This review focuses on two topics regarding controlling changing blood pressure: a novel blood pressure measurement system and blood pressure analysis using artificial intelligence. Regarding the novel blood pressure measurement system, we compare the conventional cuff-less method with the analysis of pulse waves using artificial intelligence for blood pressure estimation. Then, we describe the prediction of future blood pressure values using machine learning and deep learning. In addition, we summarize factor analysis using ""explainable AI"" to solve a black-box problem of artificial intelligence. Overall, we show that artificial intelligence is advantageous for hypertension management and can be used to establish clinical evidence for the practical management of hypertension.",2020-12-01,0,1352,92,158
1961,32948476,Developing an AI project,"Artificial intelligence applications can very powerful in areas of speech recognition, image processing and identification, medical diagnosis and clustering to name a few. There is a perception that developing your own artificial intelligence (AI) application can be a daunting task, requiring in-depth knowledge and programming skills. This is not entirely true since many desktop and laptop systems have computing power that can accommodate machine- and deeplearning development, the available options for code development and a broad support base. A generic guide in developing a platform for AI project development is presented.",2020-12-01,0,632,24,158
1902,33291844,From Ethnomedicine to Plant Biotechnology and Machine Learning: The Valorization of the Medicinal Plant Bryophyllum sp,"The subgenus Bryophyllum includes about 25 plant species native to Madagascar, and is widely used in traditional medicine worldwide. Different formulations from Bryophyllum have been employed for the treatment of several ailments, including infections, gynecological disorders, and chronic diseases, such as diabetes, neurological and neoplastic diseases. Two major families of secondary metabolites have been reported as responsible for these bioactivities: phenolic compounds and bufadienolides. These compounds are found in limited amounts in plants because they are biosynthesized in response to different biotic and abiotic stresses. Therefore, novel approaches should be undertaken with the aim of achieving the phytochemical valorization of Bryophyllum sp., allowing a sustainable production that prevents from a massive exploitation of wild plant resources. This review focuses on the study of phytoconstituents reported on Bryophyllum sp.; the application of plant tissue culture methodology as a reliable tool for the valorization of bioactive compounds; and the application of machine learning technology to model and optimize the full phytochemical potential of Bryophyllum sp. As a result, Bryophyllum species can be considered as a promising source of plant bioactive compounds, with enormous antioxidant and anticancer potential, which could be used for their large-scale biotechnological exploitation in cosmetic, food, and pharmaceutical industries.",2020-12-01,0,1466,118,158
